{
  "id": "aa34c1fd-c76b-4a3d-b1a5-2a037aa368c3",
  "title": "Presentation: Taking LLMs out of the Black Box: A Practical Guide to Human-in-the-Loop Distillation",
  "link": "https://www.infoq.com/presentations/llm-disttilation/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Ines Montani discusses practical solutions for using the latest models in real-world applications and distilling their knowledge into smaller and faster components. By Ines Montani",
  "author": "Ines Montani",
  "published": "Wed, 05 Feb 2025 13:40:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Transcripts",
    "InfoQ Dev Summit Munich 2024",
    "Artificial Intelligence",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "presentation"
  ],
  "byline": "Ines Montani",
  "length": 41202,
  "excerpt": "Ines Montani discusses practical solutions for using the latest models in real-world applications and distilling their knowledge into smaller and faster components.",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250205131506/apple-touch-icon.png",
  "text": "InfoQ Homepage Presentations Taking LLMs out of the Black Box: A Practical Guide to Human-in-the-Loop Distillation Transcript Montani: I'll be talking about taking large language models out of the black box, and a few practical tips that hopefully you can apply in your work today. Some of you might know me from my work on spaCy, which is an open-source library for natural language processing in Python. spaCy was really designed from day one to be used in real products and be used in production. That also means we had to do a lot of the boring software development stuff like backwards compatibility, making sure we don't break people's code. This actually had a pretty nice unintended side effect more recently, which is that ChatGPT is actually really good at writing spaCy code. You can try that out if you want. We also developed Prodigy, which is a modern annotation tool for creating training and evaluation data for machine learning developers and machine learning models. Prodigy is fully scriptable in Python, so it allows for a lot of nice, semi-automated workflows, including, actually, a lot of the ideas that I'll be talking about in my talk. Software in Industry As an industry developing software, we have really built up a lot of best practices over the years and a lot of ideas of how we want our software to behave and what's good software development. One of those is, we want our software to be modular. We want to have components that we can work on independently. We want to have building blocks that we can work with. We also want our tools and our software to be transparent. We want to understand what's going on, be able to look under the hood, and also debug if something is going wrong. That also means we want to be able to explain to others why our system is doing something, or, in the case of a machine learning model, why we get a specific result. Many solutions also need to be data private. Who works in a field where data privacy is important? That's an important topic. We want systems to be reliable, not just randomly go down. Whatever we build also needs to be affordable, whatever that means in a specific context. Maybe you're working on a budget, maybe you have no money. If we're building something, it needs to fit. This introduces a lot of problems, if we're looking at new ways and new technologies. If we're working with black box models, we can't really look under the hood, and we can't really understand what's going on. For a lot of them, if we're using an API, we don't even really know how they work. A lot of that is not public. We have these big monoliths that do a whole thing at once. That can be very challenging. Because a lot of the newer models are very large, which is also what makes them so good, it's not really efficient or sustainable to run them yourself in-house, which means we need to consume them via a third-party API. That also means we need to send our data to someone else's server. We're at the mercy of an API provider. If it goes down, it goes down. If it's slow, that's how it is. The costs can also really add up if you're using a model via an API at runtime. How can we fix this, and how can we still use all these exciting latest technologies that are really good, while also maintaining the best practices that we've built over years of developing software. That's what I'm going to cover in my talk. To maybe start off with a practical example, here's something that's maybe familiar to you and maybe similar if you're working in NLP, to something that you were tasked with in your job. Let's say you work for an electronics retailer or a company producing phones, and you're getting lots of reviews in from customers, and you've collected them all, and now you want to analyze what people are saying about your product. First, what you want to do is you want to find mentions of your products in these reviews and find those that are relevant. You also want to link them to your catalog that you already have, where you have all meta information about your products. Then you want to extract what people are saying, so you have different categories, like battery, camera, performance, design, and you want to extract whether people like these or not, which is also often referred to as aspect-oriented sentiment analysis. Then, finally, if you have all that information, you want to add those results to a database, maybe the database that also has your catalog. As you can see, there are some parts of this that are actually quite straightforward. We have, battery life is incredible. That's easy. You can definitely extract that. There are other parts that aren't necessarily so straightforward because of its language, and language can often be vague. Here, the reviewer says, never had to carry a power bank before, and now I need it all the time. From the context that we have about the language and the world, we know that this means that the battery life is bad. It's not explicit in the text. That's really a use case that benefits from machine learning. There are a lot of larger models these days that are really good at that, because they're trained on such a vast volume of text, and they're really good at understanding, \"This type of context\", in the text. Using in-context learning, you could basically prompt the model. You can provide it examples of your text, and it can respond with an answer, for example, of the sentiment you're interested in. Of course, if you think about what you actually need here, you have a specific use case, and you really only need a small subset of what the model is able to do. You really need the context. You're not interested in talking to it about the weather. There's really a very specific thing that you want. The question is, what if we could just take that out? What if we can extract only the part that we're interested in into a model that can also then be much smaller and much more specific, because all we want it to do is predict those sentiment categories. While there has been a lot of talk about in-context learning, because that's new, and it's also what a lot of the research focuses on, it doesn't mean that transfer learning is somehow outdated or has been replaced. It's simply a different technique. You might be familiar with models like BERT and their embeddings and all kinds of different local variants, like CamemBERT, and these embeddings encode a lot of very important and very relevant contextual information that you can initialize your model with, and add a task-specific network on top. The thing is, if we're looking at the research, we'll actually see that even the most classic BERT, base, is still very competitive and achieves very good results compared to zero-shot or few-shot in-context learning. What's also important to keep in mind here is that these are all academic benchmarks. These are calculated based on datasets we can't control. That's the idea of a benchmark. If we're already getting these very promising and good results using benchmark datasets, we'll probably be able to achieve even better results if we have control over the data which we have in our use case. What we want to do is, if we want our large generative model to do a specific thing, we start out with our text, and we start out with the prompt based on a prompt template specific to a thing we want to extract. Then we pass that to the model. What we get back is raw output, usually in the form of an answer, depending on the prompt we gave it. Then we can use a corresponding parser that corresponds to the prompt template in order to parse out the task specific output. In our case, we're not after a conversation. We're after structured data that expresses the categories that we've already defined. What we also want to do is we don't just want to match the large language model's performance. Ideally, we want to do it even better, because we have the capability to do that. If the model makes mistakes, we want to correct them. What we can do is we can pass our task output to an annotation workflow and really create a dataset that's very specific to what we're looking to extract. Using transfer learning, we can use that to distill a task-specific model that only performs the one task we're interested in, using what we already have in the weights of the large generative model. Close the Gap Between Prototype and Production This is both pretty exciting and very promising, but one thing we've definitely seen, if you get to work, is that a lot of projects these days, they really get stuck in this phase that I also call the prototype plateau. You start working. It's all very exciting, and it's working. When it comes to actually shipping your system that you've built, you realize that it doesn't work. There are a lot of reasons for that, and also solutions that are actually really important to keep in mind before you start building. In order to close the gap between prototype and production, one important thing is you want to be standardizing your inputs and outputs. You want to have the same workflow during prototyping as you have during production. If your prototype takes random human generated text that you type in and outputs a human readable text response, but your production system needs structured data, then you're going to have a problem, and it might not actually translate so well. You also want to start with an evaluation, just like when you're writing software, you're writing tests. For a machine learning model, the equivalent is an evaluation. You want examples where you know the answer, and that you can check and so you actually know if your system is improving or not. That's something that's often glossed over. A lot of people if you're excited in building something, you might go by just a vibes-based evaluation. Does it feel good? If you actually want to assess how is my model performing, you want an evaluation with accuracy scores that you can compare. Of course, accuracy is not everything. Especially if you're coming from a research background, you can be very focused on just optimizing those scores. A high accuracy in your model is useless if the model isn't actually doing what you want it to do, and if it's useful in your application. In an applied context, you don't just want to optimize for a score, you also want to test, is it actually useful? Whatever that means in your context. That also requires working on your data iteratively, just like with code. Usually, the first idea you have is usually not what you ship to production, and the same goes for data. You want to have a workflow where you can quickly try things out, and ideally also tooling to help with that, so you don't need to schedule large meetings and spend hours to try out every idea you have. Finally, we're working with language here, and that's really important to keep in mind. While as developers, we really like to fit things neatly into boxes, language doesn't work that way. It's usually vaguely gesturing at things. There's a lot of ambiguity in language that we have to keep in mind, it's not just data or it's not just vectors. On the other hand, we can also use that to our advantage. There's a lot in the language that helps us express things and get our point across, and that generalizes across language very well. If we could identify these parts, we can actually use that to our advantage in the application, and make the problem easier for our model. These are also things that we really thought about a lot when developing our tools. Because I think if you're building developer tools, that's one of the problems you want to address. How can we make it easier for people to standardize workflows between prototype and production, and actually ship things and not just get stuck in the prototype plateau? Here's an example of a prototype we might build for an application. We have a large generative model, and what we can do, and something that we've actually built with spaCy LLM is, have a way to prompt the model and transform the output, and parse it into structured data. Even while you're trying things out without any data required, you can use a large language model to create the structured data for you, and what you get out in the end is an object that contains that structured data. You can, of course, ship this to production the way it is, but you can also work on replacing the large generative model at development time so that at runtime you end up with distilled task-specific components that perform only the parts that you're interested in, and that are fully modular and also transparent, and usually much smaller and faster as well. The output in that case is also the same. You're also getting this structured machine facing object that you can standardize on. Human in the Loop As I said previously, of course we don't just want to match what the large generative model is doing. We actually want to make it better. We want to correct its mistakes. For that, we need a human at some point in the loop. That's a very important step here. To give you an example how that works, we start off with a model and all the weights it has available. As a first step, as I mentioned before, we want to have a continuous evaluation. We need a way to figure out our baseline. What are we up against? What's the performance we get out of the box without doing anything? Otherwise, you'll have no idea whether what you're doing actually makes a difference or not. Now we can use all the weights we have available in that model and prompt it, and it will return whatever data we ask for, using everything that it has available. We can pipe that forward into an annotation environment where we can look at just the exact structured data and make corrections and very quickly move through that data and create a dataset that's really specific to the task, like the aspect-oriented sentiment predictions, for instance. With transfer learning, create a component that only performs that. Of course, here, it comes in handy that we have our evaluation because we want to do that until our distilled model beats and ideally also exceeds that baseline. I'll show you some examples of this later, but you might be surprised how easily you can actually do this and apply this yourself. First, how do we access our human? Going back to that practical example, we have one of these reviews of someone who rated our Nebula phone, \"meh\". As an example, the type of structured data we're after is something like this. For simplicity, for this example, I'll only focus on assuming we have binary values for those categories. Of course, in some cases, you might want to define some other schema and have a scale of like, how much do people like the battery life, and so on? That's the structured data, that's our output. If we're presenting that to the human, a naive approach would be, let's just show the human the text, give the human the categories, and then let them correct it. If you're looking at this, you'll see that this doesn't actually capture these null values. We have no distinction here between a negative response, or no mention of that aspect at all. We can extend this a bit and collect whether it's positive or negative, and have the large generative model make the selection for us. That means you can move through the examples very quickly, and all you have to do is correct the model if it makes a mistake. The big problem we have is that humans are humans, and have a lot of disadvantages. One of them is that humans actually have a cache too and a working memory. If you ask a human to constantly in their head iterate over your label scheme and every aspect that you're interested in, humans are actually quite bad at that. You'll find that humans really lose focus, end up making mistakes, and humans are very bad at consistency. What you can do instead is you can help the human and the human cache and make multiple passes over the data, one per category and one per aspect. While it might seem like a lot more work at first, because you're looking at the same example multiple times, and you're collecting a lot more decisions, it can actually be much faster, because you reduce the cognitive load on the human. I'll show you an example of this later, where a team actually managed to increase their speed by over 10 times by doing this. You have your human, you have a model that helps you create the data, and you're collecting a task-specific dataset that doesn't just match the few-shot or zero-shot baseline, but actually improves upon it. Case Studies To give you some examples of how this works and how this can look in practice. This is the case study we did based on a workshop we held at PyData in New York. The task here was we want to stream in data from our cooking subreddit, and extract dishes, ingredients, and equipment from it. We did that together with the group, and also discussed the data while we were doing it. We used a GPT model during the annotation process to help create the data. In the workshop, we were actually able to beat the few-shot LLM baseline of 74%, which is actually pretty good out of the box without any training data. We beat that in the workshop and created a task-specific model that performed the same or even better, and that model also was more than 20 times faster. If you look at the stats here, we have a model that's 400 megabytes, which is pretty good. You can totally run that yourself, runs on your laptop, runs at over 2000 words per second, so really fast. The data development time, we calculated, how long would it have taken a single person to create all the data for it. That's about eight hours. That's one standard workday. If you think about other things you spend time on as part of your work, you probably spend more time trying to get CUDA installed or trying to get your GPU running. It's really not true anymore that data development is like this absolutely tedious task, even a single developer can do this in a workday. That was very promising. That also inspired the next case study we did, which was with a company called S\u0026P Global. What they're doing, in this project, is they're extracting commodities trading data in real-time. If crude oil is traded somewhere, they extract the price, the participants, the location, and a wide range of other attributes, and they provide that as a structured feed to their customers in real time. Of course, this is information that can really significantly impact the economy and move markets. Their environment is a high security environment. I actually went to visit them in London a while ago, and even within their office, it's very highly segregated. They have this glass box that the analysts sit in, you can only access it with a specific card. It's incredibly important that everything they do runs in-house, and no other third party gets to see it before it's published, which also is like a promise of the data product. That's why their customers are using it. What they did was they moved the dependency of the large language model to development and used that to create data for them. This plus some optimizations of how they actually present the questions to the human, including having simple, often binary questions and making multiple passes over the data, that actually made the whole process more than 10 times faster using a human and the model in a loop. They currently have eight pipelines in production, probably even more by now. This was a very successful project. If you're looking at the stats again, they're achieving very high accuracy. The models are 6 megabytes per pipeline. If you're letting that sink in, this is really tiny. You can train that on your laptop really easily. They run super-fast, at over 16,000 words per second, so they're really a great fit for processing these insights in real time and as quickly as possible. Again, also data development time, that's a single person, so in under two workdays, or with two people, you can create the data needed for a distilled task-specific pipeline in about a day. Totally doable, even if you don't have that many resources. Think of It as a Refactoring Process How did they do it? What's the secret? One of them is, if you're thinking about developing AI solutions, they're really code plus data, and so just like you refactor code, you can also refactor your data. Refactoring code is probably something you do all the time, and you're very familiar with. The same really applies to your data development process. There are different aspects of this. One big part of refactoring is you're breaking down a large problem into individual components, and you're factoring out the different steps and creating reusable functions. That's something we really accepted as the best practice, has a lot of advantages. You can do the same for your machine learning problem and your models. As part of that, the goal is you can make your problems easier. Again, you do this with code a lot, trying to reduce the complexity, and you're allowed to do that. Have an easier system, and make it easier for the model as well. One part of that is factoring out business logic, and separating logic that's really specific to your application, from logic that's general purpose and that maybe applies to any language and doesn't need any external knowledge. I'll show you an example of that later. Again, that's something you do in your code already, and that works well. You can apply that same idea to your data process. Part of refactoring is also reassessing dependencies. Do you need to pull in this massive library at runtime that you only use a function of, or can you replace that? Is there something you can compile during development time so you don't need to use it at runtime? The same is true for machine learning models. Can you move the dependency on the really complex and expensive and maybe intransparent model to development, and have a much cleaner and operationally simpler production environment? Finally, choosing the best techniques, you decide how a specific problem is best solved, and you have this massive toolbox of skills and of techniques that are available, and you pick the one that's the best fit for the task at hand. Make the Problem Easier One thing people really easily forget is that you are allowed to make your problem easier. This is not a competition. This is not academia. You're allowed to reduce the operational complexity, because less operational complexity means that less can go wrong. When I started programming, I didn't know very much. Of course, what I built was all pretty simple. Then as I got more experience, I learned about all of these new things, and of course, wanted to apply them. My code became a lot more complex. Also, if I'm looking back now, back then, I didn't really write comments because it felt like a sign of weakness. If I found an especially complex solution to my problem, commenting meant that I'm admitting that this was hard, so I didn't do that, which also makes it even harder to figure out what was going on and what I was thinking at the time. Then, of course, with more experience my code also became much more straightforward, and I was able to pick the best techniques to get the job done and actually solve it most efficiently, instead of coming up with the most complex and interesting solution. I think it's easy to forget this, because we are in a field that is heavily influenced by academia. In research, what you're doing is you're really building a Commons of Knowledge. You also want to compare the things you're building using standard evaluations. If you're comparing algorithms, everyone needs to evaluate them on the same thing, otherwise, we can't compare them. You also standardize everything that's not the novel thing that you are researching and publishing. Even if what you're standardizing isn't the best possible solution or isn't efficient, it doesn't matter. It needs to be standardized so you can focus on the novel thing you're exploring. On the other hand, if you're building an application and working in applied NLP, what you're doing is you're basically learning from that Commons of Knowledge that was built by academia and provided, and basically pick what works best, and follow some of the latest ideas. You also align your evaluation to project goals. You're not using benchmarks. Your evaluation basically needs to tell you, does this solve the problem, and is this useful in my product or project, or not? You also do whatever works. Whatever gets the job done, you can take advantage of. If that means it's less operationally complex, then that's great. Factor Out Business Logic One big part, as I said, of the refactoring process is separating out what's business logic and what's general-purpose logic. That can be quite tricky, and really requires engaging with your data and your problems. Here we have our SpacePhone review again. If we're looking at that, we can basically break down the two different types of logic in this pseudocode formula. We have the classification task, which is our model that really predicts and processes the language itself. Then we have the business logic which is specific to our application and which can build on top of that. To give you some examples here, general-purpose classification in our example would be stuff like, what are the products? There's a model. What's the model of this phone? Is it a phone? Are we comparing the phone to something else? That requires no outside context, and that's really inherent to the language, and not our specific problem. Then, on the other hand, we have stuff like our catalog reference. That's external. Nothing in the text tells us that. We also have things like, does it have a touch screen? Is it worse than the iPhone 13? The fact, is it the latest model? That is something that can change tomorrow. We have information that can really change over time. While we can include that in the model and in the predictions we make, we'll end up with a system that's immediately outdated, that we constantly need to retrain, and a problem that's a lot harder for the model to build some reasoning around, because we have nothing in the text that tells us that, whereas what we do have is we have our catalog reference, we have dates, we have things we can do math with. This process can be very powerful, but of course, it really is absolutely specific to your problem and requires engaging with it. To give you an example of this idea in context, this is a most recent case study that we published with GitLab. What they're doing is they've processed one year's worth of support tickets and usage questions from different platforms, and they want to extract actionable insights. For example, how can we better support our support engineers in answering questions? What are things that we could add to our docs? Also questions like, how are people adopting new features? How many people have upgraded to the latest version? Are people still stuck on an older version? What are potential problems there, and so on? While these things don't necessarily sound like particularly sensitive information, if you think about it, support tickets can actually include a lot of potentially sensitive data, like paths, details on people's setup. They're working in a high security environment and a hardened offline machine, so whatever they're building, it needs to run internally, and it also needs to be rerun whenever they have new tickets coming in and new data. It needs to be very efficient. Another very important feature of this project was, it needs to be easy to adapt it to new scenarios and new business questions. What's the latest version changes? Features change. Things people are doing change. It needs to be easy to answer different questions that maybe weren't intended when the system was built. Of course, you can do these things as end-to-end prediction tasks, but that means that every time something changes, you need to redo your entire pipeline. Whereas if you can factor out general-purpose features from product specific logic, it becomes a lot easier to add extraction logic for any other future problems and future questions on top. A very simple example of this is, you have things like the software version that is very specific business logic, whereas extracting numbers, makes it a lot easier for the model. If you have that, you can add your business logic on top to determine, is this a version of the software? Is this a link to the docs, and so on? I've linked the case study, explosion.ai/blog/gitlab-support-insights. They've done some pretty interesting things. Also have a pipeline that's super-fast, and are working on adding a conversational output on top. I hope we'll be able to publish more on that, because it's a very cool project that really shows the importance of data refactoring. Reality is not an End-to-End Problem What you can see here, is, as developers, we really love to put things into clear boxes and have this idea of like, if we can just have this one model that can do everything, wouldn't that be great? Unfortunately, reality doesn't really work that way. Reality isn't an end-to-end prediction problem. It's actually very nuanced and very complex. Human-in-the-loop distillation and going from a much larger general-purpose model to a much smaller and more efficient task-specific model really is a refactoring process. You refactor your code, you refactor your data, and that requires engaging with it. Iteration, which, again, is very heavily influenced by the tooling you use, can be a huge help in getting you past that prototype plateau and closing the gap between prototype and production. Because I think at the moment, we're seeing a lot of prototypes being built, but a lot of them also don't make it into production, and that's sad. If we standardize and align our workflows with better tooling, we're actually able to build a prototype and translate that directly into a production system. Again, you are allowed to make your problems easier. I think with other aspects of software development, we've really learned that making things less operationally complex is better because it means less can go wrong. If something goes wrong, it becomes a lot easier to diagnose. If you can apply that to machine learning, that's incredibly helpful, and as a result, you also get systems that are much cheaper, that are much smaller, that are much faster, that are entirely private and much easier to control. There's no need to give up on these best practices, and it's totally possible. Also, we're working with data here, and as soon as you start engaging with that, you will immediately come across edge cases and things you haven't considered, and ambiguities in the language that are very hard to think of upfront. It's very important to engage with your data, and also have a process in place that lets you iterate and make changes as needed. I also highly recommend having little workshops internally, like the one we did at PyData, where you can have long discussions about whether Cheetos, a dish or not, or whether the forehead is part of your face. All of these questions are important, and if you can't make a consistent decision, no AI model is magically going to save you and will be able to do it for you. Finally, there's really no need to compromise on software development best practices and data privacy, as you've seen in the talk. Moving dependencies to development really changes the calculation. We can be more ambitious than that, and we should be. We shouldn't stop at having a monolithic model. We can take it one step further and really make the best use of new technologies to allow us to do things that we weren't able to do before, while not making our overall system worse in the process and throwing out a lot of best practices that we've learned. It's absolutely possible. It's really something you can experiment with and apply today. Questions and Answers Participant 1: You mentioned in your talk that with model assistance it's totally feasible and quick to create data in-house. In your experience, how many examples do you think you need in order to create good results? Montani: Of course, it always depends. You'll be surprised how little you might actually need. Often, even just starting with a few hundred individual examples that are good, can really beat the few-shot baseline. It also depends the amount you choose. It depends on what accuracy figures you want to report. Do you want to just report whole numbers? Do you want to report accuracies like 98.2? That introduces a different magnitude. I think if you look at some of the case studies I linked, we've also done some experiments where we basically took an existing dataset and trained on small portions of the data. Then compared when we beat the LLM baseline. Often, even just using under 10% of the dataset already gave us really good results. It's really not a lot. I think if you start doing that, you'll really be surprised how little you need, with transfer learning. That also, of course, means that what you're doing needs to be good. Garbage in, garbage out. That's also the reason why it's more important than ever to have a way that gives you high quality data, because you can get by with very little, but it needs to be good. Participant 2: Do you have any guidelines when it comes to comparing structured outputs. In the beginning, it seems like a very simple task, but if you start nesting it, in particular, if you have lists on both sides, trying to figure out what entities you're missing, can just become so complex. How to actually get it down to maybe at least just 5 or 10 numbers, instead of 100, of like, I'm missing the token in entity 5 and missing entity 6 completely. Montani: There are different ways you can evaluate this. Some evaluations really look at the token levels. Others look at the whole entities. Also, there's a difference in, how do you calculate that if something is missing. Is that false, or do you count partial matches? That's a whole other can of worms in itself. More generally, I think it comes back to that refactoring idea of like, if you have these entities, is this actually a problem where boundaries are important? Some people always often go for named entity recognition because you're like, I can have these spans of text that give me what I want. If you take a step back, in a lot of cases, it actually turns out that you're not even really interested in the spans. You're interested in, does this text contain my phone? Then that becomes a text classification task, which is generally a lot easier and also gives you better results, because you're not actually comparing boundaries that are really very sensitive. That's what makes named entity recognition good. It's very hard to do that consistently. I think refactoring can also help there. Or if you have a lot of people who have who nested categories, taking a step back, do I need these nested categories? Can I maybe come up with a process where I focus on the most important top level first, and then maybe drill down into the subcategories. Or think in that S\u0026P case study, they realized that there are actually some types of information that's relatively straightforward. If we know that it's of this category, we can deterministically decide which sublabels apply, for example. I think actually, it really ties into the refactoring point. Often, the first label scheme you come up with is usually not the best. You want to pick something that's easy for the machine learning model, and not necessarily translating your business question one to one into a label scheme. That's usually where a lot of the problems happen. Participant 3: What's the process to find the baseline? Because in my life, it's very hard to find the baseline. Montani: What the case study companies did is, you create evaluation data. Let's say you have the text categories, is this about battery life, or is the battery life positive? Then you first have evaluation data where you know the correct answer. Then you basically let the LLM predict those, and then you compare it. For example, with spaCy LLM, in that case, you get the exact same output. You can evaluate that pipeline the same way you would evaluate any other model. Or you can try a few-shot approach. Basically, the idea is you let the model make the predictions, and then compare the output to examples where you know the answer, and that gives you the baseline. Participant 3: For example, you have a multivariable problem when you're trying to evaluate, for example, risks, and you have so many different points, and you don't have reliable data to find the baseline. Montani: That's also why I think creating data is important. If you're building something that's reliable, you can't really get around creating a good evaluation. I've also heard people say, we can just pass it to some other LLM to evaluate. It's like, you're still stuck in the cycle. You can build software and not write tests. That's legal. You don't want to do that. Even if you're doing something that's completely unsupervised at the end, you want a good evaluation that also actually matches what you're doing, not just some benchmark dataset. I think that is super important. Then once you have that, it lets you calculate a baseline. It lets you test things. I always recommend, do something really basic, do like a regular expression, and benchmark that, just to have some comparison. Or do something really simple, because if you find out, I already get really good results on that, does it actually make sense? Or what's my machine learning model up against? I think it's such an important part of it. I think people should talk about it more. Yes, do it properly. Participant 4: Would you also say that the approach that you described here would work in a similar way, if you basically use the model to then interact with users, and maybe, for example, respond based on the comment about the products, and directly interact back. Montani: How would that look like as an example, if you're building a chat interface? Participant 4: In this case, I think there was an evaluation, so you don't need to really chat. You just need to maybe respond and say, \"Thank you for the evaluation. We're working on improving the battery\", or something like that. Montani: Here you have the model, actually, you ask, you have a prompt, like, here are the categories. Respond with the categories, and whether it's positive or negative? Then you try to get the model to respond as structured as possible, and then also pass that out so you really get label true, false, or none. Participant 4: Would you, for these kinds of use cases, also use in-house trained LLM, or use the bigger ones on the market? Montani: You can do both. One thing that's nice here is that, since you're moving the dependency to development instead of runtime, it actually becomes a lot more feasible to run your own open-source LLMs. If you're not relying on it at runtime, it's actually affordable and efficient to just do it in-house, and you can fine-tune it in-house, or you just use something off the shelves, or you use an API that you have access to. I think having the dependency during development is the key and really changes things so you can use whatever. You're not using the LLM to create any data itself. You're using it to add structure to your data. Participant 5: Once you have an LLM running in production, do you have any tips of what I can check to recheck how the data works with the model, and retrain it. Montani: What do you mean by how the data works with the model? Participant 5: How is the model performing in production, and based on the new data that is coming in, can I have an automated retraining of the same model? Any tips on that? Montani: I think that ties back into the evaluation as well. Even if you have your model running, you want to capture, what does it output? Whatever your context is. Then have a human look at it and see, is this correct, or is this not correct? How does this change over time? Because you also easily can have the problem of data drift, if the input data changes, if the model changes, which is also a problem you have if you have an API and then the model just changes. That changes a lot. I think having a QA process in place where you really store what is your model doing at runtime, and then review, is it doing the correct thing? Do that regularly, iterate on that, and also see how is that changing over time as things change. That's kind of the thing of evaluation. You never get out of it. It's not something you do once and then forget about it. You constantly need to iterate and constantly do it if you're actually interested in really getting reliable feedback of how your system is doing. See more presentations with transcripts",
  "image": "https://res.infoq.com/presentations/llm-disttilation/en/card_header_image/Ines-Montani-twitter-card-1737104567444.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp\u003e\n\t\u003cspan data-nosnippet=\"\"\u003e\u003ca href=\"https://www.infoq.com/\" title=\"InfoQ Homepage\"\u003eInfoQ Homepage\u003c/a\u003e\u003c/span\u003e\n\t\n\t\t\n\t\t\t\n\t\t\t\n                \u003cspan data-nosnippet=\"\"\u003e\u003ca href=\"https://www.infoq.com/presentations\" title=\"Presentations\"\u003ePresentations\u003c/a\u003e\u003c/span\u003e\n            \n\t\t\n\t\t\u003cspan data-nosnippet=\"\"\u003eTaking LLMs out of the Black Box: A Practical Guide to Human-in-the-Loop Distillation\u003c/span\u003e\n\t\n\t\n    \n        \n    \n\u003c/p\u003e\u003cdiv id=\"presentationNotes\"\u003e\n                                    \u003ch2\u003eTranscript\u003c/h2\u003e\n\n\u003cp\u003eMontani: I\u0026#39;ll be talking about taking large language models out of the black box, and a few practical tips that hopefully you can apply in your work today. Some of you might know me from my work on spaCy, which is an open-source library for natural language processing in Python. spaCy was really designed from day one to be used in real products and be used in production. That also means we had to do a lot of the boring software development stuff like backwards compatibility, making sure we don\u0026#39;t break people\u0026#39;s code.\u003c/p\u003e\n\n\u003cp\u003eThis actually had a pretty nice unintended side effect more recently, which is that ChatGPT is actually really good at writing spaCy code. You can try that out if you want. We also developed Prodigy, which is a modern annotation tool for creating training and evaluation data for machine learning developers and machine learning models. Prodigy is fully scriptable in Python, so it allows for a lot of nice, semi-automated workflows, including, actually, a lot of the ideas that I\u0026#39;ll be talking about in my talk.\u003c/p\u003e\n\n\u003ch2\u003eSoftware in Industry\u003c/h2\u003e\n\n\u003cp\u003eAs an industry developing software, we have really built up a lot of best practices over the years and a lot of ideas of how we want our software to behave and what\u0026#39;s good software development. One of those is, we want our software to be modular. We want to have components that we can work on independently. We want to have building blocks that we can work with. We also want our tools and our software to be transparent. We want to understand what\u0026#39;s going on, be able to look under the hood, and also debug if something is going wrong. That also means we want to be able to explain to others why our system is doing something, or, in the case of a machine learning model, why we get a specific result.\u003c/p\u003e\n\n\u003cp\u003eMany solutions also need to be data private. Who works in a field where data privacy is important? That\u0026#39;s an important topic. We want systems to be reliable, not just randomly go down. Whatever we build also needs to be affordable, whatever that means in a specific context. Maybe you\u0026#39;re working on a budget, maybe you have no money. If we\u0026#39;re building something, it needs to fit. This introduces a lot of problems, if we\u0026#39;re looking at new ways and new technologies. If we\u0026#39;re working with black box models, we can\u0026#39;t really look under the hood, and we can\u0026#39;t really understand what\u0026#39;s going on. For a lot of them, if we\u0026#39;re using an API, we don\u0026#39;t even really know how they work. A lot of that is not public.\u003c/p\u003e\n\n\u003cp\u003eWe have these big monoliths that do a whole thing at once. That can be very challenging. Because a lot of the newer models are very large, which is also what makes them so good, it\u0026#39;s not really efficient or sustainable to run them yourself in-house, which means we need to consume them via a third-party API. That also means we need to send our data to someone else\u0026#39;s server. We\u0026#39;re at the mercy of an API provider. If it goes down, it goes down. If it\u0026#39;s slow, that\u0026#39;s how it is. The costs can also really add up if you\u0026#39;re using a model via an API at runtime. How can we fix this, and how can we still use all these exciting latest technologies that are really good, while also maintaining the best practices that we\u0026#39;ve built over years of developing software. That\u0026#39;s what I\u0026#39;m going to cover in my talk.\u003c/p\u003e\n\n\u003cp\u003eTo maybe start off with a practical example, here\u0026#39;s something that\u0026#39;s maybe familiar to you and maybe similar if you\u0026#39;re working in NLP, to something that you were tasked with in your job. Let\u0026#39;s say you work for an electronics retailer or a company producing phones, and you\u0026#39;re getting lots of reviews in from customers, and you\u0026#39;ve collected them all, and now you want to analyze what people are saying about your product. First, what you want to do is you want to find mentions of your products in these reviews and find those that are relevant. You also want to link them to your catalog that you already have, where you have all meta information about your products. Then you want to extract what people are saying, so you have different categories, like battery, camera, performance, design, and you want to extract whether people like these or not, which is also often referred to as aspect-oriented sentiment analysis.\u003c/p\u003e\n\n\u003cp\u003eThen, finally, if you have all that information, you want to add those results to a database, maybe the database that also has your catalog. As you can see, there are some parts of this that are actually quite straightforward. We have, battery life is incredible. That\u0026#39;s easy. You can definitely extract that. There are other parts that aren\u0026#39;t necessarily so straightforward because of its language, and language can often be vague. Here, the reviewer says, never had to carry a power bank before, and now I need it all the time. From the context that we have about the language and the world, we know that this means that the battery life is bad. It\u0026#39;s not explicit in the text. That\u0026#39;s really a use case that benefits from machine learning. There are a lot of larger models these days that are really good at that, because they\u0026#39;re trained on such a vast volume of text, and they\u0026#39;re really good at understanding, \u0026#34;This type of context\u0026#34;, in the text.\u003c/p\u003e\n\n\u003cp\u003eUsing in-context learning, you could basically prompt the model. You can provide it examples of your text, and it can respond with an answer, for example, of the sentiment you\u0026#39;re interested in. Of course, if you think about what you actually need here, you have a specific use case, and you really only need a small subset of what the model is able to do. You really need the context. You\u0026#39;re not interested in talking to it about the weather. There\u0026#39;s really a very specific thing that you want. The question is, what if we could just take that out? What if we can extract only the part that we\u0026#39;re interested in into a model that can also then be much smaller and much more specific, because all we want it to do is predict those sentiment categories.\u003c/p\u003e\n\n\u003cp\u003eWhile there has been a lot of talk about in-context learning, because that\u0026#39;s new, and it\u0026#39;s also what a lot of the research focuses on, it doesn\u0026#39;t mean that transfer learning is somehow outdated or has been replaced. It\u0026#39;s simply a different technique. You might be familiar with models like BERT and their embeddings and all kinds of different local variants, like CamemBERT, and these embeddings encode a lot of very important and very relevant contextual information that you can initialize your model with, and add a task-specific network on top. The thing is, if we\u0026#39;re looking at the research, we\u0026#39;ll actually see that even the most classic BERT, base, is still very competitive and achieves very good results compared to zero-shot or few-shot in-context learning. What\u0026#39;s also important to keep in mind here is that these are all academic benchmarks. These are calculated based on datasets we can\u0026#39;t control. That\u0026#39;s the idea of a benchmark.\u003c/p\u003e\n\n\u003cp\u003eIf we\u0026#39;re already getting these very promising and good results using benchmark datasets, we\u0026#39;ll probably be able to achieve even better results if we have control over the data which we have in our use case. What we want to do is, if we want our large generative model to do a specific thing, we start out with our text, and we start out with the prompt based on a prompt template specific to a thing we want to extract. Then we pass that to the model. What we get back is raw output, usually in the form of an answer, depending on the prompt we gave it.\u003c/p\u003e\n\n\u003cp\u003eThen we can use a corresponding parser that corresponds to the prompt template in order to parse out the task specific output. In our case, we\u0026#39;re not after a conversation. We\u0026#39;re after structured data that expresses the categories that we\u0026#39;ve already defined. What we also want to do is we don\u0026#39;t just want to match the large language model\u0026#39;s performance. Ideally, we want to do it even better, because we have the capability to do that. If the model makes mistakes, we want to correct them. What we can do is we can pass our task output to an annotation workflow and really create a dataset that\u0026#39;s very specific to what we\u0026#39;re looking to extract. Using transfer learning, we can use that to distill a task-specific model that only performs the one task we\u0026#39;re interested in, using what we already have in the weights of the large generative model.\u003c/p\u003e\n\n\u003ch2\u003eClose the Gap Between Prototype and Production\u003c/h2\u003e\n\n\u003cp\u003eThis is both pretty exciting and very promising, but one thing we\u0026#39;ve definitely seen, if you get to work, is that a lot of projects these days, they really get stuck in this phase that I also call the prototype plateau. You start working. It\u0026#39;s all very exciting, and it\u0026#39;s working. When it comes to actually shipping your system that you\u0026#39;ve built, you realize that it doesn\u0026#39;t work. There are a lot of reasons for that, and also solutions that are actually really important to keep in mind before you start building. In order to close the gap between prototype and production, one important thing is you want to be standardizing your inputs and outputs. You want to have the same workflow during prototyping as you have during production.\u003c/p\u003e\n\n\u003cp\u003eIf your prototype takes random human generated text that you type in and outputs a human readable text response, but your production system needs structured data, then you\u0026#39;re going to have a problem, and it might not actually translate so well. You also want to start with an evaluation, just like when you\u0026#39;re writing software, you\u0026#39;re writing tests. For a machine learning model, the equivalent is an evaluation. You want examples where you know the answer, and that you can check and so you actually know if your system is improving or not. That\u0026#39;s something that\u0026#39;s often glossed over.\u003c/p\u003e\n\n\u003cp\u003eA lot of people if you\u0026#39;re excited in building something, you might go by just a vibes-based evaluation. Does it feel good? If you actually want to assess how is my model performing, you want an evaluation with accuracy scores that you can compare. Of course, accuracy is not everything. Especially if you\u0026#39;re coming from a research background, you can be very focused on just optimizing those scores. A high accuracy in your model is useless if the model isn\u0026#39;t actually doing what you want it to do, and if it\u0026#39;s useful in your application. In an applied context, you don\u0026#39;t just want to optimize for a score, you also want to test, is it actually useful? Whatever that means in your context. That also requires working on your data iteratively, just like with code.\u003c/p\u003e\n\n\u003cp\u003eUsually, the first idea you have is usually not what you ship to production, and the same goes for data. You want to have a workflow where you can quickly try things out, and ideally also tooling to help with that, so you don\u0026#39;t need to schedule large meetings and spend hours to try out every idea you have. Finally, we\u0026#39;re working with language here, and that\u0026#39;s really important to keep in mind. While as developers, we really like to fit things neatly into boxes, language doesn\u0026#39;t work that way. It\u0026#39;s usually vaguely gesturing at things. There\u0026#39;s a lot of ambiguity in language that we have to keep in mind, it\u0026#39;s not just data or it\u0026#39;s not just vectors.\u003c/p\u003e\n\n\u003cp\u003eOn the other hand, we can also use that to our advantage. There\u0026#39;s a lot in the language that helps us express things and get our point across, and that generalizes across language very well. If we could identify these parts, we can actually use that to our advantage in the application, and make the problem easier for our model. These are also things that we really thought about a lot when developing our tools. Because I think if you\u0026#39;re building developer tools, that\u0026#39;s one of the problems you want to address. How can we make it easier for people to standardize workflows between prototype and production, and actually ship things and not just get stuck in the prototype plateau?\u003c/p\u003e\n\n\u003cp\u003eHere\u0026#39;s an example of a prototype we might build for an application. We have a large generative model, and what we can do, and something that we\u0026#39;ve actually built with spaCy LLM is, have a way to prompt the model and transform the output, and parse it into structured data. Even while you\u0026#39;re trying things out without any data required, you can use a large language model to create the structured data for you, and what you get out in the end is an object that contains that structured data. You can, of course, ship this to production the way it is, but you can also work on replacing the large generative model at development time so that at runtime you end up with distilled task-specific components that perform only the parts that you\u0026#39;re interested in, and that are fully modular and also transparent, and usually much smaller and faster as well. The output in that case is also the same. You\u0026#39;re also getting this structured machine facing object that you can standardize on.\u003c/p\u003e\n\n\u003ch2\u003eHuman in the Loop\u003c/h2\u003e\n\n\u003cp\u003eAs I said previously, of course we don\u0026#39;t just want to match what the large generative model is doing. We actually want to make it better. We want to correct its mistakes. For that, we need a human at some point in the loop. That\u0026#39;s a very important step here. To give you an example how that works, we start off with a model and all the weights it has available. As a first step, as I mentioned before, we want to have a continuous evaluation. We need a way to figure out our baseline. What are we up against? What\u0026#39;s the performance we get out of the box without doing anything?\u003c/p\u003e\n\n\u003cp\u003eOtherwise, you\u0026#39;ll have no idea whether what you\u0026#39;re doing actually makes a difference or not. Now we can use all the weights we have available in that model and prompt it, and it will return whatever data we ask for, using everything that it has available. We can pipe that forward into an annotation environment where we can look at just the exact structured data and make corrections and very quickly move through that data and create a dataset that\u0026#39;s really specific to the task, like the aspect-oriented sentiment predictions, for instance. With transfer learning, create a component that only performs that. Of course, here, it comes in handy that we have our evaluation because we want to do that until our distilled model beats and ideally also exceeds that baseline. I\u0026#39;ll show you some examples of this later, but you might be surprised how easily you can actually do this and apply this yourself.\u003c/p\u003e\n\n\u003cp\u003eFirst, how do we access our human? Going back to that practical example, we have one of these reviews of someone who rated our Nebula phone, \u0026#34;meh\u0026#34;. As an example, the type of structured data we\u0026#39;re after is something like this. For simplicity, for this example, I\u0026#39;ll only focus on assuming we have binary values for those categories. Of course, in some cases, you might want to define some other schema and have a scale of like, how much do people like the battery life, and so on? That\u0026#39;s the structured data, that\u0026#39;s our output. If we\u0026#39;re presenting that to the human, a naive approach would be, let\u0026#39;s just show the human the text, give the human the categories, and then let them correct it.\u003c/p\u003e\n\n\u003cp\u003eIf you\u0026#39;re looking at this, you\u0026#39;ll see that this doesn\u0026#39;t actually capture these null values. We have no distinction here between a negative response, or no mention of that aspect at all. We can extend this a bit and collect whether it\u0026#39;s positive or negative, and have the large generative model make the selection for us. That means you can move through the examples very quickly, and all you have to do is correct the model if it makes a mistake. The big problem we have is that humans are humans, and have a lot of disadvantages. One of them is that humans actually have a cache too and a working memory. If you ask a human to constantly in their head iterate over your label scheme and every aspect that you\u0026#39;re interested in, humans are actually quite bad at that.\u003c/p\u003e\n\n\u003cp\u003eYou\u0026#39;ll find that humans really lose focus, end up making mistakes, and humans are very bad at consistency. What you can do instead is you can help the human and the human cache and make multiple passes over the data, one per category and one per aspect. While it might seem like a lot more work at first, because you\u0026#39;re looking at the same example multiple times, and you\u0026#39;re collecting a lot more decisions, it can actually be much faster, because you reduce the cognitive load on the human. I\u0026#39;ll show you an example of this later, where a team actually managed to increase their speed by over 10 times by doing this. You have your human, you have a model that helps you create the data, and you\u0026#39;re collecting a task-specific dataset that doesn\u0026#39;t just match the few-shot or zero-shot baseline, but actually improves upon it.\u003c/p\u003e\n\n\u003ch2\u003eCase Studies\u003c/h2\u003e\n\n\u003cp\u003eTo give you some examples of how this works and how this can look in practice. This is the case study we did based on a workshop we held at PyData in New York. The task here was we want to stream in data from our cooking subreddit, and extract dishes, ingredients, and equipment from it. We did that together with the group, and also discussed the data while we were doing it. We used a GPT model during the annotation process to help create the data. In the workshop, we were actually able to beat the few-shot LLM baseline of 74%, which is actually pretty good out of the box without any training data. We beat that in the workshop and created a task-specific model that performed the same or even better, and that model also was more than 20 times faster.\u003c/p\u003e\n\n\u003cp\u003eIf you look at the stats here, we have a model that\u0026#39;s 400 megabytes, which is pretty good. You can totally run that yourself, runs on your laptop, runs at over 2000 words per second, so really fast. The data development time, we calculated, how long would it have taken a single person to create all the data for it. That\u0026#39;s about eight hours. That\u0026#39;s one standard workday. If you think about other things you spend time on as part of your work, you probably spend more time trying to get CUDA installed or trying to get your GPU running. It\u0026#39;s really not true anymore that data development is like this absolutely tedious task, even a single developer can do this in a workday. That was very promising.\u003c/p\u003e\n\n\u003cp\u003eThat also inspired the next case study we did, which was with a company called S\u0026amp;P Global. What they\u0026#39;re doing, in this project, is they\u0026#39;re extracting commodities trading data in real-time. If crude oil is traded somewhere, they extract the price, the participants, the location, and a wide range of other attributes, and they provide that as a structured feed to their customers in real time. Of course, this is information that can really significantly impact the economy and move markets. Their environment is a high security environment. I actually went to visit them in London a while ago, and even within their office, it\u0026#39;s very highly segregated.\u003c/p\u003e\n\n\u003cp\u003eThey have this glass box that the analysts sit in, you can only access it with a specific card. It\u0026#39;s incredibly important that everything they do runs in-house, and no other third party gets to see it before it\u0026#39;s published, which also is like a promise of the data product. That\u0026#39;s why their customers are using it. What they did was they moved the dependency of the large language model to development and used that to create data for them. This plus some optimizations of how they actually present the questions to the human, including having simple, often binary questions and making multiple passes over the data, that actually made the whole process more than 10 times faster using a human and the model in a loop.\u003c/p\u003e\n\n\u003cp\u003eThey currently have eight pipelines in production, probably even more by now. This was a very successful project. If you\u0026#39;re looking at the stats again, they\u0026#39;re achieving very high accuracy. The models are 6 megabytes per pipeline. If you\u0026#39;re letting that sink in, this is really tiny. You can train that on your laptop really easily. They run super-fast, at over 16,000 words per second, so they\u0026#39;re really a great fit for processing these insights in real time and as quickly as possible. Again, also data development time, that\u0026#39;s a single person, so in under two workdays, or with two people, you can create the data needed for a distilled task-specific pipeline in about a day. Totally doable, even if you don\u0026#39;t have that many resources.\u003c/p\u003e\n\n\u003ch2\u003eThink of It as a Refactoring Process\u003c/h2\u003e\n\n\u003cp\u003eHow did they do it? What\u0026#39;s the secret? One of them is, if you\u0026#39;re thinking about developing AI solutions, they\u0026#39;re really code plus data, and so just like you refactor code, you can also refactor your data. Refactoring code is probably something you do all the time, and you\u0026#39;re very familiar with. The same really applies to your data development process. There are different aspects of this. One big part of refactoring is you\u0026#39;re breaking down a large problem into individual components, and you\u0026#39;re factoring out the different steps and creating reusable functions. That\u0026#39;s something we really accepted as the best practice, has a lot of advantages. You can do the same for your machine learning problem and your models. As part of that, the goal is you can make your problems easier.\u003c/p\u003e\n\n\u003cp\u003eAgain, you do this with code a lot, trying to reduce the complexity, and you\u0026#39;re allowed to do that. Have an easier system, and make it easier for the model as well. One part of that is factoring out business logic, and separating logic that\u0026#39;s really specific to your application, from logic that\u0026#39;s general purpose and that maybe applies to any language and doesn\u0026#39;t need any external knowledge. I\u0026#39;ll show you an example of that later. Again, that\u0026#39;s something you do in your code already, and that works well. You can apply that same idea to your data process.\u003c/p\u003e\n\n\u003cp\u003ePart of refactoring is also reassessing dependencies. Do you need to pull in this massive library at runtime that you only use a function of, or can you replace that? Is there something you can compile during development time so you don\u0026#39;t need to use it at runtime? The same is true for machine learning models. Can you move the dependency on the really complex and expensive and maybe intransparent model to development, and have a much cleaner and operationally simpler production environment? Finally, choosing the best techniques, you decide how a specific problem is best solved, and you have this massive toolbox of skills and of techniques that are available, and you pick the one that\u0026#39;s the best fit for the task at hand.\u003c/p\u003e\n\n\u003ch2\u003eMake the Problem Easier\u003c/h2\u003e\n\n\u003cp\u003eOne thing people really easily forget is that you are allowed to make your problem easier. This is not a competition. This is not academia. You\u0026#39;re allowed to reduce the operational complexity, because less operational complexity means that less can go wrong. When I started programming, I didn\u0026#39;t know very much. Of course, what I built was all pretty simple. Then as I got more experience, I learned about all of these new things, and of course, wanted to apply them. My code became a lot more complex. Also, if I\u0026#39;m looking back now, back then, I didn\u0026#39;t really write comments because it felt like a sign of weakness. If I found an especially complex solution to my problem, commenting meant that I\u0026#39;m admitting that this was hard, so I didn\u0026#39;t do that, which also makes it even harder to figure out what was going on and what I was thinking at the time.\u003c/p\u003e\n\n\u003cp\u003eThen, of course, with more experience my code also became much more straightforward, and I was able to pick the best techniques to get the job done and actually solve it most efficiently, instead of coming up with the most complex and interesting solution. I think it\u0026#39;s easy to forget this, because we are in a field that is heavily influenced by academia. In research, what you\u0026#39;re doing is you\u0026#39;re really building a Commons of Knowledge. You also want to compare the things you\u0026#39;re building using standard evaluations. If you\u0026#39;re comparing algorithms, everyone needs to evaluate them on the same thing, otherwise, we can\u0026#39;t compare them. You also standardize everything that\u0026#39;s not the novel thing that you are researching and publishing. Even if what you\u0026#39;re standardizing isn\u0026#39;t the best possible solution or isn\u0026#39;t efficient, it doesn\u0026#39;t matter. It needs to be standardized so you can focus on the novel thing you\u0026#39;re exploring.\u003c/p\u003e\n\n\u003cp\u003eOn the other hand, if you\u0026#39;re building an application and working in applied NLP, what you\u0026#39;re doing is you\u0026#39;re basically learning from that Commons of Knowledge that was built by academia and provided, and basically pick what works best, and follow some of the latest ideas. You also align your evaluation to project goals. You\u0026#39;re not using benchmarks. Your evaluation basically needs to tell you, does this solve the problem, and is this useful in my product or project, or not? You also do whatever works. Whatever gets the job done, you can take advantage of. If that means it\u0026#39;s less operationally complex, then that\u0026#39;s great.\u003c/p\u003e\n\n\u003ch2\u003eFactor Out Business Logic\u003c/h2\u003e\n\n\u003cp\u003eOne big part, as I said, of the refactoring process is separating out what\u0026#39;s business logic and what\u0026#39;s general-purpose logic. That can be quite tricky, and really requires engaging with your data and your problems. Here we have our SpacePhone review again. If we\u0026#39;re looking at that, we can basically break down the two different types of logic in this pseudocode formula. We have the classification task, which is our model that really predicts and processes the language itself. Then we have the business logic which is specific to our application and which can build on top of that.\u003c/p\u003e\n\n\u003cp\u003eTo give you some examples here, general-purpose classification in our example would be stuff like, what are the products? There\u0026#39;s a model. What\u0026#39;s the model of this phone? Is it a phone? Are we comparing the phone to something else? That requires no outside context, and that\u0026#39;s really inherent to the language, and not our specific problem. Then, on the other hand, we have stuff like our catalog reference. That\u0026#39;s external. Nothing in the text tells us that. We also have things like, does it have a touch screen? Is it worse than the iPhone 13? The fact, is it the latest model? That is something that can change tomorrow. We have information that can really change over time.\u003c/p\u003e\n\n\u003cp\u003eWhile we can include that in the model and in the predictions we make, we\u0026#39;ll end up with a system that\u0026#39;s immediately outdated, that we constantly need to retrain, and a problem that\u0026#39;s a lot harder for the model to build some reasoning around, because we have nothing in the text that tells us that, whereas what we do have is we have our catalog reference, we have dates, we have things we can do math with. This process can be very powerful, but of course, it really is absolutely specific to your problem and requires engaging with it.\u003c/p\u003e\n\n\u003cp\u003eTo give you an example of this idea in context, this is a most recent case study that we published with GitLab. What they\u0026#39;re doing is they\u0026#39;ve processed one year\u0026#39;s worth of support tickets and usage questions from different platforms, and they want to extract actionable insights. For example, how can we better support our support engineers in answering questions? What are things that we could add to our docs? Also questions like, how are people adopting new features? How many people have upgraded to the latest version? Are people still stuck on an older version? What are potential problems there, and so on? While these things don\u0026#39;t necessarily sound like particularly sensitive information, if you think about it, support tickets can actually include a lot of potentially sensitive data, like paths, details on people\u0026#39;s setup.\u003c/p\u003e\n\n\u003cp\u003eThey\u0026#39;re working in a high security environment and a hardened offline machine, so whatever they\u0026#39;re building, it needs to run internally, and it also needs to be rerun whenever they have new tickets coming in and new data. It needs to be very efficient. Another very important feature of this project was, it needs to be easy to adapt it to new scenarios and new business questions. What\u0026#39;s the latest version changes? Features change. Things people are doing change. It needs to be easy to answer different questions that maybe weren\u0026#39;t intended when the system was built. Of course, you can do these things as end-to-end prediction tasks, but that means that every time something changes, you need to redo your entire pipeline. Whereas if you can factor out general-purpose features from product specific logic, it becomes a lot easier to add extraction logic for any other future problems and future questions on top.\u003c/p\u003e\n\n\u003cp\u003eA very simple example of this is, you have things like the software version that is very specific business logic, whereas extracting numbers, makes it a lot easier for the model. If you have that, you can add your business logic on top to determine, is this a version of the software? Is this a link to the docs, and so on? I\u0026#39;ve linked the case study, explosion.ai/blog/gitlab-support-insights. They\u0026#39;ve done some pretty interesting things. Also have a pipeline that\u0026#39;s super-fast, and are working on adding a conversational output on top. I hope we\u0026#39;ll be able to publish more on that, because it\u0026#39;s a very cool project that really shows the importance of data refactoring.\u003c/p\u003e\n\n\u003ch2\u003eReality is not an End-to-End Problem\u003c/h2\u003e\n\n\u003cp\u003eWhat you can see here, is, as developers, we really love to put things into clear boxes and have this idea of like, if we can just have this one model that can do everything, wouldn\u0026#39;t that be great? Unfortunately, reality doesn\u0026#39;t really work that way. Reality isn\u0026#39;t an end-to-end prediction problem. It\u0026#39;s actually very nuanced and very complex. Human-in-the-loop distillation and going from a much larger general-purpose model to a much smaller and more efficient task-specific model really is a refactoring process.\u003c/p\u003e\n\n\u003cp\u003eYou refactor your code, you refactor your data, and that requires engaging with it. Iteration, which, again, is very heavily influenced by the tooling you use, can be a huge help in getting you past that prototype plateau and closing the gap between prototype and production. Because I think at the moment, we\u0026#39;re seeing a lot of prototypes being built, but a lot of them also don\u0026#39;t make it into production, and that\u0026#39;s sad. If we standardize and align our workflows with better tooling, we\u0026#39;re actually able to build a prototype and translate that directly into a production system.\u003c/p\u003e\n\n\u003cp\u003eAgain, you are allowed to make your problems easier. I think with other aspects of software development, we\u0026#39;ve really learned that making things less operationally complex is better because it means less can go wrong. If something goes wrong, it becomes a lot easier to diagnose. If you can apply that to machine learning, that\u0026#39;s incredibly helpful, and as a result, you also get systems that are much cheaper, that are much smaller, that are much faster, that are entirely private and much easier to control. There\u0026#39;s no need to give up on these best practices, and it\u0026#39;s totally possible.\u003c/p\u003e\n\n\u003cp\u003eAlso, we\u0026#39;re working with data here, and as soon as you start engaging with that, you will immediately come across edge cases and things you haven\u0026#39;t considered, and ambiguities in the language that are very hard to think of upfront. It\u0026#39;s very important to engage with your data, and also have a process in place that lets you iterate and make changes as needed. I also highly recommend having little workshops internally, like the one we did at PyData, where you can have long discussions about whether Cheetos, a dish or not, or whether the forehead is part of your face. All of these questions are important, and if you can\u0026#39;t make a consistent decision, no AI model is magically going to save you and will be able to do it for you.\u003c/p\u003e\n\n\u003cp\u003eFinally, there\u0026#39;s really no need to compromise on software development best practices and data privacy, as you\u0026#39;ve seen in the talk. Moving dependencies to development really changes the calculation. We can be more ambitious than that, and we should be. We shouldn\u0026#39;t stop at having a monolithic model. We can take it one step further and really make the best use of new technologies to allow us to do things that we weren\u0026#39;t able to do before, while not making our overall system worse in the process and throwing out a lot of best practices that we\u0026#39;ve learned. It\u0026#39;s absolutely possible. It\u0026#39;s really something you can experiment with and apply today.\u003c/p\u003e\n\n\u003ch2\u003eQuestions and Answers\u003c/h2\u003e\n\n\u003cp\u003eParticipant 1: You mentioned in your talk that with model assistance it\u0026#39;s totally feasible and quick to create data in-house. In your experience, how many examples do you think you need in order to create good results?\u003c/p\u003e\n\n\u003cp\u003eMontani: Of course, it always depends. You\u0026#39;ll be surprised how little you might actually need. Often, even just starting with a few hundred individual examples that are good, can really beat the few-shot baseline. It also depends the amount you choose. It depends on what accuracy figures you want to report. Do you want to just report whole numbers? Do you want to report accuracies like 98.2? That introduces a different magnitude. I think if you look at some of the case studies I linked, we\u0026#39;ve also done some experiments where we basically took an existing dataset and trained on small portions of the data. Then compared when we beat the LLM baseline.\u003c/p\u003e\n\n\u003cp\u003eOften, even just using under 10% of the dataset already gave us really good results. It\u0026#39;s really not a lot. I think if you start doing that, you\u0026#39;ll really be surprised how little you need, with transfer learning. That also, of course, means that what you\u0026#39;re doing needs to be good. Garbage in, garbage out. That\u0026#39;s also the reason why it\u0026#39;s more important than ever to have a way that gives you high quality data, because you can get by with very little, but it needs to be good.\u003c/p\u003e\n\n\u003cp\u003eParticipant 2: Do you have any guidelines when it comes to comparing structured outputs. In the beginning, it seems like a very simple task, but if you start nesting it, in particular, if you have lists on both sides, trying to figure out what entities you\u0026#39;re missing, can just become so complex. How to actually get it down to maybe at least just 5 or 10 numbers, instead of 100, of like, I\u0026#39;m missing the token in entity 5 and missing entity 6 completely.\u003c/p\u003e\n\n\u003cp\u003eMontani: There are different ways you can evaluate this. Some evaluations really look at the token levels. Others look at the whole entities. Also, there\u0026#39;s a difference in, how do you calculate that if something is missing. Is that false, or do you count partial matches? That\u0026#39;s a whole other can of worms in itself. More generally, I think it comes back to that refactoring idea of like, if you have these entities, is this actually a problem where boundaries are important? Some people always often go for named entity recognition because you\u0026#39;re like, I can have these spans of text that give me what I want. If you take a step back, in a lot of cases, it actually turns out that you\u0026#39;re not even really interested in the spans. You\u0026#39;re interested in, does this text contain my phone?\u003c/p\u003e\n\n\u003cp\u003eThen that becomes a text classification task, which is generally a lot easier and also gives you better results, because you\u0026#39;re not actually comparing boundaries that are really very sensitive. That\u0026#39;s what makes named entity recognition good. It\u0026#39;s very hard to do that consistently. I think refactoring can also help there. Or if you have a lot of people who have who nested categories, taking a step back, do I need these nested categories? Can I maybe come up with a process where I focus on the most important top level first, and then maybe drill down into the subcategories. Or think in that S\u0026amp;P case study, they realized that there are actually some types of information that\u0026#39;s relatively straightforward. If we know that it\u0026#39;s of this category, we can deterministically decide which sublabels apply, for example. I think actually, it really ties into the refactoring point.\u003c/p\u003e\n\n\u003cp\u003eOften, the first label scheme you come up with is usually not the best. You want to pick something that\u0026#39;s easy for the machine learning model, and not necessarily translating your business question one to one into a label scheme. That\u0026#39;s usually where a lot of the problems happen.\u003c/p\u003e\n\n\u003cp\u003eParticipant 3: What\u0026#39;s the process to find the baseline? Because in my life, it\u0026#39;s very hard to find the baseline.\u003c/p\u003e\n\n\u003cp\u003eMontani: What the case study companies did is, you create evaluation data. Let\u0026#39;s say you have the text categories, is this about battery life, or is the battery life positive? Then you first have evaluation data where you know the correct answer. Then you basically let the LLM predict those, and then you compare it. For example, with spaCy LLM, in that case, you get the exact same output. You can evaluate that pipeline the same way you would evaluate any other model. Or you can try a few-shot approach. Basically, the idea is you let the model make the predictions, and then compare the output to examples where you know the answer, and that gives you the baseline.\u003c/p\u003e\n\n\u003cp\u003eParticipant 3: For example, you have a multivariable problem when you\u0026#39;re trying to evaluate, for example, risks, and you have so many different points, and you don\u0026#39;t have reliable data to find the baseline.\u003c/p\u003e\n\n\u003cp\u003eMontani: That\u0026#39;s also why I think creating data is important. If you\u0026#39;re building something that\u0026#39;s reliable, you can\u0026#39;t really get around creating a good evaluation. I\u0026#39;ve also heard people say, we can just pass it to some other LLM to evaluate. It\u0026#39;s like, you\u0026#39;re still stuck in the cycle. You can build software and not write tests. That\u0026#39;s legal. You don\u0026#39;t want to do that. Even if you\u0026#39;re doing something that\u0026#39;s completely unsupervised at the end, you want a good evaluation that also actually matches what you\u0026#39;re doing, not just some benchmark dataset. I think that is super important.\u003c/p\u003e\n\n\u003cp\u003eThen once you have that, it lets you calculate a baseline. It lets you test things. I always recommend, do something really basic, do like a regular expression, and benchmark that, just to have some comparison. Or do something really simple, because if you find out, I already get really good results on that, does it actually make sense? Or what\u0026#39;s my machine learning model up against? I think it\u0026#39;s such an important part of it. I think people should talk about it more. Yes, do it properly.\u003c/p\u003e\n\n\u003cp\u003eParticipant 4: Would you also say that the approach that you described here would work in a similar way, if you basically use the model to then interact with users, and maybe, for example, respond based on the comment about the products, and directly interact back.\u003c/p\u003e\n\n\u003cp\u003eMontani: How would that look like as an example, if you\u0026#39;re building a chat interface?\u003c/p\u003e\n\n\u003cp\u003eParticipant 4: In this case, I think there was an evaluation, so you don\u0026#39;t need to really chat. You just need to maybe respond and say, \u0026#34;Thank you for the evaluation. We\u0026#39;re working on improving the battery\u0026#34;, or something like that.\u003c/p\u003e\n\n\u003cp\u003eMontani: Here you have the model, actually, you ask, you have a prompt, like, here are the categories. Respond with the categories, and whether it\u0026#39;s positive or negative? Then you try to get the model to respond as structured as possible, and then also pass that out so you really get label true, false, or none.\u003c/p\u003e\n\n\u003cp\u003eParticipant 4: Would you, for these kinds of use cases, also use in-house trained LLM, or use the bigger ones on the market?\u003c/p\u003e\n\n\u003cp\u003eMontani: You can do both. One thing that\u0026#39;s nice here is that, since you\u0026#39;re moving the dependency to development instead of runtime, it actually becomes a lot more feasible to run your own open-source LLMs. If you\u0026#39;re not relying on it at runtime, it\u0026#39;s actually affordable and efficient to just do it in-house, and you can fine-tune it in-house, or you just use something off the shelves, or you use an API that you have access to. I think having the dependency during development is the key and really changes things so you can use whatever. You\u0026#39;re not using the LLM to create any data itself. You\u0026#39;re using it to add structure to your data.\u003c/p\u003e\n\n\u003cp\u003eParticipant 5: Once you have an LLM running in production, do you have any tips of what I can check to recheck how the data works with the model, and retrain it.\u003c/p\u003e\n\n\u003cp\u003eMontani: What do you mean by how the data works with the model?\u003c/p\u003e\n\n\u003cp\u003eParticipant 5: How is the model performing in production, and based on the new data that is coming in, can I have an automated retraining of the same model? Any tips on that?\u003c/p\u003e\n\n\u003cp\u003eMontani: I think that ties back into the evaluation as well. Even if you have your model running, you want to capture, what does it output? Whatever your context is. Then have a human look at it and see, is this correct, or is this not correct? How does this change over time? Because you also easily can have the problem of data drift, if the input data changes, if the model changes, which is also a problem you have if you have an API and then the model just changes. That changes a lot.\u003c/p\u003e\n\n\u003cp\u003eI think having a QA process in place where you really store what is your model doing at runtime, and then review, is it doing the correct thing? Do that regularly, iterate on that, and also see how is that changing over time as things change. That\u0026#39;s kind of the thing of evaluation. You never get out of it. It\u0026#39;s not something you do once and then forget about it. You constantly need to iterate and constantly do it if you\u0026#39;re actually interested in really getting reliable feedback of how your system is doing.\u003c/p\u003e\n\n\n\n\n\u003cp\u003e\u003cbig\u003e\u003cstrong\u003eSee more \u003ca href=\"https://www.infoq.com/transcripts/presentations/\"\u003epresentations with transcripts\u003c/a\u003e\u003c/strong\u003e\u003c/big\u003e\u003c/p\u003e\n\n\n\n                                \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "43 min read",
  "publishedTime": "2025-02-05T00:00:00Z",
  "modifiedTime": null
}
