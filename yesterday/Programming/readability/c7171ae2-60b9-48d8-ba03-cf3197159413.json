{
  "id": "c7171ae2-60b9-48d8-ba03-cf3197159413",
  "title": "Nvidia Announces Arm-Powered Project Digits, Its First Personal AI Computer",
  "link": "https://www.infoq.com/news/2025/01/nvidia-project-digits/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Capable of running 200B-parameter models, Nvidia Project Digits packs the new Nvidia GB10 Grace Blackwell chip to allow developers to fine-tune and run AI models on their local machines. Starting at $3,000, Project Digits targets AI researchers, data scientists, and students to allow them to create their models using a desktop system and then deploy them on cloud or data center infrastructure. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Mon, 13 Jan 2025 20:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Large language models",
    "Model Inference",
    "GPU",
    "ARM",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 2417,
  "excerpt": "Capable of running 200B-parameter models, Nvidia Project Digits packs the new Nvidia GB10 Grace Blackwell chip to allow developers to fine-tune and run AI models on their local machines. Starting at $",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250109115555/apple-touch-icon.png",
  "text": "Capable of running 200B-parameter models, Nvidia Project Digits packs the new Nvidia GB10 Grace Blackwell Superchip to allow developers to fine-tune and run AI models on their local machines. Starting at $3,000, Project Digits targets AI researchers, data scientists, and students to allow them to create their models using a desktop system and then deploy them on cloud or data center infrastructure. Nvidia Grace Blackwell brings together Nvidia's Arm-based Grace CPU and Blackwell GPU with the latest-generation CUDA cores and fifth-generation Tensor Cores connected via NVLink®-C2C. A single unit will include 128GB of unified, coherent memory and up to 4TB of NVMe storage. According to Nvidia, Project Digits delivers up to 1 PetaFLOP for 4-bit floating point, which means you can expect that level of performance for inference using quantized models but not for training. Nvidia has not disclosed the system's performance for 32-bit floating point or provided details about its memory bandwidth. The announcement of Project Digits made some developers ponder whether it can be a preferable choice to an Nvidia RTX 5090-based system. In comparison to a 5090 GPU, Project Digits has the advantage of coming in a compact box and not requiring the huge fan used on the 5090. On the other hand, the usage of low-power DDR5 memory on Project Digits seems to imply a reduced bandwidth compared to the 5090's GDDR7 memory, which further hints at Project Digits being optimized for inference. However lacking final details, it's hard to understand how the two solutions compare performance-wise. Another interesting comparison that has been brought up is with Apple's M4 Max-based systems, which may pack up to 196GB of memory and are thus suitable to run large LLMs for inference. Here, there seem to be more similarities between the two systems, including the use of DDR5X unified memory, so it seems Nvidia is seemingly aiming, among other things, to provide an alternative to that kind of solution. Project Digits will run Nvidia's own Linux distribution, DGX OS, which is based on Ubuntu and includes Nvidia-optimized Linux kernel with out-of-the-box support for GPU Direct Storage (GDS). Nvidia says the first units will be available in May this year. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/01/nvidia-project-digits/en/headerimage/nvidia-project-digits-1736795394841.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eCapable of running 200B-parameter models, \u003ca href=\"https://nvidianews.nvidia.com/news/nvidia-puts-grace-blackwell-on-every-desk-and-at-every-ai-developers-fingertips\"\u003eNvidia Project Digits\u003c/a\u003e packs the new Nvidia GB10 Grace Blackwell Superchip to allow developers to fine-tune and run AI models on their local machines. Starting at $3,000, Project Digits targets AI researchers, data scientists, and students to allow them to create their models using a desktop system and then deploy them on cloud or data center infrastructure.\u003c/p\u003e\n\n\u003cp\u003eNvidia Grace Blackwell brings together Nvidia\u0026#39;s Arm-based \u003ca href=\"https://www.nvidia.com/en-us/data-center/grace-cpu/\"\u003eGrace CPU\u003c/a\u003e and \u003ca href=\"https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/\"\u003eBlackwell GPU\u003c/a\u003e with the latest-generation CUDA cores and fifth-generation \u003ca href=\"https://www.nvidia.com/en-us/data-center/tensor-cores/\"\u003eTensor Cores\u003c/a\u003e connected via \u003ca href=\"https://www.nvidia.com/en-us/data-center/nvlink-c2c/\"\u003eNVLink®-C2C\u003c/a\u003e. A single unit will include 128GB of unified, coherent memory and up to 4TB of NVMe storage.\u003c/p\u003e\n\n\u003cp\u003eAccording to Nvidia, Project Digits delivers up to 1 PetaFLOP for 4-bit floating point, which means you can expect that level of performance for inference using quantized models but not for training. Nvidia has not disclosed the system\u0026#39;s performance for 32-bit floating point or provided details about its memory bandwidth.\u003c/p\u003e\n\n\u003cp\u003eThe announcement of Project Digits made some developers ponder whether it can be a \u003ca href=\"https://www.reddit.com/r/deeplearning/comments/1hywfah/nvidia_project_digits_vs_rtx_5090_dilemma/\"\u003epreferable choice to an Nvidia RTX 5090-based system\u003c/a\u003e. In comparison to a 5090 GPU, Project Digits has the advantage of coming in a compact box and not requiring the huge fan used on the 5090. On the other hand, the usage of low-power DDR5 memory on Project Digits seems to imply a reduced bandwidth compared to the 5090\u0026#39;s GDDR7 memory, which further hints at Project Digits being optimized for inference. However lacking final details, it\u0026#39;s hard to understand how the two solutions compare performance-wise.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://news.ycombinator.com/item?id=42619139\"\u003eAnother interesting comparison that has been brought up is with Apple\u0026#39;s M4 Max-based systems\u003c/a\u003e, which may pack up to 196GB of memory and are thus \u003ca href=\"https://news.ycombinator.com/item?id=42622421\"\u003esuitable to run large LLMs for inference\u003c/a\u003e. Here, there seem to be more similarities between the two systems, including the use of DDR5X unified memory, so it seems Nvidia is seemingly aiming, among other things, to provide an alternative to that kind of solution.\u003c/p\u003e\n\n\u003cp\u003eProject Digits will run Nvidia\u0026#39;s own Linux distribution, DGX OS, which is based on Ubuntu and includes Nvidia-optimized Linux kernel with out-of-the-box support for GPU Direct Storage (GDS). Nvidia says the first units will be available in May this year.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2025-01-13T00:00:00Z",
  "modifiedTime": null
}
