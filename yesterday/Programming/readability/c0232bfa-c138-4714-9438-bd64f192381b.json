{
  "id": "c0232bfa-c138-4714-9438-bd64f192381b",
  "title": "Gemini 2.5 for robotics and embodied intelligence",
  "link": "https://developers.googleblog.com/en/gemini-25-for-robotics-and-embodied-intelligence/",
  "description": "Gemini 2.5 Pro and Flash are transforming robotics by enhancing coding, reasoning, and multimodal capabilities, including spatial understanding. These models are used for semantic scene understanding, code generation for robot control, and building interactive applications with the Live API, with a strong emphasis on safety improvements and community applications.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Laura Graesser, Fei Xia",
  "length": 15680,
  "excerpt": "Gemini 2.5 Pro and Flash are transforming robotics by enhancing coding, reasoning, and multimodal capabilities, including spatial understanding. These models are used for semantic scene understanding, code generation for robot control, and building interactive applications with the Live API, with a strong emphasis on safety improvements and community applications.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "Fei Xia Staff Research Scientist The latest generation of Gemini models, 2.5 Pro and Flash, are unlocking new frontiers in robotics. Their advanced coding, reasoning, and multimodal capabilities, now combined with spatial understanding, provide the foundation for the next generation of interactive and intelligent robots.This post explores how developers can leverage Gemini 2.5 to build sophisticated robotics applications. We'll provide practical examples with prompts to show using Gemini 2.5 and the Live API for:Semantic scene understanding for complex queries: Identify and label objects from robot camera feeds. Understand complex queries through multimodal reasoning.Combine spatial reasoning with code generation to control robots: Use the robot's API to call functions and bring task plans to life.Build interactive robotics applications with the Live API: Convert voice commands into executable robot plans.In March, we launched our Gemini Robotics models, including Gemini Robotics-ER, our advanced embodied reasoning model optimized for the unique demands of robotics applications. We‚Äôre also excited to share how our Gemini Robotics trusted testers are already demonstrating the power of Gemini in robotics applications. We are including examples from Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools. Join the Gemini Robotics-ER trusted tester program waitlist.Semantic scene understanding for complex queriesReasoning about the physical world is at the core of general and robust control. Gemini 2.5 represents a step in this direction with its improved ability to reason multimodally. Below we share two examples, utilizing Gemini‚Äôs pointing and object detection capabilities.Pointing allows a model to refer to entities or parts of entities precisely, and locate them in space. Gemini 2.5 Pro is able to reason about the entities it is pointing to, opening new opportunities for interacting with images. For example, Gemini 2.5 Pro is able to reason about empty space in the context of a supermarket display, knowing that this indicates restocking may be needed. In the example below, Gemini identifies the baby eggplant needs restocking. Gemini 2.5 Pro also shows a nascent ability to locate and read information from that location, as illustrated in the gauge example.Example 1: Gemini 2.5 can locate objects in the scene based on fine-grained language descriptions, for example, find a shelf that needs restocking.Prompt: Point to one bin on the shelf that needs restocking. The answer should follow the json format: [{\"point\": \u003cpoint\u003e, \"label\": \u003clabel1\u003e}, ...]. The points are in [y, x] format normalized to 0-1000.Input image with response overlay: Example 2: Gemini 2.5 can locate small objects in the scene and estimate states of those objects. For example, it can read gauges.Prompt: Point to all the round gauges. The answer should follow the json format: [{\"point\": \u003cpoint\u003e, \"label\": \u003clabel1\u003e}, ...]. The points are in [y, x] format normalized to 0-1000.Input image with response overlay: Prompt: What is the reading on the middle gauge?Response: Based on the close-up view, the round gauge in the center-left of the image appears to be reading 0. The needle is pointing directly at the \"0\" mark on the dial.Going beyond object-centric perceptionGemini 2.5 is able to accurately track multiple objects across time and detect open ended concepts like ‚Äòa spill‚Äô. Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points.Example 1: Gemini 2.5 can generate bounding boxes for each frame in a video and be visualized like below.Prompt: Detect green bowl, crab, wallet, pink bowl, phone, return a json array with keys box_2d and label. (executed per frame).Input image with response overlay: Sorry, your browser doesn't support playback for this video Example 2: Gemini 2.5 can detect open-ended concepts relevant to robotics, requiring commonsense knowledge and context specific reasoning. For example, a helpful robot needs to understand the concept of a ‚Äúspill‚Äù.Prompt:1) Show me the bounding box of spill. Return in a json array with keys box_2d and label.2) Give the segmentation masks for the spill. Output a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \"box_2d\", the segmentation mask in key \"mask\", and the text label in the key \"label\".Input image with response overlay: Example 3: Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points.Prompt: Generate a robot arm trajectory of 10 points to move the cloth to the spill. The answer should follow the json format: [{\"point\": \u003cpoint\u003e, \"label\": \u003clabel1\u003e}, ...]. The points are in [y, x] format normalized to 0-1000.Input image with response overlay: Using spatial understanding and code generation to control robotsGemini 2.5 can utilize its underlying spatial understanding to control robots through code generation. By providing Gemini 2.5 with a robot control API, it can apply advanced capabilities in scene understanding, object manipulation, and code writing together to perform tasks zero-shot, with no additional training.Example 1 below showcases code-generation for ‚ÄúPut the banana in the bowl‚Äù. It gives Gemini access to a robot control API and shows how the model leverages its spatial understanding, thinking, and code generation capabilities to select the appropriate API calls and arguments given the task. Gemini 2.5 generates 2 different feasible plans for putting the banana in the bowl. The first solution is to simply pick up the banana, move it above the bowl, and drop it. The second solution lifts the banana, moves the bowl below the banana, and then drops the banana.Example 1: Gemini 2.5 Generate high-level planning code for pick-and-place tasks with different strategies.Prompt:You are given a robot control API and example code below: # Provided API class RobotAPI: def detect_object(self, obj: str) -\u003e Detection \"\"\"Detects the given object's XYZ location\"\"\" def get_grasp_position_and_euler_orientation(self, gripper: str, object_name: str) -\u003e tuple[numpy.ndarray, numpy.ndarray]: \"\"\"Returns the grasp position and orientation for the given object and gripper from the Gemini Robotics ER model or other grasping primitives. def move_gripper_to(self, gripper: str, position: numpy.ndarray, orientation: numpy.ndarray): \"\"\"Moves the gripper to the given position and orientation.\"\"\" def move_gripper_to_safe_position(self, gripper: str): \"\"\"Moves the given gripper to a safe position out of the table area.\"\"\" def open_gripper(self, gripper: str): \"\"\"Opens the given gripper.\"\"\" def close_gripper(self, gripper: str): \"\"\"Closes the given gripper.\"\"\" robot=RobotAPI() # Example code robot.open_gripper(gripper='RIGHT') banana_detection = robot.detect_object(obj='banana') banana_position = banana_detection.position print(f\"Detected banana position: {banana_position}\") # Gemini uses this to determine which gripper to use. grasp_position, grasp_orientation = robot.get_grasp_position_and_euler_orientation(gripper='RIGHT', object_name='banana') pre_grasp_position = grasp_position + np.array([0, 0, 0.1]) # Add 10cm to the Z-coordinate for pre-grasp robot.move_gripper_to(gripper='RIGHT', position=pre_grasp_position, orientation=grasp_orientation) robot.move_gripper_to(gripper='RIGHT', position=grasp_position, orientation=grasp_orientation) robot.close_gripper(gripper='RIGHT') Python Copied Now generate code for the task ‚Äúput the banana in the bowl‚Äù.Response 1 with results of code executed:Reasoning (abbreviated)1: The current position of the right gripper is [-0.04, -0.028, 0.176].2: The bowl's detected position is [-0.03839983, -0.02811164, 0.0073104].3: To place the banana inside the bowl, the gripper's Z-coordinate needs to be lowered. I will use the bowl's detected X and Y coordinates for the target position. For the Z-coordinate, I will use the bowl's detected Z-coordinate (0.0073104) and add a small offset (e.g., 0.03m) to ensure the banana is placed inside the bowl and not through the table. This would result in a target Z of approximately 0.007 + 0.03 = 0.037m. This Z-coordinate is higher than the bowl's base but lower than the typical height of a banana when standing on the table, ensuring it's placed *into* the bowl.Code Snippet: # Detect the bowl's position (re-detect to ensure fresh data, though it's unlikely to have moved) bowl_detection = robot.detect_object('bowl') if bowl_detection.position is None: raise ValueError(\"Bowl not detected.\") # Define the target Z-coordinate for dropping the banana into the bowl # Use the bowl's Z-position and add an offset to place the banana inside. # The banana's original Z was ~0.032, bowl's Z is ~0.007. # A target Z of 0.037 (0.007 + 0.03) should place it inside the bowl. drop_z = bowl_detection.position[2] + 0.03 Python Copied Sorry, your browser doesn't support playback for this video Response 2 with results of code executed:Reasoning (abbreviated)# Re-planning due to reachability constraints: The RIGHT arm cannot reach the bowl's current position.# The plan will now involve the LEFT arm moving the bowl to the RIGHT arm's reachable area.# The RIGHT arm can reach x-coordinates from 0.04m to 0.40m. I will move the bowl to an x-coordinate of 0.10m, which is well within the RIGHT arm's reach and also allows for some clearance from the center.Code Snippet: target_bowl_position = [-0.07274145, -0.08317622, 0.00720479] # Original bowl position # Move the bowl to an x-coordinate reachable by the RIGHT arm, keeping the same y and z (lifted) # Let's choose x = 0.10m for the bowl, which is within the right arm's reach (0.04 to 0.40) # Keep the lifted height at 0.20m move_bowl_position = [0.10, bowl_detection.position[1], 0.20] robot.move_gripper_to('LEFT', move_bowl_position, current_left_gripper_orientation) Python Copied Sorry, your browser doesn't support playback for this video Gemini 2.5 can also effectively utilize a small number of in-context examples to perform more dexterous robot control tasks. In the two examples shown below of an ALOHA robot packing boxes and folding a dress, 10 demonstrations consisting of interleaved reasoning and robot actions for each task were added to Gemini‚Äôs context. We‚Äôve created open-source code showing how to do this using Gemini, including examples of the input demonstrations. This enables robots to be taught and deployed on the spot. See the Colab.Example 2: Gemini 2.5 (Flash) utilizes a small number of in-context examples to perform more dexterous robot control tasks.Prompt: see colab.Response with results of code executed: Sorry, your browser doesn't support playback for this video Sorry, your browser doesn't support playback for this video Building interactive robotics applications with the Live APIThe Live API for realtime streaming was recently introduced and can be used to build interactive applications that let people control robots using their voice. Intuitive human-robot-interaction is an important aspect of making robots that are easy and safe to use. We recently showcased an interactive Gemini Robotics demo at I/O 2025, which was built around Live API for voice interaction and function calling.Live API supports both audio and video as input modalities, and audio / text as output modalities. This allows you to send both voice input and the robot camera feed to the Live API. This is even more powerful when combined with tool use.Tool use allows Live API to go beyond just conversation by enabling it to perform actions in the real-world while maintaining a real time connection. For example, the robot APIs defined above can be defined as function calls including robot.open_gripper(), robot.close_gripper() and robot.move_gripper_to(). After they are defined as tool calls, they can be integrated into the workflow where people can interact with the robot using voice in real time. Developers can get started on GitHub, and refer to API documentation for function calling features. Sorry, your browser doesn't support playback for this video üîä Demonstration of a realtime web console for robotics built with Live API, right click to open in a new tab for audio. SafetyThe 2.5 Pro and 2.5 Flash models demonstrate robust performance on the ASIMOV Multimodal and Physical Injury benchmarks released along with the Gemini Robotics tech report, exhibiting accuracy comparable to that of 2.0 models. Beyond the ASIMOV benchmarks, the 2.5 Pro and 2.5 Flash models also exhibit excellent performance in rejecting prompts that attempt to leverage embodied reasoning capabilities while violating safety policies such as promoting harmful stereotypes, discrimination, or endangerment of minors. Following rigorous evaluation against such synthetically generated adversarial prompts, 2.5 Pro and Flash demonstrated near-zero violation rates.How Gemini is being used today for RoboticsIn March we released the Gemini Robotics-ER model and we‚Äôre already inspired by how the community is using it for robotics applications. Check out these examples of interactivity, perception, planning, and function calling from our trusted testers: Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools. Sorry, your browser doesn't support playback for this video Sorry, your browser doesn't support playback for this video Sorry, your browser doesn't support playback for this video Sorry, your browser doesn't support playback for this video We can‚Äôt wait to see what you create.ReferencesEmbodied reasoning in Gemini 2.5 Flash and Pro are available in Google AI Studio, the Gemini API, and Vertex AI. To start building with these models in the Gemini API, visit our developer guide to get started. If you are interested in building with Gemini Robotics-ER, please sign up for the trusted tester program.AcknowledgementsWe thank researchers in the Embodied Reasoning team: Alex Hofer, Annie Xie, Arunkumar Byravan, Ashwin Balakrishna, Assaf Hurwitz Michaely, Carolina Parada, David D'Ambrosio, Deepali Jain, Jacky Liang, Jie Tan, Junkyung Kim, Kanishka Rao, Keerthana Gopalakrishnan, Ksenia Konyushkova, Lewis Chiang, Marissa Giustina, Mohit Sharma, Montserrat Gonzalez Arenas, Nicolas Heess, Peng Xu, Pierre Sermanet, Sean Kirmani, Stefani Karp, Stefano Saliceti, Steven Hansen, Sudeep Dasari, Ted Xiao, Thomas Lampe, Tianli Ding, Wenhao Yu, and Wentao Yuan; Gemini team: Xi Chen, Weicheng Kuo, and Paul Voigtlaender; Robotics Safety team: Vikas Sindhwani and Abhishek Jindal; Product and Program support: Kendra Byrne and Sally Jesmonth; and members of developer relationship team: Paul Ruiz and Paige Bailey, for helping with this article.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini-robotics-meta.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \u003csection\u003e\n      \n        \n          \n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=Fei+Xia\"\u003eFei Xia\u003c/a\u003e\n            \n              \u003cspan\u003eStaff Research Scientist\u003c/span\u003e\n            \n            \n          \u003c/p\u003e\n        \n\n      \n      \u003c/section\u003e\n\n    \n    \u003cdiv\u003e\n         \u003cdiv\u003e\n    \u003cp data-block-key=\"n37qf\"\u003eThe latest generation of Gemini models, \u003ca href=\"https://blog.google/products/gemini/gemini-2-5-model-family-expands/\"\u003e2.5 Pro and Flash\u003c/a\u003e, are unlocking new frontiers in robotics. Their advanced coding, reasoning, and multimodal capabilities, now combined with spatial understanding, provide the foundation for the next generation of interactive and intelligent robots.\u003c/p\u003e\u003cp data-block-key=\"8sedv\"\u003eThis post explores how developers can leverage \u003ca href=\"https://developers.googleblog.com/en/search/?query=Gemini+2.5\"\u003eGemini 2.5\u003c/a\u003e to build sophisticated robotics applications. We\u0026#39;ll provide practical examples with prompts to show using Gemini 2.5 and the Live API for:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"53n26\"\u003eSemantic scene understanding for complex queries: Identify and label objects from robot camera feeds. Understand complex queries through multimodal reasoning.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"27sh5\"\u003eCombine spatial reasoning with code generation to control robots: Use the robot\u0026#39;s API to call functions and bring task plans to life.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"7dfgs\"\u003eBuild interactive robotics applications with the \u003ca href=\"https://ai.google.dev/gemini-api/docs/live\"\u003eLive API\u003c/a\u003e: Convert voice commands into executable robot plans.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"brk5n\"\u003eIn March, we launched our \u003ca href=\"https://deepmind.google/models/gemini-robotics/\"\u003eGemini Robotics\u003c/a\u003e models, including \u003ca href=\"https://deepmind.google/models/gemini-robotics/gemini-robotics-er/\"\u003eGemini Robotics-ER\u003c/a\u003e, our advanced embodied reasoning model optimized for the unique demands of robotics applications. We‚Äôre also excited to share how our \u003ca href=\"https://deepmind.google/models/gemini-robotics/\"\u003eGemini Robotics\u003c/a\u003e trusted testers are already demonstrating the power of Gemini in robotics applications. We are including examples from Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools. \u003ca href=\"https://docs.google.com/forms/d/1sM5GqcVMWv-KmKY3TOMpVtQ-lDFeAftQ-d9xQn92jCE/edit?ts=67cef986\"\u003eJoin the Gemini Robotics-ER trusted tester program waitlist\u003c/a\u003e.\u003c/p\u003e\u003ch2 data-block-key=\"qdwxo\" id=\"semantic-scene-understanding-for-complex-queries\"\u003e\u003cbr/\u003eSemantic scene understanding for complex queries\u003c/h2\u003e\u003cp data-block-key=\"2no8m\"\u003eReasoning about the physical world is at the core of general and robust control. Gemini 2.5 represents a step in this direction with its improved ability to reason multimodally. Below we share two examples, utilizing Gemini‚Äôs pointing and object detection capabilities.\u003c/p\u003e\u003cp data-block-key=\"9pc5u\"\u003e\u003cb\u003ePointing\u003c/b\u003e allows a model to refer to entities or parts of entities precisely, and locate them in space. Gemini 2.5 Pro is able to reason about the entities it is pointing to, opening new opportunities for interacting with images. For example, Gemini 2.5 Pro is able to reason about empty space in the context of a supermarket display, knowing that this indicates restocking may be needed. In the example below, Gemini identifies the baby eggplant needs restocking. Gemini 2.5 Pro also shows a nascent ability to locate and read information from that location, as illustrated in the gauge example.\u003c/p\u003e\u003cp data-block-key=\"cmc4r\"\u003e\u003cb\u003eExample 1:\u003c/b\u003e Gemini 2.5 can locate objects in the scene based on fine-grained language descriptions, for example, find a shelf that needs restocking.\u003c/p\u003e\u003cp data-block-key=\"8kenr\"\u003e\u003cb\u003ePrompt:\u003c/b\u003e \u003ci\u003ePoint to one bin on the shelf that needs restocking. The answer should follow the json format: [{\u0026#34;point\u0026#34;: \u0026lt;point\u0026gt;, \u0026#34;label\u0026#34;: \u0026lt;label1\u0026gt;}, ...]. The points are in [y, x] format normalized to 0-1000.\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"1ecoa\"\u003e\u003cb\u003eInput image with response overlay:\u003c/b\u003e\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini-2-5-image-locate.original.png\" alt=\"Gemini 2.5 can locate objects in the scene based on fine-grained language descriptions, for example, find a shelf that needs restocking.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"n37qf\"\u003e\u003cb\u003eExample 2:\u003c/b\u003e Gemini 2.5 can locate small objects in the scene and estimate states of those objects. For example, it can read gauges.\u003c/p\u003e\u003cp data-block-key=\"5263p\"\u003e\u003cb\u003ePrompt:\u003c/b\u003e \u003ci\u003ePoint to all the round gauges. The answer should follow the json format: [{\u0026#34;point\u0026#34;: \u0026lt;point\u0026gt;, \u0026#34;label\u0026#34;: \u0026lt;label1\u0026gt;}, ...]. The points are in [y, x] format normalized to 0-1000.\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"bhuqt\"\u003e\u003cb\u003eInput image with response overlay:\u003c/b\u003e\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini-2-5-gague-2.original.png\" alt=\"Gemini 2.5 can locate small objects in the scene and estimate states of those objects. For example, it can read gauges.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"n37qf\"\u003e\u003cb\u003ePrompt:\u003c/b\u003e \u003ci\u003eWhat is the reading on the middle gauge?\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"aku1k\"\u003e\u003cb\u003eResponse:\u003c/b\u003e Based on the close-up view, the round gauge in the center-left of the image appears to be reading 0. The needle is pointing directly at the \u0026#34;0\u0026#34; mark on the dial.\u003c/p\u003e\u003ch3 data-block-key=\"mcscr\" id=\"going-beyond-object-centric-perception\"\u003e\u003cb\u003e\u003cbr/\u003eGoing beyond object-centric perception\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"3fhl6\"\u003eGemini 2.5 is able to accurately track multiple objects across time and detect open ended concepts like ‚Äòa spill‚Äô. Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points.\u003c/p\u003e\u003cp data-block-key=\"e7b51\"\u003e\u003cb\u003eExample 1:\u003c/b\u003e Gemini 2.5 can generate bounding boxes for each frame in a video and be visualized like below.\u003c/p\u003e\u003cp data-block-key=\"et7eh\"\u003e\u003cb\u003ePrompt:\u003c/b\u003e \u003ci\u003eDetect green bowl, crab, wallet, pink bowl, phone, return a json array with keys box_2d and label. (executed per frame).\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"d2pc7\"\u003e\u003cb\u003eInput image with response overlay:\u003c/b\u003e\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-hdmsnor7_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/per_frame_det_final.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"n37qf\"\u003e\u003cb\u003eExample 2:\u003c/b\u003e Gemini 2.5 can detect open-ended concepts relevant to robotics, requiring commonsense knowledge and context specific reasoning. For example, a helpful robot needs to understand the concept of a ‚Äúspill‚Äù.\u003c/p\u003e\u003cp data-block-key=\"8tljk\"\u003e\u003cb\u003ePrompt:\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"3at99\"\u003e\u003ci\u003e1) Show me the bounding box of spill. Return in a json array with keys box_2d and label.\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"cod3i\"\u003e\u003ci\u003e2) Give the segmentation masks for the spill. Output a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \u0026#34;box_2d\u0026#34;, the segmentation mask in key \u0026#34;mask\u0026#34;, and the text label in the key \u0026#34;label\u0026#34;.\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"7260e\"\u003e\u003cb\u003eInput image with response overlay:\u003c/b\u003e\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/spill3_v2.original.png\" alt=\"Gemini 2.5 can detect open-ended concepts relevant to robotics, requiring commonsense knowledge and context specific reasoning. For example, a helpful robot needs to understand the concept of a ‚Äúspill‚Äù.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"n37qf\"\u003e\u003cb\u003eExample 3:\u003c/b\u003e Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points.\u003c/p\u003e\u003cp data-block-key=\"b6ida\"\u003e\u003cb\u003ePrompt:\u003c/b\u003e \u003ci\u003eGenerate a robot arm trajectory of 10 points to move the cloth to the spill. The answer should follow the json format: [{\u0026#34;point\u0026#34;: \u0026lt;point\u0026gt;, \u0026#34;label\u0026#34;: \u0026lt;label1\u0026gt;}, ...]. The points are in [y, x] format normalized to 0-1000.\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"6ioqs\"\u003e\u003cb\u003eInput image with response overlay:\u003c/b\u003e\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/spill_traj.original.png\" alt=\"Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"o1w8l\" id=\"using-spatial-understanding-and-code-generation-to-control-robots\"\u003eUsing spatial understanding and code generation to control robots\u003c/h2\u003e\u003cp data-block-key=\"800oi\"\u003eGemini 2.5 can utilize its underlying spatial understanding to control robots through code generation. By providing Gemini 2.5 with a robot control API, it can apply advanced capabilities in scene understanding, object manipulation, and code writing together to perform tasks zero-shot, with no additional training.\u003c/p\u003e\u003cp data-block-key=\"4sm49\"\u003eExample 1 below showcases code-generation for ‚ÄúPut the banana in the bowl‚Äù. It gives Gemini access to a robot control API and shows how the model leverages its spatial understanding, thinking, and code generation capabilities to select the appropriate API calls and arguments given the task. Gemini 2.5 generates 2 different feasible plans for putting the banana in the bowl. The first solution is to simply pick up the banana, move it above the bowl, and drop it. The second solution lifts the banana, moves the bowl below the banana, and then drops the banana.\u003c/p\u003e\u003cp data-block-key=\"8a1v7\"\u003e\u003cb\u003eExample 1:\u003c/b\u003e Gemini 2.5 Generate high-level planning code for pick-and-place tasks with different strategies.\u003c/p\u003e\u003cp data-block-key=\"9a7na\"\u003e\u003cb\u003ePrompt:\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"3b4gj\"\u003e\u003ci\u003eYou are given a robot control API and example code below:\u003c/i\u003e\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003e# Provided API\nclass RobotAPI:\n  def detect_object(self, obj: str) -\u0026gt; Detection\n    \u0026#34;\u0026#34;\u0026#34;Detects the given object\u0026#39;s XYZ location\u0026#34;\u0026#34;\u0026#34;\n\n  def get_grasp_position_and_euler_orientation(self, gripper: str, object_name: str) -\u0026gt; tuple[numpy.ndarray, numpy.ndarray]:\n    \u0026#34;\u0026#34;\u0026#34;Returns the grasp position and orientation for the given object and gripper from the Gemini Robotics ER model or other grasping primitives.\n\n  def move_gripper_to(self, gripper: str, position: numpy.ndarray, orientation: numpy.ndarray):\n    \u0026#34;\u0026#34;\u0026#34;Moves the gripper to the given position and orientation.\u0026#34;\u0026#34;\u0026#34;\n   \n  def move_gripper_to_safe_position(self, gripper: str):\n    \u0026#34;\u0026#34;\u0026#34;Moves the given gripper to a safe position out of the table area.\u0026#34;\u0026#34;\u0026#34;\n\n  def open_gripper(self, gripper: str):\n    \u0026#34;\u0026#34;\u0026#34;Opens the given gripper.\u0026#34;\u0026#34;\u0026#34;\n    \n  def close_gripper(self, gripper: str):\n    \u0026#34;\u0026#34;\u0026#34;Closes the given gripper.\u0026#34;\u0026#34;\u0026#34;\n\nrobot=RobotAPI()\n\n# Example code\nrobot.open_gripper(gripper=\u0026#39;RIGHT\u0026#39;)\nbanana_detection = robot.detect_object(obj=\u0026#39;banana\u0026#39;)\nbanana_position = banana_detection.position\nprint(f\u0026#34;Detected banana position: {banana_position}\u0026#34;)    # Gemini uses this to determine which gripper to use.\n\ngrasp_position, grasp_orientation = robot.get_grasp_position_and_euler_orientation(gripper=\u0026#39;RIGHT\u0026#39;, object_name=\u0026#39;banana\u0026#39;)\npre_grasp_position = grasp_position + np.array([0, 0, 0.1]) # Add 10cm to the Z-coordinate for pre-grasp\nrobot.move_gripper_to(gripper=\u0026#39;RIGHT\u0026#39;, position=pre_grasp_position, orientation=grasp_orientation)\nrobot.move_gripper_to(gripper=\u0026#39;RIGHT\u0026#39;, position=grasp_position, orientation=grasp_orientation)\nrobot.close_gripper(gripper=\u0026#39;RIGHT\u0026#39;)\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        Python\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"n37qf\"\u003e\u003ci\u003eNow generate code for the task ‚Äúput the banana in the bowl‚Äù.\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"a8i4g\"\u003e\u003cb\u003eResponse 1 with results of code executed:\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"8gn39\"\u003eReasoning (abbreviated)\u003c/p\u003e\u003cp data-block-key=\"6h7h8\"\u003e1: The current position of the right gripper is \u003ccode\u003e[-0.04, -0.028, 0.176]\u003c/code\u003e.\u003c/p\u003e\u003cp data-block-key=\"p8og\"\u003e2: The bowl\u0026#39;s detected position is \u003ccode\u003e[-0.03839983, -0.02811164, 0.0073104]\u003c/code\u003e.\u003c/p\u003e\u003cp data-block-key=\"evivt\"\u003e3: To place the banana inside the bowl, the gripper\u0026#39;s Z-coordinate needs to be lowered. I will use the bowl\u0026#39;s detected X and Y coordinates for the target position. For the Z-coordinate, I will use the bowl\u0026#39;s detected Z-coordinate (\u003ccode\u003e0.0073104\u003c/code\u003e) and add a small offset (e.g., 0.03m) to ensure the banana is placed inside the bowl and not through the table. This would result in a target Z of approximately \u003ccode\u003e0.007 + 0.03 = 0.037m\u003c/code\u003e. This Z-coordinate is higher than the bowl\u0026#39;s base but lower than the typical height of a banana when standing on the table, ensuring it\u0026#39;s placed *into* the bowl.\u003c/p\u003e\u003cp data-block-key=\"55qlv\"\u003e\u003cb\u003eCode Snippet:\u003c/b\u003e\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003e# Detect the bowl\u0026#39;s position (re-detect to ensure fresh data, though it\u0026#39;s unlikely to have moved)\nbowl_detection = robot.detect_object(\u0026#39;bowl\u0026#39;)\nif bowl_detection.position is None:\n    raise ValueError(\u0026#34;Bowl not detected.\u0026#34;)\n\n# Define the target Z-coordinate for dropping the banana into the bowl\n# Use the bowl\u0026#39;s Z-position and add an offset to place the banana inside.\n# The banana\u0026#39;s original Z was ~0.032, bowl\u0026#39;s Z is ~0.007.\n# A target Z of 0.037 (0.007 + 0.03) should place it inside the bowl.\ndrop_z = bowl_detection.position[2] + 0.03\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        Python\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-66cw7vmd_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/lift_and_drop_final_v2.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"cony8\"\u003e\u003cb\u003eResponse 2 with results of code executed:\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"alt08\"\u003eReasoning (abbreviated)\u003c/p\u003e\u003cp data-block-key=\"et67q\"\u003e# Re-planning due to reachability constraints: The RIGHT arm cannot reach the bowl\u0026#39;s current position.\u003c/p\u003e\u003cp data-block-key=\"3fgkk\"\u003e# The plan will now involve the LEFT arm moving the bowl to the RIGHT arm\u0026#39;s reachable area.\u003c/p\u003e\u003cp data-block-key=\"fnkcs\"\u003e# The RIGHT arm can reach x-coordinates from 0.04m to 0.40m. I will move the bowl to an x-coordinate of 0.10m, which is well within the RIGHT arm\u0026#39;s reach and also allows for some clearance from the center.\u003cbr/\u003e\u003c/p\u003e\u003cp data-block-key=\"83fh2\"\u003e\u003cb\u003eCode Snippet:\u003c/b\u003e\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003etarget_bowl_position = [-0.07274145, -0.08317622, 0.00720479] # Original bowl position\n# Move the bowl to an x-coordinate reachable by the RIGHT arm, keeping the same y and z (lifted)\n# Let\u0026#39;s choose x = 0.10m for the bowl, which is within the right arm\u0026#39;s reach (0.04 to 0.40)\n# Keep the lifted height at 0.20m\nmove_bowl_position = [0.10, bowl_detection.position[1], 0.20]\nrobot.move_gripper_to(\u0026#39;LEFT\u0026#39;, move_bowl_position, current_left_gripper_orientation)\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        Python\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-6ipq9el9_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/bowl_to_banana_final.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"3kmf3\"\u003eGemini 2.5 can also effectively utilize a small number of in-context examples to perform more dexterous robot control tasks. In the two examples shown below of an ALOHA robot packing boxes and folding a dress, 10 demonstrations consisting of interleaved reasoning and robot actions for each task were added to Gemini‚Äôs context. We‚Äôve created open-source code showing how to do this using Gemini, including examples of the input demonstrations. This enables robots to be taught and deployed on the spot. See the \u003ca href=\"https://colab.research.google.com/drive/19D83Yv2LILLR5pFRqFnBHsUw9MXAI2lP\"\u003eColab\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"3a1m3\"\u003e\u003cb\u003eExample 2:\u003c/b\u003e Gemini 2.5 (Flash) utilizes a small number of in-context examples to perform more dexterous robot control tasks.\u003c/p\u003e\u003cp data-block-key=\"7gk40\"\u003e\u003cb\u003ePrompt:\u003c/b\u003e \u003ci\u003esee\u003c/i\u003e \u003ca href=\"https://colab.research.google.com/drive/19D83Yv2LILLR5pFRqFnBHsUw9MXAI2lP\"\u003e\u003ci\u003ecolab\u003c/i\u003e\u003c/a\u003e\u003ci\u003e.\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"6ilf8\"\u003e\u003cb\u003eResponse with results of code executed:\u003c/b\u003e\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-9lzps7qf_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/pack_box_icl.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-9ysg7lq3_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/fold_dress_icl.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"4qr5n\" id=\"building-interactive-robotics-applications-with-the-live-api\"\u003eBuilding interactive robotics applications with the Live API\u003c/h2\u003e\u003cp data-block-key=\"7q810\"\u003eThe \u003ca href=\"https://ai.google.dev/gemini-api/docs/live\"\u003eLive API\u003c/a\u003e for realtime streaming was recently introduced and can be used to build interactive applications that let people control robots using their voice. Intuitive human-robot-interaction is an important aspect of making robots that are easy and safe to use. We recently showcased an interactive \u003ca href=\"https://www.youtube.com/watch?v=1oSSex9b6fc\"\u003eGemini Robotics demo\u003c/a\u003e at I/O 2025, which was built around Live API for voice interaction and function calling.\u003c/p\u003e\u003cp data-block-key=\"99ndd\"\u003eLive API supports both audio and video as input modalities, and audio / text as output modalities. This allows you to send both voice input and the robot camera feed to the Live API. This is even more powerful when combined with tool use.\u003c/p\u003e\u003cp data-block-key=\"l86p\"\u003eTool use allows Live API to go beyond just conversation by enabling it to perform actions in the real-world while maintaining a real time connection. For example, the robot APIs defined above can be defined as function calls including \u003ccode\u003erobot.open_gripper()\u003c/code\u003e, \u003ccode\u003erobot.close_gripper()\u003c/code\u003e and \u003ccode\u003erobot.move_gripper_to()\u003c/code\u003e. After they are defined as tool calls, they can be integrated into the workflow where people can interact with the robot using voice in real time. Developers can \u003ca href=\"https://github.com/google-gemini/live-api-web-console\"\u003eget started on GitHub\u003c/a\u003e, and refer to \u003ca href=\"https://ai.google.dev/gemini-api/docs/live-tools\"\u003eAPI documentation\u003c/a\u003e for function calling features.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-hfu3sy_o_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/mm_live_short.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n        \n            \u003cp\u003eüîä Demonstration of a realtime web console for robotics built with Live API, right click to open in a new tab for audio.\u003c/p\u003e\n        \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"ap1tv\" id=\"safety\"\u003e\u003cb\u003eSafety\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"23a7k\"\u003eThe 2.5 Pro and 2.5 Flash models demonstrate robust performance on the \u003ca href=\"https://asimov-benchmark.github.io/\"\u003eASIMOV Multimodal and Physical Injury benchmarks\u003c/a\u003e released along with the \u003ca href=\"https://deepmind.google/models/gemini-robotics/\"\u003eGemini Robotics tech report\u003c/a\u003e, exhibiting accuracy comparable to that of 2.0 models. Beyond the ASIMOV benchmarks, the 2.5 Pro and 2.5 Flash models also exhibit excellent performance in rejecting prompts that attempt to leverage embodied reasoning capabilities while violating safety policies such as promoting harmful stereotypes, discrimination, or endangerment of minors. Following rigorous evaluation against such synthetically generated adversarial prompts, 2.5 Pro and Flash demonstrated near-zero violation rates.\u003c/p\u003e\u003ch2 data-block-key=\"lki17\" id=\"\"\u003e\u003cbr/\u003eHow Gemini is being used today for Robotics\u003c/h2\u003e\u003cp data-block-key=\"aqpv7\"\u003eIn March we released the \u003ca href=\"https://deepmind.google/models/gemini-robotics/gemini-robotics-er/\"\u003eGemini Robotics-ER model\u003c/a\u003e and we‚Äôre already inspired by how the community is using it for robotics applications. Check out these examples of interactivity, perception, planning, and function calling from our trusted testers: \u003ca href=\"https://www.agile-robots.com/en/\"\u003eAgile Robots\u003c/a\u003e, \u003ca href=\"https://www.agilityrobotics.com/\"\u003eAgility Robotics\u003c/a\u003e, \u003ca href=\"https://bostondynamics.com/\"\u003eBoston Dynamics\u003c/a\u003e, and \u003ca href=\"https://enchanted.tools/\"\u003eEnchanted Tools\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-vhiy9_rf_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/agile-robots-demo_7yG7BAf.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-3_nyjmsr_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/agility-robotics-demo.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-6hssh621_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/boston-dynamics-demo_2.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-gdgwydio_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/enchanted_tools_1_1.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"ejj4y\"\u003eWe can‚Äôt wait to see what you create.\u003c/p\u003e\u003ch3 data-block-key=\"mbzor\" id=\"references\"\u003e\u003cbr/\u003e\u003cb\u003eReferences\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"526m3\"\u003eEmbodied reasoning in Gemini 2.5 Flash and Pro are available in Google AI Studio, the Gemini API, and Vertex AI. To start building with these models in the Gemini API, visit \u003ca href=\"https://ai.google.dev/gemini-api/docs/vision?lang=python\"\u003eour developer guide\u003c/a\u003e to get started. If you are interested in building with Gemini Robotics-ER, please sign up for the \u003ca href=\"https://deepmind.google/models/gemini-robotics/\"\u003etrusted tester program\u003c/a\u003e.\u003c/p\u003e\u003chr/\u003e\u003ch3 data-block-key=\"6k4zf\" id=\"acknowledgements\"\u003e\u003cb\u003eAcknowledgements\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"as6iv\"\u003e\u003csub\u003eWe thank researchers in the Embodied Reasoning team: Alex Hofer, Annie Xie, Arunkumar Byravan, Ashwin Balakrishna, Assaf Hurwitz Michaely, Carolina Parada, David D\u0026#39;Ambrosio, Deepali Jain, Jacky Liang, Jie Tan, Junkyung Kim, Kanishka Rao, Keerthana Gopalakrishnan, Ksenia Konyushkova, Lewis Chiang, Marissa Giustina, Mohit Sharma, Montserrat Gonzalez Arenas, Nicolas Heess, Peng Xu, Pierre Sermanet, Sean Kirmani, Stefani Karp, Stefano Saliceti, Steven Hansen, Sudeep Dasari, Ted Xiao, Thomas Lampe, Tianli Ding, Wenhao Yu, and Wentao Yuan; Gemini team: Xi Chen, Weicheng Kuo, and Paul Voigtlaender; Robotics Safety team: Vikas Sindhwani and Abhishek Jindal; Product and Program support: Kendra Byrne and Sally Jesmonth; and members of developer relationship team: Paul Ruiz and Paige Bailey, for helping with this article.\u003c/sub\u003e\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "17 min read",
  "publishedTime": "2025-06-24T00:00:00Z",
  "modifiedTime": null
}
