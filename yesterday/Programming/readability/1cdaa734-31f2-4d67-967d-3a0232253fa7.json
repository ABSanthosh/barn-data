{
  "id": "1cdaa734-31f2-4d67-967d-3a0232253fa7",
  "title": "Report Shows OpenTelemetry’s Impact on Go Performance",
  "link": "https://www.infoq.com/news/2025/06/opentelemetry-go-performance/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "A new benchmark study from observability platform Coroot has shed light on the performance costs of implementing OpenTelemetry in high-throughput Go applications. The findings show that while OpenTelemetry delivers valuable trace-level insights, it introduces notable overhead, increasing CPU usage by approximately 35% and increasing network traffic and latency under load. By Craig Risi",
  "author": "Craig Risi",
  "published": "Mon, 30 Jun 2025 12:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Google Go",
    "OpenTelemetry",
    "DevOps",
    "news"
  ],
  "byline": "Craig Risi",
  "length": 3642,
  "excerpt": "A new benchmark study from observability platform Coroot has shed light on the performance costs of implementing OpenTelemetry in high-throughput Go applications. The findings show that while OpenTele",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250605075448/apple-touch-icon.png",
  "text": "A new benchmark study from observability platform Coroot has shed light on the performance costs of implementing OpenTelemetry in high-throughput Go applications. The findings show that while OpenTelemetry delivers valuable trace-level insights, it introduces notable overhead, increasing CPU usage by approximately 35% and increasing network traffic and latency under load. Using a simple HTTP service backed by an in-memory database, the study compared baseline performance with full OpenTelemetry instrumentation under identical load conditions (10,000 requests per second). Runs occurring in Docker containers across four Linux hosts revealed several findings: enabling tracing increased CPU usage from 2 to 2.7 cores (roughly 35%), memory usage rose by 5–8 MB, and 99th-percentile latency increased modestly from approximately 10 ms to 15 ms. Additionally, tracing data resulted in approximately 4 MB/s of outbound network traffic, highlighting the resource implications of full request-level telemetry. The study also contrasted SDK-based tracing with eBPF-based approaches, although caution should be noted, as Coroot sells an eBPF-based observability solution. eBPF, which avoids modifying application code, exhibited lower resource consumption—under 0.3 cores—even under heavy load when only metrics were collected. Coroot concluded that while OpenTelemetry's SDK offers detailed trace visibility, it comes with measurable overhead that must be weighed against observability needs. They argue that for use cases prioritizing low latency and running with capped resources, an eBPF-based implementation may be a more suitable compromise. This evaluation sparked conversations in the Go community. A discussion on Hacker News suggested performance gains could be achieved by optimizing SDK internals, such as using faster time functions, replacing mutexes with atomics, or marshaling methodically. On Reddit, users noted that even with zero sampling, significant overhead remains due to context propagation and span management. These perspectives underscore a broader recognition that while OpenTelemetry brings essential insights, it also introduces resource tradeoffs that require careful implementation and tuning. One user, FZambia, stated the following: \"I was initially skeptical about tracing with its overhead (both resource-wise and instrumentation process-wise) vs properly instrumented app using metrics. As time goes, I have more and more examples when tracing helped to diagnose the issue, investigating incidents. The visualization helps a lot here – cross-team communication simplifies a lot when you have a trace. Still, I see how spans contains so much unnecessary data in tags, and collecting them on every request seems so much work to do while you are not using 99.999% of those spans. Turning on sampling is again controversial - you won't find span when it's needed (sometimes it's required even if the request was successful). So reading such detailed investigations of tracing overhead is really useful, thanks!\" Coroot's benchmarks provide valuable data showing that OpenTelemetry in Go delivers powerful observability at a measurable cost, with approximately 35% CPU overhead, and increased latency under load. The community response suggests that optimizations are underway, yet teams should still balance the need for trace-level visibility against performance constraints and explore lighter-weight options like eBPF-based metrics when appropriate. About the Author Craig Risi",
  "image": "https://res.infoq.com/news/2025/06/opentelemetry-go-performance/en/headerimage/generatedHeaderImage-1750931719981.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp dir=\"ltr\"\u003eA new benchmark study from \u003ca href=\"https://coroot.com/blog/opentelemetry-for-go-measuring-the-overhead/\"\u003eobservability platform Coroot\u003c/a\u003e has shed light on the performance costs of implementing \u003ca href=\"https://www.dynatrace.com/monitoring/technologies/opentelemetry\"\u003eOpenTelemetry\u003c/a\u003e in high-throughput \u003ca href=\"https://go.dev/\"\u003eGo\u003c/a\u003e applications. The findings show that while OpenTelemetry delivers valuable trace-level insights, it introduces notable overhead, increasing CPU usage by approximately 35% and increasing network traffic and latency under load.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eUsing a simple HTTP service backed by an in-memory database, the study compared baseline performance with full OpenTelemetry instrumentation under identical load conditions (10,000 requests per second). Runs occurring in \u003ca href=\"https://www.docker.com/\"\u003eDocker\u003c/a\u003e containers across four Linux hosts revealed several findings: enabling tracing increased CPU usage from 2 to 2.7 cores (roughly 35%), memory usage rose by 5–8 MB, and 99th-percentile latency increased modestly from approximately 10 ms to 15 ms. Additionally, tracing data resulted in approximately 4 MB/s of outbound network traffic, highlighting the resource implications of full request-level telemetry.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eThe study also contrasted \u003ca href=\"https://opentelemetry.io/docs/specs/otel/trace/sdk/\"\u003eSDK-based tracing\u003c/a\u003e with eBPF-based approaches, although caution should be noted, as Coroot sells an eBPF-based observability solution. \u003ca href=\"https://ebpf.io/\"\u003eeBPF\u003c/a\u003e, which avoids modifying application code, exhibited lower resource consumption—under 0.3 cores—even under heavy load when only metrics were collected. Coroot concluded that while OpenTelemetry\u0026#39;s SDK offers detailed trace visibility, it comes with measurable overhead that must be weighed against observability needs. They argue that for use cases prioritizing low latency and running with capped resources, an eBPF-based implementation may be a more suitable compromise.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eThis evaluation sparked conversations in the Go community. A \u003ca href=\"https://news.ycombinator.com/item?id=44290331\"\u003ediscussion on Hacker News\u003c/a\u003e suggested performance gains could be achieved by optimizing SDK internals, such as using faster time functions, replacing mutexes with atomics, or marshaling methodically. On \u003ca href=\"https://www.reddit.com/r/golang/comments/1lajtwn/opentelemetry_for_go_measuring_the_overhead/\"\u003eReddit\u003c/a\u003e, users noted that even with zero sampling, significant overhead remains due to context propagation and span management. These perspectives underscore a broader recognition that while OpenTelemetry brings essential insights, it also introduces resource tradeoffs that require careful implementation and tuning.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eOne user, \u003ca href=\"https://www.reddit.com/user/FZambia/\"\u003eFZambia\u003c/a\u003e, stated the following:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp dir=\"ltr\"\u003e\u0026#34;I was initially skeptical about tracing with its overhead (both resource-wise and instrumentation process-wise) vs properly instrumented app using metrics. As time goes, I have more and more examples when tracing helped to diagnose the issue, investigating incidents. The visualization helps a lot here – cross-team communication simplifies a lot when you have a trace. Still, I see how spans contains so much unnecessary data in tags, and collecting them on every request seems so much work to do while you are not using 99.999% of those spans. Turning on sampling is again controversial - you won\u0026#39;t find span when it\u0026#39;s needed (sometimes it\u0026#39;s required even if the request was successful). So reading such detailed investigations of tracing overhead is really useful, thanks!\u0026#34;\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp dir=\"ltr\"\u003eCoroot\u0026#39;s benchmarks provide valuable data showing that OpenTelemetry in Go delivers powerful observability at a measurable cost, with approximately 35% CPU overhead, and increased latency under load. The community response suggests that optimizations are underway, yet teams should still balance the need for trace-level visibility against performance constraints and explore lighter-weight options like eBPF-based metrics when appropriate.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Craig-Risi\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eCraig Risi\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-06-30T00:00:00Z",
  "modifiedTime": null
}
