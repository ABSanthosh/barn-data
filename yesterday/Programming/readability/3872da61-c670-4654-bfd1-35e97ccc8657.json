{
  "id": "3872da61-c670-4654-bfd1-35e97ccc8657",
  "title": "AISuite is a New Open Source Python Library Providing a Unified Cross-LLM API",
  "link": "https://www.infoq.com/news/2024/12/aisuite-cross-llm-api/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Recently announced by Andrew Ng, aisuite aims to provide an OpenAI-like API around the most popular large language models (LLMs) currently available to make it easy for developers to try them out and compare results or switch from one LLM to another without having to change their code. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Wed, 04 Dec 2024 18:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Interop",
    "Python",
    "OpenAI",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "Development",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3255,
  "excerpt": "Recently announced by Andrew Ng, aisuite aims to provide an OpenAI-like API around the most popular large language models (LLMs) currently available to make it easy for developers to try them out and",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20241203101349/apple-touch-icon.png",
  "text": "Recently announced by Andrew Ng, aisuite aims to provide an OpenAI-like API around the most popular large language models (LLMs) currently available to make it easy for developers to try them out and compare results or switch from one LLM to another without having to change their code. According to Andrew Ng, using multiple LLM providers in the same application can be a hassle, whereas aisuite aims to make it so easy as changing one single string when instantiating it main component to select the desired LLM provider. For example, to use OpenAI GPT-4o, you would pass \"openai:gpt-4o\" as the model argument into the call to create aisuite chat completion agent. This is further shown in the following code snippet: import aisuite as ai client = ai.Client() messages = [ {\"role\": \"system\", \"content\": \"Respond in Pirate English.\"}, {\"role\": \"user\", \"content\": \"Tell me a joke.\"}, ] response = client.chat.completions.create( model=\"openai:gpt-4o\", messages=messages, temperature=0.75 ) print(response.choices[0].message.content) response = client.chat.completions.create( model=\"anthropic:claude-3-5-sonnet-20240620\", messages=messages, temperature=0.75 ) print(response.choices[0].message.content) To install aisuite you just run pip install aisuite. The library also provides shortcuts to install LLM providers libraries. For example, you run pip install 'aisuite[anthropic]' to install the base library plus Anthropic support. Several X users replied to Andrew Ng announcement echoeing the same feeling that aisuite actually addresses real pain points when deploying LLMs. Reddit users compared the availability of proxy libraries such as aisuite to database abstraction layers enabling to switch from, e.g., sqlite in testing to another database in production. While the overall reception was generally positive, some X and Reddit users highlighted a few limitations of aisuite, including the fact it does not yet support streaming nor other fine points such as rate limits, token usage monitoring, and so on. Likewise, it is unclear how well aisuite currently supports using custom Cloud-deployed LLMs. Anyhow, it is worth remembering that the library is still in its infancy and under active development. aisuite is not the only solution currently available to address LLM cross-compatibility. Specifically, LiteLLM appears to be a rather more mature and feature-complete solution enabling to call multiple LLMs using the same OpenAI-like API, including support for rate and budget limits on a project-by-project basis. Also worth mentioning is OpenRouter, which furthermore provides its own Web-based UI. aisuite currently supports OpenAI, Anthropic, Azure, Google, AWS, Groq, Mistral, HuggingFace and Ollama. The library is written in Python and requires developers to own API keys for any LLM providers they would like to use. The library uses either the API or the SDK published by each LLM provider to maximize stability. Currently, it is mostly focused on chat completions, but new use cases will be covered in future, say its maintainers. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2024/12/aisuite-cross-llm-api/en/headerimage/aisuite-openai-cross-llm-api-1733334051889.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eRecently announced by \u003ca href=\"https://en.wikipedia.org/wiki/Andrew_Ng\"\u003eAndrew Ng\u003c/a\u003e, \u003ca href=\"https://github.com/andrewyng/aisuite\"\u003e\u003ccode\u003eaisuite\u003c/code\u003e aims to provide an OpenAI-like API around the most popular large language models\u003c/a\u003e (LLMs) currently available to make it easy for developers to try them out and compare results or switch from one LLM to another without having to change their code.\u003c/p\u003e\n\n\u003cp\u003eAccording to Andrew Ng, \u003ca href=\"https://x.com/AndrewYNg/status/1861085482526105842\"\u003eusing multiple LLM providers in the same application can be a hassle\u003c/a\u003e, whereas \u003ccode\u003eaisuite\u003c/code\u003e aims to make it so easy as changing one single string when instantiating it main component to select the desired LLM provider. For example, to use OpenAI GPT-4o, you would pass \u0026#34;openai:gpt-4o\u0026#34; as the \u003ccode\u003emodel\u003c/code\u003e argument into the call to create \u003ccode\u003eaisuite\u003c/code\u003e chat completion agent. This is further shown in the following code snippet:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003eimport aisuite as ai\nclient = ai.Client()\n\nmessages = [\n    {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Respond in Pirate English.\u0026#34;},\n    {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me a joke.\u0026#34;},\n]\n\nresponse = client.chat.completions.create(\n    model=\u0026#34;openai:gpt-4o\u0026#34;,\n    messages=messages,\n    temperature=0.75\n)\nprint(response.choices[0].message.content)\n\nresponse = client.chat.completions.create(\n    model=\u0026#34;anthropic:claude-3-5-sonnet-20240620\u0026#34;,\n    messages=messages,\n    temperature=0.75\n)\nprint(response.choices[0].message.content)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eTo install \u003ccode\u003eaisuite\u003c/code\u003e you just run \u003ccode\u003epip install aisuite\u003c/code\u003e. The library also provides shortcuts to install LLM providers libraries. For example, you run \u003ccode\u003epip install \u0026#39;aisuite[anthropic]\u0026#39;\u003c/code\u003e to install the base library plus Anthropic support.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://x.com/AndrewYNg/status/1861085482526105842\"\u003eSeveral X users replied to Andrew Ng announcement\u003c/a\u003e echoeing the same feeling that \u003ccode\u003eaisuite\u003c/code\u003e actually addresses real pain points when deploying LLMs. Reddit users \u003ca href=\"https://www.reddit.com/r/OpenAI/comments/1h2evsb/comment/lzqjvtx/\"\u003ecompared the availability of proxy libraries such as \u003ccode\u003eaisuite\u003c/code\u003e to database abstraction layers\u003c/a\u003e enabling to switch from, e.g., sqlite in testing to another database in production.\u003c/p\u003e\n\n\u003cp\u003eWhile the overall reception was generally positive, some X and Reddit users highlighted a few limitations of \u003ccode\u003eaisuite\u003c/code\u003e, including the fact it \u003ca href=\"https://www.reddit.com/r/OpenAI/comments/1h2evsb/comment/lzk9mu0/\"\u003edoes not yet support streaming\u003c/a\u003e nor other \u003ca href=\"https://x.com/kaush_trip/status/1861087769356915069\"\u003efine points such as rate limits, token usage monitoring, and so on\u003c/a\u003e. Likewise, it is unclear how well \u003ccode\u003eaisuite\u003c/code\u003e currently supports using custom Cloud-deployed LLMs. Anyhow, it is worth remembering that the library is still in its infancy and under active development.\u003c/p\u003e\n\n\u003cp\u003e\u003ccode\u003eaisuite\u003c/code\u003e is not the only solution currently available to address LLM cross-compatibility. Specifically, \u003ca href=\"https://github.com/BerriAI/litellm\"\u003eLiteLLM\u003c/a\u003e appears to be a rather more mature and feature-complete solution enabling to call multiple LLMs using the same OpenAI-like API, including support for rate and budget limits on a project-by-project basis. Also worth mentioning is \u003ca href=\"https://github.com/OpenRouterTeam\"\u003eOpenRouter\u003c/a\u003e, which furthermore provides its own \u003ca href=\"https://openrouter.ai\"\u003eWeb-based UI\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003ccode\u003eaisuite\u003c/code\u003e currently supports OpenAI, Anthropic, Azure, Google, AWS, Groq, Mistral, HuggingFace and Ollama. The library is written in Python and requires developers to own API keys for any LLM providers they would like to use. The library uses either the API or the SDK published by each LLM provider to maximize stability. Currently, it is mostly focused on chat completions, but new use cases will be covered in future, say its maintainers.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-12-04T00:00:00Z",
  "modifiedTime": null
}
