{
  "id": "9866a9a7-49b6-4d48-8cb3-1b4dbe078f5e",
  "title": "Accelerating Large-Scale Test Migration with LLMs",
  "link": "https://medium.com/airbnb-engineering/accelerating-large-scale-test-migration-with-llms-9565c208023b?source=rss----53c7c27702d5---4",
  "description": "",
  "author": "Charles Covey-Brandt",
  "published": "Thu, 13 Mar 2025 17:01:05 GMT",
  "source": "https://medium.com/feed/airbnb-engineering",
  "categories": [
    "infrastructure",
    "engineering",
    "automation",
    "technology",
    "machine-learning"
  ],
  "byline": "Charles Covey-Brandt",
  "length": 9730,
  "excerpt": "Airbnb recently completed our first large-scale, LLM-driven code migration, updating nearly 3.5K React component test files from Enzyme to use React Testing Library (RTL) instead. We’d originally…",
  "siteName": "The Airbnb Tech Blog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "Accelerating Large-Scale Test Migration with LLMsby: Charles Covey-BrandtAirbnb recently completed our first large-scale, LLM-driven code migration, updating nearly 3.5K React component test files from Enzyme to use React Testing Library (RTL) instead. We’d originally estimated this would take 1.5 years of engineering time to do by hand, but — using a combination of frontier models and robust automation — we finished the entire migration in just 6 weeks.In this blog post, we’ll highlight the unique challenges we faced migrating from Enzyme to RTL, how LLMs excel at solving this particular type of challenge, and how we structured our migration tooling to run an LLM-driven migration at scale.BackgroundIn 2020, Airbnb adopted React Testing Library (RTL) for all new React component test development, marking our first steps away from Enzyme. Although Enzyme had served us well since 2015, it was designed for earlier versions of React, and the framework’s deep access to component internals no longer aligned with modern React testing practices.However, because of the fundamental differences between these frameworks, we couldn’t easily swap out one for the other (read more about the differences here). We also couldn’t just delete the Enzyme files, as analysis showed this would create significant gaps in our code coverage. To complete this migration, we needed an automated way to refactor test files from Enzyme to RTL while preserving the intent of the original tests and their code coverage.How We Did ItIn mid-2023, an Airbnb hackathon team demonstrated that large language models could successfully convert hundreds of Enzyme files to RTL in just a few days.Building on this promising result, in 2024 we developed a scalable pipeline for an LLM-driven migration. We broke the migration into discrete, per-file steps that we could parallelize, and configurable retry loops, and significantly expanded our prompts with additional context. Finally, we performed breadth-first prompt tuning for the long tail of complex files.1. File Validation and Refactor StepsWe started by breaking down the migration into a series of automated validation and refactor steps. Think of it like a production pipeline: each file moves through stages of validation, and when a check fails, we bring in the LLM to fix it.We modeled this flow like a state machine, moving the file to the next state only after validation on the previous state passed:Diagram shows refactor steps from Enzyme refactor, fixing Jest, fixing lint and tsc, and marking file as complete.This step-based approach provided a solid foundation for our automation pipeline. It enabled us to track progress, improve failure rates for specific steps, and rerun files or steps when needed. The step-based approach also made it simple to run migrations on hundreds of files concurrently, which was critical for both quickly migrating simple files, and chipping away at the long tail of files later in the migration.2. Retry Loops \u0026 Dynamic PromptingEarly on in the migration, we experimented with different prompt engineering strategies to improve our per-file migration success rate. However, building on the stepped approach, we found the most effective route to improve outcomes was simply brute force: retry steps multiple times until they passed or we reached a limit. We updated our steps to use dynamic prompts for each retry, giving the validation errors and the most recent version of the file to the LLM, and built a loop runner that ran each step up to a configurable number of attempts.Diagram of a retry loop. For a given step N, if the file has errors, we retry validation and attempt to fix errors unless we hit the max retries or the file no longer contains errors.With this simple retry loop, we found we could successfully migrate a large number of our simple-to-medium complexity test files, with some finishing successfully after a few retries, and most by 10 attempts.3. Increasing the ContextFor test files up to a certain complexity, just increasing our retry attempts worked well. However, to handle files with intricate test state setups or excessive indirection, we found the best approach was to push as much relevant context as possible into our prompts.By the end of the migration, our prompts had expanded to anywhere between 40,000 to 100,000 tokens, pulling in as many as 50 related files, a whole host of manually written few-shot examples, as well as examples of existing, well-written, passing test files from within the same project.Each prompt included:The source code of the component under testThe test file we were migratingValidation failures for the stepRelated tests from the same directory (maintaining team-specific patterns)General migration guidelines and common solutionsHere’s how that looked in practice (significantly trimmed down for readability):// Code example shows a trimmed down version of a prompt // including the raw source code from related files, imports, // examples, the component source itself, and the test file to migrate.const prompt = [ 'Convert this Enzyme test to React Testing Library:', `SIBLING TESTS:\\n${siblingTestFilesSourceCode}`, `RTL EXAMPLES:\\n${reactTestingLibraryExamples}`, `IMPORTS:\\n${nearestImportSourceCode}`, `COMPONENT SOURCE:\\n${componentFileSourceCode}`, `TEST TO MIGRATE:\\n${testFileSourceCode}`,].join('\\n\\n');This rich context approach proved highly effective for these more complex files — the LLM could better understand team-specific patterns, common testing approaches, and the overall architecture of the codebase.We should note that, although we did some prompt engineering at this step, the main success driver we saw was choosing the right related files (finding nearby files, good example files from the same project, filtering the dependencies for files that were relevant to the component, etc.), rather than getting the prompt engineering perfect.After building and testing our migration scripts with retries and rich contexts, when we ran our first bulk run, we successfully migrated 75% of our target files in just four hours.4. From 75% to 97%: Systematic ImprovementThat 75% success rate was really exciting to get to, but it still left us with nearly 900 files failing our step-based validation criteria. To tackle this long tail, we needed a systematic way to understand where remaining files were getting stuck and improve our migration scripts to address these issues. We also wanted to do this breadth first to aggressively chip away at our remaining files without getting stuck on the most difficult migration cases.To do this, we built two features into our migration tooling.First, we built a simple system to give us visibility into common issues our scripts were facing by stamping files with an automatically-generated comment to record the status of each migration step. Here’s what that code comment looked like:// MIGRATION STATUS: {\"enyzme\":\"done\",\"jest\":{\"passed\":8,\"failed\":2,\"total\":10,\"skipped\":0,\"successRate\":80},\"eslint\":\"pending\",\"tsc\":\"pending\",}And second, we added the ability to easily re-run single files or path patterns, filtered by the specific step they were stuck on:$ llm-bulk-migration --step=fix-jest --match=project-abc/**Using these two features, we could quickly run a feedback loop to improve our prompts and tooling:Run all remaining failing files to find common issues the LLM is getting stuck onSelect a sample of files (5 to 10) that exemplify a common issueUpdate our prompts and scripts to address that issueRe-run against the sample of failing files to validate our fixRepeat by running against all remaining files againAfter running this “sample, tune, sweep” loop for 4 days, we had pushed our completed files from 75% to 97% of the total files, and had just under 100 files remaining. By this point, we had retried many of these long tail files anywhere between 50 to 100 times, and it seemed we were pushing into a ceiling of what we could fix via automation. Rather than invest in more tuning, we opted to manually fix the remaining files, working from the baseline (failing) refactors to reduce the time to get those files over the finish line.Results and ImpactWith the validation and refactor pipeline, retry loops, and expanded context in place, we were able to automatically migrate 75% of our target files in 4 hours.After four days of prompt and script refinement using the “sample, tune, and sweep” strategy, we reached 97% of the 3.5K original Enzyme files.And for the remaining 3% of files that didn’t complete through automation, our scripts provided a great baseline for manual intervention, allowing us to complete the migration for those remaining files in another week of work.Most importantly, we were able to replace Enzyme while maintaining original test intent and our overall code coverage. And even with high retry counts on the long tail of the migration, the total cost — including LLM API usage and six weeks of engineering time — proved far more efficient than our original manual migration estimate.What’s NextThis migration underscores the power of LLMs for large-scale code transformation. We plan to expand this approach, develop more sophisticated migration tools, and explore new applications of LLM-powered automation to enhance developer productivity.Want to help shape the future of developer tools? We’re hiring engineers who love solving complex problems at scale. Check out our careers page to learn more.****************All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*j0QXnA13Sy5ruaIAU5C_eg.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"15ea\" data-testid=\"storyTitle\"\u003e\u003cstrong\u003eAccelerating Large-Scale Test Migration with LLMs\u003c/strong\u003e\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca rel=\"noopener follow\" href=\"https://medium.com/@chazcb?source=post_page---byline--9565c208023b---------------------------------------\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Charles Covey-Brandt\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*fKj6RhdtWnPJM_FikvqMFQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/airbnb-engineering?source=post_page---byline--9565c208023b---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"The Airbnb Tech Blog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*MlNQKg-sieBGW5prWoe9HQ.jpeg\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"4c1e\"\u003eby: Charles Covey-Brandt\u003c/p\u003e\u003cp id=\"89dd\"\u003eAirbnb recently completed our first large-scale, LLM-driven code migration, updating nearly 3.5K React component test files from Enzyme to use React Testing Library (RTL) instead. We’d originally estimated this would take 1.5 years of engineering time to do by hand, but — using a combination of frontier models and robust automation — we finished the entire migration in just 6 weeks.\u003c/p\u003e\u003cp id=\"29b0\"\u003eIn this blog post, we’ll highlight the unique challenges we faced migrating from Enzyme to RTL, how LLMs excel at solving this particular type of challenge, and how we structured our migration tooling to run an LLM-driven migration at scale.\u003c/p\u003e\u003ch2 id=\"d5be\"\u003eBackground\u003c/h2\u003e\u003cp id=\"4b0b\"\u003eIn 2020, Airbnb adopted React Testing Library (RTL) for all new React component test development, marking our first steps away from Enzyme. Although Enzyme had served us well since 2015, it was designed for earlier versions of React, and the framework’s deep access to component internals no longer aligned with modern React testing practices.\u003c/p\u003e\u003cp id=\"afd1\"\u003eHowever, because of the fundamental differences between these frameworks, we couldn’t easily swap out one for the other (read more about the differences \u003ca href=\"https://kentcdodds.com/blog/introducing-the-react-testing-library\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e). We also couldn’t just delete the Enzyme files, as analysis showed this would create significant gaps in our code coverage. To complete this migration, we needed an automated way to refactor test files from Enzyme to RTL while preserving the intent of the original tests \u003cem\u003eand\u003c/em\u003e their code coverage.\u003c/p\u003e\u003ch2 id=\"3b8e\"\u003eHow We Did It\u003c/h2\u003e\u003cp id=\"4e1d\"\u003eIn mid-2023, an Airbnb hackathon team demonstrated that large language models could successfully convert hundreds of Enzyme files to RTL in just a few days.\u003c/p\u003e\u003cp id=\"5756\"\u003eBuilding on this promising result, in 2024 we developed a scalable pipeline for an LLM-driven migration. We broke the migration into discrete, per-file steps that we could parallelize, and configurable retry loops, and significantly expanded our prompts with additional context. Finally, we performed breadth-first prompt tuning for the long tail of complex files.\u003c/p\u003e\u003ch2 id=\"f20f\"\u003e1. File Validation and Refactor Steps\u003c/h2\u003e\u003cp id=\"bb71\"\u003eWe started by breaking down the migration into a series of automated validation and refactor steps. Think of it like a production pipeline: each file moves through stages of validation, and when a check fails, we bring in the LLM to fix it.\u003c/p\u003e\u003cp id=\"7f54\"\u003eWe modeled this flow like a state machine, moving the file to the next state only after validation on the previous state passed:\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eDiagram shows refactor steps from Enzyme refactor, fixing Jest, fixing lint and tsc, and marking file as complete.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"e613\"\u003eThis step-based approach provided a solid foundation for our automation pipeline. It enabled us to track progress, improve failure rates for specific steps, and rerun files or steps when needed. The step-based approach also made it simple to run migrations on hundreds of files concurrently, which was critical for both quickly migrating simple files, and chipping away at the long tail of files later in the migration.\u003c/p\u003e\u003ch2 id=\"0e90\"\u003e2. Retry Loops \u0026amp; Dynamic Prompting\u003c/h2\u003e\u003cp id=\"bd6b\"\u003eEarly on in the migration, we experimented with different prompt engineering strategies to improve our per-file migration success rate. However, building on the stepped approach, we found the most effective route to improve outcomes was simply brute force: retry steps multiple times until they passed or we reached a limit. We updated our steps to use dynamic prompts for each retry, giving the validation errors and the most recent version of the file to the LLM, and built a loop runner that ran each step up to a configurable number of attempts.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eDiagram of a retry loop. For a given step N, if the file has errors, we retry validation and attempt to fix errors unless we hit the max retries or the file no longer contains errors.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"8ba7\"\u003eWith this simple retry loop, we found we could successfully migrate a large number of our simple-to-medium complexity test files, with some finishing successfully after a few retries, and most by 10 attempts.\u003c/p\u003e\u003ch2 id=\"c032\"\u003e3. Increasing the Context\u003c/h2\u003e\u003cp id=\"e324\"\u003eFor test files up to a certain complexity, just increasing our retry attempts worked well. However, to handle files with intricate test state setups or excessive indirection, we found the best approach was to push as much relevant context as possible into our prompts.\u003c/p\u003e\u003cp id=\"05f4\"\u003eBy the end of the migration, our prompts had expanded to anywhere between 40,000 to 100,000 tokens, pulling in as many as 50 related files, a whole host of manually written \u003ca href=\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003efew-shot\u003c/a\u003e examples, as well as examples of existing, well-written, passing test files from within the same project.\u003c/p\u003e\u003cp id=\"3514\"\u003eEach prompt included:\u003c/p\u003e\u003cul\u003e\u003cli id=\"e3f8\"\u003eThe source code of the component under test\u003c/li\u003e\u003cli id=\"1fdc\"\u003eThe test file we were migrating\u003c/li\u003e\u003cli id=\"eb57\"\u003eValidation failures for the step\u003c/li\u003e\u003cli id=\"b55a\"\u003eRelated tests from the same directory (maintaining team-specific patterns)\u003c/li\u003e\u003cli id=\"7906\"\u003eGeneral migration guidelines and common solutions\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"fecc\"\u003eHere’s how that looked in practice (significantly trimmed down for readability):\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"5f00\"\u003e// Code example shows a trimmed down version of a prompt \u003cbr/\u003e// including the raw source code from related files, imports, \u003cbr/\u003e// examples, the component source itself, and the test file to migrate.\u003cp\u003econst prompt = [\u003cbr/\u003e  \u0026#39;Convert this Enzyme test to React Testing Library:\u0026#39;,\u003cbr/\u003e  `SIBLING TESTS:\\n${siblingTestFilesSourceCode}`,\u003cbr/\u003e  `RTL EXAMPLES:\\n${reactTestingLibraryExamples}`,\u003cbr/\u003e  `IMPORTS:\\n${nearestImportSourceCode}`,\u003cbr/\u003e  `COMPONENT SOURCE:\\n${componentFileSourceCode}`,\u003cbr/\u003e  `TEST TO MIGRATE:\\n${testFileSourceCode}`,\u003cbr/\u003e].join(\u0026#39;\\n\\n\u0026#39;);\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"d19b\"\u003eThis rich context approach proved highly effective for these more complex files — the LLM could better understand team-specific patterns, common testing approaches, and the overall architecture of the codebase.\u003c/p\u003e\u003cp id=\"6d25\"\u003eWe should note that, although we did some prompt engineering at this step, the main success driver we saw was choosing the \u003cem\u003eright\u003c/em\u003e related files (finding nearby files, good example files from the same project, filtering the dependencies for files that were relevant to the component, etc.), rather than getting the prompt engineering perfect.\u003c/p\u003e\u003cp id=\"d2c1\"\u003eAfter building and testing our migration scripts with retries and rich contexts, when we ran our first bulk run, \u003cstrong\u003ewe successfully migrated 75% of our target files in just four hours\u003c/strong\u003e.\u003c/p\u003e\u003ch2 id=\"93c5\"\u003e4. From 75% to 97%: Systematic Improvement\u003c/h2\u003e\u003cp id=\"c73c\"\u003eThat 75% success rate was really exciting to get to, but it still left us with nearly 900 files failing our step-based validation criteria. To tackle this long tail, we needed a systematic way to understand where remaining files were getting stuck and improve our migration scripts to address these issues. We also wanted to do this \u003cem\u003ebreadth first\u003c/em\u003e to aggressively chip away at our remaining files without getting stuck on the most difficult migration cases.\u003c/p\u003e\u003cp id=\"690c\"\u003eTo do this, we built two features into our migration tooling.\u003c/p\u003e\u003cp id=\"915a\"\u003eFirst, we built a simple system to give us visibility into common issues our scripts were facing by stamping files with an automatically-generated comment to record the status of each migration step. Here’s what that code comment looked like:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"ad68\"\u003e// MIGRATION STATUS: {\u0026#34;enyzme\u0026#34;:\u0026#34;done\u0026#34;,\u0026#34;jest\u0026#34;:{\u0026#34;passed\u0026#34;:8,\u0026#34;failed\u0026#34;:2,\u0026#34;total\u0026#34;:10,\u0026#34;skipped\u0026#34;:0,\u0026#34;successRate\u0026#34;:80},\u0026#34;eslint\u0026#34;:\u0026#34;pending\u0026#34;,\u0026#34;tsc\u0026#34;:\u0026#34;pending\u0026#34;,}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"5549\"\u003eAnd second, we added the ability to easily re-run single files or path patterns, filtered by the specific step they were stuck on:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"4bb8\"\u003e$ llm-bulk-migration --step=fix-jest --match=project-abc/**\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"1aa6\"\u003eUsing these two features, we could quickly run a feedback loop to improve our prompts and tooling:\u003c/p\u003e\u003col\u003e\u003cli id=\"3d22\"\u003eRun all remaining failing files to find common issues the LLM is getting stuck on\u003c/li\u003e\u003cli id=\"b93e\"\u003eSelect a sample of files (5 to 10) that exemplify a common issue\u003c/li\u003e\u003cli id=\"f507\"\u003eUpdate our prompts and scripts to address that issue\u003c/li\u003e\u003cli id=\"5da3\"\u003eRe-run against the sample of failing files to validate our fix\u003c/li\u003e\u003cli id=\"5361\"\u003eRepeat by running against all remaining files again\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"f527\"\u003eAfter running this “sample, tune, sweep” loop for 4 days, we had pushed our completed files from 75% to 97% of the total files, and had just under 100 files remaining. By this point, we had retried many of these long tail files anywhere between 50 to 100 times, and it seemed we were pushing into a ceiling of what we could fix via automation. Rather than invest in more tuning, we opted to manually fix the remaining files, working from the baseline (failing) refactors to reduce the time to get those files over the finish line.\u003c/p\u003e\u003ch2 id=\"7d0c\"\u003eResults and Impact\u003c/h2\u003e\u003cp id=\"bc49\"\u003eWith the validation and refactor pipeline, retry loops, and expanded context in place, we were able to automatically migrate 75% of our target files in 4 hours.\u003c/p\u003e\u003cp id=\"a684\"\u003eAfter four days of prompt and script refinement using the “sample, tune, and sweep” strategy, we reached 97% of the 3.5K original Enzyme files.\u003c/p\u003e\u003cp id=\"093c\"\u003eAnd for the remaining 3% of files that didn’t complete through automation, our scripts provided a great baseline for manual intervention, allowing us to complete the migration for those remaining files in another week of work.\u003c/p\u003e\u003cp id=\"7a35\"\u003eMost importantly, we were able to replace Enzyme while maintaining original test intent and our overall code coverage. And even with high retry counts on the long tail of the migration, the total cost — including LLM API usage and six weeks of engineering time — proved far more efficient than our original manual migration estimate.\u003c/p\u003e\u003ch2 id=\"74d4\"\u003eWhat’s Next\u003c/h2\u003e\u003cp id=\"f749\"\u003eThis migration underscores the power of LLMs for large-scale code transformation. We plan to expand this approach, develop more sophisticated migration tools, and explore new applications of LLM-powered automation to enhance developer productivity.\u003c/p\u003e\u003cp id=\"3f61\"\u003eWant to help shape the future of developer tools? We’re hiring engineers who love solving complex problems at scale. Check out our \u003ca href=\"https://careers.airbnb.com\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecareers page\u003c/a\u003e to learn more.\u003c/p\u003e\u003ch2 id=\"16b0\"\u003e****************\u003c/h2\u003e\u003cp id=\"204c\"\u003e\u003cem\u003eAll product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-03-13T17:01:05.703Z",
  "modifiedTime": null
}
