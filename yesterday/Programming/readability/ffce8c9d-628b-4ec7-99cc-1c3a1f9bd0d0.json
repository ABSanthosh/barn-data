{
  "id": "ffce8c9d-628b-4ec7-99cc-1c3a1f9bd0d0",
  "title": "Case Study: How Junie Uses TeamCity to Evaluate Coding Agents",
  "link": "https://blog.jetbrains.com/teamcity/2025/06/how-junie-uses-teamcity/",
  "description": "Introduction Junie is an intelligent coding agent developed by JetBrains. It automates the full development loop: reading project files, editing code, running tests, and applying fixes, going far beyond simple code generation. Similar to how developers use tools like ChatGPT to solve coding problems, Junie takes it a step further by automating the entire process. […]",
  "author": "Olga Bedrina",
  "published": "Tue, 03 Jun 2025 07:57:45 +0000",
  "source": "https://blog.jetbrains.com/feed",
  "categories": [
    "how-tos",
    "jetbrains-ai",
    "news",
    "teamcity-2",
    "ai",
    "junie"
  ],
  "byline": "Olga Bedrina",
  "length": 9286,
  "excerpt": "How do you measure progress when building an AI coding agent? The Junie team at JetBrains uses TeamCity to evaluate agent performance at scale. Learn more from this case study.",
  "siteName": "The JetBrains Blog",
  "favicon": "https://blog.jetbrains.com/wp-content/uploads/2024/01/cropped-mstile-310x310-1-180x180.png",
  "text": "Powerful CI/CD for DevOps-centric teams How-To's JetBrains AI News TeamCityCase Study: How Junie Uses TeamCity to Evaluate Coding Agents Introduction Junie is an intelligent coding agent developed by JetBrains. It automates the full development loop: reading project files, editing code, running tests, and applying fixes, going far beyond simple code generation. Similar to how developers use tools like ChatGPT to solve coding problems, Junie takes it a step further by automating the entire process. As the agent’s architecture evolved, the team needed a secure, robust way to measure progress. They wanted to build a scalable, reproducible evaluation pipeline that would be able to track changes across hundreds of tasks. That’s where TeamCity came in. Junie’s development team uses TeamCity to orchestrate large-scale evaluations, coordinate Dockerized environments, and track important metrics that guide Junie’s improvements. The challenge Validating agent improvements at scale As Junie’s agents became more capable, with new commands and smarter decision-making, every change needed to be tested for real impact. Evaluation had to be systematic, repeatable, and grounded in data. “Did it get better or not?’ is a very poor way to evaluate. If I just try three examples from memory and see if it got better, that leads nowhere. That’s not how you achieve stable, consistent improvements. You need a benchmark with a large and diverse enough set of tasks to actually measure anything.” Danila Savenkov, Team Lead, JetBrains Junie The team identified five core requirements for this process: Scale: Evaluations had to cover at least 100 tasks per run to minimize statistical noise. Running fewer tasks made it hard to draw meaningful conclusions​. Parallel execution: Tasks needed to be evaluated in parallel, as running them sequentially would take over 24 hours and delay feedback loops​. Reproducibility: It had to be possible to trace every evaluation back to the exact version of the agent, datasets, and environment used. Local experiments or inconsistent setups were not acceptable​. Cost control: Each evaluation involved significant LLM API usage, typically costing USD 100+ per run. Tracking and managing these costs was essential​. Data preservation: Results, logs, and artifacts needed to be stored reliably for analysis, debugging, and long-term tracking​. Benchmarking with SWE-bench For a reliable signal, Junie adopted SWE-bench, a benchmark built from real GitHub issues and PRs. They also used SWE-bench Verified, a curated 500-task subset validated by OpenAI for clarity and feasibility. In parallel, Junie created in-house benchmarks for their internal monorepo (Java/Kotlin), Web stack, and Go codebases, continuously covering more languages and technologies by the benchmarks. The operational challenge Running these large-scale evaluations posed operational challenges: Spinning up consistent, isolated environments for each task. Managing dependencies and project setups. Applying patches generated by agents and running validations automatically. Collecting structured logs and metrics for deep analysis. Manual workflows wouldn’t scale. Junie needed automation that was fast, repeatable, and deeply integrated into their engineering stack. TeamCity enabled that orchestration. With it, the Junie team built an evaluation pipeline that is scalable, traceable, and deeply integrated into their development loop. The solution To support reliable, large-scale evaluation of its coding agents, Junie implemented an evaluation pipeline powered by TeamCity, a CI/CD solution developed by JetBrains. TeamCity orchestrates the execution of hundreds of tasks in parallel, manages isolated environments for each benchmark case, and coordinates patch validation and result collection. “If we tried running this locally, it just wouldn’t be realistic. A single evaluation would take a full day. That’s why we use TeamCity: to do everything in parallel, isolated environments, and to ensure the results are reproducible.” Danila Savenkov, Team Lead, JetBrains Junie The setup enables the team to trace outcomes to specific agent versions, gather detailed logs for analysis, and run evaluations efficiently, while keeping infrastructure complexity and LLM usage costs under control. Execution pipeline design At the heart of the system is a composite build configuration defined using Kotlin DSL, which gives Junie full control over task orchestration. Each top-level evaluation run includes multiple build steps. Example of a build chain in TeamCity Environment setup Each coding task is paired with a dedicated environment, typically a pre-built Docker container with the necessary dependencies already installed. This guarantees consistency across runs and eliminates local setup variability​. Agent execution Junie’s agent is launched against the task. It receives a full prompt, including the issue description, code structure, system commands, and guidelines. It then autonomously works through the problem, issuing actions such as file edits, replacements, and test runs​.  The final output is a code patch meant to resolve the issue. Patch evaluation The generated patch is passed to the next build step, where TeamCity applies it to the project and runs the validation suite. This mimics the GitHub pull request flow – if the original tests were failing and now pass, the task is marked as successfully completed​. Metric logging Execution metadata, including logs, command traces, and success/failure flags, is exported to an open-source distributed storage and processing system. Junie uses it to store evaluation artifacts and perform large-scale analysis.  With the solution’s support for SQL-like querying and scalable data processing, the team can efficiently aggregate insights across hundreds of tasks and track agent performance over time. Developers rely on this data to: Track the percentage of solved tasks (their “North Star” metric). Analyze the average cost per task for LLM API usage. Break down agent behavior ( like the most frequent commands or typical failure points). Compare performance between agent versions​. Scalability through automation By using Kotlin DSL and TeamCity’s composable build model, Junie scales evaluations to hundreds of tasks per session – far beyond what could be managed manually. For larger datasets (typically 300-2000 tasks), each execution is spun up in parallel, minimizing runtime and allowing the team to test changes frequently. “We use Kotlin DSL to configure everything. When you have 13 builds, you can still manage them manually, but when it’s 399, or 500, or 280, it starts getting tricky.” Danila Savenkov, Team Lead, JetBrains Junie Results: reproducible, scalable, insight-driven agent development TeamCity has enabled Junie to measure agent performance efficiently and at scale, making their development process faster, more reliable, and data-driven. Key outcomes ChallengeResult with TeamCityValidate agent changes at scale100+ tasks per run, reducing statistical noiseLong evaluation cycles (24+ hrs)Tasks run in parallel – now completed in a manageable windowInconsistent local testingEvery run is reproducible and traceable to the exact agent and datasetExpensive LLM usagePer-task usage is tracked, helping optimize development and costsFragile logging and data lossLogs and outcomes are automatically stored for later debugging and review Need to scale your AI workflows? TeamCity gives you the infrastructure to evaluate and iterate with confidence. Start your free trial or request a demo. Subscribe to TeamCity Blog updates Discover more",
  "image": "https://blog.jetbrains.com/wp-content/uploads/2025/06/how-junie-uses-teamcity-social.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n    \u003cdiv\u003e\n                        \u003ca href=\"https://blog.jetbrains.com/teamcity/\"\u003e\n                            \u003cimg src=\"https://blog.jetbrains.com/wp-content/uploads/2019/01/TeamCity-2.svg\" alt=\"Teamcity logo\"/\u003e\n                                                                                                \n                                                                                    \u003c/a\u003e\n                                                    \u003cp\u003ePowerful CI/CD for DevOps-centric teams\u003c/p\u003e\n                                            \u003c/div\u003e\n                            \u003csection data-clarity-region=\"article\"\u003e\n                \u003cdiv\u003e\n                    \t\t\t\t\u003cp\u003e\u003ca href=\"https://blog.jetbrains.com/teamcity/category/how-tos/\"\u003eHow-To\u0026#39;s\u003c/a\u003e\n\t\t\t\u003ca href=\"https://blog.jetbrains.com/teamcity/category/jetbrains-ai/\"\u003eJetBrains AI\u003c/a\u003e\n\t\t\t\u003ca href=\"https://blog.jetbrains.com/teamcity/category/news/\"\u003eNews\u003c/a\u003e\n\t\t\t\u003ca href=\"https://blog.jetbrains.com/teamcity/category/teamcity-2/\"\u003eTeamCity\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"major-updates\"\u003eCase Study: How Junie Uses TeamCity to Evaluate Coding Agents\u003c/h2\u003e                    \n                    \n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://www.jetbrains.com/junie/\" target=\"_blank\" rel=\"noopener\"\u003eJunie\u003c/a\u003e is an intelligent coding agent developed by JetBrains. It automates the full development loop: reading project files, editing code, running tests, and applying fixes, going far beyond simple code generation.\u003c/p\u003e\n\n\n\n\u003cp\u003eSimilar to how developers use tools like ChatGPT to solve coding problems, Junie takes it a step further by automating the entire process.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs the agent’s architecture evolved, the team needed a secure, robust way to measure progress. They wanted to build a scalable, reproducible evaluation pipeline that would be able to track changes across hundreds of tasks.\u003c/p\u003e\n\n\n\n\u003cp\u003eThat’s where \u003ca href=\"https://jetbrains.com/teamcity\" target=\"_blank\" rel=\"noopener\"\u003eTeamCity\u003c/a\u003e came in. Junie’s development team uses TeamCity to orchestrate large-scale evaluations, coordinate Dockerized environments, and track important metrics that guide Junie’s improvements.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe challenge\u003c/h2\u003e\n\n\n\n\u003ch3\u003eValidating agent improvements at scale\u003c/h3\u003e\n\n\n\n\u003cp\u003eAs Junie’s agents became more capable, with new commands and smarter decision-making, every change needed to be tested for real impact. Evaluation had to be systematic, repeatable, and grounded in data.\u003c/p\u003e\n\n\n    \u003cdiv\u003e\n                    \u003cblockquote\u003e\u003cp\u003e“Did it get better or not?’ is a very poor way to evaluate. If I just try three examples from memory and see if it got better, that leads nowhere. That’s not how you achieve stable, consistent improvements. You need a benchmark with a large and diverse enough set of tasks to actually measure anything.”\u003c/p\u003e\u003c/blockquote\u003e\n            \u003cdiv\u003e\n                                    \u003cp\u003e\u003cimg decoding=\"async\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/06/danila-savenkov-1.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDanila Savenkov, Team Lead, JetBrains Junie\u003c/strong\u003e\n                                                        \u003c/p\u003e\n            \u003c/div\u003e\n            \u003c/div\u003e\n\n\n\n\u003cp\u003eThe team identified five core requirements for this process:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eScale\u003c/strong\u003e: Evaluations had to cover at least 100 tasks per run to minimize statistical noise. Running fewer tasks made it hard to draw meaningful conclusions​.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eParallel execution\u003c/strong\u003e: Tasks needed to be evaluated in parallel, as running them sequentially would take over 24 hours and delay feedback loops​.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eReproducibility\u003c/strong\u003e: It had to be possible to trace every evaluation back to the exact version of the agent, datasets, and environment used. Local experiments or inconsistent setups were not acceptable​.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eCost control\u003c/strong\u003e: Each evaluation involved significant LLM API usage, typically costing USD 100+ per run. Tracking and managing these costs was essential​.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eData preservation\u003c/strong\u003e: Results, logs, and artifacts needed to be stored reliably for analysis, debugging, and long-term tracking​.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch3\u003eBenchmarking with SWE-bench\u003c/h3\u003e\n\n\n\n\u003cp\u003eFor a reliable signal, Junie adopted \u003ca href=\"https://www.swebench.com/\" target=\"_blank\" rel=\"noopener\"\u003eSWE-bench\u003c/a\u003e, a benchmark built from real GitHub issues and PRs. They also used SWE-bench Verified, a curated 500-task subset validated by OpenAI for clarity and feasibility.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn parallel, Junie created in-house benchmarks for their internal monorepo (Java/Kotlin), Web stack, and Go codebases, continuously covering more languages and technologies by the benchmarks.\u003c/p\u003e\n\n\n\n\u003ch3\u003eThe operational challenge\u003c/h3\u003e\n\n\n\n\u003cp\u003eRunning these large-scale evaluations posed operational challenges:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eSpinning up consistent, isolated environments for each task.\u003c/li\u003e\n\n\n\n\u003cli\u003eManaging dependencies and project setups.\u003c/li\u003e\n\n\n\n\u003cli\u003eApplying patches generated by agents and running validations automatically.\u003c/li\u003e\n\n\n\n\u003cli\u003eCollecting structured logs and metrics for deep analysis.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eManual workflows wouldn’t scale. Junie needed automation that was fast, repeatable, and deeply integrated into their engineering stack.\u003c/p\u003e\n\n\n\n\u003cp\u003eTeamCity enabled that orchestration. With it, the Junie team built an evaluation pipeline that is scalable, traceable, and deeply integrated into their development loop.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe solution\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo support reliable, large-scale evaluation of its coding agents, Junie implemented an evaluation pipeline powered by TeamCity, a CI/CD solution developed by JetBrains.\u003c/p\u003e\n\n\n\n\u003cp\u003eTeamCity orchestrates the execution of hundreds of tasks in parallel, manages isolated environments for each benchmark case, and coordinates patch validation and result collection.\u003c/p\u003e\n\n\n    \u003cdiv\u003e\n                    \u003cblockquote\u003e\u003cp\u003e“If we tried running this locally, it just wouldn’t be realistic. A single evaluation would take a full day. That’s why we use TeamCity: to do everything in parallel, isolated environments, and to ensure the results are reproducible.”\u003c/p\u003e\u003c/blockquote\u003e\n            \u003cdiv\u003e\n                                    \u003cp\u003e\u003cimg decoding=\"async\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/06/danila-savenkov-1.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDanila Savenkov, Team Lead, JetBrains Junie\u003c/strong\u003e\n                                                        \u003c/p\u003e\n            \u003c/div\u003e\n            \u003c/div\u003e\n\n\n\n\u003cp\u003eThe setup enables the team to trace outcomes to specific agent versions, gather detailed logs for analysis, and run evaluations efficiently, while keeping infrastructure complexity and LLM usage costs under control.\u003c/p\u003e\n\n\n\n\u003ch3\u003eExecution pipeline design\u003c/h3\u003e\n\n\n\n\u003cp\u003eAt the heart of the system is a composite build configuration defined using Kotlin DSL, which gives Junie full control over task orchestration. Each top-level evaluation run includes multiple build steps.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" fetchpriority=\"high\" width=\"1999\" height=\"1192\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/06/teamcity-for-junie.png\" alt=\"\"/\u003e\u003cfigcaption\u003e\u003cem\u003eExample of a build chain in TeamCity\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch3\u003eEnvironment setup\u003c/h3\u003e\n\n\n\n\u003cp\u003eEach coding task is paired with a dedicated environment, typically a pre-built Docker container with the necessary dependencies already installed. This guarantees consistency across runs and eliminates local setup variability​.\u003c/p\u003e\n\n\n\n\u003ch3\u003eAgent execution\u003c/h3\u003e\n\n\n\n\u003cp\u003eJunie’s agent is launched against the task. It receives a full prompt, including the issue description, code structure, system commands, and guidelines. It then autonomously works through the problem, issuing actions such as file edits, replacements, and test runs​. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe final output is a code patch meant to resolve the issue.\u003c/p\u003e\n\n\n\n\u003ch3\u003ePatch evaluation\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe generated patch is passed to the next build step, where TeamCity applies it to the project and runs the validation suite. This mimics the GitHub pull request flow – if the original tests were failing and now pass, the task is marked as successfully completed​.\u003c/p\u003e\n\n\n\n\u003ch3\u003eMetric logging\u003c/h3\u003e\n\n\n\n\u003cp\u003eExecution metadata, including logs, command traces, and success/failure flags, is exported to an open-source distributed storage and processing system. Junie uses it to store evaluation artifacts and perform large-scale analysis. \u003c/p\u003e\n\n\n\n\u003cp\u003eWith the solution’s support for SQL-like querying and scalable data processing, the team can efficiently aggregate insights across hundreds of tasks and track agent performance over time.\u003c/p\u003e\n\n\n\n\u003cp\u003eDevelopers rely on this data to:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eTrack the percentage of solved tasks (their “North Star” metric).\u003c/li\u003e\n\n\n\n\u003cli\u003eAnalyze the average cost per task for LLM API usage.\u003c/li\u003e\n\n\n\n\u003cli\u003eBreak down agent behavior ( like the most frequent commands or typical failure points).\u003c/li\u003e\n\n\n\n\u003cli\u003eCompare performance between agent versions​.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch3\u003eScalability through automation\u003c/h3\u003e\n\n\n\n\u003cp\u003eBy using Kotlin DSL and TeamCity’s composable build model, Junie scales evaluations to hundreds of tasks per session – far beyond what could be managed manually. For larger datasets (typically 300-2000 tasks), each execution is spun up in parallel, minimizing runtime and allowing the team to test changes frequently.\u003c/p\u003e\n\n\n    \u003cdiv\u003e\n                    \u003cblockquote\u003e\u003cp\u003e“We use Kotlin DSL to configure everything. When you have 13 builds, you can still manage them manually, but when it’s 399, or 500, or 280, it starts getting tricky.”\u003c/p\u003e\u003c/blockquote\u003e\n            \u003cdiv\u003e\n                                    \u003cp\u003e\u003cimg decoding=\"async\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/06/danila-savenkov-1.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDanila Savenkov, Team Lead, JetBrains Junie\u003c/strong\u003e\n                                                        \u003c/p\u003e\n            \u003c/div\u003e\n            \u003c/div\u003e\n\n\n\n\u003ch2\u003eResults: reproducible, scalable, insight-driven agent development\u003c/h2\u003e\n\n\n\n\u003cp\u003eTeamCity has enabled Junie to measure agent performance efficiently and at scale, making their development process faster, more reliable, and data-driven.\u003c/p\u003e\n\n\n\n\u003ch3\u003eKey outcomes\u003c/h3\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eChallenge\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003eResult with TeamCity\u003c/strong\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eValidate agent changes at scale\u003c/td\u003e\u003ctd\u003e100+ tasks per run, reducing statistical noise\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eLong evaluation cycles (24+ hrs)\u003c/td\u003e\u003ctd\u003eTasks run in parallel – now completed in a manageable window\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eInconsistent local testing\u003c/td\u003e\u003ctd\u003eEvery run is reproducible and traceable to the exact agent and dataset\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eExpensive LLM usage\u003c/td\u003e\u003ctd\u003ePer-task usage is tracked, helping optimize development and costs\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eFragile logging and data loss\u003c/td\u003e\u003ctd\u003eLogs and outcomes are automatically stored for later debugging and review\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003ch3\u003eNeed to scale your AI workflows?\u003c/h3\u003e\n\n\n\n\u003cp\u003eTeamCity gives you the infrastructure to evaluate and iterate with confidence. \u003ca href=\"https://www.jetbrains.com/teamcity/download/\" data-type=\"link\" data-id=\"https://www.jetbrains.com/teamcity/download/\" target=\"_blank\" rel=\"noopener\"\u003eStart your free trial\u003c/a\u003e or \u003ca href=\"https://www.jetbrains.com/teamcity/get-in-touch/\" data-type=\"link\" data-id=\"https://www.jetbrains.com/teamcity/get-in-touch/\" target=\"_blank\" rel=\"noopener\"\u003erequest a demo\u003c/a\u003e.\u003c/p\u003e\n                    \n                                                                                                                                                                                                                            \u003cdiv\u003e\n                                \u003cdiv\u003e\n                                                                            \u003ch4\u003eSubscribe to TeamCity Blog updates\u003c/h4\u003e\n                                                                                                            \n                                \u003c/div\u003e\n                                \n                                \u003cp\u003e\u003cimg src=\"https://blog.jetbrains.com/wp-content/themes/jetbrains/assets/img/img-form.svg\" alt=\"image description\"/\u003e\n                                                                    \u003c/p\u003e\n                            \u003c/div\u003e\n                                                            \u003c/div\u003e\n                \u003ca href=\"#\"\u003e\u003c/a\u003e\n                \n                \n            \u003c/section\u003e\n                    \u003cdiv\u003e\n                \u003cp\u003e\n                    \u003ch2\u003eDiscover more\u003c/h2\u003e\n                \u003c/p\u003e\n                \n            \u003c/div\u003e\n                \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": null,
  "modifiedTime": null
}
