{
  "id": "71cdc371-c717-498c-81f6-807049387332",
  "title": "How SREs and GenAI Work Together to Decrease eBay's Downtime: An Architect's Insights at KubeCon EU",
  "link": "https://www.infoq.com/news/2025/04/llm-incident-response/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "During his KubeCon EU keynote, Vijay Samuel, Principal MTS Architect at eBay, shared his team’s experience of enhancing incident response capabilities by incorporating ML and LLM building blocks. They realised that GenAIs are not a silver bullet but can help engineers through complex incident investigations through logs, traces, and dashboard explanations. By Olimpiu Pop",
  "author": "Olimpiu Pop",
  "published": "Sat, 05 Apr 2025 09:09:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Incident Response",
    "Machine Learning",
    "Large language models",
    "Development",
    "DevOps",
    "news"
  ],
  "byline": "Olimpiu Pop",
  "length": 4500,
  "excerpt": "During his KubeCon EU keynote, Vijay Samuel, Principal MTS Architect at eBay, shared his team’s experience of enhancing incident response capabilities by incorporating ML and LLM building blocks. They",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250328105021-1/apple-touch-icon.png",
  "text": "During his KubeCon EU keynote, Vijay Samuel, Principal MTS Architect at eBay, shared his team’s experience of enhancing the incident response capabilities of the SRE teams by incorporating ML and LLMs building blocks. Through their journey, they realised that LLMs are not a silver bullet but can help engineers in complex scenarios, primarily if multiple capabilities are composed. His introductory part framed the complexity of today’s platforms based on eBay’s infrastructure growth experience in the last five years. Their platform increased consistently, reaching more than four thousand microservices. The data generated by those translates into 15 petabytes of logs, 10 billion active time series per day, and 10 million spans per second (2% sample). Samuel: How many dashboards should I look at before formulating a hypothesis? Incident manual triage based solely on human capabilities was cumbersome and error-prone at this scale as it mainly involved trial and error. Samuel and his team experimented with different machine-learning approaches to enhance the process. The first step in moving away from static threshold-based alerts was Groot. This system attached a root cause to every alert that impacts a business KPI and even had auto-remediation capabilities for minor issues. Its anomaly detection algorithm decreased the time to detect incidents to under four minutes. As the system was learning from what he had experienced already, it failed whenever a new type of incident occurred. Samuel and his team tried to embrace the LLM promise but quickly learned first-hand what hallucinations mean when trying to rewrite the Prometheus postings index using ChatGPT. He learned from his experiments that interacting with LLMs involves a lot of randomness whenever the context is broad (\"triage the site\" or \"triage the alert\"). Still, the output becomes more accurate if you prompt very deterministically with a very \"crisp\" context. After their experiments, they concluded they could use LLMs' capabilities for small amounts of information to assist them in their investigations. \"Explainers\" are small tools that focus on small amounts of information to provide more context while investigating an incident. They have capabilities that provide trace, log metric, or change explanations, all building blocks for the observability mechanisms. Even though their progress convinced them that AI and engineers are complementary, the LLM's limitations and misses were still massive for more significant information. Given the complexity of the ecosystem, they needed to divide and conquer the problem. Initially, they just extracted the critical path, inspired by Uber’s CRISP whitepaper, ensuring that all less important spans were removed. Afterwards, through \"few shot prompting,\" they provided examples of how SRE would normally triage those incidents to allow the algorithm to learn. Further, to work with the context window limitations, they \"dictionary encoded\" everything and split the critical path into \"upstream and downstream\" chunks. Based on these, they generated partial explanations for each of those and combined them into a full explanation. The full explanation(using their basic building blocks, the explainers) allowed them to understand whether there were multiple performance issues. They were composed into more complex evaluation mechanisms using each capability as a building block. Simple explainers were aggregated to analyse and explain more complex dashboards, and further triage workflows were built to analyse multiple dashboards. Also, alerts incorporated faulty traces whenever available to provide as much context as possible. Samuel concludes his presentation by mentioning the potential evolutions of the system and what they learned from building it. For instance, adding all the metric metadata in a vector database would allow them to gain more insights, such as \"how many searches failed\" in a given period or \"why was an SLO violated?\" However, they also realised that LLMs are not a silver bullet but should be used for their strengths: simple reasoning, summarisation, internal knowledge search and even code generation at points. He also hints that broad OpenTelemetry adoption and query language standardisation would help make straightforward assumptions across the industry. About the Author Olimpiu Pop",
  "image": "https://cdn.infoq.com/statics_s1_20250328105021-1/styles/static/images/logo/logo-big.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eDuring his KubeCon EU keynote, \u003ca href=\"https://www.linkedin.com/in/vjsamuel/\"\u003eVijay Samuel\u003c/a\u003e, Principal MTS Architect at eBay, shared his team’s experience of enhancing the incident response capabilities of the SRE teams by incorporating ML and LLMs building blocks. Through their journey, they realised that LLMs are not a silver bullet but can help engineers in complex scenarios, primarily if multiple capabilities are composed.\u003c/p\u003e\n\n\u003cp\u003eHis introductory part framed the complexity of today’s platforms based on eBay’s infrastructure growth experience in the last five years. Their platform increased consistently, reaching more than four thousand microservices. The data generated by those translates into 15 petabytes of logs, 10 billion active time series per day, and 10 million spans per second (2% sample).\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eSamuel\u003c/strong\u003e: How many dashboards should I look at before formulating a hypothesis?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIncident manual triage based solely on human capabilities was cumbersome and error-prone at this scale as it mainly involved trial and error. Samuel and his team experimented with different machine-learning approaches to enhance the process. The first step in moving away from static threshold-based alerts was \u003ca href=\"https://innovation.ebayinc.com/stories/groot-ebays-event-graph-based-approach-for-root-cause-analysis/\"\u003eGroot\u003c/a\u003e. This system attached a root cause to every alert that impacts a business KPI and even had auto-remediation capabilities for minor issues. Its anomaly detection algorithm decreased the time to detect incidents to under four minutes. As the system was learning from what he had experienced already, it failed whenever a new type of incident occurred.\u003c/p\u003e\n\n\u003cp\u003eSamuel and his team tried to embrace the LLM promise but quickly learned first-hand what hallucinations mean when trying to rewrite the Prometheus postings index using ChatGPT. He learned from his experiments that interacting with LLMs involves a lot of randomness whenever the context is broad (\u0026#34;triage the site\u0026#34; or \u0026#34;triage the alert\u0026#34;). Still, the output becomes more accurate if you prompt very deterministically with a very \u0026#34;crisp\u0026#34; context.\u003c/p\u003e\n\n\u003cp\u003eAfter their experiments, they concluded they could use LLMs\u0026#39; capabilities for small amounts of information to assist them in their investigations. \u0026#34;Explainers\u0026#34; are small tools that focus on small amounts of information to provide more context while investigating an incident. They have capabilities that provide trace, log metric, or change explanations, all building blocks for the observability mechanisms.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/04/llm-incident-response/en/resources/11image2-1743842731800.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/04/llm-incident-response/en/resources/11image2-1743842731800.png\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eEven though their progress convinced them that AI and engineers are complementary, the LLM\u0026#39;s limitations and misses were still massive for more significant information. Given the complexity of the ecosystem, they needed to divide and conquer the problem. Initially, they just extracted the critical path, inspired by \u003ca href=\"https://www.infoq.com/news/2021/12/microservice-analysis-crisp/\"\u003eUber’s CRISP whitepaper\u003c/a\u003e, ensuring that all less important spans were removed. Afterwards, through \u0026#34;few shot prompting,\u0026#34; they provided examples of how SRE would normally triage those incidents to allow the algorithm to learn.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/04/llm-incident-response/en/resources/11image1-1743842731800.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/04/llm-incident-response/en/resources/11image1-1743842731800.png\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eFurther, to work with the context window limitations, they \u0026#34;dictionary encoded\u0026#34; everything and split the critical path into \u0026#34;upstream and downstream\u0026#34; chunks. Based on these, they generated partial explanations for each of those and combined them into a full explanation. The full explanation(using their basic building blocks, the explainers) allowed them to understand whether there were multiple performance issues.\u003c/p\u003e\n\n\u003cp\u003eThey were composed into more complex evaluation mechanisms using each capability as a building block. Simple explainers were aggregated to analyse and explain more complex dashboards, and further triage workflows were built to analyse multiple dashboards. Also, alerts incorporated faulty traces whenever available to provide as much context as possible.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/04/llm-incident-response/en/resources/9image3-1743842731800.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/04/llm-incident-response/en/resources/9image3-1743842731800.png\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eSamuel concludes his presentation by mentioning the potential evolutions of the system and what they learned from building it. For instance, adding all the metric metadata in a vector database would allow them to gain more insights, such as \u0026#34;how many searches failed\u0026#34; in a given period or \u0026#34;why was an SLO violated?\u0026#34; However, they also realised that LLMs are not a silver bullet but should be used for their strengths: simple reasoning, summarisation, internal knowledge search and even code generation at points. He also hints that broad OpenTelemetry adoption and query language standardisation would help make straightforward assumptions across the industry.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Olimpiu-Pop\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eOlimpiu Pop\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-04-05T00:00:00Z",
  "modifiedTime": null
}
