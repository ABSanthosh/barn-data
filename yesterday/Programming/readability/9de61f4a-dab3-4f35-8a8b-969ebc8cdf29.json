{
  "id": "9de61f4a-dab3-4f35-8a8b-969ebc8cdf29",
  "title": "Google Publishes LLM Self-Correction Algorithm SCoRe",
  "link": "https://www.infoq.com/news/2024/10/google-deepmind-score/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Researchers at Google DeepMind recently published a paper on Self-Correction via Reinforcement Learning (SCoRe), a technique for improving LLMs' ability to self-correct when solving math or coding problems. Models fine-tuned with SCoRe achieve improved performance on several benchmarks compared to baseline models. By Anthony Alford",
  "author": "Anthony Alford",
  "published": "Tue, 15 Oct 2024 13:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Generative AI",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Anthony Alford",
  "length": 3579,
  "excerpt": "Researchers at Google DeepMind recently published a paper on Self-Correction via Reinforcement Learning (SCoRe), a technique for improving LLMs' ability to self-correct when solving math or coding pro",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20241001113528/apple-touch-icon.png",
  "text": "Researchers at Google DeepMind recently published a paper on Self-Correction via Reinforcement Learning (SCoRe), a technique for improving LLMs' ability to self-correct when solving math or coding problems. Models fine-tuned with SCoRe achieve improved performance on several benchmarks compared to baseline models.  Unlike previous self-correction methods that rely on prompt engineering or separate \"teacher\" models, SCoRe uses data generated by the LLM itself to generate self-correction traces: synthetic dialogues where the LLM gives an incorrect response, followed by a correction prompt, followed by the LLM giving a correct response. This data is used in a two-stage RL process to fine-tune the LLM. When evaluated against baseline Gemini 1.0 models, the fine-tuned LLM improved by 15.6 percentage points on the MATH benchmark and 9.1 percentage points on HumanEval. According to Google,  The importance of our two-stage recipe (based on careful initialization and reward shaping) in obtaining positive self-correction perhaps more generally hints that some kind of regularization is required to ensure that LLMs learn nuanced strategies that can generalize well to novel, unseen queries at test-time. The DeepMind team developed SCoRe after studying the shortcomings of other methods. They said that \"there is no major work\" that shows that prompt engineering alone can result in successful self-correction in off-the-shelf models. Attempting to improve the model with supervised fine-tuning (SFT) typically requires a human or a stronger LLM to provide corrections. Methods which use SFT on self-generated corrections \"often [amplify] the model’s bias\" to not make corrections, or else \"suffer from the curse of distributional shift.\" SCoRe Training Stages. Image source: Google DeepMind Research Paper SCoRe improves on previous methods by using a two-stage RL process. In the first stage, the model is trained to keep its initial response the same but generate a correct response on the second attempt. In the second stage, the model is rewarded for correct answers in both responses, with a bonus reward for an improved second response. The goal is to prevent the model from learning to \"produce the best first-attempt response and only minorly edit it.\" In a discussion about SCoRe on Reddit, one user wrote: Overall, it's interesting that it's taught how to make corrections. But I would have liked to see the 2rd, 4th, 5th turns of a few examples to see what improvements the test runs are producing. Informally, it reads like the 2nd turn can make a big difference, but the subsequent turns have diminishing returns.  Users in a Hacker News discussion compared SCoRe to OpenAI's method of fine-tuning their Omni models: OpenAI stated that one of the breakthroughs needed for o1's train of thought to work was reinforcement learning to teach it to recover from faulty reasoning. [It's] incredibly similar to this paper, which discusses the difficulty in finding a training method that guides the model to learn a self-correcting technique (in which subsequent attempts learn from and improve on previous attempts), instead of just \"collapsing\" into a mode of trying to get the answer right with the very first try. InfoQ covered OpenAI's release of their Omni model earlier this year. InfoQ also covered OpenAI's use of an LLM to generate training data to improve code generated by ChatGPT. About the Author Anthony Alford",
  "image": "https://res.infoq.com/news/2024/10/google-deepmind-score/en/headerimage/generatedHeaderImage-1728140476921.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eResearchers at \u003ca href=\"https://deepmind.google/\"\u003eGoogle DeepMind\u003c/a\u003e recently published a paper on \u003ca href=\"https://arxiv.org/abs/2409.12917\"\u003eSelf-Correction via Reinforcement Learning\u003c/a\u003e (SCoRe), a technique for improving LLMs\u0026#39; ability to self-correct when solving math or coding problems. Models fine-tuned with SCoRe achieve improved performance on several benchmarks compared to baseline models. \u003c/p\u003e\n\n\u003cp\u003eUnlike previous self-correction methods that rely on prompt engineering or separate \u0026#34;teacher\u0026#34; models, SCoRe uses data generated by the LLM itself to generate self-correction \u003cem\u003etraces\u003c/em\u003e: synthetic dialogues where the LLM gives an incorrect response, followed by a correction prompt, followed by the LLM giving a correct response. This data is used in a two-stage RL process to fine-tune the LLM. When evaluated against baseline Gemini 1.0 models, the fine-tuned LLM improved by 15.6 percentage points on the \u003ca href=\"http://paperswithcode.com/sota/math-word-problem-solving-on-math\"\u003eMATH\u003c/a\u003e benchmark and 9.1 percentage points on \u003ca href=\"https://paperswithcode.com/sota/code-generation-on-humaneval\"\u003eHumanEval\u003c/a\u003e. According to Google, \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe importance of our two-stage recipe (based on careful initialization and reward shaping) in obtaining positive self-correction perhaps more generally hints that some kind of regularization is required to ensure that LLMs learn nuanced strategies that can generalize well to novel, unseen queries at test-time.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe DeepMind team developed SCoRe after studying the shortcomings of other methods. They said that \u0026#34;there is no major work\u0026#34; that shows that prompt engineering alone can result in successful self-correction in off-the-shelf models. Attempting to improve the model with supervised fine-tuning (SFT) typically requires a human or a stronger LLM to provide corrections. Methods which use SFT on self-generated corrections \u0026#34;often [amplify] the model’s bias\u0026#34; to not make corrections, or else \u0026#34;suffer from the curse of distributional shift.\u0026#34;\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"SCoRe Training Stages\" data-src=\"news/2024/10/google-deepmind-score/en/resources/1score-stages-1728140678532.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/10/google-deepmind-score/en/resources/1score-stages-1728140678532.png\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eSCoRe Training Stages. Image source: \u003ca href=\"https://arxiv.org/abs/2409.12917\"\u003eGoogle DeepMind Research Paper\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eSCoRe improves on previous methods by using a two-stage RL process. In the first stage, the model is trained to keep its initial response the same but generate a correct response on the second attempt. In the second stage, the model is rewarded for correct answers in both responses, with a bonus reward for an improved second response. The goal is to prevent the model from learning to \u0026#34;produce the best first-attempt response and only minorly edit it.\u0026#34;\u003c/p\u003e\n\n\u003cp\u003eIn a \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1fo6bdg/google_has_released_a_new_paper_training_language/\"\u003ediscussion about SCoRe\u003c/a\u003e on Reddit, one user wrote:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eOverall, it\u0026#39;s interesting that it\u0026#39;s taught how to make corrections. But I would have liked to see the 2rd, 4th, 5th turns of a few examples to see what improvements the test runs are producing. Informally, it reads like the 2nd turn can make a big difference, but the subsequent turns have diminishing returns. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eUsers in a Hacker News discussion \u003ca href=\"https://news.ycombinator.com/item?id=41600179\"\u003ecompared SCoRe to OpenAI\u0026#39;s method\u003c/a\u003e of fine-tuning their Omni models:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://openai.com/index/learning-to-reason-with-llms/\"\u003eOpenAI stated\u003c/a\u003e that one of the breakthroughs needed for o1\u0026#39;s train of thought to work was reinforcement learning to teach it to recover from faulty reasoning. [It\u0026#39;s] incredibly similar to this paper, which discusses the difficulty in finding a training method that guides the model to learn a self-correcting technique (in which subsequent attempts learn from and improve on previous attempts), instead of just \u0026#34;collapsing\u0026#34; into a mode of trying to get the answer right with the very first try.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eInfoQ covered OpenAI\u0026#39;s release of their \u003ca href=\"https://www.infoq.com/news/2024/05/openai-gpt4o/\"\u003eOmni model\u003c/a\u003e earlier this year. InfoQ also covered \u003ca href=\"https://www.infoq.com/news/2024/07/openai-criticgpt/\"\u003eOpenAI\u0026#39;s use of an LLM\u003c/a\u003e to generate training data to improve code generated by ChatGPT.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Anthony-Alford\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAnthony Alford\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-10-15T00:00:00Z",
  "modifiedTime": null
}
