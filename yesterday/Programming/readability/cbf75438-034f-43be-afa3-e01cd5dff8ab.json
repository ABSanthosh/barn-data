{
  "id": "cbf75438-034f-43be-afa3-e01cd5dff8ab",
  "title": "Gemma 3 QAT Models: Bringing state-of-the-Art AI to consumer GPUs",
  "link": "https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/",
  "description": "The release of int4 quantized versions of Gemma 3 models, optimized with Quantization Aware Training (QAT) brings significantly reduced memory requirements, allowing users to run powerful models like Gemma 3 27B on consumer-grade GPUs such as the NVIDIA RTX 3090.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Edouard YVINEC, Phil Culliton",
  "length": 6911,
  "excerpt": "The release of int4 quantized versions of Gemma 3 models, optimized with Quantization Aware Training (QAT) brings significantly reduced memory requirements, allowing users to run powerful models like Gemma 3 27B on consumer-grade GPUs such as the NVIDIA RTX 3090.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "Last month, we launched Gemma 3, our latest generation of open models. Delivering state-of-the-art performance, Gemma 3 quickly established itself as a leading model capable of running on a single high-end GPU like the NVIDIA H100 using its native BFloat16 (BF16) precision.To make Gemma 3 even more accessible, we are announcing new versions optimized with Quantization-Aware Training (QAT) that dramatically reduces memory requirements while maintaining high quality. This enables you to run powerful models like Gemma 3 27B locally on consumer-grade GPUs like the NVIDIA RTX 3090. This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Dots show estimated NVIDIA H100 GPU requirements. Understanding performance, precision, and quantizationThe chart above shows the performance (Elo score) of recently released large language models. Higher bars mean better performance in comparisons as rated by humans viewing side-by-side responses from two anonymous models. Below each bar, we indicate the estimated number of NVIDIA H100 GPUs needed to run that model using the BF16 data type.Why BFloat16 for this comparison? BF16 is a common numerical format used during inference of many large models. It means that the model parameters are represented with 16 bits of precision. Using BF16 for all models helps us to make an apples-to-apples comparison of models in a common inference setup. This allows us to compare the inherent capabilities of the models themselves, removing variables like different hardware or optimization techniques like quantization, which we'll discuss next.It's important to note that while this chart uses BF16 for a fair comparison, deploying the very largest models often involves using lower-precision formats like FP8 as a practical necessity to reduce immense hardware requirements (like the number of GPUs), potentially accepting a performance trade-off for feasibility.The Need for AccessibilityWhile top performance on high-end hardware is great for cloud deployments and research, we heard you loud and clear: you want the power of Gemma 3 on the hardware you already own. We're committed to making powerful AI accessible, and that means enabling efficient performance on the consumer-grade GPUs found in desktops, laptops, and even phones.Performance Meets Accessibility with Quantization-Aware Training in Gemma 3This is where quantization comes in. In AI models, quantization reduces the precision of the numbers (the model's parameters) it stores and uses to calculate responses. Think of quantization like compressing an image by reducing the number of colors it uses. Instead of using 16 bits per number (BFloat16), we can use fewer bits, like 8 (int8) or even 4 (int4).Using int4 means each number is represented using only 4 bits – a 4x reduction in data size compared to BF16. Quantization can often lead to performance degradation, so we’re excited to release Gemma 3 models that are robust to quantization. We released several quantized variants for each Gemma 3 model to enable inference with your favorite inference engine, such as Q4_0 (a common quantization format) for Ollama, llama.cpp, and MLX.How do we maintain quality? We use QAT. Instead of just quantizing the model after it's fully trained, QAT incorporates the quantization process during training. QAT simulates low-precision operations during training to allow quantization with less degradation afterwards for smaller, faster models while maintaining accuracy. Diving deeper, we applied QAT on ~5,000 steps using probabilities from the non-quantized checkpoint as targets. We reduce the perplexity drop by 54% (using llama.cpp perplexity evaluation) when quantizing down to Q4_0.See the Difference: Massive VRAM SavingsThe impact of int4 quantization is dramatic. Look at the VRAM (GPU memory) required just to load the model weights:Gemma 3 27B: Drops from 54 GB (BF16) to just 14.1 GB (int4)Gemma 3 12B: Shrinks from 24 GB (BF16) to only 6.6 GB (int4)Gemma 3 4B: Reduces from 8 GB (BF16) to a lean 2.6 GB (int4)Gemma 3 1B: Goes from 2 GB (BF16) down to a tiny 0.5 GB (int4) Note: This figure only represents the VRAM required to load the model weights. Running the model also requires additional VRAM for the KV cache, which stores information about the ongoing conversation and depends on the context lengthRun Gemma 3 on Your DeviceThese dramatic reductions unlock the ability to run larger, powerful models on widely available consumer hardware:Gemma 3 27B (int4): Now fits comfortably on a single desktop NVIDIA RTX 3090 (24GB VRAM) or similar card, allowing you to run our largest Gemma 3 variant locally.Gemma 3 12B (int4): Runs efficiently on laptop GPUs like the NVIDIA RTX 4060 Laptop GPU (8GB VRAM), bringing powerful AI capabilities to portable machines.Smaller Models (4B, 1B): Offer even greater accessibility for systems with more constrained resources, including phones and toasters (if you have a good one).Easy Integration with Popular ToolsWe want you to be able to use these models easily within your preferred workflow. Our official int4 and Q4_0 unquantized QAT models are available on Hugging Face and Kaggle. We’ve partnered with popular developer tools that enable seamlessly trying out the QAT-based quantized checkpoints:Ollama: Get running quickly – all our Gemma 3 QAT models are natively supported starting today with a simple command.LM Studio: Easily download and run Gemma 3 QAT models on your desktop via its user-friendly interface.MLX: Leverage MLX for efficient, optimized inference of Gemma 3 QAT models on Apple Silicon.Gemma.cpp: Use our dedicated C++ implementation for highly efficient inference directly on the CPU.llama.cpp: Integrate easily into existing workflows thanks to native support for our GGUF-formatted QAT models.More Quantizations in the GemmaverseOur official Quantization Aware Trained (QAT) models provide a high-quality baseline, but the vibrant Gemmaverse offers many alternatives. These often use Post-Training Quantization (PTQ), with significant contributions from members such as Bartowski, Unsloth, and GGML readily available on Hugging Face. Exploring these community options provides a wider spectrum of size, speed, and quality trade-offs to fit specific needs.Get Started TodayBringing state-of-the-art AI performance to accessible hardware is a key step in democratizing AI development. With Gemma 3 models, optimized through QAT, you can now leverage cutting-edge capabilities on your own desktop or laptop.Explore the quantized models and start building:Use on your PC with OllamaFind the Models on Hugging Face \u0026 KaggleRun on your phone with Google AI EdgeWe can't wait to see what you build with Gemma 3 running locally!",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3_Quantized-meta.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"sisye\"\u003eLast month, we launched Gemma 3, our latest generation of open models. Delivering state-of-the-art performance, Gemma 3 quickly established itself as a leading model capable of running on a single high-end GPU like the NVIDIA H100 using its native BFloat16 (BF16) precision.\u003c/p\u003e\u003cp data-block-key=\"4impe\"\u003eTo make Gemma 3 even more accessible, we are announcing new versions optimized with Quantization-Aware Training (QAT) that dramatically reduces memory requirements while maintaining high quality. This enables you to run powerful models like Gemma 3 27B locally on consumer-grade GPUs like the NVIDIA RTX 3090.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3QuantizedChart_RD1_01_1_qlGnyVc.original.png\" alt=\"Chatbot Arena Elo Score - Gemma 3 QAT\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Dots show estimated NVIDIA H100 GPU requirements.\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"sisye\"\u003eUnderstanding performance, precision, and quantization\u003c/h2\u003e\u003cp data-block-key=\"2a2jr\"\u003eThe chart above shows the performance (Elo score) of recently released large language models. Higher bars mean better performance in comparisons as rated by humans viewing side-by-side responses from two anonymous models. Below each bar, we indicate the estimated number of NVIDIA H100 GPUs needed to run that model using the BF16 data type.\u003c/p\u003e\u003cp data-block-key=\"ae0co\"\u003e\u003cb\u003e\u003cbr/\u003eWhy BFloat16 for this comparison?\u003c/b\u003e BF16 is a common numerical format used during inference of many large models. It means that the model parameters are represented with 16 bits of precision. Using BF16 for all models helps us to make an apples-to-apples comparison of models in a common inference setup. This allows us to compare the inherent capabilities of the models themselves, removing variables like different hardware or optimization techniques like quantization, which we\u0026#39;ll discuss next.\u003c/p\u003e\u003cp data-block-key=\"3q45i\"\u003eIt\u0026#39;s important to note that while this chart uses BF16 for a fair comparison, deploying the very largest models often involves using lower-precision formats like FP8 as a practical necessity to reduce immense hardware requirements (like the number of GPUs), potentially accepting a performance trade-off for feasibility.\u003c/p\u003e\u003ch2 data-block-key=\"8r8jc\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThe Need for Accessibility\u003c/h2\u003e\u003cp data-block-key=\"dkjr0\"\u003eWhile top performance on high-end hardware is great for cloud deployments and research, we heard you loud and clear: you want the power of Gemma 3 on the hardware you already own. We\u0026#39;re committed to making powerful AI accessible, and that means enabling efficient performance on the consumer-grade GPUs found in desktops, laptops, and even phones.\u003c/p\u003e\u003ch2 data-block-key=\"950c\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003ePerformance Meets Accessibility with Quantization-Aware Training in Gemma 3\u003c/h2\u003e\u003cp data-block-key=\"158md\"\u003eThis is where quantization comes in. In AI models, quantization reduces the precision of the numbers (the model\u0026#39;s parameters) it stores and uses to calculate responses. Think of quantization like compressing an image by reducing the number of colors it uses. Instead of using 16 bits per number (BFloat16), we can use fewer bits, like 8 (int8) or even 4 (int4).\u003c/p\u003e\u003cp data-block-key=\"3a7o0\"\u003eUsing int4 means each number is represented using only 4 bits – a 4x reduction in data size compared to BF16. Quantization can often lead to performance degradation, so we’re excited to release Gemma 3 models that are robust to quantization. We released several quantized variants for each Gemma 3 model to enable inference with your favorite inference engine, such as Q4_0 (a common quantization format) for Ollama, llama.cpp, and MLX.\u003c/p\u003e\u003cp data-block-key=\"epsbi\"\u003e\u003cb\u003e\u003cbr/\u003eHow do we maintain quality?\u003c/b\u003e We use QAT. Instead of just quantizing the model after it\u0026#39;s fully trained, QAT incorporates the quantization process during training. QAT simulates low-precision operations during training to allow quantization with less degradation afterwards for smaller, faster models while maintaining accuracy. Diving deeper, we applied QAT on ~5,000 steps using probabilities from the non-quantized checkpoint as targets. We reduce the perplexity drop by 54% (using llama.cpp perplexity evaluation) when quantizing down to Q4_0.\u003c/p\u003e\u003ch2 data-block-key=\"eatdo\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eSee the Difference: Massive VRAM Savings\u003c/h2\u003e\u003cp data-block-key=\"9f9so\"\u003eThe impact of int4 quantization is dramatic. Look at the VRAM (GPU memory) required just to load the model weights:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"41cf1\"\u003e\u003cb\u003eGemma 3 27B:\u003c/b\u003e Drops from 54 GB (BF16) to just \u003cb\u003e14.1 GB\u003c/b\u003e (int4)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"5uebr\"\u003e\u003cb\u003eGemma 3 12B:\u003c/b\u003e Shrinks from 24 GB (BF16) to only \u003cb\u003e6.6 GB\u003c/b\u003e (int4)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"9d985\"\u003e\u003cb\u003eGemma 3 4B:\u003c/b\u003e Reduces from 8 GB (BF16) to a lean \u003cb\u003e2.6 GB\u003c/b\u003e (int4)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"bf7p\"\u003e\u003cb\u003eGemma 3 1B:\u003c/b\u003e Goes from 2 GB (BF16) down to a tiny \u003cb\u003e0.5 GB\u003c/b\u003e (int4)\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3QuantizedChart_RD1_02.original.png\" alt=\"Comparison chart of model weights showing VRAM required to load\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cblockquote data-block-key=\"7ui9z\"\u003e\u003cb\u003e\u003csup\u003eNote:\u003c/sup\u003e\u003c/b\u003e \u003ci\u003e\u003csup\u003eThis figure only represents the VRAM required to load the model weights. Running the model also requires additional VRAM for the KV cache, which stores information about the ongoing conversation and depends on the context length\u003c/sup\u003e\u003c/i\u003e\u003c/blockquote\u003e\u003ch2 data-block-key=\"e1hmg\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eRun Gemma 3 on Your Device\u003c/h2\u003e\u003cp data-block-key=\"4jdc3\"\u003eThese dramatic reductions unlock the ability to run larger, powerful models on widely available consumer hardware:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"bqsl5\"\u003e\u003cb\u003eGemma 3 27B (int4):\u003c/b\u003e Now fits comfortably on a single desktop NVIDIA RTX 3090 (24GB VRAM) or similar card, allowing you to run our largest Gemma 3 variant locally.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"c4adc\"\u003e\u003cb\u003eGemma 3 12B (int4):\u003c/b\u003e Runs efficiently on laptop GPUs like the NVIDIA RTX 4060 Laptop GPU (8GB VRAM), bringing powerful AI capabilities to portable machines.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"be3pr\"\u003e\u003cb\u003eSmaller Models (4B, 1B):\u003c/b\u003e Offer even greater accessibility for systems with more constrained resources, including phones and \u003ca href=\"https://youtu.be/lgsD_wSZ0hI?si=pyQj23bOxNPLrxtL\u0026amp;t=102\"\u003etoasters\u003c/a\u003e (if you have a good one).\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"7c8ds\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eEasy Integration with Popular Tools\u003c/h2\u003e\u003cp data-block-key=\"2324e\"\u003eWe want you to be able to use these models easily within your preferred workflow. Our official int4 and Q4_0 unquantized QAT models are available on Hugging Face and Kaggle. We’ve partnered with popular developer tools that enable seamlessly trying out the QAT-based quantized checkpoints:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"avkt\"\u003e\u003ca href=\"https://ollama.com/library/gemma3\"\u003e\u003cb\u003eOllama\u003c/b\u003e\u003c/a\u003e\u003cb\u003e:\u003c/b\u003e Get running quickly – all our Gemma 3 QAT models are natively supported starting today with a simple command.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"6sp3o\"\u003e\u003ca href=\"https://lmstudio.ai/model/gemma-3-12b-it-qat\"\u003e\u003cb\u003eLM Studio\u003c/b\u003e\u003c/a\u003e\u003cb\u003e:\u003c/b\u003e Easily download and run Gemma 3 QAT models on your desktop via its user-friendly interface.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"69oug\"\u003e\u003ca href=\"https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae\"\u003e\u003cb\u003eMLX\u003c/b\u003e\u003c/a\u003e\u003cb\u003e:\u003c/b\u003e Leverage MLX for efficient, optimized inference of Gemma 3 QAT models on Apple Silicon.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"19s6q\"\u003e\u003ca href=\"https://www.kaggle.com/models/google/gemma-3/gemmaCpp\"\u003e\u003cb\u003eGemma.cpp\u003c/b\u003e\u003c/a\u003e\u003cb\u003e:\u003c/b\u003e Use our dedicated C++ implementation for highly efficient inference directly on the CPU.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"f05gl\"\u003e\u003ca href=\"https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b\"\u003e\u003cb\u003ellama.cpp\u003c/b\u003e\u003c/a\u003e\u003cb\u003e:\u003c/b\u003e Integrate easily into existing workflows thanks to native support for our GGUF-formatted QAT models.\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"c5073\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eMore Quantizations in the Gemmaverse\u003c/h2\u003e\u003cp data-block-key=\"74vnv\"\u003eOur official Quantization Aware Trained (QAT) models provide a high-quality baseline, but the vibrant \u003ca href=\"https://ai.google.dev/gemma/gemmaverse\"\u003eGemmaverse\u003c/a\u003e offers many alternatives. These often use Post-Training Quantization (PTQ), with significant contributions from members such as \u003ca href=\"https://huggingface.co/bartowski/google_gemma-3-27b-it-GGUF\"\u003eBartowski\u003c/a\u003e, \u003ca href=\"https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b\"\u003eUnsloth\u003c/a\u003e, and \u003ca href=\"https://huggingface.co/collections/ggml-org/gemma-3-67d126315ac810df1ad9e913\"\u003eGGML\u003c/a\u003e readily available on Hugging Face. Exploring these community options provides a wider spectrum of size, speed, and quality trade-offs to fit specific needs.\u003c/p\u003e\u003ch2 data-block-key=\"fnsl0\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eGet Started Today\u003c/h2\u003e\u003cp data-block-key=\"fdu54\"\u003eBringing state-of-the-art AI performance to accessible hardware is a key step in democratizing AI development. With Gemma 3 models, optimized through QAT, you can now leverage cutting-edge capabilities on your own desktop or laptop.\u003c/p\u003e\u003cp data-block-key=\"179dk\"\u003eExplore the quantized models and start building:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"6uj07\"\u003eUse on your PC with \u003ca href=\"https://ollama.com/library/gemma3\"\u003eOllama\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"741fu\"\u003eFind the Models on \u003ca href=\"https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b\"\u003eHugging Face\u003c/a\u003e \u0026amp; \u003ca href=\"https://www.kaggle.com/models/google/gemma-3/transformers\"\u003eKaggle\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"6069i\"\u003eRun on your phone with \u003ca href=\"https://developers.googleblog.com/en/gemma-3-on-mobile-and-web-with-google-ai-edge/\"\u003eGoogle AI Edge\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"4j3l0\"\u003eWe can\u0026#39;t wait to see what you build with Gemma 3 running locally!\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-04-18T00:00:00Z",
  "modifiedTime": null
}
