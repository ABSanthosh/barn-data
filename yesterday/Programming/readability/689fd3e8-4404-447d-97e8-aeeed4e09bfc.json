{
  "id": "689fd3e8-4404-447d-97e8-aeeed4e09bfc",
  "title": "University Researchers Publish Analysis of Chain-of-Thought Reasoning in LLMs",
  "link": "https://www.infoq.com/news/2024/10/cot-reasoning-llms/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Researchers from Princeton University and Yale University published a case study of Chain-of-Thought (CoT) reasoning in LLMs which shows evidence of both memorization and true reasoning. They also found that CoT can work even when examples given in the prompt are incorrect. By Anthony Alford",
  "author": "Anthony Alford",
  "published": "Tue, 22 Oct 2024 13:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Generative AI",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Anthony Alford",
  "length": 3731,
  "excerpt": "Researchers from Princeton University and Yale University published a case study of Chain-of-Thought (CoT) reasoning in LLMs which shows evidence of both memorization and true reasoning. They also fou",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20241022062452/apple-touch-icon.png",
  "text": "Researchers from Princeton University and Yale University published a case study of Chain-of-Thought (CoT) reasoning in LLMs which shows evidence of both memorization and true reasoning. They also found that CoT can work even when examples given in the prompt are incorrect. The study was motivated by the persistent debate in the research community about whether LLMs can truly reason, or if their outputs are simply based on heuristics and memorization. The team used a simple task, decoding shift ciphers, as their case study. They found that an LLM's performance using CoT prompting depended on a mix of both memorization and what the team described as \"noisy\" reasoning, as well as the overall probability of the correct output. According to the researchers: [W]e find evidence that the effect of CoT fundamentally depends on generating sequences of words that increase the probability of the correct answer when conditioned upon; as long as this is the case, CoT can thus succeed even when the demonstrations in the prompt are invalid. In the ongoing debate about whether LLMs reason or memorize, our results thus support a reasonable middle-ground: LLM behavior displays aspects of both memorization and reasoning, and also reflects the probabilistic origins of these models. The team chose the task of decoding shift ciphers because it has a \"sharp dissociation\" between its complexity and its frequency of use in the internet sources used to train LLMs. The task becomes more difficult the larger the shift value; however, the most difficult case is also the most commonly used on the internet: rot-13. If LLMs are simply memorizing, then they would perform better on rot-13 than if they are really using reasoning. By contrast, if they are truly reasoning, they would perform best on rot-1 and rot-25, and worst on rot-13. Expected and Actual Results. Image Source: Akshara Prabhakar The team created a dataset of 7-letter words which were also tokenized to exactly 2 tokens by GPT-4. They also calculated each word's probability of GPT-2 using it to complete the sentence \"The word is\". This allowed the researchers to control for how likely an LLM would output it simply based on probabilities. They then produced shifted versions of these words and ran their experiment GPT-4, Claude 3, and Llama-3.1-405B-Instruct. The team also performed an experiment where instead of words, the models were asked to decode sequences of numbers using arithmetic. This task is \"isomorphic\" to the shift-cipher task, but uses only numbers. The authors found that on this task GPT-4 performed \"nearly perfectly,\" and concluded that it \"has the core reasoning abilities\" needed to perform the shift-cipher task accurately for all shift values. Since it did not, they conclude that CoT \"is not pure symbolic reasoning.\" However, they do note that CoT improves performance vs. \"standard\" prompting, so CoT is not \"simple memorization.\" Research team member R. Thomas McCoy, a professor at Yale University, posted about the work on X. In response to a question from another user who wondered if different CoT prompts would give different results, he wrote: Yes, I think there is lots to explore there! [co-author Akshara Prabhakar] did have some cool experiments that involved translating from letters to numbers within the CoT. That generally improved performance, but got a qualitatively similar chart. So that's one case that is similar. But there could well be others that give different trends! The experimental code and data for the study are available on GitHub. About the Author Anthony Alford",
  "image": "https://res.infoq.com/news/2024/10/cot-reasoning-llms/en/headerimage/generatedHeaderImage-1729254502942.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eResearchers from \u003ca href=\"https://www.princeton.edu/\"\u003ePrinceton University\u003c/a\u003e and \u003ca href=\"https://www.yale.edu/\"\u003eYale University\u003c/a\u003e published a \u003ca href=\"https://arxiv.org/abs/2407.01687\"\u003ecase study of Chain-of-Thought\u003c/a\u003e (CoT) reasoning in LLMs which shows evidence of both memorization and true reasoning. They also found that CoT can work even when examples given in the prompt are incorrect.\u003c/p\u003e\n\n\u003cp\u003eThe study was motivated by the persistent debate in the research community about whether LLMs can truly reason, or if their outputs are simply based on heuristics and memorization. The team used a simple task, decoding shift ciphers, as their case study. They found that an LLM\u0026#39;s performance using CoT prompting depended on a mix of both memorization and what the team described as \u0026#34;noisy\u0026#34; reasoning, as well as the overall probability of the correct output. According to the researchers:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e[W]e find evidence that the effect of CoT fundamentally depends on generating sequences of words that increase the probability of the correct answer when conditioned upon; as long as this is the case, CoT can thus succeed even when the demonstrations in the prompt are invalid. In the ongoing debate about whether LLMs reason or memorize, our results thus support a reasonable middle-ground: LLM behavior displays aspects of both memorization and reasoning, and also reflects the probabilistic origins of these models.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe team chose the task of decoding shift ciphers because it has a \u0026#34;sharp dissociation\u0026#34; between its complexity and its frequency of use in the internet sources used to train LLMs. The task becomes more difficult the larger the shift value; however, the most difficult case is also the most commonly used on the internet: \u003ca href=\"https://en.wikipedia.org/wiki/ROT13\"\u003erot-13\u003c/a\u003e. If LLMs are simply memorizing, then they would perform better on rot-13 than if they are really using reasoning. By contrast, if they are truly reasoning, they would perform best on rot-1 and rot-25, and worst on rot-13.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"Expected and Actual Results\" data-src=\"news/2024/10/cot-reasoning-llms/en/resources/1experimental-results-1729254788624.jpeg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/10/cot-reasoning-llms/en/resources/1experimental-results-1729254788624.jpeg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eExpected and Actual Results. Image Source: \u003ca href=\"https://x.com/aksh_555/status/1843325405056622946\"\u003eAkshara Prabhakar\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThe team created a dataset of 7-letter words which were also tokenized to exactly 2 tokens by GPT-4. They also calculated each word\u0026#39;s probability of GPT-2 using it to complete the sentence \u0026#34;\u003cem\u003eThe word is\u003c/em\u003e\u0026#34;. This allowed the researchers to control for how likely an LLM would output it simply based on probabilities. They then produced shifted versions of these words and ran their experiment GPT-4, Claude 3, and \u003ca href=\"https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct\"\u003eLlama-3.1-405B-Instruct\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe team also performed an experiment where instead of words, the models were asked to decode sequences of numbers using arithmetic. This task is \u0026#34;isomorphic\u0026#34; to the shift-cipher task, but uses only numbers. The authors found that on this task GPT-4 performed \u0026#34;nearly perfectly,\u0026#34; and concluded that it \u0026#34;has the core reasoning abilities\u0026#34; needed to perform the shift-cipher task accurately for all shift values. Since it did not, they conclude that CoT \u0026#34;is not pure symbolic reasoning.\u0026#34; However, they do note that CoT improves performance vs. \u0026#34;standard\u0026#34; prompting, so CoT is not \u0026#34;simple memorization.\u0026#34;\u003c/p\u003e\n\n\u003cp\u003eResearch team member R. Thomas McCoy, a professor at Yale University, posted about the work on X. \u003ca href=\"https://x.com/RTomMcCoy/status/1843666441234690146\"\u003eIn response to a question\u003c/a\u003e from another user who wondered if different CoT prompts would give different results, he wrote:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eYes, I think there is lots to explore there! [co-author Akshara Prabhakar] did have some cool experiments that involved translating from letters to numbers within the CoT. That generally improved performance, but got a qualitatively similar chart. So that\u0026#39;s one case that is similar. But there could well be others that give different trends!\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://github.com/aksh555/deciphering_cot\"\u003eexperimental code and data\u003c/a\u003e for the study are available on GitHub.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Anthony-Alford\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAnthony Alford\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-10-22T00:00:00Z",
  "modifiedTime": null
}
