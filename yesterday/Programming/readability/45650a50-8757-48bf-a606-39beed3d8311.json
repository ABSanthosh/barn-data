{
  "id": "45650a50-8757-48bf-a606-39beed3d8311",
  "title": "How a Manual Remediation for a Phishing URL Took down Cloudflare R2",
  "link": "https://www.infoq.com/news/2025/03/cloudflare-incident-r2/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Due to human error in handling a phishing report and insufficient validation safeguards in admin tools, Cloudflare experienced an incident affecting its R2 Gateway service on February 5th. As part of a routine remediation for a phishing URL, the R2 service was inadvertently taken down, leading to the outage or disruption of numerous other Cloudflare services for over an hour. By Renato Losio",
  "author": "Renato Losio",
  "published": "Sat, 01 Mar 2025 09:46:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "S3",
    "Cloudflare",
    "Cloud",
    "Downtime",
    "Cloud Security",
    "Architecture \u0026 Design",
    "DevOps",
    "news"
  ],
  "byline": "Renato Losio",
  "length": 3945,
  "excerpt": "Due to human error in handling a phishing report and insufficient validation safeguards in admin tools, Cloudflare experienced an incident affecting its R2 Gateway service on February 5th. As part of",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250228123450/apple-touch-icon.png",
  "text": "Due to human error in handling a phishing report and insufficient validation safeguards in admin tools, Cloudflare experienced an incident affecting its R2 Gateway service on February 5th. As part of a routine remediation for a phishing URL, the R2 service was inadvertently taken down, leading to the outage or disruption of numerous other Cloudflare services for over an hour. According to Cloudflare’s incident report released the following day, the R2 Gateway service was taken down by a Cloudflare employee attempting to block a phishing site hosted on the Cloudflare R2 service. All operations involving R2 buckets and objects, including uploads, downloads, and metadata operations, were affected. Matt Silverlock, senior director of product at Cloudflare, and Javier Castro explain: The incident occurred due to human error and insufficient validation safeguards during a routine abuse remediation for a report about a phishing site hosted on R2. The action taken on the complaint resulted in an advanced product disablement action on the site that led to disabling the production R2 Gateway service responsible for the R2 API. Source: Cloudflare blog Cloudflare R2 storage, an S3-compatible object storage service with no egress charges, has been generally available since 2022 and is one of Cloudflare’s core offerings. While the company emphasized that the incident did not result in data loss or corruption within R2, many services were impacted in a cascading manner. Stream, Images, and Vectorize experienced downtime or significantly high error rates. Meanwhile, only a small fraction (0.002%) of deployments to Workers and Pages projects failed during the primary incident window. Silverlock and Castro add: At the R2 service level, our internal Prometheus metrics showed R2’s SLO near-immediately drop to 0% as R2’s Gateway service stopped serving all requests and terminated in-flight requests (...) Remediation and recovery was inhibited by the lack of direct controls to revert the product disablement action and the need to engage an operations team with lower level access than is routine. The R2 Gateway service then required a re-deployment in order to rebuild its routing pipeline across our edge network. Source: Cloudflare blog The incident report was published just a few hours after the event, and in a popular Reddit thread, many users praised Cloudflare’s transparency and the level of detail provided. User JakeSteam writes: Really appreciated the detailed minute by minute breakdown, helping highlight exactly why each minute of delay existed. Great work as always by cloudflare, turning something bad into a learning opportunity for all. User Miasodasto13 adds: Gotta love their transparency. Also, I can't imagine the adrenaline rush of experiencing such an event as an engineer. It must feel like disarming a ticking bomb. With each minute of downtime passing, the higher the consequences. Amanbolat Balabekov, staff software engineer at Delivery Hero, offers a different perspective: You'd think teams would build internal tools specifically for situations like this, but ironically, Cloudflare's tools failed precisely when they were needed most. It looks like to recover the service, they need to use the service itself, which creates this crazy cyclic dependency. Cloudflare has outlined several remediation and follow-up steps to address the validation gaps and prevent similar failures in the future. These include restricting access to product disablement actions and requiring two-party approval for ad-hoc product disablements. Additionally, the team is working on expanding abuse checks to prevent the accidental blocking of internal hostnames, thereby reducing the blast radius of both system- and human-driven actions. About the Author Renato Losio",
  "image": "https://res.infoq.com/news/2025/03/cloudflare-incident-r2/en/headerimage/generatedHeaderImage-1740304183913.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eDue to human error in handling a phishing report and insufficient validation safeguards in admin tools, Cloudflare experienced an incident affecting its R2 Gateway service on February 5th. As part of a routine remediation for a phishing URL, the R2 service was inadvertently taken down, leading to the outage or disruption of numerous other Cloudflare services for over an hour.\u003c/p\u003e\n\n\u003cp\u003eAccording to \u003ca href=\"https://blog.cloudflare.com/cloudflare-incident-on-february-6-2025/\"\u003eCloudflare’s incident report\u003c/a\u003e released the following day, the R2 Gateway service was taken down by a Cloudflare employee attempting to block a phishing site hosted on the Cloudflare R2 service. All operations involving R2 buckets and objects, including uploads, downloads, and metadata operations, were affected. Matt Silverlock, senior director of product at Cloudflare, and Javier Castro explain:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe incident occurred due to human error and insufficient validation safeguards during a routine abuse remediation for a report about a phishing site hosted on R2. The action taken on the complaint resulted in an advanced product disablement action on the site that led to disabling the production R2 Gateway service responsible for the R2 API.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003cimg alt=\"Cloudflare R2 Gateway\" data-src=\"news/2025/03/cloudflare-incident-r2/en/resources/1Image_20250206_172217_707-1740304392639.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/03/cloudflare-incident-r2/en/resources/1Image_20250206_172217_707-1740304392639.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eSource: Cloudflare blog\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.infoq.com/news/2022/10/cloudflare-r2-ga/\"\u003eCloudflare R2 storage\u003c/a\u003e, an S3-compatible object storage service with no egress charges, has been generally available since 2022 and is one of Cloudflare’s core offerings. While the company emphasized that the incident did not result in data loss or corruption within R2, many services were impacted in a cascading manner. Stream, Images, and Vectorize experienced downtime or significantly high error rates. Meanwhile, only a small fraction (0.002%) of deployments to Workers and Pages projects failed during the primary incident window. Silverlock and Castro add:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAt the R2 service level, our internal Prometheus metrics showed R2’s SLO near-immediately drop to 0% as R2’s Gateway service stopped serving all requests and terminated in-flight requests (...) Remediation and recovery was inhibited by the lack of direct controls to revert the product disablement action and the need to engage an operations team with lower level access than is routine. The R2 Gateway service then required a re-deployment in order to rebuild its routing pipeline across our edge network.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003cimg alt=\"Cloudflare R2 Outage\" data-src=\"news/2025/03/cloudflare-incident-r2/en/resources/1BLOG-2685_2-1740304392639.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/03/cloudflare-incident-r2/en/resources/1BLOG-2685_2-1740304392639.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eSource: Cloudflare blog\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThe incident report was published just a few hours after the event, and in a \u003ca href=\"https://www.reddit.com/r/CloudFlare/comments/1ijj5q2/cloudflare_incident_on_february_6_2025/\"\u003epopular Reddit thread\u003c/a\u003e, many users praised Cloudflare’s transparency and the level of detail provided. User \u003cem\u003eJakeSteam\u003c/em\u003e writes:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eReally appreciated the detailed minute by minute breakdown, helping highlight exactly why each minute of delay existed. Great work as always by cloudflare, turning something bad into a learning opportunity for all.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eUser \u003cem\u003eMiasodasto13\u003c/em\u003e adds:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eGotta love their transparency. Also, I can\u0026#39;t imagine the adrenaline rush of experiencing such an event as an engineer. It must feel like disarming a ticking bomb. With each minute of downtime passing, the higher the consequences.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAmanbolat Balabekov, staff software engineer at Delivery Hero, \u003ca href=\"https://www.linkedin.com/posts/amanbolat_cloudflare-incident-on-february-6-2025-activity-7297170434213470211-Hb31?utm_source=share\u0026amp;utm_medium=member_desktop\u0026amp;rcm=ACoAABaQ5R4B1z_TPIVzQKBvbJ9SpDn29zaiJcY\"\u003eoffers\u003c/a\u003e a different perspective:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eYou\u0026#39;d think teams would build internal tools specifically for situations like this, but ironically, Cloudflare\u0026#39;s tools failed precisely when they were needed most. It looks like to recover the service, they need to use the service itself, which creates this crazy cyclic dependency.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eCloudflare has outlined several remediation and follow-up steps to address the validation gaps and prevent similar failures in the future. These include restricting access to product disablement actions and requiring two-party approval for ad-hoc product disablements. Additionally, the team is working on expanding abuse checks to prevent the accidental blocking of internal hostnames, thereby reducing the blast radius of both system- and human-driven actions.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Renato-Losio\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRenato Losio\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-03-01T00:00:00Z",
  "modifiedTime": null
}
