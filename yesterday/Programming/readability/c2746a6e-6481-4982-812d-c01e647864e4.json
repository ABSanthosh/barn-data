{
  "id": "c2746a6e-6481-4982-812d-c01e647864e4",
  "title": "Google Releases MedGemma: Open AI Models for Medical Text and Image Analysis",
  "link": "https://www.infoq.com/news/2025/05/google-medgemma/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Google has released MedGemma, a pair of open-source generative AI models designed to support medical text and image understanding in healthcare applications. Based on the Gemma 3 architecture, the models are available in two configurations: MedGemma 4B, a multimodal model capable of processing both images and text, and MedGemma 27B, a larger model focused solely on medical text. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Fri, 30 May 2025 10:50:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Large language models",
    "Google",
    "Data Analysis",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 3081,
  "excerpt": "Google has released MedGemma, a pair of open-source generative AI models designed to support medical text and image understanding in healthcare applications. Based on the Gemma 3 architecture, the mod",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250527074915/apple-touch-icon.png",
  "text": "Google has released MedGemma, a pair of open-source generative AI models designed to support medical text and image understanding in healthcare applications. Based on the Gemma 3 architecture, the models are available in two configurations: MedGemma 4B, a multimodal model capable of processing both images and text, and MedGemma 27B, a larger model focused solely on medical text. According to Google, the models are designed to assist in tasks such as radiology report generation, clinical summarization, patient triage, and general medical question answering. MedGemma 4B, in particular, has been pre-trained using a wide range of de-identified medical images, including chest X-rays, dermatology photos, histopathology slides, and ophthalmologic images. Both models are available under open licenses for research and development use, and come in pre-trained and instruction-tuned variants. Despite these capabilities, Google emphasizes that MedGemma is not intended for direct clinical use without further validation and adaptation. The models are intended to serve as a foundation for developers, who can adapt and fine-tune them for specific medical use cases. Some early testers have shared observations on the models' strengths and limitations. Vikas Gaur, a clinician and AI practitioner, tested the MedGemma 4B-it model using a chest X-ray from a patient with confirmed tuberculosis. He reported that the model generated a normal interpretation, missing clinically evident signs of the disease: Despite clear TB findings in the actual case, MedGemma reported: ‘Normal chest X-ray. Heart size is within normal limits. Lungs well-expanded and clear. Gaur suggested that additional training on high-quality annotated data might help align model outputs with clinical expectations. Furthermore, Mohammad Zakaria Rajabi, a biomedical engineer, noted interest in expanding the capabilities of the larger 27B model to include image processing: We are eagerly looking forward to seeing MedGemma 27B support image analysis as well. Technical documentation indicates that the models were evaluated on over 22 datasets spanning multiple medical tasks and imaging modalities. Public datasets used in training include MIMIC-CXR, Slake-VQA, PAD-UFES-20, and others. Several proprietary and internal datasets were also used under license or participant consent. The models can be adapted through techniques like prompt engineering, fine-tuning, and integration with agentic systems using other tools from the Gemini ecosystem. However, performance can vary depending on prompt structure, and the models have not been evaluated for multi-turn conversations or multi-image inputs. MedGemma provides an accessible foundation for research and development in medical AI, but its practical effectiveness will depend on how well it is validated, fine-tuned, and integrated into specific clinical or operational contexts. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2025/05/google-medgemma/en/headerimage/generatedHeaderImage-1748542103741.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eGoogle has released \u003ca href=\"https://developers.google.com/health-ai-developer-foundations/medgemma\"\u003eMedGemma\u003c/a\u003e, a pair of open-source generative AI models designed to support medical text and image understanding in healthcare applications. Based on the Gemma 3 architecture, the models are available in two configurations: \u003ca href=\"https://github.com/google-health/medgemma\"\u003eMedGemma 4B\u003c/a\u003e, a multimodal model capable of processing both images and text, and \u003ca href=\"https://huggingface.co/google/medgemma-27b-text-it\"\u003eMedGemma 27B\u003c/a\u003e, a larger model focused solely on medical text.\u003c/p\u003e\n\n\u003cp\u003eAccording to Google, the models are designed to assist in tasks such as radiology report generation, clinical summarization, patient triage, and general medical question answering. MedGemma 4B, in particular, has been pre-trained using a wide range of de-identified medical images, including chest X-rays, dermatology photos, histopathology slides, and ophthalmologic images. Both models are available under open licenses for research and development use, and come in pre-trained and instruction-tuned variants.\u003c/p\u003e\n\n\u003cp\u003eDespite these capabilities, Google emphasizes that MedGemma is not intended for direct clinical use without further validation and adaptation. The models are intended to serve as a foundation for developers, who can adapt and fine-tune them for specific medical use cases.\u003c/p\u003e\n\n\u003cp\u003eSome early testers have shared observations on the models\u0026#39; strengths and limitations. Vikas Gaur, a clinician and AI practitioner, tested the MedGemma 4B-it model using a chest X-ray from a patient with confirmed tuberculosis. He \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:activity:7330735937179607040?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7330735937179607040%2C7332946460998541313%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287332946460998541313%2Curn%3Ali%3Aactivity%3A7330735937179607040%29\"\u003ereported\u003c/a\u003e that the model generated a normal interpretation, missing clinically evident signs of the disease:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eDespite clear TB findings in the actual case, MedGemma reported: ‘Normal chest X-ray. Heart size is within normal limits. Lungs well-expanded and clear.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eGaur suggested that additional training on high-quality annotated data might help align model outputs with clinical expectations.\u003c/p\u003e\n\n\u003cp\u003eFurthermore, Mohammad Zakaria Rajabi, a biomedical engineer, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:activity:7330735937179607040?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7330735937179607040%2C7330818644924362753%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287330818644924362753%2Curn%3Ali%3Aactivity%3A7330735937179607040%29\"\u003enoted\u003c/a\u003e interest in expanding the capabilities of the larger 27B model to include image processing:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWe are eagerly looking forward to seeing MedGemma 27B support image analysis as well.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003ca href=\"https://developers.google.com/health-ai-developer-foundations/medgemma/model-card\"\u003eTechnical documentation\u003c/a\u003e indicates that the models were evaluated on over 22 datasets spanning multiple medical tasks and imaging modalities. Public datasets used in training include \u003ca href=\"https://physionet.org/content/mimic-cxr/2.1.0/\"\u003eMIMIC-CXR\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2102.09542\"\u003eSlake-VQA\u003c/a\u003e, \u003ca href=\"https://www.kaggle.com/datasets/mahdavi1202/skin-cancer\"\u003ePAD-UFES-20\u003c/a\u003e, and others. Several proprietary and internal datasets were also used under license or participant consent.\u003c/p\u003e\n\n\u003cp\u003eThe models can be adapted through techniques like prompt engineering, fine-tuning, and integration with agentic systems using other tools from the Gemini ecosystem. However, performance can vary depending on prompt structure, and the models have not been evaluated for multi-turn conversations or multi-image inputs.\u003c/p\u003e\n\n\u003cp\u003eMedGemma provides an accessible foundation for research and development in medical AI, but its practical effectiveness will depend on how well it is validated, fine-tuned, and integrated into specific clinical or operational contexts.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-05-30T00:00:00Z",
  "modifiedTime": null
}
