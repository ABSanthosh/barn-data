{
  "id": "63657c3a-5c09-4e72-be7a-f5ecdf6f6bf4",
  "title": "Article: Analyzing Apache Kafka Stretch Clusters: WAN Disruptions, Failure Scenarios, and DR Strategies",
  "link": "https://www.infoq.com/articles/apache-kafka-stretch-cluster-failures/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Proficient in analyzing the dynamics of Apache Kafka Stretch Clusters, I assess WAN disruptions and devise effective Disaster Recovery (DR) strategies. With deep expertise, I ensure high availability and data integrity across multi-region deployments. My insights optimize operational resilience, safeguarding vital services against service level agreement violations. By Srikanth Daggumalli, Nishchai Jayanna Manjula",
  "author": "Srikanth Daggumalli, Nishchai Jayanna Manjula",
  "published": "Fri, 20 Jun 2025 09:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Performance",
    "Apache Kafka",
    "Architecture",
    "Scalability",
    "Monitoring",
    "Application Security",
    "Performance \u0026 Scalability",
    "Fault Tolerance",
    "Reliability",
    "Distributed Programming",
    "Streaming",
    "Operations management",
    "Storage",
    "Availability",
    "Disaster Recovery",
    "Distributed Data",
    "DevOps",
    "Development",
    "Architecture \u0026 Design",
    "article"
  ],
  "byline": "Srikanth Daggumalli, Nishchai Jayanna Manjula",
  "length": 47846,
  "excerpt": "Uncover the critical risks and robust disaster recovery strategies for Apache Kafka Stretch Clusters amid WAN disruptions, ensuring seamless continuity and data integrity across geographies.",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250605075544/apple-touch-icon.png",
  "text": "Key Takeaways Apache Kafka Stretch cluster setup in an on-premise environment poses a high risk of service unavailability during WAN disruptions, often leading to split-brain or brain-dead scenarios and violating Service Level Agreements (SLAs) due to degraded Recovery Time Objective (RTO) and Recovery Point Objective (RPO). Proactive monitoring of your Kafka environment to understand data skews across brokers is critical; an uneven data load on a single node can cause a stretch cluster failure. An ungraceful broker shutdown during upgrades or due to poor security posture can make the Kafka service unavailable, resulting in a loss of data. There are three popular and widely used Kafka disaster recovery strategies; each has challenges, complexities, and pitfalls. Kafka Mirror Maker 2 replication lag (Δ) for the Disaster Recovery setup can cause message loss, inconsistent data. Apache Kafka is a well-known publish-subscribe distributed system widely used across industries for use cases such as log analytics, observability, event-driven architectures, and real-time streaming. Its distribution allows it to become the critical piece or backbone of modern streaming architecture, offering many integrations and supporting near-real-time or real-time latency requirements. Today, Kafka has been offered by major companies as a service, including Confluent Kafka as well as cloud offerings such as Confluent Cloud, AWS Managed Streaming Kafka , Google Cloud Managed Service for Apache Kafka, Azure Event Hub for Apache Kafka, etc. Additionally, Kafka has been deployed across over one-hundred-thousand companies, including Fortune 100 across various industry segments such as enterprise, financials, healthcare, automobile, startups, independent software vendors, and retail sectors. The value of the data diminishes over time. Many businesses these days compete in time and demand real-time data processing with millisecond responses to power mission-critical applications. Financial services, e-commerce, IoT, and other latency-sensitive industries require highly available and resilient architectures that can withstand infrastructure failures without impacting business continuity. Related Sponsored Content Companies operating across multiple geographic locations need solutions that provide seamless failover, data consistency, and disaster recovery (DR) capabilities while minimizing operational complexity and cost. One of the common architectural patterns that companies often consider using is a single Kafka cluster spanning multiple data center locations on WAN (Wide Area Network). It is called Stretch Cluster. This pattern is commonly considered by companies for disaster recovery (DR) and high availability (HA). Stretch Cluster can theoretically provide data redundancy across regions and minimize data loss in the event of a region-wide failure. However, this approach comes with several challenges and trade-offs due to the inherent nature of Kafka and its dependency on network latency, consistency, and partition tolerance in a multi-region setup. In this article, we are going to focus on the stretch cluster architectural pattern for DR or HA-related implications and considerations. The following testing has been conducted, and the cluster behavior has been documented. Environment Details Regions London, Frankfurt Kafka Distribution Apache Kafka Kafka Version 3.6.2 Zookeeper 3.8.2 Operating system Linux High-Level Environment Setup View Below is the high-level view of a single stretch cluster setup that spans across the London and Frankfurt regions with Four Kafka brokers, where actual data resides. Four Zookeeper nodes (a three-node rather than a four-node setup would work as well, but to mimic the exact environment in both the regions, we have set up four Zookeeper nodes ). Zookeeper follows [N/2] + 1 rule for quorum establishment. In this case, the Zookeeper quorum will be [4/2] + 1 = 2 + 1 = 3. Network latency between these regions is ~ 15ms. Multiple producers (1 to n) and consumers (1 to n) are producing and consuming the data from this cluster from different regions of the world. Kafka Broker Hardware Configuration Hard Disk Size 10 TB Memory 120 GB CPU 16 Core Disk Setup RAID 0 Kafka Broker Configuration Replication factor set to 4 Data retention (aka log retention) period is 7 days Auto-creation of topics allowed Minimum in-sync replication (ISR) set to 3 The number of partitions per topic is 10 Other configuration details Acks = all max.in.flight.request.per.connection=1 compression.type=lz4 batch.size=16384 buffer.memory=67108864 auto.offset.reset=earliest send.buffer.bytes=524288 receive.buffer.bytes=131072 num.replica.fetchers=4 num.network.threads=12 num.io.threads=32 Producers and Consumers: Producers: In this use case, we have used a producer written in Java that consistently produces defined (K) Transactions Per Second (aka TPS), which can be controlled using properties files. Sample snippets using Kafka performance test api. ******** Start of Producer code snippet *******  void performAction(int topicN){         try {             ProcessBuilder processBuilder = new ProcessBuilder();             Properties prop = new Properties();             InputStream input = null;             input = new FileInputStream(\"config.properties\");             prop.load(input);                 SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS\");                 sdf.setTimeZone(TimeZone.getTimeZone(\"America/New_York\"));                 String startTime = sdf.format(new Date());             System.out.println(\"Start time ---\u003e \"+startTime+\" topic \" +topicN); String command = \"/home/opc/kafka/bin/kafka-producer-perf-test --producer-props bootstrap.servers=x.x.x.x:9092\"+ \" --topic \"+topicN+                         \" --num-records \"+ prop.getProperty(\"numOfRecords\") +                         \" --record-size \"+ prop.getProperty(\"recordSize\") +                         \" --throughput \"+ prop.getProperty(\"throughputRate\") +                         \" --producer.config \"+ \"/home/opc/kafka/bin/prod.config\";                                  //processBuilder.command(\"cmd.exe\", \"/c\", \"dir C:\\\\Users\\\\admin\");                 processBuilder.command(\"bash\", \"-c\", command);                 Process process = processBuilder.start();                 //StringBuilder output = new StringBuilder();                 BufferedReader reader = new BufferedReader(                         new InputStreamReader(process.getInputStream()));                                 String line;                 while ((line = reader.readLine()) != null) {                     //output.append(line + \"\\n\");                     if(line!=null \u0026\u0026 line.contains(\"99.9th.\")) {                             break;                         }                    }                 int exitVal = process.waitFor();                 if (exitVal == 0) {                 System.out.println(\"Success!\");                 String endTime = sdf.format(new Date());         //send to opensearch for tracking purpose - optional step                 pushToIndex(line,startTime, endTime, prop.getProperty(\"indexname\"));                 //System.out.println(output);                 //System.exit(0);                 } else {                     System.out.println(\"Something unexpected happend!!!\");                 }         } catch (IOException e) {             e.printStackTrace();         } catch(Exception e){             e.printStackTrace();         }     } ********** End of producer code snippet ************** ****** Stat of Producer properties file *****  numOfRecords=70000 recordSize=4000 throughputRate=20 topicstart=1 topicend=100 ****** End of Producer properties file *****  Consumers: In this use case, we have used consumers written in Java that consistently consume defined (K) transactions per second (TPS), which can be controlled using properties files. ***** Start of consumer code snippet *******      void performAction(int topicN){         try {             ProcessBuilder processBuilder = new ProcessBuilder();             Properties prop = new Properties();             InputStream input = null;             input = new FileInputStream(\"consumer.properties\");             prop.load(input);                 SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS\");                 sdf.setTimeZone(TimeZone.getTimeZone(\"America/New_York\"));                 String startTime = sdf.format(new Date());             System.out.println(\"Start time ---\u003e \"+startTime+\" topic \" +topicN); String command =null;             if( prop.getProperty(\"latest\").equalsIgnoreCase(\"no\")) {           command = \"/home/opc/kafka/bin/kafka-consumer-perf-test --broker-list=x.x.x.x:9092\"+                         \" --topic \"+topicN+                         \" --messages \"+ prop.getProperty(\"numOfRecords\") +                        // \" --group \"+ prop.getProperty(\"groupid\") +                         \" --timeout \"+ prop.getProperty(\"timeout\")+             \" --consumer.config \"+\"/home/opc/kafka/bin/cons.config\"; }else{ command = \"/home/opc/kafka/bin/kafka-consumer-perf-test --broker-list=x.x.x.x:9092\"+\" --topic \"+topicN+                         \" --messages \"+ prop.getProperty(\"numOfRecords\") +                        // \" --group \"+ prop.getProperty(\"groupid\") +                         \" --timeout \"+ prop.getProperty(\"timeout\")+             \" --consumer.config \"+\"/home/opc/kafka/bin/cons.config\"+                         \" --from-latest\";                             } System.out.println(\"command is ---\u003e \" +command);                                 //processBuilder.command(\"cmd.exe\", \"/c\", \"dir C:\\\\Users\\\\admin\");                 processBuilder.command(\"bash\", \"-c\", command);                 Process process = processBuilder.start();                 StringBuilder output = new StringBuilder();                 BufferedReader reader = new BufferedReader(                         new InputStreamReader(process.getInputStream()));                                 String line;                 while ((line = reader.readLine()) != null) {                    // output.append(line + \"\\n\"+\"test\");                     //if(line.contains(\":\") \u0026\u0026 line.contains(\"-\")) {                       if (line.contains(\":\") \u0026\u0026 line.contains(\"-\") \u0026\u0026 !line.contains(\"WARNING:\")) {                             break;                         }                    }                 int exitVal = process.waitFor();                 if (exitVal == 0) {                 System.out.println(\"Success!\");                //optional step - send data to opensearch                  pushToIndex(line, prop.getProperty(\"indexname\"),topicN);                 System.out.println(output);                 //System.exit(0);                 } else {                     System.out.println(\"Something unexpected happend!!!\");                 }         } catch (IOException e) {             e.printStackTrace();         } catch(Exception e){             e.printStackTrace();         }     } ***** End of consumer code snippet ******* *****Start of Consumer properties snippet ****  numOfRecords=70000 topicstart=1 topicend=100 groupid=cgg1 reportingIntervalms=10000 fetchThreads=1 latest=no processingthreads=1 timeout=60000 ***** End of consumer properties snippet **** CAP Theorem Before diving into further discussions and scenario executions, it's crucial to understand the fundamental algorithms of distributed systems. This foundational knowledge will help you better correlate WAN disruptions with their impact on system behavior. CAP Theorem (i.e., Consistency, Availability, and Partition tolerance) applies to all the distributed systems like Kafka, Hadoop, Elasticsearch, and many others, where data is being distributed across nodes to: Enable parallel data processing Maintain high availability for data and service Support horizontal or vertical scaling requirements Consistency (C) Every read gets the latest write or an error. Kafka follows eventual consistency; data ingested into one node eventually syncs across others. Reads happen from the leader, so replicas might lag behind. Availability (A) Every request gets a response, even if some nodes fail. Kafka ensures high availability using replication factor, min.insync.replicas, and acks. As long as a leader is available, Kafka keeps running. Partition Tolerance (P) The system keeps working despite network issues. Even if some nodes disconnect, Kafka elects a new leader and continues processing. Kafka is an AP system; it prioritizes Availability and Partition Tolerance, while consistency is eventual and tunable via settings. Now, let's get into the scenarios. Stretch Cluster failed scenarios Before going in-depth into scenarios, let's understand the key components and terminology of Kafka internals. Brokers represent data nodes where all the data is distributed with replicas for data high availability. The Controller is a critical component of Kafka that maintains the cluster's metadata, including information about topics, partitions, replicas, and brokers. Also performs administrative tasks. Think of it as the brain of a Kafka cluster. A Zookeeper quorum is a group of nodes (usually deployed on dedicated nodes) that act as a coordination service helping to manage the Controller election, Broker Registration, Broker Discovery, Cluster membership, Partition leader election, Topic configuration, Access Control List (ACLs), consumer group management, and offset management. Topics are logical constructs of data to which all related data is published. Examples include all the network logs published to the \"Network_logs\" topic and application logs published to the \"Application_logs\" topic. Partitions are the fundamental units or building blocks supporting scalability and parallelism within a topic. When a topic is created, it can have one or more partitions where all the data is published in order. Offsets are unique identifiers assigned to each message within a partition of a topic, essentially a sequential number that indicates the position of a message within the partition. A producer is a client application that publishes records (messages) to Kafka topics. A consumer is a client application that consumes records (messages) from Kafka topics. Scenario 1: WAN Disruption – Active Controllers as '0' – AKA Brain-Dead Scenario Scenario WAN Disruption – Active controllers as '0'. Scenario Description As discussed in earlier sections, in a distributed network setup across geographic locations, network service interruptions can occur because no service guarantees 100% availability. This scenario focuses on WAN disruption between two regions, causing Kafka service unavailability, leaving the cluster with zero active controllers and no self-recovery, even after hours of waiting. Execution Steps Kafka cluster (all brokers and Zookeeper quorum) should be up and running with the configuration stated in the environment section Produce and consume events with brokers' equilibrium state (this step is optional, but simulating a real use case) Disconnect/disrupt WAN service between regions Observe the cluster behavior. Check through the broker host CLI and ZK CLI. If you are using Confluent, you can check in Confluent center or AWS Cloud provided managed service you can check the same CloudWatch, respectively and other offerings too a. ZK quorum availability b. Brokers availability as cluster members c. Number of active controllers d. Producers and consumers progress e. Under replicated partitions, etc. Reestablish WAN service between regions and observe the cluster behavior. Expected Behavior On steady state, the Kafka cluster should be available with 4 brokers ZK quorum as 3 Active controllers as 1 Producers and consumers should produce and fetch events from topics Upon WAN service disruption, The Kafka cluster should not be available ZK quorum 3 will break Active controllers as 1 or zero Producers and consumers should be interrupted, create back pressure or time-outs. Reestablish WAN service ZK quorum 3 should be reestablished Kafka cluster should be available with 4 brokers The active controller should be 1 Producers and consumers should continue/resume Actual/Observed Behavior Kafka is unavailable and can't be recovered on its own upon WAN service disruption and reestablishment and shows active controllers as '0' Active brokers are reported as zero Active controller number is zero ZK quorum with 3 is formed on WAN service resume, and the quorum broke on disruption All brokers showed up in ZK cluster membership upon WAN service resume and only two brokers showed during WAN disruption. Producers and consumers completely interrupted and timed out. Root Cause Analysis (RCA) It's a \"brain-dead\" scenario. Upon WAN disruption, a single 4-node cluster becomes 2 independent clusters, each with 2 nodes. And the existing single Zookeeper quorum becomes two quorums i.e., one for each site. But each other quorum does not elect a new controller, resulting as it thinks that the controller already existing resulting in zero controllers upon WAN reestablishment. Imagine two cooks in a kitchen preparing a dish. Each cook assumes the other has added the salt, but in the end, no salt is added to the dish. From the logs, it was observed controller moved the exception from one broker to another broker, but no exit or hang was reported in the logs Controller exceptions: controller.log:org.apache.kafka.common.errors.ControllerMovedException: Controller moved to another broker. Aborting  controller.log:java.io.IOException: Client was shutdown before response was read controller.log:java.lang.InterruptedException From ZooKeeper (ZK) CLI, It's also observed that 'no broker claims to be the controller' in the cluster, the cluster failed to respond properly in the face of state changes, including topic or partition creation or broker failures. ERROR Unexpected exception causing shutdown while sock still open (org.apache.zookeeper.server.quorum.LearnerHandler) java.lang.Exception: shutdown Leader! reason: Not sufficient followers synced, only synced with sids: [ 2 ] ERROR Exception while listening (org.apache.zookeeper.server.quorum.QuorumCnxManager) References WAN disable can be achieved through route tables modify or delete. ZK commands. Refer ZK command section. Solution/Remedy Because there are no controller claims by brokers, all the brokers need to be restarted. WAN Disruption: WAN disruption can be done by deleting or modifying route tables. Warning: Deleting a route table may disrupt all traffic in the associated subnets. Instead, consider modifying or detaching the route table. Firewall Stop You may need to shut down firewalls Firewall status: systemctl status firewalld Firewall stop: service firewalld stop ZK Commands list: ZK Server start: bin/zookeeper-server-start etc/kafka/zookeeper.properties ZK Server stop: zookeeper-server-stop ZK Shell commands: bin/zookeeper-shell.sh \u003czookeeper host\u003e:2181 Active controllers check: get /controller or ls /controller Brokers list: /home/opc/kafka/bin/zookeeper-shell kafkabrokeraddress:2181 \u003c\u003c\u003c\"ls /brokers/ids\" Alternative to check controllers and brokers in a cluster. Available from kafka 3.7.x bin/kafka-metadata-shell.sh --bootstrap-server \u003cKAFKA_BROKER\u003e:9092 --describe-cluster Scenario 2: WAN Disruption - Active Controllers as '2' – AKA Split-Brain Scenario Scenario WAN Disruption - Active controllers as '2' Scenario Description As discussed in earlier sections, in a distributed network setup across geographic locations, network service interruptions can occur because no service guarantees 100% availability. This scenario focuses on WAN disruption between two regions, causing Kafka service unavailability with active controllers as '2' and no self-recovery, even after hours of waiting. Execution Steps Kafka Cluster (all brokers and Zookeeper quorum) should be up and running with the configuration stated in the environment section Produce and consume events with brokers' equilibrium state (this step is optional) Disconnect/Disrupt WAN service between regions Observe the cluster behavior- Check through the broker host CLI and ZK CLI. If you are using Confluent, you can check in the Confluent center or the AWS Cloud provided managed service, you can check the same CloudWatch respectively ZK quorum availability Brokers' availability as cluster members Number of active controllers Producers and consumers progress Under replicated partitions etc. Reestablish WAN service between regions and observe the cluster behavior. Expected Behavior In a steady state, the Kafka cluster should be available with 4 brokers ZK quorum as 3 Active controllers as 1 Producers and consumers should produce and fetch events from topics Upon WAN service disruption, The Kafka cluster should not be available ZK quorum 3 will break Active controllers as 1 or zero Producers and consumers should be interrupted or create back pressure or time-outs. Re-establish WAN service ZK quorum 3 should be reestablished Kafka cluster should be available with 4 brokers Active controller should be 1 Producers and consumers should continue/resume Actual/Observed Behavior Kafka is not available and can't be recovered on its own upon WAN service disruption and re-establishment and shows active controllers as '2' Active brokers are reported as zero Active controller number is two ZK quorum with 3 is formed on WAN service resume and the quorum broke on disruption All brokers showed up in ZK cluster membership upon WAN service resume and only two brokers showed during WAN disruption. Producers and consumers completely interrupted and timed out. NETWORK_EXCEPTIONs Producers got failed requests with NOT_LEADER_FOR_PARTITIONS exceptions Root Cause Analysis (RCA) This is a split-brain scenario. Upon WAN disruption, a single 4-node cluster becomes 2 independent clusters with each consisting of 2 nodes. Also, an existing single Zookeeper quorum becomes two quorums, i.e., one for each site. As a result, each quorum will elect a new controller, resulting in two controllers. Upon WAN re-establishment, we see 2 controllers for 4 4-node clusters. Think of it like two co-pilots flying one plane from different cockpits after radio loss. Both think the other is gone and take over, conflicting actions crash the system. From ZooKeeper (ZK) CLI, it's also observed that two brokers claim to be the controller in the cluster; the cluster failed to respond properly in the face of state changes, including topic or partition creation or broker failures. References WAN disable can be achieved through the route tables delete ZK commands Solution/Remedy Restart both controllers Scenario 3: Broker Disk full, resulting respective broker process crash and No Active controllers Scenario Broker disk full, resulting respective broker process crash, and No active controllers or '0' active controllers. Scenario Description On a steady Kafka cluster, which means all the brokers are up and running, and both producers and consumers are producing and consuming events. One of the 4 brokers' disks is full due to data skew, i.e., uneven or imbalanced distribution of data across and leading to an abrupt broker process crash and cascading the crash to the remaining brokers. This causes the Kafka cluster unavailability because of active controllers as '0'. Execution Steps Kafka Cluster (all brokers and Zookeeper quorum) should be up and running with the configuration stated in the environment section Produce and consume events with brokers' equilibrium state One of the 4 brokers' disks is full and causing the respective broker process to crash Observe the cluster behavior through the broker host CLI or ZK CLI. If you are using Confluent, you can check in the Confluent center or the AWS Cloud provided managed service you can check the same CloudWatch respectively ZK quorum availability Brokers availability as cluster members Number of active controllers Producers and consumers progress Under replicated partitions, etc. Expected Behavior In a steady state, Kafka cluster should be available with 4 brokers ZK quorum as 3 Active controllers as 1 Producers and consumers should produce and fetch events from topics Upon one broker's disk full, Disk error should be logged to logs. Kafka cluster should not be available; only one broker should be shown as not available or out of cluster membership ZK quorum shouldn't fail Active controllers can be zero (if the controller node is the same as a dead broker) and move the controller to another available broker. The cluster should report under replicated partitions due to one broker unavailability. Producers and consumers should be interrupted, create back pressure, or time-outs. Actual/Observed Behavior Kafka is not available and couldn't be recovered on its own upon and showed active controllers as '0' Active brokers are reported as zero The active controller number is zero No impact on Zookeeper quorum Producers and consumers completely interrupted and timed out Root Cause Analysis (RCA) From the logs, observed disk-related errors and exceptions logs/state-change.log:102362: ERROR [Broker id=1] Skipped the become-follower state change with correlation id 331 from controller 1 epoch 1 for partition topic1-9 (last update controller epoch 1) with leader 5 since the replica for the partition is offline due to disk error org.apache.kafka.common.errors.KafkaStorageException: Error while creating log for topic1-9 in dir /kafka/kafka-logs (state.change.logger) Logs show that the controller moved the exception from one broker to another broker, but no exit or hang was reported in the logs Controller exceptions: controller.log:org.apache.kafka.common.errors.ControllerMovedException: Controller moved to another broker. Aborting  controller.log:java.io.IOException: Client was shutdown before response was read controller.log:java.lang.InterruptedException Controllers '0' behavior is equal to the brain-dead scenario explained in scenario 2. References ZK active controller commands ZK broker membership commands Solution/Remedy It is essential to monitor your Kafka cluster with proper metrics such as CPU, Disk, Memory, Java Heap, etc. Once the disk reaches 80% or more, you should scale up your cluster. Because there are no controller claims by brokers, all the brokers need to be restarted. Scenario 4: Corrupted indices upon Kafka process kill Scenario Corrupted indices upon Kafka process kill Scenario Description Killing broker and ZK processes (an uncontrolled shutdown can be comparable with machine abrupt reboot) and restarting brokers lead to corrupted indices, rebuilding indices doesn't work even after hours of waiting. This scenario is comparable to an ungraceful shutdown during an environment upgrade or to a poor security posture, where an unauthorized person could terminate the service. Execution Steps Kafka Cluster (all brokers and Zookeeper quorum) should be up and running with the configuration stated in the environment section Produce and consume events with brokers' equilibrium state Kill the broker and ZK processes, and restart the processes Observe the cluster behavior through the broker host CLI or ZK CLI. If you are using Confluent, you can check in the Confluent center or the AWS Cloud provided managed service you can check the same CloudWatch respectively ZK quorum availability Brokers' availability as cluster members Number of active controllers Producers and consumers progress Under replicated partitions etc. Expected Behavior In a steady state, the Kafka cluster should be available with 4 brokers ZK quorum is 3 Active controllers as 1 Producers and consumers should produce and fetch events from topics Kill Kafka processes (kill -9), i.e., both broker and ZK processes on all 4 nodes and restart processes ZK quorum should be formed Active controllers as '1' In-flight messages may be lost if not flushed to the disk May result in corrupted indices, but the broker rebuilds corrupted indices on its own without any manual work Upon rebuild of corrupted indices, the Kafka cluster should be available The cluster might show under replicated partitions due to corrupted indices Producers and consumers will get exceptions or errors and can't continue to perform their jobs Actual/Observed Behavior Kafka is not available and couldn't be recovered on its own. Active brokers are '4' Active controller number is '1' No impact on Zookeeper quorum Producers and consumers completely interrupted and timed out Root Cause Analysis (RCA) From the logs, 'index corrupted' logs for partitions are shown, and also 'rebuilding indices logs'. Rebuilding indices didn't work and doesn't show any related logs on progress. The resulting cluster is in unavailable status even after 1 hour. WARN [Log partition=topic1-13, dir=/kafka/new/kafka-logs] Found a corrupted index file corresponding to log file /kafka/new/kafka-logs/topic1-13/00000000000000000000.log due to Corrupt time index found, time index file (/kafka/new/kafka-logs/topic1-13/00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553657637686}, recovering segment and rebuilding index files... (kafka.log.Log) Also, it is recommended to do graceful shutdowns instead of using the kill switch or doing abrupt shutdowns so the data in the Kafka local cache will be flushed to the disks by maintaining meta data information and also committing offsets. References Not applicable Solution/Remedy If Kafka fails to rebuild all the indices on its own, manual deletion of all index files and restarting all brokers will fix the issue. Conclusion on WAN disruptions failure scenarios Deploying on-premise stretch clusters requires a deep understanding of failure scenarios, particularly during network disruptions. Unlike cloud environments, on-premises setups may lack highly available network infrastructure, fault tolerance mechanisms, or dedicated fiber lines, making them more susceptible to outages. In such cases, a Kafka cluster may experience downtime if network connectivity is disrupted. Cloud providers, on the other hand, offer high-bandwidth, low-latency networking over fully redundant, dedicated metro fiber connections between regions and data centers. Their managed Kafka services are designed for high availability, typically recommending deployment across multiple zones or regions to withstand zonal failures. Additionally, these services incorporate zone-awareness features, ensuring Kafka data replication occurs across multiple zones, rather than being confined to a single one. It is also important to understand and emphasize the latency impact on a stretch cluster deployed in two different Geo locations with a WAN connection. We can not escape physics; in our case, as per WonderNetwork, the average latency between London and Frankfurt is 14.88 milliseconds for a distance of 636km. So, for mission-critical applications, the setup of a stretch cluster driven by latencies, through testing, is needed to determine how the data is being replicated and consumed across. By understanding these differences, organizations can make informed decisions about whether an on-premise stretch cluster aligns with their availability and resilience requirements or if a cloud-native Kafka deployment is a more suitable alternative. Disaster Recovery Strategies: A Business Continuity Plan (BCP) is essential for any business to maintain its brand and reputation. Almost all the companies think of High Availability (HA) and Disaster recovery (DR) strategies to meet their BCP. Below are the popular DR Strategies for Kafka. Active – Standby (aka Active-passive) Active – Active Backup and restore Before we dive deep into DR strategies further, let's understand that the Kafka Mirror Maker 2.0 we are using in the following architectural patterns. There are other components like Kafka Connect, Replicator, etc., offered by Confluent and other cloud services, but they are out of scope for this article. DNS Redirection in Apache Kafka is a cross-cluster replication tool that uses the Kafka Connect framework to replicate topics between clusters, supporting active-active and active-passive setups. It provides automatic consumer group synchronization, offset translation, and failover support for disaster recovery and multi-region architectures. Active Standby This is a popular DR strategy pattern where there will be two different Kafka clusters, which are of identical size \u0026 configuration, deployed in two different Geo regions or Data centers. In this case, it's the London and Frankfurt regions. Usually, Geo location choice depends upon Earth's different tectonic plates, company or country compliance and regulations requirements, and other factors. Out of which only one cluster is active at a given point of time, which receives and serves the clients. As shown in the following picture, Kafka cluster one (KC1) is located in the London region and Kafka cluster two (KC2) is located in the Frankfurt region with the same size and configuration. All the producers are producing data to KC1 (London region cluster), and all consumers are consuming the data from the same cluster. KC2 (Frankfurt region cluster) receives the data from the KC1 cluster through Mirror Maker as shown in Figure 1. Figure 1: Kafka Active Standby (aka Active Passive) cluster setup with Mirror role In simple terms, we can treat the mirror maker as a combination of producer and consumer, where it consumes the topics from the source cluster KC1 (London region) and produces the data to the destination cluster KC2 (Frankfurt region). During the disaster to the primary/active cluster KC1 (London region), the standby cluster (Frankfurt region) will become active, and all the producers and consumers' traffic redirected to the standby cluster. This can be achieved through DNS redirection via a load balancer in front of Kafka clusters. Fig 2 shows this scenario. Figure 2: Kafka Active Standby setup - Standby cluster becomes active When the London Region KC1 cluster becomes active, we need to set up a mirror maker instance to consume the data from the Frankfurt region cluster and produce back to it to the London region KC1 cluster. This scenario is shown in Fig. 3. It's up to the business to keep the Frankfurt region (KC2) active or promote the London region cluster (KC1) back to primary once the data is fully synced/hydrated to the current state. In some cases, the standby cluster, i.e., the Frankfurt region cluster (KC2), continues to function like an active cluster and the London region cluster (KC1) becomes the standby as well. It is essential to note that RTO (Recovery Time Objective) and RPO (Recovery Point Objective) values are high compared to the Active-Active setup. Active-standby setup cost is medium to high as we need to maintain the same Kafka clusters in both regions. Figure 3: Kafka Active Standby setup – Standby cluster data sync happening to the KC1 Concerns with the Active-Standby setup Kafka Mirror Maker Replication Lag (Δ): Let's take a scenario, and it might happen very rarely, where we lose all the cluster completely, including nodes and disks, etc. Where the Active Kafka Cluster (London region - KC1) has 'm' messages, and the mirror maker is consuming and producing to the standby cluster. Due to network latency or consumers \u0026 producing process with acknowledgements, the standby cluster (i.e., Frankfurt region - KC2) lags behind and holds only 'n' messages. Which means the delta(\\(\\Delta\\)) is m minus n. \\(\\Delta = m - n\\) where: m = Messages in the Active Kafka Cluster (London) n = Messages in the Standby Kafka Cluster (Frankfurt) Due to this replication lag (\\(\\Delta\\)), it will have the following impact on mission-critical applications Potential Message Loss: If a failover occurs before the Standby cluster catches up, some messages (\\(\\Delta\\)) may be missing. Inconsistent Data: Consumers relying on the Standby cluster may process incomplete or outdated data. Delayed Processing: Applications depending on real-time replication may experience latency in data availability. Active-Active Setup: The replication lag concern can be overcome with the Active-Active setup. Just like the Active standby setup, we will have the same Kafka clusters deployed in the London and Frankfurt regions. The producers of the ingestion pipeline will write the data to both clusters at any given point in time. Which is also known as Dual Writes. So, both clusters' data is in sync as shown in Fig. 4. Please note that the ingestion pipeline or producers should have the capability to support dual writes. It is also important to note that these write pipelines should be isolated so that any issue with one cluster should not impact other pipelines or create back pressure and producer latencies. For example, if your use case is log analytics or observability, considering Logstash in your ingestion and creating two separate pipelines within a single or more Logstash instances will act as an isolated pipeline. But only one cluster is active for the consumers at any given point of time. Figure 4: Active-Active setup or Dual writes. In case, as shown in Figure 5, any issues with the one cluster the consumers need to be redirected with DNS redirect and RTO and RPO or SLAs are near zero. Figure 5: Active-Active setup, one cluster is unavailable Once the London cluster is available and becomes active, data can sync back to the London cluster from the Frankfurt cluster using Mirror Maker. As shown in Fig. 6, consumers can be switched back to the London cluster using DNS redirect. Also, it is important to configure consumer groups to start where they left off. During this setup, during the failover, it is important to configure consumers' offset to read the messages from a particular point in time, like reply messages from the last current time minus 15 mins, or set up a mirror maker between active clusters to read 'only' consumer offset topics from the London region cluster and replicate to the Frankfurt region cluster. And during the failure, you can use source.auto.offset.reset=latest to read the data from the latest offsets in the Frankfurt region cluster. If the consumer groups are not properly configured, you may see duplicate messages in your downstream system. So, it's essential to design the downstream to handle duplicate records. For example, if it's an observability use case and if the downstream is OpenSearch, it's good practice to design a unique ID depending on the message timestamp and other parameters like session IDs, etc. So, that duplicate message will overwrite the existing duplicate record and maintain only one record. Figure 6: Active-Active setup cluster syncing data with MirrorMaker Challenges and Complexities in Active-Active Kafka Setups Message ordering challenges occur when writing to two clusters simultaneously, messages may arrive in different orders due to network delays between regions. This can be particularly problematic for applications that require strict message ordering. So, it needs to be handled in the data processing time or downstream systems. For example, defining a primary key within the message body timestamp field would order the messages based on timestamps. Data Consistency Issues can occur while establishing dual writes to two different clusters. It requires robust monitoring and reconciliation processes to ensure data integrity across clusters. Because there might be conditions where data might successfully write to one cluster but fail in another, resulting in data inconsistency. Producer complexity is an issue in this active-active setup. Producers need to handle writing to two clusters and manage failures independently for each cluster, so isolation of each pipeline is important. This setup might increase the complexity of the producer applications and require more resources to manage dual writes effectively. There are some agents or tools in the market also as open-source offerings that will provide dual pipeline options such as Elastic Logstash. Consumer group management is complex. Managing consumer offsets across multiple clusters could be challenging, especially during failover scenarios, and can lead to message duplication. Consumer applications or downstream systems need to be designed to handle duplicate messages or implement deduplication logic. Operational Overhead - Running active-active clusters requires sophisticated monitoring tools and increases operational costs significantly. Teams need to maintain detailed runbooks and conduct regular disaster recovery drills to ensure smooth failover processes. Network Latency Considerations- Cross-region network latency and bandwidth costs become critical factors in active-active setups. Reliable network connectivity between regions is essential for maintaining data consistency and managing replication effectively. Backup and Restore Backup and restore is a cost-effective architectural pattern compared to Active Standby(AP) or Active Active(AA) DR architectural patterns. It is suitable for non-critical applications with higher RTO and RPO values compared to AA or AP patterns. Backup and restore is a cost-effective option because we need only one Kafka cluster. In case the Kafka cluster is unavailable, the data from the centralized repository can be replayed/restored to the Kafka cluster once it becomes available. In this architectural pattern, shown in Figure 7, all the data will be saved to a persistent location like Amazon S3 or another data lake, where the data can be restored/replayed to the Kafka cluster when the Kafka cluster is available. It is also common to see a new environment of Kafka clusters built together using terraform templates and build a new Kafka cluster and replay data. Figure 7: Backup and restore architectural pattern Challenges and Complexities in Kafka Backup and Restore Setup Recovery time impact is a major consideration; Restoring (aka replaying) large volumes of data to Kafka can take significant time, resulting in a higher RTO. This makes it suitable only for applications that can tolerate longer downtimes. Data consistency challenges are also an issue. When restoring data from backup storage like Amazon S3, maintaining the exact message ordering and partitioning as the original cluster can be challenging. Special attention is needed to ensure data consistency during restoration. Backup performance overhead is a further consideration. Regular backups to external storage can impact the performance of the production cluster. The backup process needs careful scheduling and resource management to minimize impact on production workloads. Storage is key. Managing large volumes of backup data requires effective storage lifecycle policies and controlling costs. Regular cleanup of old backups and efficient storage utilization strategies are essential. Coordination during recovery is mandatory. The recovery process requires careful coordination between backing up offset information and actual messages. Missing either can lead to message loss or duplication during restoration. Conclusion on Kafka DR Strategies As we explored earlier, choosing a DR strategy isn't one-size-fits-all, it comes with trade-offs. It often boils down to finding the right balance between what the business needs and what it can afford. The decision usually depends on several factors. SLAs (e.g., 99.99%+ uptime) are pre-defined commitments to uptime and availability. An SLA requiring 99.99% uptime allows only 52.56 minutes of downtime per year. The RTO metric is used to define the maximum acceptable downtime for a system or application after a failure before it causes unacceptable business harm. Knowledge/content/training systems, for example, can expect one hour or more downtime because they are not mission critical. For banking applications, on the other hand, downtime acceptance could be none or a couple of minutes. The RPO metric is used to define the maximum acceptable amount of data loss during a disruptive event, measured in time from the most recent backup. For example, an observability/monitoring application can have the acceptability for losing the past hour whereas a financial application can not tolerate losing a single record of transaction data. Associated costs vary. Higher availability and faster recovery typically come at a higher price. Organizations must weigh their need for uptime and data protection against the financial investment required. DR Strategy RTO (Downtime tolerance) RPO (Data loss tolerance) Use case Cost Active Active Near-Zero (Seconds) Near-Zero (Seconds) Mission-critical apps (e.g., banking, e-commerce, stock trading) High Active Standby Minutes to Hours Near-Zero to Minutes Enterprise apps need high availability Medium to High (No dual write ingestion pipeline cost) Backup and Restore Hours to Days Hours to Days Non-critical apps with occasional DR needs or cost-sensitive apps, Low References: Kafka documentation ZooKeeper CLI commands ZooKeeper Monitoring Mirror Maker documentation DNS Failover strategies About the Authors Srikanth Daggumalli Nishchai Jayanna Manjula",
  "image": "https://res.infoq.com/articles/apache-kafka-stretch-cluster-failures/en/headerimage/apache-kafka-stretch-cluster-failures-header-1750152098922.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\t\u003ch3\u003eKey Takeaways\u003c/h3\u003e\n\t\t\t\t\t\t\t\t\t\u003cul\u003e\n\t\u003cli\u003eApache Kafka Stretch cluster setup in an on-premise environment poses a high risk of service unavailability during WAN disruptions, often leading to split-brain or brain-dead scenarios and violating Service Level Agreements (SLAs) due to degraded Recovery Time Objective (RTO) and Recovery Point Objective (RPO).\u003c/li\u003e\n\t\u003cli\u003eProactive monitoring of your Kafka environment to understand data skews across brokers is critical; an uneven data load on a single node can cause a stretch cluster failure.\u003c/li\u003e\n\t\u003cli\u003eAn ungraceful broker shutdown during upgrades or due to poor security posture can make the Kafka service unavailable, resulting in a loss of data.\u003c/li\u003e\n\t\u003cli\u003eThere are three popular and widely used Kafka disaster recovery strategies; each has challenges, complexities, and pitfalls.\u003c/li\u003e\n\t\u003cli\u003eKafka Mirror Maker 2 replication lag (Δ) for the Disaster Recovery setup can cause message loss, inconsistent data.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\u003c/div\u003e\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\u003cp\u003eApache Kafka is a well-known publish-subscribe distributed system widely used across industries for use cases such as log analytics, observability, event-driven architectures, and real-time streaming. Its distribution allows it to become the \u003ca href=\"https://www.confluent.io/resources/presentation/monitoring-and-resiliency-testing-our-apache-kafka-clusters-at-goldman-sachs/\"\u003ecritical piece\u003c/a\u003e or \u003ca href=\"https://engineering.linkedin.com/kafka/kafka-linkedin-current-and-future\"\u003ebackbone\u003c/a\u003e of modern streaming architecture, offering many integrations and supporting near-real-time or real-time latency requirements.\u003c/p\u003e\n\n\u003cp\u003eToday, Kafka has been offered by major companies as a service, including Confluent Kafka as well as cloud offerings such as \u003ca href=\"https://www.confluent.io/confluent-cloud/\"\u003eConfluent Cloud\u003c/a\u003e, \u003ca href=\"https://aws.amazon.com/msk/\"\u003eAWS Managed Streaming Kafka\u003c/a\u003e , \u003ca href=\"https://cloud.google.com/products/managed-service-for-apache-kafka?hl=en\"\u003eGoogle Cloud Managed Service for Apache Kafka\u003c/a\u003e, \u003ca href=\"https://learn.microsoft.com/en-us/azure/event-hubs/azure-event-hubs-apache-kafka-overview\"\u003eAzure Event Hub for Apache Kafka\u003c/a\u003e, etc. Additionally, Kafka has been deployed across over one-hundred-thousand companies, including Fortune 100 across various industry segments such as enterprise, financials, healthcare, automobile, startups, independent software vendors, and retail sectors.\u003c/p\u003e\n\n\u003cp\u003eThe value of the data diminishes over time. Many businesses these days compete in time and demand real-time data processing with millisecond responses to power mission-critical applications. Financial services, e-commerce, IoT, and other latency-sensitive industries require highly available and resilient architectures that can withstand infrastructure failures without impacting business continuity.\u003c/p\u003e\n\n\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\u003cdiv data-trk-view=\"true\" data-trk-impr=\"true\" data-place=\"EMBEDDED\"\u003e\n\t\n\t\u003cul\u003e\n\t\t\u003ch4\u003eRelated Sponsored Content\u003c/h4\u003e\n\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\n\t\u003c/ul\u003e\n\t\n\t\t\n\t\n\t\n\u003c/div\u003e\n\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\u003cp\u003eCompanies operating across multiple geographic locations need solutions that provide seamless failover, data consistency, and disaster recovery (DR) capabilities while minimizing operational complexity and cost. One of the common architectural patterns that companies often consider using is a single Kafka cluster spanning multiple data center locations on WAN (Wide Area Network). It is called \u003ca href=\"https://sonamvermani.medium.com/kafka-stretch-clusters-844813df7f49\"\u003eStretch Cluster\u003c/a\u003e. This pattern is commonly considered by companies for \u003ca href=\"https://en.wikipedia.org/wiki/IT_disaster_recovery\"\u003edisaster recovery (DR)\u003c/a\u003e and \u003ca href=\"https://en.wikipedia.org/wiki/High_availability\"\u003ehigh availability (HA)\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eStretch Cluster can theoretically provide data redundancy across regions and minimize data loss in the event of a region-wide failure. However, this approach comes with several \u003cstrong\u003echallenges and trade-offs\u003c/strong\u003e due to the inherent nature of Kafka and its dependency on \u003cstrong\u003enetwork latency\u003c/strong\u003e, \u003cstrong\u003econsistency\u003c/strong\u003e, and \u003cstrong\u003epartition tolerance\u003c/strong\u003e in a multi-region setup. In this article, we are going to focus on the stretch cluster architectural pattern for DR or HA-related implications and considerations.\u003c/p\u003e\n\n\u003cp\u003eThe following testing has been conducted, and the cluster behavior has been documented.\u003c/p\u003e\n\n\u003ch2\u003eEnvironment Details\u003c/h2\u003e\n\n\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\u003ctable\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eRegions\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eLondon, Frankfurt\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eKafka Distribution\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eApache Kafka\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eKafka Version\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003e3.6.2\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eZookeeper\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003e3.8.2\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eOperating system\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eLinux\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch2\u003eHigh-Level Environment Setup View\u003c/h2\u003e\n\n\u003cp\u003eBelow is the high-level view of a single stretch cluster setup that spans across the London and Frankfurt regions with\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003eFour Kafka brokers, where actual data resides.\u003c/li\u003e\n\t\u003cli\u003eFour Zookeeper nodes (a three-node rather than a four-node setup would work as well, but to mimic the exact environment in both the regions, we have set up four Zookeeper nodes ).\u003c/li\u003e\n\t\u003cli\u003eZookeeper follows [N/2] + 1 rule for quorum establishment. In this case, the Zookeeper quorum will be [4/2] + 1 = 2 + 1 = 3.\u003c/li\u003e\n\t\u003cli\u003eNetwork latency between these regions is ~ 15ms.\u003c/li\u003e\n\t\u003cli\u003eMultiple producers (1 to n) and consumers (1 to n) are producing and consuming the data from this cluster from different regions of the world.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/apache-kafka-stretch-cluster-failures/en/resources/10image1-1750154659629.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/apache-kafka-stretch-cluster-failures/en/resources/10image1-1750154659629.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003ch2\u003eKafka Broker Hardware Configuration\u003c/h2\u003e\n\n\u003ctable\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003cth scope=\"row\"\u003e\u003csmall\u003eHard Disk Size\u003c/small\u003e\u003c/th\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003e10 TB\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003cth scope=\"row\"\u003e\u003csmall\u003eMemory\u003c/small\u003e\u003c/th\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003e120 GB\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003cth scope=\"row\"\u003e\u003csmall\u003eCPU\u003c/small\u003e\u003c/th\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003e16 Core\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003cth scope=\"row\"\u003e\u003csmall\u003eDisk Setup\u003c/small\u003e\u003c/th\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eRAID 0\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch2\u003eKafka Broker Configuration\u003c/h2\u003e\n\n\u003cul\u003e\n\t\u003cli\u003eReplication factor set to 4\u003c/li\u003e\n\t\u003cli\u003eData retention (aka log retention) period is 7 days\u003c/li\u003e\n\t\u003cli\u003eAuto-creation of topics allowed\u003c/li\u003e\n\t\u003cli\u003eMinimum in-sync replication (ISR) set to 3\u003c/li\u003e\n\t\u003cli\u003eThe number of partitions per topic is 10\u003c/li\u003e\n\t\u003cli\u003eOther configuration details\n\t\u003cul\u003e\n\t\t\u003cli\u003eAcks = all\u003c/li\u003e\n\t\t\u003cli\u003emax.in.flight.request.per.connection=1\u003c/li\u003e\n\t\t\u003cli\u003ecompression.type=lz4\u003c/li\u003e\n\t\t\u003cli\u003ebatch.size=16384\u003c/li\u003e\n\t\t\u003cli\u003ebuffer.memory=67108864\u003c/li\u003e\n\t\t\u003cli\u003eauto.offset.reset=earliest\u003c/li\u003e\n\t\t\u003cli\u003esend.buffer.bytes=524288\u003c/li\u003e\n\t\t\u003cli\u003ereceive.buffer.bytes=131072\u003c/li\u003e\n\t\t\u003cli\u003enum.replica.fetchers=4\u003c/li\u003e\n\t\t\u003cli\u003enum.network.threads=12\u003c/li\u003e\n\t\t\u003cli\u003enum.io.threads=32\u003c/li\u003e\n\t\u003c/ul\u003e\n\t\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/apache-kafka-stretch-cluster-failures/en/resources/4image2-1750158893490.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/apache-kafka-stretch-cluster-failures/en/resources/4image2-1750158893490.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003ch2\u003eProducers and Consumers:\u003c/h2\u003e\n\n\u003ch3\u003eProducers:\u003c/h3\u003e\n\n\u003cp\u003eIn this use case, we have used a producer written in Java that consistently produces defined (K) Transactions Per Second (aka TPS), which can be controlled using properties files. Sample snippets using Kafka performance test api.\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e******** Start of Producer code snippet *******\n\n void performAction(int topicN){\n        try {\n            ProcessBuilder processBuilder = new ProcessBuilder();\n            Properties prop = new Properties();\n            InputStream input = null;\n            input = new FileInputStream(\u0026#34;config.properties\u0026#34;);\n            prop.load(input);\n\n                SimpleDateFormat sdf = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#39;T\u0026#39;HH:mm:ss.SSS\u0026#34;);\n                sdf.setTimeZone(TimeZone.getTimeZone(\u0026#34;America/New_York\u0026#34;));\n                String startTime = sdf.format(new Date());\n            System.out.println(\u0026#34;Start time ---\u0026gt; \u0026#34;+startTime+\u0026#34; topic \u0026#34; +topicN);\n\n\nString command = \u0026#34;/home/opc/kafka/bin/kafka-producer-perf-test --producer-props bootstrap.servers=x.x.x.x:9092\u0026#34;+ \u0026#34; --topic \u0026#34;+topicN+\n                        \u0026#34; --num-records \u0026#34;+ prop.getProperty(\u0026#34;numOfRecords\u0026#34;) +\n                        \u0026#34; --record-size \u0026#34;+ prop.getProperty(\u0026#34;recordSize\u0026#34;) +\n                        \u0026#34; --throughput \u0026#34;+ prop.getProperty(\u0026#34;throughputRate\u0026#34;) +\n                        \u0026#34; --producer.config \u0026#34;+ \u0026#34;/home/opc/kafka/bin/prod.config\u0026#34;;\n\n                \n                //processBuilder.command(\u0026#34;cmd.exe\u0026#34;, \u0026#34;/c\u0026#34;, \u0026#34;dir C:\\\\Users\\\\admin\u0026#34;);\n                processBuilder.command(\u0026#34;bash\u0026#34;, \u0026#34;-c\u0026#34;, command);\n\n                Process process = processBuilder.start();\n                //StringBuilder output = new StringBuilder();\n                BufferedReader reader = new BufferedReader(\n                        new InputStreamReader(process.getInputStream()));\n               \n                String line;\n                while ((line = reader.readLine()) != null) {\n                    //output.append(line + \u0026#34;\\n\u0026#34;);\n                    if(line!=null \u0026amp;\u0026amp; line.contains(\u0026#34;99.9th.\u0026#34;)) {\n                            break;\n                        }   \n                }\n\n                int exitVal = process.waitFor();\n                if (exitVal == 0) {\n                System.out.println(\u0026#34;Success!\u0026#34;);\n                String endTime = sdf.format(new Date());\n        //send to opensearch for tracking purpose - optional step\n                pushToIndex(line,startTime, endTime, prop.getProperty(\u0026#34;indexname\u0026#34;));\n                //System.out.println(output);\n                //System.exit(0);\n                } else {\n                    System.out.println(\u0026#34;Something unexpected happend!!!\u0026#34;);\n                }\n\n        } catch (IOException e) {\n            e.printStackTrace();\n        } catch(Exception e){\n            e.printStackTrace();\n        }\n    }\n\n********** End of producer code snippet **************\n\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e****** Stat of Producer properties file ***** \nnumOfRecords=70000\nrecordSize=4000\nthroughputRate=20\ntopicstart=1\ntopicend=100\n****** End of Producer properties file ***** \u003c/code\u003e\u003c/pre\u003e\n\n\u003ch3\u003eConsumers:\u003c/h3\u003e\n\n\u003cp\u003eIn this use case, we have used consumers written in Java that consistently consume defined (K) transactions per second (TPS), which can be controlled using properties files.\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e***** Start of consumer code snippet ******* \n    void performAction(int topicN){\n        try {\n            ProcessBuilder processBuilder = new ProcessBuilder();\n            Properties prop = new Properties();\n            InputStream input = null;\n            input = new FileInputStream(\u0026#34;consumer.properties\u0026#34;);\n            prop.load(input);\n\n                SimpleDateFormat sdf = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#39;T\u0026#39;HH:mm:ss.SSS\u0026#34;);\n                sdf.setTimeZone(TimeZone.getTimeZone(\u0026#34;America/New_York\u0026#34;));\n                String startTime = sdf.format(new Date());\n            System.out.println(\u0026#34;Start time ---\u0026gt; \u0026#34;+startTime+\u0026#34; topic \u0026#34; +topicN);\n\nString command =null;\n            if( prop.getProperty(\u0026#34;latest\u0026#34;).equalsIgnoreCase(\u0026#34;no\u0026#34;)) {\n         \ncommand = \u0026#34;/home/opc/kafka/bin/kafka-consumer-perf-test --broker-list=x.x.x.x:9092\u0026#34;+\n                        \u0026#34; --topic \u0026#34;+topicN+\n                        \u0026#34; --messages \u0026#34;+ prop.getProperty(\u0026#34;numOfRecords\u0026#34;) +\n                       // \u0026#34; --group \u0026#34;+ prop.getProperty(\u0026#34;groupid\u0026#34;) +\n                        \u0026#34; --timeout \u0026#34;+ prop.getProperty(\u0026#34;timeout\u0026#34;)+\n            \u0026#34; --consumer.config \u0026#34;+\u0026#34;/home/opc/kafka/bin/cons.config\u0026#34;;\n}else{\n\ncommand = \u0026#34;/home/opc/kafka/bin/kafka-consumer-perf-test --broker-list=x.x.x.x:9092\u0026#34;+\u0026#34; --topic \u0026#34;+topicN+\n                        \u0026#34; --messages \u0026#34;+ prop.getProperty(\u0026#34;numOfRecords\u0026#34;) +\n                       // \u0026#34; --group \u0026#34;+ prop.getProperty(\u0026#34;groupid\u0026#34;) +\n                        \u0026#34; --timeout \u0026#34;+ prop.getProperty(\u0026#34;timeout\u0026#34;)+\n            \u0026#34; --consumer.config \u0026#34;+\u0026#34;/home/opc/kafka/bin/cons.config\u0026#34;+\n                        \u0026#34; --from-latest\u0026#34;;                \n            }\n\nSystem.out.println(\u0026#34;command is ---\u0026gt; \u0026#34; +command);                \n                //processBuilder.command(\u0026#34;cmd.exe\u0026#34;, \u0026#34;/c\u0026#34;, \u0026#34;dir C:\\\\Users\\\\admin\u0026#34;);\n                processBuilder.command(\u0026#34;bash\u0026#34;, \u0026#34;-c\u0026#34;, command);\n\n                Process process = processBuilder.start();\n                StringBuilder output = new StringBuilder();\n                BufferedReader reader = new BufferedReader(\n                        new InputStreamReader(process.getInputStream()));\n               \n                String line;\n                while ((line = reader.readLine()) != null) {\n                   // output.append(line + \u0026#34;\\n\u0026#34;+\u0026#34;test\u0026#34;);\n                    //if(line.contains(\u0026#34;:\u0026#34;) \u0026amp;\u0026amp; line.contains(\u0026#34;-\u0026#34;)) {\n                      if (line.contains(\u0026#34;:\u0026#34;) \u0026amp;\u0026amp; line.contains(\u0026#34;-\u0026#34;) \u0026amp;\u0026amp; !line.contains(\u0026#34;WARNING:\u0026#34;)) {\n                            break;\n                        }   \n                }\n\n                int exitVal = process.waitFor();\n                if (exitVal == 0) {\n                System.out.println(\u0026#34;Success!\u0026#34;);\n               //optional step - send data to opensearch\n                 pushToIndex(line, prop.getProperty(\u0026#34;indexname\u0026#34;),topicN);\n                System.out.println(output);\n                //System.exit(0);\n                } else {\n                    System.out.println(\u0026#34;Something unexpected happend!!!\u0026#34;);\n                }\n\n        } catch (IOException e) {\n            e.printStackTrace();\n        } catch(Exception e){\n            e.printStackTrace();\n        }\n    }\n\n***** End of consumer code snippet *******\n\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e*****Start of Consumer properties snippet **** \n\nnumOfRecords=70000\ntopicstart=1\ntopicend=100\ngroupid=cgg1\nreportingIntervalms=10000\nfetchThreads=1\nlatest=no\nprocessingthreads=1\ntimeout=60000\n\n***** End of consumer properties snippet ****\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch2\u003eCAP Theorem\u003c/h2\u003e\n\n\u003cp\u003eBefore diving into further discussions and scenario executions, it\u0026#39;s crucial to understand the \u003cstrong\u003efundamental algorithms of distributed systems\u003c/strong\u003e. This foundational knowledge will help you better \u003cstrong\u003ecorrelate WAN disruptions\u003c/strong\u003e with their impact on system behavior.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed/\"\u003eCAP Theorem\u003c/a\u003e (i.e., \u003cstrong\u003eConsistency, Availability, and Partition tolerance\u003c/strong\u003e) applies to all the distributed systems like Kafka, Hadoop, Elasticsearch, and many others, where data is being distributed across nodes to:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003eEnable parallel data processing\u003c/li\u003e\n\t\u003cli\u003eMaintain high availability for data and service\u003c/li\u003e\n\t\u003cli\u003eSupport horizontal or vertical scaling requirements\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ctable\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eConsistency (C)\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cp\u003e\u003csmall\u003eEvery read gets the latest write or an error.\u003c/small\u003e\u003c/p\u003e\n\t\t\t\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eKafka follows eventual consistency; data ingested into one node eventually syncs across others. Reads happen from the leader, so replicas might lag behind.\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eAvailability (A)\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cp\u003e\u003csmall\u003eEvery request gets a response, even if some nodes fail.\u003c/small\u003e\u003c/p\u003e\n\t\t\t\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eKafka ensures high availability using replication factor, min.insync.replicas, and acks. As long as a leader is available, Kafka keeps running.\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003ePartition Tolerance (P)\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cp\u003e\u003csmall\u003eThe system keeps working despite network issues.\u003c/small\u003e\u003c/p\u003e\n\t\t\t\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eEven if some nodes disconnect, Kafka elects a new leader and continues processing.\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eKafka is an \u003cstrong\u003eAP system\u003c/strong\u003e; it prioritizes \u003cstrong\u003eAvailability\u003c/strong\u003e and \u003cstrong\u003ePartition Tolerance\u003c/strong\u003e, while consistency is \u003cstrong\u003eeventual\u003c/strong\u003e and tunable via settings.\u003c/p\u003e\n\n\u003cp\u003eNow, let\u0026#39;s get into the scenarios.\u003c/p\u003e\n\n\u003ch2\u003eStretch Cluster failed scenarios\u003c/h2\u003e\n\n\u003cp\u003eBefore going in-depth into scenarios, let\u0026#39;s understand the key components and terminology of Kafka internals.\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\u003cstrong\u003eBrokers\u003c/strong\u003e represent data nodes where all the data is distributed with replicas for data high availability.\u003c/li\u003e\n\t\u003cli\u003eThe \u003cstrong\u003eController\u003c/strong\u003e is a critical component of Kafka that maintains the cluster\u0026#39;s metadata, including information about topics, partitions, replicas, and brokers. Also performs administrative tasks. Think of it as the brain of a Kafka cluster.\u003c/li\u003e\n\t\u003cli\u003eA \u003cstrong\u003eZookeeper quorum\u003c/strong\u003e is a group of nodes (usually deployed on dedicated nodes) that act as a coordination service helping to manage the Controller election, Broker Registration, Broker Discovery, Cluster membership, Partition leader election, Topic configuration, Access Control List (ACLs), consumer group management, and offset management.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eTopics\u003c/strong\u003e are logical constructs of data to which all related data is published. Examples include all the network logs published to the \u0026#34;\u003ccode\u003eNetwork_logs\u003c/code\u003e\u0026#34; topic and application logs published to the \u0026#34;\u003ccode\u003eApplication_logs\u003c/code\u003e\u0026#34; topic.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003ePartitions\u003c/strong\u003e are the fundamental units or building blocks supporting scalability and parallelism within a topic. When a topic is created, it can have one or more partitions where all the data is published in order.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eOffsets\u003c/strong\u003e are unique identifiers assigned to each message within a partition of a topic, essentially a sequential number that indicates the position of a message within the partition.\u003c/li\u003e\n\t\u003cli\u003eA \u003cstrong\u003eproducer\u003c/strong\u003e is a client application that publishes records (messages) to Kafka topics.\u003c/li\u003e\n\t\u003cli\u003eA \u003cstrong\u003econsumer\u003c/strong\u003e is a client application that consumes records (messages) from Kafka topics.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eScenario 1: WAN Disruption – Active Controllers as \u0026#39;0\u0026#39; – AKA Brain-Dead Scenario\u003c/h2\u003e\n\n\u003ctable\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eScenario\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eWAN Disruption – Active controllers as \u0026#39;0\u0026#39;.\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eScenario Description\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eAs discussed in earlier sections, in a distributed network setup across geographic locations, network service interruptions can occur because no service guarantees 100% availability.\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cp\u003e\u003csmall\u003eThis scenario focuses on WAN disruption between two regions, causing Kafka service unavailability, leaving the cluster with \u003cstrong\u003ezero active controllers\u003c/strong\u003e and no self-recovery, even after hours of waiting.\u003c/small\u003e\u003c/p\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eExecution Steps\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003col\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eKafka cluster (all brokers and Zookeeper quorum) should be up and running with the configuration stated in the environment section\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eProduce and consume events with brokers\u0026#39; equilibrium state (this step is optional, but simulating a real use case)\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eDisconnect/disrupt WAN service between regions\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eObserve the cluster behavior. Check through the broker host CLI and ZK CLI. If you are using Confluent, you can check in Confluent center or AWS Cloud provided managed service you can check the same CloudWatch, respectively and other offerings too \u003c/small\u003e\u003csmall\u003e \u003c/small\u003e\n\t\t\t\t\u003cul\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003ea. ZK quorum availability\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eb. Brokers availability as cluster members\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003ec. Number of active controllers\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003ed. Producers and consumers progress\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003ee. Under replicated partitions, etc.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ul\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eReestablish WAN service between regions and observe the cluster behavior.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ol\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eExpected Behavior\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003col\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eOn steady state, the Kafka cluster should be available with \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003e4 brokers\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum as 3\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controllers as 1\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers should produce and fetch events from topics\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003csmall\u003e \u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eUpon WAN service disruption, \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eThe Kafka cluster should not be available\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum 3 will break\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controllers as 1 or zero\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers should be interrupted, create back pressure or time-outs.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003csmall\u003e \u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eReestablish WAN service \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum 3 should be reestablished\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eKafka cluster should be available with 4 brokers\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eThe active controller should be 1\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers should continue/resume\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003csmall\u003e \u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ol\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eActual/Observed Behavior\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eKafka is unavailable and can\u0026#39;t be recovered on its own upon WAN service disruption and reestablishment and shows active controllers as \u0026#39;0\u0026#39;\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eActive brokers are reported as zero\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controller number is zero\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum with 3 is formed on WAN service resume, and the quorum broke on disruption\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eAll brokers showed up in ZK cluster membership upon WAN service resume and only two brokers showed during WAN disruption.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers completely interrupted and timed out.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eRoot Cause Analysis (RCA)\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eIt\u0026#39;s a \u0026#34;\u003cstrong\u003ebrain-dead\u003c/strong\u003e\u0026#34; scenario. Upon WAN disruption, a single 4-node cluster becomes 2 independent clusters, each with 2 nodes.\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eAnd the existing single Zookeeper quorum becomes two quorums i.e., one for each site. But each other quorum does not elect a new controller, resulting as it thinks that the controller already existing resulting in zero controllers upon WAN reestablishment.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eImagine two cooks in a kitchen preparing a dish. Each cook assumes the other has added the salt, but in the end, no salt is added to the dish.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eFrom the logs, it was observed controller moved the exception from one broker to another broker, but no exit or hang was reported in the logs\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\n\t\t\t\u003cpre\u003e\u003ccode\u003eController exceptions:\ncontroller.log:org.apache.kafka.common.errors.ControllerMovedException: Controller moved to another broker. Aborting \ncontroller.log:java.io.IOException: Client was shutdown before response was read\ncontroller.log:java.lang.InterruptedException\u003c/code\u003e\u003c/pre\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eFrom ZooKeeper (ZK) CLI, It\u0026#39;s also observed that \u0026#39;no broker claims to be the controller\u0026#39; in the cluster, the cluster failed to respond properly in the face of state changes, including topic or partition creation or broker failures.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\n\t\t\t\u003cpre\u003e\u003ccode\u003eERROR Unexpected exception causing shutdown while sock still open (org.apache.zookeeper.server.quorum.LearnerHandler)\njava.lang.Exception: shutdown Leader! reason: Not sufficient followers synced, only synced with sids: [ 2 ]\n\nERROR Exception while listening (org.apache.zookeeper.server.quorum.QuorumCnxManager)\u003c/code\u003e\u003c/pre\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eReferences\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eWAN disable can be achieved through route tables modify or delete.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eZK commands. Refer ZK command section.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eSolution/Remedy\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eBecause there are no controller claims by brokers, all the brokers need to be restarted.\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch3\u003eWAN Disruption:\u003c/h3\u003e\n\n\u003cp\u003e\u003ccode\u003eWAN disruption can be done by deleting or modifying route tables.\u003c/code\u003e\u003cbr/\u003e\n\u003cstrong\u003eWarning\u003c/strong\u003e: Deleting a route table may disrupt all traffic in the associated subnets. Instead, consider modifying or detaching the route table.\u003c/p\u003e\n\n\u003ch3\u003eFirewall Stop\u003c/h3\u003e\n\n\u003cp\u003eYou may need to shut down firewalls\u003cbr/\u003e\nFirewall status: \u003ccode\u003esystemctl status firewalld\u003c/code\u003e\u003cbr/\u003e\nFirewall stop: \u003ccode\u003eservice firewalld stop\u003c/code\u003e\u003c/p\u003e\n\n\u003ch3\u003eZK Commands list:\u003c/h3\u003e\n\n\u003cp\u003eZK Server start: \u003ccode\u003ebin/zookeeper-server-start etc/kafka/zookeeper.properties\u003c/code\u003e\u003cbr/\u003e\nZK Server stop: \u003ccode\u003ezookeeper-server-stop\u003c/code\u003e\u003cbr/\u003e\nZK Shell commands: \u003ccode\u003ebin/zookeeper-shell.sh \u0026lt;zookeeper host\u0026gt;:2181\u003c/code\u003e\u003cbr/\u003e\nActive controllers check: \u003ccode\u003e\u003ctt\u003eget /controller or ls /controller\u003c/tt\u003e\u003c/code\u003e\u003cbr/\u003e\nBrokers list: \u003ccode\u003e/home/opc/kafka/bin/zookeeper-shell kafkabrokeraddress:2181 \u0026lt;\u0026lt;\u0026lt;\u0026#34;ls /brokers/ids\u0026#34;\u003c/code\u003e\u003c/p\u003e\n\n\u003cp\u003eAlternative to check controllers and brokers in a cluster. Available from kafka 3.7.x\u003cbr/\u003e\n\u003ccode\u003ebin/kafka-metadata-shell.sh --bootstrap-server \u0026lt;KAFKA_BROKER\u0026gt;:9092 --describe-cluster\u003c/code\u003e\u003c/p\u003e\n\n\u003ch2\u003eScenario 2: WAN Disruption - Active Controllers as \u0026#39;2\u0026#39; – AKA Split-Brain Scenario\u003c/h2\u003e\n\n\u003ctable\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eScenario\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eWAN Disruption - Active controllers as \u0026#39;2\u0026#39;\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eScenario Description\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eAs discussed in earlier sections, in a distributed network setup across geographic locations, network service interruptions can occur because no service guarantees 100% availability. This scenario focuses on WAN disruption between two regions, causing Kafka service unavailability with active controllers as \u0026#39;2\u0026#39; and no self-recovery, even after hours of waiting.\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eExecution Steps\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003col\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eKafka Cluster (all brokers and Zookeeper quorum) should be up and running with the configuration stated in the environment section\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eProduce and consume events with brokers\u0026#39; equilibrium state (this step is optional)\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eDisconnect/Disrupt WAN service between regions\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eObserve the cluster behavior- Check through the broker host CLI and ZK CLI. If you are using Confluent, you can check in the Confluent center or the AWS Cloud provided managed service, you can check the same CloudWatch respectively \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum availability\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eBrokers\u0026#39; availability as cluster members\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eNumber of active controllers\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers progress\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eUnder replicated partitions etc.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eReestablish WAN service between regions and observe the cluster behavior.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ol\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eExpected Behavior\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003col\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eIn a steady state, the Kafka cluster should be available with \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003e4 brokers\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum as 3\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controllers as 1\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers should produce and fetch events from topics\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eUpon WAN service disruption, \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eThe Kafka cluster should not be available\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum 3 will break\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controllers as 1 or zero\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers should be interrupted or create back pressure or time-outs.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eRe-establish WAN service \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum 3 should be reestablished\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eKafka cluster should be available with 4 brokers\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controller should be 1\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers should continue/resume\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ol\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eActual/Observed Behavior\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eKafka is not available and can\u0026#39;t be recovered on its own upon WAN service disruption and re-establishment and shows active controllers as \u0026#39;2\u0026#39;\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eActive brokers are reported as zero\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controller number is two\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum with 3 is formed on WAN service resume and the quorum broke on disruption\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eAll brokers showed up in ZK cluster membership upon WAN service resume and only two brokers showed during WAN disruption.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers completely interrupted and timed out. \u003ccode\u003eNETWORK_EXCEPTIONs\u003c/code\u003e\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers got failed requests with \u003ccode\u003eNOT_LEADER_FOR_PARTITIONS\u003c/code\u003e exceptions\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eRoot Cause Analysis (RCA)\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eThis is a split-brain scenario. Upon WAN disruption, a single 4-node cluster becomes 2 independent clusters with each consisting of 2 nodes.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eAlso, an existing single Zookeeper quorum becomes two quorums, i.e., one for each site. As a result, each quorum will elect a new controller, resulting in two controllers.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eUpon WAN re-establishment, we see 2 controllers for 4 4-node clusters. Think of it like two co-pilots flying one plane from different cockpits after radio loss. Both think the other is gone and take over, conflicting actions crash the system.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eFrom ZooKeeper (ZK) CLI, it\u0026#39;s also observed that two brokers claim to be the controller in the cluster; the cluster failed to respond properly in the face of state changes, including topic or partition creation or broker failures.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eReferences\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eWAN disable can be achieved through the route tables delete\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eZK commands\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eSolution/Remedy\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eRestart both controllers\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch2\u003eScenario 3: Broker Disk full, resulting respective broker process crash and No Active controllers\u003c/h2\u003e\n\n\u003ctable\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eScenario\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eBroker disk full, resulting respective broker process crash, and No active controllers or \u0026#39;0\u0026#39; active controllers.\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eScenario Description\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eOn a steady Kafka cluster, which means all the brokers are up and running, and both producers and consumers are producing and consuming events. One of the 4 brokers\u0026#39; disks is full due to \u003cstrong\u003edata skew\u003c/strong\u003e, i.e., \u003cstrong\u003euneven or imbalanced distribution of data across\u003c/strong\u003e and leading to an abrupt broker process crash and cascading the crash to the remaining brokers. This causes the Kafka cluster unavailability because of active controllers as \u0026#39;0\u0026#39;.\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eExecution Steps\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003col\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eKafka Cluster (all brokers and Zookeeper quorum) should be up and running with the configuration stated in the environment section\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eProduce and consume events with brokers\u0026#39; equilibrium state\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eOne of the 4 brokers\u0026#39; disks is full and causing the respective broker process to crash\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eObserve the cluster behavior through the broker host CLI or ZK CLI. If you are using Confluent, you can check in the Confluent center or the AWS Cloud provided managed service you can check the same CloudWatch respectively \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum availability\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eBrokers availability as cluster members\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eNumber of active controllers\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers progress\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eUnder replicated partitions, etc.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ol\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eExpected Behavior\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003col\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eIn a steady state, Kafka cluster should be available with \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003e4 brokers\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum as 3\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controllers as 1\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers should produce and fetch events from topics\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eUpon one broker\u0026#39;s disk full, \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eDisk error should be logged to logs.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eKafka cluster should not be available; only one broker should be shown as not available or out of cluster membership\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum shouldn\u0026#39;t fail\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controllers can be zero (if the controller node is the same as a dead broker) and move the controller to another available broker.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eThe cluster should report under replicated partitions due to one broker unavailability.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers should be interrupted, create back pressure, or time-outs.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ol\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eActual/Observed Behavior\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eKafka is not available and couldn\u0026#39;t be recovered on its own upon and showed active controllers as \u0026#39;0\u0026#39;\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eActive brokers are reported as zero\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eThe active controller number is zero\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eNo impact on Zookeeper quorum\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers completely interrupted and timed out\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eRoot Cause Analysis (RCA)\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eFrom the logs, observed disk-related errors and exceptions\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cpre\u003e\u003ccode\u003elogs/state-change.log:102362: ERROR [Broker id=1] Skipped the become-follower state change with correlation id 331 from controller 1 epoch 1 for partition topic1-9 (last update controller epoch 1) with leader 5 since the replica for the partition is offline due to disk error org.apache.kafka.common.errors.KafkaStorageException: Error while creating log for topic1-9 in dir /kafka/kafka-logs (state.change.logger)\u003c/code\u003e\u003c/pre\u003e\n\n\t\t\t\u003cp\u003e\u003csmall\u003eLogs show that the controller moved the exception from one broker to another broker, but no exit or hang was reported in the logs \u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cpre\u003e\u003ccode\u003eController exceptions: controller.log:org.apache.kafka.common.errors.ControllerMovedException: Controller moved to another broker. Aborting  controller.log:java.io.IOException: Client was shutdown before response was read controller.log:java.lang.InterruptedException\u003c/code\u003e\u003c/pre\u003e\n\n\t\t\t\u003cp\u003e\u003csmall\u003eControllers \u0026#39;0\u0026#39; behavior is equal to the brain-dead scenario explained in scenario 2.\u003c/small\u003e\u003c/p\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eReferences\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eZK active controller commands\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eZK broker membership commands\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eSolution/Remedy\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eIt is essential to monitor your Kafka cluster with proper metrics such as CPU, Disk, Memory, Java Heap, etc. Once the disk reaches 80% or more, you should scale up your cluster.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eBecause there are no controller claims by brokers, all the brokers need to be restarted.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch2\u003eScenario 4: Corrupted indices upon Kafka process kill\u003c/h2\u003e\n\n\u003ctable\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eScenario\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eCorrupted indices upon Kafka process kill\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eScenario Description\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eKilling broker and ZK processes (an uncontrolled shutdown can be comparable with machine abrupt reboot) and restarting brokers lead to corrupted indices, rebuilding indices doesn\u0026#39;t work even after hours of waiting.\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cp\u003e\u003csmall\u003eThis scenario is comparable to an ungraceful shutdown during an environment upgrade or to a poor security posture, where an unauthorized person could terminate the service.\u003c/small\u003e\u003c/p\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eExecution Steps\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003col\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eKafka Cluster (all brokers and Zookeeper quorum) should be up and running with the configuration stated in the environment section\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eProduce and consume events with brokers\u0026#39; equilibrium state\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eKill the broker and ZK processes, and restart the processes\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eObserve the cluster behavior through the broker host CLI or ZK CLI. If you are using Confluent, you can check in the Confluent center or the AWS Cloud provided managed service you can check the same CloudWatch respectively \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum availability\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eBrokers\u0026#39; availability as cluster members\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eNumber of active controllers\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers progress\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eUnder replicated partitions etc.\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ol\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eExpected Behavior\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003col\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eIn a steady state, the Kafka cluster should be available with \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003e4 brokers\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum is 3\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controllers as 1\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers should produce and fetch events from topics\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eKill Kafka processes (kill -9), i.e., both broker and ZK processes on all 4 nodes and restart processes \u003c/small\u003e\n\t\t\t\t\u003col type=\"a\"\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eZK quorum should be formed\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controllers as \u0026#39;1\u0026#39;\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eIn-flight messages may be lost if not flushed to the disk\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eMay result in corrupted indices, but the broker rebuilds corrupted indices on its own without any manual work\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eUpon rebuild of corrupted indices, the Kafka cluster should be available\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eThe cluster might show under replicated partitions due to corrupted indices\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers will get exceptions or errors and can\u0026#39;t continue to perform their jobs\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003c/ol\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\u003c/ol\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eActual/Observed Behavior\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eKafka is not available and couldn\u0026#39;t be recovered on its own.\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eActive brokers are \u0026#39;4\u0026#39;\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eActive controller number is \u0026#39;1\u0026#39;\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eNo impact on Zookeeper quorum\u003c/small\u003e\u003c/li\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eProducers and consumers completely interrupted and timed out\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eRoot Cause Analysis (RCA)\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cp\u003e\u003csmall\u003eFrom the logs, \u0026#39;index corrupted\u0026#39; logs for partitions are shown, and also \u0026#39;rebuilding indices logs\u0026#39;. Rebuilding indices didn\u0026#39;t work and doesn\u0026#39;t show any related logs on progress. The resulting cluster is in unavailable status even after 1 hour.\u003c/small\u003e\u003c/p\u003e\n\n\t\t\t\u003cpre\u003e\u003ccode\u003eWARN [Log partition=topic1-13, dir=/kafka/new/kafka-logs] Found a corrupted index file corresponding to log file /kafka/new/kafka-logs/topic1-13/00000000000000000000.log due to Corrupt time index found, time index file (/kafka/new/kafka-logs/topic1-13/00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553657637686}, recovering segment and rebuilding index files... (kafka.log.Log)\u003c/code\u003e\u003c/pre\u003e\n\n\t\t\t\u003cp\u003e\u003csmall\u003eAlso, it is recommended to do \u003cstrong\u003egraceful shutdowns\u003c/strong\u003e instead of using the kill switch or doing abrupt shutdowns so the data in the Kafka local cache will be flushed to the disks by maintaining meta data information and also committing offsets.\u003c/small\u003e\u003c/p\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eReferences\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\n\t\t\t\u003cul\u003e\n\t\t\t\t\u003cli\u003e\u003csmall\u003eNot applicable\u003c/small\u003e\u003c/li\u003e\n\t\t\t\u003c/ul\u003e\n\t\t\t\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eSolution/Remedy\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eIf Kafka fails to rebuild all the indices on its own, manual deletion of all index files and restarting all brokers will fix the issue.\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch2\u003eConclusion on WAN disruptions failure scenarios\u003c/h2\u003e\n\n\u003cp\u003eDeploying \u003cstrong\u003eon-premise stretch clusters\u003c/strong\u003e requires a deep understanding of failure scenarios, particularly during network disruptions. Unlike cloud environments, on-premises setups may lack \u003cstrong\u003ehighly available network infrastructure, fault tolerance mechanisms, or dedicated fiber lines\u003c/strong\u003e, making them more susceptible to outages. In such cases, a Kafka cluster may experience \u003cstrong\u003edowntime\u003c/strong\u003e if network connectivity is disrupted.\u003c/p\u003e\n\n\u003cp\u003eCloud providers, on the other hand, offer \u003cstrong\u003ehigh-bandwidth, low-latency networking\u003c/strong\u003e over fully redundant, dedicated metro fiber connections between regions and data centers. Their \u003cstrong\u003emanaged Kafka services\u003c/strong\u003e are designed for high availability, typically recommending deployment across multiple zones or regions to withstand zonal failures. Additionally, these services incorporate \u003cstrong\u003ezone-awareness features\u003c/strong\u003e, ensuring Kafka data replication occurs across multiple zones, rather than being confined to a single one.\u003c/p\u003e\n\n\u003cp\u003eIt is also important to understand and emphasize the latency impact on a stretch cluster deployed in two different Geo locations with a WAN connection. We can not escape physics; in our case, as per \u003ca href=\"https://wondernetwork.com/pings/London\"\u003eWonderNetwork\u003c/a\u003e, the average latency between London and Frankfurt is 14.88 milliseconds for a distance of 636km. So, for mission-critical applications, the setup of a stretch cluster driven by latencies, through testing, is needed to determine how the data is being replicated and consumed across.\u003c/p\u003e\n\n\u003cp\u003eBy understanding these differences, organizations can make informed decisions about whether an \u003cstrong\u003eon-premise stretch cluster\u003c/strong\u003e aligns with their \u003cstrong\u003eavailability and resilience\u003c/strong\u003e requirements or if a \u003cstrong\u003ecloud-native Kafka deployment\u003c/strong\u003e is a more suitable alternative.\u003c/p\u003e\n\n\u003ch2\u003eDisaster Recovery Strategies:\u003c/h2\u003e\n\n\u003cp\u003eA Business Continuity Plan (BCP) is essential for any business to maintain its brand and reputation. Almost all the companies think of High Availability (HA) and Disaster recovery (DR) strategies to meet their BCP. Below are the popular DR Strategies for Kafka.\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003eActive – Standby (aka Active-passive)\u003c/li\u003e\n\t\u003cli\u003eActive – Active\u003c/li\u003e\n\t\u003cli\u003eBackup and restore\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBefore we dive deep into DR strategies further, let\u0026#39;s understand that the Kafka Mirror Maker 2.0 we are using in the following architectural patterns. There are other components like Kafka Connect, Replicator, etc., offered by Confluent and other cloud services, but they are out of scope for this article.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://dnsspy.io/learning/dns-availability/dns-failover-strategies\"\u003eDNS Redirection\u003c/a\u003e in Apache Kafka is a \u003cstrong\u003ecross-cluster replication tool\u003c/strong\u003e that uses the \u003cstrong\u003eKafka Connect framework\u003c/strong\u003e to replicate topics between clusters, supporting \u003cstrong\u003eactive-active\u003c/strong\u003e and \u003cstrong\u003eactive-passive\u003c/strong\u003e setups. It provides \u003cstrong\u003eautomatic consumer group synchronization\u003c/strong\u003e, \u003cstrong\u003eoffset translation\u003c/strong\u003e, and \u003cstrong\u003efailover support\u003c/strong\u003e for disaster recovery and multi-region architectures.\u003c/p\u003e\n\n\u003ch3\u003eActive Standby\u003c/h3\u003e\n\n\u003cp\u003eThis is a popular DR strategy pattern where there will be two different Kafka clusters, which are of identical size \u0026amp; configuration, deployed in two different Geo regions or Data centers. In this case, it\u0026#39;s the London and Frankfurt regions. Usually, Geo location choice depends upon Earth\u0026#39;s different tectonic plates, company or country compliance and regulations requirements, and other factors. Out of which only one cluster is active at a given point of time, which receives and serves the clients.\u003c/p\u003e\n\n\u003cp\u003eAs shown in the following picture, Kafka cluster one (KC1) is located in the London region and Kafka cluster two (KC2) is located in the Frankfurt region with the same size and configuration.\u003c/p\u003e\n\n\u003cp\u003eAll the producers are producing data to KC1 (London region cluster), and all consumers are consuming the data from the same cluster. KC2 (Frankfurt region cluster) receives the data from the KC1 cluster through Mirror Maker as shown in Figure 1.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/apache-kafka-stretch-cluster-failures/en/resources/118figure-1-1750163022629.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/apache-kafka-stretch-cluster-failures/en/resources/118figure-1-1750163022629.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eFigure 1: Kafka Active Standby (aka Active Passive) cluster setup with Mirror role\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003cp\u003eIn simple terms, we can treat the mirror maker as a combination of producer and consumer, where it consumes the topics from the source cluster KC1 (London region) and produces the data to the destination cluster KC2 (Frankfurt region).\u003c/p\u003e\n\n\u003cp\u003eDuring the disaster to the primary/active cluster KC1 (London region), the standby cluster (Frankfurt region) will become active, and all the producers and consumers\u0026#39; traffic redirected to the standby cluster. This can be achieved through DNS redirection via a load balancer in front of Kafka clusters. Fig 2 shows this scenario.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/apache-kafka-stretch-cluster-failures/en/resources/93figure-2-1750163022629.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/apache-kafka-stretch-cluster-failures/en/resources/93figure-2-1750163022629.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eFigure 2: Kafka Active Standby setup - Standby cluster becomes active\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen the London Region KC1 cluster becomes active, we need to set up a mirror maker instance to consume the data from the Frankfurt region cluster and produce back to it to the London region KC1 cluster. This scenario is shown in Fig. 3.\u003c/p\u003e\n\n\u003cp\u003eIt\u0026#39;s up to the business to keep the Frankfurt region (KC2) active or promote the London region cluster (KC1) back to primary once the data is fully synced/hydrated to the current state. In some cases, the standby cluster, i.e., the Frankfurt region cluster (KC2), continues to function like an active cluster and the London region cluster (KC1) becomes the standby as well.\u003c/p\u003e\n\n\u003cp\u003eIt is essential to note that RTO (Recovery Time Objective) and RPO (Recovery Point Objective) values are high compared to the Active-Active setup. Active-standby setup cost is medium to high as we need to maintain the same Kafka clusters in both regions.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/apache-kafka-stretch-cluster-failures/en/resources/78figure-3-1750163022629.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/apache-kafka-stretch-cluster-failures/en/resources/78figure-3-1750163022629.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eFigure 3: Kafka Active Standby setup – Standby cluster data sync happening to the KC1\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003ch2\u003eConcerns with the Active-Standby setup\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003eKafka Mirror Maker Replication Lag (Δ)\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eLet\u0026#39;s take a scenario, and it might happen very rarely, where we lose all the cluster completely, including nodes and disks, etc. Where the Active Kafka Cluster (London region - KC1) has \u0026#39;m\u0026#39; messages, and the mirror maker is consuming and producing to the standby cluster. Due to network latency or consumers \u0026amp; producing process with acknowledgements, the standby cluster (i.e., Frankfurt region - KC2) lags behind and holds only \u0026#39;n\u0026#39; messages. Which means the delta(\u003cspan\u003e\\(\\Delta\\)\u003c/span\u003e) is m minus n.\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003e\\(\\Delta = m - n\\)\u003c/span\u003e\u003c/p\u003e\n\n\u003cp\u003ewhere:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\u003cstrong\u003em\u003c/strong\u003e = Messages in the Active Kafka Cluster (London)\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003en\u003c/strong\u003e = Messages in the Standby Kafka Cluster (Frankfurt)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eDue to this replication lag (\u003cspan\u003e\\(\\Delta\\)\u003c/span\u003e), it will have the following impact on mission-critical applications\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003ePotential Message Loss\u003c/strong\u003e:\u003cbr/\u003e\nIf a failover occurs \u003cstrong\u003ebefore\u003c/strong\u003e the Standby cluster catches up, \u003cstrong\u003esome messages (\u003cspan\u003e\\(\\Delta\\)\u003c/span\u003e)\u003c/strong\u003e may be missing.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eInconsistent Data\u003c/strong\u003e:\u003cbr/\u003e\nConsumers relying on the Standby cluster may process \u003cstrong\u003eincomplete or outdated\u003c/strong\u003e data.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eDelayed Processing\u003c/strong\u003e:\u003cbr/\u003e\nApplications depending on real-time replication may experience \u003cstrong\u003elatency in data availability\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch2\u003eActive-Active Setup:\u003c/h2\u003e\n\n\u003cp\u003eThe replication lag concern can be overcome with the Active-Active setup. Just like the Active standby setup, we will have the same Kafka clusters deployed in the London and Frankfurt regions.\u003c/p\u003e\n\n\u003cp\u003eThe producers of the ingestion pipeline will write the data to both clusters at any given point in time. Which is also known as Dual Writes. So, both clusters\u0026#39; data is in sync as shown in Fig. 4.\u003c/p\u003e\n\n\u003cp\u003ePlease note that the ingestion pipeline or producers should have the capability to support dual writes. It is also important to note that these write pipelines should be isolated so that any issue with one cluster should not impact other pipelines or create back pressure and producer latencies. For example, if your use case is log analytics or observability, considering Logstash in your ingestion and creating two separate pipelines within a single or more Logstash instances will act as an isolated pipeline. But only one cluster is active for the consumers at any given point of time.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/apache-kafka-stretch-cluster-failures/en/resources/51figure-4-1750163022629.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/apache-kafka-stretch-cluster-failures/en/resources/51figure-4-1750163022629.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eFigure 4: Active-Active setup or Dual writes.\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003cp\u003eIn case, as shown in Figure 5, any issues with the one cluster the consumers need to be redirected with DNS redirect and RTO and RPO or SLAs are near zero.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/apache-kafka-stretch-cluster-failures/en/resources/38figure-5-1750163022629.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/apache-kafka-stretch-cluster-failures/en/resources/38figure-5-1750163022629.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eFigure 5: Active-Active setup, one cluster is unavailable\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003cp\u003eOnce the London cluster is available and becomes active, data can sync back to the London cluster from the Frankfurt cluster using Mirror Maker. As shown in Fig. 6, consumers can be switched back to the London cluster using DNS redirect. Also, it is important to configure consumer groups to start where they left off.\u003c/p\u003e\n\n\u003cp\u003eDuring this setup, during the failover, it is important to configure consumers\u0026#39; offset to read the messages from a particular point in time, like reply messages from the last current time minus 15 mins, or set up a mirror maker between active clusters to read \u0026#39;only\u0026#39; consumer offset topics from the London region cluster and replicate to the Frankfurt region cluster. And during the failure, you can use \u003ccode\u003esource.auto.offset.reset=latest\u003c/code\u003e to read the data from the latest offsets in the Frankfurt region cluster.\u003c/p\u003e\n\n\u003cp\u003eIf the consumer groups are not properly configured, you may see duplicate messages in your downstream system. So, it\u0026#39;s essential to design the downstream to handle duplicate records. For example, if it\u0026#39;s an observability use case and if the downstream is OpenSearch, it\u0026#39;s good practice to design a unique ID depending on the message timestamp and other parameters like session IDs, etc. So, that duplicate message will overwrite the existing duplicate record and maintain only one record.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/apache-kafka-stretch-cluster-failures/en/resources/28figure-6-1750163022629.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/apache-kafka-stretch-cluster-failures/en/resources/28figure-6-1750163022629.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eFigure 6: Active-Active setup cluster syncing data with MirrorMaker\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003ch3\u003eChallenges and Complexities in Active-Active Kafka Setups\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003eMessage ordering challenges\u003c/strong\u003e occur when writing to two clusters simultaneously, messages may arrive in different orders due to network delays between regions. This can be particularly problematic for applications that require strict message ordering. So, it needs to be handled in the data processing time or downstream systems. For example, defining a primary key within the message body timestamp field would order the messages based on timestamps.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eData Consistency Issues\u003c/strong\u003e can occur while establishing dual writes to two different clusters. It requires robust monitoring and reconciliation processes to ensure data integrity across clusters. Because there might be conditions where data might successfully write to one cluster but fail in another, resulting in data inconsistency.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eProducer complexity is an issue\u003c/strong\u003e in this active-active setup. Producers need to handle writing to two clusters and manage failures independently for each cluster, so isolation of each pipeline is important. This setup might increase the complexity of the producer applications and require more resources to manage dual writes effectively. There are some agents or tools in the market also as open-source offerings that will provide dual pipeline options such as Elastic Logstash.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eConsumer group management is complex\u003c/strong\u003e. Managing consumer offsets across multiple clusters could be challenging, especially during failover scenarios, and can lead to message duplication. Consumer applications or downstream systems need to be designed to handle duplicate messages or implement deduplication logic.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eOperational Overhead\u003c/strong\u003e - Running active-active clusters requires sophisticated monitoring tools and increases operational costs significantly. Teams need to maintain detailed runbooks and conduct regular disaster recovery drills to ensure smooth failover processes.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eNetwork Latency Considerations\u003c/strong\u003e- Cross-region network latency and bandwidth costs become critical factors in active-active setups. Reliable network connectivity between regions is essential for maintaining data consistency and managing replication effectively.\u003c/p\u003e\n\n\u003ch2\u003eBackup and Restore\u003c/h2\u003e\n\n\u003cp\u003eBackup and restore is a cost-effective architectural pattern compared to Active Standby(AP) or Active Active(AA) DR architectural patterns. It is suitable for non-critical applications with higher RTO and RPO values compared to AA or AP patterns.\u003c/p\u003e\n\n\u003cp\u003eBackup and restore is a cost-effective option because we need only one Kafka cluster. In case the Kafka cluster is unavailable, the data from the centralized repository can be replayed/restored to the Kafka cluster once it becomes available. In this architectural pattern, shown in Figure 7, all the data will be saved to a persistent location like Amazon S3 or another data lake, where the data can be restored/replayed to the Kafka cluster when the Kafka cluster is available. It is also common to see a new environment of Kafka clusters built together using terraform templates and build a new Kafka cluster and replay data.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/apache-kafka-stretch-cluster-failures/en/resources/19figure-7-1750163022629.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/apache-kafka-stretch-cluster-failures/en/resources/19figure-7-1750163022629.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eFigure 7: Backup and restore architectural pattern\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003ch3\u003eChallenges and Complexities in Kafka Backup and Restore Setup\u003c/h3\u003e\n\n\u003cp\u003eRecovery time impact is a major consideration; Restoring (aka replaying) large volumes of data to Kafka can take significant time, resulting in a higher RTO. This makes it suitable only for applications that can tolerate longer downtimes.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eData consistency challenges\u003c/strong\u003e are also an issue. When restoring data from backup storage like Amazon S3, maintaining the exact message ordering and partitioning as the original cluster can be challenging. Special attention is needed to ensure data consistency during restoration.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eBackup performance overhead\u003c/strong\u003e is a further consideration. Regular backups to external storage can impact the performance of the production cluster. The backup process needs careful scheduling and resource management to minimize impact on production workloads.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eStorage is key\u003c/strong\u003e. Managing large volumes of backup data requires effective storage lifecycle policies and controlling costs. Regular cleanup of old backups and efficient storage utilization strategies are essential.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eCoordination during recovery is mandatory\u003c/strong\u003e. The recovery process requires careful coordination between backing up offset information and actual messages. Missing either can lead to message loss or duplication during restoration.\u003c/p\u003e\n\n\u003ch2\u003eConclusion on Kafka DR Strategies\u003c/h2\u003e\n\n\u003cp\u003eAs we explored earlier, choosing a DR strategy isn\u0026#39;t one-size-fits-all, it comes with trade-offs. It often boils down to finding the right balance between what the business needs and what it can afford. The decision usually depends on several factors.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eSLAs\u003c/strong\u003e (e.g., 99.99%+ uptime) are pre-defined commitments to uptime and availability. An SLA requiring 99.99% uptime allows only 52.56 minutes of downtime per year.\u003c/p\u003e\n\n\u003cp\u003eThe \u003cstrong\u003eRTO\u003c/strong\u003e metric is used to define the maximum acceptable downtime for a system or application after a failure before it causes unacceptable business harm. Knowledge/content/training systems, for example, can expect one hour or more downtime because they are not mission critical. For banking applications, on the other hand, downtime acceptance could be none or a couple of minutes.\u003c/p\u003e\n\n\u003cp\u003eThe \u003cstrong\u003eRPO\u003c/strong\u003e metric is used to define the maximum acceptable amount of data loss during a disruptive event, measured in time from the most recent backup. For example, an observability/monitoring application can have the acceptability for losing the past hour whereas a financial application can not tolerate losing a single record of transaction data.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eAssociated costs vary\u003c/strong\u003e. Higher availability and faster recovery typically come at a higher price. Organizations must weigh their need for uptime and data protection against the financial investment required.\u003c/p\u003e\n\n\u003ctable\u003e\n\t\u003cthead\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003cth scope=\"col\"\u003e\u003csmall\u003eDR Strategy\u003c/small\u003e\u003c/th\u003e\n\t\t\t\u003cth scope=\"col\"\u003e\u003csmall\u003eRTO (Downtime tolerance)\u003c/small\u003e\u003c/th\u003e\n\t\t\t\u003cth scope=\"col\"\u003e\u003csmall\u003eRPO (Data loss tolerance)\u003c/small\u003e\u003c/th\u003e\n\t\t\t\u003cth scope=\"col\"\u003e\u003csmall\u003eUse case\u003c/small\u003e\u003c/th\u003e\n\t\t\t\u003cth scope=\"col\"\u003e\u003csmall\u003eCost\u003c/small\u003e\u003c/th\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/thead\u003e\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eActive Active\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eNear-Zero (Seconds)\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eNear-Zero (Seconds)\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eMission-critical apps (e.g., banking, e-commerce, stock trading)\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eHigh\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eActive Standby\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eMinutes to Hours\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eNear-Zero to Minutes\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eEnterprise apps need high availability\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eMedium to High (No dual write ingestion pipeline cost)\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eBackup and Restore\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eHours to Days\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eHours to Days\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eNon-critical apps with occasional DR needs or cost-sensitive apps,\u003c/small\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003csmall\u003eLow\u003c/small\u003e\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e\u003cstrong\u003eReferences\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\u003ca href=\"https://kafka.apache.org/documentation/\"\u003eKafka documentation\u003c/a\u003e\u003c/li\u003e\n\t\u003cli\u003e\u003ca href=\"https://zookeeper.apache.org/doc/r3.7.2/zookeeperCLI.html\"\u003eZooKeeper CLI commands\u003c/a\u003e\u003c/li\u003e\n\t\u003cli\u003e\u003ca href=\"https://zookeeper.apache.org/doc/r3.7.2/zookeeperMonitor.html\"\u003eZooKeeper Monitoring\u003c/a\u003e\u003c/li\u003e\n\t\u003cli\u003e\u003ca href=\"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330\"\u003eMirror Maker documentation\u003c/a\u003e\u003c/li\u003e\n\t\u003cli\u003e\u003ca href=\"https://dnsspy.io/learning/dns-availability/dns-failover-strategies\"\u003eDNS Failover strategies\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Authors\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Srikanth-Daggumalli\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSrikanth Daggumalli\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Nishchai-Jayanna-Manjula\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eNishchai Jayanna Manjula\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\n                            \n                            \n\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "50 min read",
  "publishedTime": "2025-06-20T00:00:00Z",
  "modifiedTime": null
}
