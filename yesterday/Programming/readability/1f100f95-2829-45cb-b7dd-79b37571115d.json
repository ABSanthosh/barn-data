{
  "id": "1f100f95-2829-45cb-b7dd-79b37571115d",
  "title": "How using Availability Zones can eat up your budget‚Ää‚Äî‚Ääour journey from Prometheus to‚Ä¶",
  "link": "https://engineering.prezi.com/how-using-availability-zones-can-eat-up-your-budget-our-journey-from-prometheus-to-be8a816f7efe?source=rss----911e72786e31---4",
  "description": "",
  "author": "Grzegorz Sko≈Çyszewski",
  "published": "Mon, 09 Dec 2024 16:31:05 GMT",
  "source": "https://engineering.prezi.com/feed",
  "categories": [
    "monitoring",
    "prometheus",
    "victoriametrics",
    "grafana",
    "kubernetes"
  ],
  "byline": "Grzegorz Sko≈Çyszewski",
  "length": 14658,
  "excerpt": "By 2024, Prezi‚Äôs monitoring system, built around Prometheus, was becoming outdated. It was already 5+ years old, running on a deprecated internal platform and accumulating a significant amount of‚Ä¶",
  "siteName": "Prezi Engineering",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*U0lNGgJfm0Qo1ZfYDS36KA.png",
  "text": "IntroBy 2024, Prezi‚Äôs monitoring system, built around Prometheus, was becoming outdated. It was already 5+ years old, running on a deprecated internal platform and accumulating a significant amount of costs every month.At the beginning of the year, we decided to deal with the ‚Äúfuture problem‚Äù and modernize our metrics collection and storage system. Our goals were to run the monitoring system in our Kubernetes-based platform and reduce the overall complexity and costs of the system.We achieved these using VictoriaMetrics. This post describes our journey, the challenges we faced, and the results we achieved from the migration.Previous stateOur Prometheus-based system wasn‚Äôt that problematic by itself ‚Äî we ran a pair of instances, to achieve high availability, for each of our Kubernetes cluster. We also had one extra pair for non-Kubernetes resources, and one for storing a subset of metrics with longer retention. You can see the high-level architecture of the system in the diagram below.Our Prometheus-based system architectureJust before the migration, we had 5 Million active series at any given point in time. It‚Äôs also worth noting that our microservices ecosystem was already instrumented for producing metrics in Prometheus format, and it was something that we didn‚Äôt want to change ‚Äî it‚Äôs at this stage de-facto the standard (although it is slowly becoming superseded by OpenTelemetry).There are some challenges when operating such a system:Exploring metrics or configuring rules must target specific installations. This made dashboarding and alerting more difficult, and it already is difficult for most non-SRE folks in general.The instances Prometheus ran on had to be really beefy to handle our load.As mentioned in the introduction, the instances were running on the previous version of the Prezi platform that was already deprecated. We really wanted to move off.The optionsNow that you know what we were dealing with, let‚Äôs look at what we could have done with it. We set out to explore our options, considering both managed and self-hosted solutions. We quickly realized that we couldn‚Äôt afford to ship our metrics to any of the vendors out there. We would have to spend at least 2x the current cost, and, the perspective of modern self-hosted solutions being even cheaper, led us to drop that path.On the self-hosted end of the spectrum, we had:ThanosMimir/CortexVictoriaMetricsSome members of the team were already familiar with Thanos and Cortex, so these were the biased first-choice tools that we first tried to understand. But we didn‚Äôt stop there and made a complete comparison for the concerns that we cared about. You can see the table from one of our exploration documents below.Differences between Mimir, Thanos and VictoriaMetrics, taken from our exploration documentation.We initially thought that using block storage may be a downside of VictoriaMetrics. Nothing more wrong ‚Äî while it‚Äôs tempting to use the infinitely-scalable object storage (like S3), the good old block storage is just cheaper and more performant. Given that cost control was one of the priorities, we saw an opportunity to run the system cheaper, and quite possibly ‚Äî with less complex architecture. For example, thanks to using block storage, VictoriaMetrics no longer needs any external cache subsystem, as is the case with the other two.In the process of exploring what VictoriaMetrics has to offer, we also took a small detour and talked with the good folks at VictoriaMetrics to see if buying an Enterprise license for self-hosting, which enables some features that we could have wanted, is within our budget. Turns out we didn‚Äôt really need these features, but buying the license wouldn‚Äôt break the bank for us either. And, there‚Äôs nothing wrong with asking for a quote!VictoriaMetrics stood out thanks to its simplicity and cost-efficiency, which we tested in a Proof of Concept.VictoriaMetrics Proof of Concept with some challengesWe jumped into the implementation of a small proof-of-concept system based on VictoriaMetrics, to see how easy it is to work with (what‚Äôs good from the most cost-effective system if you can only get there after 3 months of tuning it back and forth?), how it performs, and to extrapolate the cost of the full system later on.VictoriaMetrics allows you to install VictoriaMetrics Single ‚Äî all-in-one, single executable, which acts almost exactly like Prometheus. It can scrape targets, store the metrics, and serve them for further processing or analysis. We knew from the start that we wanted to use VictoriaMetrics Agents to scrape targets, as that allowed us to host a central aggregation layer installation and distribute the agents ‚Äî all of them contained, collecting metrics only within their environments (be that Kubernetes cluster, or AWS VPC).The initial ideaWe wanted to host the tool on Kubernetes at the end, so it made sense to rely on the distributed version of the system‚Äî for high availability and scalability, it just sounded good. We took the off-the-shelf helm chart for the clustered version ‚Äî one, where VMInsert, VMStorage, and VMSelect are each separate components.The concept is fairly simple ‚Äî VMInsert is the write proxy, VMSelect is the read proxy, and VMStorage is the component that persists the data to underlying disks. On top of that, we also installed VMAlert ‚Äî the component used for evaluating rules (Recording and Alerting).High level overview of VictoriaMetrics Cluster architecture, taken from our exploration documentation.We didn‚Äôt want to test agent options yetWe initially used Prometheus servers with remote_write for testing but quickly found that VictoriaMetrics Agents were far more performant for our needs. Even though we had a lot of headroom on the instances, the Prometheus was just too slow to write to VictoriaMetrics.Installing VictoriaMetrics Agent was easy with the already existing scraping configuration. We simply replicated the configuration ‚Äî that was enough to make the Agent work.The cost and the performanceWe managed to create a representative small version of the system. That allowed us to test the performance of reads and writes, and see how much resources (CPU time, Memory, and storage size) the system used. We were absolutely delighted. We found queries that were timing out after 30 seconds in Prometheus, returning data in 3‚Äì7 seconds in VictoriaMetrics. We didn‚Äôt find any queries that were performing significantly worse.We also found that the resource usage footprint was minimal. The data is efficiently stored on the disk, and compressed, and the application uses very little CPU time and Memory. Our estimations at the time showed: 70% less storage, 60% less memory, and 30% less CPU time used. This, together with bin-packing in Kubernetes made us excited about saving a significant amount of money spent on the system.Well done, VictoriaMetrics!skynesher/E+ via Getty Images.Too good to be true, or how using Availability Zones can empty your walletSo it was working, and it was working well. We were scraping metrics and using remote_write to store them. We could query the metrics in Grafana (added as Prometheus data source, because VictoriaMetrics‚Äô MetricsQL, the query language, is a superset of PromQL ‚Äî which is fantastic!), we even added some alert rules and saw them trigger. That was so smooth. Too smooth.A couple of days later, we found that we had accumulated a significant amount of dollars, which was attributed to the network traffic in our environment. Turns out that running a distributed metrics system, where each time you query or write a metric, you get an extra hop (VMSelect or VMInsert to VMStorage), can be costly when you put that in the context of inter-zone traffic in your hyperscaler (AWS for us). Not only were typical metric writes and reads subject to that , but evaluating rules (and we have some really heavy recording rules) also used the same route. That was concerning and made us stop and rethink our approach.DjelicS/E+ via Getty Images.We needed to figure out something else.Back to the rootsIf you scrolled up to the previous state diagram, where I showed how we used Prometheus, you might see that we used a pair of instances for HA. We decided to keep that approach for our new system. Instead of using the clustered version of VictoriaMetrics per Availability Zone, we tested the installation based on two separate VictoriaMetrics Single instances, each in a different AZ. We went into ‚Äúsave as much as possible mode‚Äù at that time, and we traded local redundancy for a global redundancy ‚Äî since a single cluster with distributed components would be enough for us, reliability-wise ‚Äî two instances in a hot-hot setup would also do it!Installing two single-replica Deployments of VictoriaMetrics Single worked flawlessly for us (spoiler ‚Äî it still does work flawlessly more than a half year later üöÄ). We no longer cross Availability zones with our extra hop traffic.We added a pair of VictoriaMetrics Alert instances next to each VictoriaMetrics Single instance, operating in the same Availability Zone.Aggregation Layer overview based on VictoriaMetrics Single instances.We set up a load balancer in front of the instances for reading the metrics, mainly used by Grafana. Occasionally, one of the VMSingle instances goes down ‚Äî then the traffic is sent to the other one. When the instance is unavailable, we don‚Äôt lose data ‚Äî agents buffer it, and while we may skip a couple of recording rules evaluations, VictoriaMetrics provides a neat way to backfill rules using vmreplay.The only time the traffic goes across AZs now is when an agent is not hosted in the same zone as the target VictoriaMetrics Single instance. This is something that can not be worked around, as long as we want two agents to write the data (which is then deduplicated smartly by VictoriaMetrics).The final architecture and other notable mentionsFinally, our architecture looked like below:VictoriaMetrics-based system architecture(Yes, the diagram looks a bit more convoluted than the diagram for the previous system. This is the price you pay for having a more-performant and cost-effective system with a better user experience üôÉ)There are also other use cases, which I haven‚Äôt touched on above ‚Äî the long-term storage, and using VictoriaMetrics Operator to scrape non-Kubernetes and improve system configuration capabilities. I want to expand a bit on these and one extra special thing below.Long-term storageWe also wanted to migrate our long-term storage installation of Prometheus. When exploring VictoriaMetrics, using an enterprise license to have different retention configurations for series was tempting, but we checked and it wasn‚Äôt the most cost-effective way to do it.We also had a brief episode of sending these metrics to Grafana Cloud, where we have 13 months of retention. That cost us pennies, but at the time of adding it, we had two Grafana installations ‚Äî self-hosted, and Cloud instance.Having both short-term and long-term metrics in one Grafana would require us to add the Grafana Cloud Prometheus data source in our self-hosted instance. That‚Äôs nothing, but we found something better ‚Äî we just set up yet another VMSingle instance with a different retention setting. We not only pay even less but have 100% of metrics in our infrastructure.Michael Blann/DigitalVision at Getty Images.VictoriaMetrics OperatorOur scraping and rules configuration for the previous system was overly complicated, with a baggage of tech-debt ‚Äî neither we nor our users understood how to configure the system, sometimes. We wanted to change that.We chose to install and configure VictoriaMetrics using the Kubernetes Operator. All of the components are managed by the Operator, as well as the configuration of the system. That allowed us to distribute the configuration concerns to our users ‚Äî our product teams can now configure alerting for their services from their repositories. If you want to know how we pulled that off, let me know ‚Äî that would definitely be material for another post.Scraping non-Kubernetes resources with VictoriaMetrics OperatorWhen we were setting up the system in production, VictoriaMetrics Operator was still in its early days. There was no support for Service Discovery of non-Kubernetes targets (now there is one), and there was no way to install VMAgent (Operator-managed Custom Resource) that wouldn‚Äôt be injected with the same configuration as the other VMAgents in the cluster (at least not an easy, maintainable way).To overcome these and still collect metrics from our other workloads, we chose to install an additional VictoriaMetrics Agent using the helm chart and configure it statically. This works for us because the targets don‚Äôt change that much and are mostly infrastructure-related, so the people configuring the scraping are more familiar with Prometheus/VictoriaMetrics than, say, a Python-focused Software Engineer.Single pane of glass in Grafana Cloud with self-hosted metricsLastly, the very recent change that is worth mentioning ‚Äî consolidating our Grafana instances. We now have only one instance of Grafana, thanks to a smart solution offered by Grafana Labs ‚Äî Grafana Private Data Connect. We install the agent next to our VictoriaMetrics, which sets up a SOCKS5 tunnel between our and Grafana Labs‚Äô infrastructure. That allowed us to add a self-hosted VictoriaMetrics as a data source in Grafana Cloud. What‚Äôs more ‚Äî it‚Äôs free (except for the network traffic)! Neat! Well done, Grafana Labs! üí™Note: We are a happy customer of Grafana Labs and their Cloud offering, as you may know from How Prezi replaced a homegrown Log Management System at Medium or Grafana‚Äôs Big Tent Podcast S2E2, where Alex first explained how we landed on Grafana Loki for our Log Management, and then explained how we use Grafana IRM for our Incident Management. Check these out!What have we gained from migrating our system?The benefits can be summarized as follows:Cost Efficiency: Saved ~30% on system costs.Performance: Query speeds improved significantly, with heavy queries completing in 3‚Äì7 seconds (vs. 30+ seconds).User Experience: Streamlined metrics access and configuration via Kubernetes-native tools.Scalability: The system is now future-proof for growing workloads.Lastly, working on the migration allowed us to learn a ton, and work on something interesting and challenging.Migrating from Prometheus to VictoriaMetrics transformed our monitoring system, offering cost savings, performance gains, and an improved developer experience. If you‚Äôre considering a similar move, we strongly recommend evaluating VictoriaMetrics for its simplicity and efficiency.",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*tGT2rOPOAoGAiRKAbgc1rg.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/@grzegorz.skolyszewski?source=post_page---byline--be8a816f7efe--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Grzegorz Sko≈Çyszewski\" src=\"https://miro.medium.com/v2/resize:fill:88:88/0*UB4ArWwDRZY2ziFk.jpg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://engineering.prezi.com/?source=post_page---byline--be8a816f7efe--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Prezi Engineering\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ecIYF5KMJj1G4-_pkFWy0g.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003ch2 id=\"8df6\"\u003eIntro\u003c/h2\u003e\u003cp id=\"ca11\"\u003eBy 2024, Prezi‚Äôs monitoring system, built around Prometheus, was becoming outdated. It was already 5+ years old, running on a deprecated internal platform and accumulating a significant amount of costs every month.\u003c/p\u003e\u003cp id=\"8e36\"\u003eAt the beginning of the year, we decided to deal with the ‚Äúfuture problem‚Äù and modernize our metrics collection and storage system. Our goals were to run the monitoring system in our Kubernetes-based platform and reduce the overall complexity and costs of the system.\u003c/p\u003e\u003cp id=\"07fc\"\u003eWe achieved these using VictoriaMetrics. This post describes our journey, the challenges we faced, and the results we achieved from the migration.\u003c/p\u003e\u003ch2 id=\"587b\"\u003ePrevious state\u003c/h2\u003e\u003cp id=\"24d0\"\u003eOur Prometheus-based system wasn‚Äôt \u003cstrong\u003ethat \u003c/strong\u003eproblematic by itself ‚Äî we ran a pair of instances, to achieve high availability, for each of our Kubernetes cluster. We also had one extra pair for non-Kubernetes resources, and one for storing a subset of metrics with longer retention. You can see the high-level architecture of the system in the diagram below.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eOur Prometheus-based system architecture\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"0b42\"\u003eJust before the migration, we had 5 Million active series at any given point in time. It‚Äôs also worth noting that our microservices ecosystem was already instrumented for producing metrics in Prometheus format, and it was something that we didn‚Äôt want to change ‚Äî it‚Äôs at this stage de-facto the standard (although it is slowly becoming superseded by OpenTelemetry).\u003c/p\u003e\u003cp id=\"13c1\"\u003eThere are some challenges when operating such a system:\u003c/p\u003e\u003cul\u003e\u003cli id=\"ec00\"\u003eExploring metrics or configuring rules must target specific installations. This made dashboarding and alerting more difficult, and it already is difficult for most non-SRE folks in general.\u003c/li\u003e\u003cli id=\"db8f\"\u003eThe instances Prometheus ran on had to be \u003cstrong\u003ereally\u003c/strong\u003e \u003cstrong\u003ebeefy\u003c/strong\u003e to handle our load.\u003c/li\u003e\u003cli id=\"620f\"\u003eAs mentioned in the introduction, the instances were running on the previous version of the Prezi platform that was already deprecated. We really wanted to move off.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"d5a6\"\u003eThe options\u003c/h2\u003e\u003cp id=\"34d0\"\u003eNow that you know what we were dealing with, let‚Äôs look at what we could have done with it. We set out to explore our options, considering both managed and self-hosted solutions. We quickly realized that we couldn‚Äôt afford to ship our metrics to any of the vendors out there. We would have to spend at least 2x the current cost, and, the perspective of modern self-hosted solutions being even cheaper, led us to drop that path.\u003c/p\u003e\u003cp id=\"fdbf\"\u003eOn the self-hosted end of the spectrum, we had:\u003c/p\u003e\u003cul\u003e\u003cli id=\"4b7a\"\u003eThanos\u003c/li\u003e\u003cli id=\"eeb8\"\u003eMimir/Cortex\u003c/li\u003e\u003cli id=\"3455\"\u003eVictoriaMetrics\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"1f80\"\u003eSome members of the team were already familiar with Thanos and Cortex, so these were the biased first-choice tools that we first tried to understand. But we didn‚Äôt stop there and made a complete comparison for the concerns that we cared about. You can see the table from one of our exploration documents below.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eDifferences between Mimir, Thanos and VictoriaMetrics, taken from our exploration documentation.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"bf61\"\u003eWe initially thought that using \u003cem\u003eblock storage \u003c/em\u003emay be a downside of VictoriaMetrics. Nothing more wrong ‚Äî while it‚Äôs tempting to use the infinitely-scalable object storage (like S3), the good old block storage is just cheaper and more performant. Given that cost control was one of the priorities, we saw an opportunity to run the system cheaper, and quite possibly ‚Äî with less complex architecture. For example, thanks to using block storage, VictoriaMetrics no longer needs any external cache subsystem, as is the case with the other two.\u003c/p\u003e\u003cp id=\"ece7\"\u003eIn the process of exploring what VictoriaMetrics has to offer, we also took a small detour and talked with the good folks at VictoriaMetrics to see if buying an Enterprise license for self-hosting, which enables some features that we could have wanted, is within our budget. Turns out we didn‚Äôt really need these features, but buying the license wouldn‚Äôt break the bank for us either. And, there‚Äôs nothing wrong with asking for a quote!\u003c/p\u003e\u003cp id=\"8d8a\"\u003eVictoriaMetrics stood out thanks to its simplicity and cost-efficiency, which we tested in a Proof of Concept.\u003c/p\u003e\u003ch2 id=\"be8d\"\u003eVictoriaMetrics Proof of Concept with some challenges\u003c/h2\u003e\u003cp id=\"0dc8\"\u003eWe jumped into the implementation of a small proof-of-concept system based on VictoriaMetrics, to see how easy it is to work with (what‚Äôs good from the most cost-effective system if you can only get there after 3 months of tuning it back and forth?), how it performs, and to extrapolate the cost of the full system later on.\u003c/p\u003e\u003cp id=\"4d41\"\u003eVictoriaMetrics allows you to install VictoriaMetrics Single ‚Äî all-in-one, single executable, which acts almost exactly like Prometheus. It can scrape targets, store the metrics, and serve them for further processing or analysis. We knew from the start that we wanted to use VictoriaMetrics Agents to scrape targets, as that allowed us to host a central aggregation layer installation and distribute the agents ‚Äî all of them contained, collecting metrics only within their environments (be that Kubernetes cluster, or AWS VPC).\u003c/p\u003e\u003ch2 id=\"e9ca\"\u003eThe initial idea\u003c/h2\u003e\u003cp id=\"9f0a\"\u003eWe wanted to host the tool on Kubernetes at the end, so it made sense to rely on the distributed version of the system‚Äî for high availability and scalability, it just sounded good. We took the off-the-shelf helm chart for the clustered version ‚Äî one, where VMInsert, VMStorage, and VMSelect are each separate components.\u003c/p\u003e\u003cp id=\"f043\"\u003eThe concept is fairly simple ‚Äî VMInsert is the write proxy, VMSelect is the read proxy, and VMStorage is the component that persists the data to underlying disks. On top of that, we also installed VMAlert ‚Äî the component used for evaluating rules (Recording and Alerting).\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eHigh level overview of VictoriaMetrics Cluster architecture, taken from our exploration documentation.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"d2d5\"\u003eWe didn‚Äôt want to test agent options yet\u003c/h2\u003e\u003cp id=\"546c\"\u003eWe initially used Prometheus servers with \u003cem\u003eremote_write\u003c/em\u003e for testing but quickly found that VictoriaMetrics Agents were far more performant for our needs. Even though we had a lot of headroom on the instances, the Prometheus was just too slow to write to VictoriaMetrics.\u003c/p\u003e\u003cp id=\"2a79\"\u003eInstalling VictoriaMetrics Agent was easy with the already existing scraping configuration. We simply replicated the configuration ‚Äî that was enough to make the Agent work.\u003c/p\u003e\u003ch2 id=\"b043\"\u003eThe cost and the performance\u003c/h2\u003e\u003cp id=\"7c6a\"\u003eWe managed to create a representative small version of the system. That allowed us to test the performance of reads and writes, and see how much resources (CPU time, Memory, and storage size) the system used. We were absolutely delighted. We found queries that were timing out after 30 seconds in Prometheus, returning data in 3‚Äì7 seconds in VictoriaMetrics. We didn‚Äôt find any queries that were performing significantly worse.\u003c/p\u003e\u003cp id=\"70ea\"\u003eWe also found that the resource usage footprint was minimal. The data is efficiently stored on the disk, and compressed, and the application uses very little CPU time and Memory. Our estimations at the time showed: 70% less storage, 60% less memory, and 30% less CPU time used. This, together with bin-packing in Kubernetes made us excited about saving a significant amount of money spent on the system.\u003c/p\u003e\u003cp id=\"a03b\"\u003eWell done, VictoriaMetrics!\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eskynesher/E+ via Getty Images.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"6d36\"\u003eToo good to be true, or how using Availability Zones can empty your wallet\u003c/h2\u003e\u003cp id=\"0abb\"\u003eSo it was working, and it was working well. We were scraping metrics and using \u003cem\u003eremote_write\u003c/em\u003e to store them. We could query the metrics in Grafana (added as Prometheus data source, because VictoriaMetrics‚Äô \u003cem\u003eMetricsQL\u003c/em\u003e, the query language, is a superset of \u003cem\u003ePromQL\u003c/em\u003e ‚Äî which is fantastic!), we even added some alert rules and saw them trigger. That was so smooth. Too smooth.\u003c/p\u003e\u003cp id=\"6851\"\u003eA couple of days later, we found that we had accumulated a significant amount of dollars, which was attributed to the network traffic in our environment. Turns out that running a distributed metrics system, where each time you query or write a metric, you get an extra hop (VMSelect or VMInsert to VMStorage), can be costly when you put that in the context of inter-zone traffic in your hyperscaler (AWS for us). Not only were typical metric writes and reads subject to that , but evaluating rules (and we have some really heavy recording rules) also used the same route. That was concerning and made us stop and rethink our approach.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eDjelicS/E+ via Getty Images.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"a655\"\u003eWe needed to figure out something else.\u003c/p\u003e\u003ch2 id=\"5dcc\"\u003eBack to the roots\u003c/h2\u003e\u003cp id=\"c628\"\u003eIf you scrolled up to the previous state diagram, where I showed how we used Prometheus, you might see that we used a pair of instances for HA. We decided to keep that approach for our new system. Instead of using the clustered version of VictoriaMetrics per Availability Zone, we tested the installation based on two separate VictoriaMetrics Single instances, each in a different AZ. We went into ‚Äúsave as much as possible mode‚Äù at that time, and we traded local redundancy for a global redundancy ‚Äî since a single cluster with distributed components would be enough for us, reliability-wise ‚Äî two instances in a \u003cem\u003ehot-hot\u003c/em\u003e setup would also do it!\u003c/p\u003e\u003cp id=\"96ba\"\u003eInstalling two single-replica Deployments of VictoriaMetrics Single worked flawlessly for us (spoiler ‚Äî it still does work flawlessly more than a half year later üöÄ). We no longer cross Availability zones with our extra hop traffic.\u003c/p\u003e\u003cp id=\"befd\"\u003eWe added a pair of VictoriaMetrics Alert instances next to each VictoriaMetrics Single instance, operating in the same Availability Zone.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eAggregation Layer overview based on VictoriaMetrics Single instances.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"6db2\"\u003eWe set up a load balancer in front of the instances for reading the metrics, mainly used by Grafana. Occasionally, one of the VMSingle instances goes down ‚Äî then the traffic is sent to the other one. When the instance is unavailable, we don‚Äôt lose data ‚Äî agents buffer it, and while we may skip a couple of recording rules evaluations, \u003ca href=\"https://victoriametrics.com/blog/rules-replay/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVictoriaMetrics provides a neat way to backfill rules using vmreplay\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"1b5c\"\u003eThe only time the traffic goes across AZs now is when an agent is not hosted in the same zone as the target VictoriaMetrics Single instance. This is something that can not be worked around, as long as we want two agents to write the data (which is then deduplicated smartly by VictoriaMetrics).\u003c/p\u003e\u003ch2 id=\"5ff2\"\u003eThe final architecture and other notable mentions\u003c/h2\u003e\u003cp id=\"d748\"\u003eFinally, our architecture looked like below:\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eVictoriaMetrics-based system architecture\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"1f14\"\u003e(Yes, the diagram looks a bit more convoluted than the diagram for the previous system. This is the price you pay for having a more-performant and cost-effective system with a better user experience üôÉ)\u003c/p\u003e\u003cp id=\"96ed\"\u003eThere are also other use cases, which I haven‚Äôt touched on above ‚Äî the long-term storage, and using VictoriaMetrics Operator to scrape non-Kubernetes and improve system configuration capabilities. I want to expand a bit on these and one extra special thing below.\u003c/p\u003e\u003ch2 id=\"9aec\"\u003eLong-term storage\u003c/h2\u003e\u003cp id=\"e312\"\u003eWe also wanted to migrate our long-term storage installation of Prometheus. When exploring VictoriaMetrics, using an enterprise license to have different retention configurations for series was tempting, but we checked and it wasn‚Äôt the most cost-effective way to do it.\u003c/p\u003e\u003cp id=\"461e\"\u003eWe also had a brief episode of sending these metrics to Grafana Cloud, where we have 13 months of retention. That cost us pennies, but at the time of adding it, we had two Grafana installations ‚Äî self-hosted, and Cloud instance.\u003c/p\u003e\u003cp id=\"8723\"\u003eHaving both short-term and long-term metrics in one Grafana would require us to add the Grafana Cloud Prometheus data source in our self-hosted instance. That‚Äôs nothing, but we found something better ‚Äî we just set up yet another VMSingle instance with a different retention setting. We not only pay even less but have 100% of metrics in our infrastructure.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003ca href=\"https://www.gettyimages.com/search/photographer?photographer=Michael+Blann\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMichael Blann\u003c/a\u003e/DigitalVision at Getty Images.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"4c2a\"\u003eVictoriaMetrics Operator\u003c/h2\u003e\u003cp id=\"0b4c\"\u003eOur scraping and rules configuration for the previous system was overly complicated, with a baggage of tech-debt ‚Äî neither we nor our users understood how to configure the system, sometimes. We wanted to change that.\u003c/p\u003e\u003cp id=\"50d3\"\u003eWe chose to install and configure VictoriaMetrics using the Kubernetes Operator. All of the components are managed by the Operator, as well as the configuration of the system. That allowed us to distribute the configuration concerns to our users ‚Äî our product teams can now configure alerting for their services from their repositories. If you want to know how we pulled that off, let me know ‚Äî that would definitely be material for another post.\u003c/p\u003e\u003ch2 id=\"eea7\"\u003eScraping non-Kubernetes resources with VictoriaMetrics Operator\u003c/h2\u003e\u003cp id=\"00ae\"\u003eWhen we were setting up the system in production, VictoriaMetrics Operator was still in its early days. There was no support for Service Discovery of non-Kubernetes targets (now there is one), and there was no way to install VMAgent (Operator-managed Custom Resource) that wouldn‚Äôt be injected with the same configuration as the other VMAgents in the cluster (at least not an easy, maintainable way).\u003c/p\u003e\u003cp id=\"6261\"\u003eTo overcome these and still collect metrics from our other workloads, we chose to install an additional VictoriaMetrics Agent using the helm chart and configure it statically. This works for us because the targets don‚Äôt change that much and are mostly infrastructure-related, so the people configuring the scraping are more familiar with Prometheus/VictoriaMetrics than, say, a Python-focused Software Engineer.\u003c/p\u003e\u003ch2 id=\"fe99\"\u003eSingle pane of glass in Grafana Cloud with self-hosted metrics\u003c/h2\u003e\u003cp id=\"4665\"\u003eLastly, the very recent change that is worth mentioning ‚Äî consolidating our Grafana instances. We now have only one instance of Grafana, thanks to a smart solution offered by Grafana Labs ‚Äî Grafana Private Data Connect. We install the agent next to our VictoriaMetrics, which sets up a SOCKS5 tunnel between our and Grafana Labs‚Äô infrastructure. That allowed us to add a self-hosted VictoriaMetrics as a data source in Grafana Cloud. What‚Äôs more ‚Äî it‚Äôs free (except for the network traffic)! Neat! Well done, Grafana Labs! üí™\u003c/p\u003e\u003cp id=\"b9eb\"\u003eNote: We are a happy customer of Grafana Labs and their Cloud offering, as you may know from \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://engineering.prezi.com/how-prezi-replaced-a-homegrown-log-management-system-with-grafana-loki-15111174ff91\"\u003eHow Prezi replaced a homegrown Log Management System at Medium\u003c/a\u003e or \u003ca href=\"https://bigtent.fm/s2/2\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGrafana‚Äôs Big Tent Podcast S2E2\u003c/a\u003e, where Alex first explained how we landed on Grafana Loki for our Log Management, and then explained how we use Grafana IRM for our Incident Management. Check these out!\u003c/p\u003e\u003ch2 id=\"b6c1\"\u003eWhat have we gained from migrating our system?\u003c/h2\u003e\u003cp id=\"656e\"\u003eThe benefits can be summarized as follows:\u003c/p\u003e\u003cul\u003e\u003cli id=\"1143\"\u003e\u003cstrong\u003eCost Efficiency\u003c/strong\u003e: Saved ~30% on system costs.\u003c/li\u003e\u003cli id=\"7954\"\u003e\u003cstrong\u003ePerformance\u003c/strong\u003e: Query speeds improved significantly, with heavy queries completing in 3‚Äì7 seconds (vs. 30+ seconds).\u003c/li\u003e\u003cli id=\"1de4\"\u003e\u003cstrong\u003eUser Experience\u003c/strong\u003e: Streamlined metrics access and configuration via Kubernetes-native tools.\u003c/li\u003e\u003cli id=\"cd08\"\u003e\u003cstrong\u003eScalability\u003c/strong\u003e: The system is now future-proof for growing workloads.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"eb34\"\u003eLastly, working on the migration allowed us to learn a ton, and work on something interesting and challenging.\u003c/p\u003e\u003cp id=\"c1f2\"\u003eMigrating from Prometheus to VictoriaMetrics transformed our monitoring system, offering cost savings, performance gains, and an improved developer experience. If you‚Äôre considering a similar move, we strongly recommend evaluating VictoriaMetrics for its simplicity and efficiency.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "16 min read",
  "publishedTime": "2024-12-09T16:31:05.611Z",
  "modifiedTime": null
}
