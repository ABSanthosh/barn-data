{
  "id": "b9d8039e-9f6f-446e-8fae-b5e88d2d4354",
  "title": "OpenAI Announces ‘o3’ Reasoning Model",
  "link": "https://www.infoq.com/news/2024/12/openai-announces-o3/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "OpenAI has launched the O3 and O3 Mini models, setting a new standard in AI with enhanced reasoning capabilities. Notable achievements include 71.7% accuracy on SWE-Bench and 96.7% on the AIME benchmark. While these models excel in coding and mathematics, challenges remain. O3 Mini offers scalable options for developers, prioritizing safety and adaptability. By Andrew Hoblitzell",
  "author": "Andrew Hoblitzell",
  "published": "Wed, 25 Dec 2024 14:01:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Machine Learning",
    "Artificial Intelligence",
    "API",
    "OpenAI",
    "Development",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Andrew Hoblitzell",
  "length": 4613,
  "excerpt": "OpenAI has launched the O3 and O3 Mini models, setting a new standard in AI with enhanced reasoning capabilities. Notable achievements include 71.7% accuracy on SWE-Bench and 96.7% on the AIME benchma",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20241210082243/apple-touch-icon.png",
  "text": "During the 12 days of Shipmas, OpenAI unveiled its latest advancements in artificial intelligence with the announcement of the o3 model and its counterpart, o3 Mini. These models improve reasoning capabilities over prior models, offering developers new opportunities to solve increasingly complex tasks. o3 sets a new benchmark in technical performance, particularly in coding and mathematics. On SWE-Bench Verified, a coding benchmark comprised of real-world software tasks, o3 achieves a 71.7% accuracy—over 20% better than o1. Similarly, on Codeforces, a competitive programming platform, o3 scores a 2727 ELO under high compute settings. On the American Invitational Mathematics Examination (AIME) benchmark, the model achieves 96.7% accuracy, a leap from the 83.3% attained by o1. On the ARC dataset, an example of which is shown above, problems are designed to test an AI system's ability to adapt to novel tasks. o3 scored 75.7% on the Semi-Private Evaluation set under the competition's $10k compute budget (around $20 per task) and 87.5% at high-compute configurations ($2000-$3000 per task). The performance/cost tradeoff is depicted in the figure below. The ARC-AGI benchmark is specifically a challenge that previous models failed to address. o3 employs a paradigm that integrates natural language program search and execution during test time, reminiscent of techniques like AlphaZero’s Monte Carlo tree search and is also guided by a deep learning-based evaluator. François Chollet, the creator of the benchmark, noted the progress made by o3 while also the ongoing room for improvement. I don't think o3 is AGI yet. o3 still fails on some very easy tasks, indicating fundamental differences with human intelligence. .. the fact is that a large ensemble of low-compute Kaggle solutions can now score 81% on the private eval. - François Chollet These advancements necessitate more challenging benchmarks as the model still struggles with certain simple tasks that humans find trivial. OpenAI has focused on addressing this need by tackling Epoch AI’s Frontier Math Benchmark. Tamay Besiroglu of EpochAI said this \"arrives about a year ahead of my median expectations.\" With aggressive compute settings, o3 achieves ~25% accuracy. Early testing on the forthcoming ARC-AGI-2 benchmark suggests that o3 could face significant challenges, with predictions under 30% even at high compute levels. The hype around o3 is out of control. It’s not AGI, it’s not the singularity, and you definitely don’t have to change your worldview. - Elvis Saravia OpenAI’s development of its next-generation AI model, codenamed Orion, has encountered other hurdles. The anticipated GPT-5 model, initially expected to launch in early 2024, remains delayed as engineers grapple with rising costs, limited data, and design challenges. The growing complexity of building and training such models has pushed the estimated costs of GPT-5’s development to over $1 billion. o3 Mini offers scalable thinking time options—low, medium, and high—allowing developers to balance performance with cost and latency. o3 Mini excels in code generation and problem-solving, achieving competitive ELO ratings on Codeforces and matching or exceeding o1’s performance at a fraction of the cost. o3 Mini’s adaptability is exemplified in live demonstrations, where it efficiently generates complex Python scripts for automated tasks. In one example, the model created a local server that processed coding requests, executed the code, and displayed results. Such functionality demonstrates o3 Mini’s utility for streamlining development workflows and automating intricate processes. Safety remains a top priority for OpenAI as these powerful models are developed. Through a \"Deliberative Alignment\" approach, o3 demonstrates the ability to explicitly reason over safety policies before responding to prompts, enhancing both compliance and adaptability. By integrating chain-of-thought (CoT) reasoning into its training process, the model has started to balance safety and utility in everyday use. Developers interested in learning more about the new reasoning models may continue to monitor InfoQ into the new year. o3 and o3 Mini are slated for wider availability in early 2024, with o3 Mini expected to launch by the end of January and o3 following shortly after. Until then, developers and researchers can apply for early access through OpenAI’s safety testing program. About the Author Andrew Hoblitzell",
  "image": "https://res.infoq.com/news/2024/12/openai-announces-o3/en/headerimage/generatedHeaderImage-1734968957878.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eDuring the \u003ca href=\"https://www.infoq.com/news/2024/12/openai-shipmas-12-days/\"\u003e12 days of Shipmas\u003c/a\u003e, \u003ca href=\"http://openai.com\"\u003eOpenAI\u003c/a\u003e unveiled its latest advancements in artificial intelligence with the announcement of the \u003ca href=\"https://openai.com/12-days/\"\u003eo3\u003c/a\u003e model and its counterpart, \u003ca href=\"https://openai.com/12-days/\"\u003eo3 Mini\u003c/a\u003e. These models improve reasoning capabilities over prior models, offering developers new opportunities to solve increasingly complex tasks. o3 sets a new benchmark in technical performance, particularly in coding and mathematics.\u003c/p\u003e\n\n\u003cp\u003eOn \u003ca href=\"https://openai.com/index/introducing-swe-bench-verified/\"\u003eSWE-Bench Verified\u003c/a\u003e, a coding benchmark comprised of real-world software tasks, o3 achieves a 71.7% accuracy—over 20% better than o1. Similarly, on \u003ca href=\"https://codeforces.com/\"\u003eCodeforces\u003c/a\u003e, a competitive programming platform, o3 scores a 2727 ELO under high compute settings. On the \u003ca href=\"https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination?srsltid=AfmBOooPZ-FTh08dSwBoLeQ9T5yyyQTFBHY1Yfggetimijp4Ut4DHzdr\"\u003eAmerican Invitational Mathematics Examination (AIME)\u003c/a\u003e benchmark, the model achieves 96.7% accuracy, a leap from the 83.3% attained by o1.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/12/openai-announces-o3/en/resources/41-1734969856087.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/12/openai-announces-o3/en/resources/41-1734969856087.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eOn the \u003ca href=\"https://github.com/fchollet/ARC-AGI\"\u003eARC dataset\u003c/a\u003e, an example of which is shown above, problems are designed to test an AI system\u0026#39;s ability to adapt to novel tasks. o3 scored 75.7% on the Semi-Private Evaluation set under the competition\u0026#39;s $10k compute budget (around $20 per task) and 87.5% at high-compute configurations ($2000-$3000 per task). The performance/cost tradeoff is depicted in the figure below. The ARC-AGI benchmark is specifically a challenge that previous models failed to address. o3 employs a paradigm that integrates natural language program search and execution during test time, reminiscent of techniques like \u003ca href=\"https://arxiv.org/abs/1712.01815\"\u003eAlphaZero\u003c/a\u003e’s Monte Carlo tree search and is also guided by a deep learning-based evaluator. François Chollet, the creator of the benchmark, noted the progress made by o3 while also the ongoing room for improvement.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eI don\u0026#39;t think o3 is AGI yet. o3 still fails on some very easy tasks, indicating fundamental differences with human intelligence. .. the fact is that a large ensemble of low-compute Kaggle solutions can now score 81% on the private eval. - \u003ca href=\"https://fchollet.com/\" target=\"_blank\"\u003eFrançois Chollet\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/12/openai-announces-o3/en/resources/42-1734969856087.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/12/openai-announces-o3/en/resources/42-1734969856087.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThese advancements necessitate more challenging benchmarks as the model still struggles with certain simple tasks that humans find trivial. OpenAI has focused on addressing this need by tackling Epoch AI’s \u003ca href=\"https://epoch.ai/frontiermath\"\u003eFrontier Math\u003c/a\u003e Benchmark. \u003ca href=\"https://x.com/tamaybes/status/1870333137374544077\"\u003eTamay Besiroglu\u003c/a\u003e of \u003ca href=\"http://epoch.ai\"\u003eEpochAI\u003c/a\u003e said this \u0026#34;arrives about a year ahead of my median expectations.\u0026#34; With aggressive compute settings, o3 achieves ~25% accuracy. Early testing on the forthcoming ARC-AGI-2 benchmark suggests that o3 could face significant challenges, with predictions under 30% even at high compute levels.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe hype around o3 is out of control. It’s not AGI, it’s not the singularity, and you definitely don’t have to change your worldview. - \u003cspan jsaction=\"rcuQ6b:npT2md;PYDNKe:bLV6Bd;mLt3mc\" jscontroller=\"msmzHf\"\u003e\u003ca data-sb=\"/url?sa=t\u0026amp;source=web\u0026amp;rct=j\u0026amp;opi=89978449\u0026amp;url=http://elvissaravia.com/\u0026amp;ved=2ahUKEwj164uanb6KAxUzw_ACHbLfENMQFnoECDUQAQ\u0026amp;usg=AOvVaw2m7Udup_95X7ntmmYRzCFy\" data-ved=\"2ahUKEwj164uanb6KAxUzw_ACHbLfENMQFnoECDUQAQ\" href=\"http://elvissaravia.com/\" jsname=\"UWckNb\"\u003eElvis Saravia\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eOpenAI’s development of its next-generation AI model, codenamed \u003ca href=\"https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693\"\u003eOrion\u003c/a\u003e, has encountered other hurdles. The anticipated GPT-5 model, initially expected to launch in early 2024, remains delayed as engineers grapple with rising costs, limited data, and design challenges. The growing complexity of building and training such models has pushed the estimated costs of GPT-5’s development to over $1 billion.\u003c/p\u003e\n\n\u003cp\u003eo3 Mini offers scalable thinking time options—low, medium, and high—allowing developers to balance performance with cost and latency. o3 Mini excels in code generation and problem-solving, achieving competitive ELO ratings on Codeforces and matching or exceeding o1’s performance at a fraction of the cost.\u003c/p\u003e\n\n\u003cp\u003eo3 Mini’s adaptability is exemplified in live demonstrations, where it efficiently generates complex Python scripts for automated tasks. In one example, the model created a local server that processed coding requests, executed the code, and displayed results. Such functionality demonstrates o3 Mini’s utility for streamlining development workflows and automating intricate processes.\u003c/p\u003e\n\n\u003cp\u003eSafety remains a top priority for OpenAI as these powerful models are developed. Through a \u0026#34;\u003ca href=\"https://assets.ctfassets.net/kftzwdyauwt9/4pNYAZteAQXWtloDdANQ7L/978a6fd0a2ee268b2cb59637bd074cca/OpenAI_Deliberative-Alignment-Reasoning-Enables-Safer_Language-Models_122024.pdf\"\u003eDeliberative Alignment\u003c/a\u003e\u0026#34; approach, o3 demonstrates the ability to explicitly reason over safety policies before responding to prompts, enhancing both compliance and adaptability. By integrating chain-of-thought (CoT) reasoning into its training process, the model has started to balance safety and utility in everyday use.\u003c/p\u003e\n\n\u003cp\u003eDevelopers interested in learning more about the new reasoning models may continue to monitor InfoQ into the new year. o3 and o3 Mini are slated for wider availability in early 2024, with o3 Mini expected to launch by the end of January and o3 following shortly after. Until then, developers and researchers can apply for \u003ca href=\"https://openai.com/index/early-access-for-safety-testing/\"\u003eearly access\u003c/a\u003e through OpenAI’s safety testing program.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Andrew-Hoblitzell\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAndrew Hoblitzell\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-12-25T00:00:00Z",
  "modifiedTime": null
}
