{
  "id": "6129f02f-8773-4939-aa4f-77a95646d926",
  "title": "OpenAI Developer Day 2024 (SF) Announces Real-Time API, Vision Fine-Tuning, and More",
  "link": "https://www.infoq.com/news/2024/10/openai-sf-dev-day/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "On October 1, 2024, OpenAI SF DevDay unveiled innovative features, including a Real-Time API enabling instant voice interactions and function calling. Enhanced model distillation and vision fine-tuning empower developers to customize AI for diverse applications. Upcoming events in London and Singapore will further expand these capabilities. By Andrew Hoblitzell",
  "author": "Andrew Hoblitzell",
  "published": "Thu, 10 Oct 2024 23:01:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Computer Vision",
    "Artificial Intelligence",
    "OpenAI",
    "API",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Andrew Hoblitzell",
  "length": 3380,
  "excerpt": "On October 1, 2024, OpenAI SF DevDay unveiled innovative features, including a Real-Time API enabling instant voice interactions and function calling. Enhanced model distillation and vision fine-tunin",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241001113644/apple-touch-icon.png",
  "text": "On October 1, OpenAI SF DevDay 2024 introduced several new features in addition to hosting workshops, breakout sessions, and demos. Some of the new features unveiled include a Real-Time API with function calling, vision-fine tuning, distillation, and prompt caching. The Real-Time API allows for persistent WebSocket connections, enabling real-time voice interactions. This capability is crucial for applications that require instantaneous responses like virtual assistants and real-time translation services. The API allows developers to send and receive JSON-formatted events, representing various interaction elements such as text, audio, function calls, and interruptions. The API also has the ability to handle simultaneous multimodal output. While the pricing is around $0.30 per minute, it brings in new possibilities. It even supports function calling, so the AI can perform actions—not just chat. -  Jonathan Ijzerman Code snippet to establish a socket connection, send a message from the client, and receive a response from the server. The function calling feature, demonstrated through a travel agent app, allows AI to access external tools and databases, effectively acting as an intermediary that can perform tasks beyond its pre-trained knowledge. They acknowledged the need for greater user control over safety settings, potentially through a future \"safety API.\" The O1 model also was used in a coding demo. Developers can leverage O1 to not only generate code but to understand and architect it, as demonstrated by a developer who built an iPhone app by simply describing it to O1. OpenAI did acknowledge metrics like Sweebench, which focus on code accuracy, may not fully capture the model's real-world effectiveness for some other scenarios like UI developments. OpenAI also announced they were expanding fine-tuning for vision models. This allows developers to customize AI for specific tasks. The fine-tuning framework includes options for adjusting hyperparameters, such as epochs and learning rate multipliers. An integration with Weights and Biases provides a toolset for tracking and analyzing fine-tuning jobs, offering insights into model performance. \"We continuously run automated safety evals on fine-tuned models and monitor usage to ensure applications adhere to our usage policies,\" OpenAI noted about safety concerns. OpenAI introduced a model distillation API and new evaluation tools to make their APIs more affordable. Distillation allows developers to create smaller models while trying to maintain model performance. This is crucial for deploying AI in environments with limited computational resources. Prompt caching reduces latency by reusing previously processed prompts. Developers may optimize their prompts for caching by structuring them with static content at the beginning and dynamic content at the end, ensuring maximum cache hits. \"OpenAI prompt caching is not as big a discount as Gemini and Anthropic, but works without code changes. Let's see how long they cache,\" said Shawn Wang. Other events are coming up in London October 30 and Singapore November 21. Developers interested in learning more may refer to documentation released in-line with the event. About the Author Andrew Hoblitzell",
  "image": "https://res.infoq.com/news/2024/10/openai-sf-dev-day/en/headerimage/generatedHeaderImage-1728257761391.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eOn October 1, \u003ca href=\"https://openai.com/devday/\"\u003eOpenAI SF DevDay 2024\u003c/a\u003e introduced several new features in addition to hosting workshops, breakout sessions, and demos. Some of the new features unveiled include a Real-Time API with function calling, vision-fine tuning, distillation, and prompt caching.\u003c/p\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://openai.com/index/introducing-the-realtime-api/\"\u003eReal-Time API\u003c/a\u003e allows for persistent WebSocket connections, enabling real-time voice interactions. This capability is crucial for applications that require instantaneous responses like virtual assistants and real-time translation services. The API allows developers to send and receive JSON-formatted events, representing various interaction elements such as text, audio, function calls, and interruptions. The API also has the ability to handle simultaneous multimodal output.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cspan\u003e\u003cspan dir=\"ltr\"\u003eWhile the pricing is around $0.30 per minute, it brings in new possibilities. It even supports function calling, so the AI can perform actions—not just chat. \u003c/span\u003e\u003c/span\u003e-  \u003ca href=\"https://www.linkedin.com/posts/jonathanijzerman_themachine-chatgpt-ai-activity-7247138238652461057-DUYO?utm_source=share\u0026amp;utm_medium=member_desktop\"\u003eJonathan Ijzerman\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/10/openai-sf-dev-day/en/resources/1Screenshot 2024-10-06 at 7.21.23 PM-1728257312259.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/10/openai-sf-dev-day/en/resources/1Screenshot 2024-10-06 at 7.21.23 PM-1728257312259.png\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eCode snippet to establish a socket connection, send a message from the client, and receive a response from the server.\u003c/p\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://platform.openai.com/docs/guides/function-calling\"\u003efunction calling\u003c/a\u003e feature, demonstrated through a travel agent app, allows AI to access external tools and databases, effectively acting as an intermediary that can perform tasks beyond its pre-trained knowledge. They acknowledged the need for greater user control over safety settings, potentially through a future \u0026#34;safety API.\u0026#34;\u003c/p\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://openai.com/o1/\"\u003eO1 model\u003c/a\u003e also was used in a coding demo. Developers can leverage O1 to not only generate code but to understand and architect it, as demonstrated by a developer who built an iPhone app by simply describing it to O1. OpenAI did acknowledge metrics like Sweebench, which focus on code accuracy, may not fully capture the model\u0026#39;s real-world effectiveness for some other scenarios like UI developments.\u003c/p\u003e\n\n\u003cp\u003eOpenAI also announced they were expanding \u003ca href=\"https://openai.com/index/introducing-vision-to-the-fine-tuning-api/\"\u003efine-tuning for vision models\u003c/a\u003e. This allows developers to customize AI for specific tasks. The fine-tuning framework includes options for adjusting hyperparameters, such as epochs and learning rate multipliers. An integration with Weights and Biases provides a toolset for tracking and analyzing fine-tuning jobs, offering insights into model performance. \u0026#34;\u003cspan\u003eWe continuously run automated safety evals on fine-tuned models and monitor usage to ensure applications adhere to our usage policies,\u0026#34; OpenAI noted about safety concerns.\u003c/span\u003e\u003c/p\u003e\n\n\u003cp\u003eOpenAI introduced a \u003ca href=\"https://openai.com/index/api-model-distillation/\"\u003emodel distillation API\u003c/a\u003e and new evaluation tools to make their APIs more affordable. Distillation allows developers to create smaller models while trying to maintain model performance. This is crucial for deploying AI in environments with limited computational resources. \u003ca href=\"https://openai.com/index/api-prompt-caching/\"\u003ePrompt caching\u003c/a\u003e reduces latency by reusing previously processed prompts. Developers may optimize their prompts for caching by structuring them with static content at the beginning and dynamic content at the end, ensuring maximum cache hits. \u0026#34;OpenAI prompt caching is not as big a discount as Gemini and Anthropic, but works without code changes. Let\u0026#39;s see how long they cache,\u0026#34; said \u003ca href=\"https://x.com/swyx/status/1841172919013294138?s=12\"\u003eShawn Wang\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eOther \u003ca href=\"https://community.openai.com/t/openai-devday-2024-san-francisco-london-singapore/894881\"\u003eevents\u003c/a\u003e are coming up in London October 30 and Singapore November 21. Developers interested in learning more may refer to \u003ca href=\"https://openai.com/devday/content/\"\u003edocumentation\u003c/a\u003e released in-line with the event.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Andrew-Hoblitzell\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAndrew Hoblitzell\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-10-10T00:00:00Z",
  "modifiedTime": null
}
