{
  "id": "75a45e72-ff5b-429a-964a-a8753ac2a490",
  "title": "Netflix’s Distributed Counter Abstraction",
  "link": "https://netflixtechblog.com/netflixs-distributed-counter-abstraction-8d0c45eb66b2?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Tue, 12 Nov 2024 20:45:23 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": [
    "counter",
    "software-architecture",
    "system-design-interview",
    "distributed-systems",
    "scalability"
  ],
  "byline": "Netflix Technology Blog",
  "length": 30358,
  "excerpt": "In our previous blog post, we introduced Netflix’s TimeSeries Abstraction, a distributed service designed to store and query large volumes of temporal event data with low millisecond latencies…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "By: Rajiv Shringi, Oleksii Tkachuk, Kartik SathyanarayananIntroductionIn our previous blog post, we introduced Netflix’s TimeSeries Abstraction, a distributed service designed to store and query large volumes of temporal event data with low millisecond latencies. Today, we’re excited to present the Distributed Counter Abstraction. This counting service, built on top of the TimeSeries Abstraction, enables distributed counting at scale while maintaining similar low latency performance. As with all our abstractions, we use our Data Gateway Control Plane to shard, configure, and deploy this service globally.Distributed counting is a challenging problem in computer science. In this blog post, we’ll explore the diverse counting requirements at Netflix, the challenges of achieving accurate counts in near real-time, and the rationale behind our chosen approach, including the necessary trade-offs.Note: When it comes to distributed counters, terms such as ‘accurate’ or ‘precise’ should be taken with a grain of salt. In this context, they refer to a count very close to accurate, presented with minimal delays.Use Cases and RequirementsAt Netflix, our counting use cases include tracking millions of user interactions, monitoring how often specific features or experiences are shown to users, and counting multiple facets of data during A/B test experiments, among others.At Netflix, these use cases can be classified into two broad categories:Best-Effort: For this category, the count doesn’t have to be very accurate or durable. However, this category requires near-immediate access to the current count at low latencies, all while keeping infrastructure costs to a minimum.Eventually Consistent: This category needs accurate and durable counts, and is willing to tolerate a slight delay in accuracy and a slightly higher infrastructure cost as a trade-off.Both categories share common requirements, such as high throughput and high availability. The table below provides a detailed overview of the diverse requirements across these two categories.Distributed Counter AbstractionTo meet the outlined requirements, the Counter Abstraction was designed to be highly configurable. It allows users to choose between different counting modes, such as Best-Effort or Eventually Consistent, while considering the documented trade-offs of each option. After selecting a mode, users can interact with APIs without needing to worry about the underlying storage mechanisms and counting methods.Let’s take a closer look at the structure and functionality of the API.APICounters are organized into separate namespaces that users set up for each of their specific use cases. Each namespace can be configured with different parameters, such as Type of Counter, Time-To-Live (TTL), and Counter Cardinality, using the service’s Control Plane.The Counter Abstraction API resembles Java’s AtomicInteger interface:AddCount/AddAndGetCount: Adjusts the count for the specified counter by the given delta value within a dataset. The delta value can be positive or negative. The AddAndGetCount counterpart also returns the count after performing the add operation.{ \"namespace\": \"my_dataset\", \"counter_name\": \"counter123\", \"delta\": 2, \"idempotency_token\": { \"token\": \"some_event_id\", \"generation_time\": \"2024-10-05T14:48:00Z\" }}The idempotency token can be used for counter types that support them. Clients can use this token to safely retry or hedge their requests. Failures in a distributed system are a given, and having the ability to safely retry requests enhances the reliability of the service.GetCount: Retrieves the count value of the specified counter within a dataset.{ \"namespace\": \"my_dataset\", \"counter_name\": \"counter123\"}ClearCount: Effectively resets the count to 0 for the specified counter within a dataset.{ \"namespace\": \"my_dataset\", \"counter_name\": \"counter456\", \"idempotency_token\": {...}}Now, let’s look at the different types of counters supported within the Abstraction.Types of CountersThe service primarily supports two types of counters: Best-Effort and Eventually Consistent, along with a third experimental type: Accurate. In the following sections, we’ll describe the different approaches for these types of counters and the trade-offs associated with each.Best Effort Regional CounterThis type of counter is powered by EVCache, Netflix’s distributed caching solution built on the widely popular Memcached. It is suitable for use cases like A/B experiments, where many concurrent experiments are run for relatively short durations and an approximate count is sufficient. Setting aside the complexities of provisioning, resource allocation, and control plane management, the core of this solution is remarkably straightforward:// counter cache keycounterCacheKey = \u003cnamespace\u003e:\u003ccounter_name\u003e// add operationreturn delta \u003e 0 ? cache.incr(counterCacheKey, delta, TTL) : cache.decr(counterCacheKey, Math.abs(delta), TTL);// get operationcache.get(counterCacheKey);// clear counts from all replicascache.delete(counterCacheKey, ReplicaPolicy.ALL);EVCache delivers extremely high throughput at low millisecond latency or better within a single region, enabling a multi-tenant setup within a shared cluster, saving infrastructure costs. However, there are some trade-offs: it lacks cross-region replication for the increment operation and does not provide consistency guarantees, which may be necessary for an accurate count. Additionally, idempotency is not natively supported, making it unsafe to retry or hedge requests.Edit: A note on probabilistic data structures:Probabilistic data structures like HyperLogLog (HLL) can be useful for tracking an approximate number of distinct elements, like distinct views or visits to a website, but are not ideally suited for implementing distinct increments and decrements for a given key. Count-Min Sketch (CMS) is an alternative that can be used to adjust the values of keys by a given amount. Data stores like Redis support both HLL and CMS. However, we chose not to pursue this direction for several reasons:We chose to build on top of data stores that we already operate at scale.Probabilistic data structures do not natively support several of our requirements, such as resetting the count for a given key or having TTLs for counts. Additional data structures, including more sketches, would be needed to support these requirements.On the other hand, the EVCache solution is quite simple, requiring minimal lines of code and using natively supported elements. However, it comes at the trade-off of using a small amount of memory per counter key.Eventually Consistent Global CounterWhile some users may accept the limitations of a Best-Effort counter, others opt for precise counts, durability and global availability. In the following sections, we’ll explore various strategies for achieving durable and accurate counts. Our objective is to highlight the challenges inherent in global distributed counting and explain the reasoning behind our chosen approach.Approach 1: Storing a Single Row per CounterLet’s start simple by using a single row per counter key within a table in a globally replicated datastore.Let’s examine some of the drawbacks of this approach:Lack of Idempotency: There is no idempotency key baked into the storage data-model preventing users from safely retrying requests. Implementing idempotency would likely require using an external system for such keys, which can further degrade performance or cause race conditions.Heavy Contention: To update counts reliably, every writer must perform a Compare-And-Swap operation for a given counter using locks or transactions. Depending on the throughput and concurrency of operations, this can lead to significant contention, heavily impacting performance.Secondary Keys: One way to reduce contention in this approach would be to use a secondary key, such as a bucket_id, which allows for distributing writes by splitting a given counter into buckets, while enabling reads to aggregate across buckets. The challenge lies in determining the appropriate number of buckets. A static number may still lead to contention with hot keys, while dynamically assigning the number of buckets per counter across millions of counters presents a more complex problem.Let’s see if we can iterate on our solution to overcome these drawbacks.Approach 2: Per Instance AggregationTo address issues of hot keys and contention from writing to the same row in real-time, we could implement a strategy where each instance aggregates the counts in memory and then flushes them to disk at regular intervals. Introducing sufficient jitter to the flush process can further reduce contention.However, this solution presents a new set of issues:Vulnerability to Data Loss: The solution is vulnerable to data loss for all in-memory data during instance failures, restarts, or deployments.Inability to Reliably Reset Counts: Due to counting requests being distributed across multiple machines, it is challenging to establish consensus on the exact point in time when a counter reset occurred.Lack of Idempotency: Similar to the previous approach, this method does not natively guarantee idempotency. One way to achieve idempotency is by consistently routing the same set of counters to the same instance. However, this approach may introduce additional complexities, such as leader election, and potential challenges with availability and latency in the write path.That said, this approach may still be suitable in scenarios where these trade-offs are acceptable. However, let’s see if we can address some of these issues with a different event-based approach.Approach 3: Using Durable QueuesIn this approach, we log counter events into a durable queuing system like Apache Kafka to prevent any potential data loss. By creating multiple topic partitions and hashing the counter key to a specific partition, we ensure that the same set of counters are processed by the same set of consumers. This setup simplifies facilitating idempotency checks and resetting counts. Furthermore, by leveraging additional stream processing frameworks such as Kafka Streams or Apache Flink, we can implement windowed aggregations.However, this approach comes with some challenges:Potential Delays: Having the same consumer process all the counts from a given partition can lead to backups and delays, resulting in stale counts.Rebalancing Partitions: This approach requires auto-scaling and rebalancing of topic partitions as the cardinality of counters and throughput increases.Furthermore, all approaches that pre-aggregate counts make it challenging to support two of our requirements for accurate counters:Auditing of Counts: Auditing involves extracting data to an offline system for analysis to ensure that increments were applied correctly to reach the final value. This process can also be used to track the provenance of increments. However, auditing becomes infeasible when counts are aggregated without storing the individual increments.Potential Recounting: Similar to auditing, if adjustments to increments are necessary and recounting of events within a time window is required, pre-aggregating counts makes this infeasible.Barring those few requirements, this approach can still be effective if we determine the right way to scale our queue partitions and consumers while maintaining idempotency. However, let’s explore how we can adjust this approach to meet the auditing and recounting requirements.Approach 4: Event Log of Individual IncrementsIn this approach, we log each individual counter increment along with its event_time and event_id. The event_id can include the source information of where the increment originated. The combination of event_time and event_id can also serve as the idempotency key for the write.However, in its simplest form, this approach has several drawbacks:Read Latency: Each read request requires scanning all increments for a given counter potentially degrading performance.Duplicate Work: Multiple threads might duplicate the effort of aggregating the same set of counters during read operations, leading to wasted effort and subpar resource utilization.Wide Partitions: If using a datastore like Apache Cassandra, storing many increments for the same counter could lead to a wide partition, affecting read performance.Large Data Footprint: Storing each increment individually could also result in a substantial data footprint over time. Without an efficient data retention strategy, this approach may struggle to scale effectively.The combined impact of these issues can lead to increased infrastructure costs that may be difficult to justify. However, adopting an event-driven approach seems to be a significant step forward in addressing some of the challenges we’ve encountered and meeting our requirements.How can we improve this solution further?Netflix’s ApproachWe use a combination of the previous approaches, where we log each counting activity as an event, and continuously aggregate these events in the background using queues and a sliding time window. Additionally, we employ a bucketing strategy to prevent wide partitions. In the following sections, we’ll explore how this approach addresses the previously mentioned drawbacks and meets all our requirements.Note: From here on, we will use the words “rollup” and “aggregate” interchangeably. They essentially mean the same thing, i.e., collecting individual counter increments/decrements and arriving at the final value.TimeSeries Event Store:We chose the TimeSeries Data Abstraction as our event store, where counter mutations are ingested as event records. Some of the benefits of storing events in TimeSeries include:High-Performance: The TimeSeries abstraction already addresses many of our requirements, including high availability and throughput, reliable and fast performance, and more.Reducing Code Complexity: We reduce a lot of code complexity in Counter Abstraction by delegating a major portion of the functionality to an existing service.TimeSeries Abstraction uses Cassandra as the underlying event store, but it can be configured to work with any persistent store. Here is what it looks like:Handling Wide Partitions: The time_bucket and event_bucket columns play a crucial role in breaking up a wide partition, preventing high-throughput counter events from overwhelming a given partition. For more information regarding this, refer to our previous blog.No Over-Counting: The event_time, event_id and event_item_key columns form the idempotency key for the events for a given counter, enabling clients to retry safely without the risk of over-counting.Event Ordering: TimeSeries orders all events in descending order of time allowing us to leverage this property for events like count resets.Event Retention: The TimeSeries Abstraction includes retention policies to ensure that events are not stored indefinitely, saving disk space and reducing infrastructure costs. Once events have been aggregated and moved to a more cost-effective store for audits, there’s no need to retain them in the primary storage.Now, let’s see how these events are aggregated for a given counter.Aggregating Count Events:As mentioned earlier, collecting all individual increments for every read request would be cost-prohibitive in terms of read performance. Therefore, a background aggregation process is necessary to continually converge counts and ensure optimal read performance.But how can we safely aggregate count events amidst ongoing write operations?This is where the concept of Eventually Consistent counts becomes crucial. By intentionally lagging behind the current time by a safe margin, we ensure that aggregation always occurs within an immutable window.Lets see what that looks like:Let’s break this down:lastRollupTs: This represents the most recent time when the counter value was last aggregated. For a counter being operated for the first time, this timestamp defaults to a reasonable time in the past.Immutable Window and Lag: Aggregation can only occur safely within an immutable window that is no longer receiving counter events. The “acceptLimit” parameter of the TimeSeries Abstraction plays a crucial role here, as it rejects incoming events with timestamps beyond this limit. During aggregations, this window is pushed slightly further back to account for clock skews.This does mean that the counter value will lag behind its most recent update by some margin (typically in the order of seconds). This approach does leave the door open for missed events due to cross-region replication issues. See “Future Work” section at the end.Aggregation Process: The rollup process aggregates all events in the aggregation window since the last rollup to arrive at the new value.Rollup Store:We save the results of this aggregation in a persistent store. The next aggregation will simply continue from this checkpoint.We create one such Rollup table per dataset and use Cassandra as our persistent store. However, as you will soon see in the Control Plane section, the Counter service can be configured to work with any persistent store.LastWriteTs: Every time a given counter receives a write, we also log a last-write-timestamp as a columnar update in this table. This is done using Cassandra’s USING TIMESTAMP feature to predictably apply the Last-Write-Win (LWW) semantics. This timestamp is the same as the event_time for the event. In the subsequent sections, we’ll see how this timestamp is used to keep some counters in active rollup circulation until they have caught up to their latest value.Rollup CacheTo optimize read performance, these values are cached in EVCache for each counter. We combine the lastRollupCount and lastRollupTs into a single cached value per counter to prevent potential mismatches between the count and its corresponding checkpoint timestamp.But, how do we know which counters to trigger rollups for? Let’s explore our Write and Read path to understand this better.Add/Clear Count:An add or clear count request writes durably to the TimeSeries Abstraction and updates the last-write-timestamp in the Rollup store. If the durability acknowledgement fails, clients can retry their requests with the same idempotency token without the risk of overcounting. Upon durability, we send a fire-and-forget request to trigger the rollup for the request counter.GetCount:We return the last rolled-up count as a quick point-read operation, accepting the trade-off of potentially delivering a slightly stale count. We also trigger a rollup during the read operation to advance the last-rollup-timestamp, enhancing the performance of subsequent aggregations. This process also self-remediates a stale count if any previous rollups had failed.With this approach, the counts continually converge to their latest value. Now, let’s see how we scale this approach to millions of counters and thousands of concurrent operations using our Rollup Pipeline.Rollup Pipeline:Each Counter-Rollup server operates a rollup pipeline to efficiently aggregate counts across millions of counters. This is where most of the complexity in Counter Abstraction comes in. In the following sections, we will share key details on how efficient aggregations are achieved.Light-Weight Roll-Up Event: As seen in our Write and Read paths above, every operation on a counter sends a light-weight event to the Rollup server:rollupEvent: { \"namespace\": \"my_dataset\", \"counter\": \"counter123\"}Note that this event does not include the increment. This is only an indication to the Rollup server that this counter has been accessed and now needs to be aggregated. Knowing exactly which specific counters need to be aggregated prevents scanning the entire event dataset for the purpose of aggregations.In-Memory Rollup Queues: A given Rollup server instance runs a set of in-memory queues to receive rollup events and parallelize aggregations. In the first version of this service, we settled on using in-memory queues to reduce provisioning complexity, save on infrastructure costs, and make rebalancing the number of queues fairly straightforward. However, this comes with the trade-off of potentially missing rollup events in case of an instance crash. For more details, see the “Stale Counts” section in “Future Work.”Minimize Duplicate Effort: We use a fast non-cryptographic hash like XXHash to ensure that the same set of counters end up on the same queue. Further, we try to minimize the amount of duplicate aggregation work by having a separate rollup stack that chooses to run fewer beefier instances.Availability and Race Conditions: Having a single Rollup server instance can minimize duplicate aggregation work but may create availability challenges for triggering rollups. If we choose to horizontally scale the Rollup servers, we allow threads to overwrite rollup values while avoiding any form of distributed locking mechanisms to maintain high availability and performance. This approach remains safe because aggregation occurs within an immutable window. Although the concept of now() may differ between threads, causing rollup values to sometimes fluctuate, the counts will eventually converge to an accurate value within each immutable aggregation window.Rebalancing Queues: If we need to scale the number of queues, a simple Control Plane configuration update followed by a re-deploy is enough to rebalance the number of queues. \"eventual_counter_config\": { \"queue_config\": { \"num_queues\" : 8, // change to 16 and re-deploy...Handling Deployments: During deployments, these queues shut down gracefully, draining all existing events first, while the new Rollup server instance starts up with potentially new queue configurations. There may be a brief period when both the old and new Rollup servers are active, but as mentioned before, this race condition is managed since aggregations occur within immutable windows.Minimize Rollup Effort: Receiving multiple events for the same counter doesn’t mean rolling it up multiple times. We drain these rollup events into a Set, ensuring a given counter is rolled up only once during a rollup window.Efficient Aggregation: Each rollup consumer processes a batch of counters simultaneously. Within each batch, it queries the underlying TimeSeries abstraction in parallel to aggregate events within specified time boundaries. The TimeSeries abstraction optimizes these range scans to achieve low millisecond latencies.Dynamic Batching: The Rollup server dynamically adjusts the number of time partitions that need to be scanned based on cardinality of counters in order to prevent overwhelming the underlying store with many parallel read requests.Adaptive Back-Pressure: Each consumer waits for one batch to complete before issuing the rollups for the next batch. It adjusts the wait time between batches based on the performance of the previous batch. This approach provides back-pressure during rollups to prevent overwhelming the underlying TimeSeries store.Handling Convergence:In order to prevent low-cardinality counters from lagging behind too much and subsequently scanning too many time partitions, they are kept in constant rollup circulation. For high-cardinality counters, continuously circulating them would consume excessive memory in our Rollup queues. This is where the last-write-timestamp mentioned previously plays a crucial role. The Rollup server inspects this timestamp to determine if a given counter needs to be re-queued, ensuring that we continue aggregating until it has fully caught up with the writes.Now, let’s see how we leverage this counter type to provide an up-to-date current count in near-realtime.Experimental: Accurate Global CounterWe are experimenting with a slightly modified version of the Eventually Consistent counter. Again, take the term ‘Accurate’ with a grain of salt. The key difference between this type of counter and its counterpart is that the delta, representing the counts since the last-rolled-up timestamp, is computed in real-time.And then, currentAccurateCount = lastRollupCount + deltaAggregating this delta in real-time can impact the performance of this operation, depending on the number of events and partitions that need to be scanned to retrieve this delta. The same principle of rolling up in batches applies here to prevent scanning too many partitions in parallel. Conversely, if the counters in this dataset are accessed frequently, the time gap for the delta remains narrow, making this approach of fetching current counts quite effective.Now, let’s see how all this complexity is managed by having a unified Control Plane configuration.Control PlaneThe Data Gateway Platform Control Plane manages control settings for all abstractions and namespaces, including the Counter Abstraction. Below, is an example of a control plane configuration for a namespace that supports eventually consistent counters with low cardinality:\"persistence_configuration\": [ { \"id\": \"CACHE\", // Counter cache config \"scope\": \"dal=counter\", \"physical_storage\": { \"type\": \"EVCACHE\", // type of cache storage \"cluster\": \"evcache_dgw_counter_tier1\" // Shared EVCache cluster } }, { \"id\": \"COUNTER_ROLLUP\", \"scope\": \"dal=counter\", // Counter abstraction config \"physical_storage\": { \"type\": \"CASSANDRA\", // type of Rollup store \"cluster\": \"cass_dgw_counter_uc1\", // physical cluster name \"dataset\": \"my_dataset_1\" // namespace/dataset }, \"counter_cardinality\": \"LOW\", // supported counter cardinality \"config\": { \"counter_type\": \"EVENTUAL\", // Type of counter \"eventual_counter_config\": { // eventual counter type \"internal_config\": { \"queue_config\": { // adjust w.r.t cardinality \"num_queues\" : 8, // Rollup queues per instance \"coalesce_ms\": 10000, // coalesce duration for rollups \"capacity_bytes\": 16777216 // allocated memory per queue }, \"rollup_batch_count\": 32 // parallelization factor } } } }, { \"id\": \"EVENT_STORAGE\", \"scope\": \"dal=ts\", // TimeSeries Event store \"physical_storage\": { \"type\": \"CASSANDRA\", // persistent store type \"cluster\": \"cass_dgw_counter_uc1\", // physical cluster name \"dataset\": \"my_dataset_1\", // keyspace name }, \"config\": { \"time_partition\": { // time-partitioning for events \"buckets_per_id\": 4, // event buckets within \"seconds_per_bucket\": \"600\", // smaller width for LOW card \"seconds_per_slice\": \"86400\", // width of a time slice table }, \"accept_limit\": \"5s\", // boundary for immutability }, \"lifecycleConfigs\": { \"lifecycleConfig\": [ { \"type\": \"retention\", // Event retention \"config\": { \"close_after\": \"518400s\", \"delete_after\": \"604800s\" // 7 day count event retention } } ] } }]Using such a control plane configuration, we compose multiple abstraction layers using containers deployed on the same host, with each container fetching configuration specific to its scope.ProvisioningAs with the TimeSeries abstraction, our automation uses a bunch of user inputs regarding their workload and cardinalities to arrive at the right set of infrastructure and related control plane configuration. You can learn more about this process in a talk given by one of our stunning colleagues, Joey Lynch : How Netflix optimally provisions infrastructure in the cloud.PerformanceAt the time of writing this blog, this service was processing close to 75K count requests/second globally across the different API endpoints and datasets:while providing single-digit millisecond latencies for all its endpoints:Future WorkWhile our system is robust, we still have work to do in making it more reliable and enhancing its features. Some of that work includes:Regional Rollups: Cross-region replication issues can result in missed events from other regions. An alternate strategy involves establishing a rollup table for each region, and then tallying them in a global rollup table. A key challenge in this design would be effectively communicating the clearing of the counter across regions.Error Detection and Stale Counts: Excessively stale counts can occur if rollup events are lost or if a rollup fails and isn’t retried. This isn’t an issue for frequently accessed counters, as they remain in rollup circulation. This issue is more pronounced for counters that aren’t accessed frequently. Typically, the initial read for such a counter will trigger a rollup, self-remediating the issue. However, for use cases that cannot accept potentially stale initial reads, we plan to implement improved error detection, rollup handoffs, and durable queues for resilient retries.ConclusionDistributed counting remains a challenging problem in computer science. In this blog, we explored multiple approaches to implement and deploy a Counting service at scale. While there may be other methods for distributed counting, our goal has been to deliver blazing fast performance at low infrastructure costs while maintaining high availability and providing idempotency guarantees. Along the way, we make various trade-offs to meet the diverse counting requirements at Netflix. We hope you found this blog post insightful.Stay tuned for Part 3 of Composite Abstractions at Netflix, where we’ll introduce our Graph Abstraction, a new service being built on top of the Key-Value Abstraction and the TimeSeries Abstraction to handle high-throughput, low-latency graphs.AcknowledgmentsSpecial thanks to our stunning colleagues who contributed to the Counter Abstraction’s success: Joey Lynch, Vinay Chella, Kaidan Fullerton, Tom DeVoe, Mengqing Wang, Varun Khaitan",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*Yusg6kC9Jj9ayjbi",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page---byline--8d0c45eb66b2--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://netflixtechblog.com/?source=post_page---byline--8d0c45eb66b2--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Netflix TechBlog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cp id=\"f7ea\"\u003eBy: \u003ca href=\"https://www.linkedin.com/in/rajiv-shringi/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRajiv Shringi\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/oleksii-tkachuk-98b47375/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOleksii Tkachuk\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/kartik894/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKartik Sathyanarayanan\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"0da9\"\u003eIntroduction\u003c/h2\u003e\u003cp id=\"41fb\"\u003eIn our previous blog post, we introduced \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8\"\u003eNetflix’s TimeSeries Abstraction\u003c/a\u003e, a distributed service designed to store and query large volumes of temporal event data with low millisecond latencies. Today, we’re excited to present the \u003cstrong\u003eDistributed Counter Abstraction\u003c/strong\u003e. This counting service, built on top of the TimeSeries Abstraction, enables distributed counting at scale while maintaining similar low latency performance. As with all our abstractions, we use our \u003ca href=\"https://netflixtechblog.medium.com/data-gateway-a-platform-for-growing-and-protecting-the-data-tier-f1ed8db8f5c6\" rel=\"noopener\"\u003eData Gateway Control Plane\u003c/a\u003e to shard, configure, and deploy this service globally.\u003c/p\u003e\u003cp id=\"aebc\"\u003eDistributed counting is a challenging problem in computer science. In this blog post, we’ll explore the diverse counting requirements at Netflix, the challenges of achieving accurate counts in near real-time, and the rationale behind our chosen approach, including the necessary trade-offs.\u003c/p\u003e\u003cp id=\"fb3c\"\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003cem\u003eWhen it comes to distributed counters, terms such as ‘accurate’ or ‘precise’ should be taken with a grain of salt. In this context, they refer to a count very close to accurate, presented with minimal delays.\u003c/em\u003e\u003c/p\u003e\u003ch2 id=\"21f6\"\u003eUse Cases and Requirements\u003c/h2\u003e\u003cp id=\"1e4f\"\u003eAt Netflix, our counting use cases include tracking millions of user interactions, monitoring how often specific features or experiences are shown to users, and counting multiple facets of data during \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15\"\u003eA/B test experiments\u003c/a\u003e, among others.\u003c/p\u003e\u003cp id=\"e35a\"\u003eAt Netflix, these use cases can be classified into two broad categories:\u003c/p\u003e\u003col\u003e\u003cli id=\"fc33\"\u003e\u003cstrong\u003eBest-Effort\u003c/strong\u003e: For this category, the count doesn’t have to be very accurate or durable. However, this category requires near-immediate access to the current count at low latencies, all while keeping infrastructure costs to a minimum.\u003c/li\u003e\u003cli id=\"d9a3\"\u003e\u003cstrong\u003eEventually Consistent\u003c/strong\u003e: This category needs accurate and durable counts, and is willing to tolerate a slight delay in accuracy and a slightly higher infrastructure cost as a trade-off.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"7d8e\"\u003eBoth categories share common requirements, such as high throughput and high availability. The table below provides a detailed overview of the diverse requirements across these two categories.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"db7c\"\u003eDistributed Counter Abstraction\u003c/h2\u003e\u003cp id=\"16d7\"\u003eTo meet the outlined requirements, the Counter Abstraction was designed to be highly configurable. It allows users to choose between different counting modes, such as \u003cstrong\u003eBest-Effort\u003c/strong\u003e or \u003cstrong\u003eEventually Consistent\u003c/strong\u003e, while considering the documented trade-offs of each option. After selecting a mode, users can interact with APIs without needing to worry about the underlying storage mechanisms and counting methods.\u003c/p\u003e\u003cp id=\"0799\"\u003eLet’s take a closer look at the structure and functionality of the API.\u003c/p\u003e\u003ch2 id=\"4626\"\u003eAPI\u003c/h2\u003e\u003cp id=\"0433\"\u003eCounters are organized into separate namespaces that users set up for each of their specific use cases. Each namespace can be configured with different parameters, such as Type of Counter, Time-To-Live (TTL), and Counter Cardinality, using the service’s Control Plane.\u003c/p\u003e\u003cp id=\"cc02\"\u003eThe Counter Abstraction API resembles Java’s \u003ca href=\"https://docs.oracle.com/en/java/javase/22/docs/api/java.base/java/util/concurrent/atomic/AtomicInteger.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAtomicInteger\u003c/a\u003e interface:\u003c/p\u003e\u003cp id=\"d3f4\"\u003e\u003cstrong\u003eAddCount/AddAndGetCount\u003c/strong\u003e: Adjusts the count for the specified counter by the given delta value within a dataset. The delta value can be positive or negative. The \u003cem\u003eAddAndGetCount\u003c/em\u003e counterpart also returns the count after performing the add operation.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"537a\"\u003e{\u003cbr/\u003e  \u0026#34;namespace\u0026#34;: \u0026#34;my_dataset\u0026#34;,\u003cbr/\u003e  \u0026#34;counter_name\u0026#34;: \u0026#34;counter123\u0026#34;,\u003cbr/\u003e  \u0026#34;delta\u0026#34;: 2,\u003cbr/\u003e  \u0026#34;idempotency_token\u0026#34;: { \u003cbr/\u003e    \u0026#34;token\u0026#34;: \u0026#34;some_event_id\u0026#34;,\u003cbr/\u003e    \u0026#34;generation_time\u0026#34;: \u0026#34;2024-10-05T14:48:00Z\u0026#34;\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"2e22\"\u003eThe idempotency token can be used for counter types that support them. Clients can use this token to safely retry or \u003ca href=\"https://research.google/pubs/the-tail-at-scale/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehedge\u003c/a\u003e their requests. Failures in a distributed system are a given, and having the ability to safely retry requests enhances the reliability of the service.\u003c/p\u003e\u003cp id=\"b098\"\u003e\u003cstrong\u003eGetCount\u003c/strong\u003e: Retrieves the count value of the specified counter within a dataset.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"690a\"\u003e{\u003cbr/\u003e  \u0026#34;namespace\u0026#34;: \u0026#34;my_dataset\u0026#34;,\u003cbr/\u003e  \u0026#34;counter_name\u0026#34;: \u0026#34;counter123\u0026#34;\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"a50d\"\u003e\u003cstrong\u003eClearCount\u003c/strong\u003e: Effectively resets the count to 0 for the specified counter within a dataset.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"f7fa\"\u003e{\u003cbr/\u003e  \u0026#34;namespace\u0026#34;: \u0026#34;my_dataset\u0026#34;,\u003cbr/\u003e  \u0026#34;counter_name\u0026#34;: \u0026#34;counter456\u0026#34;,\u003cbr/\u003e  \u0026#34;idempotency_token\u0026#34;: {...}\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"560d\"\u003eNow, let’s look at the different types of counters supported within the Abstraction.\u003c/p\u003e\u003ch2 id=\"3afc\"\u003eTypes of Counters\u003c/h2\u003e\u003cp id=\"0ea6\"\u003eThe service primarily supports two types of counters: \u003cstrong\u003eBest-Effort\u003c/strong\u003e and \u003cstrong\u003eEventually Consistent\u003c/strong\u003e, along with a third experimental type: \u003cstrong\u003eAccurate\u003c/strong\u003e. In the following sections, we’ll describe the different approaches for these types of counters and the trade-offs associated with each.\u003c/p\u003e\u003ch2 id=\"1042\"\u003eBest Effort Regional Counter\u003c/h2\u003e\u003cp id=\"1497\"\u003eThis type of counter is powered by \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/announcing-evcache-distributed-in-memory-datastore-for-cloud-c26a698c27f7\"\u003eEVCache\u003c/a\u003e, Netflix’s distributed caching solution built on the widely popular \u003ca href=\"https://memcached.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMemcached\u003c/a\u003e. It is suitable for use cases like A/B experiments, where many concurrent experiments are run for relatively short durations and an approximate count is sufficient. Setting aside the complexities of provisioning, resource allocation, and control plane management, the core of this solution is remarkably straightforward:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"5b19\"\u003e// counter cache key\u003cbr/\u003ecounterCacheKey = \u0026lt;namespace\u0026gt;:\u0026lt;counter_name\u0026gt;\u003cp\u003e// add operation\u003cbr/\u003ereturn delta \u0026gt; 0\u003cbr/\u003e    ? cache.incr(counterCacheKey, delta, TTL)\u003cbr/\u003e    : cache.decr(counterCacheKey, Math.abs(delta), TTL);\u003c/p\u003e\u003cp\u003e// get operation\u003cbr/\u003ecache.get(counterCacheKey);\u003c/p\u003e\u003cp\u003e// clear counts from all replicas\u003cbr/\u003ecache.delete(counterCacheKey, ReplicaPolicy.ALL);\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"70af\"\u003eEVCache delivers extremely high throughput at low millisecond latency or better within a single region, enabling a multi-tenant setup within a shared cluster, saving infrastructure costs. However, there are some trade-offs: it lacks cross-region replication for the \u003cem\u003eincrement\u003c/em\u003e operation and does not provide \u003ca href=\"https://netflix.github.io/EVCache/features/#consistency\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003econsistency guarantees\u003c/a\u003e, which may be necessary for an accurate count. Additionally, idempotency is not natively supported, making it unsafe to retry or hedge requests.\u003c/p\u003e\u003cp id=\"fbcf\"\u003e\u003cstrong\u003e\u003cem\u003eEdit\u003c/em\u003e: A note on probabilistic data structures:\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"3743\"\u003eProbabilistic data structures like \u003ca href=\"https://en.wikipedia.org/wiki/HyperLogLog\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHyperLogLog\u003c/a\u003e (HLL) can be useful for tracking an approximate number of distinct elements, like distinct views or visits to a website, but are not ideally suited for implementing distinct increments and decrements for a given key. \u003ca href=\"https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCount-Min Sketch\u003c/a\u003e (CMS) is an alternative that can be used to adjust the values of keys by a given amount. Data stores like \u003ca href=\"https://redis.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRedis\u003c/a\u003e support both \u003ca href=\"https://redis.io/docs/latest/develop/data-types/probabilistic/hyperloglogs/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHLL\u003c/a\u003e and \u003ca href=\"https://redis.io/docs/latest/develop/data-types/probabilistic/count-min-sketch/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCMS\u003c/a\u003e. However, we chose not to pursue this direction for several reasons:\u003c/p\u003e\u003cul\u003e\u003cli id=\"0426\"\u003eWe chose to build on top of data stores that we already operate at scale.\u003c/li\u003e\u003cli id=\"2d06\"\u003eProbabilistic data structures do not natively support several of our requirements, such as resetting the count for a given key or having TTLs for counts. Additional data structures, including more sketches, would be needed to support these requirements.\u003c/li\u003e\u003cli id=\"f0ea\"\u003eOn the other hand, the EVCache solution is quite simple, requiring minimal lines of code and using natively supported elements. However, it comes at the trade-off of using a small amount of memory per counter key.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"1746\"\u003eEventually Consistent Global Counter\u003c/h2\u003e\u003cp id=\"3c43\"\u003eWhile some users may accept the limitations of a Best-Effort counter, others opt for precise counts, durability and global availability. In the following sections, we’ll explore various strategies for achieving durable and accurate counts. Our objective is to highlight the challenges inherent in global distributed counting and explain the reasoning behind our chosen approach.\u003c/p\u003e\u003cp id=\"e5ff\"\u003e\u003cstrong\u003eApproach 1: Storing a Single Row per Counter\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"f787\"\u003eLet’s start simple by using a single row per counter key within a table in a globally replicated datastore.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"ca59\"\u003eLet’s examine some of the drawbacks of this approach:\u003c/p\u003e\u003cul\u003e\u003cli id=\"61e8\"\u003e\u003cstrong\u003eLack of Idempotency\u003c/strong\u003e: There is no idempotency key baked into the storage data-model preventing users from safely retrying requests. Implementing idempotency would likely require using an external system for such keys, which can further degrade performance or cause race conditions.\u003c/li\u003e\u003cli id=\"2b44\"\u003e\u003cstrong\u003eHeavy Contention\u003c/strong\u003e: To update counts reliably, every writer must perform a Compare-And-Swap operation for a given counter using locks or transactions. Depending on the throughput and concurrency of operations, this can lead to significant contention, heavily impacting performance.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"a373\"\u003e\u003cstrong\u003eSecondary Keys\u003c/strong\u003e: One way to reduce contention in this approach would be to use a secondary key, such as a \u003cem\u003ebucket_id\u003c/em\u003e, which allows for distributing writes by splitting a given counter into \u003cem\u003ebuckets\u003c/em\u003e, while enabling reads to aggregate across buckets. The challenge lies in determining the appropriate number of buckets. A static number may still lead to contention with \u003cem\u003ehot keys\u003c/em\u003e, while dynamically assigning the number of buckets per counter across millions of counters presents a more complex problem.\u003c/p\u003e\u003cp id=\"121f\"\u003eLet’s see if we can iterate on our solution to overcome these drawbacks.\u003c/p\u003e\u003cp id=\"875d\"\u003e\u003cstrong\u003eApproach 2: Per Instance Aggregation\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"ca5b\"\u003eTo address issues of hot keys and contention from writing to the same row in real-time, we could implement a strategy where each instance aggregates the counts in memory and then flushes them to disk at regular intervals. Introducing sufficient jitter to the flush process can further reduce contention.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"24b1\"\u003eHowever, this solution presents a new set of issues:\u003c/p\u003e\u003cul\u003e\u003cli id=\"dba3\"\u003e\u003cstrong\u003eVulnerability to Data Loss\u003c/strong\u003e: The solution is vulnerable to data loss for all in-memory data during instance failures, restarts, or deployments.\u003c/li\u003e\u003cli id=\"c41b\"\u003e\u003cstrong\u003eInability to Reliably Reset Counts\u003c/strong\u003e: Due to counting requests being distributed across multiple machines, it is challenging to establish consensus on the exact point in time when a counter reset occurred.\u003c/li\u003e\u003cli id=\"2535\"\u003e\u003cstrong\u003eLack of Idempotency: \u003c/strong\u003eSimilar to the previous approach, this method does not natively guarantee idempotency. One way to achieve idempotency is by consistently routing the same set of counters to the same instance. However, this approach may introduce additional complexities, such as leader election, and potential challenges with availability and latency in the write path.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"038c\"\u003eThat said, this approach may still be suitable in scenarios where these trade-offs are acceptable. However, let’s see if we can address some of these issues with a different event-based approach.\u003c/p\u003e\u003cp id=\"f599\"\u003e\u003cstrong\u003eApproach 3: Using Durable Queues\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"7eb6\"\u003eIn this approach, we log counter events into a durable queuing system like \u003ca href=\"https://kafka.apache.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eApache Kafka\u003c/a\u003e to prevent any potential data loss. By creating multiple topic partitions and hashing the counter key to a specific partition, we ensure that the same set of counters are processed by the same set of consumers. This setup simplifies facilitating idempotency checks and resetting counts. Furthermore, by leveraging additional stream processing frameworks such as \u003ca href=\"https://kafka.apache.org/documentation/streams/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKafka Streams\u003c/a\u003e or \u003ca href=\"https://flink.apache.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eApache Flink\u003c/a\u003e, we can implement windowed aggregations.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"25bf\"\u003eHowever, this approach comes with some challenges:\u003c/p\u003e\u003cul\u003e\u003cli id=\"708e\"\u003e\u003cstrong\u003ePotential Delays\u003c/strong\u003e: Having the same consumer process all the counts from a given partition can lead to backups and delays, resulting in stale counts.\u003c/li\u003e\u003cli id=\"f448\"\u003e\u003cstrong\u003eRebalancing Partitions\u003c/strong\u003e: This approach requires auto-scaling and rebalancing of topic partitions as the cardinality of counters and throughput increases.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"5f3d\"\u003eFurthermore, all approaches that pre-aggregate counts make it challenging to support two of our requirements for accurate counters:\u003c/p\u003e\u003cul\u003e\u003cli id=\"5818\"\u003e\u003cstrong\u003eAuditing of Counts\u003c/strong\u003e: Auditing involves extracting data to an offline system for analysis to ensure that increments were applied correctly to reach the final value. This process can also be used to track the provenance of increments. However, auditing becomes infeasible when counts are aggregated without storing the individual increments.\u003c/li\u003e\u003cli id=\"51be\"\u003e\u003cstrong\u003ePotential Recounting\u003c/strong\u003e: Similar to auditing, if adjustments to increments are necessary and recounting of events within a time window is required, pre-aggregating counts makes this infeasible.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"ccda\"\u003eBarring those few requirements, this approach can still be effective if we determine the right way to scale our queue partitions and consumers while maintaining idempotency. However, let’s explore how we can adjust this approach to meet the auditing and recounting requirements.\u003c/p\u003e\u003cp id=\"83bd\"\u003e\u003cstrong\u003eApproach 4: Event Log of Individual Increments\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"57ab\"\u003eIn this approach, we log each individual counter increment along with its \u003cstrong\u003eevent_time\u003c/strong\u003e and \u003cstrong\u003eevent_id\u003c/strong\u003e. The event_id can include the source information of where the increment originated. The combination of event_time and event_id can also serve as the idempotency key for the write.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"e421\"\u003eHowever, \u003cem\u003ein its simplest form\u003c/em\u003e, this approach has several drawbacks:\u003c/p\u003e\u003cul\u003e\u003cli id=\"1932\"\u003e\u003cstrong\u003eRead Latency\u003c/strong\u003e: Each read request requires scanning all increments for a given counter potentially degrading performance.\u003c/li\u003e\u003cli id=\"5891\"\u003e\u003cstrong\u003eDuplicate Work\u003c/strong\u003e: Multiple threads might duplicate the effort of aggregating the same set of counters during read operations, leading to wasted effort and subpar resource utilization.\u003c/li\u003e\u003cli id=\"973d\"\u003e\u003cstrong\u003eWide Partitions\u003c/strong\u003e: If using a datastore like \u003ca href=\"https://cassandra.apache.org/_/index.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eApache Cassandra\u003c/a\u003e, storing many increments for the same counter could lead to a \u003ca href=\"https://thelastpickle.com/blog/2019/01/11/wide-partitions-cassandra-3-11.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ewide partition\u003c/a\u003e, affecting read performance.\u003c/li\u003e\u003cli id=\"21ef\"\u003e\u003cstrong\u003eLarge Data Footprint\u003c/strong\u003e: Storing each increment individually could also result in a substantial data footprint over time. Without an efficient data retention strategy, this approach may struggle to scale effectively.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"e879\"\u003eThe combined impact of these issues can lead to increased infrastructure costs that may be difficult to justify. However, adopting an event-driven approach seems to be a significant step forward in addressing some of the challenges we’ve encountered and meeting our requirements.\u003c/p\u003e\u003cp id=\"04e4\"\u003eHow can we improve this solution further?\u003c/p\u003e\u003ch2 id=\"08e8\"\u003eNetflix’s Approach\u003c/h2\u003e\u003cp id=\"0918\"\u003eWe use a combination of the previous approaches, where we log each counting activity as an event, and continuously aggregate these events in the background using queues and a sliding time window. Additionally, we employ a bucketing strategy to prevent wide partitions. In the following sections, we’ll explore how this approach addresses the previously mentioned drawbacks and meets all our requirements.\u003c/p\u003e\u003cp id=\"ff08\"\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003cem\u003eFrom here on, we will use the words “\u003c/em\u003e\u003cstrong\u003e\u003cem\u003erollup\u003c/em\u003e\u003c/strong\u003e\u003cem\u003e” and “\u003c/em\u003e\u003cstrong\u003e\u003cem\u003eaggregate\u003c/em\u003e\u003c/strong\u003e\u003cem\u003e” interchangeably. They essentially mean the same thing, i.e., collecting individual counter increments/decrements and arriving at the final value.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"68cd\"\u003e\u003cstrong\u003eTimeSeries Event Store:\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"aa41\"\u003eWe chose the \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8\"\u003eTimeSeries Data Abstraction\u003c/a\u003e as our event store, where counter mutations are ingested as event records. Some of the benefits of storing events in TimeSeries include:\u003c/p\u003e\u003cp id=\"11da\"\u003e\u003cstrong\u003eHigh-Performance\u003c/strong\u003e: The TimeSeries abstraction already addresses many of our requirements, including high availability and throughput, reliable and fast performance, and more.\u003c/p\u003e\u003cp id=\"b03a\"\u003e\u003cstrong\u003eReducing Code Complexity\u003c/strong\u003e: We reduce a lot of code complexity in Counter Abstraction by delegating a major portion of the functionality to an existing service.\u003c/p\u003e\u003cp id=\"6c3a\"\u003eTimeSeries Abstraction uses Cassandra as the underlying event store, but it can be configured to work with any persistent store. Here is what it looks like:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"2b96\"\u003e\u003cstrong\u003eHandling Wide Partitions\u003c/strong\u003e: The \u003cem\u003etime_bucket\u003c/em\u003e and \u003cem\u003eevent_bucket\u003c/em\u003e columns play a crucial role in breaking up a wide partition, preventing high-throughput counter events from overwhelming a given partition. \u003cem\u003eFor more information regarding this, refer to our previous \u003c/em\u003e\u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8\"\u003e\u003cem\u003eblog\u003c/em\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"3dc8\"\u003e\u003cstrong\u003eNo Over-Counting\u003c/strong\u003e: The \u003cem\u003eevent_time\u003c/em\u003e, \u003cem\u003eevent_id\u003c/em\u003e and \u003cem\u003eevent_item_key\u003c/em\u003e columns form the idempotency key for the events for a given counter, enabling clients to retry safely without the risk of over-counting.\u003c/p\u003e\u003cp id=\"43a9\"\u003e\u003cstrong\u003eEvent Ordering\u003c/strong\u003e: TimeSeries orders all events in descending order of time allowing us to leverage this property for events like count resets.\u003c/p\u003e\u003cp id=\"278b\"\u003e\u003cstrong\u003eEvent Retention\u003c/strong\u003e: The TimeSeries Abstraction includes retention policies to ensure that events are not stored indefinitely, saving disk space and reducing infrastructure costs. Once events have been aggregated and moved to a more cost-effective store for audits, there’s no need to retain them in the primary storage.\u003c/p\u003e\u003cp id=\"f647\"\u003eNow, let’s see how these events are aggregated for a given counter.\u003c/p\u003e\u003cp id=\"5a6c\"\u003e\u003cstrong\u003eAggregating Count Events:\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"80b9\"\u003eAs mentioned earlier, collecting all individual increments for every read request would be cost-prohibitive in terms of read performance. Therefore, a background aggregation process is necessary to continually converge counts and ensure optimal read performance.\u003c/p\u003e\u003cp id=\"2ed6\"\u003e\u003cem\u003eBut how can we safely aggregate count events amidst ongoing write operations?\u003c/em\u003e\u003c/p\u003e\u003cp id=\"0a22\"\u003eThis is where the concept of \u003cem\u003eEventually Consistent \u003c/em\u003ecounts becomes crucial. \u003cem\u003eBy intentionally lagging behind the current time by a safe margin\u003c/em\u003e, we ensure that aggregation always occurs within an immutable window.\u003c/p\u003e\u003cp id=\"5460\"\u003eLets see what that looks like:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"a980\"\u003eLet’s break this down:\u003c/p\u003e\u003cul\u003e\u003cli id=\"c8ce\"\u003e\u003cstrong\u003elastRollupTs\u003c/strong\u003e: This represents the most recent time when the counter value was last aggregated. For a counter being operated for the first time, this timestamp defaults to a reasonable time in the past.\u003c/li\u003e\u003cli id=\"881b\"\u003e\u003cstrong\u003eImmutable Window and Lag\u003c/strong\u003e: Aggregation can only occur safely within an immutable window that is no longer receiving counter events. The “acceptLimit” parameter of the TimeSeries Abstraction plays a crucial role here, as it rejects incoming events with timestamps beyond this limit. During aggregations, this window is pushed slightly further back to account for clock skews.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"1fee\"\u003eThis does mean that the counter value will lag behind its most recent update by some margin (typically in the order of seconds). \u003cem\u003eThis approach does leave the door open for missed events due to cross-region replication issues. See “Future Work” section at the end.\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli id=\"f593\"\u003e\u003cstrong\u003eAggregation Process\u003c/strong\u003e: The rollup process aggregates all events in the aggregation window \u003cem\u003esince the last rollup \u003c/em\u003eto arrive at the new value.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"19c8\"\u003e\u003cstrong\u003eRollup Store:\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"48a1\"\u003eWe save the results of this aggregation in a persistent store. The next aggregation will simply continue from this checkpoint.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"586a\"\u003eWe create one such Rollup table \u003cem\u003eper dataset\u003c/em\u003e and use Cassandra as our persistent store. However, as you will soon see in the Control Plane section, the Counter service can be configured to work with any persistent store.\u003c/p\u003e\u003cp id=\"18db\"\u003e\u003cstrong\u003eLastWriteTs\u003c/strong\u003e: Every time a given counter receives a write, we also log a \u003cstrong\u003elast-write-timestamp\u003c/strong\u003e as a columnar update in this table. This is done using Cassandra’s \u003ca href=\"https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/cqlInsert.html#cqlInsert__timestamp-value\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eUSING TIMESTAMP\u003c/a\u003e feature to predictably apply the Last-Write-Win (LWW) semantics. This timestamp is the same as the \u003cem\u003eevent_time\u003c/em\u003e for the event. In the subsequent sections, we’ll see how this timestamp is used to keep some counters in active rollup circulation until they have caught up to their latest value.\u003c/p\u003e\u003cp id=\"336a\"\u003e\u003cstrong\u003eRollup Cache\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"25be\"\u003eTo optimize read performance, these values are cached in EVCache for each counter. We combine the \u003cstrong\u003elastRollupCount\u003c/strong\u003e and \u003cstrong\u003elastRollupTs\u003c/strong\u003e \u003cem\u003einto a single cached value per counter\u003c/em\u003e to prevent potential mismatches between the count and its corresponding checkpoint timestamp.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"1bbf\"\u003eBut, how do we know which counters to trigger rollups for? Let’s explore our Write and Read path to understand this better.\u003c/p\u003e\u003cp id=\"77ab\"\u003e\u003cstrong\u003eAdd/Clear Count:\u003c/strong\u003e\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"6b09\"\u003eAn \u003cem\u003eadd\u003c/em\u003e or \u003cem\u003eclear\u003c/em\u003e count request writes durably to the TimeSeries Abstraction and updates the last-write-timestamp in the Rollup store. If the durability acknowledgement fails, clients can retry their requests with the same idempotency token without the risk of overcounting.\u003cstrong\u003e \u003c/strong\u003eUpon durability, we send a \u003cem\u003efire-and-forget \u003c/em\u003erequest to trigger the rollup for the request counter.\u003c/p\u003e\u003cp id=\"6a87\"\u003e\u003cstrong\u003eGetCount:\u003c/strong\u003e\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"23ce\"\u003eWe return the last rolled-up count as\u003cem\u003e a quick point-read operation\u003c/em\u003e, accepting the trade-off of potentially delivering a slightly stale count. We also trigger a rollup during the read operation to advance the last-rollup-timestamp, enhancing the performance of \u003cem\u003esubsequent\u003c/em\u003e aggregations. This process also \u003cem\u003eself-remediates \u003c/em\u003ea stale count if any previous rollups had failed.\u003c/p\u003e\u003cp id=\"2bdc\"\u003eWith this approach, the counts\u003cem\u003e continually converge\u003c/em\u003e to their latest value. Now, let’s see how we scale this approach to millions of counters and thousands of concurrent operations using our Rollup Pipeline.\u003c/p\u003e\u003cp id=\"9ab5\"\u003e\u003cstrong\u003eRollup Pipeline:\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"c974\"\u003eEach \u003cstrong\u003eCounter-Rollup\u003c/strong\u003e server operates a rollup pipeline to efficiently aggregate counts across millions of counters. This is where most of the complexity in Counter Abstraction comes in. In the following sections, we will share key details on how efficient aggregations are achieved.\u003c/p\u003e\u003cp id=\"d23a\"\u003e\u003cstrong\u003eLight-Weight Roll-Up Event: \u003c/strong\u003eAs seen in our Write and Read paths above, every operation on a counter sends a light-weight event to the Rollup server:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0c58\"\u003erollupEvent: {\u003cbr/\u003e  \u0026#34;namespace\u0026#34;: \u0026#34;my_dataset\u0026#34;,\u003cbr/\u003e  \u0026#34;counter\u0026#34;: \u0026#34;counter123\u0026#34;\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"8d93\"\u003eNote that this event does not include the increment. This is only an indication to the Rollup server that this counter has been accessed and now needs to be aggregated. Knowing exactly which specific counters need to be aggregated prevents scanning the entire event dataset for the purpose of aggregations.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"0e5d\"\u003e\u003cstrong\u003eIn-Memory Rollup Queues:\u003c/strong\u003e A given Rollup server instance runs a set of \u003cem\u003ein-memory\u003c/em\u003e queues to receive rollup events and parallelize aggregations. In the first version of this service, we settled on using in-memory queues to reduce provisioning complexity, save on infrastructure costs, and make rebalancing the number of queues fairly straightforward. However, this comes with the trade-off of potentially missing rollup events in case of an instance crash. For more details, see the “Stale Counts” section in “Future Work.”\u003c/p\u003e\u003cp id=\"0d6d\"\u003e\u003cstrong\u003eMinimize Duplicate Effort\u003c/strong\u003e: We use a fast non-cryptographic hash like \u003ca href=\"https://xxhash.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eXXHash\u003c/a\u003e to ensure that the same set of counters end up on the same queue. Further, we try to minimize the amount of duplicate aggregation work by having a separate rollup stack that chooses to run \u003cem\u003efewer\u003c/em\u003e \u003cem\u003ebeefier\u003c/em\u003e instances.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"223c\"\u003e\u003cstrong\u003eAvailability and Race Conditions: \u003c/strong\u003eHaving a single Rollup server instance can minimize duplicate aggregation work but may create availability challenges for triggering rollups. \u003cem\u003eIf\u003c/em\u003e we choose to horizontally scale the Rollup servers, we allow threads to overwrite rollup values while avoiding any form of distributed locking mechanisms to maintain high availability and performance. This approach remains safe because aggregation occurs within an immutable window. Although the concept of \u003cem\u003enow()\u003c/em\u003e may differ between threads, causing rollup values to sometimes fluctuate, the counts will eventually converge to an accurate value within each immutable aggregation window.\u003c/p\u003e\u003cp id=\"acf2\"\u003e\u003cstrong\u003eRebalancing Queues\u003c/strong\u003e: If we need to scale the number of queues, a simple Control Plane configuration update followed by a re-deploy is enough to rebalance the number of queues.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"845c\"\u003e      \u0026#34;eventual_counter_config\u0026#34;: {             \u003cbr/\u003e          \u0026#34;queue_config\u0026#34;: {                    \u003cbr/\u003e            \u0026#34;num_queues\u0026#34; : 8,  // change to 16 and re-deploy\u003cbr/\u003e...\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"3a8b\"\u003e\u003cstrong\u003eHandling Deployments\u003c/strong\u003e: During deployments, these queues shut down gracefully, draining all existing events first, while the new Rollup server instance starts up with potentially new queue configurations. There may be a brief period when both the old and new Rollup servers are active, but as mentioned before, this race condition is managed since aggregations occur within immutable windows.\u003c/p\u003e\u003cp id=\"a67a\"\u003e\u003cstrong\u003eMinimize Rollup Effort\u003c/strong\u003e: Receiving multiple events for the same counter doesn’t mean rolling it up multiple times. We drain these rollup events into a Set, ensuring \u003cem\u003ea given counter is rolled up only once\u003c/em\u003e \u003cem\u003eduring a rollup window\u003c/em\u003e.\u003c/p\u003e\u003cp id=\"c500\"\u003e\u003cstrong\u003eEfficient Aggregation: \u003c/strong\u003eEach rollup consumer processes a batch of counters simultaneously. Within each batch, it queries the underlying TimeSeries abstraction in parallel to aggregate events within specified time boundaries. The TimeSeries abstraction optimizes these range scans to achieve low millisecond latencies.\u003c/p\u003e\u003cp id=\"fea5\"\u003e\u003cstrong\u003eDynamic Batching\u003c/strong\u003e: The Rollup server dynamically adjusts the number of time partitions that need to be scanned based on cardinality of counters in order to prevent overwhelming the underlying store with many parallel read requests.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"9446\"\u003e\u003cstrong\u003eAdaptive Back-Pressure\u003c/strong\u003e: Each consumer waits for one batch to complete before issuing the rollups for the next batch. It adjusts the wait time between batches based on the performance of the previous batch. This approach provides back-pressure during rollups to prevent overwhelming the underlying TimeSeries store.\u003c/p\u003e\u003cp id=\"2693\"\u003e\u003cstrong\u003eHandling Convergence\u003c/strong\u003e:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"da2c\"\u003eIn order to prevent \u003cstrong\u003elow-cardinality\u003c/strong\u003e counters from lagging behind too much and subsequently scanning too many time partitions, they are kept in constant rollup circulation. For \u003cstrong\u003ehigh-cardinality\u003c/strong\u003e counters, continuously circulating them would consume excessive memory in our Rollup queues. This is where the \u003cstrong\u003elast-write-timestamp\u003c/strong\u003e mentioned previously plays a crucial role. The Rollup server inspects this timestamp to determine if a given counter needs to be re-queued, ensuring that we continue aggregating until it has fully caught up with the writes.\u003c/p\u003e\u003cp id=\"af9d\"\u003eNow, let’s see how we leverage this counter type to provide an up-to-date current count in near-realtime.\u003c/p\u003e\u003ch2 id=\"7747\"\u003eExperimental: Accurate Global Counter\u003c/h2\u003e\u003cp id=\"8e14\"\u003eWe are experimenting with a slightly modified version of the Eventually Consistent counter. Again, take the term ‘Accurate’ with a grain of salt. The key difference between this type of counter and its counterpart is that the \u003cem\u003edelta\u003c/em\u003e, representing the counts since the last-rolled-up timestamp, is computed in real-time.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"a925\"\u003eAnd then, \u003cem\u003ecurrentAccurateCount = lastRollupCount + delta\u003c/em\u003e\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"3b68\"\u003eAggregating this delta in real-time can impact the performance of this operation, depending on the number of events and partitions that need to be scanned to retrieve this delta. The same principle of rolling up in batches applies here to prevent scanning too many partitions in parallel. Conversely, if the counters in this dataset are\u003cem\u003e \u003c/em\u003eaccessed\u003cem\u003e \u003c/em\u003efrequently, the time gap for the delta remains narrow, making this approach of fetching current counts quite effective.\u003c/p\u003e\u003cp id=\"49a2\"\u003eNow, let’s see how all this complexity is managed by having a unified Control Plane configuration.\u003c/p\u003e\u003ch2 id=\"98ba\"\u003eControl Plane\u003c/h2\u003e\u003cp id=\"47f1\"\u003eThe \u003ca href=\"https://netflixtechblog.medium.com/data-gateway-a-platform-for-growing-and-protecting-the-data-tier-f1ed8db8f5c6\" rel=\"noopener\"\u003eData Gateway Platform Control Plane\u003c/a\u003e manages control settings for all abstractions and namespaces, including the Counter Abstraction. Below, is an example of a control plane configuration for a namespace that supports eventually consistent counters with low cardinality:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"f298\"\u003e\u0026#34;persistence_configuration\u0026#34;: [\u003cbr/\u003e  {\u003cbr/\u003e    \u0026#34;id\u0026#34;: \u0026#34;CACHE\u0026#34;,                             // Counter cache config\u003cbr/\u003e    \u0026#34;scope\u0026#34;: \u0026#34;dal=counter\u0026#34;,                                                   \u003cbr/\u003e    \u0026#34;physical_storage\u0026#34;: {\u003cbr/\u003e      \u0026#34;type\u0026#34;: \u0026#34;EVCACHE\u0026#34;,                       // type of cache storage\u003cbr/\u003e      \u0026#34;cluster\u0026#34;: \u0026#34;evcache_dgw_counter_tier1\u0026#34;   // Shared EVCache cluster\u003cbr/\u003e    }\u003cbr/\u003e  },\u003cbr/\u003e  {\u003cbr/\u003e    \u0026#34;id\u0026#34;: \u0026#34;COUNTER_ROLLUP\u0026#34;,\u003cbr/\u003e    \u0026#34;scope\u0026#34;: \u0026#34;dal=counter\u0026#34;,                    // Counter abstraction config\u003cbr/\u003e    \u0026#34;physical_storage\u0026#34;: {                     \u003cbr/\u003e      \u0026#34;type\u0026#34;: \u0026#34;CASSANDRA\u0026#34;,                     // type of Rollup store\u003cbr/\u003e      \u0026#34;cluster\u0026#34;: \u0026#34;cass_dgw_counter_uc1\u0026#34;,       // physical cluster name\u003cbr/\u003e      \u0026#34;dataset\u0026#34;: \u0026#34;my_dataset_1\u0026#34;                // namespace/dataset   \u003cbr/\u003e    },\u003cbr/\u003e    \u0026#34;counter_cardinality\u0026#34;: \u0026#34;LOW\u0026#34;,              // supported counter cardinality\u003cbr/\u003e    \u0026#34;config\u0026#34;: {\u003cbr/\u003e      \u0026#34;counter_type\u0026#34;: \u0026#34;EVENTUAL\u0026#34;,              // Type of counter\u003cbr/\u003e      \u0026#34;eventual_counter_config\u0026#34;: {             // eventual counter type\u003cbr/\u003e        \u0026#34;internal_config\u0026#34;: {                  \u003cbr/\u003e          \u0026#34;queue_config\u0026#34;: {                    // adjust w.r.t cardinality\u003cbr/\u003e            \u0026#34;num_queues\u0026#34; : 8,                  // Rollup queues per instance\u003cbr/\u003e            \u0026#34;coalesce_ms\u0026#34;: 10000,              // coalesce duration for rollups\u003cbr/\u003e            \u0026#34;capacity_bytes\u0026#34;: 16777216         // allocated memory per queue\u003cbr/\u003e          },\u003cbr/\u003e          \u0026#34;rollup_batch_count\u0026#34;: 32             // parallelization factor\u003cbr/\u003e        }\u003cbr/\u003e      }\u003cbr/\u003e    }\u003cbr/\u003e  },\u003cbr/\u003e  {\u003cbr/\u003e    \u0026#34;id\u0026#34;: \u0026#34;EVENT_STORAGE\u0026#34;,\u003cbr/\u003e    \u0026#34;scope\u0026#34;: \u0026#34;dal=ts\u0026#34;,                         // TimeSeries Event store\u003cbr/\u003e    \u0026#34;physical_storage\u0026#34;: {\u003cbr/\u003e      \u0026#34;type\u0026#34;: \u0026#34;CASSANDRA\u0026#34;,                     // persistent store type\u003cbr/\u003e      \u0026#34;cluster\u0026#34;: \u0026#34;cass_dgw_counter_uc1\u0026#34;,       // physical cluster name\u003cbr/\u003e      \u0026#34;dataset\u0026#34;: \u0026#34;my_dataset_1\u0026#34;,               // keyspace name\u003cbr/\u003e    },\u003cbr/\u003e    \u0026#34;config\u0026#34;: {                              \u003cbr/\u003e      \u0026#34;time_partition\u0026#34;: {                      // time-partitioning for events\u003cbr/\u003e        \u0026#34;buckets_per_id\u0026#34;: 4,                   // event buckets within\u003cbr/\u003e        \u0026#34;seconds_per_bucket\u0026#34;: \u0026#34;600\u0026#34;,           // smaller width for LOW card\u003cbr/\u003e        \u0026#34;seconds_per_slice\u0026#34;: \u0026#34;86400\u0026#34;,          // width of a time slice table\u003cbr/\u003e      },\u003cbr/\u003e      \u0026#34;accept_limit\u0026#34;: \u0026#34;5s\u0026#34;,                    // boundary for immutability\u003cbr/\u003e    },\u003cbr/\u003e    \u0026#34;lifecycleConfigs\u0026#34;: {\u003cbr/\u003e      \u0026#34;lifecycleConfig\u0026#34;: [\u003cbr/\u003e        {\u003cbr/\u003e          \u0026#34;type\u0026#34;: \u0026#34;retention\u0026#34;,                 // Event retention\u003cbr/\u003e          \u0026#34;config\u0026#34;: {\u003cbr/\u003e            \u0026#34;close_after\u0026#34;: \u0026#34;518400s\u0026#34;,\u003cbr/\u003e            \u0026#34;delete_after\u0026#34;: \u0026#34;604800s\u0026#34;          // 7 day count event retention\u003cbr/\u003e          }\u003cbr/\u003e        }\u003cbr/\u003e      ]\u003cbr/\u003e    }\u003cbr/\u003e  }\u003cbr/\u003e]\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"9fd9\"\u003eUsing such a control plane configuration, we compose multiple abstraction layers using containers deployed on the same host, with each container fetching configuration specific to its scope.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"6176\"\u003eProvisioning\u003c/h2\u003e\u003cp id=\"ec90\"\u003eAs with the TimeSeries abstraction, our automation uses a bunch of user inputs regarding their workload and cardinalities to arrive at the right set of infrastructure and related control plane configuration. You can learn more about this process in a talk given by one of our stunning colleagues, \u003ca href=\"https://www.linkedin.com/in/joseph-lynch-9976a431/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJoey Lynch\u003c/a\u003e : \u003ca href=\"https://www.youtube.com/watch?v=Lf6B1PxIvAs\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHow Netflix optimally provisions infrastructure in the cloud\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"7e07\"\u003ePerformance\u003c/h2\u003e\u003cp id=\"f469\"\u003eAt the time of writing this blog, this service was processing close to \u003cstrong\u003e75K count requests/second\u003c/strong\u003e\u003cem\u003e globally\u003c/em\u003e across the different API endpoints and datasets:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"92ef\"\u003ewhile providing\u003cstrong\u003e single-digit millisecond\u003c/strong\u003e latencies for all its endpoints:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"3772\"\u003eFuture Work\u003c/h2\u003e\u003cp id=\"19ec\"\u003eWhile our system is robust, we still have work to do in making it more reliable and enhancing its features. Some of that work includes:\u003c/p\u003e\u003cul\u003e\u003cli id=\"bafb\"\u003e\u003cstrong\u003eRegional Rollups: \u003c/strong\u003eCross-region replication issues can result in missed events from other regions. An alternate strategy involves establishing a rollup table for each region, and then tallying them in a global rollup table. A key challenge in this design would be effectively communicating the clearing of the counter across regions.\u003c/li\u003e\u003cli id=\"7818\"\u003e\u003cstrong\u003eError Detection and Stale Counts\u003c/strong\u003e: Excessively stale counts can occur if rollup events are lost or if a rollup fails and isn’t retried. This isn’t an issue for frequently accessed counters, as they remain in rollup circulation. This issue is more pronounced for counters that aren’t accessed frequently. Typically, the initial read for such a counter will trigger a rollup,\u003cem\u003e self-remediating \u003c/em\u003ethe issue. However, for use cases that cannot accept potentially stale initial reads, we plan to implement improved error detection, rollup handoffs, and durable queues for resilient retries.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"18c4\"\u003eConclusion\u003c/h2\u003e\u003cp id=\"64c0\"\u003eDistributed counting remains a challenging problem in computer science. In this blog, we explored multiple approaches to implement and deploy a Counting service at scale. While there may be other methods for distributed counting, our goal has been to deliver blazing fast performance at low infrastructure costs while maintaining high availability and providing idempotency guarantees. Along the way, we make various trade-offs to meet the diverse counting requirements at Netflix. We hope you found this blog post insightful.\u003c/p\u003e\u003cp id=\"a883\"\u003eStay tuned for \u003cstrong\u003ePart 3 \u003c/strong\u003eof Composite Abstractions at Netflix, where we’ll introduce our \u003cstrong\u003eGraph Abstraction\u003c/strong\u003e, a new service being built on top of the \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30\"\u003eKey-Value Abstraction\u003c/a\u003e \u003cem\u003eand\u003c/em\u003e the \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8\"\u003eTimeSeries Abstraction\u003c/a\u003e to handle high-throughput, low-latency graphs.\u003c/p\u003e\u003ch2 id=\"71bd\"\u003eAcknowledgments\u003c/h2\u003e\u003cp id=\"78b5\"\u003eSpecial thanks to our stunning colleagues who contributed to the Counter Abstraction’s success: \u003ca href=\"https://www.linkedin.com/in/joseph-lynch-9976a431/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJoey Lynch\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/vinaychella/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVinay Chella\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/kaidanfullerton/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKaidan Fullerton\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/tomdevoe/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTom DeVoe\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/mengqingwang/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMengqing Wang\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/varun-khaitan/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVarun Khaitan\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "32 min read",
  "publishedTime": "2024-11-12T20:34:59.05Z",
  "modifiedTime": null
}
