{
  "id": "53c094f7-00e1-4faf-a9dc-3f2cd08f62b3",
  "title": "GitHub for Beginners: How to get LLMs to do what you want",
  "link": "https://github.blog/ai-and-ml/github-copilot/github-for-beginners-how-to-get-llms-to-do-what-you-want/",
  "description": "Learn how to write effective prompts and troubleshoot results in this installment of our GitHub for Beginners series. The post GitHub for Beginners: How to get LLMs to do what you want appeared first on The GitHub Blog.",
  "author": "Kedasha Kerr",
  "published": "Mon, 31 Mar 2025 13:00:32 +0000",
  "source": "https://github.blog/feed/",
  "categories": [
    "AI \u0026 ML",
    "GitHub Copilot",
    "GitHub Education",
    "GitHub for beginners"
  ],
  "byline": "Kedasha Kerr",
  "length": 10347,
  "excerpt": "Learn how to write effective prompts and troubleshoot results in this installment of our GitHub for Beginners series.",
  "siteName": "The GitHub Blog",
  "favicon": "https://github.blog/wp-content/uploads/2019/01/cropped-github-favicon-512.png?fit=192%2C192",
  "text": "Welcome back to season two of GitHub for Beginners, a series designed to help you navigate GitHub more confidently! So far, we’ve explored how to use GitHub Copilot and some of its essential features. Today, we will be learning all about large language models (LLMs) and the basics of prompt engineering. LLMs are powerful, and the way we interact with them via prompts matters. For example, have you ever tried asking an LLM a question, but it can’t really figure out what you’re trying to ask? Understanding the power of prompts (and the limitations that come with them) can help you become even more productive. In this post, we’ll explore: How LLMs work and how prompts are processed. How to engineer the most effective prompts. How to troubleshoot prompts when we don’t get the outcomes we want. Let’s get started! What’s an LLM? Large language models are a type of AI that are trained on a large (hence the name) amount of text data to understand and generate human-like language. By predicting the next word in a sentence based on the context of the words that came before it, LLMs respond to humans in a way that is relevant and coherent. Sort of like an ultra-smart autocomplete! When it comes to using LLMs, there are three important things to understand: Context: This is the surrounding information that helps an LLM understand what you’re talking about. Just like when you have a conversation with a friend, the more context you offer, the more likely the conversation will make sense. Tokens: For LLMs, text is broken down into units of tokens. This could be a word, part of a word, or even just one single letter. AI models process tokens to generate responses, so the number of tokens you use with an LLM can impact its response. Too few tokens can lead to a lack of context, but too many could overwhelm the AI model or run into its built-in token limits. Limitations: LLMs are powerful, but not all-powerful. Instead of understanding language like humans, LLMs rely on patterns and probabilities from training data. Taking a deeper dive into training data is beyond the scope of this post, but as a general rule, the ideal data set is diverse and broad. Models are never perfect—sometimes they can hallucinate, provide incorrect answers, or give nonsensical responses. What is a prompt? A prompt is a natural language request that asks an LLM to perform a specific task or action. A prompt gives the model context via tokens, and works around the model’s potential limitations, so that the model can give you a response. For example, if you prompt an LLM with “Write a JavaScript function to calculate the factorial of a number,” it will use its training data to give you a function that accomplishes that task. Depending on how a specific model was trained, it might process your prompt differently, and present different code. Even the same model can produce different outputs. These models are nondeterministic, which means you can prompt it the same way three times and get three different results. This is why you may receive different outputs from various models out in the world, like OpenAI’s GPT, Anthropic’s Claude, and Google’s Gemini. Now that we know what a prompt is, how do we use prompts to get the outputs we want? What is prompt engineering? Imagine that a friend is helping you complete a task. It’s important to give them clear and concise instructions if there’s a specific way the task needs to be done. The same is true for LLMs: a well-crafted prompt can help the model understand and deliver exactly what you’re looking for. The act of crafting these prompts is prompt engineering. That’s why crafting the right prompt is so important: when this is done well, prompt engineering can drastically improve the quality and relevance of the outputs you get from an LLM. Here are a few key components of effective prompting: An effective prompt is clear and precise, because ambiguity can confuse the model. It’s also important to provide enough context, but not too much detail, since this can overwhelm the LLM. If you don’t get the answer you’re expecting, don’t forget to iterate and refine your prompts! Let’s try it out! Example: How to refine prompts to be more effective Imagine you’re using GitHub Copilot and say: Write a function that will square numbers in a list in a new file with no prior code to offer Copilot context. At first, this seems like a straightforward and effective prompt. But there are a lot of factors that aren’t clear: What language should the function be written in? Do you want to include negative numbers? Will the input ever have non-numbers? Should it affect the given list or return a new list? How could we refine this prompt to be more effective? Let’s change it to: Write a Python function that takes a list of integers and returns a new list where each number is squared, excluding any negative numbers. This new prompt is clear and specific about what language we want to use, what the function should do, what constraints there are, and the expected input type. When we give GitHub Copilot more context, the output will be better aligned with what we want from it! Just like coding, prompt engineering is about effective communication. By crafting your prompts thoughtfully, you can more effectively use tools like GitHub Copilot to make your workflows smoother and more efficient. That being said, working with LLMs means there will still be some instances that call for a bit of troubleshooting. How to improve results when prompting LLMs As you continue working with GitHub Copilot and other LLM tools, you may occasionally not get the output you want. Oftentimes, it’s because your initial prompt wasn’t specific enough. Here are a few scenarios you might run into when prompting LLMs. Prompt confusion It’s easy to mix multiple requests or be unclear when writing prompts, which can confuse the model you’re using. Say you highlight something in Visual Studio Code and tell Copilot fix the errors in this code and optimize it. Is the AI supposed to fix the errors or optimize it first? For that matter, what is it supposed to optimize for? Speed, memory, or readability? To solve this, you need to break your prompt down into concrete steps with context. We can adjust this prompt by separating our asks: First, fix the errors in the code snippet. Then, optimize the fixed code for better performance. Building a prompt iteratively makes it more likely that you’ll get the result you want because the specific steps the model needs to take are more clear. Token limitations Remember, tokens are units of words or partial words that a model can handle. But there’s a limit to how many tokens a given model can handle at once (this varies by model, too—and there are different models available with GitHub Copilot). If your prompt is too long or the expected output is very extensive, the LLM may hallucinate, give a partial response, or just fail entirely. That means you want to keep your prompts concise. Again, it’s important to iterate on smaller sections of your prompt, but it’s also crucial to only provide necessary context. Does the LLM actually need an entire code file to return your desired output, or would just a few lines of code in a certain function do the trick? Instead of asking it to generate an entire application, can you ask it to make each component step-by-step? Assumption errors It’s easy to assume that the LLM knows more than it actually does. If you say add authentication to my app, does the model know what your app does? Does it know which technologies you may want to use for authentication? When crafting a prompt like this, you’ll need to explicitly state your requirements. This can be done by outlining specific needs, mentioning best practices if you have any, and once again, iterating with edge cases and restraints. By stating your requirements, you’ll help ensure the LLM doesn’t overlook critical aspects of your request when it generates the output. Prompt engineering best practices Prompt engineering can be tricky to get the hang of, but you’ll get better the more you do it. Here are some best practices to remember when working with GitHub Copilot or any other LLM: Give the model enough context while considering any limitations it might have. Prompts should be clear, concise, and precise for the best results. If you need multiple tasks completed, break down your prompts into smaller chunks and iterate from there. Be specific about your requirements and needs, so that the model accurately understands the constraints surrounding your prompt. Your next steps We covered quite a bit when it comes to prompt engineering. We went over what LLMs are and why context is important, defined prompt engineering and crafting effective prompts, and learned how to avoid common pitfalls when working with large language models. If you want to watch this demo in action, we’ve created a YouTube tutorial that accompanies this blog. If you have any questions, pop them in the GitHub Community thread and we’ll be sure to respond. Remember to sign up for GitHub Copilot (if you haven’t already) to get started for free. Join us for the next part of the series where we’ll walk through security best practices. Happy coding! Looking to learn more about GitHub Copilot?Try GitHub Copilot for free or read more about Copilot. Written by Kedasha is a Developer Advocate at GitHub where she enjoys sharing the lessons she's learned with the wider developer community. She finds joy in helping others learn about the tech industry and loves sharing her experience as a software developer. Find her online @itsthatladydev. Explore more from GitHub Docs Everything you need to master GitHub, all in one place. Go to Docs GitHub Build what’s next on GitHub, the place for anyone from anywhere to build anything. Start building Customer stories Meet the companies and engineering teams that build with GitHub. Learn more Work at GitHub! Check out our current job openings. Apply now",
  "image": "https://github.blog/wp-content/uploads/2025/02/418127171-3bd956ac-6856-4c72-8601-010f10058417.png?fit=2400%2C1260",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\n\t\n\u003cp\u003eWelcome back to season two of GitHub for Beginners, a series designed to help you navigate GitHub more confidently! So far, we’ve explored how to use GitHub Copilot and some of its essential features. Today, we will be learning all about large language models (LLMs) and the basics of prompt engineering.\u003c/p\u003e\n\u003cp\u003e\n\t\t\t\u003ciframe loading=\"lazy\" src=\"https://www.youtube.com/embed/LAF-lACf2QY?feature=oembed\" title=\"YouTube video player\" allow=\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\t\t\u003c/p\u003e\n\u003cp\u003eLLMs are powerful, and the way we interact with them via prompts matters. For example, have you ever tried asking an LLM a question, but it can’t really figure out what you’re trying to ask? Understanding the power of prompts (and the limitations that come with them) can help you become even more productive.\u003c/p\u003e\n\u003cp\u003eIn this post, we’ll explore:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHow LLMs work and how prompts are processed.  \u003c/li\u003e\n\u003cli\u003eHow to engineer the most effective prompts.  \u003c/li\u003e\n\u003cli\u003eHow to troubleshoot prompts when we don’t get the outcomes we want.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eLet’s get started!\u003c/p\u003e\n\u003ch2 id=\"whats-an-llm\" id=\"whats-an-llm\"\u003eWhat’s an LLM?\u003ca href=\"#whats-an-llm\" aria-label=\"What’s an LLM?\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eLarge language models are a type of AI that are trained on a large (hence the name) amount of text data to understand and generate human-like language.\u003c/p\u003e\n\u003cp\u003eBy \u003ca href=\"https://github.blog/ai-and-ml/generative-ai/how-ai-code-generation-works/\"\u003epredicting the next word\u003c/a\u003e in a sentence based on the context of the words that came before it, LLMs respond to humans in a way that is relevant and coherent. Sort of like an ultra-smart autocomplete!\u003c/p\u003e\n\u003cp\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/03/01-llm-process.png?resize=1024%2C565\" alt=\"This image shows the process of using an LLM: entering prompt text, LLM analysis, and then receiving a response.\" width=\"1024\" height=\"565\" srcset=\"https://github.blog/wp-content/uploads/2025/03/01-llm-process.png?w=1251 1251w, https://github.blog/wp-content/uploads/2025/03/01-llm-process.png?w=300 300w, https://github.blog/wp-content/uploads/2025/03/01-llm-process.png?w=768 768w, https://github.blog/wp-content/uploads/2025/03/01-llm-process.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/p\u003e\n\u003cp\u003eWhen it comes to using LLMs, there are three important things to understand:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eContext:\u003c/strong\u003e This is the surrounding information that helps an LLM understand what you’re talking about. Just like when you have a conversation with a friend, the more context you offer, the more likely the conversation will make sense.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/03/08-context.png?resize=1024%2C598\" alt=\"This image shows a visual example of what it’s like to gain context within a text message thread between two friends, and then a flow chart showing how the conversation went from no context at all to achieving full context.\" width=\"1024\" height=\"598\" srcset=\"https://github.blog/wp-content/uploads/2025/03/08-context.png?w=1331 1331w, https://github.blog/wp-content/uploads/2025/03/08-context.png?w=300 300w, https://github.blog/wp-content/uploads/2025/03/08-context.png?w=768 768w, https://github.blog/wp-content/uploads/2025/03/08-context.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTokens:\u003c/strong\u003e For LLMs, text is broken down into units of tokens. This could be a word, part of a word, or even just one single letter. AI models process tokens to generate responses, so the number of tokens you use with an LLM can impact its response. Too few tokens can lead to a lack of context, but too many could overwhelm the AI model or run into its built-in token limits.  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/03/02-tokens.png?resize=1024%2C568\" alt=\"This image is a visual representation of how a rare word like “Supercalifragilisticexpialidocious” would be broken down into six smaller, more common tokens, or subword pieces.\" width=\"1024\" height=\"568\" srcset=\"https://github.blog/wp-content/uploads/2025/03/02-tokens.png?w=1400 1400w, https://github.blog/wp-content/uploads/2025/03/02-tokens.png?w=300 300w, https://github.blog/wp-content/uploads/2025/03/02-tokens.png?w=768 768w, https://github.blog/wp-content/uploads/2025/03/02-tokens.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLimitations:\u003c/strong\u003e LLMs are powerful, but not all-powerful. Instead of understanding language like humans, LLMs rely on patterns and probabilities from training data. Taking a deeper dive into training data is beyond the scope of this post, but as a general rule, the ideal data set is diverse and broad. Models are never perfect—sometimes they can \u003ca href=\"https://github.blog/ai-and-ml/llms/demystifying-llms-how-they-can-do-things-they-werent-trained-to-do/#hallucinations\"\u003ehallucinate\u003c/a\u003e, provide incorrect answers, or give nonsensical responses.  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/03/09-limitations.png?resize=1024%2C568\" alt=\"This image depicts how common sense reasoning plays into prompting LLMs. It explores a prompt, shares how humans and LLMs would each understand the prompt, and shares a potential hallucination.\" width=\"1024\" height=\"568\" srcset=\"https://github.blog/wp-content/uploads/2025/03/09-limitations.png?w=1400 1400w, https://github.blog/wp-content/uploads/2025/03/09-limitations.png?w=300 300w, https://github.blog/wp-content/uploads/2025/03/09-limitations.png?w=768 768w, https://github.blog/wp-content/uploads/2025/03/09-limitations.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/p\u003e\n\u003ch2 id=\"what-is-a-prompt\" id=\"what-is-a-prompt\"\u003e\u003cstrong\u003eWhat is a prompt?\u003c/strong\u003e\u003ca href=\"#what-is-a-prompt\" aria-label=\"\u0026lt;strong\u0026gt;What is a prompt?\u0026lt;/strong\u0026gt;\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eA prompt is a natural language request that asks an LLM to perform a specific task or action. A prompt gives the model context via tokens, and works around the model’s potential limitations, so that the model can give you a response. For example, if you prompt an LLM with “Write a JavaScript function to calculate the factorial of a number,” it will use its training data to give you a function that accomplishes that task.\u003c/p\u003e\n\u003cp\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/03/03-prompt-process.png?resize=1024%2C575\" alt=\"This image shares four steps in which an LLM might process your prompt. The four steps are: input prompt, tokenization, processing, and response generation.\" width=\"1024\" height=\"575\" srcset=\"https://github.blog/wp-content/uploads/2025/03/03-prompt-process.png?w=1331 1331w, https://github.blog/wp-content/uploads/2025/03/03-prompt-process.png?w=300 300w, https://github.blog/wp-content/uploads/2025/03/03-prompt-process.png?w=768 768w, https://github.blog/wp-content/uploads/2025/03/03-prompt-process.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/p\u003e\n\u003cp\u003eDepending on how a specific model was trained, it might process your prompt differently, and present different code. Even the same model can produce different outputs. These models are nondeterministic, which means you can prompt it the same way three times and get three different results. This is why you may receive different outputs from various models out in the world, like OpenAI’s GPT, Anthropic’s Claude, and Google’s Gemini.\u003c/p\u003e\n\u003cp\u003eNow that we know what a prompt is, how do we use prompts to get the outputs we want?\u003c/p\u003e\n\u003ch2 id=\"what-is-prompt-engineering\" id=\"what-is-prompt-engineering\"\u003e\u003cstrong\u003eWhat is prompt engineering?\u003c/strong\u003e\u003ca href=\"#what-is-prompt-engineering\" aria-label=\"\u0026lt;strong\u0026gt;What is prompt engineering?\u0026lt;/strong\u0026gt;\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eImagine that a friend is helping you complete a task. It’s important to give them clear and concise instructions if there’s a specific way the task needs to be done. The same is true for LLMs: a well-crafted prompt can help the model understand and deliver exactly what you’re looking for. The act of crafting these prompts is prompt engineering.\u003c/p\u003e\n\u003cp\u003eThat’s why crafting the right prompt is so important: when this is done well, \u003ca href=\"https://github.blog/ai-and-ml/generative-ai/prompt-engineering-guide-generative-ai-llms/\"\u003eprompt engineering\u003c/a\u003e can drastically improve the quality and relevance of the outputs you get from an LLM.\u003c/p\u003e\n\u003cp\u003eHere are a few key components of effective prompting:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn effective prompt is \u003cstrong\u003eclear and precise\u003c/strong\u003e, because ambiguity can confuse the model.   \u003c/li\u003e\n\u003cli\u003eIt’s also important to provide \u003cem\u003eenough\u003c/em\u003e \u003cstrong\u003econtext\u003c/strong\u003e, but not too much detail, since this can overwhelm the LLM.   \u003c/li\u003e\n\u003cli\u003eIf you don’t get the answer you’re expecting, don’t forget to \u003cstrong\u003eiterate and refine\u003c/strong\u003e your prompts! \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eLet’s try it out!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExample: How to refine prompts to be more effective\u003c/strong\u003e\u003cbr/\u003e\nImagine you’re using GitHub Copilot and say: \u003ccode\u003eWrite a function that will square numbers in a list\u003c/code\u003e in a new file with no prior code to offer Copilot context. At first, this seems like a straightforward and effective prompt. But there are a lot of factors that aren’t clear:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhat language should the function be written in?   \u003c/li\u003e\n\u003cli\u003eDo you want to include negative numbers?   \u003c/li\u003e\n\u003cli\u003eWill the input ever have non-numbers?   \u003c/li\u003e\n\u003cli\u003eShould it affect the given list or return a new list?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHow could we refine this prompt to be more effective? Let’s change it to: \u003ccode\u003eWrite a Python function that takes a list of integers and returns a new list where each number is squared, excluding any negative numbers.\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThis new prompt is clear and specific about what language we want to use, what the function should do, what constraints there are, and the expected input type. When we give GitHub Copilot more context, the output will be better aligned with what we want from it!\u003c/p\u003e\n\u003cp\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/03/04-good-communicator.png?resize=1024%2C612\" alt=\"This image consists of white text on a black background sharing that prompt engineering is the same thing as being a good communicator.\" width=\"1024\" height=\"612\" srcset=\"https://github.blog/wp-content/uploads/2025/03/04-good-communicator.png?w=1331 1331w, https://github.blog/wp-content/uploads/2025/03/04-good-communicator.png?w=300 300w, https://github.blog/wp-content/uploads/2025/03/04-good-communicator.png?w=768 768w, https://github.blog/wp-content/uploads/2025/03/04-good-communicator.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/p\u003e\n\u003cp\u003eJust like coding, prompt engineering is about effective communication. By crafting your prompts thoughtfully, you can more effectively use tools like GitHub Copilot to make your workflows smoother and more efficient. That being said, working with LLMs means there will still be some instances that call for a bit of troubleshooting.\u003c/p\u003e\n\n\u003ch2 id=\"how-to-improve-results-when-prompting-llms\" id=\"how-to-improve-results-when-prompting-llms\"\u003e\u003cstrong\u003eHow to improve results when prompting LLMs\u003c/strong\u003e\u003ca href=\"#how-to-improve-results-when-prompting-llms\" aria-label=\"\u0026lt;strong\u0026gt;How to improve results when prompting LLMs\u0026lt;/strong\u0026gt;\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAs you continue working with GitHub Copilot and other LLM tools, you may occasionally not get the output you want. Oftentimes, it’s because your initial prompt wasn’t specific enough. Here are a few scenarios you might run into when prompting LLMs.\u003c/p\u003e\n\u003ch3 id=\"prompt-confusion\" id=\"prompt-confusion\"\u003e\u003cstrong\u003ePrompt confusion\u003c/strong\u003e\u003ca href=\"#prompt-confusion\" aria-label=\"\u0026lt;strong\u0026gt;Prompt confusion\u0026lt;/strong\u0026gt;\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eIt’s easy to mix multiple requests or be unclear when writing prompts, which can confuse the model you’re using. Say you highlight something in Visual Studio Code and tell Copilot \u003ccode\u003efix the errors in this code and optimize it.\u003c/code\u003e Is the AI supposed to fix the errors or optimize it first? For that matter, what is it supposed to optimize for? Speed, memory, or readability?\u003c/p\u003e\n\u003cp\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/03/05-prompt-confusion.png?resize=1024%2C612\" alt=\"This image depicts how to overcome prompt confusion, or mixing multiple requests or unclear instructions. First, you’d fix errors, then optimize code, and finally add tests.\" width=\"1024\" height=\"612\" srcset=\"https://github.blog/wp-content/uploads/2025/03/05-prompt-confusion.png?w=1331 1331w, https://github.blog/wp-content/uploads/2025/03/05-prompt-confusion.png?w=300 300w, https://github.blog/wp-content/uploads/2025/03/05-prompt-confusion.png?w=768 768w, https://github.blog/wp-content/uploads/2025/03/05-prompt-confusion.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/p\u003e\n\u003cp\u003eTo solve this, you need to break your prompt down into concrete steps with context. We can adjust this prompt by separating our asks: \u003ccode\u003eFirst, fix the errors in the code snippet. Then, optimize the fixed code for better performance.\u003c/code\u003e Building a prompt iteratively makes it more likely that you’ll get the result you want because the specific steps the model needs to take are more clear.\u003c/p\u003e\n\u003ch3 id=\"token-limitations\" id=\"token-limitations\"\u003e\u003cstrong\u003eToken limitations\u003c/strong\u003e\u003ca href=\"#token-limitations\" aria-label=\"\u0026lt;strong\u0026gt;Token limitations\u0026lt;/strong\u0026gt;\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eRemember, tokens are units of words or partial words that a model can handle. But there’s a limit to how many tokens a given model can handle at once (this varies by model, too—and \u003ca href=\"https://docs.github.com/en/copilot/using-github-copilot/ai-models\"\u003ethere are different models available with GitHub Copilot\u003c/a\u003e). If your prompt is too long or the expected output is very extensive, the LLM may hallucinate, give a partial response, or just fail entirely.\u003c/p\u003e\n\u003cp\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/03/06-token-limitations.png?resize=1024%2C612\" alt=\"This image depicts how to overcome token limitations, since LLMs have a maximum token limit for input and output. You would need to break down large inputs into smaller chunks.\" width=\"1024\" height=\"612\" srcset=\"https://github.blog/wp-content/uploads/2025/03/06-token-limitations.png?w=1331 1331w, https://github.blog/wp-content/uploads/2025/03/06-token-limitations.png?w=300 300w, https://github.blog/wp-content/uploads/2025/03/06-token-limitations.png?w=768 768w, https://github.blog/wp-content/uploads/2025/03/06-token-limitations.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/p\u003e\n\u003cp\u003eThat means you want to keep your prompts concise. Again, it’s important to iterate on smaller sections of your prompt, but it’s also crucial to only provide necessary context. Does the LLM actually need an entire code file to return your desired output, or would just a few lines of code in a certain function do the trick? Instead of asking it to generate an entire application, can you ask it to make each component step-by-step?\u003c/p\u003e\n\u003ch3 id=\"assumption-errors\" id=\"assumption-errors\"\u003e\u003cstrong\u003eAssumption errors\u003c/strong\u003e\u003ca href=\"#assumption-errors\" aria-label=\"\u0026lt;strong\u0026gt;Assumption errors\u0026lt;/strong\u0026gt;\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eIt’s easy to assume that the LLM knows more than it actually does. If you say \u003ccode\u003eadd authentication to my app,\u003c/code\u003e does the model know what your app does? Does it know which technologies you may want to use for authentication?\u003c/p\u003e\n\u003cp\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/03/07-assumption-errors.png?resize=1024%2C612\" alt=\"This image depicts how to overcome assumption errors, or when you assume LLM has context it doesn’t have. You’d need to explicitly state requirements, outline specific needs, mention best practices if needed, and then iterate with edge cases and restraints.\" width=\"1024\" height=\"612\" srcset=\"https://github.blog/wp-content/uploads/2025/03/07-assumption-errors.png?w=1331 1331w, https://github.blog/wp-content/uploads/2025/03/07-assumption-errors.png?w=300 300w, https://github.blog/wp-content/uploads/2025/03/07-assumption-errors.png?w=768 768w, https://github.blog/wp-content/uploads/2025/03/07-assumption-errors.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/p\u003e\n\u003cp\u003eWhen crafting a prompt like this, you’ll need to explicitly state your requirements. This can be done by outlining specific needs, mentioning best practices if you have any, and once again, iterating with edge cases and restraints. By stating your requirements, you’ll help ensure the LLM doesn’t overlook critical aspects of your request when it generates the output.\u003c/p\u003e\n\n\u003ch2 id=\"prompt-engineering-best-practices\" id=\"prompt-engineering-best-practices\"\u003e\u003cstrong\u003ePrompt engineering best practices\u003c/strong\u003e\u003ca href=\"#prompt-engineering-best-practices\" aria-label=\"\u0026lt;strong\u0026gt;Prompt engineering best practices\u0026lt;/strong\u0026gt;\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003ePrompt engineering can be tricky to get the hang of, but you’ll get better the more you do it. Here are some best practices to remember when working with GitHub Copilot or any other LLM:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGive the model enough context while considering any limitations it might have.  \u003c/li\u003e\n\u003cli\u003ePrompts should be clear, concise, and precise for the best results.  \u003c/li\u003e\n\u003cli\u003eIf you need multiple tasks completed, break down your prompts into smaller chunks and iterate from there.  \u003c/li\u003e\n\u003cli\u003eBe specific about your requirements and needs, so that the model accurately understands the constraints surrounding your prompt. \u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"your-next-steps\" id=\"your-next-steps\"\u003e\u003cstrong\u003eYour next steps\u003c/strong\u003e\u003ca href=\"#your-next-steps\" aria-label=\"\u0026lt;strong\u0026gt;Your next steps\u0026lt;/strong\u0026gt;\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe covered quite a bit when it comes to prompt engineering. We went over what LLMs are and why context is important, defined prompt engineering and crafting effective prompts, and learned how to avoid common pitfalls when working with large language models.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf you want to watch this demo in action, we’ve created a \u003cstrong\u003e\u003ca href=\"https://youtu.be/LAF-lACf2QY\"\u003eYouTube tutorial\u003c/a\u003e\u003c/strong\u003e that accompanies this blog.  \u003c/li\u003e\n\u003cli\u003eIf you have any questions, pop them in the \u003ca href=\"https://github.com/orgs/community/discussions/152688\"\u003e\u003cstrong\u003eGitHub Community thread\u003c/strong\u003e\u003c/a\u003e and we’ll be sure to respond.  \u003c/li\u003e\n\u003cli\u003eRemember to \u003ca href=\"https://gh.io/gfb-copilot\"\u003esign up for GitHub Copilot\u003c/a\u003e (if you haven’t already) to get started for free.  \u003c/li\u003e\n\u003cli\u003eJoin us for the next part of the series where we’ll walk through security best practices. \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHappy coding!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLooking to learn more about GitHub Copilot?\u003c/strong\u003e\u003cbr/\u003eTry \u003ca href=\"https://github.com/features/copilot\"\u003eGitHub Copilot\u003c/a\u003e for free or read more about \u003ca href=\"https://docs.github.com/en/copilot/about-github-copilot/what-is-github-copilot\"\u003eCopilot\u003c/a\u003e.\u003c/p\u003e\n\n\t\n\n\t\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tWritten by\t\u003c/h2\u003e\n\t\n\t\t\t\u003carticle\u003e\n\t\u003cdiv\u003e\n\t\t\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cpicture\u003e\n\t\t\t\t\t\u003csource srcset=\"https://avatars.githubusercontent.com/u/47188731?v=4\u0026amp;s=200\" width=\"120\" height=\"120\" media=\"(min-width: 768px)\"/\u003e\n\t\t\t\t\t\u003cimg src=\"https://avatars.githubusercontent.com/u/47188731?v=4\u0026amp;s=200\" alt=\"Kedasha Kerr\" width=\"80\" height=\"80\" loading=\"lazy\" decoding=\"async\"/\u003e\n\t\t\t\t\u003c/picture\u003e\n\t\t\t\u003c/div\u003e\n\t\t\t\t\n\t\t\t\t\t\u003cp\u003eKedasha is a Developer Advocate at GitHub where she enjoys sharing the lessons she\u0026#39;s learned with the wider developer community. She finds joy in helping others learn about the tech industry and loves sharing her experience as a software developer. Find her online @itsthatladydev.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003c/article\u003e\n\t\u003c/div\u003e\n\u003c/section\u003e\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tExplore more from GitHub\t\u003c/h2\u003e\n\t\u003cdiv\u003e\n\t\t\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon-Circle.svg\" width=\"44\" height=\"44\" alt=\"Docs\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tDocs\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eEverything you need to master GitHub, all in one place.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Go to Docs; ref_location:bottom recirculation;\" href=\"https://docs.github.com/\" target=\"_blank\" aria-label=\"Go to Docs\"\u003e\n\t\t\t\t\tGo to Docs\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_95220f.svg\" width=\"44\" height=\"44\" alt=\"GitHub\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tGitHub\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eBuild what’s next on GitHub, the place for anyone from anywhere to build anything.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Start building; ref_location:bottom recirculation;\" href=\"https://github.blog/developer-skills/github/\" target=\"_blank\" aria-label=\"Start building\"\u003e\n\t\t\t\t\tStart building\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" fill=\"none\"\u003e\u003cpath fill=\"currentColor\" d=\"M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z\"\u003e\u003c/path\u003e\u003cpath stroke=\"currentColor\" d=\"M1.75 8H11\" stroke-width=\"1.5\" stroke-linecap=\"round\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_da43dc.svg\" width=\"44\" height=\"44\" alt=\"Customer stories\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tCustomer stories\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eMeet the companies and engineering teams that build with GitHub.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Learn more; ref_location:bottom recirculation;\" href=\"https://github.com/customer-stories\" target=\"_blank\" aria-label=\"Learn more\"\u003e\n\t\t\t\t\tLearn more\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2022/05/careers.svg\" width=\"44\" height=\"44\" alt=\"Work at GitHub!\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tWork at GitHub!\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eCheck out our current job openings.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Apply now; ref_location:bottom recirculation;\" href=\"https://www.github.careers/careers-home\" target=\"_blank\" aria-label=\"Apply now\"\u003e\n\t\t\t\t\tApply now\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\t\u003c/div\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-03-31T13:00:32Z",
  "modifiedTime": null
}
