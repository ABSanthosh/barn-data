{
  "id": "6426357f-be49-469b-a532-203ee04c8e4c",
  "title": "Inference with Gemma using Dataflow and vLLM",
  "link": "https://developers.googleblog.com/en/inference-with-gemma-using-dataflow-and-vllm/",
  "description": "vLLM's continuous batching and Dataflow's model manager optimizes LLM serving and simplifies the deployment process, delivering a powerful combination for developers to build high-performance LLM inference pipelines more efficiently.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Danny McCormick",
  "length": 9312,
  "excerpt": "vLLM's continuous batching and Dataflow's model manager optimizes LLM serving and simplifies the deployment process, delivering a powerful combination for developers to build high-performance LLM inference pipelines more efficiently.",
  "siteName": "",
  "favicon": "",
  "text": "Large language models (LLMs) like Gemma are powerful and versatile. They can translate languages, write different kinds of text content, and answer your questions in an informative way. However, deploying these LLMs to production, especially for streaming use cases, can be a significant challenge.This blog post will explore how to use two state-of-the-art tools, vLLM and Dataflow, to efficiently deploy LLMs at scale with minimal code. First, we will lay out how vLLM uses continuous batching to serve LLMs more efficiently. Second, we will describe how Dataflow's model manager makes deploying vLLM and other large model frameworks simple.What is vLLM?vLLM is an open-source library specifically designed for high-throughput and low-latency LLM inference. It optimizes the serving of LLMs by employing several specialized techniques, including continuous batching.To understand how continuous batching works, let's first look at how models traditionally batch inputs. GPUs excel at parallel processing, where multiple computations are performed simultaneously. Batching allows the GPU to utilize all of its available cores to work on an entire batch of data at once, rather than processing each input individually. This significantly speeds up the inference process; often, performing inference on 8 input records at once uses similar resources as performing inference on a single record.You can think of batching as being similar to a restaurant kitchen: instead of preparing each dish individually, the chef can group similar orders and cook them together, saving time and resources. If you have 8 stovetops, it takes a similar amount of time and effort to make either a single omelet or 8 omelets. There are some downsides, however, to traditional batching. Most importantly in this context, batching does not work as well when it takes different amounts of time to perform inference. Since most frameworks don't have access to or knowledge about the underlying mechanisms for performing inference, they typically just wait for all requests to complete before taking on a new batch. This means that a single slow record can consume all of a GPUs capacity even if the other records in the batch have completed, leading to slower and more costly jobs. When running inference with a Large Language Model (LLM), waiting for a whole batch to complete can be prohibitively slow and expensive. This is because the amount of time it takes to generate an inference for a record has a 1:1 correlation to the length of the record. For example, imagine that we batch the following 2 requests to an LLM:What is the capital of Mexico?2. What are some of the cultural differences and similarities between Mexico and the United States?We'd expect a short answer to question (1) and a long answer to question (2). Because it takes much longer to answer question (2), though, we have to wait for that question to complete while it monopolizes our GPU before we can return any of the results from the batch. Continuous batching allows vLLM to update batches while the request is still running. It achieves this by leveraging how LLMs perform inference: a looping process of repeatedly generating the next token in their response. So really, when generating the sentence \"The capital of Mexico is Mexico City\", we're running inference 7 times (once per output word). Rather than batching inputs once, vLLM's continuous batching technique allows it to recompute a batch every time the LLM runs generates a set of tokens for a batch. It can add requests to the batch on the fly and return early results when one record from a batch is completely done. vLLM's dynamic batching and other optimizations have been shown to improve inference throughput by 2-4x for popular LLMs in some cases, making it a very useful tool for model serving. For more information about vLLM, check out this white paper.Using vLLM in DataflowDeploying a vLLM instance within a streaming pipeline can be complex. Traditionally, you would need to:Spin up a single vLLM server in the middle of your pipeline. This is often not trivial since most streaming systems spin up multiple worker processes, so you need to elect a leader to spin up a dedicated vLLM process.Ensure all worker processes can communicate with that singleton server.Monitor and productionize the service so that it is tolerant to vLLM server failures.This involves a lot of multiprocessing, which can be time-consuming, error-prone, and requires specialized expertise. It also requires a deep understanding of the underlying topology. This topology can often change if you want to try different machine configurations as well (for example to compare performance on an 8 core machine and a 16 core machine).Fortunately, Dataflow simplifies this process with its model manager. This feature abstracts away the complexities of managing and deploying models within a pipeline. By default, Dataflow provisions one worker process per available core on its worker machines. These processes are responsible for handling I/O into and out of the worker as well as any transformations which are performed on the data, and they operate entirely independently. For most pipelines, including data prep pipelines for ML use cases, this is the optimal topology. This breaks down, however, for pipelines which need to serve large models like one of the Gemma models. It is neither cost efficient nor performant to load a copy of large models into every process because pipelines will most likely run into out of memory issues. The ideal topology for most such pipelines is to load only a single copy of the large model.Dataflow's model manager was built to allow users to control the exact number of copies of a model which are deployed in their pipeline, regardless of network topology. When you apply the RunInference transform, Dataflow is able to understand your intent so that it can create the ideal topology for your pipeline and deploy the optimal number of models. All you need to do is supply some configuration parameters. When using vLLM, instead of loading a model, Dataflow's model manager spins up a single vLLM instance in a dedicated inference process. Worker processes can then efficiently send records to this instance for inference. This model manager allows Dataflow to fully take advantage of vLLM's continuous batching; when a worker receives an incoming request, it asynchronously appends it to vLLMs queue of requests and waits for the response, allowing vLLM to dynamically batch as many requests as it is able to.Dataflow's model manager and RunInference transform make it incredibly easy to incorporate vLLM into your pipeline. You only need to specify some configuration details and a few lines of code. Because Dataflow is able to understand your underlying intent, it configures the whole rest of the pipeline topology for you. In 5 lines of code, you can write a full end to end pipeline to read your data, run it through vLLM, and output it to a sink. model_handler = VLLMCompletionsModelHandler('google/gemma-2b') with beam.Pipeline() as p: _ = (p | beam.ReadFromSource(\u003cconfig\u003e) | RunInference(model_handler) # Send the prompts to vLLM and get responses. | beam.WriteToSink(\u003cconfig\u003e)) You can find a complete pipeline and run it yourself here: https://cloud.google.com/dataflow/docs/notebooks/run_inference_vllmPerformancevLLM significantly boosts the performance of LLM inference in Dataflow pipelines. To compare vLLM's performance against a naive pipeline using fixed size batches, we ran 2 pipelines with a single worker with T4 GPUs. Each pipeline read in prompts from the P3 dataset, ran them against the google/gemma-2b model, and logged the result.When using a naive (default) batching strategy, it took 59.137 vCPU hours to process 10,000 prompts. When using vLLM with continuous batching, it took only 2.481 vCPU hours to process the same 10,000 prompts. That is an over 23x improvement!There are some caveats here: specifically, no tuning was done on either pipeline, and the naive pipeline likely would've performed significantly better if tuned to use larger or more uniform batches. With that said, that is part of the magic of vLLM; with less than 20 lines of code and no tuning work, we are able to produce a highly performant LLM serving pipeline! If we wanted to compare another model, we could do so in a performant manner by changing a single string in our model handler!Next StepsBy combining the power of vLLM and Dataflow, you can efficiently deploy and scale LLMs for your streaming applications with ease. To learn more about how you can do this, try running through this example notebook: https://cloud.google.com/dataflow/docs/notebooks/run_inference_vllm.To learn more about Gemma models and some of the things you can do with them, check out the Gemma docs: https://ai.google.dev/gemma/docsTo learn more about vLLM and some of the other mechanisms it uses to optimize serving, visit the vLLM docs: https://docs.vllm.ai/en/latest/",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma-Dataflow-ML-vLLM.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"o98bt\"\u003eLarge language models (LLMs) like Gemma are powerful and versatile. They can translate languages, write different kinds of text content, and answer your questions in an informative way. However, deploying these LLMs to production, especially for streaming use cases, can be a significant challenge.\u003c/p\u003e\u003cp data-block-key=\"26u6l\"\u003eThis blog post will explore how to use two state-of-the-art tools, \u003ca href=\"https://docs.vllm.ai/en/latest/\"\u003e\u003cb\u003evLLM\u003c/b\u003e\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/dataflow/docs/machine-learning\"\u003e\u003cb\u003eDataflow\u003c/b\u003e\u003c/a\u003e, to efficiently deploy LLMs at scale with minimal code. First, we will lay out how vLLM uses \u003cb\u003econtinuous batching\u003c/b\u003e to serve LLMs more efficiently. Second, we will describe how Dataflow\u0026#39;s \u003cb\u003emodel manager\u003c/b\u003e makes deploying vLLM and other large model frameworks simple.\u003c/p\u003e\u003ch2 data-block-key=\"8q3ga\"\u003e\u003cbr/\u003eWhat is vLLM?\u003c/h2\u003e\u003cp data-block-key=\"2skke\"\u003evLLM is an open-source library specifically designed for high-throughput and low-latency LLM inference. It optimizes the serving of LLMs by employing several specialized techniques, including \u003cb\u003econtinuous batching\u003c/b\u003e.\u003c/p\u003e\u003cp data-block-key=\"aq8mr\"\u003eTo understand how continuous batching works, let\u0026#39;s first look at how models traditionally batch inputs. GPUs excel at parallel processing, where multiple computations are performed simultaneously. Batching allows the GPU to utilize all of its available cores to work on an entire batch of data at once, rather than processing each input individually. This significantly speeds up the inference process; often, performing inference on 8 input records at once uses similar resources as performing inference on a single record.\u003c/p\u003e\u003cp data-block-key=\"e5cp4\"\u003eYou can think of batching as being similar to a restaurant kitchen: instead of preparing each dish individually, the chef can group similar orders and cook them together, saving time and resources. If you have 8 stovetops, it takes a similar amount of time and effort to make either a single omelet or 8 omelets.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image10_UKfHU5W.original.png\" alt=\"without batching, inference is serial\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"o98bt\"\u003eThere are some downsides, however, to traditional batching. Most importantly in this context, batching does not work as well when it takes different amounts of time to perform inference. Since most frameworks don\u0026#39;t have access to or knowledge about the underlying mechanisms for performing inference, they typically just wait for all requests to complete before taking on a new batch. This means that a single slow record can consume all of a GPUs capacity even if the other records in the batch have completed, leading to slower and more costly jobs.\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image4_wWH3T2u.original.png\" alt=\"batches of varying lengths lead to wasted compute\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"o98bt\"\u003eWhen running inference with a Large Language Model (LLM), waiting for a whole batch to complete can be prohibitively slow and expensive. This is because the amount of time it takes to generate an inference for a record has a 1:1 correlation to the length of the record. For example, imagine that we batch the following 2 requests to an LLM:\u003c/p\u003e\u003col\u003e\u003cli data-block-key=\"apvp0\"\u003eWhat is the capital of Mexico?\u003c/li\u003e\u003c/ol\u003e\u003cp data-block-key=\"75l0p\"\u003e2. What are some of the cultural differences and similarities between Mexico and the United States?\u003c/p\u003e\u003cp data-block-key=\"1q3tl\"\u003e\u003cbr/\u003eWe\u0026#39;d expect a short answer to question (1) and a long answer to question (2). Because it takes much longer to answer question (2), though, we have to wait for that question to complete while it monopolizes our GPU before we can return any of the results from the batch.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_49Awb3Q.original.png\" alt=\"prompts of varying length lead to wasted compute\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"o98bt\"\u003e\u003cb\u003eContinuous batching\u003c/b\u003e allows vLLM to update batches while the request is still running. It achieves this by leveraging how LLMs perform inference: a looping process of repeatedly generating the next token in their response. So really, when generating the sentence \u0026#34;The capital of Mexico is Mexico City\u0026#34;, we\u0026#39;re running inference 7 times (once per output word). Rather than batching inputs once, vLLM\u0026#39;s continuous batching technique allows it to recompute a batch every time the LLM runs generates a set of tokens for a batch. It can add requests to the batch on the fly and return early results when one record from a batch is completely done.\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_FSuNuVs.original.png\" alt=\"continuous batching fully utilizes the GPU\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"o98bt\"\u003evLLM\u0026#39;s dynamic batching and other optimizations have been shown to improve inference throughput by 2-4x for popular LLMs in some cases, making it a very useful tool for model serving. For more information about vLLM, check out this \u003ca href=\"https://arxiv.org/pdf/2309.06180\"\u003ewhite paper\u003c/a\u003e.\u003c/p\u003e\u003ch2 data-block-key=\"crmlb\"\u003e\u003cbr/\u003eUsing vLLM in Dataflow\u003c/h2\u003e\u003cp data-block-key=\"ijig\"\u003eDeploying a vLLM instance within a streaming pipeline can be complex. Traditionally, you would need to:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"bgu7a\"\u003eSpin up a single vLLM server in the middle of your pipeline. This is often not trivial since most streaming systems spin up multiple worker processes, so you need to elect a leader to spin up a dedicated vLLM process.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"7oded\"\u003eEnsure all worker processes can communicate with that singleton server.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"9kh18\"\u003eMonitor and productionize the service so that it is tolerant to vLLM server failures.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"a5omd\"\u003e\u003cbr/\u003eThis involves a lot of multiprocessing, which can be time-consuming, error-prone, and requires specialized expertise. It also requires a deep understanding of the underlying topology. This topology can often change if you want to try different machine configurations as well (for example to compare performance on an 8 core machine and a 16 core machine).\u003c/p\u003e\u003cp data-block-key=\"cb0l\"\u003eFortunately, Dataflow simplifies this process with its \u003ca href=\"https://github.com/apache/beam/blob/88ada9dfee3c4602ff62b0f6ebdded29518760c9/sdks/python/apache_beam/ml/inference/base.py#L342\"\u003e\u003cb\u003emodel manager\u003c/b\u003e\u003c/a\u003e. This feature abstracts away the complexities of managing and deploying models within a pipeline. By default, Dataflow provisions one worker process per available core on its worker machines. These processes are responsible for handling I/O into and out of the worker as well as any transformations which are performed on the data, and they operate entirely independently. For most pipelines, including data prep pipelines for ML use cases, this is the optimal topology.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image8_ZigJ8bU.original.png\" alt=\"Beam worker processes operate independently in parallel\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"00rll\"\u003eThis breaks down, however, for pipelines which need to serve large models like one of the Gemma models. It is neither cost efficient nor performant to load a copy of large models into every process because pipelines will most likely run into out of memory issues. The ideal topology for most such pipelines is to load only a single copy of the large model.\u003c/p\u003e\u003cp data-block-key=\"7f1oc\"\u003eDataflow\u0026#39;s model manager was built to allow users to control the \u003ca href=\"https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.ModelHandler.model_copies\"\u003eexact number of copies\u003c/a\u003e of a model which are deployed in their pipeline, regardless of network topology. When you apply the \u003ca href=\"https://beam.apache.org/documentation/ml/about-ml/#use-runinference\"\u003eRunInference\u003c/a\u003e transform, Dataflow is able to understand your intent so that it can create the ideal topology for your pipeline and deploy the optimal number of models. All you need to do is supply some configuration parameters.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image7_IPsafe8.original.png\" alt=\"the model manager allows Beam workers to share a single model\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"00rll\"\u003eWhen using vLLM, instead of loading a model, Dataflow\u0026#39;s model manager spins up a single vLLM instance in a dedicated inference process. Worker processes can then efficiently send records to this instance for inference.\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image9_tbQZOID.original.png\" alt=\"the model manager allows Beam to spin up a dedicated vLLM process\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"00rll\"\u003eThis model manager allows Dataflow to fully take advantage of vLLM\u0026#39;s continuous batching; when a worker receives an incoming request, it asynchronously appends it to vLLMs queue of requests and waits for the response, allowing vLLM to dynamically batch as many requests as it is able to.\u003c/p\u003e\u003cp data-block-key=\"esgsu\"\u003eDataflow\u0026#39;s model manager and RunInference transform make it incredibly easy to incorporate vLLM into your pipeline. You only need to specify some configuration details and a few lines of code. Because Dataflow is able to understand your underlying intent, it configures the whole rest of the pipeline topology for you. In 5 lines of code, you can write a full end to end pipeline to read your data, run it through vLLM, and output it to a sink.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003emodel_handler\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eVLLMCompletionsModelHandler\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#39;google/gemma-2b\u0026#39;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003ewith\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ePipeline\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e \u003cspan\u003eas\u003c/span\u003e \u003cspan\u003ep\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n  \u003cspan\u003e_\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ep\u003c/span\u003e \u003cspan\u003e|\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eReadFromSource\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003econfig\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n         \u003cspan\u003e|\u003c/span\u003e \u003cspan\u003eRunInference\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel_handler\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003e# Send the prompts to vLLM and get responses.\u003c/span\u003e\n         \u003cspan\u003e|\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eWriteToSink\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003econfig\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"00rll\"\u003eYou can find a complete pipeline and run it yourself here: \u003ca href=\"https://cloud.google.com/dataflow/docs/notebooks/run_inference_vllm\"\u003ehttps://cloud.google.com/dataflow/docs/notebooks/run_inference_vllm\u003c/a\u003e\u003c/p\u003e\u003ch3 data-block-key=\"f1g06\"\u003e\u003cb\u003e\u003cbr/\u003ePerformance\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"8fvmo\"\u003evLLM significantly boosts the performance of LLM inference in Dataflow pipelines. To compare vLLM\u0026#39;s performance against a naive pipeline using fixed size batches, we ran 2 pipelines with a single worker with T4 GPUs. Each pipeline read in prompts from the \u003ca href=\"https://huggingface.co/datasets/bigscience/P3?row=0\"\u003eP3 dataset\u003c/a\u003e, ran them against the google/gemma-2b model, and logged the result.\u003c/p\u003e\u003cp data-block-key=\"1hvtv\"\u003eWhen using a naive (default) batching strategy, it took 59.137 vCPU hours to process 10,000 prompts. When using vLLM with continuous batching, it took only 2.481 vCPU hours to process the same 10,000 prompts. That is an over 23x improvement!\u003c/p\u003e\u003cp data-block-key=\"a2f0u\"\u003eThere are some caveats here: specifically, no tuning was done on either pipeline, and the naive pipeline likely would\u0026#39;ve performed significantly better if tuned to use larger or more uniform batches. With that said, that is part of the magic of vLLM; with less than 20 lines of code and no tuning work, we are able to produce a highly performant LLM serving pipeline! If we wanted to compare another model, we could do so in a performant manner by changing a single string in our model handler!\u003c/p\u003e\u003ch2 data-block-key=\"2sj35\"\u003e\u003cbr/\u003eNext Steps\u003c/h2\u003e\u003cp data-block-key=\"djd5f\"\u003eBy combining the power of vLLM and Dataflow, you can efficiently deploy and scale LLMs for your streaming applications with ease. To learn more about how you can do this, try running through this example notebook: \u003ca href=\"https://cloud.google.com/dataflow/docs/notebooks/run_inference_vllm\"\u003ehttps://cloud.google.com/dataflow/docs/notebooks/run_inference_vllm\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"8psaf\"\u003eTo learn more about Gemma models and some of the things you can do with them, check out the Gemma docs: \u003ca href=\"https://ai.google.dev/gemma/docs\"\u003ehttps://ai.google.dev/gemma/docs\u003c/a\u003e\u003c/p\u003e\u003cp data-block-key=\"56j5o\"\u003eTo learn more about vLLM and some of the other mechanisms it uses to optimize serving, visit the vLLM docs: \u003ca href=\"https://docs.vllm.ai/en/latest/\"\u003ehttps://docs.vllm.ai/en/latest/\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2024-11-13T00:00:00Z",
  "modifiedTime": null
}
