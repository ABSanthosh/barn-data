{
  "id": "9f57f31b-1a54-485a-89bf-751bf0e7816d",
  "title": "Solving the inference problem for open source AI projects with GitHub Models",
  "link": "https://github.blog/ai-and-ml/llms/solving-the-inference-problem-for-open-source-ai-projects-with-github-models/",
  "description": "How using GitHubâ€™s free inference API can make your AI-powered open source software more accessible. The post Solving the inference problem for open source AI projects with GitHub Models appeared first on The GitHub Blog.",
  "author": "Sean Goedecke",
  "published": "Wed, 23 Jul 2025 16:00:00 +0000",
  "source": "https://github.blog/feed/",
  "categories": [
    "AI \u0026 ML",
    "LLMs",
    "AI models",
    "CI/CD",
    "GitHub Models"
  ],
  "byline": "Sean Goedecke",
  "length": 7801,
  "excerpt": "How using GitHubâ€™s free inference API can make your AI-powered open source software more accessible.",
  "siteName": "The GitHub Blog",
  "favicon": "https://github.blog/wp-content/uploads/2019/01/cropped-github-favicon-512.png?fit=192%2C192",
  "text": "AI features can make an open source project shine. At least, until setup asks for a paid inference API key.Â  Requiring contributors or even casual users to bring their own large language model (LLM) key stops adoption in its tracks: $ my-cool-ai-tool Error: OPENAI_API_KEY not found Developers may not want to buy a paid plan just to try out your tool, and self hosting a model can be too heavy for laptops or GitHub Actions runners.Â  GitHub Models solves that friction with a free, OpenAI-compatible inference API that every GitHub account can use with no new keys, consoles, or SDKs required. In this article, weâ€™ll show you how to drop it into your project, run it in CI/CD, and scale when your community takes off. Letâ€™s jump in. AI features feel ubiquitous today, but getting them running locally is still a challenge for a few reasons: Paid APIs: The simplest path is to ask users for an OpenAI or Anthropic key. Thatâ€™s a non-starter for many hobbyists and students because paid APIs are too expensive. Local models: Running a 2 B-parameter LLM can work for lightweight tasks, but anything that requires more intelligence will quickly blow past typical laptop memory â€” let alone the 14 GB container that backs a GitHub Actions runner. Docker images and weights: You can bundle a model with your app, but distributing multi-gigabyte weights balloons install size and slows CI. Every additional requirement filters out potential users and contributors. What you need is an inference endpoint thatâ€™s: Free for public projects Compatible with existing OpenAI SDKs Available wherever your code runs, like your laptop, server, or Actions runner Thatâ€™s what GitHub Models provides. GitHub Models in a nutshell What it is: A REST endpoint that speaks the chat/completions spec you already know. What you get: A curated set of models (GPT-4o, DeepSeek-R1, Llama 3, and more) hosted by GitHub. Who can call it: Anyone with a GitHub Personal Access Token (PAT), or a repositoryâ€™s built-in GITHUB_TOKEN when you opt-in via permissions. How much it costs: Free tier for all personal accounts and OSS orgs; metered paid tier unlocks higher throughput and larger context windows. Because the API mirrors OpenAIâ€™s, any client that accepts a baseURL will work without code changes. This includes OpenAI-JS, OpenAI Python, LangChain, llamacpp, or your own curl script. How to get started with GitHub Models Since GitHub Models is compatible with the OpenAI chat/completions API, almost every inference SDK can use it. To get started, you can use the OpenAI SDK: import OpenAI from \"openai\"; const openai = new OpenAI({ baseURL: \"https://models.github.ai/inference/chat/completions\", apiKey: process.env.GITHUB_TOKEN // or any PAT with models:read }); const res = await openai.chat.completions.create({ model: \"openai/gpt-4o\", messages: [{ role: \"user\", content: \"Hi!\" }] }); console.log(res.choices[0].message.content); If you write your AI open source software with GitHub Models as an inference provider, all GitHub users will be able to get up and running with it just by supplying a GitHub Personal Access Token (PAT). And if your software runs in GitHub Actions, your users wonâ€™t even need to supply a PAT. By requesting the models: read permission in your workflow file, the built-in GitHub token will have permissions to make inference requests to GitHub Models. This means you can build a whole array of AI-powered Actions that can be shared and installed with a single click. For instance: Code review or PR triage bots Smart issue tagging workflows Weekly repository activity report generators And anything else that a GitHub Action can do Plus, using GitHub Models makes it easy for your users to set up AI inference. And that has another positive effect: itâ€™s easier for your contributors to set up AI inference as well. When anyone with a GitHub account can run your code end to end, youâ€™ll be able to get contributions from the whole range of GitHub users, not just the ones with an OpenAI key. Zero-configuration CI with GitHub Actions Publishing an Action that relies on AI used to require users to add their inference API key as a GitHub Actions secret. Now you can ship a one-click install: yaml # .github/workflows/triage.yml permissions: contents: read issues: write models: read # ðŸ‘ˆ unlocks GitHub Models for the GITHUB_TOKEN jobs: triage: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Smart issue triage run: node scripts/triage.js The runnerâ€™s GITHUB_TOKEN carries the models:read scope, so your Action can call any model without extra setup. This makes it well suited for: Automated pull request summaries Issue deduplication and tagging Weekly repository digests Anything else you can script in an Action Scaling when your project takes off The GitHub Models inference API is free for everyone. But if you or your users want to do more inference than the free rate limits allow, you can turn on paid inference in your settings for significantly larger context windows and higher requests-per-minute.Â  When your community grows, so will traffic. So itâ€™s important to consider the following:Â  Requests per minute (RPM): While the free tier offers default limits, the paid tier offers multiples higher. Context window: Free tier tops out at standard model limits; paid enables 128k tokens on supported models. Latency: The paid tier runs in its own separate deployment, so youâ€™re not in the same queue as free tier users. To get started, you can enable paid usage in Settings \u003e Models for your org or enterprise. Your existing clients and tokens will keep working (but theyâ€™ll be faster and support bigger contexts). Take this with you LLMs are transforming how developers build and ship software, but requiring users to supply their own paid API key can be a barrier to entry. The magic only happens when the first npm install, cargo run, or go test just works. If you maintain an AI-powered open source codebase, you should consider adding GitHub Models as a default inference provider. Your users already have free AI inference via GitHub, so thereâ€™s little downside to letting them use it with your code. Thatâ€™s doubly true if your project is able to run in GitHub Actions. The best API key is no API key! By making high-quality inference a free default for every developer on GitHub, GitHub Models gets rid of the biggest blocker to OSS AI adoption. And that opens the door to more contributions, faster onboarding, and happier users. Want to give it a try? Check out the GitHub Models documentation or jump straight into the API reference and start shipping AI features that just work today. Want to give it a try? Check out the GitHub Models documentation or jump straight into the API reference and start shipping AI features that just work today. Written by Sean is a software engineer at GitHub, working on GitHub Models. Related posts Explore more from GitHub Docs Everything you need to master GitHub, all in one place. Go to Docs GitHub Build whatâ€™s next on GitHub, the place for anyone from anywhere to build anything. Start building Customer stories Meet the companies and engineering teams that build with GitHub. Learn more GitHub Universe 2025 Last chance: Save $700 on your IRL pass to Universe and join us on Oct. 28-29 in San Francisco. Register now",
  "image": "https://github.blog/wp-content/uploads/2025/07/wallpaper-generic-image-logo-github.png?fit=1920%2C1080",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\n\t\n\u003cp\u003eAI features can make an open source project shine. At least, until setup asks for a paid inference API key.Â  Requiring contributors or even casual users to bring their own large language model (LLM) key stops adoption in its tracks:\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003e$ my-cool-ai-tool\nError: OPENAI_API_KEY not found\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eDevelopers may not want to buy a paid plan just to try out your tool, and self hosting a model can be too heavy for laptops or GitHub Actions runners.Â \u003c/p\u003e\n\n\n\n\u003cp\u003eGitHub Models solves that friction with a free, OpenAI-compatible inference API that every GitHub account can use with no new keys, consoles, or SDKs required. In this article, weâ€™ll show you how to drop it into your project, run it in CI/CD, and scale when your community takes off.\u003c/p\u003e\n\n\n\n\u003cp\u003eLetâ€™s jump in.\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cp\u003eAI features feel ubiquitous today, but getting them running locally is still a challenge for a few reasons:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePaid APIs: \u003c/strong\u003eThe simplest path is to ask users for an OpenAI or Anthropic key. Thatâ€™s a non-starter for many hobbyists and students because paid APIs are too expensive.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eLocal models: \u003c/strong\u003eRunning a 2 B-parameter LLM can work for lightweight tasks, but anything that requires more intelligence will quickly blow past typical laptop memory â€” let alone the 14 GB container that backs a GitHub Actions runner.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eDocker images and weights: \u003c/strong\u003eYou can bundle a model with your app, but distributing multi-gigabyte weights balloons install size and slows CI.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eEvery additional requirement filters out potential users and contributors. What you need is an inference endpoint thatâ€™s:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eFree for public projects\u003c/li\u003e\n\n\n\n\u003cli\u003eCompatible with existing OpenAI SDKs\u003c/li\u003e\n\n\n\n\u003cli\u003eAvailable wherever your code runs, like your laptop, server, or Actions runner\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eThatâ€™s what GitHub Models provides.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-github-models-in-a-nutshell\"\u003eGitHub Models in a nutshell\u003c/h3\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eWhat it is: \u003c/strong\u003eA REST endpoint that speaks the chat/completions spec you already know.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eWhat you get: \u003c/strong\u003eA curated set of models (GPT-4o, DeepSeek-R1, Llama 3, and more) hosted by GitHub.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eWho can call it: \u003c/strong\u003eAnyone with a GitHub Personal Access Token (PAT), or a repositoryâ€™s built-in GITHUB_TOKEN when you opt-in via permissions.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eHow much it costs:\u003c/strong\u003e Free tier for all personal accounts and OSS orgs; metered paid tier unlocks higher throughput and larger context windows.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eBecause the API mirrors OpenAIâ€™s, any client that accepts a baseURL will work without code changes. This includes OpenAI-JS, OpenAI Python, LangChain, llamacpp, or your own curl script.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-to-get-started-with-github-models\"\u003eHow to get started with GitHub Models\u003c/h2\u003e\n\n\n\n\u003cp\u003eSince GitHub Models is compatible with the OpenAI \u003ccode\u003echat/completions\u003c/code\u003e API, almost every inference SDK can use it. To get started, you can use the OpenAI SDK:\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003eimport OpenAI from \u0026#34;openai\u0026#34;;\n\nconst openai = new OpenAI({\n  baseURL: \u0026#34;https://models.github.ai/inference/chat/completions\u0026#34;,\n  apiKey: process.env.GITHUB_TOKEN  // or any PAT with models:read\n});\n\nconst res = await openai.chat.completions.create({\n  model: \u0026#34;openai/gpt-4o\u0026#34;,\n  messages: [{ role: \u0026#34;user\u0026#34;, content: \u0026#34;Hi!\u0026#34; }]\n});\nconsole.log(res.choices[0].message.content);\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eIf you write your AI open source software with GitHub Models as an inference provider, all GitHub users will be able to get up and running with it just by supplying a GitHub Personal Access Token (PAT).\u003c/p\u003e\n\n\n\n\u003cp\u003eAnd if your software runs in GitHub Actions, your users wonâ€™t even need to supply a PAT. By requesting the \u003ccode\u003emodels: read\u003c/code\u003e permission in your workflow file, the built-in GitHub token will have permissions to make inference requests to GitHub Models. This means you can build a whole array of AI-powered Actions that can be shared and installed with a single click. For instance:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eCode review or PR triage bots\u003c/li\u003e\n\n\n\n\u003cli\u003eSmart issue tagging workflows\u003c/li\u003e\n\n\n\n\u003cli\u003eWeekly repository activity report generators\u003c/li\u003e\n\n\n\n\u003cli\u003eAnd anything else that a GitHub Action can do\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003ePlus, using GitHub Models makes it easy for your users to set up AI inference. And that has another positive effect: itâ€™s easier for your \u003cem\u003econtributors\u003c/em\u003e to set up AI inference as well. When anyone with a GitHub account can run your code end to end, youâ€™ll be able to get contributions from the whole range of GitHub users, not just the ones with an OpenAI key.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-zero-configuration-ci-with-github-actions\"\u003eZero-configuration CI with GitHub Actions\u003c/h3\u003e\n\n\n\n\u003cp\u003ePublishing an Action that relies on AI used to require users to add their inference API key as a GitHub Actions secret. Now you can ship a one-click install:\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003eyaml \n\n# .github/workflows/triage.yml\npermissions:\n  contents: read\n  issues: write\n  models: read   # ðŸ‘ˆ unlocks GitHub Models for the GITHUB_TOKEN\n\njobs:\n  triage:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Smart issue triage\n        run: node scripts/triage.js\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eThe runnerâ€™s \u003ccode\u003eGITHUB_TOKEN\u003c/code\u003e carries the \u003ccode\u003emodels:read\u003c/code\u003e scope, so your Action can call any model without extra setup. This makes it well suited for:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eAutomated pull request summaries\u003c/li\u003e\n\n\n\n\u003cli\u003eIssue deduplication and tagging\u003c/li\u003e\n\n\n\n\u003cli\u003eWeekly repository digests\u003c/li\u003e\n\n\n\n\u003cli\u003eAnything else you can script in an Action\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch2 id=\"scaling-when-your-project-takes-off\"\u003eScaling when your project takes off\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe GitHub Models inference API is free for everyone. But if you or your users want to do more inference than the free rate limits allow, you can turn on \u003ca href=\"https://docs.github.com/en/billing/managing-billing-for-your-products/about-billing-for-github-models\"\u003epaid inference\u003c/a\u003e in your settings for significantly larger context windows and higher requests-per-minute.Â \u003c/p\u003e\n\n\n\n\u003cp\u003eWhen your community grows, so will traffic. So itâ€™s important to consider the following:Â \u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRequests per minute (RPM)\u003c/strong\u003e: While the free tier offers default limits, the paid tier offers multiples higher.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eContext window\u003c/strong\u003e: Free tier tops out at standard model limits; paid enables 128k tokens on supported models.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eLatency\u003c/strong\u003e: The paid tier runs in its own separate deployment, so youâ€™re not in the same queue as free tier users.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eTo get started, you can enable paid usage in \u003cstrong\u003eSettings \u0026gt; Models \u003c/strong\u003efor your org or enterprise. Your existing clients and tokens will keep working (but theyâ€™ll be faster and support bigger contexts).\u003c/p\u003e\n\n\n\n\u003ch2 id=\"take-this-with-you\"\u003eTake this with you\u003c/h2\u003e\n\n\n\n\u003cp\u003eLLMs are transforming how developers build and ship software, but requiring users to supply their own paid API key can be a barrier to entry. The magic only happens when the first \u003ccode\u003enpm install\u003c/code\u003e, \u003ccode\u003ecargo run\u003c/code\u003e, or \u003ccode\u003ego test\u003c/code\u003e just works.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you maintain an AI-powered open source codebase, you should consider adding GitHub Models as a default inference provider. Your users already have free AI inference via GitHub, so thereâ€™s little downside to letting them use it with your code. Thatâ€™s doubly true if your project is able to run in GitHub Actions. The best API key is no API key!\u003c/p\u003e\n\n\n\n\u003cp\u003eBy making high-quality inference a free default for every developer on GitHub, GitHub Models gets rid of the biggest blocker to OSS AI adoption. And that opens the door to more contributions, faster onboarding, and happier users.\u003c/p\u003e\n\n\n\n\u003cp\u003eWant to give it a try? \u003cstrong\u003eCheck out the\u003c/strong\u003e\u003ca href=\"https://docs.github.com/en/github-models\"\u003e\u003cstrong\u003e GitHub Models documentation\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e or jump straight into the\u003c/strong\u003e\u003ca href=\"https://docs.github.com/en/github-models/reference\"\u003e\u003cstrong\u003e API reference\u003c/strong\u003e\u003c/a\u003e and start shipping AI features that just work today.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\n\u003cp\u003eWant to give it a try? \u003cstrong\u003eCheck out the\u003c/strong\u003e\u003ca href=\"https://docs.github.com/en/github-models\"\u003e\u003cstrong\u003e GitHub Models documentation\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e or jump straight into the\u003c/strong\u003e\u003ca href=\"https://docs.github.com/en/github-models/reference\"\u003e\u003cstrong\u003e API reference\u003c/strong\u003e\u003c/a\u003e and start shipping AI features that just work today.\u003c/p\u003e\n\u003c/div\u003e\n\n\t\n\n\t\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tWritten by\t\u003c/h2\u003e\n\t\n\t\t\t\u003carticle\u003e\n\t\u003cdiv\u003e\n\t\t\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cpicture\u003e\n\t\t\t\t\t\u003csource srcset=\"https://avatars.githubusercontent.com/u/19204567?v=4\u0026amp;s=200\" width=\"120\" height=\"120\" media=\"(min-width: 768px)\"/\u003e\n\t\t\t\t\t\u003cimg src=\"https://avatars.githubusercontent.com/u/19204567?v=4\u0026amp;s=200\" alt=\"Sean Goedecke\" width=\"80\" height=\"80\" loading=\"lazy\" decoding=\"async\"/\u003e\n\t\t\t\t\u003c/picture\u003e\n\t\t\t\u003c/div\u003e\n\t\t\t\t\n\t\t\t\t\t\u003cp\u003eSean is a software engineer at GitHub, working on GitHub Models.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003c/article\u003e\n\t\u003c/div\u003e\n\u003c/section\u003e\u003csection\u003e\n\t\u003ch2\u003e\n\t\tRelated posts\t\u003c/h2\u003e\n\t\n\u003c/section\u003e\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tExplore more from GitHub\t\u003c/h2\u003e\n\t\u003cdiv\u003e\n\t\t\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon-Circle.svg\" width=\"44\" height=\"44\" alt=\"Docs\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tDocs\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eEverything you need to master GitHub, all in one place.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Go to Docs; ref_location:bottom recirculation;\" href=\"https://docs.github.com/\" target=\"_blank\" aria-label=\"Go to Docs\"\u003e\n\t\t\t\t\tGo to Docs\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_95220f.svg\" width=\"44\" height=\"44\" alt=\"GitHub\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tGitHub\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eBuild whatâ€™s next on GitHub, the place for anyone from anywhere to build anything.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Start building; ref_location:bottom recirculation;\" href=\"https://github.com/\" target=\"_blank\" aria-label=\"Start building\"\u003e\n\t\t\t\t\tStart building\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_da43dc.svg\" width=\"44\" height=\"44\" alt=\"Customer stories\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tCustomer stories\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eMeet the companies and engineering teams that build with GitHub.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Learn more; ref_location:bottom recirculation;\" href=\"https://github.com/customer-stories\" target=\"_blank\" aria-label=\"Learn more\"\u003e\n\t\t\t\t\tLearn more\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/04/Universe24-North_Star.svg\" width=\"44\" height=\"44\" alt=\"GitHub Universe 2025\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tGitHub Universe 2025\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eLast chance: Save $700 on your IRL pass to Universe and join us on Oct. 28-29 in San Francisco.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Register now; ref_location:bottom recirculation;\" href=\"https://githubuniverse.com/?utm_source=Blog\u0026amp;utm_medium=GitHub\u0026amp;utm_campaign=module\" target=\"_blank\" aria-label=\"Register now\"\u003e\n\t\t\t\t\tRegister now\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\t\u003c/div\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2025-07-23T16:00:00Z",
  "modifiedTime": null
}
