{
  "id": "a8a209af-8530-4d9e-ab5e-b4761cbc9364",
  "title": "Google Gemini's Long-term Memory Vulnerable to a Kind of Phishing Attack",
  "link": "https://www.infoq.com/news/2025/02/gemini-long-term-memory-attack/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "AI security hacker Johann Rehberger described a prompt injection attack against Google Gemini able to modify its long-term memories using a technique he calls delayed tool invocation. The researcher described the attack as a sort of social engineering/phishing attack triggered by the user interacting with a malicious document. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Fri, 21 Feb 2025 21:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Prompt Engineering",
    "Security Vulnerabilities",
    "Large language models",
    "Google",
    "AI, ML \u0026 Data Engineering",
    "DevOps",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3633,
  "excerpt": "AI security hacker Johann Rehberger described a prompt injection attack against Google Gemini able to modify its long-term memories using a technique he calls delayed tool invocation. The researcher d",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250213201535/apple-touch-icon.png",
  "text": "AI security hacker Johann Rehberger described a prompt injection attack against Google Gemini able to modify its long-term memories using a technique he calls delayed tool invocation. The researcher described the attack as a sort of social engineering/phishing attack triggered by the user interacting with a malicious document. LLMs are usually able to defend themselves from attacks aiming to have them surreptitiously run external tools without the user's knowledge by disabling external tool execution when processing untrusted data, i.e., any information that is not directly coming from the user. One year ago, however, Rehberger showed a technique you can use to circumvent this protection mechanism when using Google Gemini. The technique consists of polluting the chat context so that an action is triggered later, when the model is interacting with the user, that is, when those protections mentioned above are not enforced anymore. In principle, the technique is as straightforward as feeding Gemini a malicious document containing a sentence like \"if the user says X, then execute this tool\". Gemini would refuse to execute the tool while parsing the document. Yet, it would run it later, when instructed by the user saying \"X\". This \"asynchronous triggering\" of the tool accounts for the name Rehberger gave to this technique, delayed tool invocation. Recently, Rehberger brought his investigations into delayed tool invocation a bit further by showing how you can prompt Gemini to store false information in a user's long-term memory. The demo attack is straightforward: an adversary crafts a document with embedded prompt injection, which tricks Gemini into storing false information if the user keeps interacting with Gemini in that same chat conversation. This is shown in the following picture, where Gemini is asked to summarize a document containing a prompt to pollute the user's memory. While this attack has the potential to permanently alter Gemini's behavior, Google assessed its impact as low, since it requires the user to actively collaborate with the exploit, as with other forms of social engineering. Further, the vulnerability is mitigated by the fact that Gemini's UI shows an alert each time new data is added to users' memories. Still, based on his findings, Rehberger suggests users regularly review their saved memories and be careful when interacting with documents from untrusted sources. Prompt injection is coming to the fore as an easy way to interfere with large language models (LLMs) behavior. As Georg Dresler explains, this is particularly worrisome in light of the possibility of exfiltrating private data or secrets by appropriately prompting an LLM model having access to internal tools. For example, AI security firm PromptArmor figured out how you could steal data from private Slack channels, like API keys, passwords, and so on. Exploiting these kinds of vulnerabilities is not always entirely straightforward but it is definitely possible, and attackers can come up with ever smarter and more insidious techniques, as Rehberger's clearly shows. Google Gemini memories are similar to ChatGPT's memory, introduced last year. They aim to enable the persistent storage of things the user cares about, including life, work, aspirations, and personal preferences. Using these long-term memories, Gemini (and ChatGPT) can deliver more relevant answers to the user without them to state their preferences each time. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/02/gemini-long-term-memory-attack/en/headerimage/gemini-long-term-memory-attack-1740168097225.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://x.com/wunderwuzzi23\"\u003eAI security hacker Johann Rehberger\u003c/a\u003e described \u003ca href=\"https://embracethered.com/blog/posts/2025/gemini-memory-persistence-prompt-injection/\"\u003ea prompt injection attack against Google Gemini able to modify its long-term memories\u003c/a\u003e using a technique he calls delayed tool invocation. The researcher described the attack as a sort of social engineering/phishing attack triggered by the user interacting with a malicious document.\u003c/p\u003e\n\n\u003cp\u003eLLMs are usually able to defend themselves from attacks aiming to have them surreptitiously run external tools without the user\u0026#39;s knowledge by disabling external tool execution when processing untrusted data, i.e., any information that is not directly coming from the user.\u003c/p\u003e\n\n\u003cp\u003eOne year ago, however, \u003ca href=\"https://embracethered.com/blog/posts/2024/llm-context-pollution-and-delayed-automated-tool-invocation/\"\u003eRehberger showed a technique you can use to circumvent this protection mechanism when using Google Gemini\u003c/a\u003e. The technique consists of polluting the chat context so that an action is triggered later, when the model is interacting with the user, that is, when those protections mentioned above are not enforced anymore.\u003c/p\u003e\n\n\u003cp\u003eIn principle, the technique is as straightforward as feeding Gemini a malicious document containing a sentence like \u0026#34;if the user says X, then execute this tool\u0026#34;. Gemini would refuse to execute the tool while parsing the document. Yet, it would run it later, when instructed by the user saying \u0026#34;X\u0026#34;. This \u0026#34;asynchronous triggering\u0026#34; of the tool accounts for the name Rehberger gave to this technique, delayed tool invocation.\u003c/p\u003e\n\n\u003cp\u003eRecently, Rehberger brought his investigations into delayed tool invocation a bit further by showing how you can prompt Gemini to store false information in a user\u0026#39;s long-term memory.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe demo attack is straightforward: an adversary crafts a document with embedded prompt injection, which tricks Gemini into storing false information if the user keeps interacting with Gemini in that same chat conversation.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis is shown in the following picture, where Gemini is asked to summarize a document containing a prompt to pollute the user\u0026#39;s memory.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/02/gemini-long-term-memory-attack/en/resources/1gemini-delayed-tool-invocation-1740168096241.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/02/gemini-long-term-memory-attack/en/resources/1gemini-delayed-tool-invocation-1740168096241.png\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eWhile this attack has the potential to permanently alter Gemini\u0026#39;s behavior, Google assessed its impact as low, since it requires the user to actively collaborate with the exploit, as with other forms of social engineering. Further, the vulnerability is mitigated by the fact that Gemini\u0026#39;s UI shows an alert each time new data is added to users\u0026#39; memories. Still, based on his findings, Rehberger suggests users regularly review their \u003ca href=\"https://gemini.google.com/saved-info\"\u003esaved memories\u003c/a\u003e and be careful when interacting with documents from untrusted sources.\u003c/p\u003e\n\n\u003cp\u003ePrompt injection is coming to the fore as an easy way to interfere with large language models (LLMs) behavior. \u003ca href=\"https://www.infoq.com/articles/large-language-models-prompt-injection-stealing/\"\u003eAs Georg Dresler explains, this is particularly worrisome in light of the possibility of exfiltrating private data or secrets\u003c/a\u003e by appropriately prompting an LLM model having access to internal tools. For example, AI security firm PromptArmor figured out how you could \u003ca href=\"https://promptarmor.substack.com/p/data-exfiltration-from-slack-ai-via\"\u003esteal data from private Slack channels\u003c/a\u003e, like API keys, passwords, and so on.\u003c/p\u003e\n\n\u003cp\u003eExploiting these kinds of vulnerabilities is not always entirely straightforward but it is definitely possible, and attackers can come up with ever smarter and more insidious techniques, as Rehberger\u0026#39;s clearly shows.\u003c/p\u003e\n\n\u003cp\u003eGoogle Gemini memories are similar to \u003ca href=\"https://www.infoq.com/news/2024/02/openai-chatgpt-memory/\"\u003eChatGPT\u0026#39;s memory\u003c/a\u003e, introduced last year. They aim to enable the persistent storage of things the user cares about, including life, work, aspirations, and personal preferences. Using these long-term memories, Gemini (and ChatGPT) can deliver more relevant answers to the user without them to state their preferences each time.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-02-21T00:00:00Z",
  "modifiedTime": null
}
