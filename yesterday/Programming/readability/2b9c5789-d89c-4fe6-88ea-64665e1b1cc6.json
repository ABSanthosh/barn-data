{
  "id": "2b9c5789-d89c-4fe6-88ea-64665e1b1cc6",
  "title": "AI Coding Tools Underperform in Field Study with Experienced Developers",
  "link": "https://www.infoq.com/news/2025/07/ai-productivity/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Recent research reveals a surprising 19% increase in task completion time among developers using AI tools like Claude 3.5. Conducted by METR, this study highlights a \"perception gap\"—while developers felt faster, real-world performance lagged due to frictions with AI integration. These findings stress the need for rigorous evaluation of AI's impact in software development. By Matt Foster",
  "author": "Matt Foster",
  "published": "Sun, 20 Jul 2025 19:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Software Development",
    "Productivity",
    "Developer Experience",
    "Development",
    "Architecture \u0026 Design",
    "news"
  ],
  "byline": "Matt Foster",
  "length": 3845,
  "excerpt": "Recent research reveals a surprising 19% increase in task completion time among developers using AI tools like Claude 3.5. Conducted by METR, this study highlights a \u0026quot;perception gap\u0026quot;—while developers",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250717101838/apple-touch-icon.png",
  "text": "A recent study challenges the widespread belief that AI tools accelerate software development. Researchers at METR conducted a randomized controlled trial of experienced open-source developers using AI-enhanced development tools like Claude 3.5 and Cursor Pro. Contrary to expectations, they found that AI-assisted programming led to a 19% increase in task completion time—even as developers believed they were working faster. The findings reveal a potential gap between AI’s perceived promise and its real-world impact. To evaluate AI’s influence under realistic conditions, the researchers designed a randomized controlled trial (RCT) rooted in production-grade environments. Rather than using synthetic benchmarks, they recruited experienced contributors to complete real tasks across mature open-source repositories. Participants were 16 professional developers with an average of five years of experience on the projects they were assigned. The repositories included realistic, 'in-anger' issues drawn from their own codebases: very large (\u003e 1.1m lines of code), well established open source projects.  Across 246 tasks, each developer was randomly assigned to a maximum of two-hour sessions either with or without access to AI assistance. Those with access used Cursor Pro, a code editor with integrated support for Claude 3.5/3.7 Sonnet. The control group was explicitly blocked from using AI tools. The study collected both objective and subjective metrics, including task duration, code quality, and developer perception. Before and after each task, developers and external experts predicted the likely effect of AI on productivity. The central result was both striking and unexpected: AI-assisted developers took 19% longer to complete tasks than those without AI. This contradicted pre-task expectations from both participants and experts, who had predicted an average speedup of ~40%. The authors attributed the slowdown to a variety of contributing factors, including time spent prompting, reviewing AI-generated suggestions, and integrating outputs with complex codebases. Through 140+ hours of screen recordings, they identified five key contributors to the slowdown. These frictions likely offset any up-front gains from code generation, revealing a significant disconnect between perceived and actual productivity. The researchers highlight this phenomenon as a 'perception gap'—where friction introduced by AI tooling is subtle enough to go unnoticed in the moment but cumulatively slows real-world output. The contrast between perception and outcome underscores the study’s importance of grounding AI tool evaluation not just in user sentiment, but in rigorous measurement. The authors caution against overgeneralizing their findings. While the study shows a measurable slowdown with AI tooling in this particular setting, they stress that many of the contributing factors are specific to their design. The developers were working in large, mature open-source codebases—projects with strict review standards and unfamiliar internal logic. The tasks were constrained to two-hour blocks, limiting exploration, and all AI interactions were funneled through a single toolchain Importantly, the authors emphasize that future systems may overcome the challenges observed here. Improvements in prompting techniques, agent scaffolding, or domain-specific fine tuning could unlock real productivity gains even in the settings tested.  As AI capabilities continue to progress rapidly, the authors frame their findings not as a verdict on the usefulness of AI tools—but as a data point in a fast-evolving landscape that still requires rigorous, real-world evaluation.   About the Author Matt Foster",
  "image": "https://res.infoq.com/news/2025/07/ai-productivity/en/headerimage/generatedHeaderImage-1752897810891.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eA \u003ca href=\"https://arxiv.org/pdf/2507.09089\"\u003erecent study\u003c/a\u003e challenges the widespread belief that AI tools accelerate software development. Researchers at \u003ca href=\"https://metr.org/\"\u003eMETR\u003c/a\u003e conducted a randomized controlled trial of experienced open-source developers using AI-enhanced development tools like Claude 3.5 and Cursor Pro. Contrary to expectations, they found that AI-assisted programming led to a 19% increase in task completion time—even as developers believed they were working faster. The findings reveal a potential gap between AI’s perceived promise and its real-world impact.\u003c/p\u003e\n\n\u003cp\u003eTo evaluate AI’s influence under realistic conditions, the researchers designed a randomized controlled trial (RCT) rooted in production-grade environments. Rather than using synthetic benchmarks, they recruited experienced contributors to complete real tasks across mature open-source repositories.\u003c/p\u003e\n\n\u003cp\u003eParticipants were 16 professional developers with an average of five years of experience on the projects they were assigned. The repositories included realistic, \u0026#39;in-anger\u0026#39; issues drawn from their own codebases: very large (\u0026gt; 1.1m lines of code), well established open source projects. \u003c/p\u003e\n\n\u003cp\u003eAcross 246 tasks, each developer was randomly assigned to a maximum of two-hour sessions either with or without access to AI assistance. Those with access used Cursor Pro, a code editor with integrated support for Claude 3.5/3.7 Sonnet. The control group was explicitly blocked from using AI tools.\u003c/p\u003e\n\n\u003cp\u003eThe study collected both objective and subjective metrics, including task duration, code quality, and developer perception. Before and after each task, developers and external experts predicted the likely effect of AI on productivity.\u003c/p\u003e\n\n\u003cp\u003eThe central result was both striking and unexpected: AI-assisted developers took 19% longer to complete tasks than those without AI. This contradicted pre-task expectations from both participants and experts, who had predicted an average speedup of ~40%.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg height=\"299\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfOc5KTvjZ_7AerRPXwQ5vDzZ4mUJ1qsU-xC3ybkVg2AuGRy1V6rm-wbtdmF5xBfCwLMynie7wTyz6iCK4oxFGp_R06nlmuEz_g2vuMItzSi8FEncQgR5aw9aRZ4g_aQ5cZpcuukw?key=yjGJeXBh6EB7MPGrXnR8jA\" width=\"602\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThe authors attributed the slowdown to a variety of contributing factors, including time spent prompting, reviewing AI-generated suggestions, and integrating outputs with complex codebases. Through 140+ hours of screen recordings, they identified five key contributors to the slowdown. These frictions likely offset any up-front gains from code generation, revealing a significant disconnect between perceived and actual productivity.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg height=\"328\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXc-b0EAhoslFxO4i8RxhAWxOk4JA2HJxR4KyYi03w1bPxAiwGzXx1mRrC9uf0QWTtiRLJdaXGDcNLIaLdVO4ehs-a3WnyiMicSxrJuMY0heVxDhwgdrmCYTZOxiCW4DqVzhow7qZQ?key=yjGJeXBh6EB7MPGrXnR8jA\" width=\"602\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThe researchers highlight this phenomenon as a \u0026#39;perception gap\u0026#39;—where friction introduced by AI tooling is subtle enough to go unnoticed in the moment but cumulatively slows real-world output. The contrast between perception and outcome underscores the study’s importance of grounding AI tool evaluation not just in user sentiment, but in rigorous measurement.\u003c/p\u003e\n\n\u003cp\u003eThe authors caution against overgeneralizing their findings. While the study shows a measurable slowdown with AI tooling in this particular setting, they stress that many of the contributing factors are specific to their design. The developers were working in large, mature open-source codebases—projects with strict review standards and unfamiliar internal logic. The tasks were constrained to two-hour blocks, limiting exploration, and all AI interactions were funneled through a single toolchain\u003c/p\u003e\n\n\u003cp\u003eImportantly, the authors emphasize that future systems may overcome the challenges observed here. Improvements in prompting techniques, agent scaffolding, or domain-specific fine tuning could unlock real productivity gains even in the settings tested. \u003c/p\u003e\n\n\u003cp\u003eAs AI capabilities continue to progress rapidly, the authors frame their findings not as a verdict on the usefulness of AI tools—but as a data point in a fast-evolving landscape that still requires rigorous, real-world evaluation.\u003cbr/\u003e\n \u003c/p\u003e\n\n\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Matt-Foster\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eMatt Foster\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-07-20T00:00:00Z",
  "modifiedTime": null
}
