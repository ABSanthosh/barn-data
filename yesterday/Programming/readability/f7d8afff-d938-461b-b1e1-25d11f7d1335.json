{
  "id": "f7d8afff-d938-461b-b1e1-25d11f7d1335",
  "title": "Article: Building a Global Caching System at Netflix: A Deep Dive to Global Replication",
  "link": "https://www.infoq.com/articles/netflix-global-cache/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Netflix's EVCache system handles 400M ops/second across 22,000 servers, managing 14.3 PB of data. This infrastructure ensures global availability and resilience through intelligent data routing and flexible replication strategies. By implementing batch compression and switching to DNS-based discovery, Netflix optimizes efficiency, reduces bandwidth usage and significantly lowers operational costs. By Sriram Rangarajan, Prudhviraj Karumanchi",
  "author": "Sriram Rangarajan, Prudhviraj Karumanchi",
  "published": "Fri, 11 Oct 2024 11:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Cloud Architecture",
    "Netflix",
    "AWS",
    "Memcached",
    "Caching",
    "Distributed Cache",
    "Architecture \u0026 Design",
    "article"
  ],
  "byline": "Sriram Rangarajan, Prudhviraj Karumanchi",
  "length": 18314,
  "excerpt": "Netflix's EVCache system powers 400M ops/second with 14.3 PB of data, optimizing global availability, scalability, and efficiency while reducing costs through intelligent data routing and compression.",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241001113644/apple-touch-icon.png",
  "text": "Key Takeaways Netflix’s global replication strategy ensures data availability across four regions, minimizing latency and enhancing system reliability during regional outages or failovers. EVCache, a distributed key-value store backed by SSDs, is central to Netflix's caching strategy, offering linear scalability and robust resilience to manage massive data volumes. Netflix's EVCache infrastructure includes 200 Memcached clusters and 22,000 server instances, handling 30 million replication events globally and 400 million operations per second. It manages around 2 trillion items, totaling 14.3 petabytes, showcasing its immense capacity and scalability. The topology-aware EVCache client utilizes client-initiated replication to optimize data routing, reduce server load, and allow flexible replication strategies. Implementing batch compression and switching to Eureka DNS for service discovery has significantly reduced network bandwidth usage and transfer costs, enhancing overall efficiency. Introduction In today’s hyper-connected world, delivering a seamless and responsive user experience is crucial, especially for global entertainment services like Netflix that aim to spread joy to millions worldwide. One key challenge in achieving this is ensuring data availability across multiple regions. This is where distributed caching comes into play. Distributed caching is a method of storing data across multiple servers to ensure quick access and high availability. It helps reduce the load on backend databases and speeds up data retrieval, providing a smoother user experience. In this article, we delve into how Netflix employs EVCache, a distributed caching solution, to master the complexities of global replication. We will explore its architecture, design principles, and innovative strategies that enable Netflix to operate at an immense scale while maintaining stringent performance standards. EVCache: The Backbone of Netflix's Caching Solution Related Sponsored Content Designing Data Intensive Applications (By O'Reilly) EVCache stands for Ephemeral Volatile Cache. It is a distributed key-value store based on Memcached, designed to deliver linear scalability and robust resilience. Despite its name, EVCache benefits from SSD backing, ensuring reliable and persistent storage. Although Memcached doesn't use SSDs for persistence by default, its extstore extension allows offloading less frequently accessed data to SSDs, freeing up RAM for frequently accessed data. Netflix leverages this in EVCache, combining the speed of RAM with the capacity of SSDs to optimize performance and storage efficiency. Netflix's EVCache is deployed across four regions, comprising 200 Memcached clusters tailored to support various use cases. Each cluster is responsible for serving an app or a group of apps. This extensive infrastructure includes 22,000 server instances, enabling the system to handle 30 million replication events globally and 400 million overall operations per second. In terms of data, EVCache manages around 2 trillion items, totaling 14.3 petabytes, showcasing its immense capacity and scalability. Key Features of EVCache: Global Replication: EVCache integrates seamless global replication into the client, allowing efficient cross-region data access. Topology-Aware Client: The EVCache client is aware of the physical and logical locations of the servers, optimizing data retrieval and storage. Resilience: EVCache is designed to withstand failures at various levels, including instance, availability zone, and regional levels. Seamless Deployments: EVCache supports seamless deployments, enabling updates and changes without downtime or service interruption. Why Replicate Cache Data? Rapid data availability is crucial for maintaining a seamless and responsive user experience. By keeping cache data readily accessible, the system avoids time-consuming and resource-intensive database queries. This immediate access keeps users engaged and satisfied. In the event of a region failover, having cache data available in the failover region ensures uninterrupted service with minimal latency, preventing any noticeable disruption for users. This level of performance and reliability is essential for meeting user expectations and maintaining high engagement. Building personalized recommendations requires significant computational resources, particularly for Netflix, where machine learning algorithms are key. Recomputing data for cache misses can be extremely CPU-intensive and costly. Each cache miss demands substantial computational power to retrieve and recompute data, including running complex machine learning models. By replicating cache data across regions, Netflix minimizes these expensive recomputation processes, reducing operational costs and ensuring a smoother user experience. This allows quick data access and efficient delivery of personalized recommendations, making the system more cost-effective and responsive. Design of the Global Replication Service Data replication between two regions The above diagram illustrates how EVCache replicates data across regions. This process is comprised of the following steps: Send Mutation: The application uses the EVCache client to send various mutation calls, such as set, add, delete, and touch, to the local EVCache servers. Send Metadata: EVCache sends an asynchronous event containing metadata to Kafka. This metadata includes critical information such as the key, TTL (time-to-live), and creation timestamp. However, it notably excludes the value to prevent overloading Kafka with potentially large data payloads. Poll Messages: The replication reader service continuously reads messages from Kafka. This service is responsible for processing and preparing the metadata events for the next stage. Local Read: Upon reading the metadata from Kafka, the replication reader service issues a read call to the local region's EVCache server to fetch the latest data for the key. This step ensures that the most up-to-date value is retrieved without placing undue load on Kafka. Additionally, the reader can filter out unwanted messages, such as those with very short TTLs (e.g., 5 seconds), to optimize the replication process based on business needs. Cross-Region Traffic: The replication reader service then makes a cross-region call to the replication writer service in the destination region. This call includes all relevant information, such as metadata and the fetched value, ensuring the data is replicated accurately. Destination Write: The replication writer service receives the cross-region call and writes the key-value pair to the EVCache server in the destination region. This step ensures that the data is consistently replicated across regions. Read Data: When data is read in the failover region, it is readily available due to the replication process. This ensures minimal latency and provides a seamless user experience. Error Handling in the Replication System Error handling in Netflix's replication system In a distributed system, failures can occur at any step of the replication process. To ensure data reliability and maintain the integrity of the replication process, Netflix employs Amazon Simple Queue Service (SQS) for robust error handling due to its reliable message queuing capabilities. When a failure occurs during any step of the replication process, the failed mutation is captured and sent to an SQS queue, ensuring that no failed mutation is lost and can be retried later. The replication service monitors the SQS queue for failed mutations and reprocesses them through the replication workflow upon detection. This retry mechanism ensures that all mutations are eventually processed, maintaining data reliability across regions and minimizing the risk of data loss. Closer Look into the Replication Reader and Writer Service Multiple reader service instances replicate data to different regions The diagram above illustrates Netflix's replication service. The Reader Service in US-EAST-1 pulls data from Kafka topics and partitions, processes it, and sends it to the Writer Service in other regions, which writes the data to EVCache servers, ensuring regional availability. Replication Reader Service The reader service pulls messages from Kafka, applies necessary transformations, and fetches the most recent values from the local EVCache service. To streamline management, Netflix uses a single Kafka cluster for the replication service, which supports over 200 EVCache clusters. Each EVCache cluster is represented as a topic in Kafka, with topics partitioned based on the volume of events they handle. Different consumer groups are designated to read from this Kafka cluster within the reader service. Each consumer group corresponds to a set of nodes, with each node responsible for reading multiple partitions. This structure allows for parallel processing and effective load distribution, ensuring high throughput and efficient data handling. Each consumer group targets a different region, ensuring data is replicated across multiple regions. The reader service is hosted on EC2 instances, providing scalability and resilience. Each reader service is a compute cluster, a group of interconnected computers (nodes) that work together to perform complex computations and data processing tasks. By organizing readers as consumer groups, Netflix can scale the system horizontally to meet demand. After the reader service fetches and transforms the necessary data, it initiates a cross-region call to the writer service in the target region. The transformation involves converting the fetched data, which includes the key, metadata, and value, into a JSON format suitable for transmission via REST. This call includes all pertinent information, ensuring the data is replicated accurately. Replication Writer Service The writer service's primary function is to receive REST calls from the reader service and write the data to the destination EVCache service. The writer service is also hosted on EC2 instances, providing scalability and resilience. Upon receiving the data, the writer service processes it and writes the key-value pair to the EVCache server in the destination region. This ensures data availability across regions. The writer service is designed to handle large volumes of data efficiently, maintaining the integrity of the replication process and ensuring that the system remains robust and reliable. Why Did We Choose Client-Initiated Replication Over Server-Initiated Replication? For EVCache, we opted for client-initiated replication primarily because the EVCache client is topology-aware. This topology awareness allows the client to efficiently manage and route data within the distributed caching environment. Specifically, the EVCache client is aware of: Node Locations: The client knows the physical or logical locations of the memcached nodes. This includes information about which nodes are in which data centers or regions. Node Availability: The client is aware of which memcached nodes are currently available and operational. This helps in avoiding nodes that are down or experiencing issues. Data Distribution: The client understands how data is distributed across the memcached nodes. This includes knowledge of which nodes hold replicas of specific data items. Network Latency: The client can make decisions based on network latency, choosing memcached nodes that provide the fastest response times for read and write operations. Advantages of Client-Initiated Replication Topology Awareness: As explained, the EVCache client's topology awareness enables it to make intelligent decisions about data routing and replication. This ensures efficient data distribution and minimizes latency. Reduced Server Load: By initiating replication at the client level, we reduce the computational burden on the servers. This allows the servers to focus on their core responsibilities, such as serving read and write requests, without the added overhead of managing replication tasks. Scalability: Client-initiated replication allows for more straightforward horizontal scaling. As the number of clients increases, the replication workload is distributed across these clients, preventing any single point of bottleneck and ensuring that the system can handle increased load. Flexibility: With client-initiated replication, it is easier to implement and manage various replication strategies and optimizations at the client level. This includes filtering out unwanted messages, applying business-specific rules, and dynamically adjusting replication behavior based on current conditions. Disadvantages of Client-Initiated Replication Complexity in Client Management: Managing replication logic at the client level can introduce complexity. Ensuring that all clients are up-to-date with the latest replication logic and handling potential inconsistencies across clients can be challenging. Increased Network Traffic: Client-initiated replication can increase network traffic, as each client is responsible for sending replication data across regions. This can result in higher bandwidth usage and potential network congestion compared to a centralized server-initiated approach, where replication traffic can be more efficiently managed and optimized at the server level. Message Duplication: With each client responsible for initiating replication, there is a possibility of duplicated efforts, especially if multiple clients are handling similar data. This can lead to inefficiencies and increased resource consumption. Error Handling: Implementing robust error handling and retry mechanisms at the client level can be more complex than a centralized server-initiated approach. Ensuring data reliability in the face of network failures or client crashes adds another layer of complexity. Efficiency Improvements Netflix continuously seeks to optimize its infrastructure for both performance and cost efficiency. Two significant improvements in the EVCache replication process have been implementing batch compression and removing network load balancers. Batch Compression To reduce network bandwidth usage, we implemented batch compression for the data transferred from the reader to the writer. This process involves batching multiple messages and applying Zstandard compression to the batch. By compressing the data before transmission, we achieved a 35% reduction in network bandwidth usage. This significant optimization not only lowers costs but also enhances the overall efficiency of the replication process. Batch compression ensures that the data transfer between reader and writer clusters is more efficient, reducing overhead and improving throughput. Comparison of Compressed vs Uncompressed Payload Sizes This graph illustrates the overall payload size reduction across various use cases and clusters. Due to the diverse traffic patterns of each cluster, we observed a cumulative savings of approximately 35% in network throughput. Removing Network Load Balancers Initially, we used Network Load Balancers (NLBs) to manage communication between the reader and writer services. However, this setup incurred additional network transfer costs. To address this, we switched from using NLBs to leveraging Eureka DNS for service discovery. Using Eureka DNS, we can fetch the IP addresses of the writer nodes and handle the routing ourselves. This change reduced network transfer costs by 50% while maintaining predictable latencies and efficient load distribution. Communication topology comparison with and without an NLB The switch to client-side load balancing streamlined our infrastructure, significantly cutting costs without compromising performance. By removing the dependency on NLBs, we achieved greater control over the traffic routing process, leading to more efficient resource utilization. Process bytes in the NLB before and after migration The above graph illustrates the amount of traffic routed via NLBs. Since we migrated off of NLBs and started leveraging client-side load balancing, we could significantly save on network transfer costs. The daily NLB usage hovered around 45 GB/s across all the regions, and after the change, the usage decreased to less than 100 MB/s. We still keep NLBs around in our architecture as a fallback option. Conclusion Building a robust, scalable, and efficient global caching system is critical for Netflix to provide a seamless user experience. By leveraging EVCache and a well-designed replication service, Netflix ensures high availability, low latency, and cost efficiency. The topology-aware EVCache client, combined with client-initiated replication, allows for efficient data management and routing, reducing server load and enhancing scalability. Through careful design choices, such as using a single Kafka cluster for streamlined management and implementing a flexible replication reader service, Netflix has optimized its caching infrastructure to handle massive volumes of data across multiple regions. This ensures that data is always readily available, even in failover scenarios, maintaining a seamless and responsive user experience. As we continue on our journey of continuous improvement and innovation, we look forward to further enhancing our systems to meet the evolving needs of our global user base. By consistently refining our approaches and embracing new technologies, Netflix aims to provide an even more resilient, responsive, and cost-effective experience for users worldwide. About the Authors Sriram Rangarajan Prudhviraj Karumanchi",
  "image": "https://res.infoq.com/articles/netflix-global-cache/en/headerimage/building-global-caching-system-netflix-header-1728478672706.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\t\u003ch3\u003eKey Takeaways\u003c/h3\u003e\n\t\t\t\t\t\t\t\t\t\u003cul\u003e\n\t\u003cli\u003eNetflix’s global replication strategy ensures data availability across four regions, minimizing latency and enhancing system reliability during regional outages or failovers.\u003c/li\u003e\n\t\u003cli\u003eEVCache, a distributed key-value store backed by SSDs, is central to Netflix\u0026#39;s caching strategy, offering linear scalability and robust resilience to manage massive data volumes.\u003c/li\u003e\n\t\u003cli\u003eNetflix\u0026#39;s EVCache infrastructure includes 200 Memcached clusters and 22,000 server instances, handling 30 million replication events globally and 400 million operations per second. It manages around 2 trillion items, totaling 14.3 petabytes, showcasing its immense capacity and scalability.\u003c/li\u003e\n\t\u003cli\u003eThe topology-aware EVCache client utilizes client-initiated replication to optimize data routing, reduce server load, and allow flexible replication strategies.\u003c/li\u003e\n\t\u003cli\u003eImplementing batch compression and switching to Eureka DNS for service discovery has significantly reduced network bandwidth usage and transfer costs, enhancing overall efficiency.\u003c/li\u003e\n\u003c/ul\u003e\n\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\u003c/div\u003e\n\t\t\t\t\t\t\t\n                                                        \n                                                        \n\n\n\n\n\n\n\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn today’s hyper-connected world, delivering a seamless and responsive user experience is crucial, especially for global entertainment services like \u003ca href=\"https://www.netflix.com/\"\u003eNetflix\u003c/a\u003e that aim to spread joy to millions worldwide. One key challenge in achieving this is ensuring data availability across multiple regions. This is where distributed caching comes into play.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Distributed_cache\"\u003eDistributed caching\u003c/a\u003e is a method of storing data across multiple servers to ensure quick access and high availability. It helps reduce the load on backend databases and speeds up data retrieval, providing a smoother user experience.\u003c/p\u003e\n\n\u003cp\u003eIn this article, we delve into how Netflix employs EVCache, a distributed caching solution, to master the complexities of global replication. We will explore its architecture, design principles, and innovative strategies that enable Netflix to operate at an immense scale while maintaining stringent performance standards.\u003c/p\u003e\n\n\u003ch2\u003eEVCache: The Backbone of Netflix\u0026#39;s Caching Solution\u003c/h2\u003e\n\n\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\u003cdiv data-trk-view=\"true\" data-trk-impr=\"true\" data-place=\"EMBEDDED\"\u003e\n\t\n\t\u003cul\u003e\n\t\t\u003ch4\u003eRelated Sponsored Content\u003c/h4\u003e\n\t\t\n\t\t\t\n\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003cspan\u003e\u003c/span\u003e\n\t\t\t\t\t\u003ch5\u003e\n\t\t\t\t\t\t\u003ca href=\"https://www.infoq.com/url/f/913f7b06-51dc-42e1-843a-2c45175417ba/\" rel=\"nofollow\"\u003e\n\t\t\t\t\t\t\tDesigning Data Intensive Applications (By O\u0026#39;Reilly)\n\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\t\u003c/h5\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\n\t\t\n\t\t\n\t\t\n\t\u003c/ul\u003e\n\t\n\t\t\n\t\n\t\n\u003c/div\u003e\n\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\u003cp\u003eEVCache stands for Ephemeral Volatile Cache. It is a distributed key-value store based on \u003ca href=\"https://memcached.org/\"\u003eMemcached\u003c/a\u003e, designed to deliver linear scalability and robust resilience. Despite its name, EVCache benefits from SSD backing, ensuring reliable and persistent storage.\u003c/p\u003e\n\n\u003cp\u003eAlthough Memcached doesn\u0026#39;t use SSDs for persistence by default, its \u003ca href=\"https://github.com/memcached/memcached/wiki/Extstore\"\u003eextstore\u003c/a\u003e extension allows offloading less frequently accessed data to SSDs, freeing up RAM for frequently accessed data. Netflix leverages this in EVCache, combining the speed of RAM with the capacity of SSDs to optimize performance and storage efficiency.\u003c/p\u003e\n\n\u003cp\u003eNetflix\u0026#39;s EVCache is deployed across four regions, comprising 200 Memcached clusters tailored to support various use cases. Each cluster is responsible for serving an app or a group of apps. This extensive infrastructure includes 22,000 server instances, enabling the system to handle 30 million replication events globally and 400 million overall operations per second. In terms of data, EVCache manages around 2 trillion items, totaling 14.3 petabytes, showcasing its immense capacity and scalability.\u003c/p\u003e\n\n\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003cstrong\u003eKey Features of EVCache:\u003c/strong\u003e\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\u003cstrong\u003eGlobal Replication\u003c/strong\u003e: EVCache integrates seamless global replication into the client, allowing efficient cross-region data access.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eTopology-Aware Client\u003c/strong\u003e: The EVCache client is aware of the physical and logical locations of the servers, optimizing data retrieval and storage.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eResilience\u003c/strong\u003e: EVCache is designed to withstand failures at various levels, including instance, availability zone, and regional levels.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eSeamless Deployments\u003c/strong\u003e: EVCache supports seamless deployments, enabling updates and changes without downtime or service interruption.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy Replicate Cache Data?\u003c/h2\u003e\n\n\u003cp\u003eRapid data availability is crucial for maintaining a seamless and responsive user experience. By keeping cache data readily accessible, the system avoids time-consuming and resource-intensive database queries. This immediate access keeps users engaged and satisfied. In the event of a region failover, having cache data available in the failover region ensures uninterrupted service with minimal latency, preventing any noticeable disruption for users. This level of performance and reliability is essential for meeting user expectations and maintaining high engagement.\u003c/p\u003e\n\n\u003cp\u003eBuilding personalized recommendations requires significant computational resources, particularly for Netflix, where machine learning algorithms are key. Recomputing data for cache misses can be extremely CPU-intensive and costly. Each cache miss demands substantial computational power to retrieve and recompute data, including running complex machine learning models. By replicating cache data across regions, Netflix minimizes these expensive recomputation processes, reducing operational costs and ensuring a smoother user experience. This allows quick data access and efficient delivery of personalized recommendations, making the system more cost-effective and responsive.\u003c/p\u003e\n\n\u003ch2\u003eDesign of the Global Replication Service\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/netflix-global-cache/en/resources/19pic1-1728477568759.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/netflix-global-cache/en/resources/19pic1-1728477568759.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eData replication between two regions\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003cp\u003eThe above diagram illustrates how EVCache replicates data across regions. This process is comprised of the following steps:\u003c/p\u003e\n\n\u003col\u003e\n\t\u003cli\u003e\u003cstrong\u003eSend Mutation\u003c/strong\u003e: The application uses the EVCache client to send various mutation calls, such as set, add, delete, and touch, to the local EVCache servers.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eSend Metadata\u003c/strong\u003e: EVCache sends an asynchronous event containing metadata to \u003ca href=\"https://kafka.apache.org/\"\u003eKafka\u003c/a\u003e. This metadata includes critical information such as the key, TTL (time-to-live), and creation timestamp. However, it notably excludes the value to prevent overloading Kafka with potentially large data payloads.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003ePoll Messages\u003c/strong\u003e: The replication reader service continuously reads messages from Kafka. This service is responsible for processing and preparing the metadata events for the next stage.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eLocal Read\u003c/strong\u003e: Upon reading the metadata from Kafka, the replication reader service issues a read call to the local region\u0026#39;s EVCache server to fetch the latest data for the key. This step ensures that the most up-to-date value is retrieved without placing undue load on Kafka. Additionally, the reader can filter out unwanted messages, such as those with very short TTLs (e.g., 5 seconds), to optimize the replication process based on business needs.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eCross-Region Traffic\u003c/strong\u003e: The replication reader service then makes a cross-region call to the replication writer service in the destination region. This call includes all relevant information, such as metadata and the fetched value, ensuring the data is replicated accurately.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eDestination Write\u003c/strong\u003e: The replication writer service receives the cross-region call and writes the key-value pair to the EVCache server in the destination region. This step ensures that the data is consistently replicated across regions.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eRead Data\u003c/strong\u003e: When data is read in the failover region, it is readily available due to the replication process. This ensures minimal latency and provides a seamless user experience.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3\u003eError Handling in the Replication System\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/netflix-global-cache/en/resources/17pic2-1728477568759.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/netflix-global-cache/en/resources/17pic2-1728477568759.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eError handling in Netflix\u0026#39;s replication system\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003cp\u003eIn a distributed system, failures can occur at any step of the replication process. To ensure data reliability and maintain the integrity of the replication process, Netflix employs Amazon \u003ca href=\"https://aws.amazon.com/sqs/\"\u003eSimple Queue Service\u003c/a\u003e (SQS) for robust error handling due to its reliable message queuing capabilities.\u003c/p\u003e\n\n\u003cp\u003eWhen a failure occurs during any step of the replication process, the failed mutation is captured and sent to an SQS queue, ensuring that no failed mutation is lost and can be retried later. The replication service monitors the SQS queue for failed mutations and reprocesses them through the replication workflow upon detection. This retry mechanism ensures that all mutations are eventually processed, maintaining data reliability across regions and minimizing the risk of data loss.\u003c/p\u003e\n\n\u003ch2\u003eCloser Look into the Replication Reader and Writer Service\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/netflix-global-cache/en/resources/17pic3-1728477568759.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/netflix-global-cache/en/resources/17pic3-1728477568759.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eMultiple reader service instances replicate data to different regions\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003cp\u003eThe diagram above illustrates Netflix\u0026#39;s replication service. The Reader Service in US-EAST-1 pulls data from Kafka topics and partitions, processes it, and sends it to the Writer Service in other regions, which writes the data to EVCache servers, ensuring regional availability.\u003c/p\u003e\n\n\u003ch3\u003eReplication Reader Service\u003c/h3\u003e\n\n\u003cp\u003eThe reader service pulls messages from Kafka, applies necessary transformations, and fetches the most recent values from the local EVCache service. To streamline management, Netflix uses a single Kafka cluster for the replication service, which supports over 200 EVCache clusters. Each EVCache cluster is represented as a topic in Kafka, with topics partitioned based on the volume of events they handle.\u003c/p\u003e\n\n\u003cp\u003eDifferent consumer groups are designated to read from this Kafka cluster within the reader service. Each consumer group corresponds to a set of nodes, with each node responsible for reading multiple partitions. This structure allows for parallel processing and effective load distribution, ensuring high throughput and efficient data handling. Each consumer group targets a different region, ensuring data is replicated across multiple regions.\u003c/p\u003e\n\n\u003cp\u003eThe reader service is hosted on \u003ca href=\"https://aws.amazon.com/ec2/\"\u003eEC2\u003c/a\u003e instances, providing scalability and resilience. Each reader service is a compute cluster, a group of interconnected computers (nodes) that work together to perform complex computations and data processing tasks. By organizing readers as consumer groups, Netflix can scale the system horizontally to meet demand.\u003c/p\u003e\n\n\u003cp\u003eAfter the reader service fetches and transforms the necessary data, it initiates a cross-region call to the writer service in the target region. The transformation involves converting the fetched data, which includes the key, metadata, and value, into a JSON format suitable for transmission via REST. This call includes all pertinent information, ensuring the data is replicated accurately.\u003c/p\u003e\n\n\u003ch3\u003eReplication Writer Service\u003c/h3\u003e\n\n\u003cp\u003eThe writer service\u0026#39;s primary function is to receive REST calls from the reader service and write the data to the destination EVCache service. The writer service is also hosted on EC2 instances, providing scalability and resilience. Upon receiving the data, the writer service processes it and writes the key-value pair to the EVCache server in the destination region. This ensures data availability across regions. The writer service is designed to handle large volumes of data efficiently, maintaining the integrity of the replication process and ensuring that the system remains robust and reliable.\u003c/p\u003e\n\n\u003ch2\u003eWhy Did We Choose Client-Initiated Replication Over Server-Initiated Replication?\u003c/h2\u003e\n\n\u003cp\u003eFor EVCache, we opted for client-initiated replication primarily because the EVCache client is topology-aware. This topology awareness allows the client to efficiently manage and route data within the distributed caching environment. Specifically, the EVCache client is aware of:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\u003cstrong\u003eNode Locations\u003c/strong\u003e: The client knows the physical or logical locations of the memcached nodes. This includes information about which nodes are in which data centers or regions.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eNode Availability\u003c/strong\u003e: The client is aware of which memcached nodes are currently available and operational. This helps in avoiding nodes that are down or experiencing issues.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eData Distribution\u003c/strong\u003e: The client understands how data is distributed across the memcached nodes. This includes knowledge of which nodes hold replicas of specific data items.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eNetwork Latency\u003c/strong\u003e: The client can make decisions based on network latency, choosing memcached nodes that provide the fastest response times for read and write operations.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eAdvantages of Client-Initiated Replication\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003eTopology Awareness\u003c/strong\u003e: As explained, the EVCache client\u0026#39;s topology awareness enables it to make intelligent decisions about data routing and replication. This ensures efficient data distribution and minimizes latency.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eReduced Server Load\u003c/strong\u003e: By initiating replication at the client level, we reduce the computational burden on the servers. This allows the servers to focus on their core responsibilities, such as serving read and write requests, without the added overhead of managing replication tasks.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eScalability\u003c/strong\u003e: Client-initiated replication allows for more straightforward horizontal scaling. As the number of clients increases, the replication workload is distributed across these clients, preventing any single point of bottleneck and ensuring that the system can handle increased load.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eFlexibility\u003c/strong\u003e: With client-initiated replication, it is easier to implement and manage various replication strategies and optimizations at the client level. This includes filtering out unwanted messages, applying business-specific rules, and dynamically adjusting replication behavior based on current conditions.\u003c/p\u003e\n\n\u003ch3\u003eDisadvantages of Client-Initiated Replication\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003eComplexity in Client Management\u003c/strong\u003e: Managing replication logic at the client level can introduce complexity. Ensuring that all clients are up-to-date with the latest replication logic and handling potential inconsistencies across clients can be challenging.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eIncreased Network Traffic\u003c/strong\u003e: Client-initiated replication can increase network traffic, as each client is responsible for sending replication data across regions. This can result in higher bandwidth usage and potential network congestion compared to a centralized server-initiated approach, where replication traffic can be more efficiently managed and optimized at the server level.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eMessage Duplication\u003c/strong\u003e: With each client responsible for initiating replication, there is a possibility of duplicated efforts, especially if multiple clients are handling similar data. This can lead to inefficiencies and increased resource consumption.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eError Handling\u003c/strong\u003e: Implementing robust error handling and retry mechanisms at the client level can be more complex than a centralized server-initiated approach. Ensuring data reliability in the face of network failures or client crashes adds another layer of complexity.\u003c/p\u003e\n\n\u003ch2\u003eEfficiency Improvements\u003c/h2\u003e\n\n\u003cp\u003eNetflix continuously seeks to optimize its infrastructure for both performance and cost efficiency. Two significant improvements in the EVCache replication process have been implementing batch compression and removing network load balancers.\u003c/p\u003e\n\n\u003ch3\u003eBatch Compression\u003c/h3\u003e\n\n\u003cp\u003eTo reduce network bandwidth usage, we implemented batch compression for the data transferred from the reader to the writer. This process involves batching multiple messages and applying \u003ca href=\"https://facebook.github.io/zstd/\"\u003eZstandard\u003c/a\u003e compression to the batch. By compressing the data before transmission, we achieved a 35% reduction in network bandwidth usage. This significant optimization not only lowers costs but also enhances the overall efficiency of the replication process. Batch compression ensures that the data transfer between reader and writer clusters is more efficient, reducing overhead and improving throughput.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/netflix-global-cache/en/resources/12pic4-1728477568759.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/netflix-global-cache/en/resources/12pic4-1728477568759.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eComparison of Compressed vs Uncompressed Payload Sizes\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003cp\u003eThis graph illustrates the overall payload size reduction across various use cases and clusters. Due to the diverse traffic patterns of each cluster, we observed a cumulative savings of approximately 35% in network throughput.\u003c/p\u003e\n\n\u003ch3\u003eRemoving Network Load Balancers\u003c/h3\u003e\n\n\u003cp\u003eInitially, we used \u003ca href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\"\u003eNetwork Load Balancers\u003c/a\u003e (NLBs) to manage communication between the reader and writer services. However, this setup incurred additional network transfer costs. To address this, we switched from using NLBs to leveraging \u003ca href=\"https://github.com/bfg/eureka-dns-server\"\u003eEureka DNS\u003c/a\u003e for service discovery. Using Eureka DNS, we can fetch the IP addresses of the writer nodes and handle the routing ourselves. This change reduced network transfer costs by 50% while maintaining predictable latencies and efficient load distribution.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/netflix-global-cache/en/resources/10pic5-1728477568759.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/netflix-global-cache/en/resources/10pic5-1728477568759.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eCommunication topology comparison with and without an NLB\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003cp\u003eThe switch to client-side load balancing streamlined our infrastructure, significantly cutting costs without compromising performance. By removing the dependency on NLBs, we achieved greater control over the traffic routing process, leading to more efficient resource utilization.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"articles/netflix-global-cache/en/resources/10pic6-1728477568759.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/netflix-global-cache/en/resources/10pic6-1728477568759.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003csmall\u003e\u003cstrong\u003eProcess bytes in the NLB before and after migration\u003c/strong\u003e\u003c/small\u003e\u003c/p\u003e\n\n\u003cp\u003eThe above graph illustrates the amount of traffic routed via NLBs. Since we migrated off of NLBs and started leveraging client-side load balancing, we could significantly save on network transfer costs.\u003c/p\u003e\n\n\u003cp\u003eThe daily NLB usage hovered around 45 GB/s across all the regions, and after the change, the usage decreased to less than 100 MB/s. We still keep NLBs around in our architecture as a fallback option.\u003c/p\u003e\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\u003cp\u003eBuilding a robust, scalable, and efficient global caching system is critical for Netflix to provide a seamless user experience. By leveraging EVCache and a well-designed replication service, Netflix ensures high availability, low latency, and cost efficiency. The topology-aware EVCache client, combined with client-initiated replication, allows for efficient data management and routing, reducing server load and enhancing scalability.\u003c/p\u003e\n\n\u003cp\u003eThrough careful design choices, such as using a single Kafka cluster for streamlined management and implementing a flexible replication reader service, Netflix has optimized its caching infrastructure to handle massive volumes of data across multiple regions. This ensures that data is always readily available, even in failover scenarios, maintaining a seamless and responsive user experience.\u003c/p\u003e\n\n\u003cp\u003eAs we continue on our journey of continuous improvement and innovation, we look forward to further enhancing our systems to meet the evolving needs of our global user base. By consistently refining our approaches and embracing new technologies, Netflix aims to provide an even more resilient, responsive, and cost-effective experience for users worldwide.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Authors\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sriram-Rangarajan\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSriram Rangarajan\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Prudhviraj-Karumanchi\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003ePrudhviraj Karumanchi\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\n                            \n                            \n\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "20 min read",
  "publishedTime": "2024-10-11T00:00:00Z",
  "modifiedTime": null
}
