{
  "id": "023a6bca-e452-4c88-8d67-6854c09b51be",
  "title": "Microsoft Introduces CoRAG: Enhancing AI Retrieval with Iterative Reasoning",
  "link": "https://www.infoq.com/news/2025/02/corag-microsoft-ai/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Microsoft AI has introduced Chain-of-Retrieval Augmented Generation (CoRAG), a new AI framework designed to enhance Retrieval-Augmented Generation (RAG) models. Unlike traditional RAG systems, which rely on a single retrieval step, CoRAG enables iterative search and reasoning, allowing AI models to refine their retrievals dynamically before generating answers. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Tue, 11 Feb 2025 17:45:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Microsoft",
    "Benchmark",
    "Retrieval-Augmented Generation",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 3677,
  "excerpt": "Microsoft AI has introduced Chain-of-Retrieval Augmented Generation (CoRAG), a new AI framework designed to enhance Retrieval-Augmented Generation (RAG) models. Unlike traditional RAG systems, which r",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250211144352/apple-touch-icon.png",
  "text": "Microsoft AI, in collaboration with Renmin University of China, has introduced Chain-of-Retrieval Augmented Generation (CoRAG), a new AI framework designed to enhance Retrieval-Augmented Generation (RAG) models. Unlike traditional RAG systems, which rely on a single retrieval step, CoRAG enables iterative search and reasoning, allowing AI models to refine their retrievals dynamically before generating answers. This improvement overcomes a significant drawback of traditional RAG systems: their lack of ability to effectively integrate information from multiple sources. In complex queries, particularly in multi-hop question answering (QA), traditional RAG models often struggle because they retrieve information only once, leading to incomplete or inaccurate results. CoRAG changes this by reformulating queries at each step, enabling AI to “think through” retrievals much like human researchers. The core innovation in CoRAG is its dynamic query reformulation mechanism. Instead of relying on a single retrieval step, the model iteratively refines its queries based on intermediate reasoning states. This process ensures that information retrieved at each stage is contextually relevant and builds toward a more complete final answer. To train CoRAG without expensive human annotations, researchers used rejection sampling, a technique that generates plausible retrieval chains from existing RAG datasets. The model is trained on these enhanced datasets, learning to generate sub-queries, sub-answers, and final answers. During inference, CoRAG offers flexible decoding strategies, such as: Greedy decoding for efficiency, Best-of-N sampling for optimized accuracy, and Tree search for balancing computational cost with performance. This scalability allows users to control retrieval depth, ensuring an optimal trade-off between accuracy and computational efficiency. Source: https://arxiv.org/abs/2501.14342 CoRAG has been tested on the KILT benchmark and multi-hop QA tasks, showing improved results compared to existing RAG models. The approach appears to be particularly effective in tasks that require retrieving and synthesizing information from multiple sources. KILT benchmark (source: https://arxiv.org/abs/2501.14342) Multi-hop QA tasks benchmark (source: https://arxiv.org/abs/2501.14342) The AI community has noted the potential impact of CoRAG. Deepesh Jain, a founder \u0026 CEO of Durapid Technologies, commented: This is a big step forward for RAG! Traditional methods often miss key details, but CoRAG’s iterative approach makes retrieval smarter and more dynamic. Letting models refine searches like humans do could unlock better answers for complex queries. Moreover, Ekaterina Baru, a senior machine learning engineer at Velotix, highlighted the resemblance to human research methods: This is a fascinating approach—using iterative retrieval to refine queries really mirrors how we, as researchers, naturally dive deeper into a problem. The performance gains on multi-hop QA tasks are impressive, and I’m curious how the trade-offs between longer chains and compute costs will evolve in production settings. Excited to see where this leads! By shifting from static retrieval to an iterative approach, CoRAG introduces a different way of handling AI-driven search and reasoning. This could be useful in areas such as automated research, enterprise knowledge systems, and AI-assisted decision-making, where retrieving accurate and well-structured information is essential. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2025/02/corag-microsoft-ai/en/headerimage/generatedHeaderImage-1739294801634.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eMicrosoft AI, in collaboration with Renmin University of China, has introduced \u003ca href=\"https://arxiv.org/abs/2501.14342\"\u003eChain-of-Retrieval Augmented Generation (CoRAG)\u003c/a\u003e, a new AI framework designed to enhance Retrieval-Augmented Generation (RAG) models. Unlike traditional RAG systems, which rely on a single retrieval step, CoRAG enables iterative search and reasoning, allowing AI models to refine their retrievals dynamically before generating answers.\u003c/p\u003e\n\n\u003cp\u003eThis improvement overcomes a significant drawback of traditional RAG systems: their lack of ability to effectively integrate information from multiple sources. In complex queries, particularly in multi-hop question answering (QA), traditional RAG models often struggle because they retrieve information only once, leading to incomplete or inaccurate results. CoRAG changes this by reformulating queries at each step, enabling AI to “think through” retrievals much like human researchers.\u003c/p\u003e\n\n\u003cdiv\u003e\u003cp\u003eThe core innovation in CoRAG is its dynamic query reformulation mechanism. Instead of relying on a single retrieval step, the model iteratively refines its queries based on intermediate reasoning states. This process ensures that information retrieved at each stage is contextually relevant and builds toward a more complete final answer.\u003c/p\u003e\u003cp\u003e\n\nTo train CoRAG without expensive human annotations, researchers used \u003ca href=\"https://bookdown.org/rdpeng/advstatcomp/rejection-sampling.html\"\u003erejection sampling\u003c/a\u003e, a technique that generates plausible retrieval chains from existing RAG datasets. The model is trained on these enhanced datasets, learning to generate sub-queries, sub-answers, and final answers.\u003c/p\u003e\u003cp\u003e\n\nDuring inference, CoRAG offers flexible decoding strategies, such as:\u003c/p\u003e\u003c/div\u003e\n\n\u003cul\u003e\n\t\u003cli\u003eGreedy decoding for efficiency,\u003c/li\u003e\n\t\u003cli\u003eBest-of-N sampling for optimized accuracy, and\u003c/li\u003e\n\t\u003cli\u003eTree search for balancing computational cost with performance.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis scalability allows users to control retrieval depth, ensuring an optimal trade-off between accuracy and computational efficiency.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"scheme\" data-src=\"news/2025/02/corag-microsoft-ai/en/resources/1Screenshot 2025-02-11 174706-1739294800524.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/02/corag-microsoft-ai/en/resources/1Screenshot 2025-02-11 174706-1739294800524.png\" rel=\"share\"/\u003e\u003cbr/\u003e\n\u003cem\u003eSource: https://arxiv.org/abs/2501.14342\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eCoRAG has been tested on the KILT benchmark and multi-hop QA tasks, showing improved results compared to existing RAG models. The approach appears to be particularly effective in tasks that require retrieving and synthesizing information from multiple sources.\u003c/p\u003e\n\n\u003cp\u003e\u003cmeta charset=\"utf-8\"/\u003e\u003cb id=\"docs-internal-guid-1a5628c4-7fff-0755-c4d6-63f35f34c3b3\"\u003e\u003cimg height=\"162\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcYgiUhjgnA2Vg7WEYJ7NdGiEP_PO7bFDsjMBY8fgTNwcKtk7vBIx56Qh90mlBL_-e1Gw6DIQ3dbRuKhGos4vwzn6cof0PNGLzLrsAoWzXaypm_s5ySGme4zPF6yNWhpceM_ah59A?key=-H26cDsaHu7D0mxE5vKCfD5e\" width=\"535\" rel=\"share\"/\u003e\u003c/b\u003e\u003cbr/\u003e\n\u003cem\u003eKILT benchmark (source: https://arxiv.org/abs/2501.14342)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cmeta charset=\"utf-8\"/\u003e\u003cb id=\"docs-internal-guid-e2443dc4-7fff-3b37-6518-4c772c09b734\"\u003e\u003cimg height=\"337\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdTh2jJlhmwkn40LFKGRaaQVKHJOhoCVmDjAistHJC12obFf6gxSMpuEYDbKI5MJwf6uOxESpf2z0Yz55J3CQ-6PMkDQRr0duzmi-7Mr6BjuTqQPJZ8PRwkWAhaJqP6ByqYenBk?key=-H26cDsaHu7D0mxE5vKCfD5e\" width=\"513\" rel=\"share\"/\u003e\u003c/b\u003e\u003cbr/\u003e\n\u003cem\u003eMulti-hop QA tasks benchmark (source: https://arxiv.org/abs/2501.14342)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThe AI community has noted the potential impact of CoRAG. Deepesh Jain, a founder \u0026amp; CEO of Durapid Technologies, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:ugcPost:7290443578860830720?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7290443578860830720%2C7290696176624906240%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287290696176624906240%2Curn%3Ali%3AugcPost%3A7290443578860830720%29\"\u003ecommented\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis is a big step forward for RAG! Traditional methods often miss key details, but CoRAG’s iterative approach makes retrieval smarter and more dynamic. Letting models refine searches like humans do could unlock better answers for complex queries.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eMoreover, Ekaterina Baru, a senior machine learning engineer at Velotix, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:ugcPost:7292858163345649664?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7292858163345649664%2C7293312022249668608%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287293312022249668608%2Curn%3Ali%3AugcPost%3A7292858163345649664%29\"\u003ehighlighted\u003c/a\u003e the resemblance to human research methods:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis is a fascinating approach—using iterative retrieval to refine queries really mirrors how we, as researchers, naturally dive deeper into a problem. The performance gains on multi-hop QA tasks are impressive, and I’m curious how the trade-offs between longer chains and compute costs will evolve in production settings. Excited to see where this leads!\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eBy shifting from static retrieval to an iterative approach, CoRAG introduces a different way of handling AI-driven search and reasoning. This could be useful in areas such as automated research, enterprise knowledge systems, and AI-assisted decision-making, where retrieving accurate and well-structured information is essential.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-02-11T00:00:00Z",
  "modifiedTime": null
}
