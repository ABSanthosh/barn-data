{
  "id": "9ce1b339-ac9d-4dc1-9cc8-c2f49770a64e",
  "title": "Stripe Rearchitects Its Observability Platform with Managed Prometheus and Grafana on AWS",
  "link": "https://www.infoq.com/news/2024/11/stripe-observability-aws-managed/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Stripe replaced its observability platform, which used a third-party vendor solution, with a new architecture utilizing managed services on AWS. The company made the move due to scalability limits, reliability issues, and increasing costs while transitioning to microservices. The migration involved dual-writing metrics, translating assets, validation, and user training. By Rafal Gancarz",
  "author": "Rafal Gancarz",
  "published": "Wed, 27 Nov 2024 14:16:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Prometheus",
    "Cost Optimization",
    "Cost Savings",
    "Scalability",
    "Microservices",
    "Observability",
    "Alerting",
    "Metrics",
    "Reliability",
    "Monitoring",
    "OpenTelemetry",
    "AWS",
    "Grafana",
    "Amazon CloudWatch",
    "DevOps",
    "Architecture \u0026 Design",
    "news"
  ],
  "byline": "Rafal Gancarz",
  "length": 3868,
  "excerpt": "Stripe replaced its observability platform, which used a third-party vendor solution, with a new architecture utilizing managed services on AWS. The company made the move due to scalability limits, re",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241126075425/apple-touch-icon.png",
  "text": "Stripe replaced its observability platform, which used a third-party vendor solution, with a new architecture utilizing managed services on AWS. The company made the move due to scalability limits, reliability issues, and increasing costs while transitioning to microservices. The migration involved dual-writing metrics, translating assets, validation, and user training. After adopting microservices, Stripe’s architecture generated around 300 million metrics, 40,000 alerts, and 100,000 dashboard queries generated by seven thousand employees. With such a large footprint, the preexisting observability platform started to struggle, resulting in scalability and reliability issues and increasing costs. The company decided to adopt a solution that would offer higher capacity and higher cost efficiency and chose to use Amazon managed service for Prometheus (AMP) and Grafana. The transitional architecture of the observability platform for metrics consisted of several components. Metric collected from compute hosts and scraped from Kubernetes clusters were delivered to the aggregation layer. Metrics from the aggregation layer and Amazon CloudWatch were ingested into Amazon Managed Prometheus by the Egress Proxy. Additionally, aggregated and unaggregated metrics were sent to the legacy time-series database to support the migration. Transitional Architecture for Metrics Storage, Aggregation, and Serving (Source: AWS Blog) Stripe, working with AWS, divided the migration process into four phases. The migration started by enabling the dual writing of metrics into legacy and new storage solutions, which was supported by creating a comprehensive suite of unit tests for validating the data flows. The second phase involved translating the assets (alerts and dashboards). Engineers created a solution for automating conversion from the legacy asset definitions to target ones using PromQL, AlertManager, and Grafana formats. Additionally, the company has promoted the adoption of template modules for alert definitions to improve monitoring coverage and consistency. The next stage of the process focused on validating the new architecture. Engineers used promtool to create automated unit tests and reverted to human reviews for unique alerts. The migration team also discovered some incorrect alerts in their legacy solution and developed heuristics for resolving such issues, falling back to human reviews if necessary. Additionally, Stripe leveraged the abstract syntax tree (AST) to categorize alert expressions for 70% of alerts that didn’t have historical data during the dual-write period. The last phase focused on adopting the new solution among the engineering user base. Cody Rioux and Michael Cowgill, staff engineers at Stripe, shared insights about transitioning between observability solutions and the impact on engineering teams: While validation is the most impactful workstream, migrating the mental models of Stripe’s user base was among the most influential on how the migration is perceived across the company. When making a choice about methods, there is a trade-off between scalable and personable methods. Scalable methods will save Observability team’s sanity, whereas personable methods will save users’ sanity, so both need to be employed. The team has highlighted key lessons learned during the rollout, including the need to engage the technical writing and developer education communities in the migration process, avoiding the trap of rushing the migration and rollout to engineering teams for the sake of hitting metrics and focusing on personalized education methods to create local experts that can bring multiplying effect in educating a broader use base. About the Author Rafal Gancarz",
  "image": "https://res.infoq.com/news/2024/11/stripe-observability-aws-managed/en/headerimage/generatedHeaderImage-1732620076729.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eStripe replaced its observability platform, which used a third-party vendor solution, \u003ca href=\"https://aws.amazon.com/blogs/mt/how-stripe-architected-massive-scale-observability-solution-on-aws/\"\u003ewith a new architecture utilizing managed services on AWS\u003c/a\u003e. The company made the move due to scalability limits, reliability issues, and increasing costs while transitioning to microservices. The migration involved dual-writing metrics, translating assets, validation, and user training.\u003c/p\u003e\n\n\u003cp\u003eAfter adopting microservices, Stripe’s architecture generated around 300 million metrics, 40,000 alerts, and 100,000 dashboard queries generated by seven thousand employees. With such a large footprint, the preexisting observability platform started to struggle, resulting in scalability and reliability issues and increasing costs.\u003c/p\u003e\n\n\u003cp\u003eThe company decided to adopt a solution that would offer higher capacity and higher cost efficiency and chose to use Amazon managed service for \u003ca href=\"https://aws.amazon.com/prometheus/\"\u003ePrometheus (AMP)\u003c/a\u003e and \u003ca href=\"https://aws.amazon.com/grafana/\"\u003eGrafana\u003c/a\u003e. The transitional architecture of the observability platform for metrics consisted of several components. Metric collected from compute hosts and scraped from Kubernetes clusters were delivered to the aggregation layer. Metrics from the aggregation layer and \u003ca href=\"https://aws.amazon.com/cloudwatch/\"\u003eAmazon CloudWatch\u003c/a\u003e were ingested into Amazon Managed Prometheus by the Egress Proxy. Additionally, aggregated and unaggregated metrics were sent to the legacy time-series database to support the migration.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/11/stripe-observability-aws-managed/en/resources/1stripe-observability-aws-1732715946892.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/11/stripe-observability-aws-managed/en/resources/1stripe-observability-aws-1732715946892.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eTransitional Architecture for Metrics Storage, Aggregation, and Serving (Source: \u003ca href=\"https://aws.amazon.com/blogs/mt/how-stripe-architected-massive-scale-observability-solution-on-aws/\"\u003eAWS Blog\u003c/a\u003e)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eStripe, working with AWS, divided the migration process into four phases. The migration started by enabling the dual writing of metrics into legacy and new storage solutions, which was supported by creating a comprehensive suite of unit tests for validating the data flows.\u003c/p\u003e\n\n\u003cp\u003eThe second phase involved translating the assets (alerts and dashboards). Engineers created a solution for automating conversion from the legacy asset definitions to target ones using \u003ca href=\"https://prometheus.io/docs/prometheus/latest/querying/basics/\"\u003ePromQL\u003c/a\u003e, \u003ca href=\"https://grafana.com/docs/grafana/latest/alerting/set-up/configure-alertmanager/\"\u003eAlertManager\u003c/a\u003e, and \u003ca href=\"https://grafana.com/docs/grafana/latest/dashboards/\"\u003eGrafana\u003c/a\u003e formats. Additionally, the company has promoted the adoption of template modules for alert definitions to improve monitoring coverage and consistency.\u003c/p\u003e\n\n\u003cp\u003eThe next stage of the process focused on validating the new architecture. Engineers used \u003ca href=\"https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/\"\u003epromtool\u003c/a\u003e to create automated unit tests and reverted to human reviews for unique alerts. The migration team also discovered some incorrect alerts in their legacy solution and developed heuristics for resolving such issues, falling back to human reviews if necessary. Additionally, Stripe leveraged the abstract \u003ca href=\"https://en.wikipedia.org/wiki/Abstract_syntax_tree\"\u003esyntax tree (AST)\u003c/a\u003e to categorize alert expressions for 70% of alerts that didn’t have historical data during the dual-write period.\u003c/p\u003e\n\n\u003cp\u003eThe last phase focused on adopting the new solution among the engineering user base. \u003ca href=\"https://www.linkedin.com/in/codyrioux\"\u003eCody Rioux\u003c/a\u003e and \u003ca href=\"https://www.linkedin.com/in/michael-cowgill-1707346/\"\u003eMichael Cowgill\u003c/a\u003e, staff engineers at Stripe, shared insights about transitioning between observability solutions and the impact on engineering teams:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhile validation is the most impactful workstream, migrating the mental models of Stripe’s user base was among the most influential on how the migration is perceived across the company. When making a choice about methods, there is a trade-off between scalable and personable methods. Scalable methods will save Observability team’s sanity, whereas personable methods will save users’ sanity, so both need to be employed.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe team has highlighted key lessons learned during the rollout, including the need to engage the technical writing and developer education communities in the migration process, avoiding the trap of rushing the migration and rollout to engineering teams for the sake of hitting metrics and focusing on personalized education methods to create local experts that can bring multiplying effect in educating a broader use base.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Rafal-Gancarz\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRafal Gancarz\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-11-27T00:00:00Z",
  "modifiedTime": null
}
