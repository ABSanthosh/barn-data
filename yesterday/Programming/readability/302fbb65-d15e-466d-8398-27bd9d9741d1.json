{
  "id": "302fbb65-d15e-466d-8398-27bd9d9741d1",
  "title": "Riverbed Data Hydration — Part 1",
  "link": "https://medium.com/airbnb-engineering/riverbed-data-hydration-part-1-e7011d62d946?source=rss----53c7c27702d5---4",
  "description": "",
  "author": "Xiangmin Liang",
  "published": "Tue, 10 Sep 2024 16:01:29 GMT",
  "source": "https://medium.com/feed/airbnb-engineering",
  "categories": [
    "data",
    "infrastructure",
    "data-science",
    "engineering",
    "architecture"
  ],
  "byline": "Xiangmin Liang",
  "length": 12104,
  "excerpt": "A deep dive into the streaming aspect of the Lambda architecture framework that optimizes how data is consumed from system-of-record data stores and updates secondary read-optimized stores at Airbnb…",
  "siteName": "The Airbnb Tech Blog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "by: Xiangmin Liang, Sivakumar Bhavanari, Amre ShakimA deep dive into the streaming aspect of the Lambda architecture framework that optimizes how data is consumed from system-of-record data stores and updates secondary read-optimized stores at Airbnb.OverviewIn our previous blog post we introduced the motivation and high-level architecture of Riverbed. As a recap, Riverbed is a part of Airbnb’s tech stack designed to streamline and optimize how data is consumed from system-of-record data stores and update secondary read-optimized stores. The framework is built around the concept of ‘materialized views’ — denormalized representations of data that can be queried in a predictable, efficient manner. The primary goal of Riverbed is to improve scalability, enable more efficient data fetching patterns, and provide enhanced filtering and search capabilities for a better user experience. It achieves this by keeping the read-optimized store up-to-date with the system-of-record data stores, and by making it easier for developers to build and manage pipelines that stitch together data from various data sources.In this blog post, we will delve deeper into the streaming aspect of the Lambda architecture framework. We’ll discuss step by step its critical components and explain how it constructs and sinks the materialized view from the Change Data Capture (CDC) events of various online data sources. Specifically, we’ll take a closer look at the join transformation within the Notification Pipeline, illustrating how we designed a DAG-like data structure to efficiently join different data sources together in a memory-efficient manner.To make the framework and its components easier to understand, let’s begin with a simplified example of a Riverbed pipeline definition:{ Review { id @documentId review User { id firstName lastName } }}Riverbed provides a declarative schema-based interface for customers to define Riverbed pipelines. From the sample definition above, a Riverbed pipeline is configured to integrate data sources from the Review and User entities, generating Riverbed sink documents with the review ID as the materialized view document ID.Based on this definition, Riverbed generates two types of streaming pipelines:Source Pipelines: Two pipelines consume CDC events from the Review and User tables respectively and publish Apache Kafka® events known as notification events, indicating which documents need to be refreshed.Notification Pipeline: This pipeline consumes the notification events published by the source pipelines and constructs materialized view documents to be written into sink stores.Now, let us delve deeper into these two types of pipelines.Source PipelinePicture 1. High-level system diagram of RiverbedPicture 1 shows the Source Pipeline as the first component in Riverbed. It is an auto-generated pipeline that listens to changes in system-of-record data sources. When changes occur, the Source Pipeline constructs NotificationEvents and emits them onto the Notification Kafka® topic to notify the Notification Pipeline on which documents should be refreshed. In the event-driven architecture of Riverbed, the Source Pipeline acts as the initial trigger for real-time updates in the read-optimized store. It not only ensures that the mutations in the underlying data sources are appropriately captured and communicated to the Notification Pipeline for subsequent processing, but also is the key solution for the concurrency and versioning issues in the framework.While the emphasis of this blog post is the Notification Pipeline, a detailed exploration of the Source Pipeline — especially its critical role in maintaining real-time data consistency and its interaction with Notification Pipelines — will be discussed in the next blog post of this series.Notification PipelinePicture 2. Notification Pipeline componentsThe Notification Pipeline is the core component of the Riverbed framework. It consumes Notification events, then queries dependent data sources and stitches together “documents” that are written into a read-optimized sink to support a materialized view. A notification event is processed by the following operations:Ingestion: For every change to a data source that the Read-Optimized Store is dependent on, we must re-index all affected documents to ensure freshness of data. In this step, Notification Pipeline consumes Notification events from Kafka® and deserializes them into objects that simply contain the document ID and primary source ID.Join: Based on these deserialized objects, Notification Pipeline queries various data stores to fetch all data sources that are necessary for building the materialized view.Stitch: This step models the join results from various data sources into a comprehensive Java Pojo called StitchModel, so that engineers can perform further customized data processing on it.Operate: In this step, a chain of various operators including filter, map, flatMap, etc, containing product-specific business logic can be applied to the StitchModel to convert it into the final document structure that will be stored in the index.Sink: As the last step, documents can be drained into various data sinks to refresh the materialized views.Among these operations, Join, Stitch and Sink are the most important as well as the most complicated ones. In the following sections, we will dive deeper into their design.Data Source JoinOne of the most crucial and intricate operations in Riverbed’s Notification Pipeline is the Join operation. A Join operation starts from the primary source ID and then fetches data for all data sources associated with the materialized view based on their relationship.JoinConditionsDagIn Riverbed, we use JoinConditionsDag, a Directed Acyclic Graph, to store the relationship metadata among data sources, where each node represents one unique data source and each edge represents the join condition between two data sources. In the Notification Pipelines, JoinConditionsDag’s root node is always a metadata node for the notification event which contains the document ID and the primary source ID. The join condition connecting to the notification event node reflects the join condition to query the primary source. Below is a sample JoinConditionsDag defining the join relationship between the primary source Listing and some of its related data sources:Picture 3: JoinConditionsDag SampleGiven notification events are used to indicate which document needs to be refreshed and does not contain any source data, Notification Pipeline joins data sources starting from the primary source ID provided by the Notification event. Guided by the JoinConditionsDag, when the Notification Pipeline processes a Notification event containing the primarySourceId, it queries the Listing table to fetch Listing data where the id matches primarySourceId. Subsequently, leveraging this Listing data, it queries the ListingDescription and Room tables to retrieve listing descriptions and rooms data, respectively, where the listingId equals id of Listing. In a similar manner, RoomAmenity data is obtained with roomId matching the id of the Room data.JoinResultsDagNow, we have the JoinConditionsDag guiding the Notification Pipeline to fetch all data sources. However, the question arises: how can we efficiently store the query results? One straightforward option is to flatten all the joined results into a table-like structure. Yet, this approach can consume a significant amount of memory, especially when performing joins with high cardinality. To optimize memory usage, we designed another DAG-like data structure named JoinResultsDag.Picture 4: JoinResultsDag StructureThere are two major components in a JoinResultsDag. Cell is the atomic container for a data record. Each cell maintains its own successor relationships by mapping successor data source aliases to the CellGroups. CellGroup is the container to store the joined records from one data source. Each data source table record is stored in each Cell.As mentioned above, the biggest difference and the advantage of using a DAG-based data structure instead of using the traditional flat join table is that it can efficiently store a large amount of join result data especially when there is a 1:M or M:N join relationship between data sources. For example, we have one pipeline to create materialized views for Airbnb Listings with information about all their Listing rooms, which also have lots of room amenities. If we use the traditional flat join table, it will look like the following table.Obviously, storing joined results using a flat table structure demands extensive resources for both storage and processing. In contrast, JoinResultsDag effectively mitigates data duplication by allowing multiple successor nodes to refer back to the same ancestor nodes.Picture 5: JoinResultsDag ExampleNow with JoinConditionsDag representing the relationship among all data sources and JoinResultsDag storing all the results, joins can be performed in Riverbed roughly as follows:Starting from the NotificationEvent, Riverbed first initializes a JoinResultsDag with the deserialized Notification event as root. Then guided by the JoinConditionsDag and following a depth-first-search traverse, it visits the data store of each source, queries data based on the join conditions defined on the JoinConditionsDag edges, encapsulates the query results rows inside each Cell and then continues fetching the data of its dependencies until finished visiting all data sources.Stitching of DataWith the joined results now stored in JoinResultsDag, an additional operation is necessary to transform these varied data pieces into a more usable and functional model. This enables engineers to apply their custom operators, mapping the data onto their specifically designed Sink Document. We refer to this process as the Stitch Operation, resulting in what is known as the StitchModel.The StitchModel, a Java POJO derived from the custom pipeline definition, serves as the intermediate data model that not only contains the actual data but also contains useful metadata about the event such as document ID, version, mutation source, etc.After the StitchModel metadata is generated, with the help of the JoinResultsDag, the Stitch operation is more straightforward. It maps the JoinResultsDag into a JSON model with the same structure and then converts the JSON model into the custom defined Java POJO utilizing the GSON library.Sink dataThe final stage in Riverbed’s Notification Pipeline is to write documents into data sinks. In Riverbed, sinks define where the processed data, now in the form of documents, will be ingested after the preceding operations are completed. Riverbed allows for multiple sinks, including Apache Hive(™) and Kafka®, so the same data can be ingested into multiple storage locations if required. This flexibility is a key advantage of the Notification Pipeline, enabling it to cater to a wide variety of use cases.Riverbed writes documents into data sinks via their write APIs. For the best performance, it encapsulates a collection of documents into the API request and then makes use of the batched write API of each data sink to update multiple documents efficiently.SummaryIn conclusion, we’ve navigated the critical steps of Riverbed’s streaming system within the Lambda architecture framework, focusing on the construction of materialized views from CDC events. Our highlight on the join transformation within the Notification Pipeline showcased a DAG-like structure for efficient and memory-conscious data joining. This discussion has shed light on the architectural approach to constructing materialized views in streaming and introduced innovative data structure designs for optimizing streaming data joins. Looking ahead, we will delve deeper into the Source Pipeline of the streaming system and explore the batch system of Riverbed, continuing our journey through advanced data architecture solutions.If this kind of work sounds appealing to you, check out our open roles — we’re hiring!",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*7kF7y_GLrhJyalhRpaHgTg.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca rel=\"noopener follow\" href=\"https://medium.com/@xiangmin.liang?source=post_page-----e7011d62d946--------------------------------\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Xiangmin Liang\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*hYqjjIUwEin_MumlgOY_UQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://medium.com/airbnb-engineering?source=post_page-----e7011d62d946--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"The Airbnb Tech Blog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*MlNQKg-sieBGW5prWoe9HQ.jpeg\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"f165\"\u003eby: Xiangmin Liang, Sivakumar Bhavanari, Amre Shakim\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"c7b3\"\u003eA deep dive into the streaming aspect of the Lambda architecture framework that optimizes how data is consumed from system-of-record data stores and updates secondary read-optimized stores at Airbnb.\u003c/p\u003e\u003ch2 id=\"ca28\"\u003eOverview\u003c/h2\u003e\u003cp id=\"1d33\"\u003eIn our \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/riverbed-optimizing-data-access-at-airbnbs-scale-c37ecf6456d9\"\u003eprevious blog post\u003c/a\u003e we introduced the motivation and high-level architecture of Riverbed. As a recap, Riverbed is a part of Airbnb’s tech stack designed to streamline and optimize how data is consumed from system-of-record data stores and update secondary read-optimized stores. The framework is built around the concept of ‘materialized views’ — denormalized representations of data that can be queried in a predictable, efficient manner. The primary goal of Riverbed is to improve scalability, enable more efficient data fetching patterns, and provide enhanced filtering and search capabilities for a better user experience. It achieves this by keeping the read-optimized store up-to-date with the system-of-record data stores, and by making it easier for developers to build and manage pipelines that stitch together data from various data sources.\u003c/p\u003e\u003cp id=\"4074\"\u003eIn this blog post, we will delve deeper into the streaming aspect of the Lambda architecture framework. We’ll discuss step by step its critical components and explain how it constructs and sinks the materialized view from the Change Data Capture (CDC) events of various online data sources. Specifically, we’ll take a closer look at the join transformation within the Notification Pipeline, illustrating how we designed a DAG-like data structure to efficiently join different data sources together in a memory-efficient manner.\u003c/p\u003e\u003cp id=\"fc5b\"\u003eTo make the framework and its components easier to understand, let’s begin with a simplified example of a Riverbed pipeline definition:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"5188\"\u003e{\u003cbr/\u003e  Review {\u003cbr/\u003e    id @documentId\u003cbr/\u003e    review\u003cp\u003e    User {\u003cbr/\u003e      id\u003cbr/\u003e      firstName\u003cbr/\u003e      lastName\u003cbr/\u003e    }\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"71a1\"\u003eRiverbed provides a declarative schema-based interface for customers to define Riverbed pipelines. From the sample definition above, a Riverbed pipeline is configured to integrate data sources from the Review and User entities, generating Riverbed sink documents with the review ID as the materialized view document ID.\u003c/p\u003e\u003cp id=\"08aa\"\u003eBased on this definition, Riverbed generates two types of streaming pipelines:\u003c/p\u003e\u003cul\u003e\u003cli id=\"0833\"\u003e\u003cstrong\u003eSource Pipelines:\u003c/strong\u003e Two pipelines consume CDC events from the Review and User tables respectively and publish Apache Kafka® events known as notification events, indicating which documents need to be refreshed.\u003c/li\u003e\u003cli id=\"8563\"\u003e\u003cstrong\u003eNotification Pipeline: \u003c/strong\u003eThis pipeline consumes the notification events published by the source pipelines and constructs materialized view documents to be written into sink stores.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"6732\"\u003eNow, let us delve deeper into these two types of pipelines.\u003c/p\u003e\u003ch2 id=\"c5a5\"\u003eSource Pipeline\u003c/h2\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cstrong\u003ePicture 1. High-level system diagram of Riverbed\u003c/strong\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"8192\"\u003ePicture 1 shows the Source Pipeline as the first component in Riverbed. It is an auto-generated pipeline that listens to changes in system-of-record data sources. When changes occur, the Source Pipeline constructs NotificationEvents and emits them onto the Notification Kafka® topic to notify the Notification Pipeline on which documents should be refreshed. In the event-driven architecture of Riverbed, the Source Pipeline acts as the initial trigger for real-time updates in the read-optimized store. It not only ensures that the mutations in the underlying data sources are appropriately captured and communicated to the Notification Pipeline for subsequent processing, but also is the key solution for the concurrency and versioning issues in the framework.\u003c/p\u003e\u003cp id=\"8373\"\u003eWhile the emphasis of this blog post is the Notification Pipeline, a detailed exploration of the Source Pipeline — especially its critical role in maintaining real-time data consistency and its interaction with Notification Pipelines — will be discussed in the next blog post of this series.\u003c/p\u003e\u003ch2 id=\"6d46\"\u003eNotification Pipeline\u003c/h2\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cstrong\u003ePicture 2. Notification Pipeline components\u003c/strong\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"71fa\"\u003eThe Notification Pipeline is the core component of the Riverbed framework. It consumes Notification events, then queries dependent data sources and stitches together “documents” that are written into a read-optimized sink to support a materialized view. A notification event is processed by the following operations:\u003c/p\u003e\u003cul\u003e\u003cli id=\"b3c3\"\u003e\u003cstrong\u003eIngestion: \u003c/strong\u003eFor every change to a data source that the Read-Optimized Store is dependent on, we must re-index all affected documents to ensure freshness of data. In this step, Notification Pipeline consumes Notification events from Kafka® and deserializes them into objects that simply contain the document ID and primary source ID.\u003c/li\u003e\u003cli id=\"6a8d\"\u003e\u003cstrong\u003eJoin: \u003c/strong\u003eBased on these deserialized objects, Notification Pipeline queries various data stores to fetch all data sources that are necessary for building the materialized view.\u003c/li\u003e\u003cli id=\"21a8\"\u003e\u003cstrong\u003eStitch: \u003c/strong\u003eThis step models the join results from various data sources into a comprehensive Java Pojo called StitchModel, so that engineers can perform further customized data processing on it.\u003c/li\u003e\u003cli id=\"8cac\"\u003e\u003cstrong\u003eOperate: \u003c/strong\u003eIn this step, a chain of various operators including filter, map, flatMap, etc, containing product-specific business logic can be applied to the StitchModel to convert it into the final document structure that will be stored in the index.\u003c/li\u003e\u003cli id=\"78b8\"\u003e\u003cstrong\u003eSink: \u003c/strong\u003eAs the last step, documents can be drained into various data sinks to refresh the materialized views.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"41f9\"\u003eAmong these operations, Join, Stitch and Sink are the most important as well as the most complicated ones. In the following sections, we will dive deeper into their design.\u003c/p\u003e\u003ch2 id=\"6673\"\u003eData Source Join\u003c/h2\u003e\u003cp id=\"5da0\"\u003eOne of the most crucial and intricate operations in Riverbed’s Notification Pipeline is the Join operation. A Join operation starts from the primary source ID and then fetches data for all data sources associated with the materialized view based on their relationship.\u003c/p\u003e\u003ch2 id=\"5ee3\"\u003eJoinConditionsDag\u003c/h2\u003e\u003cp id=\"0fdd\"\u003eIn Riverbed, we use JoinConditionsDag, a Directed Acyclic Graph, to store the relationship metadata among data sources, where each node represents one unique data source and each edge represents the join condition between two data sources. In the Notification Pipelines, JoinConditionsDag’s root node is always a metadata node for the notification event which contains the document ID and the primary source ID. The join condition connecting to the notification event node reflects the join condition to query the primary source. Below is a sample JoinConditionsDag defining the join relationship between the primary source Listing and some of its related data sources:\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cstrong\u003ePicture 3: JoinConditionsDag Sample\u003c/strong\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"713c\"\u003eGiven notification events are used to indicate which document needs to be refreshed and does not contain any source data, Notification Pipeline joins data sources starting from the primary source ID provided by the Notification event. Guided by the JoinConditionsDag, when the Notification Pipeline processes a Notification event containing the primarySourceId, it queries the Listing table to fetch Listing data where the id matches primarySourceId. Subsequently, leveraging this Listing data, it queries the ListingDescription and Room tables to retrieve listing descriptions and rooms data, respectively, where the listingId equals id of Listing. In a similar manner, RoomAmenity data is obtained with roomId matching the id of the Room data.\u003c/p\u003e\u003ch2 id=\"2b68\"\u003eJoinResultsDag\u003c/h2\u003e\u003cp id=\"5a74\"\u003eNow, we have the JoinConditionsDag guiding the Notification Pipeline to fetch all data sources. However, the question arises: how can we efficiently store the query results? One straightforward option is to flatten all the joined results into a table-like structure. Yet, this approach can consume a significant amount of memory, especially when performing joins with high cardinality. To optimize memory usage, we designed another DAG-like data structure named JoinResultsDag.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cstrong\u003ePicture 4: JoinResultsDag Structure\u003c/strong\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"7b69\"\u003eThere are two major components in a JoinResultsDag. \u003cstrong\u003eCell\u003c/strong\u003e is the atomic container for a data record. Each cell maintains its own successor relationships by mapping successor data source aliases to the CellGroups. \u003cstrong\u003eCellGroup\u003c/strong\u003e is the container to store the joined records from one data source. Each data source table record is stored in each Cell.\u003c/p\u003e\u003cp id=\"9198\"\u003eAs mentioned above, the biggest difference and the advantage of using a DAG-based data structure instead of using the traditional flat join table is that it can efficiently store a large amount of join result data especially when there is a 1:M or M:N join relationship between data sources. For example, we have one pipeline to create materialized views for Airbnb Listings with information about all their Listing rooms, which also have lots of room amenities. If we use the traditional flat join table, it will look like the following table.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"11c2\"\u003eObviously, storing joined results using a flat table structure demands extensive resources for both storage and processing. In contrast, JoinResultsDag effectively mitigates data duplication by allowing multiple successor nodes to refer back to the same ancestor nodes.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cstrong\u003ePicture 5: JoinResultsDag Example\u003c/strong\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"a066\"\u003eNow with JoinConditionsDag representing the relationship among all data sources and JoinResultsDag storing all the results, joins can be performed in Riverbed roughly as follows:\u003c/p\u003e\u003cp id=\"bd13\"\u003eStarting from the NotificationEvent, Riverbed first initializes a JoinResultsDag with the deserialized Notification event as root. Then guided by the JoinConditionsDag and following a depth-first-search traverse, it visits the data store of each source, queries data based on the join conditions defined on the JoinConditionsDag edges, encapsulates the query results rows inside each Cell and then continues fetching the data of its dependencies until finished visiting all data sources.\u003c/p\u003e\u003ch2 id=\"1235\"\u003eStitching of Data\u003c/h2\u003e\u003cp id=\"fa03\"\u003eWith the joined results now stored in JoinResultsDag, an additional operation is necessary to transform these varied data pieces into a more usable and functional model. This enables engineers to apply their custom operators, mapping the data onto their specifically designed Sink Document. We refer to this process as the Stitch Operation, resulting in what is known as the StitchModel.\u003c/p\u003e\u003cp id=\"6c10\"\u003eThe StitchModel, a Java \u003ca href=\"https://en.wikipedia.org/wiki/Plain_old_Java_object\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePOJO\u003c/a\u003e derived from the custom pipeline definition, serves as the intermediate data model that not only contains the actual data but also contains useful metadata about the event such as document ID, version, mutation source, etc.\u003c/p\u003e\u003cp id=\"2b44\"\u003eAfter the StitchModel metadata is generated, with the help of the JoinResultsDag, the Stitch operation is more straightforward. It maps the JoinResultsDag into a JSON model with the same structure and then converts the JSON model into the custom defined Java POJO utilizing the GSON library.\u003c/p\u003e\u003ch2 id=\"8840\"\u003eSink data\u003c/h2\u003e\u003cp id=\"2041\"\u003eThe final stage in Riverbed’s Notification Pipeline is to write documents into data sinks. In Riverbed, sinks define where the processed data, now in the form of documents, will be ingested after the preceding operations are completed. Riverbed allows for multiple sinks, including Apache Hive(™) and Kafka®, so the same data can be ingested into multiple storage locations if required. This flexibility is a key advantage of the Notification Pipeline, enabling it to cater to a wide variety of use cases.\u003c/p\u003e\u003cp id=\"cb04\"\u003eRiverbed writes documents into data sinks via their write APIs. For the best performance, it encapsulates a collection of documents into the API request and then makes use of the batched write API of each data sink to update multiple documents efficiently.\u003c/p\u003e\u003ch2 id=\"8451\"\u003eSummary\u003c/h2\u003e\u003cp id=\"722c\"\u003eIn conclusion, we’ve navigated the critical steps of Riverbed’s streaming system within the Lambda architecture framework, focusing on the construction of materialized views from CDC events. Our highlight on the join transformation within the Notification Pipeline showcased a DAG-like structure for efficient and memory-conscious data joining. This discussion has shed light on the architectural approach to constructing materialized views in streaming and introduced innovative data structure designs for optimizing streaming data joins. Looking ahead, we will delve deeper into the Source Pipeline of the streaming system and explore the batch system of Riverbed, continuing our journey through advanced data architecture solutions.\u003c/p\u003e\u003cp id=\"4c1a\"\u003e\u003cem\u003eIf this kind of work sounds appealing to you, check out our \u003c/em\u003e\u003ca href=\"https://careers.airbnb.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eopen roles\u003c/em\u003e\u003c/a\u003e\u003cem\u003e — we’re hiring!\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2024-09-10T16:01:28.288Z",
  "modifiedTime": null
}
