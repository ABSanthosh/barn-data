{
  "id": "b73da1e2-3340-462e-a99f-fa733b3fec9b",
  "title": "EuroLLM-9B Aims to Improve State of the Art LLM Support for European Languages",
  "link": "https://www.infoq.com/news/2024/12/eurollm-9b/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "EuroLLM-9B is an open-source large language model built in Europe and tailored to European languages, including all the official EU languages as well as 11 other non-official albeit commercially important languages. According to the team behind it, its performance makes it one of the best European-made LLM of this size. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Fri, 27 Dec 2024 13:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Translation",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3300,
  "excerpt": "EuroLLM-9B is an open-source large language model built in Europe and tailored to European languages, including all the official EU languages as well as 11 other non-official albeit commercially impor",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241210082921/apple-touch-icon.png",
  "text": "EuroLLM-9B is an open-source large language model built in Europe and tailored to European languages, including all the official EU languages as well as 11 other non-official albeit commercially important languages. According to the team behind it, its performance makes it one of the best European-made LLM of this size. EuroLLM-9B is the second LLM created within the EuroLLM initiative, coming a few months after the smaller EuroLLM-1.7B. The key component in EuroLLM-9B to make its performance stronger for European languages is the tokenizer, which was built using a vocabulary of 128,000 word pieces belonging to European languages. The model was pre-trained on approximately 4 trillion tokens using the GPU infrastructure provided by the Barcelona-based MareNostrum5 supercomputer. In the post-training phase, the EuroLLM team used publicly available datasets to fine-tune it and make it capable of handling multi-turn conversations and behave as an instruction-following model. One of the goals of the team was showing the model suitability to be fine-tuned for specific use case. According to the team, the model excels at translating texts across all supported languages, a task in which it outperforms Gemma-2-9B–IT and Aya-expanse-8B. To assess the model performance, the team ran benchmarks both in English and in EU languages. Unsurprisingly, for European languages, EuroLLM outperforms both European models, such as Mistral-7BV, Salamandra-7B, and others, as well as non-European languages, including LLama-3.1-8B, Qwen-2-5-7B, and others, with Gemma-2-9B achieving comparable results. For English languages, EuroLLM-9B shows good performance, on a par with Mistral-7B. As expected, a 9B model cannot match the performance of a 70B model. However, the scores are very good and come remarkably close to the larger models, especially when using a beam size of 4. The model is available on Hugging Face, where you can run it as shown in the following snippet: from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"utter-project/EuroLLM-9B-Instruct\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id) messages = [ { \"role\": \"system\", \"content\": \"You are EuroLLM --- an AI assistant specialized in European languages that provides safe, educational and helpful answers.\", }, { \"role\": \"user\", \"content\": \"What is the capital of Portugal? How would you describe it?\" }, ] inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\") outputs = model.generate(inputs, max_new_tokens=1024) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) As several Reddit users point out, the need for open-source models tailored to European languages is real, since even larger models like Llama 3.3 70B may perform unsatisfactorily, not to mention the cost of fine-tuning it.  The EuroLLM team is already at work on a larger version of the model, to make it more competitive with larger models, but has not clarified when it could become available. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2024/12/eurollm-9b/en/headerimage/eurollm-9b-1735302792967.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://huggingface.co/blog/eurollm-team/eurollm-9b\"\u003eEuroLLM-9B is an open-source large language model built in Europe and tailored to European languages\u003c/a\u003e, including all the official EU languages as well as 11 other non-official albeit commercially important languages. According to the team behind it, its performance makes it one of the best European-made LLM of this size.\u003c/p\u003e\n\n\u003cp\u003eEuroLLM-9B is the second LLM created within the \u003ca href=\"https://sites.google.com/view/eurollm/home\"\u003eEuroLLM initiative\u003c/a\u003e, coming a few months after the smaller \u003ca href=\"https://huggingface.co/utter-project/EuroLLM-1.7B\"\u003eEuroLLM-1.7B\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe key component in EuroLLM-9B to make its performance stronger for European languages is the tokenizer, which was built using a vocabulary of 128,000 word pieces belonging to European languages. The model was pre-trained on approximately 4 trillion tokens using the GPU infrastructure provided by the Barcelona-based \u003ca href=\"https://bsc.es\"\u003eMareNostrum5 supercomputer\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eIn the post-training phase, the EuroLLM team used publicly available datasets to fine-tune it and make it capable of handling multi-turn conversations and behave as an instruction-following model. One of the goals of the team was showing the model suitability to be fine-tuned for specific use case.\u003c/p\u003e\n\n\u003cp\u003eAccording to the team, the model excels at translating texts across all supported languages, a task in which it outperforms Gemma-2-9B–IT and Aya-expanse-8B.\u003c/p\u003e\n\n\u003cp\u003eTo assess the model performance, the team ran benchmarks both in English and in EU languages. Unsurprisingly, for European languages, EuroLLM outperforms both European models, such as Mistral-7BV, Salamandra-7B, and others, as well as non-European languages, including LLama-3.1-8B, Qwen-2-5-7B, and others, with Gemma-2-9B achieving comparable results. For English languages, EuroLLM-9B shows good performance, on a par with Mistral-7B.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/12/eurollm-9b/en/resources/1eurollm-9b-perf-1735302791715.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/12/eurollm-9b/en/resources/1eurollm-9b-perf-1735302791715.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAs expected, a 9B model cannot match the performance of a 70B model. However, the scores are very good and come remarkably close to the larger models, especially when using a beam size of 4.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe model is available on Hugging Face, where you can run it as shown in the following snippet:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003efrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \u0026#34;utter-project/EuroLLM-9B-Instruct\u0026#34;\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\nmessages = [\n    {\n        \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;,\n        \u0026#34;content\u0026#34;: \u0026#34;You are EuroLLM --- an AI assistant specialized in European languages that provides safe, educational and helpful answers.\u0026#34;,\n    },\n    {\n        \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the capital of Portugal? How would you describe it?\u0026#34;\n    },\n    ]\n\ninputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\u0026#34;pt\u0026#34;)\noutputs = model.generate(inputs, max_new_tokens=1024)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eAs several \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1h9h04x/models_for_less_popular_languages_dutch_what_is/\"\u003eReddit users point out\u003c/a\u003e, the need for open-source models tailored to European languages is real, since even larger models like Llama 3.3 70B may perform unsatisfactorily, not to mention the cost of fine-tuning it. \u003c/p\u003e\n\n\u003cp\u003eThe EuroLLM team is already at work on a larger version of the model, to make it more competitive with larger models, but has not clarified when it could become available.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-12-27T00:00:00Z",
  "modifiedTime": null
}
