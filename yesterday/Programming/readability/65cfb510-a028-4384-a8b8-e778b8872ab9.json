{
  "id": "65cfb510-a028-4384-a8b8-e778b8872ab9",
  "title": "Sharing our latest differential privacy milestones and advancements",
  "link": "https://developers.googleblog.com/en/sharing-our-latest-differential-privacy-milestones-and-advancements/",
  "description": "Google is committed to investing in privacy-enhancing technologies (PETs) to ensure user data privacy, improving products like Google Home and Google Search.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Miguel Guevara",
  "length": 6267,
  "excerpt": "Google is committed to investing in privacy-enhancing technologies (PETs) to ensure user data privacy, improving products like Google Home and Google Search.",
  "siteName": "",
  "favicon": "",
  "text": "As new digital platforms and services emerge, the challenge of keeping users’ information safe online is growing more complex – novel technologies require novel privacy solutions. At Google, we continue to invest in privacy-enhancing technologies (PETs), a family of cutting-edge tools that help solve the critical task of data processing by providing people guarantees that their personal information is kept private and secure.Over the past decade, we’ve integrated PETs throughout our product suite, used them to help tackle societal challenges and made many of our own freely available to developers and researchers around the world via open source projects.Today we’re excited to share updates on our work with differential privacy, a mathematical framework that allows for analysis of datasets in a privacy-preserving way to help ensure individual information is never revealed.Reaching a differential privacy milestoneDifferential privacy is a PET not known by most users, but one of the unsung heroes behind some of the most widely used tech features today. But like many PETs, industry adoption of differential privacy can be challenging for many reasons: complex technical integrations, limited scalability for large applications, high costs for computing resources and more.We’re pleased to announce we have achieved what we know to be the largest application of differential privacy in the world spanning close to three billion devices over the past year, helping Google improve products like Google Home, Google Search on Android and Messages. Using this technology we were able to improve the overall user experience in these products.For example, we were able to identify the root causes of crashes for Matter devices in Google Home to help increase customer satisfaction. Matter is an industry standard simplifying the set up and control of smart home devices across smart home ecosystems. As Google Home continued to add support for new device types, our team uncovered and quickly patched some connectivity issues with the Home app by using insights unlocked by our differential privacy tool.This three billion device deployment was made possible through six plus years of research on our “shuffler” model, which effectively shuffles data between “local” and “central” models to achieve more accurate analysis on larger data sets while still maintaining the strongest privacy guarantees.Democratizing access to differential privacyOver five years ago, we set out on a mission to democratize access to our PETs by releasing the first open source version of our foundational differential privacy libraries. Our goal is to make many of the same technologies we use internally freely available to anyone, in turn lowering the barrier to entry for developers and researchers worldwide.As part of this commitment, we open sourced a first-of-its-kind Fully Homomorphic Encryption (FHE) transpiler two years ago and have continued to remove barriers to entry along the way. We have also done the same with our work on Federated Learning and other privacy technologies like secure multi-party computation, which allows two parties (e.g., two research institutions) to join their data and do analysis on the combined data without ever revealing the underlying information.Since 2019, we’ve expanded access to these libraries by publishing them in new programming languages to reach as many developers as possible. Today, we are announcing the release of PipelineDP for Java Virtual Machine (JVM) called PipelineDP4j. This work is an evolution of the joint work we’ve done with OpenMined. PipelineDP4j allows developers to execute highly parallelizable computations using Java as the baseline language, and opens the door for new applications of differential privacy by reducing the barrier of entry for developers already working in Java. With the addition of this JVM release, we now cover some of the most popular developer languages – Python, Java, Go, and C++ – potentially reaching more than half of all developers worldwide.Additionally, some of our latest differential privacy algorithms are now helping power unique tools like Google Trends. One of our model developments now allows Google Trends to provide greater insights into low-volume locales. For differential privacy – and most privacy guarantees in general – datasets need to meet a minimum threshold to ensure individuals’ data isn’t revealed. Our new offering can help professionals like researchers and local journalists obtain more insights on smaller cities or areas, and thus shine a light on top of mind topics. For example, a journalist in Luxembourg making queries for Portuguese language results can now access insights that were not available before.Auditing for differentially private algorithmsThe increased adoption of differential privacy both by industry and governments is a major advancement in handling user data in a private way. Nevertheless, this widespread adoption can also lead to an increased risk of faulty mechanism design and implementation. The vast volume of algorithms developed in this field renders manual inspection of their implementation impractical – and there is a lack of flexible tools capable of testing the diverse range of techniques without significant assumptions.To allow practitioners to test whether a given mechanism violates a differential privacy guarantee, we are releasing a library, DP-Auditorium, utilizing only samples from the mechanism itself, without requiring access to any internal properties of the application.Effective testing for a privacy guarantee entails two key steps: evaluating the privacy guarantee over a fixed dataset, and exploring datasets to find the \"worst-case\" privacy guarantee. DP-Auditorium introduces versatile interfaces for both components, facilitating efficient testing and consistently outperforming existing black-box access testers. Most importantly, these interfaces are designed to be flexible, enabling contributions and expansions from the research community, thereby continually augmenting the testing capabilities of the tool.We’ll continue to build on our long-standing investment in PETs and commitment to helping developers and researchers securely process and protect user data and privacy.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/header.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n    \u003cp data-block-key=\"vgrf7\"\u003eAs new digital platforms and services emerge, the challenge of keeping users’ information safe online is growing more complex – novel technologies require novel privacy solutions. At Google, we continue to invest in privacy-enhancing technologies (PETs), a family of cutting-edge tools that help solve the critical task of data processing by providing people guarantees that their personal information is kept private and secure.\u003c/p\u003e\u003cp data-block-key=\"ddrbu\"\u003eOver the past decade, we’ve integrated PETs throughout our product suite, used them to help tackle societal challenges and made many of our own freely available to developers and researchers around the world via open source projects.\u003c/p\u003e\u003cp data-block-key=\"cjqe0\"\u003eToday we’re excited to share updates on our work with differential privacy, a mathematical framework that allows for analysis of datasets in a privacy-preserving way to help ensure individual information is never revealed.\u003c/p\u003e\u003ch3 data-block-key=\"4an86\"\u003e\u003cb\u003e\u003cbr/\u003eReaching a differential privacy milestone\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"5oure\"\u003eDifferential privacy is a PET not known by most users, but one of the unsung heroes behind some of the most widely used tech features today. But like many PETs, industry adoption of differential privacy can be challenging for many reasons: complex technical integrations, limited scalability for large applications, high costs for computing resources and more.\u003c/p\u003e\u003cp data-block-key=\"6e4v\"\u003eWe’re pleased to announce we have achieved what we know to be the largest application of differential privacy in the world spanning close to three billion devices over the past year, helping Google improve products like Google Home, Google Search on Android and Messages. Using this technology we were able to improve the overall user experience in these products.\u003c/p\u003e\u003cp data-block-key=\"ftbgl\"\u003eFor example, we were able to identify the root causes of crashes for Matter devices in Google Home to help increase customer satisfaction. Matter is an \u003ca href=\"https://www.theverge.com/23568091/matter-compatible-devices-accessories-apple-amazon-google-samsung\"\u003eindustry standard\u003c/a\u003e simplifying the set up and control of smart home devices across smart home ecosystems. As Google Home continued to add support for new device types, our team uncovered and quickly patched some connectivity issues with the Home app by using insights unlocked by our differential privacy tool.\u003c/p\u003e\u003cp data-block-key=\"6fsrt\"\u003eThis three billion device deployment was made possible through six plus years of research on our \u003ca href=\"https://research.google/pubs/prochlo-strong-privacy-for-analytics-in-the-crowd/\"\u003e“shuffler” model\u003c/a\u003e, which effectively shuffles data between “local” and “central” models to achieve more accurate analysis on larger data sets while still maintaining the strongest privacy guarantees.\u003c/p\u003e\u003ch3 data-block-key=\"5hq7d\"\u003e\u003cb\u003e\u003cbr/\u003eDemocratizing access to differential privacy\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"eknml\"\u003eOver five years ago, we set out on a mission to democratize access to our PETs by releasing the \u003ca href=\"https://developers.googleblog.com/en/enabling-developers-and-organizations-to-use-differential-privacy/\"\u003efirst open source version\u003c/a\u003e of our foundational differential privacy libraries. Our goal is to make many of the same technologies we use internally freely available to anyone, in turn lowering the barrier to entry for developers and researchers worldwide.\u003c/p\u003e\u003cp data-block-key=\"6h76j\"\u003eAs part of this commitment, we open sourced a first-of-its-kind Fully Homomorphic Encryption (FHE) transpiler \u003ca href=\"https://developers.googleblog.com/2021/06/our-latest-updates-on-fully-homomorphic-encryption.html\"\u003etwo years ago\u003c/a\u003e and have continued to \u003ca href=\"https://developers.googleblog.com/2022/12/new-privacy-enhancing-technology-for-everyone.html\"\u003eremove barriers to entry\u003c/a\u003e along the way. We have also done the same with our work on \u003ca href=\"https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/\"\u003eFederated Learning\u003c/a\u003e and other privacy technologies like \u003ca href=\"https://security.googleblog.com/2019/06/helping-organizations-do-more-without-collecting-more-data.html\"\u003esecure multi-party computation\u003c/a\u003e, which allows two parties (e.g., two research institutions) to join their data and do analysis on the combined data without ever revealing the underlying information.\u003c/p\u003e\u003cp data-block-key=\"3hbev\"\u003eSince 2019, we’ve expanded access to these libraries by publishing them in new programming languages to reach as many developers as possible. Today, we are announcing the release of PipelineDP for Java Virtual Machine (JVM) called \u003ca href=\"https://github.com/google/differential-privacy/tree/main/pipelinedp4j\"\u003ePipelineDP4j\u003c/a\u003e. This work is an evolution of the joint work we’ve done with \u003ca href=\"https://openmined.org/\"\u003eOpenMined\u003c/a\u003e. PipelineDP4j allows developers to execute highly parallelizable computations using Java as the baseline language, and opens the door for new applications of differential privacy by reducing the barrier of entry for developers already working in Java. With the addition of this JVM release, we now cover some of the most popular developer languages – Python, Java, Go, and C++ – potentially reaching \u003ca href=\"https://www.statista.com/statistics/793628/worldwide-developer-survey-most-used-languages/\"\u003emore than half\u003c/a\u003e of all developers worldwide.\u003c/p\u003e\u003cp data-block-key=\"b44jb\"\u003eAdditionally, some of our latest differential privacy algorithms are now helping power unique tools like Google Trends. One of our model developments now allows Google Trends to provide greater insights into low-volume locales. For differential privacy – and most privacy guarantees in general – datasets need to meet a minimum threshold to ensure individuals’ data isn’t revealed. Our new offering can help professionals like researchers and local journalists obtain more insights on smaller cities or areas, and thus shine a light on top of mind topics. For example, a journalist in Luxembourg making queries for Portuguese language results can now access insights that were not available before.\u003c/p\u003e\u003ch3 data-block-key=\"3e7db\"\u003e\u003cb\u003e\u003cbr/\u003eAuditing for differentially private algorithms\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"28obn\"\u003eThe increased adoption of differential privacy both by industry and governments is a major advancement in handling user data in a private way. Nevertheless, this widespread adoption can also lead to an increased risk of faulty mechanism design and implementation. The vast volume of algorithms developed in this field renders manual inspection of their implementation impractical – and there is a lack of flexible tools capable of testing the diverse range of techniques without significant assumptions.\u003c/p\u003e\u003cp data-block-key=\"22m98\"\u003eTo allow practitioners to test whether a given mechanism violates a differential privacy guarantee, we are releasing a library, DP-Auditorium, utilizing only samples from the mechanism itself, without requiring access to any internal properties of the application.\u003c/p\u003e\u003cp data-block-key=\"5s74s\"\u003eEffective testing for a privacy guarantee entails two key steps: evaluating the privacy guarantee over a fixed dataset, and exploring datasets to find the \u0026#34;worst-case\u0026#34; privacy guarantee. DP-Auditorium introduces versatile interfaces for both components, facilitating efficient testing and consistently outperforming existing black-box access testers. Most importantly, these interfaces are designed to be flexible, enabling contributions and expansions from the research community, thereby continually augmenting the testing capabilities of the tool.\u003c/p\u003e\u003cp data-block-key=\"9h7cd\"\u003eWe’ll continue to build on our long-standing investment in PETs and commitment to helping developers and researchers securely process and protect user data and privacy.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-10-31T00:00:00Z",
  "modifiedTime": null
}
