{
  "id": "55134e53-6d5f-41a5-8aaf-13368c3b99b1",
  "title": "Optimizing Our E2E Pipeline",
  "link": "https://slack.engineering/speedup-e2e-testing/",
  "description": "In the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time… The post Optimizing Our E2E Pipeline appeared first on Engineering at Slack.",
  "author": "Dan Carton",
  "published": "Mon, 14 Apr 2025 09:00:30 +0000",
  "source": "https://slack.engineering/feed",
  "categories": [
    "Uncategorized",
    "ci-cd",
    "developer-experience",
    "developer-productivity",
    "devops",
    "frontend",
    "testing"
  ],
  "byline": "",
  "length": 7078,
  "excerpt": "In the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time…",
  "siteName": "Engineering at Slack",
  "favicon": "https://slack.engineering/wp-content/uploads/sites/7/2020/05/cropped-octothrope-1.png?w=192",
  "text": "In the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time and resources for engineers at Slack. The Problem: Unnecessary Frontend Builds For one of our largest code repositories (a monolithic repository, or monorepo), Slack has a CI/CD pipeline that runs E2E tests before merging code into the \u003cspan style=\"font-weight: 400\"\u003emain\u003c/span\u003e branch. This is critical for ensuring that changes are validated across the entire stack for the Slack application: frontend, backend, database, and the handful of services in between. However, we noticed a bottleneck: building the frontend code was taking longer than expected and occurred too frequently, even when there were no frontend-related changes. Here’s the breakdown: Developer Workflow: A developer makes changes and pushes to a branch. Build Process: The frontend code is built (~5 minutes). Deployment: The build is deployed to a QA environment. Testing: We run over 200 E2E tests, taking another 5 minutes. This entire process took about 10 minutes per run. Half of that time, around 5 minutes, was consumed by frontend builds, even when no frontend changes were involved. Given that hundreds of pull requests (PRs) are merged daily, these redundant builds were not only time-consuming, but costly: Thousands of frontend builds per week, storing nearly a gigabyte of data per build in AWS S3. Half of these builds do not contain frontend changes compared to the last merge into \u003cspan style=\"font-weight: 400\"\u003emain\u003c/span\u003e, causing terabytes of duplicate data. 5 minutes per build, adding unnecessary delays to pipelines (thousands of hours a week). The Solution: Smarter Build Strategy with Cached Frontend Assets To tackle this, we leveraged existing tools to rethink our build strategy. Step 1: Conditional Frontend Builds Our first step was determining whether a fresh frontend build was necessary. We detected changes by utilizing \u003cspan style=\"font-weight: 400\"\u003egit diff\u003c/span\u003e and its 3-dot notation to identify the difference between the latest common commit of the current checked-out branch and \u003cspan style=\"font-weight: 400\"\u003emain\u003c/span\u003e. If changes were detected, we invoke a frontend build job. If no changes were detected, we skipped the build entirely and reused a prebuilt version.  Step 2: Prebuilt Assets and Internal CDN When a frontend build is not needed, we locate an existing build from AWS S3. To be efficient, we use a recent frontend build that is still in Production. We delegate the task of serving the prebuilt frontend assets for our E2E tests to an internal CDN. This reduced the need for creating a new build on each PR, while still ensuring we test on current assets. The Challenges: Efficiency at Scale While the approach seemed straightforward, scaling this solution to our monorepo presented a few challenges: Identifying Frontend Changes: Our repository contains over 100,000 tracked files. Determining whether frontend changes were present required efficient file tracking, which git handled in just a couple of seconds.  Finding Prebuilt Assets: With hundreds of PRs merged into this repository daily, identifying a prebuilt version that was fresh enough required robust asset management. By using straightforward S3 storage concepts, we were able to balance recency, coherent file naming, and performance to manage our assets. Being Fast: We were able to distinguish if a frontend build was unnecessary and find a recent build artifact in just under 3 seconds on average. The Results: A 60% Drop in Build Frequency and 50% Drop in Build Time Our efforts paid off, delivering remarkable improvements: 60% Reduction in Build Frequency: By intelligently reusing prebuilt frontend assets, we reduced the number of unnecessary frontend builds by over half. Hundreds of Hours Saved Monthly: Cloud compute time and developer wait times are reduced. Several Terabytes of Storage Savings: We reduced our AWS S3 storage by several terabytes each month. These duplicate assets would have otherwise been stored for one year. 50% Build Time Improvement: This was the second major project by the Frontend DevXP Team and its partnering teams. The first project, which upgraded our Webpack setup, reduced the average build from ~10 minutes to ~5 minutes. This project took the average build from ~5 minutes down to just ~2 minutes. With both projects being successful, we reduced our average build time for E2E pipelines from ~10 minutes to ~2 minutes: a huge improvement for the year! Two unexpected outcomes: More Reliable and Trustworthy E2E Results: Our test flakiness, which refers to tests failing intermittently or inconsistently despite no code changes, was significantly reduced. This improvement resulted from the optimized pipeline, decreased likelihood of needing complex frontend builds, and consistent asset delivery. We observed our lowest percentage of test flakiness as a result, based on monthly measurements. Rediscovering Legacy Code: Implementing this optimization required a deep dive into legacy code of multiple systems that hadn’t been significantly modified in a long time. This exploration yielded valuable insights, prompted new questions about the codebase’s behavior, and generated a backlog of tasks for future enhancements. Conclusion: Rethinking Frontend Build Efficiency By strategically utilizing existing tools like \u003cspan style=\"font-weight: 400\"\u003egit diff\u003c/span\u003e and internal CDNs, we managed to save valuable developer time, reduce cloud costs, and improve overall build efficiency. For teams in other companies facing similar bottlenecks in DevOps and DevXP, the lesson is to question what’s truly necessary in your pipeline and optimize accordingly. The improvement from this project seems obvious in hind-sight, but it’s common to overlook inefficiencies in systems that haven’t outright failed. In our case, rethinking how we handled frontend assets turned into a massive win for the organization. Acknowledgments There are a lot of moving parts in a project like this: complex pipelines for building and testing, cloud infrastructure, an internal CDN, intricate build systems for frontend code, and existing custom setups throughout our entire system. It includes code written in Python, JavaScript, Bash, PHP/Hack, Rust, YAML, and Ruby. We achieved this without any downtime! Okay, almost. There was ten minutes of internal downtime for our deployment pipeline, but it was fixed pretty quickly. This work was not possible without contributions from: Anirudh Janga, Josh Cartmell, Arminé Iradian, Anupama Jasthi, Matt Jennings, Zack Weeden, John Long, Issac Gerges, Andrew MacDonald, Vani Anantha and Dave Harrington Interested in taking on interesting projects, making people’s work lives easier, or just building some pretty cool forms? We’re hiring!",
  "image": "https://slack.engineering/wp-content/uploads/sites/7/2025/04/image2.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\u003cp\u003eIn the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time and resources for engineers at Slack.\u003c/p\u003e\n\u003ch2\u003eThe Problem: Unnecessary Frontend Builds\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eFor one of our largest code repositories (a monolithic repository, or monorepo), Slack has a CI/CD pipeline that runs E2E tests before merging code into the \u003c/span\u003e\u003ccode\u003e\u0026lt;span style=\u0026#34;font-weight: 400\u0026#34;\u0026gt;main\u0026lt;/span\u0026gt;\u003c/code\u003e\u003cspan\u003e branch. This is critical for ensuring that changes are validated across the entire stack for the Slack application: frontend, backend, database, and the handful of services in between. However, we noticed a bottleneck: building the frontend code was taking longer than expected and occurred too frequently, even when there were no frontend-related changes. Here’s the breakdown:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eDeveloper Workflow\u003c/b\u003e\u003cspan\u003e: A developer makes changes and pushes to a branch.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eBuild Process\u003c/b\u003e\u003cspan\u003e: The frontend code is built (~5 minutes).\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eDeployment\u003c/b\u003e\u003cspan\u003e: The build is deployed to a QA environment.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eTesting\u003c/b\u003e\u003cspan\u003e: We run over 200 E2E tests, taking another 5 minutes.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eThis entire process took about 10 minutes per run. Half of that time, around 5 minutes, was consumed by frontend builds, even when no frontend changes were involved.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" width=\"1661\" height=\"780\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png?w=640\" alt=\"Stages of the end to end testing pipeline.\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png 1661w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png?resize=640,301 640w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png?resize=768,361 768w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png?resize=1280,601 1280w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png?resize=1536,721 1536w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png?resize=380,178 380w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png?resize=800,376 800w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png?resize=1160,545 1160w\" sizes=\"(max-width: 1661px) 100vw, 1661px\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eGiven that hundreds of pull requests (PRs) are merged daily, these redundant builds were not only time-consuming, but costly:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eThousands of frontend builds per week\u003c/b\u003e\u003cspan\u003e, storing nearly a gigabyte of data per build in AWS S3.\u003c/span\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003eHalf of these builds do not contain frontend changes compared to the last merge into \u003c/span\u003e\u003ccode\u003e\u0026lt;span style=\u0026#34;font-weight: 400\u0026#34;\u0026gt;main\u0026lt;/span\u0026gt;\u003c/code\u003e\u003cspan\u003e, causing terabytes of duplicate data.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cb\u003e5 minutes\u003c/b\u003e\u003cspan\u003e per build, adding unnecessary delays to pipelines (thousands of hours a week).\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cb\u003eThe Solution: Smarter Build Strategy with Cached Frontend Assets\u003c/b\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eTo tackle this, we leveraged existing tools to rethink our build strategy.\u003c/span\u003e\u003c/p\u003e\n\u003ch4\u003eStep 1: Conditional Frontend Builds\u003c/h4\u003e\n\u003cp\u003e\u003cspan\u003eOur first step was determining whether a fresh frontend build was necessary. We detected changes by utilizing \u003c/span\u003e\u003ccode\u003e\u0026lt;span style=\u0026#34;font-weight: 400\u0026#34;\u0026gt;git diff\u0026lt;/span\u0026gt;\u003c/code\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-comparing-branches-in-pull-requests#three-dot-and-two-dot-git-diff-comparisons\"\u003e\u003cspan\u003eits 3-dot notation\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to identify the difference between the latest common commit of the current checked-out branch and \u003c/span\u003e\u003ccode\u003e\u0026lt;span style=\u0026#34;font-weight: 400\u0026#34;\u0026gt;main\u0026lt;/span\u0026gt;\u003c/code\u003e\u003cspan\u003e. If changes were detected, we invoke a frontend build job. If no changes were detected, we skipped the build entirely and reused a prebuilt version. \u003c/span\u003e\u003c/p\u003e\n\u003ch4\u003eStep 2: Prebuilt Assets and Internal CDN\u003c/h4\u003e\n\u003cp\u003e\u003cspan\u003eWhen a frontend build is not needed, we locate an existing build from AWS S3. To be efficient, we use a recent frontend build that is still in Production. We delegate the task of serving the prebuilt frontend assets for our E2E tests to an internal CDN. \u003c/span\u003e\u003cspan\u003eThis reduced the need for creating a new build on each PR, while still ensuring we test on current assets.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cb\u003eThe Challenges: Efficiency at Scale\u003c/b\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWhile the approach seemed straightforward, scaling this solution to our monorepo presented a few challenges:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eIdentifying Frontend Changes\u003c/b\u003e\u003cspan\u003e: Our repository contains over 100,000 tracked files. Determining whether frontend changes were present required efficient file tracking, which \u003c/span\u003e\u003cspan\u003egit\u003c/span\u003e\u003cspan\u003e handled in just a couple of seconds. \u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eFinding Prebuilt Assets\u003c/b\u003e\u003cspan\u003e: With hundreds of PRs merged into this repository daily, identifying a prebuilt version that was fresh enough required robust asset management. By using straightforward S3 storage concepts, we were able to balance recency, coherent file naming, and performance to manage our assets.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eBeing Fast: \u003c/b\u003e\u003cspan\u003eWe were able to distinguish if a frontend build was unnecessary \u003c/span\u003e\u003ci\u003e\u003cspan\u003eand\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e find a recent build artifact in just under 3 seconds on average.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" width=\"1661\" height=\"780\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png?w=640\" alt=\"End to end pipeline depicting local pre-built frontends at the third stage.\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png 1661w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png?resize=640,301 640w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png?resize=768,361 768w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png?resize=1280,601 1280w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png?resize=1536,721 1536w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png?resize=380,178 380w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png?resize=800,376 800w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png?resize=1160,545 1160w\" sizes=\"(max-width: 1661px) 100vw, 1661px\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cb\u003eThe Results: A 60% Drop in Build Frequency and 50% Drop in Build Time\u003c/b\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eOur efforts paid off, delivering remarkable improvements:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003e60% Reduction in Build Frequency\u003c/b\u003e\u003cspan\u003e: By intelligently reusing prebuilt frontend assets, we reduced the number of unnecessary frontend builds by over half.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eHundreds of Hours Saved Monthly\u003c/b\u003e\u003cspan\u003e: Cloud compute time and developer wait times are reduced.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eSeveral Terabytes of Storage Savings\u003c/b\u003e\u003cspan\u003e: We reduced our AWS S3 storage by several terabytes each month. These duplicate assets would have otherwise been stored for one year.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003e50% Build Time Improvement\u003c/b\u003e\u003cspan\u003e: This was the second major project by the Frontend DevXP Team and its partnering teams. The first project, which upgraded our Webpack setup, reduced the average build from ~10 minutes to ~5 minutes. This project took the average build from ~5 minutes down to just ~2 minutes. With both projects being successful, we reduced our average build time for E2E pipelines from ~10 minutes to ~2 minutes: a \u003c/span\u003e\u003cb\u003ehuge improvement\u003c/b\u003e\u003cspan\u003e for the year!\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1658\" height=\"932\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png?w=640\" alt=\"January baseline was an average build time of 10 minutes and 6 minutes for testing. By August average build time was 2 minutes, an 80% decrease for the year.\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png 1658w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png?resize=640,360 640w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png?resize=768,432 768w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png?resize=1280,720 1280w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png?resize=1536,863 1536w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png?resize=380,214 380w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png?resize=800,450 800w, https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png?resize=1160,652 1160w\" sizes=\"auto, (max-width: 1658px) 100vw, 1658px\"/\u003e\u003c/p\u003e\n\u003ch3\u003eTwo unexpected outcomes:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eMore Reliable and Trustworthy E2E Results\u003c/b\u003e\u003cspan\u003e: Our test flakiness, which refers to tests failing intermittently or inconsistently despite no code changes, was significantly reduced. This improvement resulted from the optimized pipeline, decreased likelihood of needing complex frontend builds, and consistent asset delivery. We observed our lowest percentage of test flakiness as a result, based on monthly measurements.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eRediscovering Legacy Code\u003c/b\u003e\u003cspan\u003e: Implementing this optimization required a deep dive into legacy code of multiple systems that hadn’t been significantly modified in a long time. This exploration yielded valuable insights, prompted new questions about the codebase’s behavior, and generated a backlog of tasks for future enhancements.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cb\u003eConclusion: Rethinking Frontend Build Efficiency\u003c/b\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eBy strategically utilizing existing tools like \u003c/span\u003e\u003ccode\u003e\u0026lt;span style=\u0026#34;font-weight: 400\u0026#34;\u0026gt;git diff\u0026lt;/span\u0026gt;\u003c/code\u003e\u003cspan\u003e and internal CDNs, we managed to save valuable developer time, reduce cloud costs, and improve overall build efficiency.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor teams in other companies facing similar bottlenecks in DevOps and DevXP, the lesson is to question what’s truly necessary in your pipeline and optimize accordingly. The improvement from this project seems obvious in hind-sight, but it’s common to overlook inefficiencies in systems that haven’t outright failed. In our case, rethinking how we handled frontend assets turned into a massive win for the organization.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cb\u003eAcknowledgments\u003c/b\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eThere are a lot of moving parts in a project like this: complex pipelines for building and testing, cloud infrastructure, an internal CDN, intricate build systems for frontend code, and existing custom setups throughout our entire system. It includes code written in Python, JavaScript, Bash, PHP/Hack, Rust, YAML, and Ruby. We achieved this without any downtime! Okay, almost. There was ten minutes of internal downtime for our deployment pipeline, but it was fixed pretty quickly.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis work was not possible without contributions from:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAnirudh Janga, Josh Cartmell, Arminé Iradian, Anupama Jasthi, Matt Jennings, Zack Weeden, John Long, Issac Gerges, Andrew MacDonald, Vani Anantha and Dave Harrington\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003eInterested in taking on interesting projects, making people’s work lives easier, or just building some pretty cool forms? We’re hiring! \u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\t\t\n\t\n\n\n\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-04-14T09:00:30Z",
  "modifiedTime": "2025-04-14T14:22:38Z"
}
