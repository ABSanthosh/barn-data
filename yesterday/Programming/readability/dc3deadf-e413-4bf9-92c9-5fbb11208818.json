{
  "id": "dc3deadf-e413-4bf9-92c9-5fbb11208818",
  "title": "Llama 4 Scout and Maverick Now Available on Amazon Bedrock and SageMaker JumpStart",
  "link": "https://www.infoq.com/news/2025/05/llama-4-aws-bedrock-sagemaker/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "AWS recently announced the availability of Meta's latest foundation models, Llama 4 Scout and Llama 4 Maverick, in Amazon Bedrock and AWS SageMaker JumpStart. Both models provide multimodal capabilities and follow the mixture-of-experts architecture. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Sun, 18 May 2025 16:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "AWS",
    "Cloud",
    "Large language models",
    "Facebook",
    "Development",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 2544,
  "excerpt": "AWS recently announced the availability of Meta's latest foundation models, Llama 4 Scout and Llama 4 Maverick, in Amazon Bedrock and AWS SageMaker JumpStart. Both models provide multimodal capabiliti",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250513062617/apple-touch-icon.png",
  "text": "AWS recently announced the availability of Meta's latest foundation models, Llama 4 Scout and Llama 4 Maverick, in Amazon Bedrock and AWS SageMaker JumpStart. Both models provide multimodal capabilities and follow the mixture-of-experts architecture. Launched by Meta last April, Llama 4 Scout and Maverick include 17 billion active parameters distributed across 16 and 128 experts, respectively. Llama 4 Scout is optimized to run on a single NVIDIA H100 GPU for general-purpose tasks. According to Meta, Llama 4 Maverick provides enhanced reasoning and coding capabilities and outperforms other models in its class. Amazon highlights the value of the mixture-of-experts architecture in reducing compute costs, making advanced AI more accessible and cost-effective: Thanks to their more efficient mixture of experts (MoE) architecture—a first for Meta—that activates only the most relevant parts of the model for each task, customers can benefit from these powerful capabilities that are more compute efficient for model training and inference, translating into lower costs at greater performance. While Llama 4 Scout supports a context window of up to 10 million tokens, Amazon Bedrock currently allows up to 3.5 million tokens, but plans to expand it shortly. Llama 4 Maverick supports a maximum of one million tokens. In both cases, these represent a significant increase over the 128K context window available for Llama 3 models. On Amazon SageMaker JumpStart, you can use the new models with SageMaker Studio or the Amazon SageMaker Python SDK depending on your use case. Both models default to a ml.p5.48xlarge instance, which features NVIDIA H100 Tensor Core GPUs. Alternatively, you can choose a ml.p5en.48xlarge instance powered by NVIDIA H200 Tensor Core GPUs. Llama 4 Scout also supports the ml.g6e.48xlarge instance type, which uses NVIDIA L40S Tensor Core GPUs. Llama 4 models are available on several other cloud providers, including Databricks, GroqCloud, Lambda.ai, Cerebras Inference Cloud, and others. Additionally, you can access them on Hugging Face. In addition to Scout and Maverick, Behemoth is the third model in the Llama 4 family, featuring 288 billion active parameters distributed across 16 experts. Meta describes Behemoth, currently in preview, as the most intelligent teacher model for distillation, having used it to train both Scout and Maverick. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/05/llama-4-aws-bedrock-sagemaker/en/headerimage/llama-4-aws-bedrock-1747583695819.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eAWS recently announced the availability of Meta\u0026#39;s latest foundation models, \u003ca href=\"https://www.aboutamazon.com/news/aws/aws-meta-llama-4-models-available\"\u003eLlama 4 Scout and Llama 4 Maverick, in Amazon Bedrock and AWS SageMaker JumpStart\u003c/a\u003e. Both models provide multimodal capabilities and follow the mixture-of-experts architecture.\u003c/p\u003e\n\n\u003cp\u003eLaunched by Meta last April, Llama 4 Scout and Maverick include 17 billion active parameters distributed across 16 and 128 experts, respectively. Llama 4 Scout is optimized to run on a single NVIDIA H100 GPU for general-purpose tasks. According to Meta, Llama 4 Maverick provides enhanced reasoning and coding capabilities and outperforms other models in its class. Amazon highlights the value of the \u003ca href=\"https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/\"\u003emixture-of-experts architecture in reducing compute costs\u003c/a\u003e, making advanced AI more accessible and cost-effective:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThanks to their more efficient mixture of experts (MoE) architecture—a first for Meta—that activates only the most relevant parts of the model for each task, customers can benefit from these powerful capabilities that are more compute efficient for model training and inference, translating into lower costs at greater performance.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWhile Llama 4 Scout supports a context window of up to 10 million tokens, Amazon Bedrock currently allows up to 3.5 million tokens, but plans to expand it shortly. Llama 4 Maverick supports a maximum of one million tokens. In both cases, these represent a significant increase over the 128K context window available for Llama 3 models.\u003c/p\u003e\n\n\u003cp\u003eOn Amazon SageMaker JumpStart, \u003ca href=\"https://aws.amazon.com/blogs/machine-learning/llama-4-family-of-models-from-meta-are-now-available-in-sagemaker-jumpstart/\"\u003eyou can use the new models\u003c/a\u003e with SageMaker Studio or the Amazon SageMaker Python SDK depending on your use case. Both models default to a \u003ca href=\"https://aws.amazon.com/ec2/instance-types/p5/\"\u003e\u003ccode\u003eml.p5.48xlarge\u003c/code\u003e\u003c/a\u003e instance, which features NVIDIA H100 Tensor Core GPUs. Alternatively, you can choose a \u003ccode\u003eml.p5en.48xlarge\u003c/code\u003e instance powered by NVIDIA H200 Tensor Core GPUs. Llama 4 Scout also supports the \u003ccode\u003eml.g6e.48xlarge\u003c/code\u003e instance type, which uses NVIDIA L40S Tensor Core GPUs.\u003c/p\u003e\n\n\u003cp\u003eLlama 4 models are available on several other cloud providers, including Databricks, GroqCloud, Lambda.ai, Cerebras Inference Cloud, and others. Additionally, you can \u003ca href=\"https://huggingface.co/blog/llama4-release\"\u003eaccess them on Hugging Face\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eIn addition to Scout and Maverick, Behemoth is the third model in the Llama 4 family, featuring 288 billion active parameters distributed across 16 experts. Meta describes Behemoth, currently in preview, as the most intelligent teacher model for distillation, having used it to train both Scout and Maverick.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2025-05-18T00:00:00Z",
  "modifiedTime": null
}
