{
  "id": "83f27d30-95d4-492d-a866-c64bd90e36f8",
  "title": "LinkedIn Announces Northguard and Xinfra: Scaling Beyond Kafka for Log Storage and Pub/Sub",
  "link": "https://www.infoq.com/news/2025/06/linkedin-northguard-xinfra/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "LinkedIn today announced Northguard, a scalable log storage system that replaces Kafka, and Xinfra, a virtualized Pub/Sub layer. Northguard delivers sharded data \u0026 metadata, log striping, strong consistency, and self-balancing clusters at a larger scale than Kafka, while Xinfra enables seamless migration and unified access across Kafka and Northguard. By Eran Stiller",
  "author": "Eran Stiller",
  "published": "Wed, 25 Jun 2025 13:06:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Cloud Architecture",
    "LinkedIn",
    "Northguard",
    "Apache Kafka",
    "Xinfra",
    "Architecture \u0026 Design",
    "Development",
    "news"
  ],
  "byline": "Eran Stiller",
  "length": 5510,
  "excerpt": "LinkedIn today announced Northguard, a scalable log storage system that replaces Kafka, and Xinfra, a virtualized Pub/Sub layer. Northguard delivers sharded data \u0026 metadata, log striping, strong consi",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250605075544/apple-touch-icon.png",
  "text": "LinkedIn today announced Northguard, a scalable log storage system that replaces Kafka, and Xinfra, a virtualized Pub/Sub layer. Northguard delivers sharded data \u0026 metadata, log striping, strong consistency, and self-balancing clusters at a larger scale than Kafka, while Xinfra enables seamless migration and unified access across Kafka and Northguard. According to LinkedIn's engineers, Kafka had become increasingly difficult to manage at LinkedIn's scale (32T records/day, 17 PB/day, 400K topics, 150 clusters). Northguard's architecture, consisting of shared data \u0026 metadata, decentralized coordination, and minimal global state, eliminates Kafka's single-controller and partition-based limitations. Northguard's data model organizes logs into records, segments, ranges, and topics. Records (key, value, headers) are written into segments — immutable units of replication. Segments form ranges, which represent a contiguous keyspace slice and support dynamic splitting and merging for scaling and ordering. Topics, on the other hand, are collections of ranges that cover the entire keyspace, with flexible storage policies for replication and retention. A Northguard Range containing three segments (source) This fine-grained structure enables a balanced load, high availability, and seamless scaling. Brokers naturally self-balance as producers produce new segments, eliminating the need for expensive rebalancing or data movement when brokers are added or fail. New segments get added to the range and get assigned to potentially new brokers (source) Compared to traditional indexed partitions, ranges provide a more flexible scaling mechanism. Range splits only interrupt clients' writing to the affected range (not the entire topic) while maintaining total ordering guarantees. This difference reduces disruption and avoids costly \"stop-the-world\" synchronization. Additionally, aligned ranges across topics simplify stream processing joins by reducing the need for shuffles. Northguard's metadata model uses sharded, Raft-backed replicated state machines (DS-RSM), distributed across vnodes. Each vnode manages metadata for topics, ranges, and segments — tracking state changes (e.g., splits, merges, sealing), replica sets, and retention policies. By sharding metadata via consistent hashing and using decentralized coordination, Northguard avoids Kafka's controller bottlenecks and supports millions of replicas with strong consistency and high availability. LinkedIn optimized Northguard's protocols for performance and durability. Metadata operations such as create, delete, and query use unary request/response calls routed to vnode leaders. Produce, consume, and replication flows are sessionized streaming protocols with pipelining and windowing to maximize throughput and minimize latency. Producers write to active segment leaders, receiving acknowledgments only after fsync on all replicas, ensuring strong durability. Consumers use a similar streaming model with client-controlled flow, supporting efficient high-throughput reads. Active segment replication and sealed segment replication utilize the same efficient streaming architecture, resulting in Northguard's high performance and self-healing capabilities. Migrating from Kafka to Northguard at LinkedIn's scale required seamless, zero-downtime transitions for thousands of mission-critical apps. To support this, LinkedIn built Xinfra, a virtualized Pub/Sub layer that abstracts away physical clusters. Xinfra enables topics to span Kafka and Northguard through dual-write mechanisms, allowing live migrations without client changes. An example use case where a consumer subscribes to three topics under the same virtual cluster, with each topic located in different clusters (source) Segment storage in Northguard is pluggable, with the default \"fps-store\" implementation optimized for durability and latency. It uses a write-ahead log (WAL), creates one file per segment, applies Direct I/O to bypass OS buffering, and maintains a sparse index in RocksDB. Batches of records are flushed and fsynced across replicas within milliseconds, ensuring durability even under failure scenarios. This design avoids cache inconsistencies, supports efficient reads from old segments, and allows predictable performance as clusters grow. To ensure reliability at scale, Northguard undergoes rigorous testing under deterministic simulation. Entire clusters and clients run in a single-threaded, controlled environment where faults — such as broker shutdowns, network partitions, disk errors, and rolling upgrades — are injected and replayed. This method allows LinkedIn to simulate years of activity daily, catch edge cases early, and continuously validate correctness under complex failure scenarios. LinkedIn engineers state that they \"have successfully migrated thousands of topics from Kafka to Northguard, accounting for trillions of records per day\" and that over 90% of applications at LinkedIn are already running Xinfra clients. InfoQ reached out to LinkedIn regarding intentions to open-source Northguard and Xinfra. LinkedIn stated that they \"are focused on finalizing the implementation of Northguard and Xinfra within our internal systems, and as we continue to build, learn, and iterate on these tools, we'll explore the possibilities of open-sourcing them.\" About the Author Eran Stiller",
  "image": "https://res.infoq.com/news/2025/06/linkedin-northguard-xinfra/en/headerimage/LinkedIn-Northguard-Xinfra-Header-1750758516876.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eLinkedIn today announced \u003ca href=\"https://www.linkedin.com/blog/engineering/infrastructure/introducing-northguard-and-xinfra\" target=\"_blank\"\u003eNorthguard, a scalable log storage system that replaces Kafka, and Xinfra, a virtualized Pub/Sub layer\u003c/a\u003e. Northguard delivers sharded data \u0026amp; metadata, log striping, strong consistency, and self-balancing clusters at a larger scale than Kafka, while Xinfra enables seamless migration and unified access across Kafka and Northguard.\u003c/p\u003e\n\n\u003cp\u003eAccording to LinkedIn\u0026#39;s engineers, \u003ca href=\"https://kafka.apache.org/\" target=\"_blank\"\u003eKafka\u003c/a\u003e had become increasingly difficult to manage at LinkedIn\u0026#39;s scale (32T records/day, 17 PB/day, 400K topics, 150 clusters). Northguard\u0026#39;s architecture, consisting of shared data \u0026amp; metadata, decentralized coordination, and minimal global state, eliminates Kafka\u0026#39;s single-controller and partition-based limitations.\u003c/p\u003e\n\n\u003cp\u003eNorthguard\u0026#39;s data model organizes logs into \u003cem\u003erecords\u003c/em\u003e, \u003cem\u003esegments\u003c/em\u003e, \u003cem\u003eranges\u003c/em\u003e, and \u003cem\u003etopics\u003c/em\u003e. Records (key, value, headers) are written into segments — immutable units of replication. Segments form ranges, which represent a contiguous keyspace slice and support dynamic splitting and merging for scaling and ordering. Topics, on the other hand, are collections of ranges that cover the entire keyspace, with flexible storage policies for replication and retention.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/06/linkedin-northguard-xinfra/en/resources/1LinkedIn-Northguard-Range-1750758765365.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/06/linkedin-northguard-xinfra/en/resources/1LinkedIn-Northguard-Range-1750758765365.png\" rel=\"share\"/\u003e\u003cbr/\u003e\n\u003cem\u003eA Northguard Range containing three segments (\u003ca href=\"https://www.linkedin.com/blog/engineering/infrastructure/introducing-northguard-and-xinfra\" target=\"_blank\"\u003esource\u003c/a\u003e)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThis fine-grained structure enables a balanced load, high availability, and seamless scaling. Brokers naturally self-balance as producers produce new segments, eliminating the need for expensive rebalancing or data movement when brokers are added or fail.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/06/linkedin-northguard-xinfra/en/resources/1LinkedIn-Northguard-Segment-Balancing-1750758765365.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/06/linkedin-northguard-xinfra/en/resources/1LinkedIn-Northguard-Segment-Balancing-1750758765365.png\" rel=\"share\"/\u003e\u003cbr/\u003e\n\u003cem\u003eNew segments get added to the range and get assigned to potentially new brokers (\u003ca href=\"https://www.linkedin.com/blog/engineering/infrastructure/introducing-northguard-and-xinfra\" target=\"_blank\"\u003esource\u003c/a\u003e)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eCompared to traditional \u003ca href=\"https://learn.conduktor.io/kafka/kafka-topics-internals-segments-and-indexes/\" target=\"_blank\"\u003eindexed partitions\u003c/a\u003e, ranges provide a more flexible scaling mechanism. Range splits only interrupt clients\u0026#39; writing to the affected range (not the entire topic) while maintaining total ordering guarantees. This difference reduces disruption and avoids costly \u0026#34;stop-the-world\u0026#34; synchronization. Additionally, aligned ranges across topics simplify stream processing joins by reducing the need for shuffles.\u003c/p\u003e\n\n\u003cp\u003eNorthguard\u0026#39;s metadata model uses sharded, \u003ca href=\"https://en.wikipedia.org/wiki/Raft_(algorithm)\" target=\"_blank\"\u003eRaft\u003c/a\u003e-backed \u003ca href=\"https://www.geeksforgeeks.org/system-design/replicated-state-machines-in-distributed-systems/\" target=\"_blank\"\u003ereplicated state machines\u003c/a\u003e (DS-RSM), distributed across vnodes. Each vnode manages metadata for topics, ranges, and segments — tracking state changes (e.g., splits, merges, sealing), replica sets, and retention policies. By sharding metadata via consistent hashing and using decentralized coordination, Northguard avoids Kafka\u0026#39;s controller bottlenecks and supports millions of replicas with strong consistency and high availability.\u003c/p\u003e\n\n\u003cp\u003eLinkedIn optimized Northguard\u0026#39;s protocols for performance and durability. Metadata operations such as create, delete, and query use unary request/response calls routed to vnode leaders. Produce, consume, and replication flows are sessionized streaming protocols with pipelining and windowing to maximize throughput and minimize latency.\u003c/p\u003e\n\n\u003cp\u003eProducers write to active segment leaders, receiving acknowledgments only after \u003ca href=\"https://pubs.opengroup.org/onlinepubs/009695399/functions/fsync.html\" target=\"_blank\"\u003efsync\u003c/a\u003e on all replicas, ensuring strong durability. Consumers use a similar streaming model with client-controlled flow, supporting efficient high-throughput reads. Active segment replication and sealed segment replication utilize the same efficient streaming architecture, resulting in Northguard\u0026#39;s high performance and self-healing capabilities.\u003c/p\u003e\n\n\u003cp\u003eMigrating from Kafka to Northguard at LinkedIn\u0026#39;s scale required seamless, zero-downtime transitions for thousands of mission-critical apps. To support this, LinkedIn built Xinfra, a virtualized Pub/Sub layer that abstracts away physical clusters. Xinfra enables topics to span Kafka and Northguard through dual-write mechanisms, allowing live migrations without client changes.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/06/linkedin-northguard-xinfra/en/resources/1LinkedIn-Xinfra-Virtual-Cluster-1750758765365.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/06/linkedin-northguard-xinfra/en/resources/1LinkedIn-Xinfra-Virtual-Cluster-1750758765365.png\" rel=\"share\"/\u003e\u003cbr/\u003e\n\u003cem\u003eAn example use case where a consumer subscribes to three topics under the same virtual cluster, with each topic located in different clusters (\u003ca href=\"https://www.linkedin.com/blog/engineering/infrastructure/introducing-northguard-and-xinfra\" target=\"_blank\"\u003esource\u003c/a\u003e)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eSegment storage in Northguard is pluggable, with the default \u0026#34;fps-store\u0026#34; implementation optimized for durability and latency. It uses a \u003ca href=\"https://en.wikipedia.org/wiki/Write-ahead_logging\" target=\"_blank\"\u003ewrite-ahead log\u003c/a\u003e (WAL), creates one file per segment, applies Direct I/O to bypass OS buffering, and maintains a sparse index in \u003ca href=\"https://rocksdb.org/\" target=\"_blank\"\u003eRocksDB\u003c/a\u003e. Batches of records are flushed and fsynced across replicas within milliseconds, ensuring durability even under failure scenarios. This design avoids cache inconsistencies, supports efficient reads from old segments, and allows predictable performance as clusters grow.\u003c/p\u003e\n\n\u003cp\u003eTo ensure reliability at scale, Northguard undergoes rigorous testing under deterministic simulation. Entire clusters and clients run in a single-threaded, controlled environment where faults — such as broker shutdowns, network partitions, disk errors, and rolling upgrades — are injected and replayed. This method allows LinkedIn to simulate years of activity daily, catch edge cases early, and continuously validate correctness under complex failure scenarios.\u003c/p\u003e\n\n\u003cp\u003eLinkedIn engineers state that they \u0026#34;have successfully migrated thousands of topics from Kafka to Northguard, accounting for trillions of records per day\u0026#34; and that over 90% of applications at LinkedIn are already running Xinfra clients.\u003c/p\u003e\n\n\u003cp\u003eInfoQ reached out to LinkedIn regarding intentions to open-source Northguard and Xinfra. LinkedIn stated that they \u0026#34;are focused on finalizing the implementation of Northguard and Xinfra within our internal systems, and as we continue to build, learn, and iterate on these tools, we\u0026#39;ll explore the possibilities of open-sourcing them.\u0026#34;\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Eran-Stiller\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eEran Stiller\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-06-25T00:00:00Z",
  "modifiedTime": null
}
