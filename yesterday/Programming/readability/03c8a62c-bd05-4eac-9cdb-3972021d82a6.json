{
  "id": "03c8a62c-bd05-4eac-9cdb-3972021d82a6",
  "title": "Prime Intellect Releases INTELLECT-2: A 32B Parameter Model Trained via Decentralized Reinforcement",
  "link": "https://www.infoq.com/news/2025/05/prime-intellect-2/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Prime Intellect has released INTELLECT-2, a 32 billion parameter language model trained using fully asynchronous reinforcement learning across a decentralized network of compute contributors. Unlike traditional centralized model training, INTELLECT-2 is developed on a permissionless infrastructure where rollout generation, policy updates, and training are distributed and loosely coupled. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Wed, 21 May 2025 17:45:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Large language models",
    "Reinforcement Learning",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 3223,
  "excerpt": "Prime Intellect has released INTELLECT-2, a 32 billion parameter language model trained using fully asynchronous reinforcement learning across a decentralized network of compute contributors. Unlike t",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250513062617/apple-touch-icon.png",
  "text": "Prime Intellect has released INTELLECT-2, a 32 billion parameter language model trained using fully asynchronous reinforcement learning across a decentralized network of compute contributors. Unlike traditional centralized model training, INTELLECT-2 is developed on a permissionless infrastructure where rollout generation, policy updates, and training are distributed and loosely coupled. The system is built around PRIME-RL, a new training framework designed for asynchronous RL in untrusted environments. It separates the tasks of generating rollouts, updating models, and broadcasting weights. Policy updates are handled by SHARDCAST, a component that distributes model weights using a tree-based HTTP network. Inference rollouts submitted by workers are verified through TOPLOC, a locality-sensitive hashing mechanism that detects tampering or numerical discrepancies before allowing the results to influence training. Source: https://arxiv.org/html/2505.07291v1 INTELLECT-2 was trained on 285,000 math and coding tasks sourced from datasets such as NuminaMath-1.5 or SYNTHETIC-1. The reward signal combines binary task success with token-length penalties or bonuses, allowing fine-grained control over inference-time compute budgets. Training stability was supported by techniques such as two-sided GRPO clipping, gradient norm management, and both offline and online filtering of high-value tasks. The asynchronous training process overlaps inference, communication, and model updates, avoiding typical bottlenecks found in centralized RL systems. A Rust-based orchestrator running on a testnet coordinates the global pool of contributors, handling hardware checks, heartbeats, task assignments, and contribution tracking—operating similarly to peer-to-peer or blockchain-based systems. Performance evaluations showed improvements on targeted math and programming tasks, particularly over QwQ-32B, a previous RL-trained model. Broader benchmark improvements were more modest, suggesting gains were mostly confined to training data domains. Prime Intellect noted that improvements might be more significant using stronger base models, such as Qwen3, or by integrating more complex environments and reasoning tools. One Reddit user remarked on the broader implications: Distributed training and distributed inference seem like the way to go. Maybe something similar to P2P or blockchain with some kind of rewards for computational contributions/transactions. Not necessarily yet another cryptocurrency, but maybe credits that can be used for free computing on the network. Future work includes increasing the inference-to-training compute ratio, enabling multi-turn reasoning with integrated tools like web search or Python, crowdsourcing RL tasks, and experimenting with decentralized model merging methods such as DiLoCo. The model, code, training framework, and documentation are publicly available on the Prime Intellect website. Additional tools and interfaces, including a Hugging Face release and a chat demo, are also publicly accessible. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2025/05/prime-intellect-2/en/headerimage/generatedHeaderImage-1747848572897.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003ePrime Intellect has released \u003ca href=\"https://www.primeintellect.ai/blog/intellect-2-release\"\u003eINTELLECT-2\u003c/a\u003e, a 32 billion parameter language model trained using fully asynchronous reinforcement learning across a decentralized network of compute contributors. Unlike traditional centralized model training, INTELLECT-2 is developed on a permissionless infrastructure where rollout generation, policy updates, and training are distributed and loosely coupled.\u003c/p\u003e\n\n\u003cp\u003eThe system is built around \u003ca href=\"https://github.com/PRIME-RL/PRIME\"\u003ePRIME-RL\u003c/a\u003e, a new training framework designed for asynchronous RL in untrusted environments. It separates the tasks of generating rollouts, updating models, and broadcasting weights. Policy updates are handled by SHARDCAST, a component that distributes model weights using a tree-based HTTP network. Inference rollouts submitted by workers are verified through TOPLOC, a locality-sensitive hashing mechanism that detects tampering or numerical discrepancies before allowing the results to influence training.\u003c/p\u003e\n\n\u003cp\u003e\u003cmeta charset=\"utf-8\"/\u003e\u003cb id=\"docs-internal-guid-746a8c0f-7fff-5d82-7c43-a5a185fbe3a8\"\u003e\u003cimg alt=\"architecture\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXc5Srp9bIgdmLdnNBXPRpccRjtu3k8K4-ksqe687w7QP9TsHZJdg1gZYn7TX-DHqjTcqETmBWCqmtvqBcNGK_5oNuhawcWPj3e5w_UUMTOFI0ZowLR2BZ0bCmZn0I2eRMT3A3K2?key=bY-9jbHCmyFdSt8xIdWxug\" rel=\"share\"/\u003e\u003c/b\u003e\u003cbr/\u003e\n\u003cem\u003eSource: https://arxiv.org/html/2505.07291v1\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eINTELLECT-2 was trained on 285,000 math and coding tasks sourced from datasets such as \u003ca href=\"https://huggingface.co/datasets/AI-MO/NuminaMath-1.5\"\u003eNuminaMath-1.5\u003c/a\u003e or \u003ca href=\"https://huggingface.co/collections/PrimeIntellect/synthetic-1-67a2c399cfdd6c9f7fae0c37\"\u003eSYNTHETIC-1\u003c/a\u003e. The reward signal combines binary task success with token-length penalties or bonuses, allowing fine-grained control over inference-time compute budgets. Training stability was supported by techniques such as two-sided \u003ca href=\"https://huggingface.co/docs/trl/en/grpo_trainer\"\u003eGRPO\u003c/a\u003e clipping, gradient norm management, and both offline and online filtering of high-value tasks.\u003c/p\u003e\n\n\u003cp\u003eThe asynchronous training process overlaps inference, communication, and model updates, avoiding typical bottlenecks found in centralized RL systems. A Rust-based orchestrator running on a testnet coordinates the global pool of contributors, handling hardware checks, heartbeats, task assignments, and contribution tracking—operating similarly to peer-to-peer or blockchain-based systems.\u003c/p\u003e\n\n\u003cp\u003ePerformance evaluations showed improvements on targeted math and programming tasks, particularly over \u003ca href=\"https://huggingface.co/Qwen/QwQ-32B\"\u003eQwQ-32B\u003c/a\u003e, a previous RL-trained model. Broader benchmark improvements were more modest, suggesting gains were mostly confined to training data domains. Prime Intellect noted that improvements might be more significant using stronger base models, such as \u003ca href=\"https://github.com/QwenLM/Qwen3\"\u003eQwen3\u003c/a\u003e, or by integrating more complex environments and reasoning tools.\u003c/p\u003e\n\n\u003cp\u003eOne Reddit user \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kkgzip/comment/mruj9gb/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button\"\u003eremarked\u003c/a\u003e on the broader implications:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eDistributed training and distributed inference seem like the way to go. Maybe something similar to P2P or blockchain with some kind of rewards for computational contributions/transactions. Not necessarily yet another cryptocurrency, but maybe credits that can be used for free computing on the network.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eFuture work includes increasing the inference-to-training compute ratio, enabling multi-turn reasoning with integrated tools like web search or Python, crowdsourcing RL tasks, and experimenting with decentralized model merging methods such as \u003ca href=\"https://arxiv.org/abs/2311.08105\"\u003eDiLoCo\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe model, code, training framework, and documentation are publicly available on the \u003ca href=\"http://primeintellect.ai/intellect-2\"\u003ePrime Intellect website\u003c/a\u003e. Additional tools and interfaces, including a Hugging Face release and a chat demo, are also publicly accessible.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-05-21T00:00:00Z",
  "modifiedTime": null
}
