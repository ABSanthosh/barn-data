{
  "id": "04a18fef-d8f0-4669-b9a9-8e36506339a3",
  "title": "Arm Scalable Matrix Extension 2 Coming to Android to Accelerate On-Device AI",
  "link": "https://www.infoq.com/news/2025/07/arm-sme2-android/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Available in the Armv9-A architecture, Arm Scalable Matrix Extension 2 (SME2) is a set of advanced CPU instructions designed to accelerate matrix heavy computation. The new Arm technology aims to help mobile developers to run advanced AI models directly on CPU with improved performance and efficiency, without requiring any changes to their apps. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Sun, 13 Jul 2025 09:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Android",
    "ARM",
    "iOS",
    "Artificial Intelligence",
    "Open Source",
    "Machine Learning",
    "Mobile",
    "AI, ML \u0026 Data Engineering",
    "Development",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3072,
  "excerpt": "Available in the Armv9-A architecture, Arm Scalable Matrix Extension 2 (SME2) is a set of advanced CPU instructions designed to accelerate matrix heavy computation. The new Arm technology aims to help",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250605075448/apple-touch-icon.png",
  "text": "Available in the Armv9-A architecture, Arm Scalable Matrix Extension 2 (SME2) is a set of advanced CPU instructions designed to accelerate matrix heavy computation. The new Arm technology aims to help mobile developers to run advanced AI models directly on CPU with improved performance and efficiency, without requiring any changes to their apps. SME2 builds on the previously available SME extension, which introduced matrix operations and streaming vectors, by adding acceleration and support for multi-vector data-processing instructions, load to and store from multi-vectors, and a multi-vector predication mechanism. While the performance benefits of SME2 are already available on the latest iOS devices and Apple M4-series chips, they will soon reach Android devices as well, says Alex Spinelli, Arm’s VP of AI and Developer Platforms and Services. Matrix workflows are key for real-time mobile inference tasks such as image and language processing and voice generation. In particular, comparisons between SME2-enabled and non-SME2-enabled workflows shows a significant improvement, says Arm: On SME2-enabled hardware, Google’s Gemma 3 model delivers 6x faster chat responses, and can start summarizing up to 800 words in under a second on a single CPU core. Likewise, a 2.6x speed up has been measured for prompt processing on a vivo X200 Pro flagship smartphone running a 3.8B parameter Phi-3 Mini model. To help developers take advantage of SME2, Arm provides a library called KleidiAI, which is integrated in Google’s XNNPACK. XNNPACK powers several machine learning and AI frameworks, including Alibaba’s MNN, Google’s LiteRT, Microsoft’s ONNX Runtime, and llama.cpp. When SME2 is enabled and compatible, XNNPACK automatically routes the matrix heavy operations to SME2 via KleidiAI, so developers directly benefit with no changes needed in application logic or infrastructure. KleidiAI is designed to be integrated easily into C and C++ codebases thanks to its micro-kernel based architecture. A micro-kernel, in Arm's parlance, refers to the \"near-minimum amount of software to accelerate a given ML operator with high performance\", such as for example, packing or matrix multiplication. A key detail to explain why a micro-kernel is not simply a function, is that each micro-kernel processes only a portion of the output tensor, enabling the full operation to be dispatched across multiple threads. In addition, KleidiAI has other features that will be welcome to developers, including it not relying on external dependencies, not using dynamic memory or requiring memory management, and a highly modular design where each micro-kernel is a stand-alone library consisting only of .c and .h files. To help developers take advantage of SME2, Arm has released additional resources showcasing real-world examples of LLM-based apps using LiteRT, MNN, PyTorch and other supported frameworks. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/07/arm-sme2-android/en/headerimage/arm-sme2-android-1752325153941.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eAvailable in the \u003ca href=\"https://www.arm.com/architecture/cpu/a-profile/armv9\"\u003eArmv9-A architecture\u003c/a\u003e, Arm Scalable Matrix Extension 2 (SME2) is a set of advanced CPU instructions designed to accelerate matrix heavy computation. The new Arm technology aims to help mobile developers to \u003ca href=\"https://newsroom.arm.com/blog/arm-sme2-android-mobile-apps\"\u003erun advanced AI models directly on CPU\u003c/a\u003e with improved performance and efficiency, without requiring any changes to their apps.\u003c/p\u003e\n\n\u003cp\u003eSME2 \u003ca href=\"https://developer.arm.com/documentation/109246/0100/SME-Overview/SME-and-SME2?lang=en\"\u003ebuilds on the previously available SME extension\u003c/a\u003e, which introduced matrix operations and streaming vectors, by adding acceleration and support for multi-vector data-processing instructions, load to and store from multi-vectors, and a multi-vector predication mechanism.\u003c/p\u003e\n\n\u003cp\u003eWhile the performance benefits of SME2 are already available on the \u003ca href=\"https://learn.arm.com/learning-paths/cross-platform/multiplying-matrices-with-sme2/1-get-started#devices\"\u003elatest iOS devices and Apple M4-series chips\u003c/a\u003e, they will soon reach Android devices as well, says Alex Spinelli, Arm’s VP of AI and Developer Platforms and Services.\u003c/p\u003e\n\n\u003cp\u003eMatrix workflows are key for real-time mobile inference tasks such as image and language processing and voice generation. In particular, comparisons between SME2-enabled and non-SME2-enabled workflows shows a significant improvement, says Arm:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eOn SME2-enabled hardware, Google’s Gemma 3 model delivers 6x faster chat responses, and can start summarizing up to 800 words in under a second on a single CPU core.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eLikewise, a \u003ca href=\"https://newsroom.arm.com/news/powering-next-generation-ai-smartphones-mediatek-vivo\"\u003e2.6x speed up has been measured\u003c/a\u003e for prompt processing on a vivo X200 Pro flagship smartphone running a 3.8B parameter Phi-3 Mini model.\u003c/p\u003e\n\n\u003cp\u003eTo help developers take advantage of SME2, Arm provides a library called \u003ca href=\"https://gitlab.arm.com/kleidi/kleidiai\"\u003eKleidiAI\u003c/a\u003e, which is integrated in Google’s XNNPACK. XNNPACK powers several machine learning and AI frameworks, including Alibaba’s MNN, Google’s LiteRT, Microsoft’s ONNX Runtime, and llama.cpp.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhen SME2 is enabled and compatible, XNNPACK automatically routes the matrix heavy operations to SME2 via KleidiAI, so developers directly benefit with no changes needed in application logic or infrastructure.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eKleidiAI is designed to be integrated easily into C and C++ codebases thanks to its \u003ca href=\"https://gitlab.arm.com/kleidi/kleidiai/-/tree/main/kai/ukernels/matmul\"\u003emicro-kernel\u003c/a\u003e based architecture.\u003c/p\u003e\n\n\u003cp\u003eA micro-kernel, in Arm\u0026#39;s parlance, refers to the \u0026#34;near-minimum amount of software to accelerate a given ML operator with high performance\u0026#34;, such as for example, packing or matrix multiplication. A key detail to explain why a micro-kernel is not simply a function, is that each micro-kernel processes only a portion of the output tensor, enabling the full operation to be dispatched across multiple threads.\u003c/p\u003e\n\n\u003cp\u003eIn addition, KleidiAI has other features that will be welcome to developers, including it not relying on external dependencies, not using dynamic memory or requiring memory management, and a highly modular design where each micro-kernel is a stand-alone library consisting only of \u003ccode\u003e.c\u003c/code\u003e and \u003ccode\u003e.h\u003c/code\u003e files.\u003c/p\u003e\n\n\u003cp\u003eTo help developers take advantage of SME2, Arm has released additional \u003ca href=\"https://developer.arm.com/mobile-graphics-and-gaming/ai-mobile#start\"\u003eresources showcasing real-world examples\u003c/a\u003e of LLM-based apps using LiteRT, MNN, PyTorch and other supported frameworks.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-07-13T00:00:00Z",
  "modifiedTime": null
}
