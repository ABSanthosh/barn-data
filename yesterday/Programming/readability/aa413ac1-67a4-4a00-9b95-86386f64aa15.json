{
  "id": "aa413ac1-67a4-4a00-9b95-86386f64aa15",
  "title": "Netflix Tudum Architecture: from CQRS with Kafka to CQRS with RAW Hollow",
  "link": "https://netflixtechblog.com/netflix-tudum-architecture-from-cqrs-with-kafka-to-cqrs-with-raw-hollow-86d141b72e52?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Thu, 10 Jul 2025 19:31:06 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": [
    "data-architecture",
    "cqrs",
    "kafka",
    "tudum"
  ],
  "byline": "Netflix Technology Blog",
  "length": 9575,
  "excerpt": "Tudum.com is Netflix’s official fan destination, enabling fans to dive deeper into their favorite Netflix shows and movies. Tudum offers exclusive first-looks, behind-the-scenes content, talent…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "By Eugene Yemelyanau, Jake GriceIntroductionTudum.com is Netflix’s official fan destination, enabling fans to dive deeper into their favorite Netflix shows and movies. Tudum offers exclusive first-looks, behind-the-scenes content, talent interviews, live events, guides, and interactive experiences. “Tudum” is named after the sonic ID you hear when pressing play on a Netflix show or movie. Attracting over 20 million members each month, Tudum is designed to enrich the viewing experience by offering additional context and insights into the content available on Netflix.Initial architectureAt the end of 2021, when we envisioned Tudum’s implementation, we considered architectural patterns that would be maintainable, extensible, and well-understood by engineers. With the goal of building a flexible, configuration-driven system, we looked to server-driven UI (SDUI) as an appealing solution. SDUI is a design approach where the server dictates the structure and content of the UI, allowing for dynamic updates and customization without requiring changes to the client application. Client applications like web, mobile, and TV devices, act as rendering engines for SDUI data. After our teams weighed and vetted all the details, the dust settled and we landed on an approach similar to Command Query Responsibility Segregation (CQRS). At Tudum, we have two main use cases that CQRS is perfectly capable of solving:Tudum’s editorial team brings exclusive interviews, first-look photos, behind the scenes videos, and many more forms of fan-forward content, and compiles it all into pages on the Tudum.com website. This content comes onto Tudum in the form of individually published pages, and content elements within the pages. In support of this, Tudum’s architecture includes a write path to store all of this data, including internal comments, revisions, version history, asset metadata, and scheduling settings.Tudum visitors consume published pages. In this case, Tudum needs to serve personalized experiences for our beloved fans, and accesses only the latest version of our content.Initial Tudum data architectureThe high-level diagram above focuses on storage \u0026 distribution, illustrating how we leveraged Kafka to separate the write and read databases. The write database would store internal page content and metadata from our CMS. The read database would store read-optimized page content, for example: CDN image URLs rather than internal asset IDs, and movie titles, synopses, and actor names instead of placeholders. This content ingestion pipeline allowed us to regenerate all consumer-facing content on demand, applying new structure and data, such as global navigation or branding changes. The Tudum Ingestion Service converted internal CMS data into a read-optimized format by applying page templates, running validations, performing data transformations, and producing the individual content elements into a Kafka topic. The Data Service Consumer, received the content elements from Kafka, stored them in a high-availability database (Cassandra), and acted as an API layer for the Page Construction service and other internal Tudum services to retrieve content.A key advantage of decoupling read and write paths is the ability to scale them independently. It is a well-known architectural approach to connect both write and read databases using an event driven architecture. As a result, content edits would eventually appear on tudum.com.Challenges with eventual consistencyDid you notice the emphasis on “eventually?” A major downside of this architecture was the delay between making an edit and observing that edit reflected on the website. For instance, when the team publishes an update, the following steps must occur:Call the REST endpoint on the 3rd party CMS to save the data.Wait for the CMS to notify the Tudum Ingestion layer via a webhook.Wait for the Tudum Ingestion layer to query all necessary sections via API, validate data and assets, process the page, and produce the modified content to Kafka.Wait for the Data Service Consumer to consume this message from Kafka and store it in the database.Finally, after some cache refresh delay, this data would eventually become available to the Page Construction service. Great!By introducing a highly-scalable eventually-consistent architecture we were missing the ability to quickly render changes after writing them — an important capability for internal previews.In our performance profiling, we found the source of delay was our Page Data Service which acted as a facade for an underlying Key Value Data Abstraction database. Page Data Service utilized a near cache to accelerate page building and reduce read latencies from the database.This cache was implemented to optimize the N+1 key lookups necessary for page construction by having a complete data set in memory. When engineers hear “slow reads,” the immediate answer is often “cache,” which is exactly what our team adopted. The KVDAL near cache can refresh in the background on every app node. Regardless of which system modifies the data, the cache is updated with each refresh cycle. If you have 60 keys and a refresh interval of 60 seconds, the near cache will update one key per second. This was problematic for previewing recent modifications, as these changes were only reflected with each cache refresh. As Tudum’s content grew, cache refresh times increased, further extending the delay.RAW HollowAs this pain point grew, a new technology was being developed that would act as our silver bullet. RAW Hollow is an innovative in-memory, co-located, compressed object database developed by Netflix, designed to handle small to medium datasets with support for strong read-after-write consistency. It addresses the challenges of achieving consistent performance with low latency and high availability in applications that deal with less frequently changing datasets. Unlike traditional SQL databases or fully in-memory solutions, RAW Hollow offers a unique approach where the entire dataset is distributed across the application cluster and resides in the memory of each application process.This design leverages compression techniques to scale datasets up to 100 million records per entity, ensuring extremely low latencies and high availability. RAW Hollow provides eventual consistency by default, with the option for strong consistency at the individual request level, allowing users to balance between high availability and data consistency. It simplifies the development of highly available and scalable stateful applications by eliminating the complexities of cache synchronization and external dependencies. This makes RAW Hollow a robust solution for efficiently managing datasets in environments like Netflix’s streaming services, where high performance and reliability are paramount.Revised architectureTudum was a perfect fit to battle-test RAW Hollow while it was pre-GA internally. Hollow’s high-density near cache significantly reduces I/O. Having our primary dataset in memory enables Tudum’s various microservices (page construction, search, personalization) to access data synchronously in O(1) time, simplifying architecture, reducing code complexity, and increasing fault tolerance.Updated Tudum data architectureIn our simplified architecture, we eliminated the Page Data Service, Key Value store, and Kafka infrastructure, in favor of RAW Hollow. By embedding the in-memory client directly into our read-path services, we avoid per-request I/O and reduce roundtrip time.Migration resultsThe updated architecture yielded a monumental reduction in data propagation times, and the reduced I/O led to faster request times as an added bonus. Hollow’s compression alleviated our concerns about our data being “too big” to fit in memory. Storing three years’ of unhydrated data requires only a 130MB memory footprint — 25% of its uncompressed size in an Iceberg table!Writers and editors can preview changes in seconds instead of minutes, while still maintaining high-availability and in-memory caching for Tudum visitors — the best of both worlds.But what about the faster request times? The diagram below illustrates the before \u0026 after timing to fulfil a request for Tudum’s home page. All of Tudum’s read-path services leverage Hollow in-memory state, leading to a significant increase in page construction speed and personalization algorithms. Controlling for factors like TLS, authentication, request logging, and WAF filtering, homepage construction time decreased from ~1.4 seconds to ~0.4 seconds!Home page construction timeAn attentive reader might notice that we have now tightly-coupled our Page Construction Service with the Hollow In-Memory State. This tight-coupling is used only in Tudum-specific applications. However, caution is needed if sharing the Hollow In-Memory Client with other engineering teams, as it could limit your ability to make schema changes or deprecations.Key LearningsCQRS is a powerful design paradigm for scale, if you can tolerate some eventual consistency.Minimizing the number of sequential operations can significantly reduce response times. I/O is often the main enemy of performance.Caching is complicated. Cache invalidation is a hard problem. By holding an entire dataset in memory, you can eliminate an entire class of problems.In the next episode, we’ll share how Tudum.com leverages Server Driven UI to rapidly build and deploy new experiences for Netflix fans. Stay tuned!CreditsThanks to Drew Koszewnik, Govind Venkatraman Krishnan, Nick Mooney, George Carlucci",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*5J0sqUrM77vCIDsM8wq_Dw.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv tabindex=\"-1\" aria-hidden=\"false\"\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page---byline--86d141b72e52---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"32\" height=\"32\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"10a0\"\u003eBy \u003ca href=\"https://www.linkedin.com/in/eugeneemelyanov/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEugene Yemelyanau\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/jake-grice/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJake Grice\u003c/a\u003e\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"ffb3\"\u003e\u003cem\u003eIntroduction\u003c/em\u003e\u003c/h2\u003e\u003cp id=\"2f88\"\u003e\u003ca href=\"http://tudum.com\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eTudum.com\u003c/em\u003e\u003c/a\u003e\u003cem\u003e is Netflix’s official fan destination, enabling fans to dive deeper into their favorite Netflix shows and movies. Tudum offers exclusive first-looks, behind-the-scenes content, talent interviews, live events, guides, and interactive experiences. “Tudum” is named after the sonic ID you hear when pressing play on a Netflix show or movie. Attracting over 20 million members each month, Tudum is designed to enrich the viewing experience by offering additional context and insights into the content available on Netflix.\u003c/em\u003e\u003c/p\u003e\u003ch2 id=\"655e\"\u003eInitial architecture\u003c/h2\u003e\u003cp id=\"3da2\"\u003eAt the end of 2021, when we envisioned Tudum’s implementation, we considered architectural patterns that would be maintainable, extensible, and well-understood by engineers. With the goal of building a flexible, configuration-driven system, we looked to \u003cstrong\u003eserver-driven UI\u003c/strong\u003e (SDUI) as an appealing solution. SDUI is a design approach where the server dictates the structure and content of the UI, allowing for dynamic updates and customization without requiring changes to the client application. Client applications like web, mobile, and TV devices, act as rendering engines for SDUI data. After our teams weighed and vetted all the details, the dust settled and we landed on an approach similar to Command Query Responsibility Segregation (\u003ca href=\"https://www.geeksforgeeks.org/cqrs-command-query-responsibility-segregation/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCQRS\u003c/a\u003e). At Tudum, we have two main use cases that CQRS is perfectly capable of solving:\u003c/p\u003e\u003cul\u003e\u003cli id=\"9a85\"\u003e\u003cstrong\u003eTudum’s editorial team\u003c/strong\u003e brings exclusive interviews, first-look photos, behind the scenes videos, and many more forms of fan-forward content, and compiles it all into pages on the \u003ca href=\"http://tudum.com\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTudum.com\u003c/a\u003e website. This content comes onto Tudum in the form of individually published pages, and content elements within the pages. In support of this, Tudum’s architecture includes a write path to store all of this data, including internal comments, revisions, version history, asset metadata, and scheduling settings.\u003c/li\u003e\u003cli id=\"a323\"\u003e\u003cstrong\u003eTudum visitors\u003c/strong\u003e consume published pages. In this case, Tudum needs to serve personalized experiences for our beloved fans, and accesses only the latest version of our content.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eInitial Tudum data architecture\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"6413\"\u003eThe high-level diagram above focuses on storage \u0026amp; distribution, illustrating how we leveraged Kafka to separate the write and read databases. The write database would store internal page content and metadata from our CMS. The read database would store read-optimized page content, for example: CDN image URLs rather than internal asset IDs, and movie titles, synopses, and actor names instead of placeholders. This content ingestion pipeline allowed us to regenerate all consumer-facing content on demand, applying new structure and data, such as global navigation or branding changes. The Tudum Ingestion Service converted internal CMS data into a read-optimized format by applying page templates, running validations, performing data transformations, and producing the individual content elements into a Kafka topic. The Data Service Consumer, received the content elements from Kafka, stored them in a high-availability database (Cassandra), and acted as an API layer for the Page Construction service and other internal Tudum services to retrieve content.\u003c/p\u003e\u003cp id=\"704d\"\u003eA key advantage of decoupling read and write paths is the ability to scale them independently. It is a well-known architectural approach to connect both write and read databases using an event driven architecture. As a result, content edits would \u003cstrong\u003e\u003cem\u003eeventually\u003c/em\u003e\u003c/strong\u003e appear on \u003ca href=\"http://tudum.com\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003etudum.com\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"38cc\"\u003eChallenges with eventual consistency\u003c/h2\u003e\u003cp id=\"05f4\"\u003eDid you notice the emphasis on “\u003cstrong\u003e\u003cem\u003eeventually\u003c/em\u003e\u003c/strong\u003e?” A major downside of this architecture was the delay between making an edit and observing that edit reflected on the website. For instance, when the team publishes an update, the following steps must occur:\u003c/p\u003e\u003col\u003e\u003cli id=\"d25e\"\u003eCall the REST endpoint on the 3rd party CMS to save the data.\u003c/li\u003e\u003cli id=\"3462\"\u003eWait for the CMS to notify the Tudum Ingestion layer via a webhook.\u003c/li\u003e\u003cli id=\"3b1e\"\u003eWait for the Tudum Ingestion layer to query all necessary sections via API, validate data and assets, process the page, and produce the modified content to Kafka.\u003c/li\u003e\u003cli id=\"140d\"\u003eWait for the Data Service Consumer to consume this message from Kafka and store it in the database.\u003c/li\u003e\u003cli id=\"d298\"\u003eFinally, after some \u003cstrong\u003ecache refresh delay\u003c/strong\u003e, this data would \u003cstrong\u003e\u003cem\u003eeventually\u003c/em\u003e\u003c/strong\u003e become available to the Page Construction service. Great!\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"4a60\"\u003eBy introducing a highly-scalable eventually-consistent architecture we were missing the ability to quickly render changes after writing them — an important capability for internal previews.\u003c/p\u003e\u003cp id=\"6b8e\"\u003eIn our performance profiling, we found the source of delay was our Page Data Service which acted as a facade for an underlying \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30\" target=\"_blank\" data-discover=\"true\"\u003eKey Value Data Abstraction\u003c/a\u003e database. Page Data Service utilized a \u003cstrong\u003enear cache\u003c/strong\u003e to accelerate page building and reduce read latencies from the database.\u003c/p\u003e\u003cp id=\"9814\"\u003eThis cache was implemented to optimize the N+1 key lookups necessary for page construction by having a complete data set in memory. When engineers hear “\u003cem\u003eslow reads\u003c/em\u003e,” the immediate answer is often “\u003cem\u003ecache\u003c/em\u003e,” which is exactly what our team adopted. The KVDAL near cache can refresh in the background on every app node. Regardless of which system modifies the data, the cache is updated with each refresh cycle. If you have 60 keys and a refresh interval of 60 seconds, the near cache will update one key per second. This was problematic for previewing recent modifications, as these changes were only reflected with each cache refresh. As Tudum’s content grew, cache refresh times increased, further extending the delay.\u003c/p\u003e\u003ch2 id=\"0410\"\u003eRAW Hollow\u003c/h2\u003e\u003cp id=\"76e2\"\u003eAs this pain point grew, a new technology was being developed that would act as our silver bullet. \u003ca href=\"https://hollow.how/raw-hollow-sigmod.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRAW Hollow\u003c/a\u003e is an innovative in-memory, co-located, compressed object database developed by Netflix, designed to handle small to medium datasets with support for strong read-after-write consistency. It addresses the challenges of achieving consistent performance with low latency and high availability in applications that deal with less frequently changing datasets. Unlike traditional SQL databases or fully in-memory solutions, RAW Hollow offers a unique approach where the entire dataset is distributed across the application cluster and resides in the memory of each application process.\u003c/p\u003e\u003cp id=\"3cf2\"\u003eThis design leverages compression techniques to scale datasets up to 100 million records per entity, ensuring extremely low latencies and high availability. RAW Hollow provides eventual consistency by default, with the option for strong consistency at the individual request level, allowing users to balance between high availability and data consistency. It simplifies the development of highly available and scalable stateful applications by eliminating the complexities of cache synchronization and external dependencies. This makes RAW Hollow a robust solution for efficiently managing datasets in environments like Netflix’s streaming services, where high performance and reliability are paramount.\u003c/p\u003e\u003ch2 id=\"1a6a\"\u003eRevised architecture\u003c/h2\u003e\u003cp id=\"e493\"\u003eTudum was a perfect fit to battle-test RAW Hollow while it was pre-GA internally. Hollow’s high-density near cache significantly reduces I/O. Having our primary dataset in memory enables Tudum’s various microservices (page construction, search, personalization) to access data synchronously in O(1) time, simplifying architecture, reducing code complexity, and increasing fault tolerance.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eUpdated Tudum data architecture\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"24fa\"\u003eIn our simplified architecture, we eliminated the Page Data Service, Key Value store, and Kafka infrastructure, in favor of RAW Hollow. By embedding the in-memory client directly into our read-path services, we avoid per-request I/O and reduce roundtrip time.\u003c/p\u003e\u003ch2 id=\"6980\"\u003eMigration results\u003c/h2\u003e\u003cp id=\"9b6a\"\u003eThe updated architecture yielded a monumental reduction in data propagation times, and the reduced I/O led to faster request times as an added bonus. Hollow’s compression alleviated our concerns about our data being “too big” to fit in memory. Storing three years’ of unhydrated data requires only a 130MB memory footprint — 25% of its uncompressed size in an Iceberg table!\u003c/p\u003e\u003cp id=\"adc5\"\u003eWriters and editors can preview changes in seconds instead of minutes, while still maintaining high-availability and in-memory caching for Tudum visitors — the best of both worlds.\u003c/p\u003e\u003cp id=\"b810\"\u003eBut what about the faster request times? The diagram below illustrates the before \u0026amp; after timing to fulfil a request for Tudum’s home page. All of Tudum’s read-path services leverage Hollow in-memory state, leading to a significant increase in page construction speed and personalization algorithms. Controlling for factors like TLS, authentication, request logging, and WAF filtering, homepage construction time decreased from ~1.4 seconds to ~0.4 seconds!\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eHome page construction time\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"f4b8\"\u003eAn attentive reader might notice that we have now tightly-coupled our Page Construction Service with the Hollow In-Memory State. This tight-coupling is used only in Tudum-specific applications. However, caution is needed if sharing the Hollow In-Memory Client with other engineering teams, as it could limit your ability to make schema changes or deprecations.\u003c/p\u003e\u003ch2 id=\"a4d4\"\u003eKey Learnings\u003c/h2\u003e\u003col\u003e\u003cli id=\"a421\"\u003eCQRS is a powerful design paradigm for scale, if you can tolerate some eventual consistency.\u003c/li\u003e\u003cli id=\"7771\"\u003eMinimizing the number of sequential operations can significantly reduce response times. I/O is often the main enemy of performance.\u003c/li\u003e\u003cli id=\"c7fc\"\u003eCaching is complicated. Cache invalidation is a hard problem. By holding an entire dataset in memory, you can eliminate an entire class of problems.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"2f6b\"\u003eIn the next episode, we’ll share how \u003ca href=\"http://tudum.com\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTudum.com\u003c/a\u003e leverages Server Driven UI to rapidly build and deploy new experiences for Netflix fans. Stay tuned!\u003c/p\u003e\u003ch2 id=\"5218\"\u003eCredits\u003c/h2\u003e\u003cp id=\"5972\"\u003eThanks to \u003ca href=\"https://www.linkedin.com/in/koszewnik\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDrew Koszewnik\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/govindvenkatramankrishnan\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGovind Venkatraman Krishnan\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/nick-mooney-193849/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNick Mooney\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/georgecarlucci/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGeorge Carlucci\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-07-10T19:31:06.122Z",
  "modifiedTime": null
}
