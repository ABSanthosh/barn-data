{
  "id": "3ba444c7-369b-4910-8386-d97f11bf0f81",
  "title": "OpenAI Launches BrowseComp to Benchmark AI Agents' Web Search and Deep Research Skills",
  "link": "https://www.infoq.com/news/2025/05/openai-browsecomp-ai-benchmark/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "OpenAI has released BrowseComp, a new benchmark designed to test AI agents' ability to locate difficult-to-find information on the web. The benchmark contains 1,266 challenging problems that require agents to persistently navigate through multiple websites to retrieve entangled information. By Vinod Goje",
  "author": "Vinod Goje",
  "published": "Sun, 04 May 2025 12:42:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Agents",
    "Artificial Intelligence",
    "Large language models",
    "Benchmark",
    "Generative AI",
    "OpenAI",
    "GPT-4",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Vinod Goje",
  "length": 4809,
  "excerpt": "OpenAI has released BrowseComp, a new benchmark designed to test AI agents' ability to locate difficult-to-find information on the web. The benchmark contains 1,266 challenging problems that require a",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250422123038_u1/apple-touch-icon.png",
  "text": "OpenAI has released BrowseComp, a new benchmark designed to test AI agents' ability to locate difficult-to-find information on the web. The benchmark contains 1,266 challenging problems that require agents to persistently navigate through multiple websites to retrieve entangled information. Unlike existing benchmarks such as SimpleQA that focus on basic fact retrieval and are already \"saturated by models with access to fast browsing tools, such as GPT-4o with browsing,\" BrowseComp challenges agents to sift through tens or even hundreds of websites to find answers. The benchmark questions have short, unambiguous answers that can be easily verified against reference solutions. OpenAI positions BrowseComp as \"analogous to how programming competitions are an incomplete but useful benchmark for coding agents.\" While it doesn't address all aspects of real-world user queries, it measures the \"important core capability of exercising persistence and creativity in finding information\" that will be essential for next-generation AI assistants. While humans struggle with web navigation due to \"limited memory and world knowledge,\" vulnerability to \"distraction and fatigue,\" and inability to multitask, machine intelligence theoretically offers advantages through superior recall and tireless operation. However, current AI systems fall short of their potential. Despite recent advances in large language models, AI agents still \"underperform when tasked with locating nuanced, context-dependent facts across multiple sources.\" Traditional benchmarks primarily measure recall of easily accessible information rather than evaluating the complex browsing capabilities needed for practical applications such as research assistance, policy summarization, or fact-checking tasks that demand persistence and adaptive search strategies. The BrowseComp dataset was created entirely by human trainers who developed fact-seeking questions with \"single, indisputable, short answers that would not change over time.\" To ensure questions met the benchmark's standard of difficulty, trainers verified that leading models including GPT-4o (with and without browsing), OpenAI o1, and an early version of their deep research model could not solve them. Additionally, trainers confirmed answers weren't discoverable within the first page of five different Google searches, and aimed to create problems that would take most people more than ten minutes to solve. The benchmark uses an \"inverted question\" approach where trainers started with facts and then constructed questions that made those facts \"hard to find but easy to verify,\" typically by combining multiple characteristics with large search spaces. OpenAI evaluated several of its models on the BrowseComp benchmark, including non-browsing models like GPT-4o, GPT-4.5, and OpenAI o1, as well as web-enabled systems like GPT-4o with browsing and their Deep Research model. The results reveal that Deep Research \"significantly outperforms all other models, solving around half of the problems.\" This agent model demonstrates capabilities in \"autonomously searching the web, evaluating and synthesizing information from multiple sources, and adapting its search strategy\" critical skills for tackling BrowseComp's intentionally difficult questions. Source: Accuracy and calibration of OpenAI models on BrowseComp The release of BrowseComp has sparked discussion about the future of web search and AI-assisted research. Michael Buckbee, founder of Knowatoa, expressed both optimism and concern about these developments. While I'm positive about the impact of AI on search, if there's one innovation that threatens the search market as we know it, it's 'Deep Research' agents, Buckbee said. We're hurtling towards a future where people don't see search results at all but just 'reports' of search results. The new AI modes, deep research tools, and interfaces all clearly depict what this looks like. Nishant Sinha, AI advisor and builder, highlighted the significance of BrowseComp's difficulty level: Browser use agents have grown in their accuracy for locating UI elements on a web page and even executing a series of natural language instructions. But this benchmark stress tests them! Not just find a piece of information easily accessible but something that is 'hidden' behind several doors. Developers and researchers interested in exploring BrowseComp can access the benchmark through its GitHub repository. For a deeper understanding of the methodology and findings, read the full research paper. Also, readers are encouraged to read our recent coverage of OpenAI's Deep Research model. About the Author Vinod Goje",
  "image": "https://res.infoq.com/news/2025/05/openai-browsecomp-ai-benchmark/en/headerimage/generatedHeaderImage-1746313986878.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://openai.com/about/\"\u003eOpenAI\u003c/a\u003e has \u003ca href=\"https://openai.com/index/browsecomp/\"\u003ereleased\u003c/a\u003e BrowseComp, a new \u003ca href=\"https://www.ibm.com/think/topics/llm-benchmarks\"\u003ebenchmark\u003c/a\u003e designed to test \u003ca href=\"https://aws.amazon.com/what-is/ai-agents/\"\u003eAI agents\u0026#39;\u003c/a\u003e ability to locate difficult-to-find information on the web. The benchmark contains 1,266 challenging problems that require agents to persistently navigate through multiple websites to retrieve entangled information.\u003c/p\u003e\n\n\u003cp\u003eUnlike existing benchmarks such as \u003ca href=\"https://openai.com/index/introducing-simpleqa/\"\u003eSimpleQA\u003c/a\u003e that focus on basic fact retrieval and are already \u0026#34;saturated by models with access to fast browsing tools, such as \u003ca href=\"https://www.youtube.com/watch?v=6m7fJcX_dqc\"\u003eGPT-4o with browsing\u003c/a\u003e,\u0026#34; BrowseComp challenges agents to sift through tens or even hundreds of websites to find answers. The benchmark questions have short, unambiguous answers that can be easily verified against reference solutions.\u003c/p\u003e\n\n\u003cp\u003eOpenAI positions BrowseComp as \u0026#34;analogous to how programming competitions are an incomplete but useful benchmark for coding agents.\u0026#34; While it doesn\u0026#39;t address all aspects of real-world user queries, it measures the \u0026#34;important core capability of exercising persistence and creativity in finding information\u0026#34; that will be essential for next-generation AI assistants.\u003c/p\u003e\n\n\u003cp\u003eWhile humans struggle with web navigation due to \u0026#34;limited memory and world knowledge,\u0026#34; vulnerability to \u0026#34;distraction and fatigue,\u0026#34; and inability to multitask, machine intelligence theoretically offers advantages through superior \u003ca href=\"https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall\"\u003erecall\u003c/a\u003e and tireless operation. However, current AI systems fall short of their potential. Despite recent advances in \u003ca href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\"\u003elarge language models\u003c/a\u003e, AI agents still \u0026#34;underperform when tasked with locating nuanced, context-dependent facts across multiple sources.\u0026#34; Traditional benchmarks primarily measure recall of easily accessible information rather than evaluating the complex browsing capabilities needed for practical applications such as research assistance, policy summarization, or fact-checking tasks that demand persistence and adaptive search strategies.\u003c/p\u003e\n\n\u003cp\u003eThe BrowseComp dataset was created entirely by human trainers who developed fact-seeking questions with \u0026#34;single, indisputable, short answers that would not change over time.\u0026#34; To ensure questions met the benchmark\u0026#39;s standard of difficulty, trainers verified that leading models including GPT-4o (with and without browsing), \u003ca href=\"https://openai.com/about/\"\u003eOpenAI\u003c/a\u003e o1, and an early version of their \u003ca href=\"https://openai.com/index/introducing-deep-research/\"\u003edeep research model\u003c/a\u003e could not solve them. Additionally, trainers confirmed answers weren\u0026#39;t discoverable within the first page of five different Google searches, and aimed to create problems that would take most people more than ten minutes to solve. The benchmark uses an \u0026#34;inverted question\u0026#34; approach where trainers started with facts and then constructed questions that made those facts \u0026#34;hard to find but easy to verify,\u0026#34; typically by combining multiple characteristics with large search spaces.\u003c/p\u003e\n\n\u003cp\u003eOpenAI evaluated several of its models on the BrowseComp benchmark, including non-browsing models like \u003ca href=\"https://www.ibm.com/think/topics/gpt-4o\"\u003eGPT-4o\u003c/a\u003e, \u003ca href=\"https://openai.com/index/introducing-gpt-4-5/\"\u003eGPT-4.5\u003c/a\u003e, and \u003ca href=\"https://openai.com/o1/\"\u003eOpenAI o1\u003c/a\u003e, as well as web-enabled systems like GPT-4o with browsing and their \u003ca href=\"https://openai.com/index/introducing-deep-research/\"\u003eDeep Research model\u003c/a\u003e. The results reveal that Deep Research \u0026#34;significantly outperforms all other models, solving around half of the problems.\u0026#34; This agent model demonstrates capabilities in \u0026#34;autonomously searching the web, evaluating and synthesizing information from multiple sources, and adapting its search strategy\u0026#34; critical skills for tackling BrowseComp\u0026#39;s intentionally difficult questions.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/05/openai-browsecomp-ai-benchmark/en/resources/1accuracy-1746314178707.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/05/openai-browsecomp-ai-benchmark/en/resources/1accuracy-1746314178707.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://cdn.openai.com/pdf/5e10f4ab-d6f7-442e-9508-59515c65e35d/browsecomp.pdf\"\u003eSource\u003c/a\u003e: Accuracy and calibration of OpenAI models on BrowseComp\u003c/p\u003e\n\n\u003cp\u003eThe release of BrowseComp has sparked discussion about the future of web search and AI-assisted research.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.linkedin.com/in/michaelbuckbee/\"\u003eMichael Buckbee\u003c/a\u003e, founder of Knowatoa, \u003ca href=\"https://www.linkedin.com/posts/michaelbuckbee_im-generally-very-positive-about-the-impact-activity-7319000158162046976-DXaw/\"\u003eexpressed\u003c/a\u003e both optimism and concern about these developments.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhile I\u0026#39;m positive about the impact of AI on search, if there\u0026#39;s one innovation that threatens the search market as we know it, it\u0026#39;s \u0026#39;Deep Research\u0026#39; agents,\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eBuckbee said.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWe\u0026#39;re hurtling towards a future where people don\u0026#39;t see search results at all but just \u0026#39;reports\u0026#39; of search results. The new AI modes, deep research tools, and interfaces all clearly depict what this looks like.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.linkedin.com/in/nishant-sinha-a610311/\"\u003eNishant Sinha\u003c/a\u003e, AI advisor and builder, \u003ca href=\"https://www.linkedin.com/posts/nishant-sinha-a610311_finally-a-strong-benchmark-for-computer-activity-7316423957375590400-Z_C3/\"\u003ehighlighted\u003c/a\u003e the significance of BrowseComp\u0026#39;s difficulty level:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eBrowser use agents have grown in their accuracy for locating UI elements on a web page and even executing a series of natural language instructions. But this benchmark stress tests them! Not just find a piece of information easily accessible but something that is \u0026#39;hidden\u0026#39; behind several doors.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eDevelopers and researchers interested in exploring BrowseComp can access the benchmark through its \u003ca href=\"https://github.com/openai/simple-evals\"\u003eGitHub repository\u003c/a\u003e. For a deeper understanding of the methodology and findings, read the \u003ca href=\"https://cdn.openai.com/pdf/5e10f4ab-d6f7-442e-9508-59515c65e35d/browsecomp.pdf\"\u003efull research paper\u003c/a\u003e. Also, readers are encouraged to read our \u003ca href=\"https://www.infoq.com/news/2025/02/deep-research-openai/\"\u003erecent coverage of OpenAI\u0026#39;s Deep Research model\u003c/a\u003e.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Vinod-Goje\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eVinod Goje\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-05-04T00:00:00Z",
  "modifiedTime": null
}
