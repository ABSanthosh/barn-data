{
  "id": "c02d4187-dada-4f99-98a4-b8597fd7b4b4",
  "title": "Gemma 3 on mobile and web with Google AI Edge",
  "link": "https://developers.googleblog.com/en/gemma-3-on-mobile-and-web-with-google-ai-edge/",
  "description": "Gemma 3 1B, a new small language model for mobile and web applications via Google AI Edge, is now available, with increased efficiency, improved performance, and offline availability.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Marissa Ikonomidis, T.J. Alumbaugh, Mark Sherwood, Cormac Brick",
  "length": 9171,
  "excerpt": "Gemma 3 1B, a new small language model for mobile and web applications via Google AI Edge, is now available, with increased efficiency, improved performance, and offline availability.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "Gemma 3 1B is a new model size in the Gemma family of open weight models that truly opens the possibility for distributing in-app small language models (SLMs) across mobile and web. When deploying SLMs in production settings, models need to be small enough to download quickly, run fast enough to hold user attention, and support a wide range of end user devices.At only 529MB in size, Gemma 3 1B runs at up to 2585 tok/sec on prefill via Google AI Edge’s LLM inference, creating the ability to process a page of content in under a second. Including Gemma 3 1B in your app, you can use natural language to drive your application or generate content from in-app data or context, all fully customizable and fine-tunable.In this post, we'll guide you through some example use cases for Gemma 3 in your application, how to get started with Gemma on Android, dive into some of the performance metrics, and explain how all of this was achieved. Sorry, your browser doesn't support playback for this video Turn app data into personalized content on Android using Gemma 3 1B What Can I Do With Gemma 3 in My App?With a fully on-device Gemma 3 1B model, you are able to take advantage of the benefits of AI Edge:Offline Availability: Enable your app to work fully when WiFi or cellular data is unavailable.2. Cost: With no cloud bills, enable free or freemium apps.3. Latency: Some features need to be faster than a server call allows.4. Privacy: Bring intelligence to data that is unable to leave the device or is end-to-end encrypted.Gemma 1B is extremely versatile and can even be fine-tuned for your own domain and use cases. Here are just a few of our favorite use cases for Gemma 1B:Data Captioning: Turn your app data into engaging and shareable descriptions, i.e, Sleep Data -\u003e “You slept well for 7 hours but you stirred awake 5 times between 2am and 4am”.2. In-Game Dialog: Create NPC dialog based on the current game state.3. Smart Reply: Provide users with intelligent conversation-aware suggested responses while messaging.4. Document Q\u0026A: Use Gemma 3 along with our new AI Edge RAG SDK to ingest long documents and answer user questions.Getting startedStep 1: Load the Demo appDownload Google AI Edge’s pre-built demo app from GitHub and push it to your local Android device. For best performance with Gemma 3 1B, we recommend a device with at least 4GB of memory. $ wget https://github.com/google-ai-edge/mediapipe-samples/releases/download/v0.1.3/llm_inference_v0.1.3-debug.apk $ adb install llm_inference_v0.1.3-debug.apk Alternatively, you can follow our instructions to build the app from source.Step 2: Select CPU or GPUThe Gemma 3 model file offers great deployment flexibility, running seamlessly on either your device's CPU or mobile GPU. You can choose to run Gemma 3 on CPU or GPU when you first start the app, or switch between models and backends by going back to the model selection dialog.Step 3: Download the Model from Hugging FaceOn the model selection screen in the demo app, choose your model. The app will direct you to Hugging Face to login and accept the Gemma terms of use. Gemma 3 1B, quantized at int4, will be downloaded directly from the LiteRT HuggingFace community organization, and will then be optimized once to run on your device (but this only takes a few seconds!).Step 4: Run the ModelNow it's time to put Gemma 3 to work! Under the hood, Gemma 3 is powered by Google AI Edge’s LLM Inference API, designed for efficient on-device processing.You can interact with the model by chatting with it. Or, you can give it other text processing tasks. For example, try the following:Copy a few paragraphs from a blog post (like this one) or an article.Switch over to the LLM Demo app.Paste the copied text into the input box.Type \"Create a social media post for this content. Keep it short and sweet. Less than 50 words\" and press enter.Step 5: Customize Gemma 3 (optional)One of the great things about the Gemma family of open weight models are the fine-tuned versions produced by the modeling community. Follow this Colab to see how you can use your own data to create your own version of Gemma 3 1B, quantize it, and get it running on mobile devices (CPU and GPU) in your own applications!Performance Sorry, your browser doesn't support playback for this video Create social media content locally in-browser using Gemma 3 1B The demo and measurements here are for the Gemma 3 1B model with int4 parameters quantized via quantized-aware training (QAT) which provides significant storage savings and increased decode throughput. The benchmarked Gemma 3 model supports multiple prefill lengths of 32, 128, 512 and 1024 and it uses a context length of 2048. Measurements were taken on an Android Samsung Galaxy S24 Ultra with cpufreq governor set to performance. Observed performance may vary depending on your phone’s hardware and current activity level. Measurements were taken on MacBook Pro 2023 (Apple M3 Pro chip) Observed performance may vary depending on your computer’s hardware and current activity level. Under the hoodThe performance results described above were achieved through extensive optimization efforts. These optimizations were designed to work well across open weight models, including Gemma. Here are some key features that significantly boosted performance and enabled new, reusable functionality.Quantization: Quantization-aware training was applied to Gemma using a 4-bit integer channel-wise scheme on weights to maintain optimal performance, model quality, and size. In addition to weight quantization, we also dynamically quantize the activation to int8 during execution to best utilize CPU capability.Updating the KV Cache layouts: The KV cache is used in Transformer based models to store the key-value pairs from previous steps so they can be used to generate subsequent tokens. Reads and writes to the KV cache happen frequently so it is important that these operations are efficient. These operations were optimized by introducing a KV Cache layout to reduce extra transposes and reshapes. This optimization improved latency on Gemma models by approximately 25% for CPU and 20% for GPU. An extra operation was also added to more to performantly update the KV cache in-place on the GPU.Improved Loading Time: To make the most of CPU and GPU processing, we use specialized tensor layouts. Generating these optimized weight layouts can take time, power and significant memory. During the first model load, the weights are cached on disk in their optimized format and subsequent loads read from the cache. If tensor layouts are further optimized, the existing cache will automatically be invalidated and the new format will be stored on disk during the next model load.GPU Weight Sharing: The LLM inference process has two phases: prefill and decode. These phases typically use separate resources for their respective models. To dramatically reduce the memory footprint of LLMs, both phases can share the same weights. While this technique isn't entirely new, this is the first time it has been done in an easily reusable way in the LiteRT Runtime and GPU Delegate. For ops that support this feature, the GPU delegate checks if the weights are already present in GPU memory and can be reused. In the future, other models will be able to trivially take advantage of this capability.What’s nextDuring the development of Gemma 3, we focused on delivering excellent performance while also building reusable infrastructure for open weight models. In 2025, we plan to leverage this work to support a wider set of third-party models. With additional performance optimizations and an emphasis on further reducing memory use, we intend to continue making models more accessible on a wider range of devices. To keep up with the latest developments, set up notifications for ai_edge_torch on GitHub. More to come soon!AcknowledgementsAdvait Jain, Akshat Sharma, Alan Kelly, Andrei Kulik, Byungchul Kim, Chunlei Niu, Chun-nien Chan, Chuo-Ling Chang, Claudio Basile, Cormac Brick, Ekaterina Ignasheva, Eric Yang, Fengwu Yao, Frank Ban, Gerardo Carranza, Grant Jensen, Haoliang Zhang, Henry Wang, Ho Ko, Jae Yoo, Jiuqiang Tang, Juhyun Lee, Jun Jiang, Khanh LeViet, Kris Tonthat, Lin Chen, Lu Wang, Malini P V, Marissa Ikonomidis, Mark Sherwood, Matthew Soulanille, Matthias Grundmann, Mogan Shieh, Mohammadreza Heydary, Na Li, Pauline Sho, Pedro Gonnet, Ping Yu, Pulkit Bhuwalka, Quentin Khan, Ram Iyengar, Raman Sarokin, Rishika Sinha, Rishubh Khurana, Ronghui Zhu, Sachin Kotwani, Sebastian Schmidt, Steven Toribio, Suleman Shahid, T.J. Alumbaugh, Tenghui Zhu, Terry (Woncheol) Heo, Tyler Mullen, Vamsi Manchala, Vitalii Dziuba, Wai Hon Law, Weiyi Wang, Xu Chen, Yishuang Pang, Youchuan Hu, Yu-hui Chen, Zichuan Wei",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemma-3-google-ai-edge-1.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"09pfg\"\u003e\u003ca href=\"https://developers.googleblog.com/en/introducing-gemma3\"\u003eGemma 3 1B\u003c/a\u003e is a new model size in the Gemma family of open weight models that truly opens the possibility for distributing in-app small language models (SLMs) across mobile and web. When deploying SLMs in production settings, models need to be small enough to download quickly, run fast enough to hold user attention, and support a wide range of end user devices.\u003c/p\u003e\u003cp data-block-key=\"cfu6q\"\u003eAt only 529MB in size, Gemma 3 1B runs at up to 2585 tok/sec on prefill via Google AI Edge’s LLM inference, creating the ability to process a page of content in under a second. Including Gemma 3 1B in your app, you can use natural language to drive your application or generate content from in-app data or context, all fully customizable and fine-tunable.\u003c/p\u003e\u003cp data-block-key=\"ejrhk\"\u003eIn this post, we\u0026#39;ll guide you through some example use cases for Gemma 3 in your application, how to get started with Gemma on Android, dive into some of the performance metrics, and explain how all of this was achieved.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-4sjhaq4d_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/Gemma-3-personalized-content_1.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n        \n            \u003cp\u003e\n                Turn app data into personalized content on Android using Gemma 3 1B\n            \u003c/p\u003e\n        \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"09pfg\"\u003eWhat Can I Do With Gemma 3 in My App?\u003c/h2\u003e\u003cp data-block-key=\"8jsad\"\u003eWith a fully on-device Gemma 3 1B model, you are able to take advantage of the benefits of AI Edge:\u003c/p\u003e\u003col\u003e\u003cli data-block-key=\"5tegg\"\u003e\u003cb\u003eOffline Availability:\u003c/b\u003e Enable your app to work fully when WiFi or cellular data is unavailable.\u003c/li\u003e\u003c/ol\u003e\u003cp data-block-key=\"at9mk\"\u003e2.\u003cb\u003e Cost:\u003c/b\u003e With no cloud bills, enable free or freemium apps.\u003c/p\u003e\u003cp data-block-key=\"funpi\"\u003e3.\u003cb\u003e Latency:\u003c/b\u003e Some features need to be faster than a server call allows.\u003c/p\u003e\u003cp data-block-key=\"15ee\"\u003e4.\u003cb\u003e Privacy:\u003c/b\u003e Bring intelligence to data that is unable to leave the device or is end-to-end encrypted.\u003c/p\u003e\u003cp data-block-key=\"3tqpp\"\u003e\u003cbr/\u003eGemma 1B is extremely versatile and can even be \u003ca href=\"https://colab.sandbox.google.com/github/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb\"\u003efine-tuned\u003c/a\u003e for your own domain and use cases. Here are just a few of our favorite use cases for Gemma 1B:\u003c/p\u003e\u003col\u003e\u003cli data-block-key=\"fejq9\"\u003e\u003cb\u003eData Captioning:\u003c/b\u003e Turn your app data into engaging and shareable descriptions, i.e, Sleep Data -\u0026gt; “You slept well for 7 hours but you stirred awake 5 times between 2am and 4am”.\u003c/li\u003e\u003c/ol\u003e\u003cp data-block-key=\"21iof\"\u003e2.\u003cb\u003e In-Game Dialog:\u003c/b\u003e Create NPC dialog based on the current game state.\u003c/p\u003e\u003cp data-block-key=\"136n3\"\u003e3.\u003cb\u003e Smart Reply:\u003c/b\u003e Provide users with intelligent conversation-aware suggested responses while messaging.\u003c/p\u003e\u003cp data-block-key=\"6mftm\"\u003e4.\u003cb\u003e Document Q\u0026amp;A:\u003c/b\u003e Use Gemma 3 along with our new \u003ca href=\"https://github.com/google-ai-edge/ai-edge-apis/tree/main/local_agents/rag\"\u003eAI Edge RAG SDK\u003c/a\u003e to ingest long documents and answer user questions.\u003c/p\u003e\u003ch2 data-block-key=\"ct3cl\"\u003e\u003cbr/\u003eGetting started\u003c/h2\u003e\u003cp data-block-key=\"b18t1\"\u003e\u003cb\u003eStep 1: Load the Demo app\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"7553u\"\u003eDownload Google AI Edge’s \u003ca href=\"https://github.com/google-ai-edge/mediapipe-samples/releases/download/v0.1.3/llm_inference_v0.1.3-debug.apk\"\u003epre-built demo app\u003c/a\u003e from GitHub and push it to your local Android device. For best performance with Gemma 3 1B, we recommend a device with at least 4GB of memory.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e$ wget https://github.com/google-ai-edge/mediapipe-samples/releases/download/v0.1.3/llm_inference_v0.1.3-debug.apk\n$ adb install llm_inference_v0.1.3-debug.apk\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"09pfg\"\u003eAlternatively, you can follow our \u003ca href=\"https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/android/README.md\"\u003einstructions\u003c/a\u003e to build the app from source.\u003c/p\u003e\u003ch3 data-block-key=\"5u1sf\"\u003e\u003cb\u003e\u003cbr/\u003eStep 2: Select CPU or GPU\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"ejec8\"\u003eThe Gemma 3 model file offers great deployment flexibility, running seamlessly on either your device\u0026#39;s CPU or mobile GPU. You can choose to run Gemma 3 on CPU or GPU when you first start the app, or switch between models and backends by going back to the model selection dialog.\u003c/p\u003e\u003ch3 data-block-key=\"e013l\"\u003e\u003cb\u003e\u003cbr/\u003eStep 3: Download the Model from Hugging Face\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"a3lc2\"\u003eOn the model selection screen in the demo app, choose your model. The app will direct you to Hugging Face to login and accept the Gemma terms of use. Gemma 3 1B, quantized at int4, will be downloaded directly from the \u003ca href=\"https://huggingface.co/litert-community\"\u003eLiteRT HuggingFace community organization\u003c/a\u003e, and will then be optimized once to run on your device (but this only takes a few seconds!).\u003c/p\u003e\u003ch3 data-block-key=\"15gbr\"\u003e\u003cb\u003e\u003cbr/\u003eStep 4: Run the Model\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"9vft3\"\u003eNow it\u0026#39;s time to put Gemma 3 to work! Under the hood, Gemma 3 is powered by Google AI Edge’s \u003ca href=\"https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/\"\u003eLLM Inference API\u003c/a\u003e, designed for efficient on-device processing.\u003c/p\u003e\u003cp data-block-key=\"4c4k0\"\u003eYou can interact with the model by chatting with it. Or, you can give it other text processing tasks. For example, try the following:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"falg0\"\u003eCopy a few paragraphs from a blog post (like this one) or an article.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"4retb\"\u003eSwitch over to the LLM Demo app.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"6ft8f\"\u003ePaste the copied text into the input box.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ed6e5\"\u003eType \u0026#34;Create a social media post for this content. Keep it short and sweet. Less than 50 words\u0026#34; and press enter.\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"ctusk\"\u003e\u003cb\u003eStep 5: Customize Gemma 3 (optional)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"jgss\"\u003eOne of the great things about the Gemma family of open weight models are the fine-tuned versions produced by the \u003ca href=\"https://huggingface.co/litert-community\"\u003emodeling community\u003c/a\u003e. Follow this \u003ca href=\"https://colab.sandbox.google.com/github/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb\"\u003eColab\u003c/a\u003e to see how you can use your own data to create your own version of Gemma 3 1B, quantize it, and get it running on mobile devices (CPU and GPU) in your own applications!\u003c/p\u003e\u003ch2 data-block-key=\"5v42q\"\u003e\u003cbr/\u003ePerformance\u003c/h2\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-jwzllq00_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/gemma-3-1B-performance.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n        \n            \u003cp\u003e\n                Create social media content locally in-browser using Gemma 3 1B\n            \u003c/p\u003e\n        \n    \n\u003c/div\u003e  \u003cp data-block-key=\"09pfg\"\u003eThe demo and measurements here are for the Gemma 3 1B model with int4 parameters quantized via quantized-aware training (QAT) which provides significant storage savings and increased decode throughput. The benchmarked Gemma 3 model supports multiple prefill lengths of 32, 128, 512 and 1024 and it uses a context length of 2048.\u003c/p\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2025-03-10_at_5.05.36PM.original.png\" alt=\"Measurements were taken on an Android Samsung Galaxy S24 Ultra with cpufreq governor set to performance.\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    Measurements were taken on an Android Samsung Galaxy S24 Ultra with cpufreq governor set to performance.\nObserved performance may vary depending on your phone’s hardware and current activity level.\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2025-03-10_at_5.05.47PM.original.png\" alt=\"Web performance measurements taken on MacBook Pro 2023 (Apple M3 Pro chip)\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    Measurements were taken on MacBook Pro 2023 (Apple M3 Pro chip)\nObserved performance may vary depending on your computer’s hardware and current activity level.\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"09pfg\"\u003eUnder the hood\u003c/h2\u003e\u003cp data-block-key=\"887g7\"\u003eThe performance results described above were achieved through extensive optimization efforts. These optimizations were designed to work well across open weight models, including Gemma. Here are some key features that significantly boosted performance and enabled new, reusable functionality.\u003c/p\u003e\u003cp data-block-key=\"3mcn4\"\u003e\u003cb\u003e\u003ci\u003eQuantization\u003c/i\u003e\u003c/b\u003e\u003ci\u003e:\u003c/i\u003e Quantization-aware training was applied to Gemma using a 4-bit integer channel-wise scheme on weights to maintain optimal performance, model quality, and size. In addition to weight quantization, we also dynamically quantize the activation to int8 during execution to best utilize CPU capability.\u003c/p\u003e\u003cp data-block-key=\"eocq0\"\u003e\u003cb\u003e\u003ci\u003eUpdating the KV Cache layouts\u003c/i\u003e\u003c/b\u003e: The KV cache is used in Transformer based models to store the key-value pairs from previous steps so they can be used to generate subsequent tokens. Reads and writes to the KV cache happen frequently so it is important that these operations are efficient. These operations were optimized by introducing a KV Cache layout to reduce extra transposes and reshapes. This optimization improved latency on Gemma models by approximately 25% for CPU and 20% for GPU. An extra operation was also added to more to performantly update the KV cache in-place on the GPU.\u003c/p\u003e\u003cp data-block-key=\"bfm38\"\u003e\u003cb\u003e\u003ci\u003eImproved Loading Time\u003c/i\u003e\u003c/b\u003e: To make the most of CPU and GPU processing, we use specialized tensor layouts. Generating these optimized weight layouts can take time, power and significant memory. During the first model load, the weights are cached on disk in their optimized format and subsequent loads read from the cache. If tensor layouts are further optimized, the existing cache will automatically be invalidated and the new format will be stored on disk during the next model load.\u003c/p\u003e\u003cp data-block-key=\"4ttjs\"\u003e\u003cb\u003e\u003ci\u003eGPU Weight Sharing\u003c/i\u003e\u003c/b\u003e: The LLM inference process has two phases: prefill and decode. These phases typically use separate resources for their respective models. To dramatically reduce the memory footprint of LLMs, both phases can share the same weights. While this technique isn\u0026#39;t entirely new, this is the first time it has been done in an easily reusable way in the LiteRT Runtime and GPU Delegate. For ops that support this feature, the GPU delegate checks if the weights are already present in GPU memory and can be reused. In the future, other models will be able to trivially take advantage of this capability.\u003c/p\u003e\u003ch2 data-block-key=\"fekll\"\u003e\u003cbr/\u003eWhat’s next\u003c/h2\u003e\u003cp data-block-key=\"8tvci\"\u003eDuring the development of Gemma 3, we focused on delivering excellent performance while also building reusable infrastructure for open weight models. In 2025, we plan to leverage this work to support a wider set of third-party models. With additional performance optimizations and an emphasis on further reducing memory use, we intend to continue making models more accessible on a wider range of devices. To keep up with the latest developments, set up notifications for \u003ca href=\"https://github.com/google-ai-edge/ai-edge-torch\"\u003eai_edge_torch\u003c/a\u003e on GitHub. More to come soon!\u003c/p\u003e\u003chr/\u003e\u003ch3 data-block-key=\"a9stm\"\u003eAcknowledgements\u003c/h3\u003e\u003cp data-block-key=\"6dvr4\"\u003e\u003csub\u003eAdvait Jain, Akshat Sharma, Alan Kelly, Andrei Kulik, Byungchul Kim, Chunlei Niu, Chun-nien Chan, Chuo-Ling Chang, Claudio Basile, Cormac Brick, Ekaterina Ignasheva, Eric Yang, Fengwu Yao, Frank Ban, Gerardo Carranza, Grant Jensen, Haoliang Zhang, Henry Wang, Ho Ko, Jae Yoo, Jiuqiang Tang, Juhyun Lee, Jun Jiang, Khanh LeViet, Kris Tonthat, Lin Chen, Lu Wang, Malini P V, Marissa Ikonomidis, Mark Sherwood, Matthew Soulanille, Matthias Grundmann, Mogan Shieh, Mohammadreza Heydary, Na Li, Pauline Sho, Pedro Gonnet, Ping Yu, Pulkit Bhuwalka, Quentin Khan, Ram Iyengar, Raman Sarokin, Rishika Sinha, Rishubh Khurana, Ronghui Zhu, Sachin Kotwani, Sebastian Schmidt, Steven Toribio, Suleman Shahid, T.J. Alumbaugh, Tenghui Zhu, Terry (Woncheol) Heo, Tyler Mullen, Vamsi Manchala, Vitalii Dziuba, Wai Hon Law, Weiyi Wang, Xu Chen, Yishuang Pang, Youchuan Hu, Yu-hui Chen, Zichuan Wei\u003c/sub\u003e\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-03-12T00:00:00Z",
  "modifiedTime": null
}
