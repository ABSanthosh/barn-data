{
  "id": "0d9a7938-1576-4992-93c3-d7d5dd35902e",
  "title": "We’re All Just Looking for Connection",
  "link": "https://slack.engineering/were-all-just-looking-for-connection/",
  "description": "We’ve been working to bring components of Quip’s technology into Slack with the canvas feature, while also maintaining the stand-alone Quip product. Quip’s backend, which powers both Quip and canvas, is written in Python. This is the story of a tricky bug we encountered last July and the lessons we learned along the way about… The post We’re All Just Looking for Connection appeared first on Engineering at Slack.",
  "author": "Brett Wines",
  "published": "Thu, 10 Oct 2024 21:39:15 +0000",
  "source": "https://slack.engineering/feed",
  "categories": [
    "Uncategorized",
    "database",
    "infrastructure",
    "networking",
    "python"
  ],
  "byline": "",
  "length": 9855,
  "excerpt": "We’ve been working to bring components of Quip’s technology into Slack with the canvas feature, while also maintaining the stand-alone Quip product. Quip’s backend, which powers both Quip and canvas, is written in Python. This is the story of a tricky bug we encountered last July and the lessons we learned along the way about…",
  "siteName": "Engineering at Slack",
  "favicon": "https://slack.engineering/wp-content/uploads/sites/7/2020/05/cropped-octothrope-1.png?w=192",
  "text": "We’ve been working to bring components of Quip’s technology into Slack with the canvas feature, while also maintaining the stand-alone Quip product. Quip’s backend, which powers both Quip and canvas, is written in Python. This is the story of a tricky bug we encountered last July and the lessons we learned along the way about being careful with TCP state. We hope that showing you how we tackled our bug helps you avoid — or find — similar bugs in the future!Our adventure began with a spike in EOFError errors during SQL queries. The errors were distributed across multiple services and multiple database hosts: Investigation The stacktrace showed an asyncio.IncompleteReadError, which we translate to EOFError, when reading the response from the database: File \"core/mysql.py\", line 299, in __read_result_set header = await self.__read_packet(timeout=timeout) File \"core/mysql.py\", line 532, in __read_packet header = await self.conn.read_exactly(4, timeout=timeout) File \"core/runtime_asyncio.py\", line 1125, in read_exactly raise EOFError() from None Here’s the relevant code, which had not been touched recently, along with some associated baffled commentary: async def _perform_query_locked(...) -\u003e core.sql.Result: ... if not self.conn.is_connected(): await self.__connect(...) await self.__send_command(...) result_set = await self.__read_result_set(...) # \u003c-- EOFError There are a few places where we close our connection to the database, e.g. if we see one of a certain set of errors. One initial hypothesis was that another Python coroutine closed the connection for its own reasons, after we issued the query but before we read the response. We quickly discarded that theory since connection access is protected with an in-memory lock, which is acquired at a higher level in the code. Since that would be impossible, we reasoned that the other side of the connection must have closed on us.This theory was bolstered when we found that our database proxies closed a bunch of connections at the time of the event. The closes were distributed independent of database host, which was consistent with the incident’s surface area. The proxy-close metric, ClientConnectionsClosed, represents closes initiated by either side, but we have a separate metric for when we initiate the close, and there was no increase in activity from us at the time. The timing here was extremely suspicious, although it still didn’t quite make sense why this should result in such a high rate of errors on the response read since we had just ensured connection state. Something clearly wasn’t adding up, so it was time to dig deeper. Suspecting the if not self.conn.is_connected(): line, we looked at our implementation for AsyncioConnection: class AsyncioConnection(core.runtime.Connection[memoryview]): def closed(self) -\u003e bool: return not self.writer or self.writer.is_closing() def is_connected(self) -\u003e bool: return self.connected Both of these functions turned out to be incorrect. First, let’s look at closed(). Each Asyncio connection has a StreamReader and a StreamWriter. Our code suggested that the closed state is only a function of the writer, which is controlled by the application server, not the reader. This seemed suspiciously incomplete to us since we expected only the reader to be aware of whether the other side closed the connection. We proved this with a unit test: async def test_reader_at_eof_vs_writer_is_closing(self): conn = await self.create_connection() # Ask the unit test's server to close await conn.write(self.encode_command(\"/quit\")) # Don't read. Still open since we haven't seen the # response yet self.assertFalse(conn.writer.is_closing()) # Read, then sleep(0) since it requires another run # through the scheduler loop so the stream can detect # the zero read/eof response = await conn.read_until(self.io_suffix) await core.runtime.sleep(0) self.assertTrue(conn.reader.at_eof()) # passes self.assertTrue(conn.writer.is_closing()) # fails Next, let’s look at is_connected(). First, it’s not a function of closed(), so the two have the potential to drift. Second, it could return a false positive: the only place where we set the instance variable self.connected to false is when the application server closes the connection, so similar to closed() it’s unaware of the reader being in the EOF state.We decided to log a metric so we could compare self.connected to not self.closed() to see exactly how much they drift. We found six more bugs, which led to one coworker coining the delightful phrase “when you go picking up rocks, you might find things under those rocks,” which we immediately adopted: is_connected() could also be a false negative. We saw that the false negatives only occurred on the services that maintain websocket connections to clients. The only place we set self.connected to true is when the application server initiates the connection, so it’s never set to true when it’s initiated by the client. There is a check we perform upon connection lock release to see if there is any data on the connection we haven’t read. It was incorrect to perform this check in some circumstances on websocket connections since it’s expected that the client can send data at any point in time. Our HTTP client pool could contain closed clients. We didn’t correctly handle exceptions during reconnect. We found another place where we should be removing connections from the pool. The Redis-specific cleanup we thought we would always do upon connection close wasn’t happening when the close wasn’t initiated by the application server. Resolution At this point we had all the pieces of the puzzle and were able to understand the sequence of events: The proxy closed database connections on us. After communicating with AWS we learned that they introduced behavior where the proxy closes connections after 24h even if it isn’t idle, so the closes were tied to our daily release, which restarts all of our servers. We failed to recognize the connections were closed since the close was not initiated by us, and as a result we did not reconnect prior to issuing SQL queries. We’d issue a query, then attempt to read the response, but the reader would be in the EOF state from the close, so it would raise EOFError. We deployed the fix and saw the near-total reduction in EOFError during SQL queries: As we deployed the fix for the case where connections are initiated by the client, the false negatives disappeared entirely, too: The Asyncio Migration The primary impact of this bug fix, however, wasn’t even the improved handling of connection closes. The primary impact was the unblocking of the Python runtime migration. The migration project began in 2020 and was from our custom runtime — we were early adopters of Python 3 — to asyncio, the standard Python IO library introduced in 3.4. The project ran smoothly and was enabled in all environments with just one exception: one service on our largest single-tenant cluster, since the customer would sporadically experience timeouts when exporting spreadsheets to PDF. The issue only occurred at times of peak load and was difficult to reproduce, so the project stalled.This PDF export handler makes an RPC request to another service that handles the PDF generation itself. That request goes over an existing connection from a connection pool, if one exists. If the RPC service is overloaded, it may close that connection, and if so we would fail to recognize that. The handler would fire off the RPC request, but the other side wouldn’t be listening, so that request would hang on write and eventually time out since the buffer would never get drained by the RPC service. For small exports we expect it would have raised an EOFError immediately instead of hanging since it wouldn’t completely fill the buffer.After launching the connection state fixes, we re-enabled asyncio on the final service, and confirmed that the errors didn’t recur. The asyncio project resumed, entered a very satisfying phase of deleting code in our custom runtime scheduler, and concluded shortly after. Conclusion This is tricky to get right! As we see from https://man7.org/linux/man-pages/man3/shutdown.3p.html a socket can be in a half-shutdown state. We had expected from the documentation that StreamWriter.is_closing() would encompass that half-shutdown state, but we should have been more careful: Return True if the stream is closed or in the process of being closed.In fact, both the default event loop and uvloop contain the same behavior as we did, where the closing state is only set upon client-initiated close. This explains why the unit test mentioned before failed both with uvloop enabled and disabled and why it was insufficient to depend only on StreamWriter.is_closing().When the other side of a TCP connection sends FIN, we enter the CLOSE WAIT state. But in that state we can still attempt to read for as long as we like. We’ll only exit that state when we tell the OS to close or shut down the socket. Above all, my personal takeaway is to always pick up rocks[1]. In this case it could have been tempting to simply call it a day after adding EOFError to the list of exceptions upon which we reissue the query to a failover database. But with low-level code, it’s usually worth the extra time and effort to dig a bit deeper if something just doesn’t quite seem right. Even if things are mostly fine on the surface, bugs down there can have far-reaching effects. Special thanks to Ross Cohen for his hard work and camaraderie through this rollercoaster of a bug. Couldn’t ask for a better buddy on this one! If you enjoyed this post, you might also like working here. Check out our careers page! [1] You might find things under those rocks!",
  "image": "https://slack.engineering/wp-content/uploads/sites/7/2024/10/54040277712_ab333a09d9_b.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\u003cdiv\u003e\u003cp\u003eWe’ve been working to bring components of Quip’s technology into Slack with the \u003ca href=\"https://slack.com/features/canvas\"\u003ecanvas\u003c/a\u003e feature, while also maintaining the stand-alone Quip product. Quip’s backend, which powers both Quip and canvas, is written in Python. This is the story of a tricky bug we encountered last July and the lessons we learned along the way about being careful with TCP state. We hope that showing you how we tackled our bug helps you avoid — or find — similar bugs in the future!\u003c/p\u003e\u003cp\u003eOur adventure began with a spike in \u003ccode\u003eEOFError\u003c/code\u003e errors during SQL queries. The errors were distributed across multiple services and multiple database hosts:\u003c/p\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" width=\"1035\" height=\"804\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image.jpg?w=640\" alt=\"A timeseries graph showing distribution of errors across DB group types.\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image.jpg 1035w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image.jpg?resize=640,497 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image.jpg?resize=768,597 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image.jpg?resize=380,295 380w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image.jpg?resize=800,621 800w\" sizes=\"(max-width: 1035px) 100vw, 1035px\"/\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eInvestigation\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cbr/\u003eThe stacktrace showed an \u003ccode\u003easyncio.IncompleteReadError\u003c/code\u003e, which we translate to \u003ccode\u003eEOFError\u003c/code\u003e, when reading the response from the database:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eFile \u0026#34;core/mysql.py\u0026#34;, line 299, in __read_result_set\n    header = await self.__read_packet(timeout=timeout)\nFile \u0026#34;core/mysql.py\u0026#34;, line 532, in __read_packet\n    header = await self.conn.read_exactly(4, timeout=timeout)\nFile \u0026#34;core/runtime_asyncio.py\u0026#34;, line 1125, in read_exactly\n    raise EOFError() from None\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere’s the relevant code, which had not been touched recently, along with some associated baffled commentary:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003easync def _perform_query_locked(...) -\u0026gt; core.sql.Result:\n    ...\n    if not self.conn.is_connected():\n        await self.__connect(...)\n        await self.__send_command(...)\n        result_set = await self.__read_result_set(...) # \u0026lt;-- EOFError\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv\u003e\u003cp\u003e\u003cimg decoding=\"async\" width=\"1114\" height=\"203\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image2.jpg?w=640\" alt=\"A Github comment from Brett: \u0026#34;wow, insane, before every query we check for EOFError preemptively and reconnect if needed, but the stacktrace shows us as failing in the call to __read_result_set(), so we must\u0026#39;ve already reconnected...\u0026#34;\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image2.jpg 1114w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image2.jpg?resize=640,117 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image2.jpg?resize=768,140 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image2.jpg?resize=380,69 380w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image2.jpg?resize=800,146 800w\" sizes=\"(max-width: 1114px) 100vw, 1114px\"/\u003e\u003c/p\u003e\u003cp\u003eThere are a few places where we close our connection to the database, e.g. if we see one of a certain set of errors. One initial hypothesis was that another Python coroutine closed the connection for its own reasons, after we issued the query but before we read the response. We quickly discarded that theory since connection access is protected with an in-memory lock, which is acquired at a higher level in the code. Since that would be impossible, we reasoned that the other side of the connection must have closed on us.\u003c/p\u003e\u003cp\u003eThis theory was bolstered when we found that our database proxies closed a bunch of connections at the time of the event. The closes were distributed independent of database host, which was consistent with the incident’s surface area. The proxy-close metric, \u003ccode\u003eClientConnectionsClosed\u003c/code\u003e, represents closes initiated by either side, but we have a separate metric for when we initiate the close, and there was no increase in activity from us at the time. The timing here was extremely suspicious, although it still didn’t quite make sense why this should result in such a high rate of errors on the response read since we had just ensured connection state.\u003c/p\u003e\u003c/div\u003e\n\u003cdiv\u003e\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"309\" height=\"267\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image3.jpg?w=309\" alt=\"A graph of the ClientConnectionsClosed metrics showing distinct spikes.\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"3356\" height=\"1444\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image4.jpg?w=640\" alt=\"A timeseries graph showing distribution of closes across proxies.\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image4.jpg 3356w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image4.jpg?resize=640,275 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image4.jpg?resize=768,330 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image4.jpg?resize=1280,551 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image4.jpg?resize=1536,661 1536w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image4.jpg?resize=2048,881 2048w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image4.jpg?resize=380,164 380w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image4.jpg?resize=800,344 800w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Image4.jpg?resize=1160,499 1160w\" sizes=\"auto, (max-width: 3356px) 100vw, 3356px\"/\u003e\u003c/p\u003e\u003cp\u003eSomething clearly wasn’t adding up, so it was time to dig deeper. Suspecting the \u003ccode\u003eif not self.conn.is_connected():\u003c/code\u003e line, we looked at our implementation for \u003ccode\u003eAsyncioConnection\u003c/code\u003e:\u003c/p\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode\u003eclass AsyncioConnection(core.runtime.Connection[memoryview]):\n    def closed(self) -\u0026gt; bool:\n        return not self.writer or self.writer.is_closing()\n\n    def is_connected(self) -\u0026gt; bool:\n        return self.connected\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBoth of these functions turned out to be incorrect. First, let’s look at \u003ccode\u003eclosed()\u003c/code\u003e. Each Asyncio connection has a \u003ca href=\"https://docs.python.org/3/library/asyncio-stream.html#streamreader\"\u003e\u003ccode\u003eStreamReader\u003c/code\u003e\u003c/a\u003e and a \u003ca href=\"https://docs.python.org/3/library/asyncio-stream.html#streamwriter\"\u003e\u003ccode\u003eStreamWriter\u003c/code\u003e\u003c/a\u003e. Our code suggested that the closed state is only a function of the writer, which is controlled by the application server, not the reader. This seemed suspiciously incomplete to us since we expected only the reader to be aware of whether the other side closed the connection. We proved this with a unit test:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003easync def test_reader_at_eof_vs_writer_is_closing(self):\n    conn = await self.create_connection()\n\n    # Ask the unit test\u0026#39;s server to close\n    await conn.write(self.encode_command(\u0026#34;/quit\u0026#34;))\n\n    # Don\u0026#39;t read. Still open since we haven\u0026#39;t seen the\n    # response yet\n    self.assertFalse(conn.writer.is_closing())\n\n    # Read, then sleep(0) since it requires another run\n    # through the scheduler loop so the stream can detect\n    # the zero read/eof\n    response = await conn.read_until(self.io_suffix)\n    await core.runtime.sleep(0)\n\n    self.assertTrue(conn.reader.at_eof()) # passes \n    self.assertTrue(conn.writer.is_closing()) # fails\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv\u003e\u003cp\u003eNext, let’s look at \u003ccode\u003eis_connected()\u003c/code\u003e. First, it’s not a function of \u003ccode\u003eclosed()\u003c/code\u003e, so the two have the potential to drift. Second, it could return a false positive: the only place where we set the instance variable \u003ccode\u003eself.connected\u003c/code\u003e to false is when the application server closes the connection, so similar to \u003ccode\u003eclosed()\u003c/code\u003e it’s unaware of the reader being in the EOF state.\u003c/p\u003e\u003cp\u003eWe decided to log a metric so we could compare \u003ccode\u003eself.connected\u003c/code\u003e to \u003ccode\u003enot self.closed()\u003c/code\u003e to see exactly how much they drift. We found six more bugs, which led to one coworker coining the delightful phrase “when you go picking up rocks, you might find things under those rocks,” which we immediately adopted:\u003c/p\u003e\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"3282\" height=\"448\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.39.40 PM.png?w=640\" alt=\"\" data-wp-editing=\"1\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.39.40 PM.png 3282w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.39.40 PM.png?resize=640,87 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.39.40 PM.png?resize=768,105 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.39.40 PM.png?resize=1280,175 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.39.40 PM.png?resize=1536,210 1536w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.39.40 PM.png?resize=2048,280 2048w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.39.40 PM.png?resize=380,52 380w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.39.40 PM.png?resize=800,109 800w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.39.40 PM.png?resize=1160,158 1160w\" sizes=\"auto, (max-width: 3282px) 100vw, 3282px\"/\u003e\u003c/p\u003e\u003c/div\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan\u003e\u003ccode\u003eis_connected()\u003c/code\u003e could also be a false negative. We saw that the false negatives only occurred on the services that maintain websocket connections to clients. The only place we set \u003ccode\u003eself.connected\u003c/code\u003e to true is when the application server initiates the connection, so it’s never set to true when it’s initiated by the client.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eThere is a check we perform upon connection lock release to see if there is any data on the connection we haven’t read. It was incorrect to perform this check in some circumstances on websocket connections since it’s expected that the client can send data at any point in time.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eOur HTTP client pool could contain closed clients.\u003c/li\u003e\n\u003cli\u003eWe didn’t correctly handle exceptions during reconnect.\u003c/li\u003e\n\u003cli\u003eWe found another place where we should be removing connections from the pool.\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eThe Redis-specific cleanup we thought we would always do upon connection close wasn’t happening when the close wasn’t initiated by the application server.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\u003cspan\u003eResolution\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003eAt this point we had all the pieces of the puzzle and were able to understand the sequence of events:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe proxy closed database connections on us. After communicating with AWS we learned that they introduced behavior where the proxy closes connections after 24h even if it isn’t idle, so the closes were tied to our daily release, which restarts all of our servers.\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eWe failed to recognize the connections were closed since the close was not initiated by us, and as a result we did not reconnect prior to issuing SQL queries.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eWe’d issue a query, then attempt to read the response, but the reader would be in the EOF state from the close, so it would raise \u003c/span\u003e\u003ccode\u003eEOFError\u003c/code\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eWe deployed the fix and saw the near-total reduction in \u003c/span\u003e\u003ccode\u003eEOFError\u003c/code\u003e\u003cspan\u003e during SQL queries:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"2314\" height=\"1924\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.13.11 PM.png?w=640\" alt=\"Three Kibana screenshots showing a near-total reduction in EOFError rate\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.13.11 PM.png 2314w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.13.11 PM.png?resize=640,532 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.13.11 PM.png?resize=768,639 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.13.11 PM.png?resize=1280,1064 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.13.11 PM.png?resize=1536,1277 1536w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.13.11 PM.png?resize=2048,1703 2048w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.13.11 PM.png?resize=380,316 380w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.13.11 PM.png?resize=800,665 800w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.13.11 PM.png?resize=1160,964 1160w\" sizes=\"auto, (max-width: 2314px) 100vw, 2314px\"/\u003e\u003c/p\u003e\n\u003cp\u003eAs we deployed the fix for the case where connections are initiated by the client, the false negatives disappeared entirely, too:\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"3356\" height=\"1572\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.08.57 PM.png?w=640\" alt=\"A timeseries graph showing a reduction in the discrepancy metric.\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.08.57 PM.png 3356w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.08.57 PM.png?resize=640,300 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.08.57 PM.png?resize=768,360 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.08.57 PM.png?resize=1280,600 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.08.57 PM.png?resize=1536,719 1536w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.08.57 PM.png?resize=2048,959 2048w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.08.57 PM.png?resize=380,178 380w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.08.57 PM.png?resize=800,375 800w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-9.08.57 PM.png?resize=1160,543 1160w\" sizes=\"auto, (max-width: 3356px) 100vw, 3356px\"/\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cimg decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/image8_43fdd2.png?w=640\" alt=\"\"/\u003e\u003cspan\u003eThe Asyncio Migration\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eThe primary impact of this bug fix, however, wasn’t even the improved handling of connection closes. The primary impact was the unblocking of the Python runtime migration.\u003c/span\u003e\u003c/p\u003e\n\u003cdiv\u003e\u003cp\u003eThe migration project began in 2020 and was from our custom runtime — we were early adopters of Python 3 — to \u003ca href=\"https://docs.python.org/3/library/asyncio.html\"\u003easyncio\u003c/a\u003e, the standard Python IO library introduced in 3.4. The project ran smoothly and was enabled in all environments with just one exception: one service on our largest single-tenant cluster, since the customer would sporadically experience timeouts when exporting spreadsheets to PDF. The issue only occurred at times of peak load and was difficult to reproduce, so the project stalled.\u003c/p\u003e\u003cp\u003eThis PDF export handler makes an RPC request to another service that handles the PDF generation itself. That request goes over an existing connection from a connection pool, if one exists. If the RPC service is overloaded, it may close that connection, and if so we would fail to recognize that. The handler would fire off the RPC request, but the other side wouldn’t be listening, so that request would hang on write and eventually time out since the buffer would never get drained by the RPC service. For small exports we expect it would have raised an \u003ccode\u003eEOFError\u003c/code\u003e immediately instead of hanging since it wouldn’t completely fill the buffer.\u003c/p\u003e\u003cp\u003eAfter launching the connection state fixes, we re-enabled asyncio on the final service, and confirmed that the errors didn’t recur. The asyncio project resumed, entered a very satisfying phase of deleting code in our custom runtime scheduler, and concluded shortly after.\u003c/p\u003e\u003c/div\u003e\n\u003ch3\u003e\u003cspan\u003eConclusion\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003eThis is tricky to get right! As we see from \u003ca href=\"https://man7.org/linux/man-pages/man3/shutdown.3p.html\"\u003ehttps://man7.org/linux/man-pages/man3/shutdown.3p.html\u003c/a\u003e a socket can be in a half-shutdown state. We had expected \u003ca href=\"https://docs.python.org/3/library/asyncio-stream.html#asyncio.StreamWriter.is_closing\"\u003efrom the documentation\u003c/a\u003e that \u003ccode\u003eStreamWriter.is_closing()\u003c/code\u003e would encompass that half-shutdown state, but we should have been more careful:\u003c/p\u003e\n\u003cdiv\u003e\u003cp\u003e\u003cem\u003eReturn True if the stream is closed or in the process of being closed.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eIn fact, both the \u003ca href=\"https://github.com/python/cpython/blob/v3.7.16/Lib/asyncio/selector_events.py#L658\"\u003edefault event loop\u003c/a\u003e and \u003ca href=\"https://github.com/MagicStack/uvloop/blob/v0.20.0/uvloop/handles/basetransport.pyx#L256\"\u003euvloop\u003c/a\u003e contain the same behavior as we did, where the closing state is only set upon client-initiated close. This explains why the unit test mentioned before failed both with uvloop enabled and disabled and why it was insufficient to depend only on \u003ccode\u003eStreamWriter.is_closing()\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eWhen the other side of a TCP connection sends \u003ccode\u003eFIN\u003c/code\u003e, we enter the \u003ccode\u003eCLOSE WAIT\u003c/code\u003e state. But in that state we can still attempt to read for as long as we like. We’ll only exit that state when we tell the OS to close or shut down the socket.\u003c/p\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1970\" height=\"1540\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-8.49.18 PM.png?w=640\" alt=\"\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-8.49.18 PM.png 1970w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-8.49.18 PM.png?resize=640,500 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-8.49.18 PM.png?resize=768,600 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-8.49.18 PM.png?resize=1280,1001 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-8.49.18 PM.png?resize=1536,1201 1536w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-8.49.18 PM.png?resize=380,297 380w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-8.49.18 PM.png?resize=800,625 800w, https://slack.engineering/wp-content/uploads/sites/7/2024/10/Screenshot-2024-10-04-at-8.49.18 PM.png?resize=1160,907 1160w\" sizes=\"auto, (max-width: 1970px) 100vw, 1970px\"/\u003e\u003c/p\u003e\n\u003cp\u003eAbove all, my personal takeaway is to always pick up rocks[1]. In this case it could have been tempting to simply call it a day after adding \u003ccode\u003eEOFError\u003c/code\u003e to the list of exceptions upon which we reissue the query to a failover database. But with low-level code, it’s usually worth the extra time and effort to dig a bit deeper if something just doesn’t quite seem right. Even if things are mostly fine on the surface, bugs down there can have far-reaching effects.\u003c/p\u003e\n\u003cp\u003eSpecial thanks to Ross Cohen for his hard work and camaraderie through this rollercoaster of a bug. Couldn’t ask for a better buddy on this one!\u003c/p\u003e\n\u003cp\u003eIf you enjoyed this post, you might also like working here. Check out our \u003ca href=\"https://slack.com/careers\"\u003ecareers page\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003e[1]\u003ci\u003e\u003cspan\u003e You might find things under those rocks!\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2024-10-10T21:39:15Z",
  "modifiedTime": "2024-10-11T17:37:11Z"
}
