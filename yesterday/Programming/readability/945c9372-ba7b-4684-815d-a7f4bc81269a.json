{
  "id": "945c9372-ba7b-4684-815d-a7f4bc81269a",
  "title": "QCon SF 2024 - Incremental Data Processing at Netflix",
  "link": "https://www.infoq.com/news/2024/11/qcon-sf-netflix-incremental-proc/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Jun He gave a talk at QCon SF 2024 titled Efficient Incremental Processing with Netflix Maestro and Apache Iceberg. He showed how Netflix used the system to reduce processing time and cost while improving data freshness. By Anthony Alford",
  "author": "Anthony Alford",
  "published": "Mon, 25 Nov 2024 14:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Big Data",
    "Netflix",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Anthony Alford",
  "length": 3527,
  "excerpt": "Jun He gave a talk at QCon SF 2024 titled Efficient Incremental Processing with Netflix Maestro and Apache Iceberg. He showed how Netflix used the system to reduce processing time and cost while impro",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20241119073511/apple-touch-icon.png",
  "text": "Jun He gave a talk at QCon SF 2024 titled Efficient Incremental Processing with Netflix Maestro and Apache Iceberg. He showed how Netflix used the system to reduce processing time and cost while improving data freshness. He, staff software engineer in the Big Data Orchestration team at Netflix, began with a discussion of how Netflix uses their data to inform many decisions and features, from recommending movies for users to watch to deciding to terminate production of a show. These insights are sourced from many data pipelines and ML workflows. With this volume of data and processing come three challenges: data accuracy, data freshness, and cost efficiency. He used the scenario of late-arriving data events to illustrate all three. Because Netflix sources events asynchronously from user devices such as laptops and mobile phones, these events may be ingested or processed much later than the time when the event occurred. Until the event is processed, the data is inaccurate and \"stale,\" and if the system is designed such that fixing the data requires re-processing large numbers of events, that drives cost inefficiencies. Incremental processing can address all these challenges, but it has two requirements: capturing incremental state changes and tracking whether a change has been processed. He then began to describe Netflix's solution to this problem: Incremental Processing Support (IPS). This system is built using two major components: Apache Iceberg and Maestro. Iceberg is a \"high-performance format for huge analytic tables.\" For Netflix, using Iceberg simplifies a lot of data management tasks; in particular, it supports change capture with needing to read the data. Overall, Netflix manages \"more than one million tables\" using Iceberg, with \"hundreds of thousands of workflows\" operating on the data. To manage and orchestrate those workflows, Netflix developed Maestro, a \"general-purpose workflow orchestrator.\" Netflix created Maestro instead of adopting another workflow orchestrator such as Airflow, in part because of their large scale needs. Maestro also integrates well with other Netflix tools such as Metaflow. The typical change capture interface in IPS is a table, which matches the schema of the \"main\" table but contains only changed data; this would be used in a SQL query that performs a JOIN with the change capture table to only process changed data. However, not all existing workflows could use this interface, so the team developed two additional primitives: IpCapture and IpCommit. These could be added at the beginning and end, respectively, of an existing workflow, with no other modification required. He walked through several examples of how workflows could adopt IPS with minimal modification and pointed out that some workflows reduced their processing costs to as little as 10% of their original cost. After launching IPS, the Netflix team noticed three common usage patterns emerge: appending incrementally processed data to a target table; using changed data as a \"row-level filter\" to reduce transformations; and using the range parameters of the changed data in business logic. When He took questions from the audience after his talk, one question was about the cost efficiency of the different usage patterns. He noted that the first two patterns usually had more improved costs compared to the third. About the Author Anthony Alford",
  "image": "https://res.infoq.com/news/2024/11/qcon-sf-netflix-incremental-proc/en/headerimage/generatedHeaderImage-1732377586382.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eJun He gave a talk at QCon SF 2024 titled \u003ca href=\"https://qconsf.com/presentation/nov2024/efficient-incremental-processing-netflix-maestro-and-apache-iceberg\"\u003eEfficient Incremental Processing with Netflix Maestro and Apache Iceberg\u003c/a\u003e. He showed how Netflix used the system to reduce processing time and cost while improving data freshness.\u003c/p\u003e\n\n\u003cp\u003eHe, \u003ca href=\"https://www.linkedin.com/in/jheua/\"\u003estaff software engineer\u003c/a\u003e in the Big Data Orchestration team at Netflix, began with a discussion of how Netflix uses their data to inform many decisions and features, from recommending movies for users to watch to deciding to terminate production of a show. These insights are sourced from many data pipelines and ML workflows.\u003c/p\u003e\n\n\u003cp\u003eWith this volume of data and processing come three challenges: data accuracy, data freshness, and cost efficiency. He used the scenario of late-arriving data events to illustrate all three. Because Netflix sources events asynchronously from user devices such as laptops and mobile phones, these events may be ingested or processed much later than the time when the event occurred. Until the event is processed, the data is inaccurate and \u0026#34;stale,\u0026#34; and if the system is designed such that fixing the data requires re-processing large numbers of events, that drives cost inefficiencies.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"Late Arriving Event\" data-src=\"news/2024/11/qcon-sf-netflix-incremental-proc/en/resources/1qcon-sf-netflix-event-1732377742637.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/11/qcon-sf-netflix-incremental-proc/en/resources/1qcon-sf-netflix-event-1732377742637.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eIncremental processing can address all these challenges, but it has two requirements: capturing incremental state changes and tracking whether a change has been processed. He then began to describe Netflix\u0026#39;s solution to this problem: Incremental Processing Support (IPS). This system is built using two major components: \u003ca href=\"https://iceberg.apache.org/\"\u003eApache Iceberg\u003c/a\u003e and \u003ca href=\"https://github.com/Netflix/maestro\"\u003eMaestro\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eIceberg is a \u0026#34;high-performance format for huge analytic tables.\u0026#34; For Netflix, using Iceberg simplifies a lot of data management tasks; in particular, it supports change capture with needing to read the data. Overall, Netflix manages \u0026#34;more than one million tables\u0026#34; using Iceberg, with \u0026#34;hundreds of thousands of workflows\u0026#34; operating on the data.\u003c/p\u003e\n\n\u003cp\u003eTo manage and orchestrate those workflows, Netflix developed Maestro, a \u0026#34;general-purpose workflow orchestrator.\u0026#34; Netflix created Maestro instead of adopting another workflow orchestrator such as Airflow, in part because of their large scale needs. Maestro also integrates well with other Netflix tools such as Metaflow.\u003c/p\u003e\n\n\u003cp\u003eThe typical change capture interface in IPS is a table, which matches the schema of the \u0026#34;main\u0026#34; table but contains only changed data; this would be used in a SQL query that performs a JOIN with the change capture table to only process changed data. However, not all existing workflows could use this interface, so the team developed two additional primitives: IpCapture and IpCommit. These could be added at the beginning and end, respectively, of an existing workflow, with no other modification required. He walked through several examples of how workflows could adopt IPS with minimal modification and pointed out that some workflows reduced their processing costs to as little as 10% of their original cost.\u003c/p\u003e\n\n\u003cp\u003eAfter launching IPS, the Netflix team noticed three common usage patterns emerge: appending incrementally processed data to a target table; using changed data as a \u0026#34;row-level filter\u0026#34; to reduce transformations; and using the range parameters of the changed data in business logic. When He took questions from the audience after his talk, one question was about the cost efficiency of the different usage patterns. He noted that the first two patterns usually had more improved costs compared to the third.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Anthony-Alford\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAnthony Alford\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-11-25T00:00:00Z",
  "modifiedTime": null
}
