{
  "id": "d3f8b6bb-1023-4479-9964-58ee3ada6b12",
  "title": "Microsoft and Tsinghua University Present DIFF Transformer for LLMs",
  "link": "https://www.infoq.com/news/2024/10/microsoft-diff-transformer/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Researchers from Microsoft AI and Tsinghua University have introduced a new architecture called the Differential Transformer (DIFF Transformer), aimed at improving the performance of large language models. This model enhances attention mechanisms by refining how models handle context and minimizing distractions from irrelevant information. By Daniel Dominguez",
  "author": "Daniel Dominguez",
  "published": "Sun, 20 Oct 2024 10:47:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Large language models",
    "Artificial Intelligence",
    "Microsoft",
    "Microsoft Research",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Daniel Dominguez",
  "length": 2847,
  "excerpt": "Researchers from Microsoft AI and Tsinghua University have introduced a new architecture called the Differential Transformer (DIFF Transformer), aimed at improving the performance of large language mo",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241001113644/apple-touch-icon.png",
  "text": "Researchers from Microsoft AI and Tsinghua University have introduced a new architecture called the Differential Transformer (DIFF Transformer), aimed at improving the performance of large language models. This model enhances attention mechanisms by refining how models handle context and minimizing distractions from irrelevant information. The key feature of the DIFF Transformer is its differential attention mechanism. It computes attention by comparing two separate attention maps, which helps the model focus more effectively on relevant parts of the input. This adjustment improves accuracy, particularly in tasks like question answering and text summarization. The architecture also improves scalability, achieving similar performance to larger models with fewer training resources. This efficiency is beneficial for handling longer sequences of data, making it suitable for tasks that require processing large amounts of information at once. Experiments show that the DIFF Transformer consistently surpasses traditional transformers in tasks like language modeling and information retrieval, offering improved performance and efficiency in large language models (LLMs). Its design enhances practical applications such as long-context modeling, key information retrieval, hallucination mitigation, and in-context learning, while also reducing activation outliers. These improvements lead to better accuracy across diverse datasets and greater robustness to changes in input order, making the DIFF Transformer more suitable for low-resource environments. The following table compares the zero-shot performance of the DIFF Transformer with several well-trained Transformer models, including OpenLLaMA-v2-3B, StableLM-base-alpha-3B-v2, and StableLM-3B-4E1T and the DIFF Transformer shows better or comparable results. Enthusiasts and professionals have shown interest in its real world application, particularly in scenarios where prediction accuracy might justify increased computational resources. Data Science Kuldeep Singh shares on X: While Google's Transformer might have introduced \"Attention is all you need,\" Microsoft  and Tsinghua_Uni are here with the DIFF Transformer, stating, \"Sparse-Attention is all you need.\" AI Researcher Manu Otel wrote: But, the diff transformer comes with a small tradeoff, it has double the key heads. Discussions around the DIFF Transformer highlight a trade-off between computational cost and prediction accuracy. The model's need to perform attention operations twice could slow down both training and inference, but there's speculation on whether this could lead to better results with fewer training iterations or less data. About the Author Daniel Dominguez",
  "image": "https://res.infoq.com/news/2024/10/microsoft-diff-transformer/en/headerimage/generatedHeaderImage-1729349674552.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eResearchers from \u003ca href=\"https://www.microsoft.com/en-us/ai\"\u003eMicrosoft AI\u003c/a\u003e and \u003ca href=\"https://www.tsinghua.edu.cn/en/\"\u003eTsinghua University\u003c/a\u003e have introduced a new architecture called the \u003ca href=\"https://arxiv.org/pdf/2410.05258\"\u003eDifferential Transformer (DIFF Transformer)\u003c/a\u003e, aimed at improving the performance of large language models. This model enhances attention mechanisms by refining how models handle context and minimizing distractions from irrelevant information.\u003c/p\u003e\n\n\u003cp\u003eThe key feature of the \u003ca href=\"https://arxiv.org/html/2410.05258v1\"\u003eDIFF Transformer\u003c/a\u003e is its differential attention mechanism. It computes attention by comparing two separate attention maps, which helps the model focus more effectively on relevant parts of the input. This adjustment improves accuracy, particularly in tasks like question answering and text summarization.\u003c/p\u003e\n\n\u003cp\u003eThe architecture also improves scalability, achieving similar performance to larger models with fewer training resources. This efficiency is beneficial for handling longer sequences of data, making it suitable for tasks that require processing large amounts of information at once.\u003c/p\u003e\n\n\u003cp\u003eExperiments show that the \u003ca href=\"https://arxiv.org/html/2410.05258v1\"\u003eDIFF Transformer\u003c/a\u003e consistently surpasses traditional transformers in tasks like \u003ca href=\"https://en.wikipedia.org/wiki/Language_model\"\u003elanguage modeling\u003c/a\u003e and \u003ca href=\"https://en.wikipedia.org/wiki/Information_retrieval\"\u003einformation retrieval\u003c/a\u003e, offering improved performance and efficiency in large language models (LLMs). Its design enhances practical applications such as long-context modeling, key information retrieval, hallucination mitigation, and in-context learning, while also reducing activation outliers. These improvements lead to better accuracy across diverse datasets and greater robustness to changes in input order, making the \u003ca href=\"https://arxiv.org/html/2410.05258v1\"\u003eDIFF Transformer\u003c/a\u003e more suitable for low-resource environments.\u003c/p\u003e\n\n\u003cp\u003eThe following table compares the zero-shot performance of the DIFF Transformer with several well-trained Transformer models, including \u003ca href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\"\u003eOpenLLaMA-v2-3B\u003c/a\u003e, \u003ca href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b-v2\"\u003eStableLM-base-alpha-3B-v2\u003c/a\u003e, and \u003ca href=\"https://huggingface.co/stabilityai/stablelm-3b-4e1t\"\u003eStableLM-3B-4E1T\u003c/a\u003e and the DIFF Transformer shows better or comparable results.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2410.05258\"\u003e\u003cimg alt=\"\" data-src=\"news/2024/10/microsoft-diff-transformer/en/resources/1table-1729349959930.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/10/microsoft-diff-transformer/en/resources/1table-1729349959930.png\" rel=\"share\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eEnthusiasts and professionals have \u003ca href=\"https://huggingface.co/papers/2410.05258\"\u003eshown interest\u003c/a\u003e in its real world application, particularly in scenarios where prediction accuracy might justify increased computational resources.\u003c/p\u003e\n\n\u003cp\u003eData Science \u003ca href=\"https://x.com/kuldeep_s_s/status/1846271917558555084\"\u003eKuldeep Singh\u003c/a\u003e shares on \u003ca href=\"https://x.com/kuldeep_s_s/status/1846271917558555084\"\u003eX\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhile Google\u0026#39;s Transformer might have introduced \u0026#34;Attention is all you need,\u0026#34; Microsoft  and Tsinghua_Uni are here with the DIFF Transformer, stating, \u0026#34;Sparse-Attention is all you need.\u0026#34;\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAI Researcher \u003ca href=\"https://x.com/manuotel/status/1846090104122888461\"\u003eManu Otel\u003c/a\u003e wrote:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eBut, the diff transformer comes with a small tradeoff, it has double the key heads.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.reddit.com/r/MachineLearning/comments/1g13gkd/r_differential_transformer/\"\u003eDiscussions\u003c/a\u003e around the DIFF Transformer highlight a trade-off between computational cost and prediction accuracy. The model\u0026#39;s need to perform attention operations twice could slow down both training and inference, but there\u0026#39;s speculation on whether this could lead to better results with fewer training iterations or less data.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Daniel-Dominguez\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eDaniel Dominguez\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-10-20T00:00:00Z",
  "modifiedTime": null
}
