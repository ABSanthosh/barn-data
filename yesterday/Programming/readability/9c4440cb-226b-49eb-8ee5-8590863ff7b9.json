{
  "id": "9c4440cb-226b-49eb-8ee5-8590863ff7b9",
  "title": "Meta Open-Sources Large Concept Model, a Language Model That Predicts Entire Sentences",
  "link": "https://www.infoq.com/news/2025/01/meta-large-concept-model/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Meta recently open-sourced Large Concept Model (LCM), a language model designed to operate at a higher abstraction level than tokens. Instead, LCM uses a sentence embedding space that is independent of language and modality and can outperform a similarly-sized Llama 3.1 model on multilingual summarization tasks. By Anthony Alford",
  "author": "Anthony Alford",
  "published": "Tue, 28 Jan 2025 14:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Large language models",
    "Generative AI",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Anthony Alford",
  "length": 3516,
  "excerpt": "Meta recently open-sourced Large Concept Model (LCM), a language model designed to operate at a higher abstraction level than tokens. Instead, LCM uses a sentence embedding space that is independent o",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250128073913/apple-touch-icon.png",
  "text": "Meta recently open-sourced Large Concept Model (LCM), a language model designed to operate at a higher abstraction level than tokens. Instead, LCM uses a sentence embedding space that is independent of language and modality and can outperform a similarly-sized Llama 3.1 model on multilingual summarization tasks. Unlike most LLMs, which map text into a token embedding space and generate text autoregressively by predicting the next token in a sequence, LCM operates at the sentence level. LCM uses the pre-trained SONAR sentence embedding model, which supports both text (in 200 languages) and speech data (in 76 languages). Meta developed LCM to better model the human ability to do abstract and hierarchical reasoning. It can also help the model in dealing with long-form content: in zero-shot tests on the XLSum benchmark, a 7B parameter LCM outperformed Llama-3.1-8B. According to Meta: We see the models and results discussed in this paper as a step towards increasing scientific diversity and a move away from current best practice in large scale language modeling. We acknowledge that there is still a long path to reach the performance of current flagship LLMs. This will require of course further improving the core architecture, but also careful data selection and curation, extensive ablations, optimized and diverse instruction fine-tuning, and finally, scaling to models with more than 70B parameters. The LCM architecture is based on the SONAR embedding space and the SONAR encoders and decoders for both speech and text. LCM uses a \"standard decoder-only Transformer\" architecture to predict the next item in a sequence. One advantage of using SONAR is that the output sequence can be decoded into any supported language or modality without re-generating the sequence. It can also be fine-tuned on a subset of the languages while still exhibiting good zero-shot performance on tasks using other languages. Meta did several experiments and evaluations of the 7B parameter LCM on long-form text summarization and summary expansion tasks and compared its performance to similarly-sized baseline models including Gemma-7B, Llama-3.1-8B and Mistral-7B. Because these tasks are difficult to score automatically, Meta used several different metrics for each task, such as ROUGE-L for similarity and Seahorse-Large-Q4 for attribution. LCM outperformed the other models on the grammaticality metric, which measures the amount of duplication in the output. In a Hacker News discussion about LCM, some readers expressed scepticism; one said it \"feels like a failure to learn the bitter lesson.\" But Chaitanya Chokkareddy, chief innovation officer of Ozonetel Systems, noted that his company is doing similar research: This maps a little to what we are doing research on what we are calling as shape of stories. We can clearly see in 2D space itself how different \"concepts\" are explored. Using the shape of stories for semantic chunking we can clearly see in multiple articles how we can chunk by \"concepts\". Now we are trying to see if we can just use these chunks and train a next \"chunk\" predictor instead of a next word predictor. In the paper, they take a sentence to mean a concept. We believe that a \"semantic chunk\" is better suited for a concept instead of a sentence. The LCM implementation and experiment code is available on GitHub. About the Author Anthony Alford",
  "image": "https://res.infoq.com/news/2025/01/meta-large-concept-model/en/headerimage/generatedHeaderImage-1737210739800.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://ai.meta.com/\"\u003eMeta\u003c/a\u003e recently open-sourced \u003ca href=\"https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/\"\u003eLarge Concept Model\u003c/a\u003e (LCM), a language model designed to operate at a higher abstraction level than tokens. Instead, LCM uses a sentence embedding space that is independent of language and modality and can outperform a similarly-sized Llama 3.1 model on multilingual summarization tasks.\u003c/p\u003e\n\n\u003cp\u003eUnlike most LLMs, which map text into a token embedding space and generate text autoregressively by predicting the next token in a sequence, LCM operates at the sentence level. LCM uses the pre-trained \u003ca href=\"https://github.com/facebookresearch/SONAR\"\u003eSONAR\u003c/a\u003e sentence embedding model, which supports both text (in 200 languages) and speech data (in 76 languages). Meta developed LCM to better model the human ability to do abstract and hierarchical reasoning. It can also help the model in dealing with long-form content: in zero-shot tests on the \u003ca href=\"https://paperswithcode.com/dataset/xsum\"\u003eXLSum\u003c/a\u003e benchmark, a 7B parameter LCM outperformed Llama-3.1-8B. According to Meta:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWe see the models and results discussed in this paper as a step towards increasing scientific diversity and a move away from current best practice in large scale language modeling. We acknowledge that there is still a long path to reach the performance of current flagship LLMs. This will require of course further improving the core architecture, but also careful data selection and curation, extensive ablations, optimized and diverse instruction fine-tuning, and finally, scaling to models with more than 70B parameters.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe LCM architecture is based on the SONAR embedding space and the SONAR encoders and decoders for both speech and text. LCM uses a \u0026#34;standard decoder-only Transformer\u0026#34; architecture to predict the next item in a sequence. One advantage of using SONAR is that the output sequence can be decoded into any supported language or modality without re-generating the sequence. It can also be fine-tuned on a subset of the languages while still exhibiting good zero-shot performance on tasks using other languages.\u003c/p\u003e\n\n\u003cp\u003eMeta did several experiments and evaluations of the 7B parameter LCM on long-form text summarization and summary expansion tasks and compared its performance to similarly-sized baseline models including Gemma-7B, Llama-3.1-8B and Mistral-7B. Because these tasks are difficult to score automatically, Meta used several different metrics for each task, such as \u003ca href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)\"\u003eROUGE-L\u003c/a\u003e for similarity and \u003ca href=\"https://huggingface.co/google/seahorse-large-q4\"\u003eSeahorse-Large-Q4\u003c/a\u003e for attribution. LCM outperformed the other models on the grammaticality metric, which measures the amount of duplication in the output.\u003c/p\u003e\n\n\u003cp\u003eIn a Hacker News \u003ca href=\"https://news.ycombinator.com/item?id=42563534\"\u003ediscussion about LCM\u003c/a\u003e, some readers expressed scepticism; one said it \u0026#34;feels like a failure to learn the bitter lesson.\u0026#34; But Chaitanya Chokkareddy, chief innovation officer of \u003ca href=\"https://ozonetel.com/\"\u003eOzonetel Systems\u003c/a\u003e, noted that his company is doing similar research:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis maps a little to what we are doing research on what we are calling as shape of stories. We can clearly see in 2D space itself how different \u0026#34;concepts\u0026#34; are explored. Using the shape of stories for semantic chunking we can clearly see in multiple articles how we can chunk by \u0026#34;concepts\u0026#34;. Now we are trying to see if we can just use these chunks and train a next \u0026#34;chunk\u0026#34; predictor instead of a next word predictor. In the paper, they take a sentence to mean a concept. We believe that a \u0026#34;semantic chunk\u0026#34; is better suited for a concept instead of a sentence.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe LCM \u003ca href=\"https://github.com/facebookresearch/large_concept_model?tab=readme-ov-file\"\u003eimplementation and experiment code\u003c/a\u003e is available on GitHub.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Anthony-Alford\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAnthony Alford\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-01-28T00:00:00Z",
  "modifiedTime": null
}
