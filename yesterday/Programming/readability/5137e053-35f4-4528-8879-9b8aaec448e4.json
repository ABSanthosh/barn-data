{
  "id": "5137e053-35f4-4528-8879-9b8aaec448e4",
  "title": "Behind the Scenes: Building a Robust Ads Event Processing Pipeline",
  "link": "https://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Fri, 09 May 2025 19:44:04 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": null,
  "byline": "Netflix Technology Blog",
  "length": 12496,
  "excerpt": "In a digital advertising platform, a robust feedback system is essential for the lifecycle and success of an ad campaign. This system comprises of diverse sub-systems designed to monitor, measure…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "Kinesh SatiyaIntroductionIn a digital advertising platform, a robust feedback system is essential for the lifecycle and success of an ad campaign. This system comprises of diverse sub-systems designed to monitor, measure, and optimize ad campaigns. At Netflix, we embarked on a journey to build a robust event processing platform that not only meets the current demands but also scales for future needs. This blog post delves into the architectural evolution and technical decisions that underpin our Ads event processing pipeline.Ad serving acts like the “brain” — making decisions, optimizing delivery and ensuring right Ad is shown to the right member at the right time. Meanwhile, ad events, after an Ad is rendered, function like “heartbeats”, continuously providing real-time feedback (oxygen/nutrients) that fuels better decision-making, optimizations, reporting, measurement, and billing. Expanding on this analogy:Just as the brain relies on continuous blood flow, ad serving depends on a steady stream of ad events to adjust next ad serving decision, frequency capping, pacing, and personalization.If the nervous system stops sending signals (ad events stop flowing), the brain (ad serving) lacks critical insights and starts making poor decisions or even fails.The healthier and more accurate the event stream (just like strong heart function), the better the ad serving system can adapt, optimize, and drive business outcomes.Let’s dive into the journey of building this pipeline.The PilotIn November 2022, we launched a brand new basic ads plan, in partnership with Microsoft. The software systems extended the existing Netflix playback systems to play ads. Initially, the system was designed to be simple, secure, and efficient, with an underlying ethos of device-originated and server-proxied operations. The system consisted of three main components: the Microsoft Ad Server, Netflix Ads Manager, and Ad Event Handler. Each ad served required tracking to ensure the feedback loop functioned effectively, providing the external ad server with insights on impressions, frequency capping (advertiser policy that limits the number of times a user sees a specific ad), and monetization processes.Key features of this system include:Client Request: Client devices request for ads during an ad break from Netflix playback systems, which is then decorated with information by ads manager to request ads from the ad server.Server-Side Ad Insertion: The Ad Server sends ad responses using the VAST (Video Ad Serving Template) format.Netflix Ads Manager: This service parses VAST documents, extracts tracking event information, and creates a simplified response structure for Netflix playback systems and client devices. — The tracking information is packed into a structured protobuf data model. — This structure is encrypted to create an opaque token. — The final response, informs the client devices, when to send an event and the corresponding token.Client Device: During ad playback, client devices send events accompanied by a token. The Netflix telemetry system then enqueues all these events in Kafka for asynchronous processing.Ads Event Handler: This component is a Kafka consumer, that reads/decrypts the event payload and forwards the tracking information encoded back to the ad server and other vendors.Fig 1: Basic Ad Event Handling SystemThere is an excellent prior blog post that explains how this systems was tested end-to-end at scale. This system design allowed us to quickly add new integrations for verification with vendors like DV, IAS and Nielsen for measurement.The ExpansionAs we continued to expand our third-party (3P) advertising vendors for measurement, tracking and verification, we identified a critical trend: growth in the volume of data encapsulated within opaque tokens. These tokens, which are cached on client devices, present a risk of elevated memory usage, potentially impacting device performance. We also anticipated increase in third-party tracking URLs, metadata needs, and more event types as our business added new capabilities.To strategically address these challenges, we introduced a new persistence layer using Key-Value abstraction, between ad serving and event handling system: Ads Metadata Registry. This transient storage service stores metadata for each Ad served, and upon callback, event handler would read the tracking information to relay information to the vendors. The contract between the client device and Ads systems continues to use the opaque token per event, but now, instead of tracking information, it contains reference identifiers — Ad ID, the corresponding metadata record ID in the registry and the event name. This approach future proofed our systems to handle any growth in data that needs to pass from ad serving to event handling systems.Fig 2: Storage service between Ad Serving \u0026 ReportingThe EvolutionIn January of 2024, we decided to invest in in-house advertising technology platform. This implied that the event processing pipeline had to evolve significantly — attain parity with existing offerings and continue to support new product launches with rapid iterations using in-house Netflix Ad Server. This required re-evaluation of the entire architecture across all of Ads engineering teams.First, we made an inventory of the use-cases that would need to be supported through ad events.We’d need to start supporting frequency capping in-house for all ads through Netflix Ad server.Incorporate pricing information for impressions to set the stage for billing events, which are used to charge advertisers.A robust reporting system to share campaign reports with advertisers, combined with metrics data collection, helps assess the delivery and effectiveness of the campaign.Scale event handler to perform tracking information look-ups across different vendors.Next, we examined upcoming launches, such as Pause/Display ads, to gain deeper insights into our strategic initiatives. We recognized that Display Ads would utilize a distinct logging framework, suggesting that different upstream pipelines might deliver ad telemetry. However, the downstream use-cases were expected to remain largely consistent. Additionally, by reviewing the goals of our telemetry teams, we saw large initiatives aimed at upgrading the platform, indicating potential future migrations.Keeping the above insights \u0026 challenges in mind,We planned a centralized ad event collection system. This centralized service would consolidate common operations like decryption of tokens, enrichment, hashing identifiers into a single step execution and provide a single unified data contract to consumers that is highly extensible (like being agnostic to ad server \u0026 ad media).We proposed moving all consumers of ad telemetry downstream of the centralized service. This creates a clean separation between upstream systems and consumers in Ads Engineering.In the initial development phase of our advertising system, a crucial component was the creation of ad sessions based on individual ad events. This system was constructed using ad playback telemetry, which allowed us to gather essential metrics from these ad sessions. A significant decision in this plan was to position the ad sessionization process downstream of the raw ad events.The proposal also recommended moving all our Ads data processing pipelines for reporting/analytics/metrics for Ads using the data published by the centralized system.Putting together all the components in our vision -Fig 3: Ad Event processing pipelineKey components on event processing pipeline -Ads Event Publisher: This centralized system is responsible for collecting ads telemetry and providing unified ad events to the ads engineering teams. It supports various functions such as measurement, finance/billing, reporting, frequency capping, and maintaining an essential feedback loop back to the ad server.Realtime ConsumersFrequency Capping: This system tracks impressions for each campaign, profile, and any other frequency capping parameters set up for the campaign. It is utilized by the Ad Server during each ad decision to ensure ads are served with frequency limits.Ads Metrics: This component is a Flink job that transforms raw data to a set of dimensions and metrics, subsequently writing to Apache Druid OLAP database. The streaming data is further backed by an offline process that corrects any inaccuracy during streaming ingestion and providing accurate metrics. It provides real-time metrics to assess the delivery health of campaigns and applies budget capping functionality.Ads Sessionizer: An Apache Flink job that consolidates all events related to a single ad into an Ad Session. This session provides real-time information about ad playback, offering essential business insights and reporting. It is a crucial job that supports all downstream analytical and reporting processes.Ads Event Handler: This service continuously sends information to ad vendors by reading tracking information from ad events, ensuring accurate and timely data exchange.Billing/Revenue: These are offline workflows designed to curate impressions, supporting billing and revenue recognition processes.Ads Reporting \u0026 Metrics: This service powers reporting module for our account managers and provides a centralized metrics API that help assess the delivery of a campaign.This was a massive multi-quarter effort across different engineering teams. With extensive planning (kudos to our TPM team!) and coordination, we were able to iterate fast, build several services and execute the vision above, to power our in-house ads technology platform.ConclusionThese systems have significantly accelerated our ability to launch new capabilities for the business.Through our partnership with Microsoft, Display Ad events were integrated into the new pipeline for reusability and ensuring when launching through Netflix ads systems, all use-cases were covered.Programmatic buying capabilities now support the exchange of numerous trackers and dynamic bid prices on impression events.Sharing opt-out signals helps ensure privacy and compliance with GDPR regulations for Ads business in Europe, supporting accurate reporting and measurement.New event types like Ad clicks and scanning of QR codes events also flow through the pipeline, ensuring all metrics and reporting are tracked consistently.Key TakewaysStrategic, incremental evolution: The development of our ads event processing systems has been a carefully orchestrated journey. Each iteration was meticulously planned by addressing existing challenges, anticipating future needs, and showcasing teamwork, planning, and coordination across various teams. These pillars have been fundamental to the success of this journey.Data contract: A clear data contract has been pivotal in ensuring consistency in interpretation and interoperability across our systems. By standardizing the data models, and establishing a clear data exchange between ad serving, and centralized event collection, our teams have been able to iterate at exceptional speed and continue to deliver many launches on time.Separation of concerns: Consumers are relieved from the need to understand each source of ad telemetry or manage updates and migrations. Instead, a centralized system handles these tasks, allowing consumers to focus on their core business logic.We have an exciting list of projects on the horizon. These include managing ad events from ads on Netflix live streams, de-duplication processes, and enriching data signals to deliver enhanced reporting and insights. Additionally, we are advancing our Native Ads strategy, integrating Conversion API for improved conversion tracking, among many others.This is definitely not a season finale; it’s just the beginning of our journey to create a best-in-class ads technology platform. We warmly invite you to share your thoughts and comments with us. If you’re interested in learning more or becoming a part of this innovative journey, Ads Engineering is hiring!AcknowledgementsA special thanks to our amazing colleagues and teams who helped build our foundational post-impression system: Simon Spencer, Priyankaa Vijayakumar, Indrajit Roy Choudhury; Ads TPM team — Sonya Bellamy; the Ad Serving Team — Andrew Sweeney, Tim Zheng, Haidong Tang and Ed Barker; the Ads Data Engineering Team — Sonali Sharma, Harsha Arepalli, and Wini Tran; Product Data Systems — David Klosowski; and the entire Ads Reporting and Measurement team!",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*aPL3RHeFEzlw_psaLddWKw.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv tabindex=\"-1\" aria-hidden=\"false\"\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page---byline--e4e86caf9249---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"32\" height=\"32\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"3140\"\u003e\u003ca href=\"https://www.linkedin.com/in/kineshsatiya/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKinesh Satiya\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"c0fb\"\u003eIntroduction\u003c/h2\u003e\u003cp id=\"0bc0\"\u003eIn a digital advertising platform, a robust feedback system is essential for the lifecycle and success of an ad campaign. This system comprises of diverse sub-systems designed to monitor, measure, and optimize ad campaigns. At Netflix, we embarked on a journey to build a robust event processing platform that not only meets the current demands but also scales for future needs. This blog post delves into the architectural evolution and technical decisions that underpin our Ads event processing pipeline.\u003c/p\u003e\u003cp id=\"5676\"\u003eAd serving acts like the “brain” — making decisions, optimizing delivery and ensuring right Ad is shown to the right member at the right time. Meanwhile, ad events, after an Ad is rendered, function like “heartbeats”, continuously providing real-time feedback (oxygen/nutrients) that fuels better decision-making, optimizations, reporting, measurement, and billing. Expanding on this analogy:\u003c/p\u003e\u003cul\u003e\u003cli id=\"ea42\"\u003eJust as the brain relies on continuous blood flow, ad serving depends on a steady stream of ad events to adjust next ad serving decision, frequency capping, pacing, and personalization.\u003c/li\u003e\u003cli id=\"6f87\"\u003eIf the nervous system stops sending signals (ad events stop flowing), the brain (ad serving) lacks critical insights and starts making poor decisions or even fails.\u003c/li\u003e\u003cli id=\"e937\"\u003eThe healthier and more accurate the event stream (just like strong heart function), the better the ad serving system can adapt, optimize, and drive business outcomes.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"2ec6\"\u003eLet’s dive into the journey of building this pipeline.\u003c/p\u003e\u003ch2 id=\"35f9\"\u003eThe Pilot\u003c/h2\u003e\u003cp id=\"1001\"\u003eIn November 2022, we launched a brand \u003ca href=\"https://about.netflix.com/en/news/announcing-basic-with-ads-us\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003enew basic ads plan\u003c/a\u003e, in partnership with Microsoft. The software systems extended the existing Netflix playback systems to play ads. Initially, the system was designed to be simple, secure, and efficient, with an underlying ethos of device-originated and server-proxied operations. The system consisted of three main components: the Microsoft Ad Server, Netflix Ads Manager, and Ad Event Handler. Each ad served required tracking to ensure the feedback loop functioned effectively, providing the external ad server with insights on impressions, frequency capping (advertiser policy that limits the number of times a user sees a specific ad), and monetization processes.\u003c/p\u003e\u003cp id=\"4b23\"\u003eKey features of this system include:\u003c/p\u003e\u003col\u003e\u003cli id=\"568d\"\u003e\u003cstrong\u003eClient Request: \u003c/strong\u003eClient devices request for ads during an ad break from Netflix playback systems, which is then decorated with information by ads manager to request ads from the ad server.\u003c/li\u003e\u003cli id=\"405c\"\u003e\u003cstrong\u003eServer-Side Ad Insertion:\u003c/strong\u003e The Ad Server sends ad responses using the VAST (Video Ad Serving Template) format.\u003c/li\u003e\u003cli id=\"8552\"\u003e\u003cstrong\u003eNetflix Ads Manager:\u003c/strong\u003e This service parses VAST documents, extracts tracking event information, and creates a simplified response structure for Netflix playback systems and client devices. \u003cbr/\u003e — The tracking information is packed into a structured protobuf data model.\u003cbr/\u003e — This structure is encrypted to create an opaque token.\u003cbr/\u003e — The final response, informs the client devices, when to send an event and the corresponding token.\u003c/li\u003e\u003cli id=\"d205\"\u003e\u003cstrong\u003eClient Device:\u003c/strong\u003e During ad playback, client devices send events accompanied by a token. The Netflix telemetry system then enqueues all these events in Kafka for asynchronous processing.\u003c/li\u003e\u003cli id=\"e73a\"\u003e\u003cstrong\u003eAds Event Handler:\u003c/strong\u003e This component is a Kafka consumer, that reads/decrypts the event payload and forwards the tracking information encoded back to the ad server and other vendors.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eFig 1: Basic Ad Event Handling System\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"ee5a\"\u003eThere is an \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba\" target=\"_blank\"\u003eexcellent prior blog\u003c/a\u003e post that explains how this systems was tested end-to-end at scale. This system design allowed us to quickly add new integrations for verification with vendors like DV, IAS and Nielsen for measurement.\u003c/p\u003e\u003ch2 id=\"1fc6\"\u003eThe Expansion\u003c/h2\u003e\u003cp id=\"06ee\"\u003eAs we continued to expand our third-party (3P) advertising vendors for measurement, tracking and verification, we identified a critical trend: growth in the volume of data encapsulated within opaque tokens. These tokens, which are cached on client devices, present a risk of elevated memory usage, potentially impacting device performance. We also anticipated increase in third-party tracking URLs, metadata needs, and more event types as our business added new capabilities.\u003c/p\u003e\u003cp id=\"b65f\"\u003eTo strategically address these challenges, we introduced a new persistence layer using \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30\" target=\"_blank\"\u003eKey-Value abstraction\u003c/a\u003e, between ad serving and event handling system: Ads Metadata Registry. This transient storage service stores metadata for each Ad served, and upon callback, event handler would read the tracking information to relay information to the vendors. The contract between the client device and Ads systems continues to use the opaque token per event, but now, instead of tracking information, it contains reference identifiers — Ad ID, the corresponding metadata record ID in the registry and the event name. This approach future proofed our systems to handle any growth in data that needs to pass from ad serving to event handling systems.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eFig 2: Storage service between Ad Serving \u0026amp; Reporting\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"f553\"\u003eThe Evolution\u003c/h2\u003e\u003cp id=\"b9ba\"\u003eIn January of 2024, we decided to invest in in-house advertising technology platform. This implied that the event processing pipeline had to evolve significantly — attain parity with existing offerings and continue to support new product launches with rapid iterations using in-house Netflix Ad Server. This required re-evaluation of the entire architecture across all of Ads engineering teams.\u003c/p\u003e\u003cp id=\"83b3\"\u003eFirst, we made an inventory of the use-cases that would need to be supported through ad events.\u003c/p\u003e\u003col\u003e\u003cli id=\"5134\"\u003eWe’d need to start supporting frequency capping in-house for all ads through Netflix Ad server.\u003c/li\u003e\u003cli id=\"8187\"\u003eIncorporate pricing information for impressions to set the stage for billing events, which are used to charge advertisers.\u003c/li\u003e\u003cli id=\"2966\"\u003eA robust reporting system to share campaign reports with advertisers, combined with metrics data collection, helps assess the delivery and effectiveness of the campaign.\u003c/li\u003e\u003cli id=\"da68\"\u003eScale event handler to perform tracking information look-ups across different vendors.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"c00a\"\u003eNext, we examined upcoming launches, such as Pause/Display ads, to gain deeper insights into our strategic initiatives. We recognized that Display Ads would utilize a distinct logging framework, suggesting that different upstream pipelines might deliver ad telemetry. However, the downstream use-cases were expected to remain largely consistent. Additionally, by reviewing the goals of our telemetry teams, we saw large initiatives aimed at upgrading the platform, indicating potential future migrations.\u003c/p\u003e\u003cp id=\"ac48\"\u003eKeeping the above insights \u0026amp; challenges in mind,\u003c/p\u003e\u003cul\u003e\u003cli id=\"3983\"\u003eWe planned a centralized ad event collection system. This centralized service would consolidate common operations like decryption of tokens, enrichment, hashing identifiers into a single step execution and provide a single unified data contract to consumers that is highly extensible (like being agnostic to ad server \u0026amp; ad media).\u003c/li\u003e\u003cli id=\"4f2e\"\u003eWe proposed moving all consumers of ad telemetry downstream of the centralized service. This creates a clean separation between upstream systems and consumers in Ads Engineering.\u003c/li\u003e\u003cli id=\"ee4f\"\u003eIn the initial development phase of our advertising system, a crucial component was the creation of ad sessions based on individual ad events. This system was constructed using ad playback telemetry, which allowed us to gather essential metrics from these ad sessions. A significant decision in this plan was to position the ad sessionization process downstream of the raw ad events.\u003c/li\u003e\u003cli id=\"f367\"\u003eThe proposal also recommended moving all our Ads data processing pipelines for reporting/analytics/metrics for Ads using the data published by the centralized system.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"ea56\"\u003ePutting together all the components in our vision -\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eFig 3: Ad Event processing pipeline\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"da0e\"\u003eKey components on event processing pipeline -\u003c/p\u003e\u003cp id=\"66c8\"\u003e\u003cstrong\u003eAds Event Publisher:\u003c/strong\u003e This centralized system is responsible for collecting ads telemetry and providing unified ad events to the ads engineering teams. It supports various functions such as measurement, finance/billing, reporting, frequency capping, and maintaining an essential feedback loop back to the ad server.\u003c/p\u003e\u003cp id=\"6e2e\"\u003e\u003cstrong\u003eRealtime Consumers\u003c/strong\u003e\u003c/p\u003e\u003col\u003e\u003cli id=\"8bf9\"\u003e\u003cstrong\u003eFrequency Capping: \u003c/strong\u003eThis system tracks impressions for each campaign, profile, and any other frequency capping parameters set up for the campaign. It is utilized by the Ad Server during each ad decision to ensure ads are served with frequency limits.\u003c/li\u003e\u003cli id=\"0475\"\u003e\u003cstrong\u003eAds Metrics: \u003c/strong\u003eThis component is a Flink job that transforms raw data to a set of dimensions and metrics, subsequently writing to Apache Druid OLAP database. The streaming data is further backed by an offline process that corrects any inaccuracy during streaming ingestion and providing accurate metrics. It provides real-time metrics to assess the delivery health of campaigns and applies budget capping functionality.\u003c/li\u003e\u003cli id=\"efd4\"\u003e\u003cstrong\u003eAds Sessionizer: \u003c/strong\u003eAn Apache Flink job that consolidates all events related to a single ad into an Ad Session. This session provides real-time information about ad playback, offering essential business insights and reporting. It is a crucial job that supports all downstream analytical and reporting processes.\u003c/li\u003e\u003cli id=\"f42c\"\u003e\u003cstrong\u003eAds Event Handler: \u003c/strong\u003eThis service continuously sends information to ad vendors by reading tracking information from ad events, ensuring accurate and timely data exchange.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"ff42\"\u003e\u003cstrong\u003eBilling/Revenue: \u003c/strong\u003eThese are offline workflows designed to curate impressions, supporting billing and revenue recognition processes.\u003c/p\u003e\u003cp id=\"56fe\"\u003e\u003cstrong\u003eAds Reporting \u0026amp; Metrics: \u003c/strong\u003eThis service powers reporting module for our account managers and provides a centralized metrics API that help assess the delivery of a campaign.\u003c/p\u003e\u003cp id=\"1868\"\u003eThis was a massive multi-quarter effort across different engineering teams. With extensive planning (kudos to our TPM team!) and coordination, we were able to iterate fast, build several services and execute the vision above, to power our in-house ads technology platform.\u003c/p\u003e\u003ch2 id=\"67f3\"\u003eConclusion\u003c/h2\u003e\u003cp id=\"7e2d\"\u003eThese systems have significantly accelerated our ability to launch new capabilities for the business.\u003c/p\u003e\u003cul\u003e\u003cli id=\"6878\"\u003eThrough our partnership with Microsoft, Display Ad events were integrated into the new pipeline for reusability and ensuring when launching through Netflix ads systems, all use-cases were covered.\u003c/li\u003e\u003cli id=\"abd9\"\u003eProgrammatic buying capabilities now support the exchange of numerous trackers and dynamic bid prices on impression events.\u003c/li\u003e\u003cli id=\"8aac\"\u003eSharing opt-out signals helps ensure privacy and compliance with GDPR regulations for Ads business in Europe, supporting accurate reporting and measurement.\u003c/li\u003e\u003cli id=\"5a25\"\u003eNew event types like Ad clicks and scanning of QR codes events also flow through the pipeline, ensuring all metrics and reporting are tracked consistently.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"d5ce\"\u003e\u003cstrong\u003eKey Takeways\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli id=\"1e7a\"\u003e\u003cstrong\u003eStrategic, incremental evolution:\u003c/strong\u003e The development of our ads event processing systems has been a carefully orchestrated journey. Each iteration was meticulously planned by addressing existing challenges, anticipating future needs, and showcasing teamwork, planning, and coordination across various teams. These pillars have been fundamental to the success of this journey.\u003c/li\u003e\u003cli id=\"8b0f\"\u003e\u003cstrong\u003eData contract:\u003c/strong\u003e A clear data contract has been pivotal in ensuring consistency in interpretation and interoperability across our systems. By standardizing the data models, and establishing a clear data exchange between ad serving, and centralized event collection, our teams have been able to iterate at exceptional speed and continue to deliver many launches on time.\u003c/li\u003e\u003cli id=\"bb6b\"\u003e\u003cstrong\u003eSeparation of concerns: \u003c/strong\u003eConsumers are relieved from the need to understand each source of ad telemetry or manage updates and migrations. Instead, a centralized system handles these tasks, allowing consumers to focus on their core business logic.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"27da\"\u003eWe have an exciting list of projects on the horizon. These include managing ad events from ads on Netflix live streams, de-duplication processes, and enriching data signals to deliver enhanced reporting and insights. Additionally, we are advancing our Native Ads strategy, integrating Conversion API for improved conversion tracking, among many others.\u003c/p\u003e\u003cp id=\"1bbb\"\u003eThis is definitely not a season finale; it’s just the beginning of our journey to create a best-in-class ads technology platform. We warmly invite you to share your thoughts and comments with us. If you’re interested in learning more or becoming a part of this innovative journey, \u003ca href=\"https://jobs.netflix.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAds Engineering is hiring\u003c/a\u003e!\u003c/p\u003e\u003ch2 id=\"f86f\"\u003e\u003cem\u003eAcknowledgements\u003c/em\u003e\u003c/h2\u003e\u003cp id=\"bf29\"\u003e\u003cem\u003eA special thanks to our amazing colleagues and teams who helped build our foundational post-impression system: \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/simonspencer1/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eSimon Spencer\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/priyankaavj/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003ePriyankaa Vijayakumar,\u003c/em\u003e\u003c/a\u003e\u003cem\u003e \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/indrajit-roy-choudhury-5b011754/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eIndrajit Roy Choudhury\u003c/em\u003e\u003c/a\u003e\u003cem\u003e; Ads TPM team — \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/sonyabellamy/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eSonya Bellamy\u003c/em\u003e\u003c/a\u003e\u003cem\u003e; the Ad Serving Team —\u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/andrewjsweeney/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003e Andrew Sweeney\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/tim-z-b9112034/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eTim Zheng\u003c/em\u003e\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/haidongt/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eHaidong Tang\u003c/em\u003e\u003c/a\u003e\u003cem\u003e and \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/edhbarker/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eEd Barker\u003c/em\u003e\u003c/a\u003e\u003cem\u003e; the Ads Data Engineering Team — \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/sonalisharma/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eSonali Sharma\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/harshavardhan-arepalli-3a9b0992/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eHarsha Arepalli\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/winifredtran/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eWini Tran\u003c/em\u003e\u003c/a\u003e\u003cem\u003e; Product Data Systems — \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/d3cay/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eDavid Klosowski;\u003c/em\u003e\u003c/a\u003e\u003cem\u003e and the entire Ads Reporting and Measurement team!\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": "2025-05-09T19:43:41.892Z",
  "modifiedTime": null
}
