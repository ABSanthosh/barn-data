{
  "id": "69a2e02a-e2dc-456b-959e-db338da74d5d",
  "title": "What you should know from the Google I/O 2025 Developer keynote",
  "link": "https://developers.googleblog.com/en/google-io-2025-developer-keynote-recap/",
  "description": "Top announcements from Google I/O 2025 focus on building across Google platforms and innovating with AI models from Google DeepMind, with key focus on new tools, APIs, and features designed to enhance developer productivity and create AI-powered experiences using Gemini, Android, Firebase, and web.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "The Google I/O team",
  "length": 9943,
  "excerpt": "Top announcements from Google I/O 2025 focus on building across Google platforms and innovating with AI models from Google DeepMind, with key focus on new tools, APIs, and features designed to enhance developer productivity and create AI-powered experiences using Gemini, Android, Firebase, and web.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "This year at Google I/O we’re showing how you can build across Google’s different platforms, and innovate using our best AI models from Google DeepMind. Here are the top announcements from the Developer keynote. Building with GeminiGoogle AI Studio is the fastest way to evaluate models and start building with the Gemini API.Google AI Studio makes it easy to build with the Gemini API: We’ve integrated Gemini 2.5 Pro into the native code editor, enabling you to prototype faster. It’s tightly optimized with the GenAI SDK so you can instantly generate web apps from text, image, or video prompts. Start from a simple prompt, or get inspired by starter apps in the showcase.Build agentic experiences with the Gemini API: Build agents with Gemini 2.5 advanced reasoning capabilities via the Gemini API and new tools, like URL Context. It enables the model to pull context from web pages with just a link. We also announced the Gemini SDKs will support Model Context Protocol (MCP) definitions, making it easier to leverage open source tools.Gemini 2.5 Flash Native Audio in the Live API: Build agentic applications that hear and speak, with full control over the model’s voice, tone, speed, and overall style, in 24 languages. Gemini 2.5 Flash Native Audio is much better at understanding conversational flow and ignoring stray sounds or voices, leading to smoother, more natural back-and-forth.Generate high-quality UI designs with Stitch: A new AI-powered tool to generate user interface designs and corresponding frontend code for web applications. Iterate on your designs conversationally using chat, adjust themes, and easily export your creations to CSS/HTML or Figma to keep working. Try Stitch for UI design.Our async code agent, Jules, is now in public beta: Jules is a parallel, asynchronous coding agent that works directly with your GitHub repositories. You can ask Jules to take on tasks such as version upgrades, writing tests, updating features, and bug fixes, to name a few. It spins up a Cloud VM, makes coordinated edits across your codebase, runs tests, and you can open a pull request from its branch when you're happy with the code.AndroidLearn how we’re making it easier for you to build great experiences across devices.Building experiences with generative AI: Generative AI enhances apps by making them intelligent, personalized, and agentic. We announced new ML Kit GenAI APIs using Gemini Nano for common on-device tasks. We showcased an AI sample app, Androidify, which lets you create an Android robot of yourself using a selfie. Discover how Androidify is built, and read the developer documentation to get started.Building excellent apps adaptively across 500 million devices: Mobile Android apps form the foundation across phones, foldables, tablets, and ChromeOS, and this year we’re helping you bring them to cars and Android XR. You can also take advantage of Material 3 Expressive to help make your apps shine.Gemini in Android Studio - AI agents to help you work: Gemini in Android Studio is the AI-powered coding companion that makes developers more productive at every stage of the dev lifecycle. We previewed Journeys, an agentic experience that helps with writing and executing end-to-end tests. We also previewed the Version Upgrade Agent which helps update dependencies. Learn more about how these agentic experiences in Gemini in Android Studio can help you build better apps, faster.WebWe’re making it easier to create powerful web experiences, from building better UI and faster debugging, to creating new AI-powered features.Carousels are now easier than ever to build with a few lines of CSS and HTML: Build beautiful carousels with CSS that are interactive at first paint. With Chrome 135, we've combined a few new CSS primitives to make building carousels, and other types of off-screen UI, dramatically easier. Use familiar CSS concepts to create rich, interactive, smooth, and more accessible carousels, in a fraction of the time.Introducing the new experimental Interest Invoker API: Declaratively toggle popovers when visitor interest is active for a small duration. Combine with the Anchor Positioning API and Popover API to build complex, responsive, layered UI elements like tooltips and hover cards, without JavaScript. Interest Invoker API is available as an origin trial.Baseline features availability is now in your familiar tools: VS Code now displays the Baseline status of features as you build, with support coming soon to other VS Code-based IDEs and WebStorm by JetBrains. Baseline is now also supported in ESLint for CSS, HTML ESLint, and Stylelint. RUMvision combines Baseline information with real-user data, letting you strategically select the optimal Baseline target for your audience. Plus, with the web-features data set now 100% mapped, you can now access the Baseline status of every feature on every major browser.AI in Chrome DevTools supports your debugging workflow: Boost your development workflow with Gemini integrated directly into Chrome DevTools. With AI assistance, you can now directly apply suggested changes to the files in your workspace in the Elements panel. Plus, the reimagined Performance Panel now features a powerful ‘Ask AI’ integration that provides contextual performance insights to help optimize your web application’s Core Web Vitals.New built-in AI APIs using Gemini Nano are now available, including multimodal capabilities: Gemini Nano brings enhanced privacy, reduced latency, and lower cost. Starting from Chrome 138, the Summarizer API, Language Detector API, Translator API, and Prompt API for Chrome Extensions are available in Stable. The Writer and Rewriter APIs are available in origin trials, and the Proofreader API and Prompt API with multimodal capabilities are in Canary. Join our early preview program to help shape the future of AI on the web.FirebasePrototype, build, and run modern, AI-powered, full-stack apps users love with Firebase. Use Firebase Studio, a cloud-based, AI workspace powered by Gemini 2.5, to turn your ideas into a full-stack app in minutes, from prompt to publish.Figma designs can be brought to life in Firebase Studio: Import a Figma design directly into Firebase Studio using the builder.io plugin, then add features and functionality using Gemini in Firebase without having to write any code.Firebase Studio will now suggest a backend: Rolling out over the next several weeks, when you use the App Prototyping agent, Firebase Studio can detect the need for a backend. Firebase Studio will now recommend Firebase Auth and Cloud Firestore, and when you're ready to publish the app to Firebase App Hosting, Firebase Studio will provision those services for you.Firebase AI Logic: Integrate Google’s gen AI models directly through your client apps, or through Genkit for server-side implementation. As part of the evolution from Vertex AI in Firebase to Firebase AI Logic, we’re also releasing new features such as client side integrations for the Gemini Developer API, hybrid inference, enhanced observability, and deeper integrations with Firebase products such as App Check and Remote Config.Building with open modelsThere's so much you can do when building with Gemini, but sometimes it's better to train and tune your own model. That’s why we released Gemma, our family of open models designed to be state of the art, and fit on devices.Gemma 3n is in early preview: This model can run on as little as 2GB of RAM thanks to research innovations. It is the first model built on the new, advanced mobile-first architecture that will also power the next generation of Gemini Nano, and is engineered for unmatched AI performance directly on portable devices.MedGemma is our most capable open model for multimodal medical text and image comprehension: A variant of Gemma 3, MedGemma is a great starting point for developers to fine tune and adapt to build their own healthcare-based AI applications. Its small size makes it efficient for inference, and because it’s open, it enables developers with the flexibility to fine-tune the model and run it in their preferred environments. MedGemma is available for use now as part of Health AI Developer Foundations.Colab is launching an agent first experience that transforms coding: Powered by Gemini 2.5 Flash, Colab helps you navigate complex tasks, such as fine-tuning a model. We showcased how the new AI-first Colab can build UI, saving you lots of coding time.SignGemma is a sign language understanding model coming later this year to the Gemma family: It is the most capable model for translating sign languages into spoken language text to date (best at American Sign Language to English), enabling you to develop new ways for Deaf/Hard of Hearing users to access technology. Share your input at goo.gle/SignGemma.DolphinGemma is the world’s first large language model for dolphins: Working with researchers at Georgia Tech and the Wild Dolphin Project, DolphinGemma was fine-tuned on data from decades of field research, to help scientists better understand patterns in how dolphins communicate.Google Developer ProgramWe expanded AI benefits for the Google Developer Program, including Gemini Code Assist Standard, a new gen AI developer annual credit, and 3 months of Google One AI Premium. We also announced a new Google Cloud \u0026 NVIDIA community where you can connect with experts from both companies in a dedicated forum, and soon gain access to exclusive learning content and credits.Tune into all of the developer newsFollowing the keynotes, we’ll be livestreaming sessions across AI, Android, web, and cloud May 20-21. Then, check out all of the Google I/O announcements and updates with 100+ sessions, codelabs, and more available on demand starting May 22.Make sure to connect with our thriving global community of developers, and follow along on LinkedIn and Instagram as we bring I/O Connect events to developers around the world.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/developer-keynote-recap-google-io.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp data-block-key=\"4mwzg\"\u003eThis year at Google I/O we’re showing how you can build across Google’s different platforms, and innovate using our best AI models from Google DeepMind. Here are the top announcements from the Developer keynote.\u003c/p\u003e\u003cdiv\u003e\n    \u003ch2 data-block-key=\"y5q25\" id=\"\"\u003eBuilding with Gemini\u003c/h2\u003e\u003cp data-block-key=\"p8kg\"\u003e\u003ca href=\"http://aistudio.google.com/\"\u003eGoogle AI Studio\u003c/a\u003e is the fastest way to evaluate models and \u003ca href=\"https://blog.google/technology/developers/google-ai-developer-updates-io-2025/\"\u003estart building with the Gemini API\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"d0gp\"\u003e\u003cb\u003eGoogle AI Studio makes it easy to build with the Gemini API:\u003c/b\u003e We’ve integrated Gemini 2.5 Pro into the native code editor, enabling you to prototype faster. It’s tightly optimized with the GenAI SDK so you can instantly generate web apps from text, image, or video prompts. Start from a simple prompt, or get inspired by \u003ca href=\"https://aistudio.google.com/apps\"\u003estarter apps\u003c/a\u003e in the showcase.\u003c/p\u003e\u003cp data-block-key=\"obkg\"\u003e\u003cb\u003eBuild agentic experiences with the Gemini API:\u003c/b\u003e Build agents with Gemini 2.5 advanced reasoning capabilities via the Gemini API and new tools, like URL Context. It enables the model to pull context from web pages with just a link. We also announced the Gemini SDKs will support Model Context Protocol (MCP) definitions, making it easier to leverage open source tools.\u003c/p\u003e\u003cp data-block-key=\"fd6qm\"\u003e\u003cb\u003eGemini 2.5 Flash Native Audio in the Live API:\u003c/b\u003e Build agentic applications that \u003ca href=\"https://aistudio.google.com/live\"\u003ehear and speak\u003c/a\u003e, with full control over the model’s voice, tone, speed, and overall style, in 24 languages. Gemini 2.5 Flash Native Audio is much better at understanding conversational flow and ignoring stray sounds or voices, leading to smoother, more natural back-and-forth.\u003c/p\u003e\u003cp data-block-key=\"d1lnc\"\u003e\u003cb\u003eGenerate high-quality UI designs with Stitch:\u003c/b\u003e A new AI-powered tool to generate user interface designs and corresponding frontend code for web applications. Iterate on your designs conversationally using chat, adjust themes, and easily export your creations to CSS/HTML or Figma to keep working. \u003ca href=\"http://stitch.withgoogle.com/\"\u003eTry Stitch for UI design\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"attf9\"\u003e\u003cb\u003eOur async code agent, Jules, is now in public beta:\u003c/b\u003e \u003ca href=\"http://jules.google/\"\u003eJules\u003c/a\u003e is a parallel, asynchronous coding agent that works directly with your GitHub repositories. You can ask Jules to take on tasks such as version upgrades, writing tests, updating features, and bug fixes, to name a few. It spins up a Cloud VM, makes coordinated edits across your codebase, runs tests, and you can open a pull request from its branch when you\u0026#39;re happy with the code.\u003c/p\u003e\u003ch2 data-block-key=\"7gzgx\" id=\"android\"\u003e\u003cbr/\u003eAndroid\u003c/h2\u003e\u003cp data-block-key=\"eq6i4\"\u003eLearn how we’re making it easier for you to \u003ca href=\"https://android-developers.googleblog.com/2025/05/16-things-to-know-for-android-developers-google-io-2025.html\"\u003ebuild great experiences across devices\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"3g9cd\"\u003e\u003cb\u003eBuilding experiences with generative AI:\u003c/b\u003e Generative AI enhances apps by making them intelligent, personalized, and agentic. We announced new \u003ca href=\"https://android-developers.googleblog.com/2025/05/on-device-gen-ai-apis-ml-kit-gemini-nano.html\"\u003eML Kit GenAI APIs\u003c/a\u003e using Gemini Nano for common on-device tasks. We showcased an AI sample app, Androidify, which lets you create an Android robot of yourself using a selfie. Discover how \u003ca href=\"https://android-developers.googleblog.com/2025/05/androidify-building-ai-driven-experiences-jetpack-compose-gemini-camerax.html\"\u003eAndroidify is built\u003c/a\u003e, and read the \u003ca href=\"http://d.android.com/ai\"\u003edeveloper documentation\u003c/a\u003e to get started.\u003c/p\u003e\u003cp data-block-key=\"1d30h\"\u003e\u003cb\u003eBuilding excellent apps adaptively across 500 million devices:\u003c/b\u003e Mobile Android apps form the foundation across phones, foldables, tablets, and ChromeOS, and this year we’re helping you bring them to cars and \u003ca href=\"https://android-developers.googleblog.com/2025/05/updates-to-android-xr-sdk-developer-preview.html\"\u003eAndroid XR\u003c/a\u003e. You can also take advantage of \u003ca href=\"https://m3.material.io/blog/building-with-m3-expressive?utm_source=blog\u0026amp;utm_medium=motion\u0026amp;utm_campaign=IO25\"\u003eMaterial 3 Expressive\u003c/a\u003e to help make your apps shine.\u003c/p\u003e\u003cp data-block-key=\"40v7c\"\u003e\u003cb\u003eGemini in Android Studio - AI agents to help you work:\u003c/b\u003e \u003ca href=\"https://developer.android.com/gemini-in-android\"\u003eGemini in Android Studio\u003c/a\u003e is the AI-powered coding companion that makes developers more productive at every stage of the dev lifecycle. We previewed Journeys, an agentic experience that helps with writing and executing end-to-end tests. We also previewed the Version Upgrade Agent which helps update dependencies. Learn more about how these \u003ca href=\"https://android-developers.googleblog.com/2025/05/google-io-2025-whats-new-in-android-development-tools.html\"\u003eagentic experiences in Gemini in Android Studio\u003c/a\u003e can help you build better apps, faster.\u003c/p\u003e\u003ch2 data-block-key=\"82lgh\" id=\"web\"\u003e\u003cbr/\u003eWeb\u003c/h2\u003e\u003cp data-block-key=\"58fj6\"\u003eWe’re \u003ca href=\"https://developer.chrome.com/blog/web-at-io25\"\u003emaking it easier to create powerful web experiences\u003c/a\u003e, from building better UI and faster debugging, to creating new AI-powered features.\u003c/p\u003e\u003cp data-block-key=\"aqv92\"\u003e\u003cb\u003eCarousels are now easier than ever to build with a few lines of CSS and HTML:\u003c/b\u003e Build beautiful carousels with CSS that are interactive at first paint. With Chrome 135, we\u0026#39;ve combined a few new CSS primitives to make building carousels, and other types of off-screen UI, dramatically easier. Use familiar CSS concepts to create rich, interactive, smooth, and more accessible carousels, in a fraction of the time.\u003c/p\u003e\u003cp data-block-key=\"bqkus\"\u003e\u003cb\u003eIntroducing the new experimental Interest Invoker API:\u003c/b\u003e Declaratively toggle popovers when visitor interest is active for a small duration. Combine with the \u003ca href=\"https://developer.chrome.com/blog/anchor-positioning-api\"\u003eAnchor Positioning API\u003c/a\u003e and \u003ca href=\"https://developer.chrome.com/blog/introducing-popover-api\"\u003ePopover API\u003c/a\u003e to build complex, responsive, layered UI elements like tooltips and hover cards, without JavaScript. Interest Invoker API is available as an \u003ca href=\"https://developer.chrome.com/origintrials/#/register_trial/813462682693795841\"\u003eorigin trial\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"1lr5v\"\u003e\u003cb\u003eBaseline features availability is now in your familiar tools:\u003c/b\u003e \u003ca href=\"https://web.dev/blog/baseline-vscode\"\u003eVS Code\u003c/a\u003e now displays the Baseline status of features as you build, with support coming soon to other VS Code-based IDEs and WebStorm by JetBrains. Baseline is now also supported in ESLint for CSS, HTML ESLint, and Stylelint. RUMvision combines Baseline information with real-user data, letting you strategically select the optimal Baseline target for your audience. Plus, with the web-features data set now 100% mapped, you can now access the Baseline status of every feature on every major browser.\u003c/p\u003e\u003cp data-block-key=\"584hl\"\u003e\u003cb\u003eAI in Chrome DevTools supports your debugging workflow\u003c/b\u003e: Boost your development workflow with Gemini integrated directly into Chrome DevTools. With \u003ca href=\"https://developer.chrome.com/docs/devtools/ai-assistance\"\u003eAI assistance\u003c/a\u003e, you can now directly apply suggested changes to the files in your workspace in the Elements panel. Plus, the reimagined Performance Panel now features a powerful ‘Ask AI’ integration that provides contextual performance insights to help optimize your web application’s Core Web Vitals.\u003c/p\u003e\u003cp data-block-key=\"3nnbk\"\u003e\u003cb\u003eNew built-in AI APIs using Gemini Nano are now available, including multimodal capabilities:\u003c/b\u003e Gemini Nano brings enhanced privacy, reduced latency, and lower cost. Starting from Chrome 138, the Summarizer API, Language Detector API, Translator API, and Prompt API for Chrome Extensions are available in Stable. The Writer and Rewriter APIs are available in origin trials, and the Proofreader API and Prompt API with multimodal capabilities are in Canary. Join our \u003ca href=\"https://developer.chrome.com/docs/ai/join-epp\"\u003eearly preview program\u003c/a\u003e to help shape the future of AI on the web.\u003c/p\u003e\u003ch2 data-block-key=\"imj6o\" id=\"firebase\"\u003e\u003cbr/\u003eFirebase\u003c/h2\u003e\u003cp data-block-key=\"e6l7d\"\u003ePrototype, build, and run modern, AI-powered, full-stack apps users love with Firebase. Use \u003ca href=\"http://firebase.studio/\"\u003eFirebase Studio\u003c/a\u003e, a cloud-based, AI workspace powered by Gemini 2.5, to \u003ca href=\"https://firebase.blog/posts/2025/05/whats-new-at-google-io\"\u003eturn your ideas into a full-stack app in minutes, from prompt to publish\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"6b127\"\u003e\u003cb\u003eFigma designs can be brought to life in Firebase Studio\u003c/b\u003e: \u003ca href=\"https://firebase.blog/posts/2025/05/studio-updates-from-io\"\u003eImport a Figma design\u003c/a\u003e directly into Firebase Studio using the builder.io plugin, then add features and functionality using Gemini in Firebase without having to write any code.\u003c/p\u003e\u003cp data-block-key=\"6c2if\"\u003e\u003cb\u003eFirebase Studio will now suggest a backend:\u003c/b\u003e Rolling out over the next several weeks, when you use the App Prototyping agent, Firebase Studio can \u003ca href=\"https://firebase.blog/posts/2025/05/studio-updates-from-io\"\u003edetect the need for a backend\u003c/a\u003e. Firebase Studio will now recommend Firebase Auth and Cloud Firestore, and when you\u0026#39;re ready to publish the app to Firebase App Hosting, Firebase Studio will provision those services for you.\u003c/p\u003e\u003cp data-block-key=\"crjsf\"\u003e\u003cb\u003eFirebase AI Logic\u003c/b\u003e: Integrate Google’s gen AI models directly through your client apps, or through Genkit for server-side implementation. As part of the evolution from Vertex AI in Firebase to \u003ca href=\"https://firebase.blog/posts/2025/05/building-ai-apps\"\u003eFirebase AI Logic\u003c/a\u003e, we’re also releasing new features such as client side integrations for the Gemini Developer API, hybrid inference, enhanced observability, and deeper integrations with Firebase products such as App Check and Remote Config.\u003c/p\u003e\u003ch2 data-block-key=\"fa6qg\" id=\"building-with-open-models\"\u003e\u003cbr/\u003eBuilding with open models\u003c/h2\u003e\u003cp data-block-key=\"dj7tj\"\u003eThere\u0026#39;s so much you can do when building with Gemini, but sometimes it\u0026#39;s better to train and tune your own model. That’s why we released Gemma, our family of open models designed to be state of the art, and fit on devices.\u003c/p\u003e\u003cp data-block-key=\"dtqfj\"\u003e\u003cb\u003eGemma 3n is in early preview:\u003c/b\u003e \u003ca href=\"https://developers.googleblog.com/en/introducing-gemma-3n\"\u003eThis model\u003c/a\u003e can run on as little as 2GB of RAM thanks to research innovations. It is the first model built on the new, advanced mobile-first architecture that will also power the next generation of \u003ca href=\"https://deepmind.google/technologies/gemini/nano/\"\u003eGemini Nano\u003c/a\u003e, and is engineered for unmatched AI performance directly on portable devices.\u003c/p\u003e\u003cp data-block-key=\"fc0fb\"\u003e\u003cb\u003eMedGemma is our most capable open model for multimodal medical text and image comprehension:\u003c/b\u003e A variant of Gemma 3, MedGemma is a great starting point for developers to fine tune and adapt to build their own healthcare-based AI applications. Its small size makes it efficient for inference, and because it’s open, it enables developers with the flexibility to fine-tune the model and run it in their preferred environments. MedGemma is available for use now as part of \u003ca href=\"https://developers.google.com/health-ai-developer-foundations/medgemma\"\u003eHealth AI Developer Foundations\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"cddej\"\u003e\u003cb\u003eColab is launching an agent first experience that transforms coding\u003c/b\u003e: Powered by Gemini 2.5 Flash, Colab helps you navigate complex tasks, such as fine-tuning a model. We showcased how \u003ca href=\"https://developers.googleblog.com/en/fully-reimagined-ai-first-google-colab/\"\u003ethe new AI-first Colab\u003c/a\u003e can build UI, saving you lots of coding time.\u003c/p\u003e\u003cp data-block-key=\"e8epv\"\u003e\u003cb\u003eSignGemma is a sign language understanding model coming later this year to the Gemma family:\u003c/b\u003e It is the most capable model for translating sign languages into spoken language text to date (best at American Sign Language to English), enabling you to develop new ways for Deaf/Hard of Hearing users to access technology. Share your input at \u003ca href=\"https://goo.gle/SignGemma\"\u003egoo.gle/SignGemma\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"542uo\"\u003e\u003cb\u003eDolphinGemma is the world’s first large language model for dolphins\u003c/b\u003e: Working with researchers at Georgia Tech and the \u003ca href=\"https://www.wilddolphinproject.org/\"\u003eWild Dolphin Project\u003c/a\u003e, DolphinGemma was fine-tuned on data from decades of field research, to help scientists better understand patterns in how dolphins communicate.\u003c/p\u003e\u003ch2 data-block-key=\"rg9jx\" id=\"google-developer-program\"\u003e\u003cbr/\u003eGoogle Developer Program\u003c/h2\u003e\u003cp data-block-key=\"dp36a\"\u003eWe expanded AI benefits for the \u003ca href=\"https://developers.google.com/program\"\u003eGoogle Developer Program\u003c/a\u003e, including Gemini Code Assist Standard, a new gen AI developer annual credit, and 3 months of Google One AI Premium. We also announced a new\u003ca href=\"http://developers.google.com/community/nvidia\"\u003e Google Cloud \u0026amp; NVIDIA community\u003c/a\u003e where you can connect with experts from both companies in a dedicated forum, and soon gain access to exclusive learning content and credits.\u003c/p\u003e\u003ch2 data-block-key=\"vu918\" id=\"tune-into-all-of-the-developer-news\"\u003e\u003cbr/\u003eTune into all of the developer news\u003c/h2\u003e\u003cp data-block-key=\"49c8e\"\u003eFollowing the keynotes, we’ll be \u003ca href=\"https://www.youtube.com/playlist?list=PLOU2XLYxmsIJEQRQDtuYKVUmxYvRfWN5m\"\u003elivestreaming\u003c/a\u003e sessions across AI, Android, web, and cloud May 20-21. Then, check out all of the Google I/O announcements and updates with 100+ sessions, codelabs, and more \u003ca href=\"https://io.google/2025/?utm_source=blogpost\u0026amp;utm_medium=pr\u0026amp;utm_campaign=devrecap\u0026amp;utm_content=\"\u003eavailable on demand\u003c/a\u003e starting May 22.\u003c/p\u003e\u003cp data-block-key=\"58l80\"\u003eMake sure to connect with our thriving global \u003ca href=\"https://developers.google.com/community\"\u003ecommunity of developers\u003c/a\u003e, and follow along on \u003ca href=\"https://www.linkedin.com/showcase/googledevelopers/posts/?feedView=all\"\u003eLinkedIn\u003c/a\u003e and \u003ca href=\"https://www.instagram.com/googlefordevs/\"\u003eInstagram\u003c/a\u003e as we bring I/O Connect events to developers around the world.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-05-20T00:00:00Z",
  "modifiedTime": null
}
