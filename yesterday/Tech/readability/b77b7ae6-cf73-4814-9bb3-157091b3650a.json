{
  "id": "b77b7ae6-cf73-4814-9bb3-157091b3650a",
  "title": "An analysis of DeepSeek's R1-Zero and R1",
  "link": "https://arcprize.org/blog/r1-zero-r1-results-analysis",
  "description": "Comments",
  "author": "",
  "published": "Wed, 29 Jan 2025 17:44:45 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "By Mike Knoop",
  "length": 10970,
  "excerpt": "An analysis of Deepseek's R1",
  "siteName": "ARC Prize",
  "favicon": "https://arcprize.org/media/images/favicon.png",
  "text": "ARC Prize remains undefeated.New ideas still needed. An Analysis of DeepSeek's R1-Zero and R1 R1-Zero is more important than R1 Special thanks to Tuhin and Abu from Baseten and Yuchen from Hyperbolic Labs for hosting r1-zero for us. Hardly any providers are hosting this model variant, and its availability is important for research purposes. ARC Prize Foundation’s goal is to define, measure, and inspire new ideas towards AGI. To this end, we strive to create the strongest global innovation environment possible. We do not have AGI yet and are still innovation constrained – scaling up pure LLM pretraining is not the path, despite this being the dominant AI industry narrative and mainstream public view as of last summer. The reason narratives are important is they end up driving economic activity, like investment, research focus, funding, geopolitics, trade, etc. For example, in 2023-24 there was ~$20B invested into new LLM startups compared to only ~$200M into new AGI startups. We launched ARC Prize 2024 last June to grow awareness of limits of scaling LLMs and promote a useful benchmark, ARC-AGI-1, towards a new direction that requires AI systems to adapt to novel, unseen problems instead of being able to rely strictly on memorization. DeepSeek R1 architecture by @SirrahChan. Last week, DeepSeek published their new R1-Zero and R1 “reasoner” systems that is competitive with OpenAI’s o1 system on ARC-AGI-1. R1-Zero, R1, and o1 (low compute) all score around 15-20% – in contrast to GPT-4o’s 5%, the pinnacle of years of pure LLM scaling. Based on this week’s US market reaction, the public is starting to understand the limits of scaling pure LLMs too. However, there is still broad public ignorance about impending inference demand. In December 2024, OpenAI announced a new breakthrough o3 system that we verified. It scored 76% in a low compute mode and 88% in a high compute mode. The o3 system demonstrates the first practical, general implementation of a computer adapting to novel unseen problems. Despite being huge tech news, o3 beating ARC-AGI-1 went largely unnoticed and unreported by mainstream press. This is an incredibly important moment for the field of AI and for computer science and these systems demand study. But due to the closed nature of o1/o3, we’re forced to rely on speculation. Thanks to ARC-AGI-1 and now (nearly) open source R1-Zero and R1, we can add to our understanding. In particular, R1-Zero is significantly more important than R1. “Nearly” because DeepSeek did not publish a reproducible way to generate their model weights from scratch R1-Zero removes the human bottleneck In our o1 and o3 analysis, we speculated how these reasoning systems work. The key ideas: Generate chains-of-thought (CoT) for a problem domain. Label the intermediary CoT steps using a combination of human experts (“supervised fine tuning” or SFT) and automated machines (“reinforcement learning” or RL). Train base model using (2). At test time, iteratively inference from the process model. Techniques used to iterative sample, along with ARC-AGI-1 scores, are reviewed below: System ARC-AGI-1 Method Avg Tokens Avg Cost r1-zero 14% No SFT / no search 11K $.11 r1 15.8% SFT / no search 6K $.06 o1 (low) 20.5% SFT / no search 7K $.43 o1 (med) 31% SFT / no search 13K $.79 o1 (high) 35% SFT / no search 22K $1.31 o3 (low) 75.7% SFT / search + sampling 335K $20 o3 (high) 87.5% SFT / search + sampling 57M $3.4K Note: ARC-AGI-1 semi-private score shown. With DeepSeek’s new published research, we can better inform our speculation. The key insight is that higher degrees of novelty adaptation (and reliability) for LLM reasoning systems are achieved along three dimensions: Adding human labels aka SFT to CoT process model training CoT search instead of linear inference (parallel per-step CoT inference) Whole CoT sampling (parallel trajectory inference) Item (1) is bottlenecked by human data generation and constrains which domains these reasoning systems benefit most. For example, the MMLU professional law category is surprisingly much lower than the math and logic on o1. Items (2) and (3) are bottlenecked by efficiency. o1 and o3 both show logarithmic improvement in benchmark accuracy on ARC-AGI-1 as they spend more inference compute at test time, while the different ways to spend that compute adjust the x-axis of the curve. In my opinion, the most interesting thing DeepSeek has done is to publish R1-Zero separately. R1-Zero is a model which does not use SFT, the (1) item. Instead it relies purely on reinforcement learning. R1-Zero and R1 show strong score agreement on ARC-AGI-1, scoring 14% and 15% respectively. DeepSeeks’s own reported benchmark scores also show strong agreement between R1-Zero and R1, eg. on MATH AIME 2024 scores are 71% and 76% respectively (up from ~40% on the base DeepSeek V3). In the paper, R1-Zero authors say “DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing” and has been corroborated online. However in our testing, we found little to no evidence of incoherence when testing R1-Zero on ARC-AGI-1 which is similar to the math and coding domains the system was RL’d on. Taken together, these findings suggest: SFT (eg. human expert labeling) is not necessary for accurate and legible CoT reasoning in domains with strong verification. The R1-Zero training process is capable of creating its own internal domain specific language (“DSL”) in token space via RL optimization. SFT is necessary for increasing CoT reasoning domain generality. This makes intuitive sense, as language itself is effectively a reasoning DSL. The exact same “words” can be learned in one domain and applied in another, like a program. The pure RL approach can not yet discover a broad shared vocabulary and I expect this will be a strong focus for future research. Ultimately, R1-Zero demonstrates the prototype of a potential scaling regime with zero human bottlenecks – even in the training data acquisition itself. Almost certainly DeepSeek has set its sights on OpenAI’s o3 system. It is important to watch whether SFT ends up being a requirement to add CoT search and sampling, or whether a hypothetical “R2-Zero” could exist along the same logarithmic accuracy vs inference scaling curve. Based on R1-Zero results, I believe SFT will not be required to beat ARC-AGI-1 in this hypothetical scaled up version. Dollars for reliability There are two major shifts happening in AI, economically speaking: You can now spend more $ to get higher accuracy and reliability Training $ is moving to inference $ Both are going to drive a massive amount of demand for inference and neither will curtail the demand for more compute. In fact, they will increase the demand for compute. AI reasoning systems promise much greater returns than simply higher accuracy on benchmarks. The number one issue preventing more AI automation use (e.g. inference demand) is reliability. I’ve spoken with hundreds of Zapier’s customers trying to deploy AI agents in their businesses and the feedback is strongly consistent: “I don’t trust them yet because they don’t work reliably”. Previously I’ve argued that progress towards ARC-AGI would result in higher reliability. The challenge with LLM agents is they need strong local domain steering to work reliability. Stronger generalization capability requires the ability to adapt to unseen situations. We’re now starting to see evidence this view is correct. And so it’s no surprise several companies are now introducing agents (Anthropic, OpenAI, Apple, …) Agents will drive significant near-term demand inference due the reliability needs. More broadly, developers can choose to spend more compute to increase user trust in the system. More reliability does not mean 100% accuracy though – but you’d expect to be more consistently inaccurate. This is okay because users and developers can now more confidently steer behavior via prompting when accuracy is low. Problems that were impossible for computers previously now have dollar amounts attached to them. And as efficiency climbs, those dollar amounts will go down. Inference as training The other major shift occurring is in the provenance of data going into LLM systems for pretraining. Previously, most data was either purchased, scraped, or synthetically generated from an existing LLM (eg. distilling or augmenting). These reasoning systems offer a new option which is to generate “real” data as opposed to “synthetic”. The AI industry uses the term synthetic to identify low quality data that is typically recycled through an LLM to boost the overall amount of training data – with diminishing returns. But now with reasoning systems and verifiers, we can create brand new legitimate data to train on. This can either be done offline where the developer pays to create the data or at inference time where the end user pays! This is a fascinating shift in economics and suggests there could be a runaway power concentrating moment for AI system developers who have the largest number of paying customers. Those customers are footing the bill to create new high quality data … which improves the model … which becomes better and more preferred by users … you get the idea. If we can break through the human expert CoT barrier and create an extremely efficient system to create new data via search/synthesis and verification, then we should expect a massive influx of compute to go into these inference systems as they quite literally get better just by inputting dollars and raw data. Eventually this type of AI training will eclipse pretraining on human generated data altogether. Conclusion We will continue to see market corrections as increased inference demand becomes clear. AI system efficiency is only going to drive more usage, not just due to Jevons Paradox but because new regimes of training are unlocked as efficiency increases. With R1 being open and reproducible, more people and teams will be pushing CoT and search to the limits. This will more quickly tell us where the frontier actually lies and will fuel a wave of innovation that increases the chance of reaching AGI quickly. Several people have already told me they plan to use R1-style systems for ARC Prize 2025 and I’m excited to see the results. The fact that R1 is open is a great thing for the world. DeepSeek has pushed the frontier of science forward.",
  "image": "https://arcprize.org/media/images/og-image-default.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp\u003e\n            \u003ch2\u003eARC Prize remains undefeated.\u003cbr/\u003eNew ideas still needed\u003cspan\u003e.\u003c/span\u003e\u003c/h2\u003e\n        \u003c/p\u003e\u003cdiv\u003e\n\u003ch2 id=\"an-analysis-of-deepseeks-r1-zero-and-r1\"\u003eAn Analysis of DeepSeek\u0026#39;s R1-Zero and R1\u003c/h2\u003e\n\u003ch2 id=\"r1-zero-is-more-important-than-r1\"\u003eR1-Zero is more important than R1\u003c/h2\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eSpecial thanks to \u003ca href=\"https://x.com/tuhinone\"\u003eTuhin\u003c/a\u003e and \u003ca href=\"https://www.linkedin.com/in/abuqader/\"\u003eAbu\u003c/a\u003e from \u003ca href=\"https://www.baseten.co/\"\u003eBaseten\u003c/a\u003e and \u003ca href=\"https://x.com/yuchenj_uw\"\u003eYuchen\u003c/a\u003e from \u003ca href=\"https://hyperbolic.xyz/\"\u003eHyperbolic Labs\u003c/a\u003e for hosting r1-zero for us. Hardly any providers are hosting this model variant, and its availability is important for research purposes.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eARC Prize Foundation’s goal is to define, measure, and inspire new ideas towards AGI. To this end, we strive to create the strongest global innovation environment possible.\u003c/p\u003e\n\n\u003cp\u003eWe do not have AGI yet and are still innovation constrained – scaling up pure LLM pretraining is not the path, despite this being the dominant AI industry narrative and mainstream public view as of last summer.\u003c/p\u003e\n\n\u003cp\u003eThe reason narratives are important is they end up driving economic activity, like investment, research focus, funding, geopolitics, trade, etc. For example, in 2023-24 there was ~$20B invested into new LLM startups compared to only ~$200M into new AGI startups.\u003c/p\u003e\n\n\u003cp\u003eWe \u003ca href=\"https://arcprize.org/blog/launch\"\u003elaunched ARC Prize 2024 last June\u003c/a\u003e to grow awareness of limits of scaling LLMs and promote a useful benchmark, ARC-AGI-1, towards a new direction that requires AI systems to adapt to novel, unseen problems instead of being able to rely strictly on memorization.\u003c/p\u003e\n\n\u003cfigure\u003e\n  \u003cimg src=\"https://arcprize.org/media/images/blog/r1-arch.jpg\" alt=\"R1 Training Architecture\"/\u003e\n  \u003cfigcaption\u003eDeepSeek R1 architecture by \u003ca href=\"https://x.com/SirrahChan/status/1881488738473357753\" target=\"_blank\"\u003e@SirrahChan\u003c/a\u003e.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eLast week, DeepSeek \u003ca href=\"https://arxiv.org/abs/2501.12948\"\u003epublished\u003c/a\u003e their new R1-Zero and R1 “reasoner” systems that is \u003ca href=\"https://x.com/arcprize/status/1881761987090325517\"\u003ecompetitive with OpenAI’s o1 system\u003c/a\u003e on ARC-AGI-1. R1-Zero, R1, and o1 (low compute) all score around 15-20% – in contrast to \u003ccode\u003eGPT-4o\u003c/code\u003e’s 5%, the pinnacle of years of pure LLM scaling. Based on this week’s \u003ca href=\"https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/\"\u003eUS market reaction\u003c/a\u003e, the public is starting to understand the limits of scaling pure LLMs too. However, there is still broad public ignorance about impending inference demand.\u003c/p\u003e\n\n\u003cp\u003eIn December 2024, OpenAI announced a \u003ca href=\"https://arcprize.org/blog/oai-o3-pub-breakthrough\"\u003enew breakthrough o3 system that we verified\u003c/a\u003e. It scored 76% in a low compute mode and 88% in a high compute mode. The o3 system demonstrates the first practical, general implementation of a computer adapting to novel unseen problems.\u003c/p\u003e\n\n\u003cp\u003eDespite being \u003ca href=\"https://www.techmeme.com/241220/h2200\"\u003ehuge tech news\u003c/a\u003e, o3 beating ARC-AGI-1 \u003ca href=\"https://x.com/benspringwater/status/1881507009184530449\"\u003ewent largely unnoticed and unreported\u003c/a\u003e by mainstream press.\u003c/p\u003e\n\n\u003cp\u003eThis is an incredibly important moment for the field of AI and for computer science and these systems demand study. But due to the closed nature of o1/o3, we’re forced to rely on speculation. Thanks to ARC-AGI-1 and now (nearly) open source R1-Zero and R1, we can add to our understanding. In particular, R1-Zero is significantly more important than R1.\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e“Nearly” because DeepSeek did not publish a reproducible way to generate their model weights from scratch\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"r1-zero-removes-the-human-bottleneck\"\u003eR1-Zero removes the human bottleneck\u003c/h2\u003e\n\n\u003cp\u003eIn our \u003ca href=\"https://arcprize.org/blog/openai-o1-results-arc-prize\"\u003eo1\u003c/a\u003e and \u003ca href=\"https://arcprize.org/blog/oai-o3-pub-breakthrough\"\u003eo3 analysis\u003c/a\u003e, we speculated how these reasoning systems work. The key ideas:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eGenerate chains-of-thought (CoT) for a problem domain.\u003c/li\u003e\n  \u003cli\u003eLabel the intermediary CoT steps using a combination of human experts (“supervised fine tuning” or SFT) and automated machines (“reinforcement learning” or RL).\u003c/li\u003e\n  \u003cli\u003eTrain base model using (2).\u003c/li\u003e\n  \u003cli\u003eAt test time, iteratively inference from the process model.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eTechniques used to iterative sample, along with ARC-AGI-1 scores, are reviewed below:\u003c/p\u003e\n\n\u003cdiv\u003e\n  \u003ctable\u003e\n    \u003cthead\u003e\n      \u003ctr\u003e\n        \u003cth\u003eSystem\u003c/th\u003e\n        \u003cth\u003eARC-AGI-1\u003c/th\u003e\n        \u003cth\u003eMethod\u003c/th\u003e\n        \u003cth\u003eAvg Tokens\u003c/th\u003e\n        \u003cth\u003eAvg Cost\u003c/th\u003e\n      \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n      \u003ctr\u003e\n        \u003ctd\u003er1-zero\u003c/td\u003e\n        \u003ctd\u003e14%\u003c/td\u003e\n        \u003ctd\u003eNo SFT / no search\u003c/td\u003e\n        \u003ctd\u003e11K\u003c/td\u003e\n        \u003ctd\u003e$.11\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003er1\u003c/td\u003e\n        \u003ctd\u003e15.8%\u003c/td\u003e\n        \u003ctd\u003eSFT / no search\u003c/td\u003e\n        \u003ctd\u003e6K\u003c/td\u003e\n        \u003ctd\u003e$.06\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eo1 (low)\u003c/td\u003e\n        \u003ctd\u003e20.5%\u003c/td\u003e\n        \u003ctd\u003eSFT / no search\u003c/td\u003e\n        \u003ctd\u003e7K\u003c/td\u003e\n        \u003ctd\u003e$.43\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eo1 (med)\u003c/td\u003e\n        \u003ctd\u003e31%\u003c/td\u003e\n        \u003ctd\u003eSFT / no search\u003c/td\u003e\n        \u003ctd\u003e13K\u003c/td\u003e\n        \u003ctd\u003e$.79\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eo1 (high)\u003c/td\u003e\n        \u003ctd\u003e35%\u003c/td\u003e\n        \u003ctd\u003eSFT / no search\u003c/td\u003e\n        \u003ctd\u003e22K\u003c/td\u003e\n        \u003ctd\u003e$1.31\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eo3 (low)\u003c/td\u003e\n        \u003ctd\u003e75.7%\u003c/td\u003e\n        \u003ctd\u003eSFT / search + sampling\u003c/td\u003e\n        \u003ctd\u003e335K\u003c/td\u003e\n        \u003ctd\u003e$20\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eo3 (high)\u003c/td\u003e\n        \u003ctd\u003e87.5%\u003c/td\u003e\n        \u003ctd\u003eSFT / search + sampling\u003c/td\u003e\n        \u003ctd\u003e57M\u003c/td\u003e\n        \u003ctd\u003e$3.4K\u003c/td\u003e\n      \u003c/tr\u003e\n    \u003c/tbody\u003e\n  \u003c/table\u003e\n\u003c/div\u003e\n\n\u003cp\u003e\u003cem\u003eNote: ARC-AGI-1 semi-private score shown.\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eWith DeepSeek’s new published research, we can better inform our speculation. The key insight is that higher degrees of novelty adaptation (and reliability) for LLM reasoning systems are achieved along three dimensions:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eAdding human labels aka SFT to CoT process model training\u003c/li\u003e\n  \u003cli\u003eCoT search instead of linear inference (parallel per-step CoT inference)\u003c/li\u003e\n  \u003cli\u003eWhole CoT sampling (parallel trajectory inference)\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eItem (1) is bottlenecked by human data generation and constrains which domains these reasoning systems benefit most. For example, the \u003ca href=\"https://openai.com/index/learning-to-reason-with-llms/\"\u003eMMLU professional law category\u003c/a\u003e is surprisingly much lower than the math and logic on o1.\u003c/p\u003e\n\n\u003cp\u003eItems (2) and (3) are bottlenecked by efficiency. o1 and o3 both \u003ca href=\"https://arcprize.org/blog/oai-o3-pub-breakthrough\"\u003eshow logarithmic improvement\u003c/a\u003e in benchmark accuracy on ARC-AGI-1 as they spend more inference compute at test time, while the different ways to spend that compute adjust the x-axis of the curve.\u003c/p\u003e\n\n\u003cp\u003eIn my opinion, the most interesting thing DeepSeek has done is to publish R1-Zero separately. R1-Zero is a model which does not use SFT, the (1) item. Instead it relies purely on reinforcement learning.\u003c/p\u003e\n\n\u003cp\u003eR1-Zero and R1 show strong score agreement on  ARC-AGI-1, scoring 14% and 15% respectively. DeepSeeks’s own reported benchmark scores also show strong agreement between R1-Zero and R1, eg. on MATH AIME 2024 scores are 71% and 76% respectively (up from ~40% on the base DeepSeek V3).\u003c/p\u003e\n\n\u003cp\u003eIn the paper, R1-Zero authors say “DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing” and \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/\"\u003ehas been corroborated online\u003c/a\u003e. However in our testing, we found little to no evidence of incoherence when testing R1-Zero on ARC-AGI-1 which is similar to the math and coding domains the system was RL’d on.\u003c/p\u003e\n\n\u003cp\u003eTaken together, these findings suggest:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eSFT (eg. human expert labeling) is not necessary for accurate and legible CoT reasoning in domains with strong verification.\u003c/li\u003e\n  \u003cli\u003eThe R1-Zero training process is capable of creating its own internal domain specific language (“DSL”) in token space via RL optimization.\u003c/li\u003e\n  \u003cli\u003eSFT is necessary for increasing CoT reasoning domain generality.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThis makes intuitive sense, as language itself is effectively a reasoning DSL. The exact same “words” can be learned in one domain and applied in another, like a program. The pure RL approach can not yet discover a broad shared vocabulary and I expect this will be a strong focus for future research.\u003c/p\u003e\n\n\u003cp\u003eUltimately, R1-Zero demonstrates the prototype of a potential scaling regime with zero human bottlenecks – even in the training data acquisition itself.\u003c/p\u003e\n\n\u003cp\u003eAlmost certainly DeepSeek has set its sights on OpenAI’s o3 system. It is important to watch whether SFT ends up being a requirement to add CoT search and sampling, or whether a hypothetical “R2-Zero” could exist along the same logarithmic accuracy vs inference scaling curve. Based on R1-Zero results, I believe SFT will not be required to beat ARC-AGI-1 in this hypothetical scaled up version.\u003c/p\u003e\n\n\u003ch2 id=\"dollars-for-reliability\"\u003eDollars for reliability\u003c/h2\u003e\n\n\u003cp\u003eThere are two major shifts happening in AI, economically speaking:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eYou can now spend more $ to get higher accuracy and reliability\u003c/li\u003e\n  \u003cli\u003eTraining $ is moving to inference $\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eBoth are going to drive a massive amount of demand for inference and neither will curtail the demand for more compute. In fact, they will increase the demand for compute.\u003c/p\u003e\n\n\u003cp\u003eAI reasoning systems promise much greater returns than simply higher accuracy on benchmarks. The number one issue preventing more AI automation use (e.g. inference demand) is reliability. I’ve spoken with hundreds of Zapier’s customers trying to deploy AI agents in their businesses and the feedback is strongly consistent: “I don’t trust them yet because they don’t work reliably”.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.cognitiverevolution.ai/the-arc-prize-efficiency-intuition-and-agi-with-mike-knoop-co-founder-of-zapier/\"\u003ePreviously\u003c/a\u003e I’ve argued that progress towards ARC-AGI would result in higher reliability. The challenge with LLM agents is they need strong local domain steering to work reliability. Stronger generalization capability requires the ability to adapt to unseen situations. We’re now starting to \u003ca href=\"https://x.com/woj_zaremba/status/1882290021778313272\"\u003esee evidence\u003c/a\u003e this view is correct. And so it’s no surprise several companies are now introducing agents (Anthropic, OpenAI, Apple, …)\u003c/p\u003e\n\n\u003cp\u003eAgents will drive significant near-term demand inference due the reliability needs. More broadly, developers can choose to spend more compute to increase user trust in the system. More reliability does not mean 100% accuracy though – but you’d expect to be more \u003ca href=\"https://commons.wikimedia.org/wiki/File:Statistical_bias_and_statistical_noise_illustration.png\"\u003econsistently inaccurate\u003c/a\u003e. This is okay because users and developers can now more confidently steer behavior via prompting when accuracy is low.\u003c/p\u003e\n\n\u003cp\u003eProblems that were impossible for computers previously now have dollar amounts attached to them. And as efficiency climbs, those dollar amounts will go down.\u003c/p\u003e\n\n\u003ch2 id=\"inference-as-training\"\u003eInference as training\u003c/h2\u003e\n\n\u003cp\u003eThe other major shift occurring is in the provenance of data going into LLM systems for pretraining. Previously, most data was either purchased, scraped, or synthetically generated from an existing LLM (eg. distilling or augmenting).\u003c/p\u003e\n\n\u003cp\u003eThese reasoning systems offer a new option which is to generate “real” data as opposed to “synthetic”. The AI industry uses the term synthetic to identify low quality data that is typically recycled through an LLM to boost the overall amount of training data – with diminishing returns.\u003c/p\u003e\n\n\u003cp\u003eBut now with reasoning systems and verifiers, we can create brand new legitimate data to train on. This can either be done offline where the developer pays to create the data or at inference time where the end user pays!\u003c/p\u003e\n\n\u003cp\u003eThis is a fascinating shift in economics and suggests there could be a runaway power concentrating moment for AI system developers who have the largest number of paying customers. Those customers are footing the bill to create new high quality data … which improves the model … which becomes better and more preferred by users … you get the idea.\u003c/p\u003e\n\n\u003cp\u003eIf we can break through the human expert CoT barrier and create an extremely efficient system to create new data via search/synthesis and verification, then we should expect a massive influx of compute to go into these inference systems as they quite literally get better just by inputting dollars and raw data. Eventually this type of AI training will eclipse pretraining on human generated data altogether.\u003c/p\u003e\n\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\n\u003cp\u003eWe will continue to see market corrections as increased inference demand becomes clear. AI system efficiency is only going to drive more usage, not just due to \u003ca href=\"https://en.wikipedia.org/wiki/Jevons_paradox\"\u003eJevons Paradox\u003c/a\u003e but because new regimes of training are unlocked as efficiency increases.\u003c/p\u003e\n\n\u003cp\u003eWith R1 being open and reproducible, more people and teams will be pushing CoT and search to the limits. This will more quickly tell us where the frontier actually lies and will fuel a wave of innovation that increases the chance of reaching AGI quickly.\u003c/p\u003e\n\n\u003cp\u003eSeveral people have already told me they plan to use R1-style systems for \u003ca href=\"https://arcprize.org/blog/arc-prize-2025\"\u003eARC Prize 2025\u003c/a\u003e and I’m excited to see the results.\u003c/p\u003e\n\n\u003cp\u003eThe fact that R1 is open is a great thing for the world. DeepSeek has pushed the frontier of science forward.\u003c/p\u003e\n\n\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": null,
  "modifiedTime": null
}
