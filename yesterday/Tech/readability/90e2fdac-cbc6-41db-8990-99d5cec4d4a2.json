{
  "id": "90e2fdac-cbc6-41db-8990-99d5cec4d4a2",
  "title": "Microsoft’s new AI agent can control software and robots",
  "link": "https://arstechnica.com/ai/2025/02/microsofts-new-ai-agent-can-control-software-and-robots/",
  "description": "Magma could enable AI agents to take multistep actions in the real and digital worlds.",
  "author": "Benj Edwards",
  "published": "Thu, 20 Feb 2025 22:39:12 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "agentic AI",
    "google",
    "Google Gemini",
    "machine learning",
    "microsoft",
    "Microsoft Magma",
    "multimodal AI",
    "openai",
    "Operator",
    "robot AI",
    "robots"
  ],
  "byline": "Benj Edwards",
  "length": 2706,
  "excerpt": "Magma could enable AI agents to take multistep actions in the real and digital worlds.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "On Wednesday, Microsoft Research introduced Magma, an integrated AI foundation model that combines visual and language processing to control software interfaces and robotic systems. If the results hold up outside of Microsoft's internal testing, it could mark a meaningful step forward for an all-purpose multimodal AI that can operate interactively in both real and digital spaces. Microsoft claims that Magma is the first AI model that not only processes multimodal data (like text, images, and video) but can also natively act upon it—whether that’s navigating a user interface or manipulating physical objects. The project is a collaboration between researchers at Microsoft, KAIST, the University of Maryland, the University of Wisconsin-Madison, and the University of Washington. We've seen other large language model-based robotics projects like Google's PALM-E and RT-2 or Microsoft's ChatGPT for Robotics that utilize LLMs for an interface. However, unlike many prior multimodal AI systems that require separate models for perception and control, Magma integrates these abilities into a single foundation model. A combined graphic that shows off various capabilities of the Magma model. Credit: Microsoft Research Microsoft is positioning Magma as a step toward agentic AI, meaning a system that can autonomously craft plans and perform multistep tasks on a human's behalf rather than just answering questions about what it sees. \"Given a described goal,\" Microsoft writes in its research paper. \"Magma is able to formulate plans and execute actions to achieve it. By effectively transferring knowledge from freely available visual and language data, Magma bridges verbal, spatial, and temporal intelligence to navigate complex tasks and settings.\" Microsoft is not alone in its pursuit of agentic AI. OpenAI has been experimenting with AI agents through projects like Operator that can perform UI tasks in a web browser, and Google has explored multiple agentic projects with Gemini 2.0. Spatial intelligence While Magma builds off of Transformer-based LLM technology that feeds training tokens into a neural network, it's different from traditional vision-language models (like GPT-4V, for example) by going beyond what they call \"verbal intelligence\" to also include \"spatial intelligence\" (planning and action execution). By training on a mix of images, videos, robotics data, and UI interactions, Microsoft claims that Magma is a true multimodal agent rather than just a perceptual model.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2025/02/magma_hotdog_1-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                      \n                      \n          \u003cp\u003eOn Wednesday, Microsoft Research introduced \u003ca href=\"https://microsoft.github.io/Magma/\"\u003eMagma\u003c/a\u003e, an integrated AI foundation model that combines visual and language processing to control software interfaces and robotic systems. If the results hold up outside of Microsoft\u0026#39;s internal testing, it could mark a meaningful step forward for an all-purpose multimodal AI that can operate interactively in both real and digital spaces.\u003c/p\u003e\n\u003cp\u003eMicrosoft claims that Magma is the first AI model that not only processes multimodal data (like text, images, and video) but can also natively act upon it—whether that’s navigating a user interface or manipulating physical objects. The project is a collaboration between researchers at Microsoft, \u003ca href=\"https://www.kaist.ac.kr/en/\"\u003eKAIST\u003c/a\u003e, the University of Maryland, the University of Wisconsin-Madison, and the University of Washington.\u003c/p\u003e\n\u003cp\u003eWe\u0026#39;ve seen other large language model-based robotics projects like Google\u0026#39;s \u003ca href=\"https://arstechnica.com/information-technology/2023/03/embodied-ai-googles-palm-e-allows-robot-control-with-natural-commands/\"\u003ePALM-E\u003c/a\u003e and \u003ca href=\"https://arstechnica.com/information-technology/2023/07/googles-rt-2-ai-model-brings-us-one-step-closer-to-wall-e/\"\u003eRT-2\u003c/a\u003e or Microsoft\u0026#39;s \u003ca href=\"https://arstechnica.com/information-technology/2023/02/robots-let-chatgpt-touch-the-real-world-thanks-to-microsoft/\"\u003eChatGPT for Robotics\u003c/a\u003e that utilize LLMs for an interface. However, unlike many prior multimodal AI systems that require separate models for perception and control, Magma integrates these abilities into a single foundation model.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"461\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/02/magma_teaser-1024x461.png\" alt=\"A combined graphic that shows off various capabilities of the Magma model.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/02/magma_teaser-1024x461.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/magma_teaser-640x288.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/magma_teaser-768x346.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/magma_teaser-1536x691.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/magma_teaser-2048x922.png 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/magma_teaser-980x441.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/magma_teaser-1440x648.png 1440w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A combined graphic that shows off various capabilities of the Magma model.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://microsoft.github.io/Magma/\" target=\"_blank\"\u003e\n          \n          Microsoft Research\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eMicrosoft is positioning Magma as a step toward agentic AI, meaning a system that can autonomously craft plans and perform multistep tasks on a human\u0026#39;s behalf rather than just answering questions about what it sees.\u003c/p\u003e\n\u003cp\u003e\u0026#34;Given a described goal,\u0026#34; Microsoft writes in its research paper. \u0026#34;Magma is able to formulate plans and execute actions to achieve it. By effectively transferring knowledge from freely available visual and language data, Magma bridges verbal, spatial, and temporal intelligence to navigate complex tasks and settings.\u0026#34;\u003c/p\u003e\n\u003cp\u003eMicrosoft is not alone in its pursuit of agentic AI. OpenAI has been experimenting with AI agents through projects like \u003ca href=\"https://arstechnica.com/ai/2025/01/openai-launches-operator-an-ai-agent-that-can-operate-your-computer/\"\u003eOperator\u003c/a\u003e that can perform UI tasks in a web browser, and Google has explored multiple agentic projects with \u003ca href=\"https://arstechnica.com/information-technology/2024/12/google-goes-agentic-with-gemini-2-0s-ambitious-ai-agent-features/\"\u003eGemini 2.0\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eSpatial intelligence\u003c/h2\u003e\n\u003cp\u003eWhile Magma builds off of Transformer-based LLM technology that feeds training tokens into a neural network, it\u0026#39;s different from traditional vision-language models (like \u003ca href=\"https://arstechnica.com/information-technology/2023/09/chatgpt-goes-multimodal-with-image-recognition-and-speech-synthesis/\"\u003eGPT-4V\u003c/a\u003e, for example) by going beyond what they call \u0026#34;verbal intelligence\u0026#34; to also include \u0026#34;spatial intelligence\u0026#34; (planning and action execution). By training on a mix of images, videos, robotics data, and UI interactions, Microsoft claims that Magma is a true multimodal agent rather than just a perceptual model.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-02-20T22:39:12Z",
  "modifiedTime": "2025-02-20T22:39:12Z"
}
