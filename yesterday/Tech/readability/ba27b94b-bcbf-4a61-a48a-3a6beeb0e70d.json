{
  "id": "ba27b94b-bcbf-4a61-a48a-3a6beeb0e70d",
  "title": "Apple's MLX adding CUDA support",
  "link": "https://github.com/ml-explore/mlx/pull/1983",
  "description": "Comments",
  "author": "",
  "published": "Mon, 14 Jul 2025 21:40:30 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "ml-explore",
  "length": 19736,
  "excerpt": "This PR is an ongoing effort to add a CUDA backend to MLX, very little things work now but you can run the tutorial example already. To build and test: $ cmake . -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_B...",
  "siteName": "GitHub",
  "favicon": "https://github.com/fluidicon.png",
  "text": "This PR is an ongoing effort to add a CUDA backend to MLX, very little things work now but you can run the tutorial example already. To build and test: $ cmake . -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_BUILD_EXAMPLES=ON $ cmake --build build -j 16 $ ./build/examples/cpp/tutorial array([[2, 3], [4, 5]], dtype=float32) array([[1, 1], [1, 1]], dtype=float32) For development I usually use: $ cmake . -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_BUILD_EXAMPLES=ON -DCMAKE_C_COMPILER_LAUNCHER=ccache -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DCMAKE_CUDA_COMPILER_LAUNCHER=ccache -DCMAKE_BUILD_TYPE=Debug -GNinja Only tested on a Ubuntu 22.04 with CUDA 11.6, in theory other environments can also work but there are no testings. This PR is not updated frequently, if anyone is interested in the realtime development, please check my forked repo. There are mainly 2 reasons for a CUDA backend: CUDA supports unified memory. Including hardware support in some devices, and software support for devices without hardware unified memory. NVIDIA hardware is widely used for academic and massive computations. Being able to write/test code locally on a Mac and then deploy to super computers would make a good developer experience. This work is sponsored by Apple. angeloskath, lin72h, EricLBuehler, stemann, kylebeggs, pcuenca, Vaibhavs10, v-shobhit, ghishadow, a-r-r-o-w, and 22 more reacted with heart emoji awni, lin72h, EricLBuehler, stemann, radudiaconu0, kylebeggs, pcuenca, HongyuS, ghishadow, a-r-r-o-w, and 14 more reacted with rocket emoji I wanna add rocm support based on your cuda pull request. would that be ok with you? @zcbenz Awesome progress so far @zcbenz !! I'm wondering what the best way to get this incorporated into MLX. I can think of a couple of options: Once this is ready we can make this into a cuda branch in MLX and then send PRs to it. This will make it easier from a review / PR management standpoint Just merge the backbone infra for supporting CUDA and send more incremental PRs over time I kind of prefer the latter.. but I'm open to suggestions. I wanna add rocm support based on your cuda pull request. would that be ok with you? @radudiaconu0 Of course I'm ok with it! Before you begin, you might want to decide how the ROCm backend lives together with CUDA backend first. I'm not familiar with ROCm, but I saw 2 patterns in projects with both backends: Both backends share the same code, with help of #defines and name aliases. Transpile CUDA code to HIP on the fly during build time, which is used by PyTorch. Another thing to notice is this PR is bound to heavy changes in following weeks, I'm still experimenting what is the best interface for integration. Awesome progress indeed! Just chiming in regarding the best way to incorporate this. Imho merging often is the way to go (option 2 basically). Combined with running CUDA tests in CI it will be the easiest to live with (since we 'll know when we break it even if we don't use it). Otherwise the cuda branch would have to be constantly rebased on top of main which could be annoying. I wanna add rocm support based on your cuda pull request. would that be ok with you? @radudiaconu0 Of course I'm ok with it! Before you begin, you might want to decide how the ROCm backend lives together with CUDA backend first. I'm not familiar with ROCm, but I saw 2 patterns in projects with both backends: Both backends share the same code, with help of #defines and name aliases. Transpile CUDA code to HIP on the fly during build time, which is used by PyTorch. Another thing to notice is this PR is bound to heavy changes in following weeks, I'm still experimenting what is the best interface for integration. I would try to make a separate hip folder or to use hipify on your CUDA code to make it use rocm/hip I'm wondering what the best way to get this incorporated into MLX. I find myself keep refactoring the code when porting new kernels, I think I still need to implement a few more primitives before getting the backbone code stable, probably a few more weeks of experimenting. Once the code is ready for review, I can split this PR into a backbone PR, and a few small PRs for each primitive. And future works would then be submitted in incremental PRs. In CUDA the kernel parameters' size must be known at compile-time, i.e. we can't pass dynamic-sized shape/strides via constant memory like what the Metal kernels do. I'm currently passing shape/strides to kernels via fixed-size cuda::std::array, which is what PyTorch has been doing. This comes with a limitation of maximum ndim in arrays, which PyTorch sets to 25, I'm using 8 for now and it can be easily changed if found not enough. This comes with a limitation of maximum ndim in arrays, which PyTorch sets to 25, I'm using 8 for now and it can be easily changed if found not enough. Sounds great! As long as we can change it by setting one number somewhere I think that's perfectly fine. The C++ logistic_regression example can run now: $ ./build/examples/cpp/logistic_regression Loss array(0.0344943, dtype=float32), Accuracy, array(1, dtype=float32), Throughput 518.05 (it/s). The logistic_regression is slow, I did some profiling. Each step takes about 2ms (i.e. 500 it/s), and each step consists of: Graph building (the MLX calls before invoking eval, which is the empty area before eval_impl and Event::wait). Kernel launching (the eval_impl part). Waiting for the results of kernels (the Event::wait part). We can see that we spent as much time launching kernels as waiting for the results. Looking closer at the kernel launching part, between each eval_gpu calls there is a very long Event::is_signaled call, which is an atomic read under the hood and we want to cut down its time a lot. Inside the eval_gpu call, we have some cudaMalloc calls (the red blocks) that can be removed by introducing buffer cache in future. Then look at the \"CUDA HW\" panel, which indicates that kernel running time is the same with eval_gpu, which likely means that the kernel is executed synchronously instead of asynchronously, which I need to check what went wrong. There are very large paddings between the kernels, some of them belong to ops that do not need to launch kernels (like broadcast), some of them are the slow Event::is_signaled calls that need to be improved. And once the kernels run asynchronously, there ought to be no paddings between them then. Finally there are long paddings between the eval_impls, which is the main thread waiting for the finish signal from the launched kernels, and it is really really slow compared to actual kernel running time. I think I need a reimplementation of Event to cut it down, current implementation uses cuda::std::atomic which seems very inefficient. Overall the overhead does not look very big: it is only about 2ms per step, and it should be a fixed number as it is not related to the size of arrays. However in the case of logistic_regression as the computation itself is very fast, the overhead dramatically slows things down. Then look at the \"CUDA HW\" panel, which indicates that kernel running time is the same with eval_gpu, which likely means that the kernel is executed synchronously instead of asynchronously, which I need to check what went wrong. After a closer look, I think the \"NVTX\" row under \"CUDA HW\" only means to mark the event that started the kernel, and it does not indicate the event started at the same time with kernel. The kernels are started asynchronously. So in the case of logistic_regression where the computation takes less time than overhead, how fast it runs depends on how fast we can push ops to CUDA stream. For PyTorch the time duration between 2 simple ops is 5µs: And for us it is at least 41µs: There are a few things I can do to reduce the overhead: Reimplement Event with cudaEvent, which should be the fasted op provided by CUDA. Add buffer cache to reduce cudaMalloc calls. Record the locations of buffers to avoid unnecessary cudaMemPrefetch calls. Batch the cleanup of temporary arrays to reduce the latency between 2 kernels. At last there is still a good news though: the time spent on running kernel is the same with PyTorch, which means we don't need to improve the kernel implementation. some of them are the slow Event::is_signaled calls that need to be improved. Where are those calls coming from? Is it here? We might be able to reduce the number of times we call that if needed.. I'm not actually certain it needs to be there. I think it's just a mechanism to eagerly clean up unused events. Where are those calls coming from? Is it here? Yes it is where the calls came from. Removing it would be great, I think it is an expensive op on all platforms. Tried the ideas: switching the implementation of Event from cuda::std::atomic to cudaEvent bumped training speed from 500 it/s to 900; reducing the prefetch calls increased it from 900 it/s to 1100. The next optimization is tricky: after evaluating each op, the operands and temporaries are saved until kernel finishes running, in Metal it is done like this: if (d.command_buffer_needs_commit(s.index)) { d.end_encoding(s.index); scheduler::notify_new_task(s); command_buffer-\u003eaddCompletedHandler( [s, buffers = std::move(buffers)](MTL::CommandBuffer* cbuf) { scheduler::notify_task_completion(s); check_error(cbuf); }); d.commit_command_buffer(s.index); d.get_command_buffer(s.index); } else { command_buffer-\u003eaddCompletedHandler( [s, buffers = std::move(buffers)](MTL::CommandBuffer* cbuf) { check_error(cbuf); }); } } In CUDA there is a cudaLaunchHostFunc API that was used to implement this. However according to the profiling it adds at least 20µs latency in cuda stream, which means each kernel has to wait at least 20µs before running. To get rid of this latency, I improved the CUDA backend by saving operands and temporaries of the op until finalize() is called, i.e. when mx::eval_impl() finishes running. In this way the cudaLaunchHostFunc is only called once per mx::eval(), instead of once per op::eval_gpu(). And the duration between 2 kernels is now under 1µs, which is better than PyTorch and I believe it is the best we can do. The downside is the arrays take longer to be destroyed, which could increase memory usages. The code also no longer waits if there are more tasks than MAX_ACTIVE_TASKS. After this optimization the speed increased from 1100 it/s to 1600. There were still many kernels that get unusually delayed. What did the delayed kernels have in common? Before launching the kernel they all called an API: cudaMemPrefetch (the green blocks). In the CUDA backend we use the unified memory APIs, which automatically transfers data between host and device, since I know the data is going to be used in GPU, I used the cudaMemPrefetch API to prefetch the memory in device so the kernel does not have to wait for the implicit memory transfer during execution. It turns out the prefetching heavily delayed the kernel executions. Removing prefetching increased speed from 1600 it/s to 2100, and we now have a really beautiful timeline in profiler. One optimization I haven't done yet is buffer cache: I will add it when most ops are implemented and there is not no more third party libraries to be integrated. Can we do better? The remaining are mostly hard work: optimize the kernels and make CPU code run faster, which I think should be visited after we have implemented all ops. Very nice @zcbenz ! To get rid of this latency, I improved the CUDA backend by saving operands and temporaries of the op until finalize() is called, i.e. when mx::eval_impl() finishes running. That one is a bit concerning. For large graphs it can really blow up memory use if you hold the temporaries until the end of the graph eval. I don't think it's worth doing that. We might want to do something in-between like saving them once ever ~10 calls to eval_gpu or something like that. That one is a bit concerning. For large graphs it can really blow up memory use if you hold the temporaries until the end of the graph eval. I don't think it's worth doing that I agree, this feels like an immature optimization for a special case. I'll make the behavior easy to configure so we can optimize for more cases in future. Hi @zcbenz, NVIDIA Jetson devices have hardware unified memory, and I have both a Xavier (sm72) AGX 32GB, and an Orin (sm87) AGX 64GB. I have tried building your cuda fork frost-beta/mlx-cuda, on Orin. (needed to change supported sm to include sm87, and also needed to change libcublasLt_static to libcublasLt, otherwise I'd get relocation truncated to fit: R_AARCH64_CALL26 against symbol) It takes about 3 hours for a full build. When I tried to run examples, I get the following. root@54712748d876:/opt/mlx# ./build/examples/cpp/tutorial terminate called after throwing an instance of 'std::runtime_error' what(): Device 0 does not support synchronization in managed memory. Aborted (core dumped) root@54712748d876:/opt/mlx# ./build/examples/cpp/logistic_regression terminate called after throwing an instance of 'std::runtime_error' what(): Device 0 does not support synchronization in managed memory. Aborted (core dumped) root@54712748d876:/opt/mlx# ./build/examples/cpp/metal_capture terminate called after throwing an instance of 'std::runtime_error' what(): Device 0 does not support synchronization in managed memory. I'm willing to collaborate, if you're interested, I can run builds on Jetson devices and report back and try tinkering to some degree (I'm not an expert in CUDA) @corupta Can you remove the throw statement in Device::Device in mlx/backend/cuda/device.cpp and check if the example runs? On the slow compilation, you can pass -DMLX_FAST_COMPILE=ON -DMLX_CUDA_ARCHITECTURES=native to cmake to speed up a lot (by disabling many things). I'm currently working on JIT compilation support which will solve this. tldr: it gives segfault With Fast Compile on, it takes about 15 mins to build. Also, libcublasLt_static doesn't cause a problem when fast compile is on. I've tried building it in various settings (frost-beta/mlx-cuda in both today's latest commit and yesterday's, with/without libcublasLt set as dynamic) all resulted in below for all the example files: (replaced throw with a std::cerr statement) Device 0 does not support synchronization in managed memory. Ignoring... Segmentation fault (core dumped) Some info about environments I use: Jetson Orin =\u003e Ubuntu 22.04 Cuda 12.6 native or Ubuntu 24.04 Cuda 12.8 in docker Jetson Xavier =\u003e Ubuntu 20.04 Cuda 11.4 native or Ubuntu 20.04 Cuda 12.2 or Cuda 11.8 in docker (by docker I mean using the nvidia patched docker runtime within these devices, so docker should expose gpu without any issue). Same result in orin native/docker, xavier docker cuda 12.2. xavier native gives a build error related to __grid_constant__ being not defined and lots of others, I think it is because of the cuda version 11.4 being too low. xavier docker cuda 11.8 gave build errors such as (similar errors in different files for different operators such as +, \u003e, etc) /opt/mlx/mlx/backend/cuda/kernels/reduce_ops.cuh(125): error: more than one instance of constructor \"__nv_bfloat16::__nv_bfloat16\" matches the argument list: function \"__nv_bfloat16::__nv_bfloat16(float)\" /usr/local/cuda/targets/aarch64-linux/include/cuda_bf16.hpp(174): here function \"__nv_bfloat16::__nv_bfloat16(double)\" /usr/local/cuda/targets/aarch64-linux/include/cuda_bf16.hpp(175): here argument types are: (int) detected during instantiation of \"auto mlx::core::cu::ReduceInit\u003cmlx::core::cu::Prod, T\u003e::value() [with T=__nv_bfloat16]\" /opt/mlx/mlx/backend/cuda/reduce/segmented_reduce.cu(53): here /opt/mlx/mlx/backend/cuda/kernels/utils.cuh(66): error: more than one operator \"-\" matches these operands: built-in operator \"- arithmetic\" function \"mlx::core::operator-(const mlx::core::complex64_t \u0026)\" /opt/mlx/mlx/types/complex.h(77): here operand types are: - __nv_bfloat16 detected during: instantiation of \"T mlx::core::cu::Limits\u003cT, cuda::std::__4::enable_if_t\u003c\u003cexpression\u003e, void\u003e\u003e::min() [with T=__nv_bfloat16]\" /opt/mlx/mlx/backend/cuda/kernels/reduce_ops.cuh(140): here instantiation of \"T mlx::core::cu::ReduceInit\u003cmlx::core::cu::Max, T\u003e::value() [with T=__nv_bfloat16]\" /opt/mlx/mlx/backend/cuda/reduce/segmented_reduce.cu(53): here Thanks for testing the build, unfortunately this kind of error requires me to work the actual environment to debug. Currently I'm only testing on a few cloud environments, but I will look into making it work on Jetson once most development is done, the devices with hardware unified memory definitely need first class support. This was referenced Jun 10, 2025 I'm closing this as it has been split into smaller pieces (check the linked PR above), future changes to CUDA backend will be submitted with incremental PRs. awni, JakeMalis, lin72h, bhupesh-sf, corupta, ghishadow, calvinf, jmorganca, Lukas1h, johnhamlin, and 2 more reacted with rocket emoji",
  "image": "https://opengraph.githubassets.com/603202e4dbc02049f7c7fa4fe113dd5e76fd23e5b462460990e78cfe9dcf1b31/ml-explore/mlx/pull/1983",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n      \u003cdiv data-gid=\"PR_kwDOKzRn186PkHq4\" data-url=\"/ml-explore/mlx/pull/1983/partials/body\" data-channel-event-name=\"body_updated\" data-channel=\"eyJjIjoicHVsbF9yZXF1ZXN0OjI0MDg2MTA0ODgiLCJ0IjoxNzUyNTQyNzI3fQ==--097d726eda20a1891008ec3d0c0683297879d5ce50360317b1abaf4f997cff01\"\u003e\n\n\u003cp\u003e\u003ca href=\"https://github.com/zcbenz\" data-view-component=\"true\"\u003e\u003cimg data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" src=\"https://avatars.githubusercontent.com/u/639601?s=60\u0026amp;v=4\" alt=\"zcbenz\" size=\"40\" height=\"40\" width=\"40\" data-view-component=\"true\"/\u003e\u003c/a\u003e\n  \n  \n\u003c/p\u003e\u003cdiv id=\"issue-2937359776\"\u003e\n          \n\n          \u003cdiv\u003e\n  \n  \u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n    \u003cdiv\u003e\n      \u003cp dir=\"auto\"\u003eThis PR is an ongoing effort to add a CUDA backend to MLX, very little things work now but you can run the tutorial example already.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eTo build and test:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"$ cmake . -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_BUILD_EXAMPLES=ON\n$ cmake --build build -j 16\n$ ./build/examples/cpp/tutorial\narray([[2, 3],\n       [4, 5]], dtype=float32)\narray([[1, 1],\n       [1, 1]], dtype=float32)\"\u003e\u003cpre\u003e$ \u003cspan\u003ecmake \u003cspan\u003e.\u003c/span\u003e -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_BUILD_EXAMPLES=ON\u003c/span\u003e\n$ \u003cspan\u003ecmake --build build -j 16\u003c/span\u003e\n$ \u003cspan\u003e./build/examples/cpp/tutorial\u003c/span\u003e\n\u003cspan\u003earray([[2, 3],\u003c/span\u003e\n\u003cspan\u003e       [4, 5]], dtype=float32)\u003c/span\u003e\n\u003cspan\u003earray([[1, 1],\u003c/span\u003e\n\u003cspan\u003e       [1, 1]], dtype=float32)\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eFor development I usually use:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"$ cmake . -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_BUILD_EXAMPLES=ON -DCMAKE_C_COMPILER_LAUNCHER=ccache -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DCMAKE_CUDA_COMPILER_LAUNCHER=ccache -DCMAKE_BUILD_TYPE=Debug -GNinja\"\u003e\u003cpre\u003e$ \u003cspan\u003ecmake \u003cspan\u003e.\u003c/span\u003e -Bbuild -DMLX_BUILD_CUDA=ON -DMLX_BUILD_EXAMPLES=ON -DCMAKE_C_COMPILER_LAUNCHER=ccache -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DCMAKE_CUDA_COMPILER_LAUNCHER=ccache -DCMAKE_BUILD_TYPE=Debug -GNinja\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eOnly tested on a Ubuntu 22.04 with CUDA 11.6, in theory other environments can also work but there are no testings.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThis PR is not updated frequently, if anyone is interested in the realtime development, please check \u003ca href=\"https://github.com/frost-beta/mlx-cuda/commits?author=zcbenz\u0026amp;since=2025-03-20\"\u003emy forked repo\u003c/a\u003e.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003eThere are mainly 2 reasons for a CUDA backend:\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eCUDA supports unified memory. Including hardware support in some devices, and software support for devices without hardware unified memory.\u003c/li\u003e\n\u003cli\u003eNVIDIA hardware is widely used for academic and massive computations. Being able to write/test code locally on a Mac and then deploy to super computers would make a good developer experience.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003eThis work is sponsored by Apple.\u003c/p\u003e\n    \u003c/div\u003e\n  \u003c/task-lists\u003e\n  \n\u003c/div\u003e\n\n          \n          \u003cdiv data-view-component=\"true\"\u003e\n  \u003cform data-turbo=\"false\" action=\"/ml-explore/mlx/reactions\" accept-charset=\"UTF-8\" method=\"post\"\u003e\n    \n    \u003cdiv\u003e\n          \u003ctool-tip id=\"tooltip-1c081777-625f-41ce-ab66-85123bc908ca\" for=\"reactions--reaction_button_component-7e2e3c\" popover=\"manual\" data-direction=\"n\" data-type=\"description\" data-view-component=\"true\"\u003eangeloskath, lin72h, EricLBuehler, stemann, kylebeggs, pcuenca, Vaibhavs10, v-shobhit, ghishadow, a-r-r-o-w, and 22 more reacted with heart emoji\u003c/tool-tip\u003e\n          \u003ctool-tip id=\"tooltip-f828d0c8-5f40-44a1-a638-31caa46e6683\" for=\"reactions--reaction_button_component-bff5fa\" popover=\"manual\" data-direction=\"n\" data-type=\"description\" data-view-component=\"true\"\u003eawni, lin72h, EricLBuehler, stemann, radudiaconu0, kylebeggs, pcuenca, HongyuS, ghishadow, a-r-r-o-w, and 14 more reacted with rocket emoji\u003c/tool-tip\u003e\n      \n    \u003c/div\u003e\n\u003c/form\u003e\u003c/div\u003e\n\n\u003c/div\u003e\n\u003c/div\u003e\n\n\n       \n            \n\n\n      \n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186joZCO\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186joZCO/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/radudiaconu0/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/radudiaconu0\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/52667211?s=80\u0026amp;u=e21b399a7cd57ebf3a335b994c15e8920d15770f\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@radudiaconu0\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"6198e67db0a550721cdccdacfc7c4de77b24dfaa78bbb2d46d2ecbdc1c285b77\" id=\"issuecomment-2745274510\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eI wanna add rocm support based on your cuda pull request. would that be ok with you?\u003cbr/\u003e\n\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e@zcbenz\u003c/a\u003e\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186jo60h\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186jo60h/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/awni/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/awni\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/1542805?s=80\u0026amp;u=b4b259ba4dbf81a7707a7b39022ed864dbc177b1\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@awni\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"323c0d1f27ab1667863e4199c5b71c70305d2d4841f1dc6c49abda37a3cbd90d\" id=\"issuecomment-2745412897\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eAwesome progress so far \u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e@zcbenz\u003c/a\u003e !!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eI\u0026#39;m wondering what the best way to get this incorporated into MLX. I can think of a couple of options:\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eOnce this is ready we can make this into a cuda branch in MLX and then send PRs to it. This will make it easier from a review / PR management standpoint\u003c/li\u003e\n\u003cli\u003eJust merge the backbone infra for supporting CUDA and send more incremental PRs over time\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003eI kind of prefer the latter.. but I\u0026#39;m open to suggestions.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186jq3Se\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186jq3Se/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"a4fa6d6e16ec8b74e774df3999a0114670a9badb5f3217dca482590f3eb6764f\" id=\"issuecomment-2745922718\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eI wanna add rocm support based on your cuda pull request. would that be ok with you?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/radudiaconu0/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/radudiaconu0\"\u003e@radudiaconu0\u003c/a\u003e Of course I\u0026#39;m ok with it!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eBefore you begin, you might want to decide how the ROCm backend lives together with CUDA backend first. I\u0026#39;m not familiar with ROCm, but I saw 2 patterns in projects with both backends:\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003eBoth backends share the same code, with help of \u003ccode\u003e#define\u003c/code\u003es and name aliases.\u003c/li\u003e\n\u003cli\u003eTranspile CUDA code to HIP on the fly during build time, which is \u003ca href=\"https://github.com/pytorch/pytorch/blob/539db4af4bc8d2fe3a79f09dd7cb17a0619de7be/tools/amd_build/build_amd.py\"\u003eused by PyTorch\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"auto\"\u003eAnother thing to notice is this PR is bound to heavy changes in following weeks, I\u0026#39;m still experimenting what is the best interface for integration.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186jq4DJ\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186jq4DJ/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/angeloskath/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/angeloskath\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/1242043?s=80\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@angeloskath\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"32acc4294afb9a075d73b5fbe4d5625187d18e5b7897ad159dc2dbae37ce5557\" id=\"issuecomment-2745925833\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eAwesome progress indeed!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eJust chiming in regarding the best way to incorporate this. Imho merging often is the way to go (option 2 basically). Combined with running CUDA tests in CI it will be the easiest to live with (since we \u0026#39;ll know when we break it even if we don\u0026#39;t use it). Otherwise the cuda branch would have to be constantly rebased on top of main which could be annoying.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186jq5g9\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186jq5g9/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/radudiaconu0/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/radudiaconu0\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/52667211?s=80\u0026amp;u=e21b399a7cd57ebf3a335b994c15e8920d15770f\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@radudiaconu0\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"2268dda4afcf30455e4360a0b5600fe0262d62c3c92d81cc9e913fbbbb537c49\" id=\"issuecomment-2745931837\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cblockquote\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eI wanna add rocm support based on your cuda pull request. would that be ok with you?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/radudiaconu0/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/radudiaconu0\"\u003e@radudiaconu0\u003c/a\u003e Of course I\u0026#39;m ok with it!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eBefore you begin, you might want to decide how the ROCm backend lives together with CUDA backend first. I\u0026#39;m not familiar with ROCm, but I saw 2 patterns in projects with both backends:\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003eBoth backends share the same code, with help of \u003ccode\u003e#define\u003c/code\u003es and name aliases.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003eTranspile CUDA code to HIP on the fly during build time, which is \u003ca href=\"https://github.com/pytorch/pytorch/blob/539db4af4bc8d2fe3a79f09dd7cb17a0619de7be/tools/amd_build/build_amd.py\"\u003eused by PyTorch\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"auto\"\u003eAnother thing to notice is this PR is bound to heavy changes in following weeks, I\u0026#39;m still experimenting what is the best interface for integration.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003eI would try to make a separate hip folder or to use hipify on your CUDA code to make it use rocm/hip\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186jq59t\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186jq59t/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"cc9b0f351df507001161384a3ea6f65ab3652199c3c3e10246f0812be4b904d8\" id=\"issuecomment-2745933677\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eI\u0026#39;m wondering what the best way to get this incorporated into MLX.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003eI find myself keep refactoring the code when porting new kernels, I think I still need to implement a few more primitives before getting the backbone code stable, probably a few more weeks of experimenting.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eOnce the code is ready for review, I can split this PR into a backbone PR, and a few small PRs for each primitive. And future works would then be submitted in incremental PRs.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186j5IKa\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186j5IKa/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"dd4d0c95de35066352054e17c1aab3c6bdd6a2e60c7a54db3a67f29525482c4d\" id=\"issuecomment-2749661850\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eIn CUDA the kernel parameters\u0026#39; size must be known at compile-time, i.e. we can\u0026#39;t pass dynamic-sized shape/strides via constant memory like what the Metal kernels do.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eI\u0026#39;m currently passing shape/strides to kernels via fixed-size \u003ccode\u003ecuda::std::array\u003c/code\u003e, which is \u003ca href=\"https://github.com/pytorch/pytorch/blob/f320c7b76682720aab420b1ecd1eaa8ad9171be8/aten/src/ATen/cuda/detail/OffsetCalculator.cuh#L15-L31\"\u003ewhat PyTorch has been doing\u003c/a\u003e. This comes with a limitation of maximum ndim in arrays, which PyTorch sets to 25, I\u0026#39;m using 8 for now and it can be easily changed if found not enough.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186j5J2k\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186j5J2k/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/awni/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/awni\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/1542805?s=80\u0026amp;u=b4b259ba4dbf81a7707a7b39022ed864dbc177b1\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@awni\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"4874e2f3c3a59d78d381e5c66dc69c5bf5ee5f4532b99f078270b5a74709c4ef\" id=\"issuecomment-2749668772\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eThis comes with a limitation of maximum ndim in arrays, which PyTorch sets to 25, I\u0026#39;m using 8 for now and it can be easily changed if found not enough.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003eSounds great! As long as we can change it by setting one number somewhere I think that\u0026#39;s perfectly fine.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186lvKSZ\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186lvKSZ/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"f5cc17906a900f7f1786b13d401a474fd38e0fce07fb6b1a773a961addecf933\" id=\"issuecomment-2780603545\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eThe C++ \u003ccode\u003elogistic_regression\u003c/code\u003e example can run now:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"$ ./build/examples/cpp/logistic_regression \nLoss array(0.0344943, dtype=float32), Accuracy, array(1, dtype=float32), Throughput 518.05 (it/s).\"\u003e\u003cpre\u003e$ \u003cspan\u003e./build/examples/cpp/logistic_regression \u003c/span\u003e\n\u003cspan\u003eLoss array(0.0344943, dtype=float32), Accuracy, array(1, dtype=float32), Throughput 518.05 (it/s).\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186lz77M\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186lz77M/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"f8b83f6c9e0523f8c2fe9f00f6b56a0b17e794ab1fd88081adc52204eea2ea08\" id=\"issuecomment-2781855436\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eThe \u003ccode\u003elogistic_regression\u003c/code\u003e is slow, I did some profiling.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://private-user-images.githubusercontent.com/639601/430758737-87984a7d-dc8e-43e8-9297-57f491fd7485.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwNzU4NzM3LTg3OTg0YTdkLWRjOGUtNDNlOC05Mjk3LTU3ZjQ5MWZkNzQ4NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mODgxMmY5ZjNhODkyOWE1MGYxMTViMzE1M2JlZDY0MmU1ZWZiZDQwNWVkMjBhZDIzNDJmNDg0ZWU4M2QwODVkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9._Dnzt8VqAomkC1nUnYLjm9L6RyFRguzk4sz4xsuiNUs\"\u003e\u003cimg src=\"https://private-user-images.githubusercontent.com/639601/430758737-87984a7d-dc8e-43e8-9297-57f491fd7485.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwNzU4NzM3LTg3OTg0YTdkLWRjOGUtNDNlOC05Mjk3LTU3ZjQ5MWZkNzQ4NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mODgxMmY5ZjNhODkyOWE1MGYxMTViMzE1M2JlZDY0MmU1ZWZiZDQwNWVkMjBhZDIzNDJmNDg0ZWU4M2QwODVkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9._Dnzt8VqAomkC1nUnYLjm9L6RyFRguzk4sz4xsuiNUs\" alt=\"step\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eEach step takes about 2ms (i.e. 500 it/s), and each step consists of:\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003eGraph building (the MLX calls before invoking \u003ccode\u003eeval\u003c/code\u003e, which is the empty area before \u003ccode\u003eeval_impl\u003c/code\u003e and \u003ccode\u003eEvent::wait\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003eKernel launching (the \u003ccode\u003eeval_impl\u003c/code\u003e part).\u003c/li\u003e\n\u003cli\u003eWaiting for the results of kernels (the \u003ccode\u003eEvent::wait\u003c/code\u003e part).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"auto\"\u003eWe can see that we spent as much time launching kernels as waiting for the results.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://private-user-images.githubusercontent.com/639601/430759543-7d478322-a72e-417a-9a26-cf92ec42927d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwNzU5NTQzLTdkNDc4MzIyLWE3MmUtNDE3YS05YTI2LWNmOTJlYzQyOTI3ZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1kMDBlNzA4ZTM1ZWFhYjZhZTNlOGFkYzk0M2IxNzYzMjM0MTQ2NzQxYjJhNDQ2ZDYyYThmOTM5N2Y3Mjg3MWI3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.X5v2PKh4wcyMzdM50CLhtlB864Ej-aVCxvdPM5r_zuE\"\u003e\u003cimg src=\"https://private-user-images.githubusercontent.com/639601/430759543-7d478322-a72e-417a-9a26-cf92ec42927d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwNzU5NTQzLTdkNDc4MzIyLWE3MmUtNDE3YS05YTI2LWNmOTJlYzQyOTI3ZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1kMDBlNzA4ZTM1ZWFhYjZhZTNlOGFkYzk0M2IxNzYzMjM0MTQ2NzQxYjJhNDQ2ZDYyYThmOTM5N2Y3Mjg3MWI3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.X5v2PKh4wcyMzdM50CLhtlB864Ej-aVCxvdPM5r_zuE\" alt=\"eval\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eLooking closer at the kernel launching part, between each \u003ccode\u003eeval_gpu\u003c/code\u003e calls there is a very long \u003ccode\u003eEvent::is_signaled\u003c/code\u003e call, which is an atomic read under the hood and we want to cut down its time a lot.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eInside the \u003ccode\u003eeval_gpu\u003c/code\u003e call, we have some \u003ccode\u003ecudaMalloc\u003c/code\u003e calls (the red blocks) that can be removed by introducing buffer cache in future.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://private-user-images.githubusercontent.com/639601/430761799-e414bec3-effc-4297-b5f5-54be9b6568f6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwNzYxNzk5LWU0MTRiZWMzLWVmZmMtNDI5Ny1iNWY1LTU0YmU5YjY1NjhmNi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wYzVhMmQ0MzI5ZTNkODBhNTRlMzdhMDRjNzA0OTM5MTE2N2Y4ZjQ1NmVjMjA3MjEyZjRlYWVjMzc2MDcxMDI3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.f1ajTK77dU6zxOeuxLZZDkx1tzRs05SPEatcgLD8Ps0\"\u003e\u003cimg src=\"https://private-user-images.githubusercontent.com/639601/430761799-e414bec3-effc-4297-b5f5-54be9b6568f6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwNzYxNzk5LWU0MTRiZWMzLWVmZmMtNDI5Ny1iNWY1LTU0YmU5YjY1NjhmNi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wYzVhMmQ0MzI5ZTNkODBhNTRlMzdhMDRjNzA0OTM5MTE2N2Y4ZjQ1NmVjMjA3MjEyZjRlYWVjMzc2MDcxMDI3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.f1ajTK77dU6zxOeuxLZZDkx1tzRs05SPEatcgLD8Ps0\" alt=\"CUDA HW\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThen look at the \u0026#34;CUDA HW\u0026#34; panel, which indicates that kernel running time is the same with \u003ccode\u003eeval_gpu\u003c/code\u003e, which likely means that the kernel is executed synchronously instead of asynchronously, which I need to check what went wrong.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThere are very large paddings between the kernels, some of them belong to ops that do not need to launch kernels (like broadcast), some of them are the slow \u003ccode\u003eEvent::is_signaled\u003c/code\u003e calls that need to be improved. And once the kernels run asynchronously, there ought to be no paddings between them then.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://private-user-images.githubusercontent.com/639601/430763789-c5a48891-e0da-4af3-8a25-486c6c865761.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwNzYzNzg5LWM1YTQ4ODkxLWUwZGEtNGFmMy04YTI1LTQ4NmM2Yzg2NTc2MS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lY2Q5ZTRjY2ZjYjczMjQ0YmIzYTI2ZjY2MGI4YzQ1Y2UzOWVlNjhiN2I0ZGQ5Mzg3ZjQzYzY5YWE2NDQ0NWU5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.ITQcSpSnoUpHmoDMhEL1ULUP_GeU0Pyd2ihIhzU2Mo0\"\u003e\u003cimg src=\"https://private-user-images.githubusercontent.com/639601/430763789-c5a48891-e0da-4af3-8a25-486c6c865761.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwNzYzNzg5LWM1YTQ4ODkxLWUwZGEtNGFmMy04YTI1LTQ4NmM2Yzg2NTc2MS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lY2Q5ZTRjY2ZjYjczMjQ0YmIzYTI2ZjY2MGI4YzQ1Y2UzOWVlNjhiN2I0ZGQ5Mzg3ZjQzYzY5YWE2NDQ0NWU5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.ITQcSpSnoUpHmoDMhEL1ULUP_GeU0Pyd2ihIhzU2Mo0\" alt=\"Screenshot 2025-04-07 at 11 04 43\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eFinally there are long paddings between the \u003ccode\u003eeval_impl\u003c/code\u003es, which is the main thread waiting for the finish signal from the launched kernels, and it is really really slow compared to actual kernel running time. I think I need a reimplementation of \u003ccode\u003eEvent\u003c/code\u003e to cut it down, current implementation uses \u003ccode\u003ecuda::std::atomic\u003c/code\u003e which seems very inefficient.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003eOverall the overhead does not look very big: it is only about 2ms per step, and it should be a fixed number as it is not related to the size of arrays. However in the case of \u003ccode\u003elogistic_regression\u003c/code\u003e as the computation itself is very fast, the overhead dramatically slows things down.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186l1Nzo\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186l1Nzo/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"9b468dd4b51f9f88598e7f24d7bf17e382fd244532648163e87c6686d9c1db54\" id=\"issuecomment-2782190824\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eThen look at the \u0026#34;CUDA HW\u0026#34; panel, which indicates that kernel running time is the same with \u003ccode\u003eeval_gpu\u003c/code\u003e, which likely means that the kernel is executed synchronously instead of asynchronously, which I need to check what went wrong.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003eAfter a closer look, I think the \u0026#34;NVTX\u0026#34; row under \u0026#34;CUDA HW\u0026#34; only means to mark the event that started the kernel, and it does not indicate the event started at the same time with kernel. The kernels are started asynchronously.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186l19c1\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186l19c1/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"fdb108c6eba9dc4af9d4bf703bf5c216b10fb82ec623a95e5a66e071334f4dd3\" id=\"issuecomment-2782385973\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eSo in the case of \u003ccode\u003elogistic_regression\u003c/code\u003e where the computation takes less time than overhead, how fast it runs depends on how fast we can push ops to CUDA stream.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eFor PyTorch the time duration between 2 simple ops is 5µs:\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://private-user-images.githubusercontent.com/639601/430840956-87351a14-ff07-4a90-b515-8c6ded348e1c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwODQwOTU2LTg3MzUxYTE0LWZmMDctNGE5MC1iNTE1LThjNmRlZDM0OGUxYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01ZTBlMDMxYzUxYTkzNDEyYjQxZjAyYzQ0NDUxYTBmMzZkNGQ4MDlhNzdlZTBiZTMwNGVmZjRkNGU2MGVmYjY0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.G0mP-B95omir5Et6ZB3B7zLAI4pLE0Js4M3LdX2d9TU\"\u003e\u003cimg src=\"https://private-user-images.githubusercontent.com/639601/430840956-87351a14-ff07-4a90-b515-8c6ded348e1c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwODQwOTU2LTg3MzUxYTE0LWZmMDctNGE5MC1iNTE1LThjNmRlZDM0OGUxYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01ZTBlMDMxYzUxYTkzNDEyYjQxZjAyYzQ0NDUxYTBmMzZkNGQ4MDlhNzdlZTBiZTMwNGVmZjRkNGU2MGVmYjY0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.G0mP-B95omir5Et6ZB3B7zLAI4pLE0Js4M3LdX2d9TU\" alt=\"5µs\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eAnd for us it is at least 41µs:\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://private-user-images.githubusercontent.com/639601/430840927-fb9fee77-24de-4d0d-bb32-4e5033b4beed.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwODQwOTI3LWZiOWZlZTc3LTI0ZGUtNGQwZC1iYjMyLTRlNTAzM2I0YmVlZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00YzJiOTU0ZTdiZTA1NTMyNGIxMGFiNzgwZDFkMzNlZTZlZmQ3NmM4OGM5M2E4ZmU3YTM3YmU3NTkzNDliN2EzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.yaOKD7KIuFErguEQpPRd22hrCFneDqvqwo4V1-RRdks\"\u003e\u003cimg src=\"https://private-user-images.githubusercontent.com/639601/430840927-fb9fee77-24de-4d0d-bb32-4e5033b4beed.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMwODQwOTI3LWZiOWZlZTc3LTI0ZGUtNGQwZC1iYjMyLTRlNTAzM2I0YmVlZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00YzJiOTU0ZTdiZTA1NTMyNGIxMGFiNzgwZDFkMzNlZTZlZmQ3NmM4OGM5M2E4ZmU3YTM3YmU3NTkzNDliN2EzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.yaOKD7KIuFErguEQpPRd22hrCFneDqvqwo4V1-RRdks\" alt=\"41µs\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003eThere are a few things I can do to reduce the overhead:\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003eReimplement \u003ccode\u003eEvent\u003c/code\u003e with cudaEvent, which should be the fasted op provided by CUDA.\u003c/li\u003e\n\u003cli\u003eAdd buffer cache to reduce \u003ccode\u003ecudaMalloc\u003c/code\u003e calls.\u003c/li\u003e\n\u003cli\u003eRecord the locations of buffers to avoid unnecessary \u003ccode\u003ecudaMemPrefetch\u003c/code\u003e calls.\u003c/li\u003e\n\u003cli\u003eBatch the cleanup of temporary arrays to reduce the latency between 2 kernels.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"auto\"\u003eAt last there is still a good news though: the time spent on running kernel is the same with PyTorch, which means we don\u0026#39;t need to improve the kernel implementation.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186l7m0M\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186l7m0M/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/awni/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/awni\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/1542805?s=80\u0026amp;u=b4b259ba4dbf81a7707a7b39022ed864dbc177b1\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@awni\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"0a6b0c2e8bd6bce32511bee67f085ec1b5b78b58d6b7670e31deadc9f3822cd5\" id=\"issuecomment-2783866124\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003esome of them are the slow \u003ccode\u003eEvent::is_signaled\u003c/code\u003e calls that need to be improved.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003eWhere are those calls coming from? Is it \u003ca href=\"https://github.com/ml-explore/mlx/blob/main/mlx/transforms.cpp#L206-L207\"\u003ehere\u003c/a\u003e? We might be able to reduce the number of times we call that if needed.. I\u0026#39;m not actually certain it needs to be there. I think it\u0026#39;s just a mechanism to eagerly clean up unused events.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186l_RMB\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186l_RMB/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"f59fac506eebe78a4fa4fe89bc4b1473be97e625e462b57e72a745c35561ec70\" id=\"issuecomment-2784826113\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eWhere are those calls coming from? Is it \u003ca href=\"https://github.com/ml-explore/mlx/blob/main/mlx/transforms.cpp#L206-L207\"\u003ehere\u003c/a\u003e?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003eYes it is where the calls came from.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eRemoving it would be great, I think it is an expensive op on all platforms.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186mEaCv\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186mEaCv/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"88d9ea04f9bd0a6a3dce354212343a528f713fd1fad758fbe992e8ab227c8fa6\" id=\"issuecomment-2786173103\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eTried the ideas: switching the implementation of \u003ccode\u003eEvent\u003c/code\u003e from \u003ccode\u003ecuda::std::atomic\u003c/code\u003e to \u003ccode\u003ecudaEvent\u003c/code\u003e bumped training speed from 500 it/s to 900; reducing the prefetch calls increased it from 900 it/s to 1100.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003eThe next optimization is tricky: after evaluating each op, the operands and temporaries are saved until kernel finishes running, in Metal it is done like this:\u003c/p\u003e\n\u003cdiv itemprop=\"text\"\u003e\n    \u003ctable data-tab-size=\"8\" data-paste-markdown-skip=\"\"\u003e\n\n        \u003ctbody\u003e\u003ctr\u003e\n          \u003ctd id=\"L55\" data-line-number=\"55\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC55\"\u003e   \u003cspan\u003eif\u003c/span\u003e (d.\u003cspan\u003ecommand_buffer_needs_commit\u003c/span\u003e(s.\u003cspan\u003eindex\u003c/span\u003e)) { \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L56\" data-line-number=\"56\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC56\"\u003e     d.\u003cspan\u003eend_encoding\u003c/span\u003e(s.\u003cspan\u003eindex\u003c/span\u003e); \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L57\" data-line-number=\"57\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC57\"\u003e     \u003cspan\u003escheduler::notify_new_task\u003c/span\u003e(s); \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L58\" data-line-number=\"58\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC58\"\u003e     command_buffer-\u0026gt;\u003cspan\u003eaddCompletedHandler\u003c/span\u003e( \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L59\" data-line-number=\"59\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC59\"\u003e         [s, buffers = \u003cspan\u003estd::move\u003c/span\u003e(buffers)](MTL::CommandBuffer* cbuf) { \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L60\" data-line-number=\"60\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC60\"\u003e           \u003cspan\u003escheduler::notify_task_completion\u003c/span\u003e(s); \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L61\" data-line-number=\"61\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC61\"\u003e           \u003cspan\u003echeck_error\u003c/span\u003e(cbuf); \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L62\" data-line-number=\"62\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC62\"\u003e         }); \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L63\" data-line-number=\"63\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC63\"\u003e     d.\u003cspan\u003ecommit_command_buffer\u003c/span\u003e(s.\u003cspan\u003eindex\u003c/span\u003e); \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L64\" data-line-number=\"64\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC64\"\u003e     d.\u003cspan\u003eget_command_buffer\u003c/span\u003e(s.\u003cspan\u003eindex\u003c/span\u003e); \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L65\" data-line-number=\"65\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC65\"\u003e   } \u003cspan\u003eelse\u003c/span\u003e { \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L66\" data-line-number=\"66\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC66\"\u003e     command_buffer-\u0026gt;\u003cspan\u003eaddCompletedHandler\u003c/span\u003e( \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L67\" data-line-number=\"67\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC67\"\u003e         [s, buffers = \u003cspan\u003estd::move\u003c/span\u003e(buffers)](MTL::CommandBuffer* cbuf) { \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L68\" data-line-number=\"68\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC68\"\u003e           \u003cspan\u003echeck_error\u003c/span\u003e(cbuf); \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L69\" data-line-number=\"69\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC69\"\u003e         }); \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L70\" data-line-number=\"70\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC70\"\u003e   } \u003c/td\u003e\n        \u003c/tr\u003e\n\n        \u003ctr\u003e\n          \u003ctd id=\"L71\" data-line-number=\"71\"\u003e\u003c/td\u003e\n          \u003ctd id=\"LC71\"\u003e } \u003c/td\u003e\n        \u003c/tr\u003e\n    \u003c/tbody\u003e\u003c/table\u003e\n  \u003c/div\u003e\n\n\u003cp dir=\"auto\"\u003eIn CUDA there is a \u003ccode\u003ecudaLaunchHostFunc\u003c/code\u003e API that was used to implement this. However according to the profiling it adds at least 20µs latency in cuda stream, which means each kernel has to wait at least 20µs before running.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eTo get rid of this latency, I improved the CUDA backend by saving operands and temporaries of the op until \u003ccode\u003efinalize()\u003c/code\u003e is called, i.e. when \u003ccode\u003emx::eval_impl()\u003c/code\u003e finishes running. In this way the \u003ccode\u003ecudaLaunchHostFunc\u003c/code\u003e is only called once per \u003ccode\u003emx::eval()\u003c/code\u003e, instead of once per \u003ccode\u003eop::eval_gpu()\u003c/code\u003e. And the duration between 2 kernels is now under 1µs, which is better than PyTorch and I believe it is the best we can do.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://private-user-images.githubusercontent.com/639601/431336388-41b66066-0b1e-4917-8be5-d716a35cccc5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMxMzM2Mzg4LTQxYjY2MDY2LTBiMWUtNDkxNy04YmU1LWQ3MTZhMzVjY2NjNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zOWM4NjgzODU3NjYyOGY1OTkwZTQzNGRhZDU0MDExYzNkYTQxMDdmNDY2ZDdhMDE3ZTAwMGFjZjQzZjk2OGI4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.a9Riv9nS0JNXQ6qy6tKovMp02rL-3xcNbxz6Gw_7jXA\"\u003e\u003cimg src=\"https://private-user-images.githubusercontent.com/639601/431336388-41b66066-0b1e-4917-8be5-d716a35cccc5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMxMzM2Mzg4LTQxYjY2MDY2LTBiMWUtNDkxNy04YmU1LWQ3MTZhMzVjY2NjNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zOWM4NjgzODU3NjYyOGY1OTkwZTQzNGRhZDU0MDExYzNkYTQxMDdmNDY2ZDdhMDE3ZTAwMGFjZjQzZjk2OGI4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.a9Riv9nS0JNXQ6qy6tKovMp02rL-3xcNbxz6Gw_7jXA\" alt=\"duration\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe downside is the arrays take longer to be destroyed, which could increase memory usages. The code also no longer waits if there are more tasks than MAX_ACTIVE_TASKS.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eAfter this optimization the speed increased from 1100 it/s to 1600.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003eThere were still many kernels that get unusually delayed.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://private-user-images.githubusercontent.com/639601/431339330-d1fe91b4-cee8-472b-8b48-39522747532c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMxMzM5MzMwLWQxZmU5MWI0LWNlZTgtNDcyYi04YjQ4LTM5NTIyNzQ3NTMyYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00MDg2MjBjNmM2YjFlYjdmMTg4ZGQ3Njk0MzE5M2Q0ZTljYWVmMWFlMjA1ODg0NzU3NmNlNjkzMDM1OWJmYmY2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.fbXTGyGNw_F-8hBKClfyODaELUlR1Y7rFI6n2l5OewU\"\u003e\u003cimg src=\"https://private-user-images.githubusercontent.com/639601/431339330-d1fe91b4-cee8-472b-8b48-39522747532c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMxMzM5MzMwLWQxZmU5MWI0LWNlZTgtNDcyYi04YjQ4LTM5NTIyNzQ3NTMyYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00MDg2MjBjNmM2YjFlYjdmMTg4ZGQ3Njk0MzE5M2Q0ZTljYWVmMWFlMjA1ODg0NzU3NmNlNjkzMDM1OWJmYmY2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.fbXTGyGNw_F-8hBKClfyODaELUlR1Y7rFI6n2l5OewU\" alt=\"prefetch\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWhat did the delayed kernels have in common? Before launching the kernel they all called an API: \u003ccode\u003ecudaMemPrefetch\u003c/code\u003e (the green blocks).\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eIn the CUDA backend we use the unified memory APIs, which automatically transfers data between host and device, since I know the data is going to be used in GPU, I used the \u003ccode\u003ecudaMemPrefetch\u003c/code\u003e API to prefetch the memory in device so the kernel does not have to wait for the implicit memory transfer during execution.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eIt turns out the prefetching heavily delayed the kernel executions.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eRemoving prefetching increased speed from 1600 it/s to 2100, and we now have a really beautiful timeline in profiler.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://private-user-images.githubusercontent.com/639601/431342746-442202c6-a5fd-40b7-b689-bc4932ce3f14.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMxMzQyNzQ2LTQ0MjIwMmM2LWE1ZmQtNDBiNy1iNjg5LWJjNDkzMmNlM2YxNC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03YTNhMGY1ZmNhZTJkMTA5ZTc0ZTMxNWI1ODhmOWI3OGFhMmZmMDM3MDMxNTBmMGNjNDJiYzU3NmJkNTJhOWY0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.0pjJQGGztSS8I4IlEPyABlLMlfGBh3iVUQjGCDIv4S8\"\u003e\u003cimg src=\"https://private-user-images.githubusercontent.com/639601/431342746-442202c6-a5fd-40b7-b689-bc4932ce3f14.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTI1NDMwMjgsIm5iZiI6MTc1MjU0MjcyOCwicGF0aCI6Ii82Mzk2MDEvNDMxMzQyNzQ2LTQ0MjIwMmM2LWE1ZmQtNDBiNy1iNjg5LWJjNDkzMmNlM2YxNC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDcxNVQwMTI1MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03YTNhMGY1ZmNhZTJkMTA5ZTc0ZTMxNWI1ODhmOWI3OGFhMmZmMDM3MDMxNTBmMGNjNDJiYzU3NmJkNTJhOWY0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.0pjJQGGztSS8I4IlEPyABlLMlfGBh3iVUQjGCDIv4S8\" alt=\"noprefetch\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003eOne optimization I haven\u0026#39;t done yet is buffer cache: I will add it when most ops are implemented and there is not no more third party libraries to be integrated.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003eCan we do better? The remaining are mostly hard work: optimize the kernels and make CPU code run faster, which I think should be visited after we have implemented all ops.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186mFSQP\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186mFSQP/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/awni/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/awni\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/1542805?s=80\u0026amp;u=b4b259ba4dbf81a7707a7b39022ed864dbc177b1\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@awni\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"dbe79e62ef90399d75802040e045fa1f9181835aa715f9479bff0368c161414e\" id=\"issuecomment-2786403343\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eVery nice \u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e@zcbenz\u003c/a\u003e !\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eTo get rid of this latency, I improved the CUDA backend by saving operands and temporaries of the op until finalize() is called, i.e. when mx::eval_impl() finishes running.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003eThat one is a bit concerning. For large graphs it can really blow up memory use if you hold the temporaries until the end of the graph eval. I don\u0026#39;t think it\u0026#39;s worth doing that. We might want to do something in-between like saving them once ever ~10 calls to \u003ccode\u003eeval_gpu\u003c/code\u003e or something like that.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \u003cdiv data-gid=\"IC_kwDOKzRn186mP9jc\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186mP9jc/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"71d19930b1f0e05188ac12e181818e9e5476b5427a9388338ccfcae11b16cc11\" id=\"issuecomment-2789202140\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eThat one is a bit concerning. For large graphs it can really blow up memory use if you hold the temporaries until the end of the graph eval. I don\u0026#39;t think it\u0026#39;s worth doing that\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003eI agree, this feels like an immature optimization for a special case. I\u0026#39;ll make the behavior easy to configure so we can optimize for more cases in future.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n      \n\n\n  \n\n        \n\n        \n\n        \u003cdiv data-gid=\"IC_kwDOKzRn186vOBrB\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186vOBrB/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/corupta/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/corupta\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/8008704?s=80\u0026amp;u=cb193027dd90d00420bf6e28b57b48eb2e1ed56b\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@corupta\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"cc6c68b83f4c7bafad5f1021b2a42ec9518352681b389d8432186430c66559b9\" id=\"issuecomment-2939689665\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eHi \u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e@zcbenz\u003c/a\u003e,\u003cbr/\u003e\nNVIDIA Jetson devices have hardware unified memory, and I have both a Xavier (sm72) AGX 32GB, and an Orin (sm87) AGX 64GB.\u003cbr/\u003e\nI have tried building your cuda fork \u003ca href=\"https://github.com/frost-beta/mlx-cuda/tree/cuda\"\u003efrost-beta/mlx-cuda\u003c/a\u003e, on Orin. (needed to change supported sm to include \u003ccode\u003esm87\u003c/code\u003e, and also needed to change libcublasLt_static to libcublasLt, otherwise I\u0026#39;d get \u003ccode\u003erelocation truncated to fit: R_AARCH64_CALL26 against symbol\u003c/code\u003e) It takes about 3 hours for a full build. When I tried to run examples, I get the following.\u003c/p\u003e\n\u003cdiv data-snippet-clipboard-copy-content=\"root@54712748d876:/opt/mlx# ./build/examples/cpp/tutorial\nterminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39;\n  what():  Device 0 does not support synchronization in managed memory.\nAborted (core dumped)\nroot@54712748d876:/opt/mlx# ./build/examples/cpp/logistic_regression\nterminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39;\n  what():  Device 0 does not support synchronization in managed memory.\nAborted (core dumped)\nroot@54712748d876:/opt/mlx# ./build/examples/cpp/metal_capture\nterminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39;\n  what():  Device 0 does not support synchronization in managed memory.\"\u003e\u003cpre\u003e\u003ccode\u003eroot@54712748d876:/opt/mlx# ./build/examples/cpp/tutorial\nterminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39;\n  what():  Device 0 does not support synchronization in managed memory.\nAborted (core dumped)\nroot@54712748d876:/opt/mlx# ./build/examples/cpp/logistic_regression\nterminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39;\n  what():  Device 0 does not support synchronization in managed memory.\nAborted (core dumped)\nroot@54712748d876:/opt/mlx# ./build/examples/cpp/metal_capture\nterminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39;\n  what():  Device 0 does not support synchronization in managed memory.\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eI\u0026#39;m willing to collaborate, if you\u0026#39;re interested, I can run builds on Jetson devices and report back and try tinkering to some degree (I\u0026#39;m not an expert in CUDA)\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n        \u003cdiv data-gid=\"IC_kwDOKzRn186vOgWT\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186vOgWT/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"821092ca7325e612d516525e41832077d67a3a5ccf8f39728c4d1fd3ada581f7\" id=\"issuecomment-2939815315\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/corupta/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/corupta\"\u003e@corupta\u003c/a\u003e Can you remove the \u003ccode\u003ethrow\u003c/code\u003e statement in \u003ccode\u003eDevice::Device\u003c/code\u003e in \u003ccode\u003emlx/backend/cuda/device.cpp\u003c/code\u003e and check if the example runs?\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eOn the slow compilation, you can pass  \u003ccode\u003e-DMLX_FAST_COMPILE=ON -DMLX_CUDA_ARCHITECTURES=native\u003c/code\u003e to cmake to speed up a lot (by disabling many things). I\u0026#39;m currently working on JIT compilation support which will solve this.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n        \u003cdiv data-gid=\"IC_kwDOKzRn186vZQIg\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186vZQIg/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/corupta/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/corupta\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/8008704?s=80\u0026amp;u=cb193027dd90d00420bf6e28b57b48eb2e1ed56b\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@corupta\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"55271ede0811d2eb120f2748e10287d002779061d16252e5ec4d86b0c53deb93\" id=\"issuecomment-2942632480\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003etldr: it gives segfault\u003cbr/\u003e\nWith Fast Compile on, it takes about 15 mins to build. Also, libcublasLt_static doesn\u0026#39;t cause a problem when fast compile is on. I\u0026#39;ve tried building it in various settings (frost-beta/mlx-cuda in both today\u0026#39;s latest commit and yesterday\u0026#39;s, with/without libcublasLt set as dynamic) all resulted in below for all the example files: (replaced \u003ccode\u003ethrow\u003c/code\u003e with a \u003ccode\u003estd::cerr\u003c/code\u003e statement)\u003c/p\u003e\n\u003cdiv data-snippet-clipboard-copy-content=\"Device 0 does not support synchronization in managed memory. Ignoring...\nSegmentation fault (core dumped)\"\u003e\u003cpre\u003e\u003ccode\u003eDevice 0 does not support synchronization in managed memory. Ignoring...\nSegmentation fault (core dumped)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eSome info about environments I use:\u003cbr/\u003e\nJetson Orin =\u0026gt; Ubuntu 22.04 Cuda 12.6 native or Ubuntu 24.04 Cuda 12.8 in docker\u003cbr/\u003e\nJetson Xavier =\u0026gt; Ubuntu 20.04 Cuda 11.4 native or Ubuntu 20.04 Cuda 12.2 or Cuda 11.8 in docker\u003cbr/\u003e\n(by docker I mean using the nvidia patched docker runtime within these devices, so docker should expose gpu without any issue).\u003cbr/\u003e\nSame result in orin native/docker, xavier docker cuda 12.2. xavier native gives a build error related to \u003ccode\u003e__grid_constant__\u003c/code\u003e being not defined and lots of others, I think it is because of the cuda version 11.4 being too low.\u003cbr/\u003e\nxavier docker cuda 11.8 gave build errors such as (similar errors in different files for different operators such as \u003ccode\u003e+\u003c/code\u003e, \u003ccode\u003e\u0026gt;\u003c/code\u003e, etc)\u003c/p\u003e\n\u003cdiv data-snippet-clipboard-copy-content=\"/opt/mlx/mlx/backend/cuda/kernels/reduce_ops.cuh(125): error: more than one instance of constructor \u0026#34;__nv_bfloat16::__nv_bfloat16\u0026#34; matches the argument list:\n            function \u0026#34;__nv_bfloat16::__nv_bfloat16(float)\u0026#34;\n/usr/local/cuda/targets/aarch64-linux/include/cuda_bf16.hpp(174): here\n            function \u0026#34;__nv_bfloat16::__nv_bfloat16(double)\u0026#34;\n/usr/local/cuda/targets/aarch64-linux/include/cuda_bf16.hpp(175): here\n            argument types are: (int)\n          detected during instantiation of \u0026#34;auto mlx::core::cu::ReduceInit\u0026lt;mlx::core::cu::Prod, T\u0026gt;::value() [with T=__nv_bfloat16]\u0026#34;\n/opt/mlx/mlx/backend/cuda/reduce/segmented_reduce.cu(53): here\n\n/opt/mlx/mlx/backend/cuda/kernels/utils.cuh(66): error: more than one operator \u0026#34;-\u0026#34; matches these operands:\n            built-in operator \u0026#34;- arithmetic\u0026#34;\n            function \u0026#34;mlx::core::operator-(const mlx::core::complex64_t \u0026amp;)\u0026#34;\n/opt/mlx/mlx/types/complex.h(77): here\n            operand types are: - __nv_bfloat16\n          detected during:\n            instantiation of \u0026#34;T mlx::core::cu::Limits\u0026lt;T, cuda::std::__4::enable_if_t\u0026lt;\u0026lt;expression\u0026gt;, void\u0026gt;\u0026gt;::min() [with T=__nv_bfloat16]\u0026#34;\n/opt/mlx/mlx/backend/cuda/kernels/reduce_ops.cuh(140): here\n            instantiation of \u0026#34;T mlx::core::cu::ReduceInit\u0026lt;mlx::core::cu::Max, T\u0026gt;::value() [with T=__nv_bfloat16]\u0026#34;\n/opt/mlx/mlx/backend/cuda/reduce/segmented_reduce.cu(53): here\n\"\u003e\u003cpre\u003e\u003ccode\u003e/opt/mlx/mlx/backend/cuda/kernels/reduce_ops.cuh(125): error: more than one instance of constructor \u0026#34;__nv_bfloat16::__nv_bfloat16\u0026#34; matches the argument list:\n            function \u0026#34;__nv_bfloat16::__nv_bfloat16(float)\u0026#34;\n/usr/local/cuda/targets/aarch64-linux/include/cuda_bf16.hpp(174): here\n            function \u0026#34;__nv_bfloat16::__nv_bfloat16(double)\u0026#34;\n/usr/local/cuda/targets/aarch64-linux/include/cuda_bf16.hpp(175): here\n            argument types are: (int)\n          detected during instantiation of \u0026#34;auto mlx::core::cu::ReduceInit\u0026lt;mlx::core::cu::Prod, T\u0026gt;::value() [with T=__nv_bfloat16]\u0026#34;\n/opt/mlx/mlx/backend/cuda/reduce/segmented_reduce.cu(53): here\n\n/opt/mlx/mlx/backend/cuda/kernels/utils.cuh(66): error: more than one operator \u0026#34;-\u0026#34; matches these operands:\n            built-in operator \u0026#34;- arithmetic\u0026#34;\n            function \u0026#34;mlx::core::operator-(const mlx::core::complex64_t \u0026amp;)\u0026#34;\n/opt/mlx/mlx/types/complex.h(77): here\n            operand types are: - __nv_bfloat16\n          detected during:\n            instantiation of \u0026#34;T mlx::core::cu::Limits\u0026lt;T, cuda::std::__4::enable_if_t\u0026lt;\u0026lt;expression\u0026gt;, void\u0026gt;\u0026gt;::min() [with T=__nv_bfloat16]\u0026#34;\n/opt/mlx/mlx/backend/cuda/kernels/reduce_ops.cuh(140): here\n            instantiation of \u0026#34;T mlx::core::cu::ReduceInit\u0026lt;mlx::core::cu::Max, T\u0026gt;::value() [with T=__nv_bfloat16]\u0026#34;\n/opt/mlx/mlx/backend/cuda/reduce/segmented_reduce.cu(53): here\n\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n        \u003cdiv data-gid=\"IC_kwDOKzRn186vZmBO\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186vZmBO/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"0ba95492dd8c3af0a41aff0a9d10d4010db8a96eadf3a844f8d7362aa809e5d9\" id=\"issuecomment-2942722126\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eThanks for testing the build, unfortunately this kind of error requires me to work the actual environment to debug. Currently I\u0026#39;m only testing on a few cloud environments, but I will look into making it work on Jetson once most development is done, the devices with hardware unified memory definitely need first class support.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n        \u003cdiv data-gid=\"CRE_kwDOKzRn186akqi2\"\u003e\u003cp\u003e\n  This was referenced \u003c/p\u003e\u003crelative-time datetime=\"2025-06-10T00:08:13Z\"\u003eJun 10, 2025\u003c/relative-time\u003e\n\u003c/div\u003e\n\n        \u003cdiv data-gid=\"IC_kwDOKzRn186w7oWR\" data-url=\"/ml-explore/mlx/comments/IC_kwDOKzRn186w7oWR/partials/timeline_issue_comment\"\u003e\n\n  \u003cp\u003e\u003ca data-hovercard-type=\"user\" data-hovercard-url=\"/users/zcbenz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zcbenz\"\u003e\u003cimg src=\"https://avatars.githubusercontent.com/u/639601?s=80\u0026amp;u=b726a177170e74aa10fc3061f27da82822fa29d3\u0026amp;v=4\" width=\"40\" height=\"40\" alt=\"@zcbenz\"/\u003e\u003c/a\u003e\n\n\u003c/p\u003e\n\n\n  \n\u003cdiv data-body-version=\"aea40e11a8e8b4a3f7fc072e9bddcdac487f5ef9a82b3214fa887b452a2496fd\" id=\"issuecomment-2968421777\"\u003e\n\n        \n\u003ctask-lists disabled=\"\" sortable=\"\"\u003e\n\u003cdiv\u003e\n          \u003cp dir=\"auto\"\u003eI\u0026#39;m closing this as it has been split into smaller pieces (check the linked PR above), future changes to CUDA backend will be submitted with incremental PRs.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/task-lists\u003e\n\n\n        \u003cdiv data-view-component=\"true\"\u003e\n  \u003cform data-turbo=\"false\" action=\"/ml-explore/mlx/reactions\" accept-charset=\"UTF-8\" method=\"post\"\u003e\n    \n      \n    \u003cdiv\u003e\n          \u003ctool-tip id=\"tooltip-1a0fad27-a163-478d-8756-72007f62cf64\" for=\"reactions--reaction_button_component-7fffe7\" popover=\"manual\" data-direction=\"n\" data-type=\"description\" data-view-component=\"true\"\u003eawni, JakeMalis, lin72h, bhupesh-sf, corupta, ghishadow, calvinf, jmorganca, Lukas1h, johnhamlin, and 2 more reacted with rocket emoji\u003c/tool-tip\u003e\n      \n    \u003c/div\u003e\n\u003c/form\u003e\u003c/div\u003e\n      \u003c/div\u003e\n\n\u003c/div\u003e\n\n\n        \n\n\n\n  \n  \n\n\n\n      \n\n    \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "21 min read",
  "publishedTime": null,
  "modifiedTime": null
}
