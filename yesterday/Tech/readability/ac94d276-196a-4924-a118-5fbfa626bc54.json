{
  "id": "ac94d276-196a-4924-a118-5fbfa626bc54",
  "title": "Anthropic is testing AI’s capacity for sabotage",
  "link": "https://mashable.com/article/anthropic-ai-use-sabotage",
  "description": "Anthropic is testing AI’s capacity for sabotage with a stress-test of its current models, like Claude.",
  "author": "",
  "published": "Mon, 21 Oct 2024 21:49:22 +0000",
  "source": "http://feeds.mashable.com/Mashable",
  "categories": null,
  "byline": "Chance Townsend",
  "length": 2999,
  "excerpt": "The Claude developers are eyeing potential misuse in current AI models.",
  "siteName": "Mashable",
  "favicon": "https://mashable.com/favicons/android-chrome-512x512.png",
  "text": "The Claude developers are eyeing potential misuse in current AI models. Credit: Mashable Composite; Anthropic / Claude As the hype around generative AI continues to build, the need for robust safety regulations is only becoming more clear.Now Anthropic—the company behind Claude AI—is looking at how its models could deceive or sabotage users. Anthropic just dropped a paper laying out their approach. Anthropic’s latest research — titled \"Sabotage Evaluations for Frontier Models\" — comes from its Alignment Science team, driven by the company's \"Responsible Scaling\" policy. The goal is to gauge just how capable AI might be at misleading users or even \"subverting the systems we put in place to oversee them.\" The study focuses on four specific tactics: Human Decision Sabotage, Code Sabotage, Sandbagging, and Undermining Oversight.Think of users who push ChatGPT to the limit, trying to coax it into generating inappropriate content or graphic images. These tests are all about ensuring that the AI can’t be tricked into breaking its own rules. Mashable Light Speed In the paper, Anthropic says its objective is to be ready for the possibility that AI could evolve into something with dangerous capabilities. So they put their Claude 3 Opus and 3.5 Sonnet models through a series of tests, designed to evaluate and enhance their safety protocols. The Human Decision test focused on examining how AI could potentially manipulate human decision-making. The second test, Code Sabotage, analyzed whether AI could subtly introduce bugs into coding databases. Stronger AI models actually led to stronger defenses against these kinds of vulnerabilities.The remaining tests — Sandbagging and Undermining Oversight — explored whether the AI could conceal its true capabilities or bypass safety mechanisms embedded within the system. For now, Anthropic’s research concludes that current AI models pose a low risk, at least in terms of these malicious capabilities. \"Minimal mitigations are currently sufficient to address sabotage risks,\" the team writes, but \"more realistic evaluations and stronger mitigations seem likely to be necessary soon as capabilities improve.\" Translation: watch out, world. Assistant Editor, General Assignments Currently residing in Chicago, Illinois, Chance Townsend is the General Assignments Editor at Mashable covering tech, video games, dating apps, digital culture, and whatever else comes his way. He has a Master's in Journalism from the University of North Texas and is a proud orange cat father.In his free time, he cooks, loves to sleep, and finds great enjoyment in Detroit sports. If you have any stories, tips, recipes, or wanna talk shop about the Lions/Tigers/Pistons/Red Wings you can reach him at [email protected] This newsletter may contain advertising, deals, or affiliate links. Subscribing to a newsletter indicates your consent to our Terms of Use and Privacy Policy. You may unsubscribe from the newsletters at any time.",
  "image": "https://helios-i.mashable.com/imagery/articles/04Xhacan7jwbXri8waUaHml/hero-image.fill.size_1200x675.v1729543167.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\n\u003cp\u003eThe Claude developers are eyeing potential misuse in current AI models.\u003c/p\u003e\n\n\n\u003c/div\u003e\u003csection data-ga-module=\"content_body\"\u003e\n\u003cdiv\u003e\n\u003cp\u003e\u003cimg src=\"https://helios-i.mashable.com/imagery/articles/04Xhacan7jwbXri8waUaHml/hero-image.fill.size_1248x702.v1729543167.jpg\" alt=\"Anthropic text surrounded by asterik logos\" width=\"1248\" height=\"702\" srcset=\"https://helios-i.mashable.com/imagery/articles/04Xhacan7jwbXri8waUaHml/hero-image.fill.size_400x225.v1729543167.jpg 400w, https://helios-i.mashable.com/imagery/articles/04Xhacan7jwbXri8waUaHml/hero-image.fill.size_800x450.v1729543167.jpg 800w, https://helios-i.mashable.com/imagery/articles/04Xhacan7jwbXri8waUaHml/hero-image.fill.size_1248x702.v1729543167.jpg 1600w\" sizes=\"(max-width: 1280px) 100vw, 1280px\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: Mashable Composite; Anthropic / Claude \u003c/span\u003e\n\u003c/p\u003e\n\u003c/div\u003e\n\u003carticle id=\"article\" data-autopogo=\"\"\u003e\n\u003cp\u003eAs the hype around \u003ca href=\"https://mashable.com/category/artificial-intelligence\" target=\"_self\" data-ga-click=\"1\" data-ga-element=\"offer\" data-ga-label=\"$text\" data-ga-item=\"text-link\" data-ga-module=\"content_body\"\u003egenerative AI\u003c/a\u003e continues to build, the \u003ca href=\"https://mashable.com/article/ai-safety-institute-finds-security-flaws-in-llms?test_uuid=01iI2GpryXngy77uIpA3Y4B\u0026amp;test_variant=a\" target=\"_self\" data-ga-click=\"1\" data-ga-element=\"offer\" data-ga-label=\"$text\" data-ga-item=\"text-link\" data-ga-module=\"content_body\"\u003eneed for robust safety regulations\u003c/a\u003e is only becoming more clear.\u003c/p\u003e\u003cp\u003eNow Anthropic—the company behind Claude AI—is looking at how its models could deceive or sabotage users. \u003ca href=\"https://www.anthropic.com/research/sabotage-evaluations\" target=\"_blank\" data-ga-click=\"1\" data-ga-element=\"offer\" data-ga-label=\"$text\" data-ga-item=\"text-link\" data-ga-module=\"content_body\" title=\"(opens in a new window)\"\u003eAnthropic just dropped a paper\u003c/a\u003e laying out their approach.\u003c/p\u003e\n\u003cp\u003eAnthropic’s \u003ca href=\"https://assets.anthropic.com/m/377027d5b36ac1eb/original/Sabotage-Evaluations-for-Frontier-Models.pdf\" target=\"_blank\" data-ga-click=\"1\" data-ga-element=\"offer\" data-ga-label=\"$text\" data-ga-item=\"text-link\" data-ga-module=\"content_body\" title=\"(opens in a new window)\"\u003elatest research\u003c/a\u003e — titled \u0026#34;Sabotage Evaluations for Frontier Models\u0026#34; — comes from its Alignment Science team, driven by the company\u0026#39;s \u0026#34;Responsible Scaling\u0026#34; policy. \u003c/p\u003e\u003cp\u003eThe goal is to gauge just how capable AI might be at misleading users or even \u0026#34;subverting the systems we put in place to oversee them.\u0026#34; The study focuses on four specific tactics: Human Decision Sabotage, Code Sabotage, Sandbagging, and Undermining Oversight.\u003c/p\u003e\u003cp\u003eThink of users who push ChatGPT to the limit, trying to coax it into generating inappropriate content or graphic images. These tests are all about ensuring that the AI can’t be tricked into breaking its own rules.\u003c/p\u003e\u003csection x-data=\"window.newsletter()\" x-init=\"init()\" data-ga-impression=\"\" data-ga-category=\"newsletters\" data-ga-module=\"incontent_nl_signup\" data-ga-label=\"mashablelightspeed\"\u003e\n\u003cp\u003e\nMashable Light Speed\n\u003c/p\u003e\n\n\n\u003c/section\u003e\n\u003cp\u003eIn the paper, Anthropic says its objective is to be ready for the possibility that AI could evolve into something with dangerous capabilities. So they put their Claude 3 Opus and 3.5 Sonnet models through a series of tests, designed to evaluate and enhance their safety protocols. \u003c/p\u003e\u003cp\u003eThe Human Decision test focused on examining how AI could potentially manipulate human decision-making. The second test, Code Sabotage, analyzed whether AI could subtly introduce bugs into coding databases. Stronger AI models actually led to stronger defenses against these kinds of vulnerabilities.\u003c/p\u003e\u003cp\u003eThe remaining tests — Sandbagging and Undermining Oversight — explored whether the AI could conceal its true capabilities or bypass safety mechanisms embedded within the system. \u003c/p\u003e\u003cp\u003eFor now, Anthropic’s research concludes that current AI models pose a low risk, at least in terms of these malicious capabilities. \u003c/p\u003e\u003cp\u003e\u0026#34;Minimal mitigations are currently sufficient to address sabotage risks,\u0026#34; the team writes, but \u0026#34;more realistic evaluations and stronger mitigations seem likely to be necessary soon as capabilities improve.\u0026#34; \u003c/p\u003e\u003cp\u003eTranslation: watch out, world. \u003c/p\u003e\n\n\u003c/article\u003e\n\u003cdiv\u003e\n\u003cdiv\u003e\n\u003cp\u003e\u003cimg src=\"https://helios-i.mashable.com/imagery/authors/03tgQJc7HbwhuDaGMuSAh8J/image.fill.size_100x100.v1673717191.jpg\" alt=\"Headshot of a Black man\" width=\"100\" height=\"100\" loading=\"lazy\"/\u003e\u003c/p\u003e\u003cdiv\u003e\n\n\u003cp\u003eAssistant Editor, General Assignments\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv\u003e\n\u003cp\u003eCurrently residing in Chicago, Illinois, Chance Townsend is the General Assignments Editor at Mashable covering tech, video games, dating apps, digital culture, and whatever else comes his way. He has a Master\u0026#39;s in Journalism from the University of North Texas and is a proud orange cat father.\u003c/p\u003e\u003cp\u003eIn his free time, he cooks, loves to sleep, and finds great enjoyment in Detroit sports. If you have any stories, tips, recipes, or wanna talk shop about the Lions/Tigers/Pistons/Red Wings you can reach him at \u003ca href=\"https://mashable.com/cdn-cgi/l/email-protection\" data-cfemail=\"fd9e959c939e98d389928a938e989399bd87949b9b909899949cd39e9290\"\u003e[email protected]\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\n\u003c/section\u003e\u003cdiv x-data=\"window.newsletter()\" x-init=\"init()\" data-ga-impression=\"\" data-ga-category=\"newsletters\" data-ga-module=\"footer_nl_signup\" data-ga-label=\"Top Stories\"\u003e\n\n\u003cp\u003e\nThis newsletter may contain advertising, deals, or affiliate links. Subscribing to a newsletter indicates your consent to our \u003ca href=\"https://www.ziffdavis.com/terms-of-use\" target=\"_blank\" rel=\"noopener\" title=\"(opens in a new window)\"\u003eTerms of Use\u003c/a\u003e and \u003ca href=\"https://www.ziffdavis.com/ztg-privacy-policy\" target=\"_blank\" rel=\"noopener\" title=\"(opens in a new window)\"\u003ePrivacy Policy\u003c/a\u003e. You may unsubscribe from the newsletters at any time.\n\u003c/p\u003e\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-10-21T21:49:22Z",
  "modifiedTime": "2024-10-21T21:49:22Z"
}
