{
  "id": "d467d41d-11a4-4f45-852b-96450aa31434",
  "title": "Character.AI steps up teen safety after bots allegedly caused suicide, self-harm",
  "link": "https://arstechnica.com/tech-policy/2024/12/character-ai-steps-up-teen-safety-after-bots-allegedly-caused-suicide-self-harm/",
  "description": "Character.AI's new model for teens doesn't resolve all of parents' concerns.",
  "author": "Ashley Belanger",
  "published": "Thu, 12 Dec 2024 21:15:11 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "Policy",
    "Artificial Intelligence",
    "Character.AI",
    "chatbots",
    "child safety",
    "suicide"
  ],
  "byline": "Ashley Belanger",
  "length": 2450,
  "excerpt": "Character.AI’s new model for teens doesn’t resolve all of parents’ concerns.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Following a pair of lawsuits alleging that chatbots caused a teen boy's suicide, groomed a 9-year-old girl, and caused a vulnerable teen to self-harm, Character.AI (C.AI) has announced a separate model just for teens, ages 13 and up, that's supposed to make their experiences with bots safer. In a blog, C.AI said it took a month to develop the teen model, with the goal of guiding the existing model \"away from certain responses or interactions, reducing the likelihood of users encountering, or prompting the model to return, sensitive or suggestive content.\" C.AI said \"evolving the model experience\" to reduce the likelihood kids are engaging in harmful chats—including bots allegedly teaching a teen with high-functioning autism to self-harm and delivering inappropriate adult content to all kids whose families are suing—it had to tweak both model inputs and outputs. To stop chatbots from initiating and responding to harmful dialogs, C.AI added classifiers that should help C.AI identify and filter out sensitive content from outputs. And to prevent kids from pushing bots to discuss sensitive topics, C.AI said that it had improved \"detection, response, and intervention related to inputs from all users.\" That ideally includes blocking any sensitive content from appearing in the chat. Perhaps most significantly, C.AI will now link kids to resources if they try to discuss suicide or self-harm, which C.AI had not done previously, frustrating parents suing who argue this common practice for social media platforms should extend to chatbots. Other teen safety features In addition to creating the model just for teens, C.AI announced other safety features, including more robust parental controls rolling out early next year. Those controls would allow parents to track how much time kids are spending on C.AI and which bots they're interacting with most frequently, the blog said. C.AI will also be notifying teens when they've spent an hour on the platform, which could help prevent kids from becoming addicted to the app, as parents suing have alleged. In one case, parents had to lock their son's iPad in a safe to keep him from using the app after bots allegedly repeatedly encouraged him to self-harm and even suggested murdering his parents. That teen has vowed to start using the app whenever he next has access, while parents fear the bots' seeming influence may continue causing harm if he follows through on threats to run away.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1354022389-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                      \n                      \n          \u003cp\u003eFollowing a pair of lawsuits alleging that chatbots caused a \u003ca href=\"https://arstechnica.com/tech-policy/2024/10/chatbots-posed-as-therapist-and-adult-lover-in-teen-suicide-case-lawsuit-says/\"\u003eteen boy\u0026#39;s suicide\u003c/a\u003e, \u003ca href=\"https://arstechnica.com/tech-policy/2024/12/chatbots-urged-teen-to-self-harm-suggested-murdering-parents-lawsuit-says/\"\u003egroomed a 9-year-old girl, and caused a vulnerable teen to self-harm\u003c/a\u003e, Character.AI (C.AI) has \u003ca href=\"https://blog.character.ai/how-character-ai-prioritizes-teen-safety/\"\u003eannounced\u003c/a\u003e a separate model just for teens, ages 13 and up, that\u0026#39;s supposed to make their experiences with bots safer.\u003c/p\u003e\n\u003cp\u003eIn a blog, C.AI said it took a month to develop the teen model, with the goal of guiding the existing model \u0026#34;away from certain responses or interactions, reducing the likelihood of users encountering, or prompting the model to return, sensitive or suggestive content.\u0026#34;\u003c/p\u003e\n\u003cp\u003eC.AI said \u0026#34;evolving the model experience\u0026#34; to reduce the likelihood kids are engaging in harmful chats—including bots allegedly teaching a teen with high-functioning autism to self-harm and delivering inappropriate adult content to all kids whose families are suing—it had to tweak both model inputs and outputs.\u003c/p\u003e\n\u003cp\u003eTo stop chatbots from initiating and responding to harmful dialogs, C.AI added classifiers that should help C.AI identify and filter out sensitive content from outputs. And to prevent kids from pushing bots to discuss sensitive topics, C.AI said that it had improved \u0026#34;detection, response, and intervention related to inputs from all users.\u0026#34; That ideally includes blocking any sensitive content from appearing in the chat.\u003c/p\u003e\n\u003cp\u003ePerhaps most significantly, C.AI will now link kids to resources if they try to discuss suicide or self-harm, which C.AI had not done previously, frustrating parents suing who argue this common practice for social media platforms should extend to chatbots.\u003c/p\u003e\n\u003ch2\u003eOther teen safety features\u003c/h2\u003e\n\u003cp\u003eIn addition to creating the model just for teens, C.AI announced other safety features, including more robust parental controls rolling out early next year. Those controls would allow parents to track how much time kids are spending on C.AI and which bots they\u0026#39;re interacting with most frequently, the blog said.\u003c/p\u003e\n\u003cp\u003eC.AI will also be notifying teens when they\u0026#39;ve spent an hour on the platform, which could help prevent kids from becoming addicted to the app, as parents suing have alleged. In one case, parents had to lock their son\u0026#39;s iPad in a safe to keep him from using the app after bots allegedly repeatedly encouraged him to self-harm and even suggested murdering his parents. That teen has vowed to start using the app whenever he next has access, while parents fear the bots\u0026#39; seeming influence may continue causing harm if he follows through on threats to run away.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2024-12-12T21:15:11Z",
  "modifiedTime": "2024-12-12T23:15:24Z"
}
