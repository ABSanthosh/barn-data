{
  "id": "2d093e79-229e-4ef1-9269-aaa1692002e4",
  "title": "Make Your Own Backup System – Part 1: Strategy Before Scripts",
  "link": "https://it-notes.dragas.net/2025/07/18/make-your-own-backup-system-part-1-strategy-before-scripts/",
  "description": "Comments",
  "author": "",
  "published": "Sat, 19 Jul 2025 19:43:23 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Stefano Marinelli",
  "length": 11819,
  "excerpt": "Scattered IT Notes - by Stefano Marinelli",
  "siteName": "IT Notes",
  "favicon": "https://it-notes.dragas.net/assets/img/favicon-32x32.png",
  "text": "Backup: Beyond the Simple CopyFor as long as I can remember, backup is something that has been underestimated by far too many people. Between flawed techniques, \"Schrödinger's backups\" (i.e., never tested, thus both valid and invalid at the same time), and conceptual errors about what they are and how they work (RAID is not a backup!), too much data has been lost due to deficiencies in this area.Nowadays, backup is often an afterthought. Many rely entirely on \"the cloud\" without ever asking how - or if - their data is actually protected. It's a detail many overlook, but even major cloud providers operate on a shared responsibility model. Their terms often clarify that while they secure the infrastructure, the ultimate responsibility for protecting and backing up your data lies with you. By putting everything \"in the cloud\", on clusters owned by other companies, or on distributed Kubernetes systems, backup seems unnecessary. When I sometimes ask developers or colleagues how they handle backups for all this, they look at me as if I'm speaking an archaic, unknown, and indecipherable language. The thought has simply never crossed their minds. But data is not ephemeral; it must be preserved in every way possible.I've always had a philosophy: data must always be restorable (and as quickly as possible), in an open format (meaning you shouldn't have to buy anything to restore or consult it), and consistent. These points may seem obvious, but they are not.I have encountered various scenarios of data loss:Datacenters destroyed by fire – I had 142 servers there, but they were all restored in just a few hours.Server rooms flooded.Servers destroyed in earthquakes, often due to collapsing walls.Increasing incidents of various ransomware attacks.Intentional damage by entities seeking to create problems.Mistakes made by administrators, which can happen to anyone.The risk escalates for servers connected to the internet, like e-commerce and email servers. Here, not only is data integrity crucial, but so is the uninterrupted operation of services. This series of posts will revisit some of my old articles to explain my core ideas on the subject and describe, at least in part, my primary techniques.Many consider a backup to be a simple copy. I often hear people say they have backups because they \"copy the data\", but this is often wrong and extremely dangerous, providing a false sense of security. Copying the files of a live database is an almost useless operation, as the result will nearly always be impossible to restore. It is essential to at least perform a proper dump and then transfer that file. Yet, many people do this and will only realize their mistake when they face an emergency and need to restore.The Backup Plan: Asking the Right QuestionsBefore touching a single file, you must start with a plan, and that plan starts with asking the right questions:\"How much risk am I willing to take? What data do I need to protect? What downtime can I tolerate in case of data loss? What type and amount of storage space do I have available?\"The first question is particularly critical. A common but risky approach is to store a backup on the same machine that requires backing up. While convenient, this method fails in the event of a machine failure. Even relying on a classic USB drive for daily backups is not foolproof, as these devices are as susceptible to failure as any other hardware component. And contrary to popular belief, even high-end uninterruptible power supplies (UPS) are not immune to catastrophic failures.Thus, the initial step is to establish a management plan, balancing security and cost. The safest backup is often the one stored farthest from the source machine. However, this approach introduces challenges related to space and bandwidth. While local area network (LAN) backups are relatively straightforward, off-network backups involve additional connectivity considerations. This might lead to a compromise on the amount of data backed up to maintain operational speed during both backup and recovery processes.Safety doesn't always equate to practicality. For instance, with a 200 MBit/sec connection and 2 TB of backup data, the recovery time could be significant. However, if the goal is not rapid restorability but simply ensuring the data is available, the safest backup is often the one closest to us. That is, a backup we can \"touch\", disconnect, and consult even when offline.Therefore, it is essential to develop a backup policy tailored to specific needs, keeping in mind that no 'perfect' solution exists.The Core Decision: Full Disk vs. Individual FilesWhen planning a backup strategy, one key decision is whether to back up the entire disk or just individual files. Or both of them. Each approach has its pros and cons.Entire Disk (or Storage) BackupAdvantages:Complete Recovery: Restoring a full disk backup can quickly revert a system to its exact previous state, boot loader included.Integration in Virtualization Systems: If your VM is a single file on a filesystem like ZFS or btrfs, you can simply copy that file (after taking a snapshot) to get a complete backup of the VM. Solutions like Proxmox offer easy management of full disk backups, accessible via command line or web interface.Flexibility in Virtual Environments: Products like the Proxmox Backup Server offer the ability to recover individual files from a full backup, combining the benefits of both approaches.Disadvantages:Downtime for Physical Machines: Often, it's necessary to shut down the machine to create a full disk backup, leading to operational interruptions. A hybrid approach, if the physical host is running FreeBSD for example, is to take a snapshot and copy all the host's datasets. The restore process, however, will require some manual operations.Large Space Requirements: Full disk backups can consume substantial space, including unnecessary data.Potential Slowdowns and Compatibility Issues: The backup process can be slow and may encounter issues with non-standard file system configurations.Individual File BackupWhile it might seem simpler, backing up individual files can get complicated.Advantages:Basic Utility Use: Possible with standard system utilities like tar, cp, rsync, etc.Granular Backups: Allows for backing up specific files and comparing them to previous versions.Delta Copying: Only modified parts of the files are backed up, saving space and reducing data transfer.Portability and Partial Recovery: Files can be moved individually and partially restored as needed.Compression and Deduplication: These features are often available at the file or block level.Operational Continuity: Can be done without shutting down the machine.Disadvantages:Storage Space Requirements: Simple copy solutions might require significant storage.Need for File System Snapshot: For efficient and consistent backups, a snapshot (like native ZFS snapshots, btrfs, LVM Volume snapshots, or Microsoft's VSS) is highly recommended before copying.Hidden Pitfalls: Issues may not become apparent until a backup is needed. And by then, it may be too late.The Key to Consistency: The Power of SnapshotsBacking up a \"live\" file system involves a \"start\" and an \"end\" moment. During this time, the data can change, leading to fatal inconsistencies. I've encountered such issues in the past: a large MySQL database was compromised, and I was tasked with its recovery. I confidently took the client's last file-based backup and restored various files (not a native dump). Unsurprisingly, the database failed to restart. The large data file had changed too much between the start and end of the copy, rendering it inconsistent. Fortunately, I also had a proper dump, so I managed to recover from that.The issue is evident: backing up a live file system is risky. An open database, even a basic one like a browser's history, is highly likely to get corrupted, making the backup useless.The solution is to create a snapshot of the entire file system before beginning the backup. This approach freezes a consistent \"point-in-time\" view of the data. To date, using snapshots, I have managed to recover everything.Here are the methods I've explored over the years:Native File System Snapshot (e.g., BTRFS or ZFS): If your file system inherently supports snapshots, it's wise to use this feature. It's likely to be the most efficient and technically sound option.LVM Snapshot: For those using LVM, creating a snapshot of the logical volume is a viable approach. This method can lead to some space wastage and, while I still use it, has occasionally caused the file system to freeze during the snapshot's destruction, necessitating a reboot. This has been a rare but recurring issue across different hardware setups, especially under high I/O load.DattoBD: I've tracked this tool since its inception. I used it extensively in the past, but I sometimes faced stability issues (kernel panics or the inability to delete snapshots, forcing a reboot). For snapshots with Datto, I often use UrBackup scripts, which are convenient and efficient.The Architecture: Push or Pull?A longstanding debate among experts is whether backups should be initiated by the client (push) or requested by the server (pull). In my view, it depends.Generally, I prefer centralized backup systems on dedicated servers, maintained in highly secure environments with minimal services running. Therefore, I lean towards the \"pull\" method, where the server connects to the client to initiate the backup.Ideally, the backup server should not be reachable from the outside. It should be protected, hardened, and only be able to reach the setups it needs to back up. The goal is to minimize the possibility that the backup data could be compromised or deleted in case the client machine itself is attacked (which, unfortunately, is not so rare).This is not always possible, but there are ways to mitigate this problem. One way is to ensure that machines that must be backed up via \"push\" (i.e., by contacting the backup server themselves) can only access their own space. More importantly, the backup server, for security reasons, should maintain its own filesystem snapshots for a certain period. In this way, even in the worst-case scenario (workload compromised -\u003e connection to backup server -\u003e deletion of backups to demand a ransom), the backup server has its own snapshots. These server-side snapshots should not be accessible from the client host and should be kept long enough to ensure any compromise can be detected in time.My Guiding Principles for a Good Backup SystemOver the years, I've favored having granular control over backups, often finding the need to recover specific files or emails accidentally deleted by clients. A good backup system, in my opinion, should possess these key features:Instant Recovery and Speed: The system should enable quick recovery and operate at a high processing speed.External Storage: Backups must be stored externally, not on the same system being backed up. Still, local snapshots are a good idea for immediate rollbacks.Security: I avoid using mainstream cloud storage services like Dropbox or Google Drive for primary backups. Own your data! Efficient Space Management: This includes features like compression and deduplication.Minimal Invasiveness: The system should require minimal additional components to function.Conclusion: What's NextThe approach to backup is varied, and in this series, I will describe the main scenarios I usually face. I will start with the backup servers and their primary configurations, then move on to the various software and techniques I use.But that will begin with the next post, where I'll talk about building the backup server which, of course, will be powered by FreeBSD - like all my backup servers for the last 20 years.",
  "image": "https://it-notes.dragas.net//featured/hard_disk.webp",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003ch3\u003eBackup: Beyond the Simple Copy\u003c/h3\u003e\u003cp\u003eFor as long as I can remember, backup is something that has been underestimated by far too many people. Between flawed techniques, \u0026#34;Schrödinger\u0026#39;s backups\u0026#34; (i.e., never tested, thus both valid and invalid at the same time), and conceptual errors about what they are and how they work (RAID is not a backup!), too much data has been lost due to deficiencies in this area.\u003c/p\u003e\u003cp\u003eNowadays, backup is often an afterthought. Many rely entirely on \u0026#34;the cloud\u0026#34; without ever asking how - or if - their data is actually protected. It\u0026#39;s a detail many overlook, but even major cloud providers operate on a shared responsibility model. Their terms often clarify that while they secure the infrastructure, the ultimate responsibility for protecting and backing up your data lies with you. By putting everything \u0026#34;in the cloud\u0026#34;, on clusters owned by other companies, or on distributed Kubernetes systems, backup seems unnecessary. When I sometimes ask developers or colleagues how they handle backups for all this, they look at me as if I\u0026#39;m speaking an archaic, unknown, and indecipherable language. The thought has simply never crossed their minds. But data is not ephemeral; it must be preserved in every way possible.\u003c/p\u003e\u003cp\u003eI\u0026#39;ve always had a philosophy: data must always be restorable (and as quickly as possible), in an open format (meaning you shouldn\u0026#39;t have to buy anything to restore or consult it), and consistent. These points may seem obvious, but they are not.\u003c/p\u003e\u003cp\u003eI have encountered various scenarios of data loss:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://www.reuters.com/article/idUSKBN2B20NT/\"\u003eDatacenters destroyed by fire\u003c/a\u003e – I had 142 servers there, but they were all restored in just a few hours.\u003c/li\u003e\u003cli\u003eServer rooms flooded.\u003c/li\u003e\u003cli\u003eServers destroyed in earthquakes, often due to collapsing walls.\u003c/li\u003e\u003cli\u003eIncreasing incidents of various ransomware attacks.\u003c/li\u003e\u003cli\u003eIntentional damage by entities seeking to create problems.\u003c/li\u003e\u003cli\u003eMistakes made by administrators, which can happen to anyone.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe risk escalates for servers connected to the internet, like e-commerce and email servers. Here, not only is data integrity crucial, but so is the uninterrupted operation of services. This series of posts will revisit some of my old articles to explain my core ideas on the subject and describe, at least in part, my primary techniques.\u003c/p\u003e\u003cp\u003eMany consider a backup to be a simple copy. I often hear people say they have backups because they \u0026#34;copy the data\u0026#34;, but this is often wrong and extremely dangerous, providing a false sense of security. Copying the files of a live database is an almost useless operation, as the result will nearly always be impossible to restore. It is essential to at least perform a proper dump and then transfer that file. Yet, many people do this and will only realize their mistake when they face an emergency and need to restore.\u003c/p\u003e\u003ch3\u003eThe Backup Plan: Asking the Right Questions\u003c/h3\u003e\u003cp\u003eBefore touching a single file, you must start with a plan, and that plan starts with asking the right questions:\u003c/p\u003e\u003cp\u003e\u0026#34;How much risk am I willing to take? What data do I need to protect? What downtime can I tolerate in case of data loss? What type and amount of storage space do I have available?\u0026#34;\u003c/p\u003e\u003cp\u003eThe first question is particularly critical. A common but risky approach is to store a backup on the same machine that requires backing up. While convenient, this method fails in the event of a machine failure. Even relying on a classic USB drive for daily backups is not foolproof, as these devices are as susceptible to failure as any other hardware component. And contrary to popular belief, even high-end uninterruptible power supplies (UPS) are not immune to catastrophic failures.\u003c/p\u003e\u003cp\u003eThus, the initial step is to establish a management plan, balancing security and cost. The safest backup is often the one stored farthest from the source machine. However, this approach introduces challenges related to space and bandwidth. While local area network (LAN) backups are relatively straightforward, off-network backups involve additional connectivity considerations. This might lead to a compromise on the amount of data backed up to maintain operational speed during both backup and recovery processes.\u003c/p\u003e\u003cp\u003eSafety doesn\u0026#39;t always equate to practicality. For instance, with a 200 MBit/sec connection and 2 TB of backup data, the recovery time could be significant. However, if the goal is not rapid restorability but simply ensuring the data is available, the safest backup is often the one closest to us. That is, a backup we can \u0026#34;touch\u0026#34;, disconnect, and consult even when offline.\u003c/p\u003e\u003cp\u003eTherefore, it is essential to develop a backup policy tailored to specific needs, keeping in mind that no \u0026#39;perfect\u0026#39; solution exists.\u003c/p\u003e\u003ch3\u003eThe Core Decision: Full Disk vs. Individual Files\u003c/h3\u003e\u003cp\u003eWhen planning a backup strategy, one key decision is whether to back up the entire disk or just individual files. Or both of them. Each approach has its pros and cons.\u003c/p\u003e\u003ch4\u003eEntire Disk (or Storage) Backup\u003c/h4\u003e\u003cp\u003eAdvantages:\u003c/p\u003e\u003cul\u003e\u003cli\u003eComplete Recovery: Restoring a full disk backup can quickly revert a system to its exact previous state, boot loader included.\u003c/li\u003e\u003cli\u003eIntegration in Virtualization Systems: If your VM is a single file on a filesystem like ZFS or btrfs, you can simply copy that file (after taking a snapshot) to get a complete backup of the VM. Solutions like Proxmox offer easy management of full disk backups, accessible via command line or web interface.\u003c/li\u003e\u003cli\u003eFlexibility in Virtual Environments: Products like the Proxmox Backup Server offer the ability to recover individual files from a full backup, combining the benefits of both approaches.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDisadvantages:\u003c/p\u003e\u003cul\u003e\u003cli\u003eDowntime for Physical Machines: Often, it\u0026#39;s necessary to shut down the machine to create a full disk backup, leading to operational interruptions. A hybrid approach, if the physical host is running FreeBSD for example, is to take a snapshot and copy all the host\u0026#39;s datasets. The restore process, however, will require some manual operations.\u003c/li\u003e\u003cli\u003eLarge Space Requirements: Full disk backups can consume substantial space, including unnecessary data.\u003c/li\u003e\u003cli\u003ePotential Slowdowns and Compatibility Issues: The backup process can be slow and may encounter issues with non-standard file system configurations.\u003c/li\u003e\u003c/ul\u003e\u003ch4\u003eIndividual File Backup\u003c/h4\u003e\u003cp\u003eWhile it might seem simpler, backing up individual files can get complicated.\u003c/p\u003e\u003cp\u003eAdvantages:\u003c/p\u003e\u003cul\u003e\u003cli\u003eBasic Utility Use: Possible with standard system utilities like tar, cp, rsync, etc.\u003c/li\u003e\u003cli\u003eGranular Backups: Allows for backing up specific files and comparing them to previous versions.\u003c/li\u003e\u003cli\u003eDelta Copying: Only modified parts of the files are backed up, saving space and reducing data transfer.\u003c/li\u003e\u003cli\u003ePortability and Partial Recovery: Files can be moved individually and partially restored as needed.\u003c/li\u003e\u003cli\u003eCompression and Deduplication: These features are often available at the file or block level.\u003c/li\u003e\u003cli\u003eOperational Continuity: Can be done without shutting down the machine.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eDisadvantages:\u003c/p\u003e\u003cul\u003e\u003cli\u003eStorage Space Requirements: Simple copy solutions might require significant storage.\u003c/li\u003e\u003cli\u003eNeed for File System Snapshot: For efficient and consistent backups, a snapshot (like native ZFS snapshots, btrfs, LVM Volume snapshots, or Microsoft\u0026#39;s VSS) is highly recommended before copying.\u003c/li\u003e\u003cli\u003eHidden Pitfalls: Issues may not become apparent until a backup is needed. And by then, it may be too late.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eThe Key to Consistency: The Power of Snapshots\u003c/h3\u003e\u003cp\u003eBacking up a \u0026#34;live\u0026#34; file system involves a \u0026#34;start\u0026#34; and an \u0026#34;end\u0026#34; moment. During this time, the data can change, leading to fatal inconsistencies. I\u0026#39;ve encountered such issues in the past: a large MySQL database was compromised, and I was tasked with its recovery. I confidently took the client\u0026#39;s last file-based backup and restored various files (not a native dump). Unsurprisingly, the database failed to restart. The large data file had changed too much between the start and end of the copy, rendering it inconsistent. Fortunately, I also had a proper dump, so I managed to recover from that.\u003c/p\u003e\u003cp\u003eThe issue is evident: backing up a live file system is risky. An open database, even a basic one like a browser\u0026#39;s history, is highly likely to get corrupted, making the backup useless.\u003c/p\u003e\u003cp\u003eThe solution is to create a snapshot of the entire file system before beginning the backup. This approach freezes a consistent \u0026#34;point-in-time\u0026#34; view of the data. To date, using snapshots, I have managed to recover everything.\u003c/p\u003e\u003cp\u003eHere are the methods I\u0026#39;ve explored over the years:\u003c/p\u003e\u003cul\u003e\u003cli\u003eNative File System Snapshot (e.g., \u003ca href=\"https://it-notes.dragas.net/2020/06/28/btrfs-automatic-snapshots-and-remote-backups/\"\u003eBTRFS\u003c/a\u003e or \u003ca href=\"https://it-notes.dragas.net/2022/05/30/how-we-are-migrating-many-of-our-servers-from-linux-to-freebsd-part-2/\"\u003eZFS\u003c/a\u003e): If your file system inherently supports snapshots, it\u0026#39;s wise to use this feature. It\u0026#39;s likely to be the most efficient and technically sound option.\u003c/li\u003e\u003cli\u003eLVM Snapshot: For those using LVM, creating a snapshot of the logical volume is a viable approach. This method can lead to some space wastage and, while I still use it, has occasionally caused the file system to freeze during the snapshot\u0026#39;s destruction, necessitating a reboot. This has been a rare but recurring issue across different hardware setups, especially under high I/O load.\u003c/li\u003e\u003cli\u003eDattoBD: I\u0026#39;ve tracked this tool since its inception. I used it extensively in the past, but I sometimes faced stability issues (kernel panics or the inability to delete snapshots, forcing a reboot). For snapshots with Datto, I often use UrBackup scripts, which are convenient and efficient.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eThe Architecture: Push or Pull?\u003c/h3\u003e\u003cp\u003eA longstanding debate among experts is whether backups should be initiated by the client (push) or requested by the server (pull). In my view, it depends.\u003c/p\u003e\u003cp\u003eGenerally, I prefer centralized backup systems on dedicated servers, maintained in highly secure environments with minimal services running. Therefore, I lean towards the \u0026#34;pull\u0026#34; method, where the server connects to the client to initiate the backup.\u003c/p\u003e\u003cp\u003eIdeally, the backup server should not be reachable from the outside. It should be protected, hardened, and only be able to reach the setups it needs to back up. The goal is to minimize the possibility that the backup data could be compromised or deleted in case the client machine itself is attacked (which, unfortunately, is not so rare).\u003c/p\u003e\u003cp\u003eThis is not always possible, but there are ways to mitigate this problem. One way is to ensure that machines that must be backed up via \u0026#34;push\u0026#34; (i.e., by contacting the backup server themselves) can only access their own space. More importantly, the backup server, for security reasons, should maintain its own filesystem snapshots for a certain period. In this way, even in the worst-case scenario (workload compromised -\u0026gt; connection to backup server -\u0026gt; deletion of backups to demand a ransom), the backup server has its own snapshots. These server-side snapshots should not be accessible from the client host and should be kept long enough to ensure any compromise can be detected in time.\u003c/p\u003e\u003ch3\u003eMy Guiding Principles for a Good Backup System\u003c/h3\u003e\u003cp\u003eOver the years, I\u0026#39;ve favored having granular control over backups, often finding the need to recover specific files or emails accidentally deleted by clients. A good backup system, in my opinion, should possess these key features:\u003c/p\u003e\u003cul\u003e\u003cli\u003eInstant Recovery and Speed: The system should enable quick recovery and operate at a high processing speed.\u003c/li\u003e\u003cli\u003eExternal Storage: Backups must be stored externally, not on the same system being backed up. Still, local snapshots are a good idea for immediate rollbacks.\u003c/li\u003e\u003cli\u003eSecurity: I avoid using mainstream cloud storage services like Dropbox or Google Drive for primary backups. Own your data! \u003c/li\u003e\u003cli\u003eEfficient Space Management: This includes features like compression and deduplication.\u003c/li\u003e\u003cli\u003eMinimal Invasiveness: The system should require minimal additional components to function.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eConclusion: What\u0026#39;s Next\u003c/h3\u003e\u003cp\u003eThe approach to backup is varied, and in this series, I will describe the main scenarios I usually face. I will start with the backup servers and their primary configurations, then move on to the various software and techniques I use.\u003c/p\u003e\u003cp\u003eBut that will begin with the next post, where I\u0026#39;ll talk about building the backup server which, of course, will be powered by FreeBSD - like all my backup servers for the last 20 years.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": null,
  "modifiedTime": null
}
