{
  "id": "e8a0e7ee-6659-4374-a727-3194d437faa0",
  "title": "DeepMind has detailed all the ways AGI could wreck the world",
  "link": "https://arstechnica.com/ai/2025/04/google-deepmind-releases-its-plan-to-keep-agi-from-running-wild/",
  "description": "DeepMind says AGI could arrive in 2030, and it has some ideas to keep us safe.",
  "author": "Ryan Whitwam",
  "published": "Thu, 03 Apr 2025 21:43:32 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Google",
    "agi",
    "Artificial Intelligence",
    "google"
  ],
  "byline": "Ryan Whitwam",
  "length": 7491,
  "excerpt": "DeepMind says AGI could arrive in 2030, and it has some ideas to keep us safe.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Skip to content DeepMind says AGI could arrive in 2030, and it has some ideas to keep us safe. As AI hype permeates the Internet, tech and business leaders are already looking toward the next step. AGI, or artificial general intelligence, refers to a machine with human-like intelligence and capabilities. If today's AI systems are on a path to AGI, we will need new approaches to ensure such a machine doesn't work against human interests. Unfortunately, we don't have anything as elegant as Isaac Asimov's Three Laws of Robotics. Researchers at DeepMind have been working on this problem and have released a new technical paper (PDF) that explains how to develop AGI safely, which you can download at your convenience. It contains a huge amount of detail, clocking in at 108 pages before references. While some in the AI field believe AGI is a pipe dream, the authors of the DeepMind paper project that it could happen by 2030. With that in mind, they aimed to understand the risks of a human-like synthetic intelligence, which they acknowledge could lead to \"severe harm.\" All the ways AGI could suck for humanity This work has identified four possible types of AGI risk, along with suggestions on how we might ameliorate said risks. The DeepMind team, led by company co-founder Shane Legg, categorized the negative AGI outcomes as misuse, misalignment, mistakes, and structural risks. Misuse and misalignment are discussed in the paper at length, but the latter two are only covered briefly. The four categories of AGI risk, as determined by DeepMind. Credit: Google DeepMind The four categories of AGI risk, as determined by DeepMind. Credit: Google DeepMind The first possible issue, misuse, is fundamentally similar to current AI risks. However, because AGI will be more powerful by definition, the damage it could do is much greater. A ne'er-do-well with access to AGI could misuse the system to do harm, for example, by asking the system to identify and exploit zero-day vulnerabilities or create a designer virus that could be used as a bioweapon. DeepMind says companies developing AGI will have to conduct extensive testing and create robust post-training safety protocols. Essentially, AI guardrails on steroids. They also suggest devising a method to suppress dangerous capabilities entirely, sometimes called \"unlearning,\" but it's unclear if this is possible without substantially limiting models. Misalignment is largely not something we have to worry about with generative AI as it currently exists. This type of AGI harm is envisioned as a rogue machine that has shaken off the limits imposed by its designers. Terminators, anyone? More specifically, the AI takes actions it knows the developer did not intend. DeepMind says its standard for misalignment here is more advanced than simple deception or scheming as seen in the current literature. To avoid that, DeepMind suggests developers use techniques like amplified oversight, in which two copies of an AI check each other's output, to create robust systems that aren't likely to go rogue. If that fails, DeepMind suggests intensive stress testing and monitoring to watch for any hint that an AI might be turning against us. Keeping AGIs in virtual sandboxes with strict security and direct human oversight could help mitigate issues arising from misalignment. Basically, make sure there's an \"off\" switch. If, on the other hand, an AI didn't know that its output would be harmful and the human operator didn't intend for it to be, that's a mistake. We get plenty of those with current AI systems—remember when Google said to put glue on pizza? The \"glue\" for AGI could be much stickier, though. DeepMind notes that militaries may deploy AGI due to \"competitive pressure,\" but such systems could make serious mistakes as they will be tasked with much more elaborate functions than today's AI. The paper doesn't have a great solution for mitigating mistakes. It boils down to not letting AGI get too powerful in the first place. DeepMind calls for deploying slowly and limiting AGI authority. The study also suggests passing AGI commands through a \"shield\" system that ensures they are safe before implementation. Lastly, there are structural risks, which DeepMind defines as the unintended but real consequences of multi-agent systems contributing to our already complex human existence. For example, AGI could create false information that is so believable that we no longer know who or what to trust. The paper also raises the possibility that AGI could accumulate more and more control over economic and political systems, perhaps by devising heavy-handed tariff schemes. Then one day, we look up and realize the machines are in charge instead of us. This category of risk is also the hardest to guard against because it would depend on how people, infrastructure, and institutions operate in the future. AGI in five years? No one knows if the thinking machines are really just a few years away, but there are plenty of tech leaders who are confident enough to say so. Part of the problem in predicting the emergence of AGI is that we're still just speculating about how human-like intelligence would manifest itself in a machine. Anyone who has used generative AI systems over the past years has seen real, tangible improvements, but does that trajectory lead to true human-like capabilities? We recently talked about a range of AI topics, including AGI, with Google's Tulsee Doshi, director of product management for Gemini. \"Different people have different definitions of AGI, and so depending on who you talk to, how close or far we are from AGI is a different conversation,\" said Doshi. \"What I would say is LLMs, Gemini, and the training of smarter and smarter models is on the path to models that are going to be at extremely high intelligence. And that has a ton of value in and of itself.\" This paper is not the final word on AGI safety—DeepMind notes this is just a \"starting point for vital conversations.\" If the team is right, and AGI will transform the world in five short years, those conversations need to happen soon. If not, well, a lot of people are going to look kind of silly. Ryan Whitwam is a senior technology reporter at Ars Technica, covering the ways Google, AI, and mobile technology continue to change the world. Over his 20-year career, he's written for Android Police, ExtremeTech, Wirecutter, NY Times, and more. He has reviewed more phones than most people will ever own. You can follow him on Bluesky, where you will see photos of his dozens of mechanical keyboards. 35 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1467485842-1-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"app\"\u003e\n    \u003cp\u003e\u003ca href=\"#main\"\u003e\n  Skip to content\n\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cmain id=\"main\"\u003e\n            \u003carticle data-id=\"2086801\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n      \n\n      \n\n      \u003cp\u003e\n        DeepMind says AGI could arrive in 2030, and it has some ideas to keep us safe.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \u003cp\u003eAs AI hype permeates the Internet, tech and business leaders are already looking toward the next step. AGI, or artificial general intelligence, refers to a machine with \u003ca href=\"https://arstechnica.com/science/2025/03/ai-versus-the-brain-and-the-race-for-general-intelligence/\"\u003ehuman-like intelligence and capabilities\u003c/a\u003e. If today\u0026#39;s AI systems are on a path to AGI, we will need new approaches to ensure such a machine doesn\u0026#39;t work against human interests.\u003c/p\u003e\n\u003cp\u003eUnfortunately, we don\u0026#39;t have anything as elegant as Isaac Asimov\u0026#39;s Three Laws of Robotics. Researchers at DeepMind have been working on this problem and have released a \u003ca href=\"https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf\"\u003enew technical paper\u003c/a\u003e (PDF) that explains how to develop AGI safely, which you can download at your convenience.\u003c/p\u003e\n\u003cp\u003eIt contains a huge amount of detail, clocking in at 108 pages \u003cem\u003ebefore\u003c/em\u003e references. While some in the AI field believe AGI is a pipe dream, the authors of the DeepMind paper project that it could happen by 2030. With that in mind, they aimed to understand the risks of a human-like synthetic intelligence, which they acknowledge could lead to \u0026#34;severe harm.\u0026#34;\u003c/p\u003e\n\u003ch2\u003eAll the ways AGI could suck for humanity\u003c/h2\u003e\n\u003cp\u003eThis work has identified four possible types of AGI risk, along with suggestions on how we might ameliorate said risks. The DeepMind team, led by company co-founder Shane Legg, categorized the negative AGI outcomes as misuse, misalignment, mistakes, and structural risks. Misuse and misalignment are discussed in the paper at length, but the latter two are only covered briefly.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cdiv\u003e\n            \u003cp\u003e\u003ca data-pswp-width=\"1886\" data-pswp-height=\"1402\" data-pswp-srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk.png 1886w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-640x476.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-1024x761.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-768x571.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-1536x1142.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-980x729.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-1440x1070.png 1440w\" data-cropped=\"false\" href=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk.png\" target=\"_blank\"\u003e\n              \u003cimg width=\"1886\" height=\"1402\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk.png\" alt=\"table of AGI risks\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk.png 1886w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-640x476.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-1024x761.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-768x571.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-1536x1142.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-980x729.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/agi-risk-1440x1070.png 1440w\" sizes=\"auto, (max-width: 1886px) 100vw, 1886px\"/\u003e\n            \u003c/a\u003e\u003c/p\u003e\u003cdiv id=\"caption-2086802\"\u003e\u003cp\u003e\n              The four categories of AGI risk, as determined by DeepMind.\n                              \u003c/p\u003e\u003cp\u003e\n                  Credit:\n                                      Google DeepMind\n                                  \u003c/p\u003e\n                          \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      The four categories of AGI risk, as determined by DeepMind.\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Google DeepMind\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eThe first possible issue, misuse, is fundamentally similar to current AI risks. However, because AGI will be more powerful by definition, the damage it could do is much greater. A ne\u0026#39;er-do-well with access to AGI could misuse the system to do harm, for example, by asking the system to identify and exploit zero-day vulnerabilities or create a designer virus that could be used as a bioweapon.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eDeepMind says companies developing AGI will have to conduct extensive testing and create robust post-training safety protocols. Essentially, AI guardrails on steroids. They also suggest devising a method to suppress dangerous capabilities entirely, sometimes called \u0026#34;unlearning,\u0026#34; but it\u0026#39;s unclear if this is possible without substantially limiting models.\u003c/p\u003e\n\u003cp\u003eMisalignment is largely not something we have to worry about with generative AI as it currently exists. This type of AGI harm is envisioned as a rogue machine that has shaken off the limits imposed by its designers. \u003ca href=\"https://arstechnica.com/ai/2024/10/40-years-later-the-terminator-still-shapes-our-view-of-ai/\"\u003eTerminators\u003c/a\u003e, anyone? More specifically, the AI takes actions it knows the developer did not intend. DeepMind says its standard for misalignment here is more advanced than simple deception or scheming as seen in the current literature.\u003c/p\u003e\n\u003cp\u003eTo avoid that, DeepMind suggests developers use techniques like amplified oversight, in which two copies of an AI check each other\u0026#39;s output, to create robust systems that aren\u0026#39;t likely to go rogue. If that fails, DeepMind suggests intensive stress testing and monitoring to watch for any hint that an AI might be turning against us. Keeping AGIs in virtual sandboxes with strict security and direct human oversight could help mitigate issues arising from misalignment. Basically, make sure there\u0026#39;s an \u0026#34;off\u0026#34; switch.\u003c/p\u003e\n\u003cp\u003eIf, on the other hand, an AI didn\u0026#39;t know that its output would be harmful and the human operator didn\u0026#39;t intend for it to be, that\u0026#39;s a mistake. We get plenty of those with current AI systems—remember when \u003ca href=\"https://arstechnica.com/information-technology/2024/05/googles-ai-overview-can-give-false-misleading-and-dangerous-answers/\"\u003eGoogle said to put glue on pizza\u003c/a\u003e? The \u0026#34;glue\u0026#34; for AGI could be much stickier, though. DeepMind notes that militaries may deploy AGI due to \u0026#34;competitive pressure,\u0026#34; but such systems could make serious mistakes as they will be tasked with much more elaborate functions than today\u0026#39;s AI.\u003c/p\u003e\n\u003cp\u003eThe paper doesn\u0026#39;t have a great solution for mitigating mistakes. It boils down to not letting AGI get too powerful in the first place. DeepMind calls for deploying slowly and limiting AGI authority. The study also suggests passing AGI commands through a \u0026#34;shield\u0026#34; system that ensures they are safe before implementation.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003cp\u003eLastly, there are structural risks, which DeepMind defines as the unintended but real consequences of multi-agent systems contributing to our already complex human existence. For example, AGI could create false information that is so believable that we no longer know who or what to trust. The paper also raises the possibility that AGI could accumulate more and more control over economic and political systems, perhaps by \u003ca href=\"https://arstechnica.com/tech-policy/2025/04/critics-suspect-trumps-weird-tariff-math-came-from-chatbots/\"\u003edevising heavy-handed tariff schemes\u003c/a\u003e. Then one day, we look up and realize the machines are in charge instead of us. This category of risk is also the hardest to guard against because it would depend on how people, infrastructure, and institutions operate in the future.\u003c/p\u003e\n\u003ch2\u003eAGI in five years?\u003c/h2\u003e\n\u003cp\u003eNo one knows if the thinking machines are really just a few years away, but there are plenty of tech leaders who are \u003ca href=\"https://arstechnica.com/google/2025/02/sergey-brin-says-agi-is-within-reach-if-googlers-work-60-hour-weeks/\"\u003econfident enough to say so\u003c/a\u003e. Part of the problem in predicting the emergence of AGI is that we\u0026#39;re still just speculating about how human-like intelligence would manifest itself in a machine. Anyone who has used generative AI systems over the past years has seen real, tangible improvements, but does that trajectory lead to true human-like capabilities?\u003c/p\u003e\n\u003cp\u003eWe recently talked about a range of AI topics, including AGI, with Google\u0026#39;s Tulsee Doshi, director of product management for Gemini. \u0026#34;Different people have different definitions of AGI, and so depending on who you talk to, how close or far we are from AGI is a different conversation,\u0026#34; said Doshi. \u0026#34;What I would say is LLMs, Gemini, and the training of smarter and smarter models is on the path to models that are going to be at extremely high intelligence. And that has a ton of value in and of itself.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThis paper is not the final word on AGI safety—\u003ca href=\"https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/\"\u003eDeepMind notes\u003c/a\u003e this is just a \u0026#34;starting point for vital conversations.\u0026#34; If the team is right, and AGI will transform the world in five short years, those conversations need to happen soon. If not, well, a lot of people are going to look kind of silly.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/ryanwhitwam/\"\u003e\u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/02/AV4.jpg\" alt=\"Photo of Ryan Whitwam\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Ryan Whitwam is a senior technology reporter at Ars Technica, covering the ways Google, AI, and mobile technology continue to change the world. Over his 20-year career, he\u0026#39;s written for Android Police, ExtremeTech, Wirecutter, NY Times, and more. He has reviewed more phones than most people will ever own. You can \u003ca href=\"https://bsky.app/profile/rwhitwam.bsky.social\"\u003efollow him on Bluesky\u003c/a\u003e, where you will see photos of his dozens of mechanical keyboards.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/ai/2025/04/google-deepmind-releases-its-plan-to-keep-agi-from-running-wild/#comments\" title=\"35 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    35 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \n\n\n  \n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/space/2025/04/the-harrowing-story-of-what-flying-starliner-was-like-when-its-thrusters-failed/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/54401722655_0e5ce69fdb_k-768x432.jpg\" alt=\"Listing image for first story in Most Read: Starliner’s flight to the space station was far wilder than most of us thought\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/main\u003e\n\n\n\n\n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-04-03T21:43:32Z",
  "modifiedTime": "2025-04-04T00:50:33Z"
}
