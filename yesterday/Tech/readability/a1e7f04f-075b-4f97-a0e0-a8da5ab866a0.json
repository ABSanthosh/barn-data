{
  "id": "a1e7f04f-075b-4f97-a0e0-a8da5ab866a0",
  "title": "Tokasaurus: An LLM Inference Engine for High-Throughput Workloads",
  "link": "https://scalingintelligence.stanford.edu/blogs/tokasaurus/",
  "description": "Comments",
  "author": "",
  "published": "Thu, 05 Jun 2025 21:27:07 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 9038,
  "excerpt": "TL;DR",
  "siteName": "Scaling Intelligence Lab at Stanford University",
  "favicon": "",
  "text": "TL;DR We’re releasing Tokasaurus, a new LLM inference engine optimized for throughput-intensive workloads. With small models, Tokasaurus benefits from very low CPU overhead and dynamic Hydragen grouping to exploit shared prefixes. For larger models, Tokasaurus supports async tensor parallelism for GPUs with NVLink and a fast implementation of pipeline parallelism for GPUs without. On throughput-focused benchmarks, Tokasaurus can outperform vLLM and SGLang by up to 3x+. Table of Contents Intro Optimizing Small Models Optimizing Big Models Try it Out Benchmarking Details Acknowledgements Intro As LLMs get smarter, faster, and cheaper, the community keeps finding new ways to use them. Our own recent work has explored using models to scan every file in a codebase, sample 10,000 attempts for math and code problems, and collaborate with other models to minimize cloud costs. Inference is now also an important part of the training process, where we use models to generate synthetic data or as part of RL pipelines that generate and train on model completions. Crucially, these new inference workloads look quite different than the original LLM use case of serving a chatbot. Here, we care primarily about the total time and cost required to complete a large batch of sequences, and we care much less (if at all) about the individual latency of a single generation. In other words, we want high throughput! Open-source inference engines (i.e. dedicated systems for running efficient LLM inference) like FlexGen, vLLM, and SGLang have been enormously valuable to the community. Inspired by and learning from these projects, we built a new engine, Tokasaurus, designed from the ground up to handle throughput-focused workloads. We’ve optimized Tokasaurus for efficiently serving large and small models alike, allowing it to outperform existing engines on throughput benchmarks. In the rest of this blog, we’ll walk through some of these optimizations and show off a few settings where Tokasaurus really shines. Optimizing Small Models To benchmark Tokasaurus with small models, we’ll use two workloads: Completing chatbot prompts from the ShareGPT dataset (this is a common benchmark for testing inference engines). Reproducing an experiment from Large Language Monkeys, where we take 128 problems from the GSM8K math dataset and sample 1024 answers to every problem. The distinguishing feature of this workload is that there’s a lot of prefix sharing across sequences. Tokasaurus outperforms vLLM and SGLang on both of these benchmarks, in particular achieving over 2x the throughput of other engines on the Large Language Monkeys workload. Two main features contribute to these wins with small models: Minimizing CPU Overhead LLM engines perform many different tasks on the CPU, like handling web requests, tokenizing inputs/detokenizing outputs, managing KV cache allocation, and preparing inputs for the model. If these CPU-side tasks cause the GPU-side model to stall, throughput can take a big hit. To avoid these stalls, inference engines commonly make many CPU-side tasks asynchronous: while the GPU runs a forward pass for batch N, the CPU-side of the engine post-processes the results from batch N-1 and prepares the inputs for batch N+1. Tokasaurus goes one step further, making the CPU-side of the engine (what we call the manager) both asynchronous and adaptive. The manager’s goal is to maintain a deep queue of inputs for the model to run forward passes on. The manager monitors the size of this queue and can detect if the model is close to exhausting it (and therefore stalling the GPU). In these cases, the manager will automatically start skipping optional steps (like checking for stop strings and onboarding new sequences) until the model’s input queue is sufficiently deep again. This combination of asynchrony and adaptivity lets Tokasaurus serve small models with much less CPU overhead. Dynamic Prefix Identification and Exploration Prefix sharing comes up all the time in LLM inference — not just when repeatedly sampling like in the Large Language Monkeys benchmark, but also when asking many questions about a long document or reusing a system prompt across many chatbot conversations. Shared prefixes allow attention to be computed more efficiently. We first explored this idea last year with Hydragen (aka cascade attention and bifurcated attention), but at the time we didn’t address the problem of detecting these shared prefixes in an engine where sequences are constantly starting and finishing. With Tokasaurus, we solve this detection problem by running a greedy depth-first search algorithm before every model forward pass that iteratively finds the longest shared prefixes possible. Hydragen is most impactful for small models, which spend a relatively larger fraction of total FLOPs on attention. Optimizing Bigger Models Tokasaurus can also efficiently serve bigger models across multiple GPUs! Here, the most important optimizations are our implementations of pipeline parallelism (PP) and tensor parallelism (TP), which allow us to maximize throughput on GPUs with or without NVLink. Pipeline Parallelism for the GPU Poor One of our original goals with Tokasaurus was to efficiently run multi-GPU inference on our lab’s L40S GPUs, which don’t have fast inter-GPU NVLink connections. Without NVLink, the communication costs incurred running TP across a node of eight GPUs are substantial. Therefore, efficient support for PP (which requires much less inter-GPU communication) was a high priority. PP needs a large batch in order to run efficiently, since batches from the manager are subdivided into microbatches that are spread out across pipeline stages. When optimizing for throughput, we’re generally already using the largest batch size that fits in GPU memory, so PP is often a natural fit for throughput-focused workloads. When benchmarking against vLLM’s and SGLang’s pipeline parallel implementations using Llama-3.1-70B on eight L40S GPUs, Tokasaurus improves throughput by over 3x: Async Tensor Parallel for the GPU Rich If you do have GPUs with NVLink (e.g. B200s and certain models of H100s and A100s), Tokasaurus has something for you too! Models in Tokasaurus can be torch compiled end-to-end, allowing us to take advantage of Async Tensor Parallelism (Async-TP). This is a relatively new feature in PyTorch that can overlap inter-GPU communication with other computations, partially hiding the cost of communication. In our benchmarks, we found that Async-TP adds a lot of CPU overhead to the model forward pass and only starts improving throughput with very large batch sizes (e.g. 6k+ tokens). Tokasaurus maintains torch-compiled versions of our models with and without Async-TP enabled, allowing us to automatically switch on Async-TP whenever the batch size is big enough: Try it Out Tokasaurus started as an internal lab effort to run our inference experiments faster, and we’re excited to share it more broadly! You can check out the Tokasaurus code on GitHub and install the package from PyPI with: Currently, we support models from the Llama-3 and Qwen-2 families and support any combination of data, tensor, and pipeline parallel within a single node. Tokasaurus is written in pure Python (although we do use attention and sampling ops from the excellent FlashInfer package). We hope that this makes the engine easier to fork and hack on, à la GPT-fast. Benchmarking Details The commands for reproducing our benchmarks are available here. For each benchmark, we configure all engines with the same KV cache size and maximum number of running requests. We’ve made a best effort to tune each engine’s remaining parameters. We report the average throughput across runs after completing a warmup run. For each benchmark, all engines are run on the same machine. We use this script from SGLang for our ShareGPT benchmarks and this custom script for the Large Language Monkeys benchmark. To standardize our benchmarking scripts and interface, all experiments send requests through the OpenAI API. We also experimented with vLLM’s Python API (i.e. LLM.generate()) on the Large Language Monkeys benchmark with Llama-1B and measured roughly a 5% throughput increase (thanks to the vLLM team for the tip!). Acknowledgements Huge thanks to Prime Intellect and Together AI for providing us with compute for this project. Also, we’re grateful to Dan Biderman, Simon Guo, Manat Kaur, and Avanika Narayan for beta testing the engine! If you find Tokasaurus useful, please use the following citation: @misc{juravsky2025tokasaurus, author = {Jordan Juravsky and Ayush Chakravarthy and Ryan Ehrlich and Sabri Eyuboglu and Bradley Brown and Joseph Shetaye and Christopher R{\\'e} and Azalia Mirhoseini}, title = {Tokasaurus: An LLM Inference Engine for High-Throughput Workloads}, year = {2025}, howpublished = {\\url{https://scalingintelligence.stanford.edu/blogs/tokasaurus/}} }",
  "image": "https://scalingintelligence.stanford.edu/imgs/thumbs/tokasaurus.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"center\"\u003e\n          \n          \n\n          \n\n          \u003chr/\u003e\n\n          \n            \n              \u003cp\u003e\u003cimg src=\"https://scalingintelligence.stanford.edu/imgs/teasers/tokasaurus.png\" alt=\"\"/\u003e\n                \n\n              \u003c/p\u003e\n            \n\n            \u003ch2 id=\"tldr\"\u003eTL;DR\u003c/h2\u003e\n\n\u003cp\u003eWe’re releasing Tokasaurus, a new LLM inference engine optimized for throughput-intensive workloads. With small models, Tokasaurus benefits from very low CPU overhead and dynamic \u003ca href=\"https://arxiv.org/abs/2402.05099\"\u003eHydragen\u003c/a\u003e grouping to exploit shared prefixes. For larger models, Tokasaurus supports async tensor parallelism for GPUs with NVLink and a fast implementation of pipeline parallelism for GPUs without. On throughput-focused benchmarks, Tokasaurus can outperform vLLM and SGLang by up to 3x+.\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"#intro\"\u003eIntro\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#optimizing-small-models\"\u003eOptimizing Small Models\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#optimizing-bigger-models\"\u003eOptimizing Big Models\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#try-it-out\"\u003eTry it Out\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#benchmarking-details\"\u003eBenchmarking Details\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#acknowledgements\"\u003eAcknowledgements\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\n\u003cp\u003eAs LLMs get \u003ca href=\"https://www.youtube.com/watch?v=gAjR4_CbPpQ\"\u003esmarter, faster, and cheaper,\u003c/a\u003e the community keeps finding new ways to use them. Our own recent work has explored using models to \u003ca href=\"https://arxiv.org/abs/2501.14723\"\u003escan every file in a codebase\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2407.21787\"\u003esample 10,000 attempts for math and code problems\u003c/a\u003e, and \u003ca href=\"https://arxiv.org/abs/2502.15964\"\u003ecollaborate with other models to minimize cloud costs\u003c/a\u003e. Inference is now also an important part of the training process, where we use models to \u003ca href=\"https://arxiv.org/abs/2404.14219\"\u003egenerate synthetic data\u003c/a\u003e or as part of \u003ca href=\"https://arxiv.org/abs/2504.04736\"\u003eRL pipelines\u003c/a\u003e that generate and train on model completions.\u003c/p\u003e\n\n\u003cp\u003eCrucially, these new inference workloads look quite different than the original LLM use case of serving a chatbot. Here, we care primarily about the total time and cost required to complete a large batch of sequences, and we care much less (if at all) about the individual latency of a single generation. In other words, we want high throughput!\u003c/p\u003e\n\n\u003cp\u003eOpen-source inference engines (i.e. dedicated systems for running efficient LLM inference) like \u003ca href=\"https://arxiv.org/abs/2303.06865\"\u003eFlexGen\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2309.06180\"\u003evLLM\u003c/a\u003e, and \u003ca href=\"https://arxiv.org/abs/2312.07104\"\u003eSGLang\u003c/a\u003e have been enormously valuable to the community. Inspired by and learning from these projects, we built a new engine, \u003ca href=\"https://github.com/ScalingIntelligence/tokasaurus/tree/main\"\u003eTokasaurus\u003c/a\u003e, designed from the ground up to handle throughput-focused workloads. We’ve optimized Tokasaurus for efficiently serving large and small models alike, allowing it to outperform existing engines on throughput benchmarks. In the rest of this blog, we’ll walk through some of these optimizations and show off a few settings where Tokasaurus really shines.\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003ch2 id=\"optimizing-small-models\"\u003eOptimizing Small Models\u003c/h2\u003e\n\n\u003cp\u003eTo benchmark Tokasaurus with small models, we’ll use two workloads:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eCompleting chatbot prompts from the ShareGPT dataset (this is a common benchmark for testing inference engines).\u003c/li\u003e\n  \u003cli\u003eReproducing an experiment from \u003ca href=\"https://arxiv.org/abs/2407.21787\"\u003eLarge Language Monkeys\u003c/a\u003e, where we take 128 problems from the \u003ca href=\"https://arxiv.org/abs/2110.14168\"\u003eGSM8K\u003c/a\u003e math dataset and sample 1024 answers to every problem. The distinguishing feature of this workload is that there’s a lot of prefix sharing across sequences.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cimg src=\"https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/small.png\" alt=\"Tokasaurus small models\"/\u003e\n  \u003cimg src=\"https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/monkeys.png\" alt=\"Tokasaurus large batch sampling\"/\u003e\n\u003c/p\u003e\n\n\u003cp\u003eTokasaurus outperforms vLLM and SGLang on both of these benchmarks, in particular achieving over 2x the throughput of other engines on the Large Language Monkeys workload. Two main features contribute to these wins with small models:\u003c/p\u003e\n\n\u003ch3 id=\"minimizing-cpu-overhead\"\u003eMinimizing CPU Overhead\u003c/h3\u003e\n\n\u003cp\u003eLLM engines perform many different tasks on the CPU, like handling web requests, tokenizing inputs/detokenizing outputs, managing KV cache allocation, and preparing inputs for the model. If these CPU-side tasks cause the GPU-side model to stall, throughput can take a \u003ca href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\"\u003ebig hit\u003c/a\u003e. To avoid these stalls, inference engines commonly make many CPU-side \u003ca href=\"https://blog.vllm.ai/2024/09/05/perf-update.html\"\u003etasks\u003c/a\u003e \u003ca href=\"https://lmsys.org/blog/2024-12-04-sglang-v0-4/\"\u003easynchronous\u003c/a\u003e: while the GPU runs a forward pass for batch N, the CPU-side of the engine post-processes the results from batch N-1 and prepares the inputs for batch N+1.\u003c/p\u003e\n\n\u003cp\u003eTokasaurus goes one step further, making the CPU-side of the engine (what we call the manager) both asynchronous and adaptive. The manager’s goal is to maintain a deep queue of inputs for the model to run forward passes on. The manager monitors the size of this queue and can detect if the model is close to exhausting it (and therefore stalling the GPU). In these cases, the manager will automatically start skipping optional steps (like checking for stop strings and onboarding new sequences) until the model’s input queue is sufficiently deep again. This combination of asynchrony and adaptivity lets Tokasaurus serve small models with much less CPU overhead.\u003c/p\u003e\n\n\u003ch3 id=\"dynamic-prefix-identification-and-exploration\"\u003eDynamic Prefix Identification and Exploration\u003c/h3\u003e\n\n\u003cp\u003ePrefix sharing comes up all the time in LLM inference — not just when repeatedly sampling like in the Large Language Monkeys benchmark, but also when asking many questions about a long document or reusing a system prompt across many chatbot conversations.\u003c/p\u003e\n\n\u003cp\u003eShared prefixes allow attention to be computed more efficiently. We first explored this idea last year with \u003ca href=\"https://arxiv.org/abs/2402.05099\"\u003eHydragen\u003c/a\u003e (aka \u003ca href=\"https://flashinfer.ai/2024/02/02/cascade-inference.html\"\u003ecascade attention\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/2403.08845\"\u003ebifurcated attention\u003c/a\u003e), but at the time we didn’t address the problem of detecting these shared prefixes in an engine where sequences are constantly starting and finishing. With Tokasaurus, we solve this detection problem by running a greedy depth-first search algorithm before every model forward pass that iteratively finds the longest shared prefixes possible. Hydragen is most impactful for small models, which spend a relatively larger fraction of total FLOPs on attention.\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003ch2 id=\"optimizing-bigger-models\"\u003eOptimizing Bigger Models\u003c/h2\u003e\n\n\u003cp\u003eTokasaurus can also efficiently serve bigger models across multiple GPUs! Here, the most important optimizations are our implementations of pipeline parallelism (PP) and tensor parallelism (TP), which allow us to maximize throughput on GPUs with or without NVLink.\u003c/p\u003e\n\n\u003ch3 id=\"pipeline-parallelism-for-the-gpu-poor\"\u003ePipeline Parallelism for the GPU Poor\u003c/h3\u003e\n\n\u003cp\u003eOne of our original goals with Tokasaurus was to efficiently run multi-GPU inference on our lab’s L40S GPUs, which don’t have fast inter-GPU NVLink connections. Without NVLink, the communication costs incurred running TP across a node of eight GPUs are substantial. Therefore, efficient support for PP (which requires much less inter-GPU communication) was a high priority. PP needs a large batch in order to run efficiently, since batches from the manager are subdivided into microbatches that are spread out across pipeline stages. When optimizing for throughput, we’re generally already using the largest batch size that fits in GPU memory, so PP is often a natural fit for throughput-focused workloads. When benchmarking against vLLM’s and SGLang’s pipeline parallel implementations using Llama-3.1-70B on eight L40S GPUs, Tokasaurus improves throughput by over 3x:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/pipeline.png\" alt=\"Tokasaurus small models\"/\u003e\n\u003c/p\u003e\n\n\u003ch3 id=\"async-tensor-parallel-for-the-gpu-rich\"\u003eAsync Tensor Parallel for the GPU Rich\u003c/h3\u003e\n\n\u003cp\u003eIf you do have GPUs with NVLink (e.g. B200s and certain models of H100s and A100s), Tokasaurus has something for you too! Models in Tokasaurus can be torch compiled end-to-end, allowing us to take advantage of \u003ca href=\"https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487\"\u003eAsync Tensor Parallelism (Async-TP)\u003c/a\u003e. This is a relatively new feature in PyTorch that can overlap inter-GPU communication with other computations, partially hiding the cost of communication. In our benchmarks, we found that Async-TP adds a lot of CPU overhead to the model forward pass and only starts improving throughput with very large batch sizes (e.g. 6k+ tokens). Tokasaurus maintains torch-compiled versions of our models with and without Async-TP enabled, allowing us to automatically switch on Async-TP whenever the batch size is big enough:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/big.png\" alt=\"Tokasaurus small models\"/\u003e\n\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003ch2 id=\"try-it-out\"\u003eTry it Out\u003c/h2\u003e\n\n\u003cp\u003eTokasaurus started as an internal lab effort to run our inference experiments faster, and we’re excited to share it more broadly! You can check out the Tokasaurus code on \u003ca href=\"https://github.com/ScalingIntelligence/tokasaurus/tree/main\"\u003eGitHub\u003c/a\u003e and install the package from PyPI with:\u003c/p\u003e\n\n\n\n\u003cp\u003eCurrently, we support models from the Llama-3 and Qwen-2 families and support any combination of data, tensor, and pipeline parallel within a single node.\u003c/p\u003e\n\n\u003cp\u003eTokasaurus is written in pure Python (although we do use attention and sampling ops from the excellent \u003ca href=\"https://arxiv.org/abs/2501.01005\"\u003eFlashInfer\u003c/a\u003e package). We hope that this makes the engine easier to fork and hack on, à la \u003ca href=\"https://github.com/pytorch-labs/gpt-fast\"\u003eGPT-fast\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"benchmarking-details\"\u003eBenchmarking Details\u003c/h2\u003e\n\n\u003cp\u003eThe commands for reproducing our benchmarks are available \u003ca href=\"https://github.com/ScalingIntelligence/tokasaurus/blob/main/logs/blog_commands.md\"\u003ehere\u003c/a\u003e. For each benchmark, we configure all engines with the same KV cache size and maximum number of running requests. We’ve made a best effort to tune each engine’s remaining parameters. We report the average throughput across runs after completing a warmup run. For each benchmark, all engines are run on the same machine.\u003c/p\u003e\n\n\u003cp\u003eWe use \u003ca href=\"https://github.com/sgl-project/sglang/blob/7e257cd666c0d639626487987ea8e590da1e9395/python/sglang/bench_serving.py\"\u003ethis script\u003c/a\u003e from SGLang for our ShareGPT benchmarks and \u003ca href=\"https://github.com/ScalingIntelligence/tokasaurus/blob/a0155181f09c0cf40783e01a625b041985667a92/tokasaurus/benchmarks/monkeys_gsm8k.py\"\u003ethis custom script\u003c/a\u003e for the Large Language Monkeys benchmark. To standardize our benchmarking scripts and interface, all experiments send requests through the OpenAI API. We also experimented with vLLM’s Python API (i.e. \u003ccode\u003eLLM.generate()\u003c/code\u003e) on the Large Language Monkeys benchmark with Llama-1B and measured roughly a 5% throughput increase (thanks to the vLLM team for the tip!).\u003c/p\u003e\n\n\u003ch2 id=\"acknowledgements\"\u003eAcknowledgements\u003c/h2\u003e\n\n\u003cp\u003eHuge thanks to \u003ca href=\"https://www.primeintellect.ai/\"\u003ePrime Intellect\u003c/a\u003e and \u003ca href=\"https://www.together.ai/\"\u003eTogether AI\u003c/a\u003e for providing us with compute for this project.\u003c/p\u003e\n\n\u003cp\u003eAlso, we’re grateful to Dan Biderman, Simon Guo, Manat Kaur, and Avanika Narayan for beta testing the engine!\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003eIf you find Tokasaurus useful, please use the following citation:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan\u003e@misc\u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003ejuravsky2025tokasaurus\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n  \u003cspan\u003eauthor\u003c/span\u003e       \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{Jordan Juravsky and Ayush Chakravarthy and Ryan Ehrlich and Sabri Eyuboglu and Bradley Brown and Joseph Shetaye and Christopher R{\\\u0026#39;e} and Azalia Mirhoseini}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n  \u003cspan\u003etitle\u003c/span\u003e        \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{Tokasaurus: An LLM Inference Engine for High-Throughput Workloads}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n  \u003cspan\u003eyear\u003c/span\u003e         \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{2025}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n  \u003cspan\u003ehowpublished\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{\\url{https://scalingintelligence.stanford.edu/blogs/tokasaurus/}}\u003c/span\u003e\n\u003cspan\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\n\n          \n        \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-06-05T00:00:00Z",
  "modifiedTime": null
}
