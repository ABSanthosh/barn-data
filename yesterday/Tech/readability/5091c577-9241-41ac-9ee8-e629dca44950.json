{
  "id": "5091c577-9241-41ac-9ee8-e629dca44950",
  "title": "100 things we announced at I/O",
  "link": "https://blog.google/technology/ai/google-io-2025-all-our-announcements/",
  "description": "Learn more about the biggest announcements and launches from Google’s 2025 I/O developer conference.",
  "author": "Molly McHugh-JohnsonContributorThe Keyword",
  "published": "Wed, 21 May 2025 23:00:00 +0000",
  "source": "https://www.blog.google/rss/",
  "categories": [
    "Search",
    "Gemini",
    "AI"
  ],
  "byline": "Molly McHugh-Johnson",
  "length": 22085,
  "excerpt": "Learn more about the biggest announcements and launches from Google’s 2025 I/O developer conference.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png?version=pr20250514-1654",
  "text": "May 21, 2025 [[read-time]] min read That’s a wrap on I/O 2025! Here’s what we announced, launched and demoed. Yesterday at Google I/O, we shared how we’re taking the progress we’re making in AI and applying it across our products. Major upgrades are coming to our Gemini app, our generative AI tools and everything in between — including some truly incredible progress we’re making with our AI models (and new ways you can access them).Here’s a list of I/O 2025’s highlights — many of which you can try today! Ask anything with AI in Search Try it now! AI Mode is starting to roll out for everyone in the U.S. right on Search. But if you want to get access right away, opt in via Labs.For questions where you want an even more thorough response, we’re bringing deep research capabilities into AI Mode in Labs, with Deep Search.Live capabilities from Project Astra are coming to AI Mode in Labs. With Search Live, coming this summer, you can talk back-and-forth with Search about what you see in real-time, using your camera.We’re also bringing agentic capabilities from Project Mariner to AI Mode in Labs, starting with event tickets, restaurant reservations and local appointments.Coming soon: When you need some extra help crunching numbers or visualizing data, AI Mode in Labs will analyze complex datasets and create graphics that bring them to life, all custom built for your query. We’ll bring this to sports and finance queries.We’re introducing a new AI Mode shopping experience that brings together advanced AI capabilities with our Shopping Graph to help you browse for inspiration, think through considerations and find the right product for you.Try it now! You can virtually try on billions of apparel listings just by uploading a photo of yourself. Our “try on” experiment is rolling out to Search Labs users in the U.S. starting today — opt in to try it out now.We also showed off a new agentic checkout to help you buy at a price that fits your budget with ease. Just tap “track price” on any product listing, set what you want to spend and we’ll let you know if the price drops.We shared some updates on AI Overviews: Since last year’s I/O, AI Overviews have scaled up to 1.5 billion monthly users in 200 countries and territories. That means Google Search is bringing generative AI to more people than any other product in the world.In our biggest markets like the U.S. and India, AI Overviews is driving over 10% increase in usage of Google for the types of queries that show AI Overviews.And starting this week, Gemini 2.5 is coming to Search for both AI Mode and AI Overviews in the U.S.Try new, helpful features for Gemini 12. Try it now! Now Gemini is an even better study partner with our new interactive quiz feature. Simply ask Gemini to “create a practice quiz on…” and Gemini will generate questions.13. In the coming weeks we’ll also make Gemini Live more personal by connecting some of your favorite Google apps so you can take actions mid-conversation, like adding something to your calendar or asking for more details about a location. We’re starting with Google Maps, Calendar, Tasks and Keep, with more app connections coming later.14. Try it now! Starting today, camera and screen sharing capabilities for Gemini Live are beginning to roll out beyond Android to Gemini app users on iOS.15. Try it now! Starting today, we’re introducing a new Create menu within Canvas that helps you explore the breadth of what Canvas can build for you, allowing you to transform text into interactive infographics, web pages, immersive quizzes and even podcast-style Audio Overviews in 45 languages.16. Try it now! Starting today, you can upload PDFs and images directly into Deep Research so your research reports draw from a combination of public information and details that you provide.17. Soon, you’ll be able to link your documents from Drive or from Gmail and customize the sources Deep Research pulls from, like academic literature.18. We announced Agent Mode, an experimental feature where you will be able to simply describe your end goal and Gemini can get things done on your behalf. An experimental version of Agent Mode in the Gemini app will be coming soon to Google AI Ultra subscribers.19. Try it now! Gemini in Chrome will begin rolling out on desktop to Google AI Pro and Google AI Ultra subscribers in the U.S. who use English as their Chrome language on Windows and macOS.20. The Gemini app now has over 400 million monthly active users.Learn more about advancements for Gemini models 21. With our latest update, Gemini 2.5 Pro is now the world-leading model across the WebDev Arena and LMArena leaderboards.22. We’re infusing LearnLM directly into Gemini 2.5, which is now the world’s leading model for learning. As detailed in our latest report, Gemini 2.5 Pro outperformed competitors on every category of learning science principles.23. We introduced a new preview version of our leading model, Gemini 2.5 Flash, with stronger performance on coding and complex reasoning tasks that is optimized for speed and efficiency.24. 2.5 Flash is now available to everyone in the Gemini app, and we'll make our updated version generally available in Google AI Studio for developers and in Vertex AI for enterprises in early June, with 2.5 Pro soon after.25. 2.5 Pro will get even better with Deep Think, an experimental, enhanced reasoning mode for highly-complex math and coding.26. We’re bringing new capabilities to both 2.5 Pro and 2.5 Flash, including advanced security safeguards. Our new security approach helped significantly increase Gemini’s protection rate against indirect prompt injection attacks during tool use, making Gemini 2.5 our most secure model family to date.27. We're bringing Project Mariner's computer use capabilities into the Gemini API and Vertex AI. Companies like Automation Anywhere, UiPath, Browserbase, Autotab, The Interaction Company and Cartwheel are exploring its potential, and we're excited to roll it out more broadly for developers to experiment with this summer.28. Both 2.5 Pro and Flash will now include thought summaries in the Gemini API and in Vertex AI. Thought summaries take the model’s raw thoughts and organize them into a clear format with headers, key details and information about model actions, like when they use tools.29. We launched 2.5 Flash with thinking budgets to give developers more control over cost by balancing latency and quality, and we’re extending this capability to 2.5 Pro. This allows you to control the number of tokens a model uses to think before it responds, or even turn its thinking capabilities off. Gemini 2.5 Pro with budgets will be generally available for stable production use in the coming weeks, along with our generally available model.30. We added native SDK support for Model Context Protocol (MCP) definitions in the Gemini API for easier integration with open-source tools. We’re also exploring ways to deploy MCP servers and other hosted tools, making it easier for you to build agentic applications.31. We introduced a new research model, called Gemini Diffusion. This text diffusion model learns to generate outputs by converting random noise into coherent text or code, like how our current models in image and video generation work. We’ll continue our work on different approaches to lowering latency in all our Gemini models, with a faster 2.5 Flash Lite coming soon.Access our AI tools with new options 32. We introduced Google AI Ultra, a new AI subscription plan with the highest usage limits and access to our most capable models and premium features, plus 30 TB of storage and access to YouTube Premium.33. Google AI Ultra is available in the U.S. now, with more countries coming soon. It’s $249.99 a month, with a special offer for first-time users of 50% off for your first three months.34. College students in the U.S., Brazil, Indonesia, Japan and the U.K. are also eligible to get a free upgrade of Gemini for a whole school year — more countries are coming soon.35. There’s also Google Al Pro, which gives you a suite of Al tools for $19.99/month. This Pro plan will level up your Gemini app experience. It also includes products like Flow, NotebookLM and more, all with special features and higher rate limits.Explore your creativity with new generative AI 36. Try it now! We announced Veo 3, which lets you generate video with audio and is now available in the Gemini app for Google AI Ultra subscribers in the U.S., as well as in Vertex AI.37. We also added new capabilities to our popular Veo 2 model, including new camera controls, outpainting and object add and remove.38. We showed you four new films created with Veo alongside other tools and techniques. View these films from our partners and other inspirational content on Flow TV.39. Try it now! Imagen 4 is our latest Imagen model, and it has remarkable clarity in fine details like skin, fur and intricate textures, and excels in both photorealistic and abstract styles. Imagen 4 is available today in the Gemini app.40. Imagen 4 is also available in Whisk, and to enterprises in Vertex AI.41. Soon, Imagen 4 will be available in a Fast version that’s up to 10x faster than Imagen 3.42. Imagen 4 can create images in a range of aspect ratios and up to 2K resolution so you can get even higher-quality for printing and presentations.43. It is also significantly better at spelling and typography, making it easier to create your own greeting cards, posters and even comics.44. Try it now! Flow is our new AI filmmaking tool. Using Google DeepMind’s best-in-class models, Flow lets you weave cinematic films with control of characters, scenes and styles, so more people than ever can create visually striking movies with AI.45. Flow is available today for Google AI Pro and Ultra plan subscribers in the United States.46. In April, we expanded access to Music AI Sandbox, powered by Lyria 2. Lyria 2 brings powerful composition and endless exploration, and is now available for creators through YouTube Shorts and enterprises in Vertex AI.47. Lyria 2 can arrange rich vocals that sound like a solo singer or a full choir.48. Lyria RealTime is an interactive music generation model that allows anyone to interactively create, control, and perform music in real time. This model is now available via the Gemini API in Google AI Studio and Vertex AI.49. We announced a partnership between Google DeepMind and Primordial Soup, a new venture dedicated to storytelling innovation founded by pioneering director Darren Aronofsky. Primordial Soup is producing three short films using Google DeepMind’s generative AI models, tools and capabilities, including Veo.50. The first film, “ANCESTRA,” is directed by award-winning filmmaker Eliza McNitt and will premiere at the Tribeca Festival on June 13, 2025.51. To make it easier for people and organizations to detect AI-generated content, we announced SynthID Detector, a verification portal that helps to quickly and efficiently identify content that is watermarked with SynthID.52. And since launch, SynthID has already watermarked over 10 billion pieces of content.53. We are starting to roll out the SynthID Detector portal to a group of early testers. Journalists, media professionals and researchers can join our waitlist to gain access to the SynthID Detector.Take a look at the future of AI assistance 54. We’re working to extend our best multimodal foundation model, Gemini 2.5 Pro, to become a “world model” that can make plans and imagine new experiences by understanding and simulating aspects of the world, just as the brain does.55. Updates to Project Astra, our research prototype that explores the capabilities of a universal AI assistant, include more natural voice output with native audio, improved memory and computer control. Over time we’ll bring these new capabilities to Gemini Live and new experiences in Search, Live API for devs and new form factors like Android XR glasses.56. And as part of our Project Astra research, we partnered with the visual interpreting service Aira to build a prototype that assists members of the blind and low-vision community with everyday tasks, complementing the skills and tools they already use.57. With Project Astra, we’re prototyping a conversational tutor that can help with homework. Not only can it follow along with what you’re working on, but it can also walk you through problems step-by-step, identify mistakes and even generate diagrams to help explain concepts if you get stuck.58. This research experience will be coming to Google products later this year and Android Trusted Testers can sign up for the waitlist for a preview.59. We took a look at the first Android XR device coming later this year: Samsung’s Project Moohan. This headset will offer immersive experiences on an infinite screen.60. And we shared a sneak peek at how Gemini will work on glasses with Android XR in real-world scenarios, including messaging friends, making appointments, asking for turn-by-turn directions, taking photos and more.61. We even demoed live language translation between two people, showing the potential for these glasses to break down language barriers.62. Android XR prototype glasses are now in the hands of trusted testers, who are helping us make sure we’re building a truly assistive product and doing so in a way that respects privacy for you and those around you.63. Plus we’re partnering with innovative eyewear brands, starting with Gentle Monster and Warby Parker, to create glasses with Android XR that you’ll want to wear all day.64. We’re advancing our partnership with Samsung to go beyond headsets and extend Android XR to glasses. Together we’re creating a software and reference hardware platform that will enable the ecosystem to make great glasses. Developers will be able to start building for this platform later this year.Communicate better, in near real time 65. A few years ago, we introduced Project Starline, a research project that enabled remote conversations that used 3D video technology to make it feel like two people were in the same room. Now, it’s evolving into a new platform called Google Beam.66. We’re working with Zoom and HP to bring the first Google Beam devices to market with select customers later this year. We’re also partnering with industry leaders like Zoom, Diversified and AVI-SPL to bring Google Beam to businesses and organizations worldwide.67. You’ll even see the first Google Beam products from HP at InfoComm in a few weeks.68. We announced speech translation, which is available now in Google Meet. This translation feature not only happens in near real-time, thanks to Google AI, but it’s able to maintain the quality, tone, and expressiveness of someone’s voice. The free-flowing conversation enables people to understand each other and feel connected, with no language barrier.Build better with developer launches 69. Over 7 million developers are building with Gemini, five times more than this time last year.70. Gemini usage on Vertex AI is up 40 times compared to this time last year.71. We’re releasing new previews for text-to-speech in 2.5 Pro and 2.5 Flash. These have first-of-its-kind support for multiple speakers, enabling text-to-speech with two voices via native audio out. Like Native Audio dialogue, text-to-speech is expressive, and can capture really subtle nuances, such as whispers. It works in over 24 languages and seamlessly switches between them.72. The Live API is introducing a preview version of audio-visual input and native audio out dialogue, so you can directly build conversational experiences.73. Try it now! Jules is a parallel, asynchronous agent for your GitHub repositories to help you improve and understand your codebase. It is now open to all developers in beta. With Jules you can delegate multiple backlog items and coding tasks at the same time, and even get an audio overview of all the recent updates to your codebase.74. Gemma 3n is our latest fast and efficient open multimodal model that’s engineered to run smoothly on your phones, laptops, and tablets. It handles audio, text, image, and video. The initial rollout is underway on Google AI Studio and Google Cloud with plans to expand to open-source tools in the coming weeks.75. Try it now! Google AI Studio now has a cleaner UI, integrated documentation, usage dashboards, new apps, and a new Generate Media tab to explore and experiment with our cutting-edge generative models, including Imagen, Veo and native image generation.76. Colab will soon be a new, fully agentic experience. Simply tell Colab what you want to achieve, and watch as it takes action in your notebook, fixing errors and transforming code to help you solve hard problems faster.77. SignGemma is an upcoming open model that translates sign language into spoken language text, (best at American Sign Language to English), enabling developers to create new apps and integrations for Deaf and Hard of Hearing users.78. MedGemma is our most capable open model for multimodal medical text and image comprehension designed for developers to adapt and build their health applications, like analyzing medical images. MedGemma is available now for use now as part of Health AI Developer Foundations.79. Stitch is a new AI-powered tool to generate high-quality UI designs and corresponding frontend code for desktop and mobile by using natural language descriptions or image prompts.80. Try it now! We announced Journeys in Android Studio, which lets developers test critical user journeys using Gemini by describing test steps in natural language.81. Version Upgrade Agent in Android Studio is coming soon to automatically update dependencies to the latest compatible version, parsing through release notes, building the project and fixing any errors.82. We introduced new updates across the Google Pay API designed to help developers create smoother, safer, and more successful checkout experiences, including Google Pay in Android WebViews.83. Flutter 3.32 has new features designed to accelerate development and enhance apps.84. And we shared updates for our Agent Development Kit (ADK), the Vertex AI Agent Engine, and our Agent2Agent (A2A) protocol, which enables interactions between multiple agents.85. Try it now! Developer Preview for Wear OS 6 introduces Material 3 Expressive and updated developer tools for Watch Faces, richer media controls and the Credential Manager for authentication.86. Try it now! We announced that Gemini Code Assist for individuals and Gemini Code Assist for GitHub are generally available, and developers can get started in less than a minute. Gemini 2.5 now powers both the free and paid versions of Gemini Code Assist, features advanced coding performance; and helps developers excel at tasks like creating visually compelling web apps, along with code transformation and editing.87. Here’s an example of a recent update you can explore in Gemini Code Assist: Quickly resume where you left off and jump into new directions with chat history and threads.88. Firebase announced new features and tools to help developers build AI-powered apps more easily, including updates to the recently launched Firebase Studio and Firebase AI Logic, which enables developers to integrate AI into their apps faster.89. We also introduced a new Google Cloud and NVIDIA developer community, a dedicated forum to connect with experts from both companies.Work smarter with AI enhancements 90. Gmail is getting new, personalized smart replies that incorporate your own context and tone. They’ll pull from your past emails and files in your Drive to draft a response, while also matching your typical tone so your replies sound like you. Try it yourself later this year.91. Try it now! Google Vids is now available to Google AI Pro and Ultra users.92. Try it now! Starting today, we’re making the NotebookLM app available on Play Store and App Store, to help users take Audio Overviews on the go.93. Also for NotebookLM, we’re bringing more flexibility to Audio Overviews, allowing you to select the ideal length for your summaries, whether you prefer a quick overview or a deeper exploration.94. Video Overviews are coming soon to NotebookLM, helping you turn dense information like PDFs, docs, images, diagrams and key quotes into more digestible narrated overviews.95. We even shared one of our NotebookLM notebooks with you — which included a couple of previews of Video Overviews!96. Our new Labs experiment Sparkify helps you turn your questions into a short animated video, made possible by our latest Gemini and Veo models. These capabilities will be coming to Google products later this year, but in the meantime you can sign up for the waitlist for a chance to try it out.97. We’re also bringing improvements based on your feedback to Learn About, an experiment in Labs where conversational AI meets your curiosity.Finally… we’ll leave you with a few numbers:99. As Sundar shared in his opening keynote, people are adopting AI more than ever before. As one example: This time last year, we were processing 9.7 trillion tokens a month across our products and APIs. Now, we’re processing over 480 trillion — 50 times more.100. Given that, it’s no wonder that the word “AI” was said 92 times during the keynote. But the amount of “AIs” we heard actually took second place — to Gemini! ♊",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO_launches.width-1300.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\n\n    \n    \n\n\n\n\n\n    \n\n    \n      \n\n\u003cdiv data-analytics-module=\"{\n    \u0026#34;module_name\u0026#34;: \u0026#34;Hero Menu\u0026#34;,\n    \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n  }\"\u003e\n      \u003cdiv\u003e\n          \n            \u003cp\u003eMay 21, 2025\u003c/p\u003e\n          \n          \n            \u003cp data-reading-time-render=\"\"\u003e[[read-time]] min read\u003c/p\u003e\n          \n        \u003c/div\u003e\n      \n        \u003cp\u003e\n          That’s a wrap on I/O 2025! Here’s what we announced, launched and demoed.\n        \u003c/p\u003e\n      \n    \u003c/div\u003e\n\n    \n\n    \n      \n\n\n\n\n\n\n\n\n\n\n\u003cdiv\u003e\n    \u003cfigure\u003e\n      \u003cdiv\u003e\n        \u003cp\u003e\u003cimg alt=\"Three iridescent 3D geometric shapes, possibly forming IO, against a starry dark blue background, framed by a light gray border with a grid pattern.\" data-component=\"uni-progressive-image\" fetchpriority=\"high\" height=\"150px\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO2025_launches.width-200.format-webp.webp\" width=\"360px\" data-sizes=\"(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px\" data-srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO2025_launches.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO2025_launches.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO2025_launches.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO2025_launches.width-2200.format-webp.webp 2200w\"/\u003e\n        \u003c/p\u003e\n      \u003c/div\u003e\n      \n    \u003c/figure\u003e\n  \u003c/div\u003e\n\n\n\n\n\n\n    \n\n    \n    \u003cdiv data-reading-time=\"true\" data-component=\"uni-article-body\"\u003e\n\n            \n              \n\n\n\n\n\n\u003cuni-article-speakable page-title=\"100 things we announced at I/O\" listen-to-article=\"Listen to article\" data-date-modified=\"2025-05-21T22:26:49.485461+00:00\" data-tracking-ids=\"G-HGNBTNCHCQ,G-6NKTLKV14N\" data-voice-list=\"en.ioh-pngnat:Cyan,en.usb-pngnat:Lime\" data-script-src=\"https://www.gstatic.com/readaloud/player/web/api/js/api.js\"\u003e\u003c/uni-article-speakable\u003e\n\n            \n\n            \n            \n\n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"jthcr\"\u003eYesterday at Google I/O, we shared how we’re taking the progress we’re making in AI and applying it across our products. Major upgrades are coming to our Gemini app, our generative AI tools and everything in between — including some truly incredible progress we’re making with our AI models (and new ways you can access them).\u003c/p\u003e\u003cp data-block-key=\"amj76\"\u003eHere’s a list of I/O 2025’s highlights — many of which you can try today!\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\n        \u003cp\u003e\u003ch2 data-block-key=\"jthcr\"\u003eAsk anything with AI in Search\u003c/h2\u003e\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"3\" thumbnail-alt=\"Video showing how new AI features in Google Search work.\" video-id=\"sxUBThVQLjU\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\u003col\u003e\u003cli data-block-key=\"jthcr\"\u003e\u003cb\u003eTry it now!\u003c/b\u003e \u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search\"\u003eAI Mode\u003c/a\u003e is starting to roll out for everyone in the U.S. right on Search. But if you want to get access right away, \u003ca href=\"https://labs.google.com/search/experiment/22\"\u003eopt in via Labs\u003c/a\u003e.\u003c/li\u003e\u003cli data-block-key=\"e5aei\"\u003eFor questions where you want an even more thorough response, we’re bringing deep research capabilities into AI Mode in Labs, with \u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/#deep-search\"\u003eDeep Search\u003c/a\u003e.\u003c/li\u003e\u003cli data-block-key=\"f52i3\"\u003eLive capabilities from \u003ca href=\"https://deepmind.google/technologies/project-astra/\"\u003eProject Astra\u003c/a\u003e are coming to AI Mode in Labs. With Search Live, coming this summer, you can talk back-and-forth with Search about what you see in real-time, using your camera.\u003c/li\u003e\u003cli data-block-key=\"bvsmt\"\u003eWe’re also bringing agentic capabilities from \u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/#agentic-capabilities\"\u003eProject Mariner\u003c/a\u003e to AI Mode in Labs, starting with event tickets, restaurant reservations and local appointments.\u003c/li\u003e\u003cli data-block-key=\"djqa5\"\u003eComing soon: When you need some extra help crunching numbers or visualizing data, AI Mode in Labs will \u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/#custom-charts\"\u003eanalyze complex datasets and create graphics\u003c/a\u003e that bring them to life, all custom built for your query. We’ll bring this to sports and finance queries.\u003c/li\u003e\u003cli data-block-key=\"cu239\"\u003eWe’re introducing a new \u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/#shopping\"\u003eAI Mode shopping experience\u003c/a\u003e that brings together advanced AI capabilities with our Shopping Graph to help you browse for inspiration, think through considerations and find the right product for you.\u003c/li\u003e\u003cli data-block-key=\"f898r\"\u003e\u003cb\u003eTry it now!\u003c/b\u003e You can \u003ca href=\"https://blog.google/products/shopping/how-to-use-google-shopping-try-it-on/\"\u003evirtually try on billions of apparel listings\u003c/a\u003e just by uploading a photo of yourself. Our “try on” experiment is rolling out to Search Labs users in the U.S. starting today — \u003ca href=\"https://labs.google.com/search/experiment/36\"\u003eopt in\u003c/a\u003e to try it out now.\u003c/li\u003e\u003cli data-block-key=\"ekctt\"\u003eWe also showed off a \u003ca href=\"https://blog.google/products/shopping/google-shopping-ai-mode-virtual-try-on-update/\"\u003enew agentic checkout\u003c/a\u003e to help you buy at a price that fits your budget with ease. Just tap “track price” on any product listing, set what you want to spend and we’ll let you know if the price drops.\u003c/li\u003e\u003cli data-block-key=\"96i15\"\u003eWe shared some \u003ca href=\"https://blog.google/technology/ai/io-2025-keynote/#ai-mode\"\u003eupdates\u003c/a\u003e on AI Overviews: Since last year’s I/O, AI Overviews have scaled up to 1.5 billion monthly users in 200 countries and territories. That means Google Search is bringing generative AI to more people than any other product in the world.\u003c/li\u003e\u003cli data-block-key=\"2sliu\"\u003eIn our biggest markets like the U.S. and India, AI Overviews is \u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/\"\u003edriving over 10% increase\u003c/a\u003e in usage of Google for the types of queries that show AI Overviews.\u003c/li\u003e\u003cli data-block-key=\"806oj\"\u003eAnd starting this week, \u003ca href=\"https://blog.google/technology/ai/io-2025-keynote/#ai-mode\"\u003eGemini 2.5 is coming to Search\u003c/a\u003e for both AI Mode and AI Overviews in the U.S.\u003c/li\u003e\u003c/ol\u003e\u003ch2 data-block-key=\"dgcst\"\u003eTry new, helpful features for Gemini\u003c/h2\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"5\" thumbnail-alt=\"A video showing a person using Gemini Live on their phone to ask questions about their surroundings as they walk down a street.\" video-id=\"aQKWQNbAH90\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"jthcr\"\u003e\u003cb\u003e12. Try it now!\u003c/b\u003e Now Gemini is an even better study partner with our \u003ca href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#quizzes\"\u003enew interactive quiz feature\u003c/a\u003e. Simply ask Gemini to “create a practice quiz on…” and Gemini will generate questions.\u003c/p\u003e\u003cp data-block-key=\"181m9\"\u003e13. In the coming weeks we’ll also \u003ca href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#gemini-live\"\u003emake Gemini Live more personal\u003c/a\u003e by connecting some of your favorite Google apps so you can take actions mid-conversation, like adding something to your calendar or asking for more details about a location. We’re starting with Google Maps, Calendar, Tasks and Keep, with more app connections coming later.\u003c/p\u003e\u003cp data-block-key=\"2j9qm\"\u003e\u003cb\u003e14. Try it now!\u003c/b\u003e Starting today, \u003ca href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#gemini-live\"\u003ecamera and screen sharing capabilities for Gemini Live\u003c/a\u003e are beginning to roll out beyond Android to Gemini app users on iOS.\u003c/p\u003e\u003cp data-block-key=\"3vfia\"\u003e\u003cb\u003e15. Try it now!\u003c/b\u003e Starting today, we’re introducing a new \u003ca href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#canvas\"\u003eCreate menu within Canvas\u003c/a\u003e that helps you explore the breadth of what Canvas can build for you, allowing you to transform text into interactive infographics, web pages, immersive quizzes and even podcast-style Audio Overviews in 45 languages.\u003c/p\u003e\u003cp data-block-key=\"b8ub1\"\u003e\u003cb\u003e16. Try it now!\u003c/b\u003e Starting today, you can upload PDFs and images \u003ca href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#deep-research\"\u003edirectly into Deep Research\u003c/a\u003e so your research reports draw from a combination of public information and details that you provide.\u003c/p\u003e\u003cp data-block-key=\"dmdes\"\u003e17. Soon, you’ll be able to link your \u003ca href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#deep-research\"\u003edocuments from Drive or from Gmail\u003c/a\u003e and customize the sources Deep Research pulls from, like academic literature.\u003c/p\u003e\u003cp data-block-key=\"61ud8\"\u003e18. We announced \u003ca href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#plans\"\u003eAgent Mode\u003c/a\u003e, an experimental feature where you will be able to simply describe your end goal and Gemini can get things done on your behalf. An experimental version of Agent Mode in the Gemini app will be coming soon to Google AI Ultra subscribers.\u003c/p\u003e\u003cp data-block-key=\"cjt71\"\u003e19. \u003cb\u003eTry it now!\u003c/b\u003e \u003ca href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#chrome\"\u003eGemini in Chrome\u003c/a\u003e will begin rolling out on desktop to Google AI Pro and Google AI Ultra subscribers in the U.S. who use English as their Chrome language on Windows and macOS.\u003c/p\u003e\u003cp data-block-key=\"ajs2a\"\u003e20. The Gemini app now has \u003ca href=\"https://blog.google/technology/ai/io-2025-keynote/\"\u003eover 400 million\u003c/a\u003e monthly active users.\u003c/p\u003e\u003ch2 data-block-key=\"8k5dc\"\u003eLearn more about advancements for Gemini models\u003c/h2\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"7\" thumbnail-alt=\"A video showing Native Audio Output.\" video-id=\"n3LPxbVzVeo\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"jthcr\"\u003e21. With our latest update, Gemini 2.5 Pro is now the world-leading model across the \u003ca href=\"https://web.lmarena.ai/leaderboard\"\u003eWebDev Arena\u003c/a\u003e and \u003ca href=\"https://lmarena.ai/?leaderboard\"\u003eLMArena\u003c/a\u003e leaderboards.\u003c/p\u003e\u003cp data-block-key=\"fpbb0\"\u003e22. We’re \u003ca href=\"http://goo.gle/learnlm\"\u003einfusing LearnLM\u003c/a\u003e directly into Gemini 2.5, which is now the world’s leading model for learning. As detailed in our latest report, Gemini 2.5 Pro outperformed competitors on every category of learning science principles.\u003c/p\u003e\u003cp data-block-key=\"apcqm\"\u003e23. We introduced a \u003ca href=\"https://blog.google/technology/developers/google-ai-developer-updates-io-2025/\"\u003enew preview version\u003c/a\u003e of our leading model, Gemini 2.5 Flash, with stronger performance on coding and complex reasoning tasks that is optimized for speed and efficiency.\u003c/p\u003e\u003cp data-block-key=\"1fedg\"\u003e24. 2.5 Flash is now available to everyone in the \u003ca href=\"http://gemini.google.com/\"\u003eGemini app\u003c/a\u003e, and we\u0026#39;ll make our updated version generally available in \u003ca href=\"http://aistudio.google.com/\"\u003eGoogle AI Studio\u003c/a\u003e for developers and in \u003ca href=\"https://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/studio\"\u003eVertex AI\u003c/a\u003e for enterprises in early June, with 2.5 Pro soon after.\u003c/p\u003e\u003cp data-block-key=\"4httt\"\u003e25. 2.5 Pro will get even better with \u003ca href=\"https://deepmind.google/models/gemini/pro\"\u003eDeep Think\u003c/a\u003e, an experimental, enhanced reasoning mode for highly-complex math and coding.\u003c/p\u003e\u003cp data-block-key=\"6vsia\"\u003e26. We’re bringing new capabilities to both \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#performance\"\u003e2.5 Pro\u003c/a\u003e and \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#flash-improvements\"\u003e2.5 Flash\u003c/a\u003e, including advanced security safeguards. Our \u003ca href=\"https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/\"\u003enew security approach\u003c/a\u003e helped significantly increase Gemini’s protection rate against indirect prompt injection attacks during tool use, making Gemini 2.5 our most secure model family to date.\u003c/p\u003e\u003cp data-block-key=\"6g5m4\"\u003e27. We\u0026#39;re bringing \u003ca href=\"https://deepmind.google/technologies/project-mariner/\"\u003eProject Mariner\u003c/a\u003e\u0026#39;s computer use capabilities into the \u003ca href=\"https://ai.google.dev/\"\u003eGemini API\u003c/a\u003e and \u003ca href=\"https://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/studio\"\u003eVertex AI\u003c/a\u003e. Companies like Automation Anywhere, UiPath, Browserbase, Autotab, The Interaction Company and Cartwheel are exploring its potential, and we\u0026#39;re excited to roll it out more broadly for developers to experiment with this summer.\u003c/p\u003e\u003cp data-block-key=\"3h66o\"\u003e28. Both 2.5 Pro and Flash will now include thought summaries in the \u003ca href=\"https://ai.google.dev/\"\u003eGemini API\u003c/a\u003e and in \u003ca href=\"https://auth.cloud.google/signin?continueUrl=https://console.cloud.google/\u0026amp;wiffid=CAoSJGY4OWIzNDdlLWM3OWUtNGZlMS1hMzZlLThkMmNjMWQzNjMxZg\"\u003eVertex AI\u003c/a\u003e. Thought summaries take the model’s raw thoughts and organize them into a clear format with headers, key details and information about model actions, like when they use tools.\u003c/p\u003e\u003cp data-block-key=\"4npjg\"\u003e29. We launched 2.5 Flash with \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#developer-experience\"\u003ethinking budgets\u003c/a\u003e to give developers more control over cost by balancing latency and quality, and we’re extending this capability to 2.5 Pro. This allows you to control the number of tokens a model uses to think before it responds, or even turn its thinking capabilities off. Gemini 2.5 Pro with budgets will be generally available for stable production use in the coming weeks, along with our generally available model.\u003c/p\u003e\u003cp data-block-key=\"cl79n\"\u003e30. We \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#developer-experience\"\u003eadded native SDK support for Model Context Protocol (MCP) definitions\u003c/a\u003e in the Gemini API for easier integration with open-source tools. We’re also exploring ways to deploy MCP servers and other hosted tools, making it easier for you to build agentic applications.\u003c/p\u003e\u003cp data-block-key=\"531bh\"\u003e31. We introduced a new research model, called \u003ca href=\"https://deepmind.google/models/gemini-diffusion\"\u003eGemini Diffusion\u003c/a\u003e. This text diffusion model learns to generate outputs by converting random noise into coherent text or code, like how our current models in \u003ca href=\"https://deepmind.google/models/imagen\"\u003eimage\u003c/a\u003e and \u003ca href=\"https://deepmind.google/models/veo\"\u003evideo\u003c/a\u003e generation work. We’ll continue our work on different approaches to lowering latency in all our Gemini models, with a faster \u003ca href=\"https://blog.google/technology/google-deepmind/gemini-diffusion/\"\u003e2.5 Flash Lite\u003c/a\u003e coming soon.\u003c/p\u003e\u003ch2 data-block-key=\"6q9pt\"\u003eAccess our AI tools with new options\u003c/h2\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"9\" thumbnail-alt=\"A video explaining Google AI Ultra\" video-id=\"2CquRQiDzx8\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"jthcr\"\u003e32. We introduced \u003ca href=\"http://one.google.com/ai?utm_source=g1\u0026amp;utm_medium=web\u0026amp;utm_campaign=google_ai_plan_blog\"\u003eGoogle AI Ultra\u003c/a\u003e, a new AI subscription plan with the highest usage limits and access to our most capable models and premium features, plus 30 TB of storage and access to YouTube Premium.\u003c/p\u003e\u003cp data-block-key=\"1gc0j\"\u003e33. \u003ca href=\"https://blog.google/products/google-one/google-ai-ultra/\"\u003eGoogle AI Ultra is available\u003c/a\u003e in the U.S. now, with more countries coming soon. It’s $249.99 a month, with a special offer for first-time users of 50% off for your first three months.\u003c/p\u003e\u003cp data-block-key=\"dvg3r\"\u003e34. \u003ca href=\"https://blog.google/products/gemini/google-one-ai-premium-students-free/\"\u003eCollege students\u003c/a\u003e in the U.S., Brazil, Indonesia, Japan and the U.K. are also eligible to get a free upgrade of Gemini for a whole school year — more countries are coming soon.\u003c/p\u003e\u003cp data-block-key=\"b244c\"\u003e35. There’s also \u003ca href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#plans\"\u003eGoogle Al Pro\u003c/a\u003e, which gives you a suite of Al tools for $19.99/month. This Pro plan will level up your Gemini app experience. It also includes products like Flow, NotebookLM and more, all with special features and higher rate limits.\u003c/p\u003e\u003ch2 data-block-key=\"rnbo\"\u003eExplore your creativity with new generative AI\u003c/h2\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"11\" thumbnail-alt=\"A video showing Google\u0026#39;s new tool, Flow.\" video-id=\"A0VttaLy4sU\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"jthcr\"\u003e36. \u003cb\u003eTry it now!\u003c/b\u003e We announced \u003ca href=\"https://blog.google/technology/ai/generative-media-models-io-2025/#veo-3\"\u003eVeo 3\u003c/a\u003e, which lets you generate video with audio and is now available in the Gemini app for Google AI Ultra subscribers in the U.S., as well as in Vertex AI.\u003c/p\u003e\u003cp data-block-key=\"1nn8g\"\u003e37. We also added new capabilities to our popular \u003ca href=\"https://blog.google/technology/ai/generative-media-models-io-2025/#veo-2-updates\"\u003eVeo 2\u003c/a\u003e model, including new camera controls, outpainting and object add and remove.\u003c/p\u003e\u003cp data-block-key=\"1ll18\"\u003e38. We showed you four new films created with Veo alongside other tools and techniques. View these films from our partners and other inspirational content on \u003ca href=\"https://labs.google/flow/about\"\u003eFlow TV\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"1rvv8\"\u003e39. \u003cb\u003eTry it now!\u003c/b\u003e \u003ca href=\"https://blog.google/technology/ai/generative-media-models-io-2025/#imagen-4\"\u003eImagen 4\u003c/a\u003e is our latest Imagen model, and it has remarkable clarity in fine details like skin, fur and intricate textures, and excels in both photorealistic and abstract styles. Imagen 4 is available today in the Gemini app.\u003c/p\u003e\u003cp data-block-key=\"fhgml\"\u003e40. Imagen 4 is also available in \u003ca href=\"http://labs.google/whisk\"\u003eWhisk\u003c/a\u003e, and to enterprises in Vertex AI.\u003c/p\u003e\u003cp data-block-key=\"cekea\"\u003e41. Soon, Imagen 4 will be available in a Fast version that’s up to 10x faster than Imagen 3.\u003c/p\u003e\u003cp data-block-key=\"2jurq\"\u003e42. Imagen 4 can create images in a range of aspect ratios and up to 2K resolution so you can get even higher-quality for printing and presentations.\u003c/p\u003e\u003cp data-block-key=\"971eq\"\u003e43. It is also significantly better at spelling and typography, making it easier to create your own greeting cards, posters and even comics.\u003c/p\u003e\u003cp data-block-key=\"806mi\"\u003e44.\u003cb\u003e Try it now!\u003c/b\u003e \u003ca href=\"https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/\"\u003eFlow\u003c/a\u003e is our new AI filmmaking tool. Using Google DeepMind’s best-in-class models, Flow lets you weave cinematic films with control of characters, scenes and styles, so more people than ever can create visually striking movies with AI.\u003c/p\u003e\u003cp data-block-key=\"9o7jh\"\u003e45. Flow is available today for Google AI Pro and Ultra plan subscribers in the United States.\u003c/p\u003e\u003cp data-block-key=\"9kb1f\"\u003e46. In April, we \u003ca href=\"https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/\"\u003eexpanded access\u003c/a\u003e to Music AI Sandbox, powered by \u003ca href=\"https://deepmind.google/technologies/lyria\"\u003eLyria 2\u003c/a\u003e. Lyria 2 brings powerful composition and endless exploration, and is now available for creators \u003ca href=\"https://www.youtube.com/shorts/uQY6Z6O_dZQ\"\u003ethrough YouTube Shorts\u003c/a\u003e and enterprises in \u003ca href=\"https://cloud.google.com/blog/products/ai-machine-learning/announcing-veo-3-imagen-4-and-lyria-2-on-vertex-ai\"\u003eVertex AI\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"e0br1\"\u003e47. Lyria 2 can arrange rich vocals that sound like a solo singer or a full choir.\u003c/p\u003e\u003cp data-block-key=\"f010v\"\u003e48. Lyria RealTime is an interactive music generation model that allows anyone to interactively create, control, and perform music in real time. This model is now available via the \u003ca href=\"https://ai.google.dev/gemini-api/docs/lyria/music-generation\"\u003eGemini API\u003c/a\u003e in \u003ca href=\"http://aistudio.google.com/apps/bundled/midi-dj\"\u003eGoogle AI Studio\u003c/a\u003e and Vertex AI.\u003c/p\u003e\u003cp data-block-key=\"atump\"\u003e49. We \u003ca href=\"https://blog.google/technology/google-labs/deepmind-primordial-soup-collaboration/\"\u003eannounced a partnership\u003c/a\u003e between Google DeepMind and Primordial Soup, a new venture dedicated to storytelling innovation founded by pioneering director Darren Aronofsky. Primordial Soup is producing three short films using Google DeepMind’s generative AI models, tools and capabilities, including Veo.\u003c/p\u003e\u003cp data-block-key=\"4n7vu\"\u003e50. The first film, “\u003ca href=\"https://youtu.be/QUCIA6HALcg?si=7M-RBlJ4FxNCj9Js\"\u003eANCESTRA\u003c/a\u003e,” is directed by award-winning filmmaker Eliza McNitt and will premiere at the Tribeca Festival on June 13, 2025.\u003c/p\u003e\u003cp data-block-key=\"585ff\"\u003e51. To make it easier for people and organizations to detect AI-generated content, we announced \u003ca href=\"https://blog.google/technology/ai/google-synthid-ai-content-detector/\"\u003eSynthID Detector\u003c/a\u003e, a verification portal that helps to quickly and efficiently identify content that is watermarked with SynthID.\u003c/p\u003e\u003cp data-block-key=\"63ab8\"\u003e52. And since launch, SynthID has already watermarked over \u003ca href=\"http://x.com/Google/status/1924894913419784478\"\u003e10 billion pieces of content\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"7t1fa\"\u003e53. We are starting to roll out the SynthID Detector portal to a group of early testers. Journalists, media professionals and researchers can join our \u003ca href=\"https://docs.google.com/forms/d/17AiEqHpgnp7GwsIfQNFLCB-5nSr7YxnLI_4jmm7kBGU/edit\"\u003ewaitlist\u003c/a\u003e to gain access to the SynthID Detector.\u003c/p\u003e\u003ch2 data-block-key=\"441f4\"\u003eTake a look at the future of AI assistance\u003c/h2\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"13\" thumbnail-alt=\"Video demonstrating Google\u0026#39;s Project Astra.\" video-id=\"3h4SRE9W6pY\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"jthcr\"\u003e54. We’re working to extend our best multimodal foundation model, \u003ca href=\"https://deepmind.google/technologies/gemini/pro/\"\u003eGemini 2.5 Pro\u003c/a\u003e, to become a \u003ca href=\"https://blog.google/technology/google-deepmind/gemini-universal-ai-assistant/\"\u003e“world model”\u003c/a\u003e that can make plans and imagine new experiences by understanding and simulating aspects of the world, just as the brain does.\u003c/p\u003e\u003cp data-block-key=\"d8gpb\"\u003e55. \u003ca href=\"https://www.youtube.com/watch?v=JcDBFAm9PPI\u0026amp;pp=0gcJCY0JAYcqIYzv\"\u003eUpdates to Project Astra\u003c/a\u003e, our research prototype that explores the capabilities of a universal AI assistant, include more natural voice output with native audio, improved memory and computer control. Over time we’ll bring these new capabilities to Gemini Live and new experiences in Search, Live API for devs and new form factors like Android XR glasses.\u003c/p\u003e\u003cp data-block-key=\"e7973\"\u003e56. And as part of our Project Astra research, we partnered with the visual interpreting service Aira to build a prototype that \u003ca href=\"https://www.youtube.com/watch?v=3h4SRE9W6pY\"\u003eassists members of the blind and low-vision community\u003c/a\u003e with everyday tasks, complementing the skills and tools they already use.\u003c/p\u003e\u003cp data-block-key=\"1sug\"\u003e57. With Project Astra, we’re \u003ca href=\"https://www.youtube.com/watch?v=MQ4JfafE5Wo\"\u003eprototyping a conversational tutor\u003c/a\u003e that can help with homework. Not only can it follow along with what you’re working on, but it can also walk you through problems step-by-step, identify mistakes and even generate diagrams to help explain concepts if you get stuck.\u003c/p\u003e\u003cp data-block-key=\"209c7\"\u003e58. This research experience \u003ca href=\"https://blog.google/outreach-initiatives/education/google-gemini-learnlm-update/\"\u003ewill be coming to Google products\u003c/a\u003e later this year and Android Trusted Testers can \u003ca href=\"https://deepmind.google/technologies/project-astra/\"\u003esign up for the waitlist\u003c/a\u003e for a preview.\u003c/p\u003e\u003cp data-block-key=\"b08kj\"\u003e59. We took a look at the first Android XR device coming later this year: Samsung’s Project Moohan. This headset will offer immersive experiences on an infinite screen.\u003c/p\u003e\u003cp data-block-key=\"7fo7t\"\u003e60. And we shared a sneak peek at how Gemini will work on \u003ca href=\"https://blog.google/products/android/android-xr-gemini-glasses-headsets/\"\u003eglasses with Android XR\u003c/a\u003e in real-world scenarios, including messaging friends, making appointments, asking for turn-by-turn directions, taking photos and more.\u003c/p\u003e\u003cp data-block-key=\"3vr73\"\u003e61. We even demoed live language translation between two people, showing the potential for these glasses to break down language barriers.\u003c/p\u003e\u003cp data-block-key=\"70l14\"\u003e62. Android XR prototype glasses are now in the hands of trusted testers, who are helping us make sure we’re building a truly assistive product and doing so in a way that respects privacy for you and those around you.\u003c/p\u003e\u003cp data-block-key=\"6njg4\"\u003e63. Plus we’re partnering with innovative eyewear brands, starting with Gentle Monster and Warby Parker, to create glasses with Android XR that you’ll want to wear all day.\u003c/p\u003e\u003cp data-block-key=\"7h21c\"\u003e64. We’re advancing our partnership with Samsung to go beyond headsets and extend Android XR to glasses. Together we’re creating a software and reference hardware platform that will enable the ecosystem to make great glasses. Developers will be able to start building for this platform later this year.\u003c/p\u003e\u003ch2 data-block-key=\"cff5k\"\u003eCommunicate better, in near real time\u003c/h2\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"15\" thumbnail-alt=\"A demo of Google Beam.\" video-id=\"OTObIPmDyjc\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"jthcr\"\u003e65. A few years ago, we introduced Project Starline, a research project that enabled remote conversations that used 3D video technology to make it feel like two people were in the same room. Now, it’s evolving into a new platform called \u003ca href=\"https://blog.google/technology/research/project-starline-google-beam-update/\"\u003eGoogle Beam\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"42jn8\"\u003e66. We’re working with Zoom and HP to bring the first Google Beam devices to market with select customers later this year. We’re also partnering with industry leaders like Zoom, Diversified and AVI-SPL to bring Google Beam to businesses and organizations worldwide.\u003c/p\u003e\u003cp data-block-key=\"56nrt\"\u003e67. You’ll even see the first Google Beam products from HP at InfoComm in a few weeks.\u003c/p\u003e\u003cp data-block-key=\"flo81\"\u003e68. We announced speech translation, \u003ca href=\"https://blog.google/products/workspace/google-workspace-gemini-may-2025-updates/\"\u003ewhich is available now in Google Meet\u003c/a\u003e. This translation feature not only happens in near real-time, thanks to Google AI, but it’s able to maintain the quality, tone, and expressiveness of someone’s voice. The free-flowing conversation enables people to understand each other and feel connected, with no language barrier.\u003c/p\u003e\u003ch2 data-block-key=\"eoqt4\"\u003eBuild better with developer launches\u003c/h2\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"17\" thumbnail-alt=\"A demo of Jules.\" video-id=\"cWvwpUw0vFA\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"jthcr\"\u003e69. Over \u003ca href=\"https://blog.google/technology/ai/io-2025-keynote/\"\u003e7 million developers\u003c/a\u003e are building with Gemini, five times more than this time last year.\u003c/p\u003e\u003cp data-block-key=\"4e89l\"\u003e70. Gemini usage on Vertex AI is \u003ca href=\"https://blog.google/technology/ai/io-2025-keynote/\"\u003eup 40 times\u003c/a\u003e compared to this time last year.\u003c/p\u003e\u003cp data-block-key=\"b2213\"\u003e71. We’re \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#new-capabilities\"\u003ereleasing\u003c/a\u003e new previews for text-to-speech in 2.5 Pro and 2.5 Flash. These have first-of-its-kind support for multiple speakers, enabling text-to-speech with two voices via native audio out. Like Native Audio dialogue, text-to-speech is expressive, and can capture really subtle nuances, such as whispers. It works in over 24 languages and seamlessly switches between them.\u003c/p\u003e\u003cp data-block-key=\"3k5fd\"\u003e72. The \u003ca href=\"https://ai.google.dev/gemini-api/docs/live\"\u003eLive API\u003c/a\u003e is introducing a preview version of audio-visual input and native audio out dialogue, so you can directly build conversational experiences.\u003c/p\u003e\u003cp data-block-key=\"6ulpb\"\u003e73. \u003cb\u003eTry it now!\u003c/b\u003e \u003ca href=\"https://blog.google/technology/google-labs/jules/\"\u003eJules\u003c/a\u003e is a parallel, asynchronous agent for your GitHub repositories to help you improve and understand your codebase. It is now open to all developers in beta. With Jules you can delegate multiple backlog items and coding tasks at the same time, and even get an audio overview of all the recent updates to your codebase.\u003c/p\u003e\u003cp data-block-key=\"f3tmj\"\u003e74. \u003ca href=\"https://developers.googleblog.com/en/introducing-gemma-3n/\"\u003eGemma 3n\u003c/a\u003e is our latest fast and efficient open multimodal model that’s engineered to run smoothly on your phones, laptops, and tablets. It handles audio, text, image, and video. The initial rollout is underway on Google AI Studio and Google Cloud with plans to expand to open-source tools in the coming weeks.\u003c/p\u003e\u003cp data-block-key=\"8rbl8\"\u003e75. \u003cb\u003eTry it now!\u003c/b\u003e \u003ca href=\"https://blog.google/technology/developers/google-ai-developer-updates-io-2025/\"\u003eGoogle AI Studio\u003c/a\u003e now has a cleaner UI, integrated documentation, usage dashboards, new apps, and a new Generate Media tab to explore and experiment with our cutting-edge generative models, including Imagen, Veo and native image generation.\u003c/p\u003e\u003cp data-block-key=\"32jl9\"\u003e76. \u003ca href=\"https://blog.google/technology/developers/google-ai-developer-updates-io-2025/\"\u003eColab\u003c/a\u003e will soon be a new, fully agentic experience. Simply tell Colab what you want to achieve, and watch as it takes action in your notebook, fixing errors and transforming code to help you solve hard problems faster.\u003c/p\u003e\u003cp data-block-key=\"4h39h\"\u003e77. \u003ca href=\"https://developers.googleblog.com/en/google-io-2025-developer-keynote-recap/\"\u003eSignGemma\u003c/a\u003e is an upcoming open model that translates sign language into spoken language text, (best at American Sign Language to English), enabling developers to create new apps and integrations for Deaf and Hard of Hearing users.\u003c/p\u003e\u003cp data-block-key=\"2b9lq\"\u003e78. \u003ca href=\"https://developers.google.com/health-ai-developer-foundations/medgemma\"\u003eMedGemma\u003c/a\u003e is our most capable open model for multimodal medical text and image comprehension designed for developers to adapt and build their health applications, like analyzing medical images. MedGemma is available now for use now as part of \u003ca href=\"https://developers.google.com/health-ai-developer-foundations/medgemma\"\u003eHealth AI Developer Foundations\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"f1ph9\"\u003e79. \u003ca href=\"https://blog.google/technology/developers/google-ai-developer-updates-io-2025/\"\u003eStitch\u003c/a\u003e is a new AI-powered tool to generate high-quality UI designs and corresponding frontend code for desktop and mobile by using natural language descriptions or image prompts.\u003c/p\u003e\u003cp data-block-key=\"1sovh\"\u003e80. \u003cb\u003eTry it now!\u003c/b\u003e We announced \u003ca href=\"https://android-developers.googleblog.com/2025/05/google-io-2025-whats-new-in-android-development-tools.html\"\u003eJourneys in Android Studio\u003c/a\u003e, which lets developers test critical user journeys using Gemini by describing test steps in natural language.\u003c/p\u003e\u003cp data-block-key=\"2tmdn\"\u003e81. \u003ca href=\"https://android-developers.googleblog.com/2025/05/google-io-2025-whats-new-in-android-development-tools.html\"\u003eVersion Upgrade Agent in Android Studio\u003c/a\u003e is coming soon to automatically update dependencies to the latest compatible version, parsing through release notes, building the project and fixing any errors.\u003c/p\u003e\u003cp data-block-key=\"19kv8\"\u003e82. We introduced new \u003ca href=\"https://developers.googleblog.com/en/new-google-pay-features-to-enhance-your-payment-flows/\"\u003eupdates across the Google Pay API\u003c/a\u003e designed to help developers create smoother, safer, and more successful checkout experiences, including Google Pay in Android WebViews.\u003c/p\u003e\u003cp data-block-key=\"eabr3\"\u003e83. \u003ca href=\"https://medium.com/flutter/whats-new-in-flutter-3-32-40c1086bab6e\"\u003eFlutter 3.32\u003c/a\u003e has new features designed to accelerate development and enhance apps.\u003c/p\u003e\u003cp data-block-key=\"dho24\"\u003e84. And we shared updates for our Agent Development Kit (ADK), the Vertex AI Agent Engine, and our Agent2Agent (A2A) protocol, which enables interactions between multiple agents.\u003c/p\u003e\u003cp data-block-key=\"5c838\"\u003e85. \u003cb\u003eTry it now!\u003c/b\u003e \u003ca href=\"https://android-developers.googleblog.com/2025/05/whats-new-in-wear-os-6.html\"\u003eDeveloper Preview for Wear OS 6\u003c/a\u003e introduces Material 3 Expressive and updated developer tools for Watch Faces, richer media controls and the Credential Manager for authentication.\u003c/p\u003e\u003cp data-block-key=\"b867s\"\u003e86. \u003cb\u003eTry it now!\u003c/b\u003e We announced that \u003ca href=\"https://blog.google/technology/developers/gemini-code-assist-updates-google-io-2025/\"\u003eGemini Code Assist\u003c/a\u003e for individuals and Gemini Code Assist for GitHub are generally available, and developers can get started in less than a minute. Gemini 2.5 now powers both the free and paid versions of Gemini Code Assist, features advanced coding performance; and helps developers excel at tasks like creating visually compelling web apps, along with code transformation and editing.\u003c/p\u003e\u003cp data-block-key=\"bgs0o\"\u003e87. Here’s an example of a recent update you can explore in Gemini Code Assist: Quickly resume where you left off and jump into new directions with \u003ca href=\"https://developers.google.com/gemini-code-assist/docs/write-code-gemini#create_multiple_chats\"\u003echat history and threads\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"lmrk\"\u003e88. \u003ca href=\"https://firebase.blog/posts/2025/05/whats-new-at-google-io\"\u003eFirebase announced\u003c/a\u003e new features and tools to help developers build AI-powered apps more easily, including updates to the recently launched Firebase Studio and Firebase AI Logic, which enables developers to integrate AI into their apps faster.\u003c/p\u003e\u003cp data-block-key=\"d401t\"\u003e89. We also \u003ca href=\"https://blog.google/products/google-cloud/nvidia-google-cloud-developer-community/\"\u003eintroduced\u003c/a\u003e a new \u003ca href=\"http://developers.google.com/community/nvidia\"\u003eGoogle Cloud and NVIDIA developer community\u003c/a\u003e, a dedicated forum to connect with experts from both companies.\u003c/p\u003e\u003ch2 data-block-key=\"1m1el\"\u003eWork smarter with AI enhancements\u003c/h2\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"19\" thumbnail-alt=\"A video showing Gemini features in Gmail.\" video-id=\"jJkLX8N4axI\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;100 things we announced at I/O\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"jthcr\"\u003e90. \u003ca href=\"https://workspace.google.com/blog/product-announcements/new-ways-to-do-your-best-work\"\u003eGmail is getting new\u003c/a\u003e, personalized smart replies that incorporate your own context and tone. They’ll pull from your past emails and files in your Drive to draft a response, while also matching your typical tone so your replies sound like you. Try it yourself later this year.\u003c/p\u003e\u003cp data-block-key=\"aorqs\"\u003e91. \u003cb\u003eTry it now!\u003c/b\u003e Google Vids is now available to \u003ca href=\"https://blog.google/products/google-one/google-ai-ultra/\"\u003eGoogle AI Pro and Ultra users\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"am1dq\"\u003e92. \u003cb\u003eTry it now!\u003c/b\u003e Starting today, we’re making the \u003ca href=\"https://blog.google/technology/ai/notebooklm-app/\"\u003eNotebookLM app\u003c/a\u003e available on Play Store and App Store, to help users take Audio Overviews on the go.\u003c/p\u003e\u003cp data-block-key=\"1cgeg\"\u003e93. Also for NotebookLM, we’re bringing\u003ca href=\"https://blog.google/technology/ai/notebooklm-app/\"\u003e more flexibility to Audio Overviews\u003c/a\u003e, allowing you to select the ideal length for your summaries, whether you prefer a quick overview or a deeper exploration.\u003c/p\u003e\u003cp data-block-key=\"28fs1\"\u003e94. Video Overviews are coming soon to NotebookLM, helping you turn dense information like PDFs, docs, images, diagrams and key quotes into more digestible narrated overviews.\u003c/p\u003e\u003cp data-block-key=\"f33hh\"\u003e95. We even shared \u003ca href=\"https://blog.google/feed/notebooklm-google-io-2025/\"\u003eone of our NotebookLM notebooks\u003c/a\u003e with you — which included a couple of previews of Video Overviews!\u003c/p\u003e\u003cp data-block-key=\"bl5q4\"\u003e96. \u003ca href=\"http://sparkify.withgoogle.com/\"\u003eOur new Labs experiment Sparkify\u003c/a\u003e helps you turn your questions into a short animated video, made possible by our latest Gemini and Veo models. These capabilities will be coming to Google products later this year, but in the meantime you can \u003ca href=\"http://sparkify.withgoogle.com/\"\u003esign up for the waitlist\u003c/a\u003e for a chance to try it out.\u003c/p\u003e\u003cp data-block-key=\"9gvn1\"\u003e97. We’re also bringing improvements based on your feedback to \u003ca href=\"https://learning.google.com/experiments/learn-about/signup\"\u003eLearn About\u003c/a\u003e, an experiment in Labs where conversational AI meets your curiosity.\u003c/p\u003e\u003ch2 data-block-key=\"cchia\"\u003eFinally… we’ll leave you with a few numbers:\u003c/h2\u003e\u003cp data-block-key=\"8o1ad\"\u003e99. As Sundar shared in his opening keynote, \u003ca href=\"https://blog.google/technology/ai/io-2025-keynote/\"\u003epeople are adopting AI more than ever before\u003c/a\u003e. As one example: This time last year, we were processing 9.7 trillion tokens a month across our products and APIs. Now, we’re processing over 480 trillion — 50 times more.\u003c/p\u003e\u003cp data-block-key=\"83pqu\"\u003e100. Given that, it’s no wonder that the word “AI” was said \u003ca href=\"https://x.com/Google/status/1924901352070676540\"\u003e92 times during the keynote\u003c/a\u003e. But the amount of “AIs” we heard actually took second place — to Gemini! ♊\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\n\u003cuni-related-content-tout title=\"I/O 2025\" cta=\"See more\" summary=\"We’re doing cutting-edge research to build the most helpful AI that’s more intelligent, agentic and personalized.\" hideimage=\"False\" eyebrow=\"Collection\" image-alt-text=\"I/O 2025 icon\" role=\"none\" fullurl=\"https://blog.google/technology/developers/google-io-2025-collection/\" pagetype=\"collectiondetailpage\" isarticlepage=\"\"\u003e\n  \n    \u003cdiv slot=\"rct-image-slot\"\u003e\n      \n    \u003cfigure\u003e\n        \u003cpicture\u003e\n            \n\n\n    \n\n    \n        \u003csource media=\"(max-resolution: 1.5dppx)\" sizes=\"300px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_ss.width-300.format-webp.webp 300w\"/\u003e\n    \n        \u003csource media=\"(min-resolution: 1.5dppx)\" sizes=\"600px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_ss.width-600.format-webp.webp 600w\"/\u003e\n    \n\n    \u003cimg src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_ss.width-600.format-webp.webp\" alt=\"I/O 2025 icon\" sizes=\" 300px,  600px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_ss.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_ss.width-600.format-webp.webp 600w\" data-target=\"image\" loading=\"lazy\"/\u003e\n    \n\n\n        \u003c/picture\u003e\n    \u003c/figure\u003e\n\n\n    \u003c/div\u003e\n  \n\u003c/uni-related-content-tout\u003e\n\n  \n\n\n            \n            \n\n            \n              \n\n\n\n\n            \n          \u003c/div\u003e\n  \u003c/article\u003e\u003c/div\u003e",
  "readingTime": "23 min read",
  "publishedTime": "2025-05-21T23:00:00Z",
  "modifiedTime": null
}
