{
  "id": "05957c83-9d7d-48cf-aa0b-0bcf4607b041",
  "title": "My favorite use-case for AI is writing logs",
  "link": "https://newsletter.vickiboykis.com/archive/my-favorite-use-case-for-ai-is-writing-logs/",
  "description": "Comments",
  "author": "",
  "published": "Thu, 17 Jul 2025 23:38:48 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Normcore Tech",
  "length": 9758,
  "excerpt": "One of my favorite AI dev products today is Full Line Code Completion in PyCharm (bundled with the IDE since late 2023). It’s extremely well-thought out,...",
  "siteName": "Normcore Tech",
  "favicon": "https://newsletter.vickiboykis.com/static/images/icons/icon@72.png",
  "text": "July 17, 2025 One of my favorite AI dev products today is Full Line Code Completion in PyCharm (bundled with the IDE since late 2023). It’s extremely well-thought out, unintrusive, and makes me a more effective developer. Most importantly, it still keeps me mostly in control of my code. I’ve now used it in GoLand as well. I’ve been a happy JetBrains customer for a long time now, and it’s because they ship features like this. I frequently work with code that involves sequential data processing, computations, and async API calls across multiple services. I also deal with a lot of precise vector operations in PyTorch that shape suffixes don’t always illuminate. So, print statement debugging and writing good logs has been a critical part of my workflows for years. As Kerningan and Pike say in The Practice of Programming about preferring print to debugging, …[W]e find stepping through a program less productive than thinking harder and adding output statements and self-checking code at critical places. Clicking over statements takes longer than scanning the output of judiciously-placed displays. It takes less time to decide where to put print statements than to single-step to the critical section of code, even assuming we know where that is. One thing that is annoying about logging is that f-strings are great but become repetitive to write if you have to write them over and over, particularly if you’re formatting values or accessing elements of data frames, lists, and nested structures, and particularly if you have to scan your codebase to find those variables. Writing good logs is important but also breaks up a debugging flow. from loguru import logger logger.info(f'Adding a log for {your_variable} and {len(my_list)} and {df.head(0)}') The amount of cognitive overhead in this deceptively simple log is several levels deep: you have to first stop to type logger.info (or is it logging.info? I use both loguru and logger depending on the codebase and end up always getting the two confused.) Then, the parentheses, the f-string itself, and then the variables in brackets. Now, was it your_variable or your_variable_with_edits from five lines up? And what’s the syntax for accessing a subset of df.head again? With full-line-code completion, JetBrains’ model auto-infers the log completion from the surrounding text, with a limit of 384 characters. Inference starts by taking the file extension as input, combined with the filepath, and then the part of the code above the input cursor, so that all of the tokens in the file extension, plus path, plus code above the caret, fit. Everything is combined and sent to the model in the prompt. The constrained output good enough most of the time that it speeds up my workflow a lot. An added bonus is that it often writes a much clearer log than I, a lazy human, would write, logs. Because they’re so concise, I often don’t even remove when I’m done debugging because they’re now valuable in prod. Here’s an example from a side project I’m working on. In the first case, the is autocomplete inferring that I actually want to check the Redis URL, a logical conclusion here. redis = aioredis.from_url(settings.redis_url, decode_responses=True) In this second case, it assumes I’d like the shape of the dataframe, also a logical conclusion because the profiling dataframes is a very popular use-case for logs. from pandas import DataFrame data = [['apples', '2.5'], ['oranges', '3.0'], ['bananas', '4.0']] column_names = ['Fruit', 'Quantity'] df = DataFrame(data, columns=column_names) Implementation The coolest part of this feature is that the inference model is entirely local to your machine. This enforces a few very important requirements on the development team, namely compression and speed. The model has to be small enough to bundle with the IDE for desktop memory footprints (already coming in at around ~1GB for the MacOS binary), which eliminates 99% of current LLMsAnd yet, the model has to be smart enough to interpolate lines of code from its small context windowThe local requirement eliminates any model inference engines like vLLM, SGLM, or Ray which implement KV cache optimization like PagedAttentionIt has to be a model that’s fast enough to produce its first token (and all subsequent tokens) extremely quickly,Finally, it has to be optimized for Python specifically since this model is only available in PyCharm This is drastically different from the current assumptions around how we build and ship LLMs: that they need to be extremely large, general-purpose models served over proprietary APIs. we We find ourselves in a very constrained solution space because we no longer have to do all this other stuff that generalized LLMs have to do: write poetry, reason through math problems, act as OCR, offer code canvas templating, write marketing emails, and generate Studio Ghibli memes. All we have to do is train a model to complete a single line of code with a context of 384 characters! And then compress the crap out of that model so that it can fit on-device and perform inference. So how did they do it? Luckily, JetBrains published a paper on this, and there are a bunch of interesting notes. The work is split into two parts, model training, and then the integration of the plugin itself. The model is trained is done in PyTorch and then quantized. First, they train a GPT-2 style Transformer decoder-only model of 100 million parameters, including a tokenizer (aka autoregressive text completion like you’d get from Claude, OpenAI, Gemini, and friends these days). They later changed this architecture to Llama2 after the success of the growing llama.cpp and GGUF community, as well as the better performance of the newer architecture.The original dataset they used to train the model was a subset of The Stack, a code dataset across permissive licenses with 6TB of code in 30 programming languagesThe initial training set was “just” 45 GB and in preparing the data for training, in data cleaning, for space constraints, they remove all code comments in the training data specifically to focus on code generationThey do a neat trick for tokenizing Python (using a BPE-style tokenizer optimized for character pairs rather than bytes, since code is made up of smaller snippets and idioms than natural language text) which is indentation-sensitive, by converting spaces and tabs to start-end \u003cSCOPE_IN\u003e\u003cSCOPE_OUT\u003e tokens, to remove tokens that might be different only because they have different whitespacing. They ended up going with a tokenizer vocab size of 16,384.They do another very cool step in training which is to remove imports because they find that developers usually add imports in after writing the actual code, a fact that the model needs to anticipateThey then split into train/test for evaluation and trained for several days on 8 NVidia A100 GPU with a cross-entropy loss objective function Because they were able to so clearly focus on the domain and understanding of how code inference works, focus on a single programming languages with its own nuances, they were able to make the training data set smaller, make the output more exact, and spend much less time and money training the model. The actual plugin that’s included in PyCharm “is implemented in Kotlin, however, it utilizes an additional native server that is run locally and is implemented in C++” for serving the inference tokens. In order to prepare the model for serving, they: Quantized it from FP32 to INT8 which compressed the model from 400 MB to 100 MBPrepared as a served ONNX RT artifact, which allowed them to use CPU inference, which removed the CUDA overhead tax(later, they switched to using llama.cpp to serve the llama model architecture for the server.Finally, in order to perform inference on a sequence of tokens, they use beam search. Generally, Transformer-decoders are trained on predicting the next token in any given sequence so any individual step will give you a list of tokens along with their ranked probabilities (cementing my long-running theory that everything is a search problem).Since this is computationally impossible at large numbers of tokens, a number of solutions exist to solve the problem of decoding optimally. Beam search creates a graph of all possible returned token sequences and expands at each node with the highest potential probability, limiting to k possible beams. In FLCC, the max number of beams, k, is 20, and they chose to limit generation to collect only those hypotheses that end with a newline character.Additionally they made use of a number of caching strategies, including initializing the model at 50% of total context - i.e. it starts by preloading ~192 characters of previous code, to give you space to either go back and edit old code, which now no longer has to be put into context, or to add new code, which is then added to the context. That way, if your cursor clicks on code you’ve already written, the model doesn’t need to re-infer. There are a number of other very cool architecture and model decisions from the paper that are very worth reading and that show the level of care put into the input data, the modeling, and the inference architecture. The bottom line is that, for me as a user, this experience is extremely thoughtful. It has saved me countless times both in print log debugging and in the logs I ship to prod. In LLM land, there’s both a place for large, generalist models, and there’s a place for small models, and while much of the rest of the world writes about the former, I’m excited to also find more applications built with the latter.",
  "image": "https://vickiboykis.com/images/inline_completion.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n            \n            \u003cdate\u003e\n                \n                \n                July 17, 2025\n                \n                \n            \u003c/date\u003e\n            \n\n            \n\n            \n\n            \n\n            \n            \n            \u003cp\u003eOne of my favorite AI dev products today is \u003ca href=\"https://plugins.jetbrains.com/plugin/14823-full-line-code-completion?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003eFull Line Code Completion\u003c/a\u003e in PyCharm (bundled with the IDE since late 2023). It’s extremely well-thought out, unintrusive, and makes me a more effective developer. Most importantly, it still keeps me mostly in control of my code. I’ve now used it in \u003ca href=\"https://vickiboykis.com/2025/01/23/you-can-just-hack-on-atproto/?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003eGoLand\u003c/a\u003e as well. I’ve been a happy JetBrains customer for a long time now, and it’s because they ship features like this.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg draggable=\"false\" src=\"https://vickiboykis.com/images/inline_completion.png\" width=\"400\"/\u003e\u003cfigcaption\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eI frequently work with code that involves sequential data processing, computations, and async API calls across multiple services. I also deal with a lot of precise vector operations in PyTorch that \u003ca href=\"https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003eshape suffixes\u003c/a\u003e don’t always illuminate. So, print statement debugging and writing good logs has been a critical part of my workflows for years.\u003c/p\u003e\n\u003cp\u003eAs Kerningan and Pike say in \u003cem\u003eThe Practice of Programming\u003c/em\u003e about preferring print to debugging,\u003c/p\u003e\n\u003cblockquote\u003e\u003cp\u003e…[W]e find stepping through a program less productive than thinking harder and adding output statements and self-checking code at critical places. Clicking over statements takes longer than scanning the output of judiciously-placed displays. It takes less time to decide where to put print statements than to single-step to the critical section of code, even assuming we know where that is.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eOne thing that is annoying about logging is that \u003ca href=\"https://docs.python.org/3/tutorial/inputoutput.html?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs#formatted-string-literals\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003ef-strings\u003c/a\u003e are great but become repetitive to write if you have to write them over and over, particularly if you’re formatting values or accessing elements of data frames, lists, and nested structures, and particularly if you have to scan your codebase to find those variables. Writing good logs is important but also breaks up a debugging flow.\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003ccode\u003e\u003cspan\u003efrom\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eloguru\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003elogger\u003c/span\u003e\n\n\u003cspan\u003elogger\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003einfo\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ef\u003c/span\u003e\u003cspan\u003e\u0026#39;Adding a log for \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003eyour_variable\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e and \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emy_list\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e and \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003edf\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ehead\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\u0026#39;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe amount of cognitive overhead in this deceptively simple log is several levels deep: you have to first stop to type \u003ca href=\"http://logger.info?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003elogger.info\u003c/a\u003e (or is it \u003ca href=\"http://logging.info?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003elogging.info\u003c/a\u003e? I use both loguru and logger depending on the codebase and end up always getting the two confused.) Then, the parentheses, the f-string itself, and then the variables in brackets. Now, was it \u003ccode\u003eyour_variable\u003c/code\u003e or \u003ccode\u003eyour_variable_with_edits\u003c/code\u003e from five lines up? And what’s the syntax for accessing a subset of \u003ccode\u003edf.head\u003c/code\u003e again?\u003c/p\u003e\n\u003cp\u003eWith full-line-code completion, JetBrains’ model auto-infers the log completion from the surrounding text, with a limit of 384 characters. Inference starts by taking the file extension as input, combined with the filepath, and then the part of the code above the input cursor, so that all of the tokens in the file extension, plus path, plus code above the caret, fit. Everything is combined and sent to the model in the prompt.\u003c/p\u003e\n\u003cp\u003eThe constrained output good enough most of the time that it speeds up my workflow a lot. An added bonus is that it often writes a much clearer log than I, a lazy human, would write, logs. Because they’re so concise, I often don’t even remove when I’m done debugging because they’re now valuable in prod.\u003c/p\u003e\n\u003cp\u003eHere’s an example from a side project I’m working on. In the first case, the is autocomplete inferring that I actually want to check the Redis URL, a logical conclusion here.\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003ccode\u003e\u003cspan\u003eredis\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eaioredis\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_url\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003esettings\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eredis_url\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003edecode_responses\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cfigure\u003e\u003cimg draggable=\"false\" src=\"https://vickiboykis.com/images/redis_completion.png\" width=\"400\"/\u003e\u003cfigcaption\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eIn this second case, it assumes I’d like the shape of the dataframe, also a logical conclusion because the profiling dataframes is a very popular use-case for logs.\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003ccode\u003e\u003cspan\u003efrom\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003epandas\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eDataFrame\u003c/span\u003e\n\n\u003cspan\u003edata\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e[[\u003c/span\u003e\u003cspan\u003e\u0026#39;apples\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#39;2.5\u0026#39;\u003c/span\u003e\u003cspan\u003e],\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;oranges\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#39;3.0\u0026#39;\u003c/span\u003e\u003cspan\u003e],\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;bananas\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#39;4.0\u0026#39;\u003c/span\u003e\u003cspan\u003e]]\u003c/span\u003e\n\u003cspan\u003ecolumn_names\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;Fruit\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#39;Quantity\u0026#39;\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\n\u003cspan\u003edf\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eDataFrame\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edata\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ecolumns\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003ecolumn_names\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cfigure\u003e\u003cimg draggable=\"false\" src=\"https://vickiboykis.com/images/df_completion.png\" width=\"400\"/\u003e\u003cfigcaption\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe coolest part of this feature is that the inference model is entirely local to your machine.\u003c/p\u003e\n\u003cp\u003eThis enforces a few very important requirements on the development team, namely compression and speed.\u003c/p\u003e\n\u003cul\u003e\u003cli\u003e\u003cp\u003eThe model has to be small enough to bundle with the IDE for desktop memory footprints (already coming in at around ~1GB for the MacOS binary), which eliminates 99% of current LLMs\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAnd yet, the model has to be smart enough to interpolate lines of code from its small context window\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe local requirement eliminates any model inference engines like vLLM, SGLM, or Ray which implement KV cache optimization like \u003ca href=\"https://huggingface.co/docs/text-generation-inference/conceptual/paged_attention?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003ePagedAttention\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIt has to be a model that’s fast enough to produce \u003ca href=\"https://bentoml.com/llm/inference-optimization/llm-inference-metrics?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs#latency\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003eits first token\u003c/a\u003e (and all subsequent tokens) extremely quickly,\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFinally, it has to be optimized for Python specifically since this model is only available in PyCharm\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003cp\u003eThis is drastically different from the current assumptions around how we build and ship LLMs: that they need to be extremely large, general-purpose models served over proprietary APIs. we We find ourselves in a very constrained solution space because we no longer have to do all this other stuff that generalized LLMs have to do: write poetry, reason through math problems, act as OCR, offer code canvas templating, write marketing emails, and generate Studio Ghibli memes.\u003c/p\u003e\n\u003cp\u003eAll we have to do is train a model to complete a single line of code with a context of 384 characters! And then compress the crap out of that model so that it can fit on-device and perform inference.\u003c/p\u003e\n\u003cp\u003eSo how did they do it? Luckily, JetBrains \u003ca href=\"https://arxiv.org/abs/2405.08704v1?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003epublished a paper on this\u003c/a\u003e, and there are a bunch of interesting notes. The work is split into two parts, model training, and then the integration of the plugin itself.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg draggable=\"false\" src=\"https://vickiboykis.com/images/jetbrains_model.png\" width=\"400\"/\u003e\u003cfigcaption\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eThe \u003cstrong\u003emodel is trained\u003c/strong\u003e is done in PyTorch and then quantized.\u003c/p\u003e\n\u003cul\u003e\u003cli\u003e\u003cp\u003eFirst, they train a GPT-2 style Transformer \u003ca href=\"https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003edecoder-only\u003c/a\u003e model of 100 million parameters, including a tokenizer (aka autoregressive text completion like you’d get from Claude, OpenAI, Gemini, and friends these days). They later changed this architecture to Llama2 after the success of the growing llama.cpp and \u003ca href=\"https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003eGGUF community\u003c/a\u003e, as well as the better performance of the newer architecture.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe original dataset they used to train the model was a subset of \u003ca href=\"https://huggingface.co/datasets/bigcode/the-stack?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003eThe Stack\u003c/a\u003e, a code dataset across permissive licenses with 6TB of code in 30 programming languages\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eThe initial training set was “just” 45 GB and in preparing the data for training, in data cleaning, for space constraints, they remove all code comments in the training data specifically to focus on code generation\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThey do a neat trick for tokenizing Python (using a \u003ca href=\"https://huggingface.co/learn/llm-course/en/chapter6/5?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003eBPE-style tokenizer\u003c/a\u003e optimized for character pairs rather than bytes, since code is made up of smaller snippets and idioms than natural language text) which is indentation-sensitive, by converting spaces and tabs to start-end \u003ccode\u003e\u0026lt;SCOPE_IN\u0026gt;\u0026lt;SCOPE_OUT\u0026gt;\u003c/code\u003e tokens, to remove tokens that might be different only because they have different whitespacing. They ended up going with a tokenizer vocab size of 16,384.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThey do another very cool step in training which is to remove imports because they find that developers usually add imports in after writing the actual code, a fact that the model needs to anticipate\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThey then split into train/test for evaluation and trained for several days on 8 NVidia A100 GPU with a\u003ca href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003e cross-entropy loss objective function\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003cp\u003eBecause they were able to so clearly focus on the domain and understanding of how code inference works, focus on a single programming languages with its own nuances, they were able to make the training data set smaller, make the output more exact, and spend much less time and money training the model.\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003eactual plugin\u003c/strong\u003e that’s included in PyCharm “is implemented in Kotlin, however, it utilizes an additional native server that is run locally and is implemented in C++” for serving the inference tokens.\u003c/p\u003e\n\u003cp\u003eIn order to prepare the model for serving, they:\u003c/p\u003e\n\u003cul\u003e\u003cli\u003e\u003cp\u003eQuantized it from \u003ca href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003eFP32 to INT8\u003c/a\u003e which compressed the model from 400 MB to 100 MB\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003ePrepared as a served ONNX RT artifact, which allowed them to use CPU inference, which removed the CUDA overhead tax(later, they switched to using llama.cpp to serve the llama model architecture for the server.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFinally, in order to perform inference on a sequence of tokens, they use \u003ca href=\"https://d2l.ai/chapter_recurrent-modern/beam-search.html?utm_source=vicki\u0026amp;utm_medium=email\u0026amp;utm_campaign=my-favorite-use-case-for-ai-is-writing-logs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"\u003ebeam search. \u003c/a\u003eGenerally, Transformer-decoders are trained on predicting the next token in any given sequence so any individual step will give you a list of tokens along with their ranked probabilities (cementing my long-running theory that everything is a search problem).\u003c/p\u003e\u003cp\u003eSince this is computationally impossible at large numbers of tokens, a number of solutions exist to solve the problem of decoding optimally. Beam search creates a graph of all possible returned token sequences and expands at each node with the highest potential probability, limiting to \u003ccode\u003ek\u003c/code\u003e possible beams. In FLCC, the max number of beams, k, is 20, and they chose to limit generation to collect only those hypotheses that end with a newline character.\u003c/p\u003e\u003cp\u003eAdditionally they made use of a number of caching strategies, including initializing the model at 50% of total context - i.e. it starts by preloading ~192 characters of previous code, to give you space to either go back and edit old code, which now no longer has to be put into context, or to add new code, which is then added to the context. That way, if your cursor clicks on code you’ve already written, the model doesn’t need to re-infer.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003cp\u003eThere are a number of other very cool architecture and model decisions from the paper that are very worth reading and that show the level of care put into the input data, the modeling, and the inference architecture.\u003c/p\u003e\n\u003cp\u003eThe bottom line is that, for me as a user, this experience is extremely thoughtful. It has saved me countless times both in print log debugging and in the logs I ship to prod.\u003c/p\u003e\n\u003cp\u003eIn LLM land, there’s both a place for large, generalist models, and there’s a place for small models, and while much of the rest of the world writes about the former, I’m excited to also find more applications built with the latter.\u003c/p\u003e\n            \n            \n\n            \n\n            \n\n\n\n        \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-07-17T21:10:59.319629Z",
  "modifiedTime": null
}
