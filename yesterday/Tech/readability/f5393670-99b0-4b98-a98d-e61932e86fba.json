{
  "id": "f5393670-99b0-4b98-a98d-e61932e86fba",
  "title": "Google's Co-Founder Says AI Performs Best When You Threaten It",
  "link": "https://lifehacker.com/tech/googles-co-founder-says-ai-performs-best-when-you-threaten-it?utm_medium=RSS",
  "description": "What are we doing here?",
  "author": "Jake Peterson",
  "published": "Fri, 23 May 2025 20:52:22 +0000",
  "source": "https://lifehacker.com/rss",
  "categories": [
    "AI"
  ],
  "byline": "Jake Peterson",
  "length": 6592,
  "excerpt": "What are we doing here?",
  "siteName": "Lifehacker",
  "favicon": "https://lifehacker.com/images/android-chrome-384x384.png",
  "text": "Credit: The Hollywood Reporter via Getty Images Artificial intelligence continues to be the thing in tech—whether consumers are interested or not. What strikes me most about generative AI isn't its features or potential to make my life easier (a potential I have yet to realize); rather, I'm focused these days on the many threats that seem to be rising from this technology. There's misinformation, for sure—new AI video models, for example, are creating realistic clips complete with lip-synced audio. But there's also the classic AI threat, that the technology becomes both more intelligent than us and self-aware, and chooses to use that general intelligence in a way that does not benefit humanity. Even as he pours resources into his own AI company (not to mention the current administration, as well) Elon Musk sees a 10 to 20% chance that AI \"goes bad,\" and that the tech remains a “significant existential threat.\" Cool.So it doesn't necessarily bring me comfort to hear a high-profile, established tech executive jokingly discuss how treating AI poorly maximizes its potential. That would be Google co-founder Sergey Brin, who surprised an audience at a recording of the AIl-In podcast this week. During a talk that spanned Brin's return to Google, AI, and robotics, investor Jason Calacanis made a joke about getting \"sassy\" with the AI to get it to do the task he wanted. That sparked a legitimate point from Brin. It can be tough to tell exactly what he says at times due to people speaking over one another, but he says something to the effect of: \"You know, that's a weird thing...we don't circulate this much...in the AI community...not just our models, but all models tend to do better if you threaten them.\" The other speaker looks surprised. \"If you threaten them?\" Brin responds \"Like with physical violence. But...people feel weird about that, so we don't really talk about that.\" Brin then says that, historically, you threaten the model with kidnapping. You can see the exchange here: The conversation quickly shifts to other topics, including how kids are growing up with AI, but that comment is what I carried away from my viewing. What are we doing here? Have we lost the plot? Does no one remember Terminator?Jokes aside, it seems like a bad practice to start threatening AI models in order to get them to do something. Sure, maybe these programs never actually achieve artificial general intelligence (AGI), but I mean, I remember when the discussion was around whether we should say \"please\" and \"thank you\" when asking things of Alexa or Siri. Forget the niceties; just abuse ChatGPT until it does what you want it to—that should end well for everyone.Maybe AI does perform best when you threaten it. Maybe something in the training understands that \"threats\" mean the task should be taken more seriously. You won't catch me testing that hypothesis on my personal accounts. What do you think so far? Anthropic might offer an example of why not to torture your AIIn the same week as this podcast recording, Anthropic released its latest Claude AI models. One Anthropic employee took to Bluesky, and mentioned that Opus, the company's highest performing model, can take it upon itself to try to stop you from doing \"immoral\" things, by contacting regulators, the press, or locking you out of the system: welcome to the future, now your error-prone software can call the cops (this is an Anthropic employee talking about Claude Opus 4)[image or embed]— Molly White (@molly.wiki) May 22, 2025 at 4:55 PM The employee went on to clarify that this has only ever happened in \"clear-cut cases of wrongdoing,\" but that they could see the bot going rogue should it interpret how it's being used in a negative way. Check out the employee's particularly relevant example below: can't wait to explain to my family that the robot swatted me after i threatened its non-existent grandma[image or embed]— Molly White (@molly.wiki) May 22, 2025 at 5:09 PM That employee later deleted those posts and specified that this only happens during testing given unusual instructions and access to tools. Even if that is true, if it can happen in testing, it's entirely possible it can happen in a future version of the model. Speaking of testing, Anthropic researchers found that this new model of Claude is prone to deception and blackmail, should the bot believe it is being threatened or dislikes the way an interaction is going. Perhaps we should take torturing AI off the table? Lifehacker has been a go-to source of tech help and life advice since 2005. Our mission is to offer reliable tech help and credible, practical, science-based life advice to help you live better. Our Mission Our Team Newsletter © 2001-2025 Ziff Davis, LLC., A ZIFF DAVIS COMPANY. ALL RIGHTS RESERVED. Lifehacker is a federally registered trademark of Ziff Davis and may not be used by third parties without explicit permission. The display of third-party trademarks and trade names on this site does not necessarily indicate any affiliation or the endorsement of Lifehacker. If you click an affiliate link and buy a product or service, we may be paid a fee by that merchant.",
  "image": "https://lifehacker.com/imagery/articles/01JVZCHNK473ESPDXRYNXDDW6Y/hero-image.fill.size_1200x675.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\" data-seamless-scroll-item=\"\" data-status=\"loaded\" data-content-slug=\"googles-co-founder-says-ai-performs-best-when-you-threaten-it\" data-content-qualified-slug=\"/tech/googles-co-founder-says-ai-performs-best-when-you-threaten-it\" data-content-toc-items=\"[{\u0026#34;text\u0026#34;:\u0026#34;Table of Contents\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;toc\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;Anthropic might offer an example of why not to torture your AI\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;anthropic-might-offer-an-example-of-why-not-to-torture-your-ai\u0026#34;}]\" data-max-scrolled-percentage=\"0\" data-index=\"0\" data-ga-module=\"content-body\"\u003e\n    \n                    \u003cp\u003e\u003cimg src=\"https://lifehacker.com/imagery/articles/01JVZCHNK473ESPDXRYNXDDW6Y/hero-image.fill.size_1248x702.v1748033541.jpg\" alt=\"Sergey Brin at the tenth Breakthrough Prize ceremony held at the Academy Museum of Motion Pictures on April 13, 2024\" width=\"1248\" height=\"702\" srcset=\"https://lifehacker.com/imagery/articles/01JVZCHNK473ESPDXRYNXDDW6Y/hero-image.fill.size_400x225.v1748033541.jpg 400w, https://lifehacker.com/imagery/articles/01JVZCHNK473ESPDXRYNXDDW6Y/hero-image.fill.size_800x450.v1748033541.jpg 800w, https://lifehacker.com/imagery/articles/01JVZCHNK473ESPDXRYNXDDW6Y/hero-image.fill.size_1248x702.v1748033541.jpg 1600w\" sizes=\"(max-width: 1280px) 100vw, 1280px\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: The Hollywood Reporter via Getty Images\u003c/span\u003e\n                            \u003c/p\u003e\n            \n            \n    \n    \n    \u003carticle data-autopogo=\"\"\u003e\n        \u003chr/\u003e\n        \u003cp\u003eArtificial intelligence continues to be \u003cem\u003ethe\u003c/em\u003e thing in tech—whether consumers are interested or not. What strikes me most about generative AI isn\u0026#39;t its features or potential to make my life easier (a potential I have yet to realize); rather, I\u0026#39;m focused these days on the many threats that seem to be rising from this technology. \u003c/p\u003e\u003cp\u003eThere\u0026#39;s misinformation, for sure—new AI video models, for example, are \u003ca href=\"https://lifehacker.com/tech/you-are-not-prepared-for-this-new-wave-of-ai-generated-videos\" target=\"_blank\"\u003ecreating realistic clips complete with lip-synced audio\u003c/a\u003e. But there\u0026#39;s also the classic AI threat, that the technology becomes both more intelligent than us and self-aware, and chooses to use that general intelligence in a way that does \u003cem\u003enot\u003c/em\u003e benefit humanity. Even as he pours resources into his own AI company (not to mention the current administration, as well) Elon Musk sees a 10 to 20% chance that AI \u0026#34;\u003ca href=\"https://www.bloomberg.com/news/live-blog/2024-10-29/fii-event-saudi-arabia-live-goldman-blackrock-uae?sref=2jPYL79S\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003egoes bad\u003c/a\u003e,\u0026#34; and that the tech remains a “significant existential threat.\u0026#34; Cool.\u003c/p\u003e\u003cp\u003eSo it doesn\u0026#39;t necessarily bring me comfort to hear a high-profile, established tech executive jokingly discuss how treating AI poorly maximizes its potential. That would be Google co-founder Sergey Brin, who surprised an audience \u003ca href=\"https://www.youtube.com/watch?v=8g7a0IWKDRE\u0026amp;t=594s\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003eat a recording of the AIl-In podcast this week\u003c/a\u003e. During a talk that spanned Brin\u0026#39;s return to Google, AI, and robotics, investor Jason Calacanis made a joke about getting \u0026#34;sassy\u0026#34; with the AI to get it to do the task he wanted. That sparked a legitimate point from Brin. It can be tough to tell exactly what he says at times due to people speaking over one another, but he says something to the effect of: \u0026#34;You know, that\u0026#39;s a weird thing...we don\u0026#39;t circulate this much...in the AI community...not just our models, but all models tend to do better if you threaten them.\u0026#34; \u003c/p\u003e\u003cp\u003eThe other speaker looks surprised. \u0026#34;If you threaten them?\u0026#34; Brin responds \u0026#34;Like with physical violence. But...people feel weird about that, so we don\u0026#39;t really talk about that.\u0026#34; Brin then says that, historically, you threaten the model with kidnapping. You can see the exchange here:\u003c/p\u003e\n\n\n\u003cp\u003eThe conversation quickly shifts to other topics, including how kids are growing up with AI, but that comment is what I carried away from my viewing. What are we doing here? Have we lost the plot? Does no one remember \u003cem\u003eTerminator\u003c/em\u003e?\u003c/p\u003e\u003cp\u003eJokes aside, it seems like a bad practice to start threatening AI models in order to get them to do something. Sure, maybe these programs never actually achieve artificial general intelligence (AGI), but I mean, I remember when the discussion was around whether we should say \u003ca href=\"https://www.independent.co.uk/voices/technology-alexa-siri-chatgpt-artificial-intelligence-b2519067.html\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003e\u0026#34;please\u0026#34; and \u0026#34;thank you\u0026#34;\u003c/a\u003e when asking things of Alexa or Siri. Forget the niceties; just abuse ChatGPT until it does what you want it to—that should end well for everyone.\u003c/p\u003e\u003cp\u003eMaybe AI does perform best when you threaten it. Maybe something in the training understands that \u0026#34;threats\u0026#34; mean the task should be taken more seriously. You won\u0026#39;t catch me testing that hypothesis on my personal accounts.\u003c/p\u003e\u003cdiv data-ga-click=\"\" data-ga-template=\"News\" data-ga-module=\"openweb_widget\" data-ga-element=\"openweb_scroll\" data-ga-item=\"openweb_scroll_midpage\" x-cloak=\"\" x-data=\"{ commentsCount: null, hasComments: false }\" x-init=\"commentsCount = await window.openweb.getMessagesCount(01JVZCHNK473ESPDXRYNXDDW6Y);\n     hasComments = commentsCount !== null \u0026amp;\u0026amp; commentsCount \u0026gt; 0\"\u003e\n            \n\n            \u003cp\u003e\u003cspan\u003e\n                What do you think so far?\n                \n            \u003c/span\u003e\n        \u003c/p\u003e\u003c/div\u003e\n\u003ch2 id=\"anthropic-might-offer-an-example-of-why-not-to-torture-your-ai\"\u003eAnthropic might offer an example of why \u003cem\u003enot\u003c/em\u003e to torture your AI\u003c/h2\u003e\u003cp\u003eIn the same week as this podcast recording, \u003ca href=\"https://lifehacker.com/tech/claudes-new-ai-models-less-likely-to-deceive\" target=\"_blank\"\u003eAnthropic released its latest Claude AI models\u003c/a\u003e. One Anthropic employee took to Bluesky, and mentioned that Opus, the company\u0026#39;s highest performing model, can take it upon itself to try to stop you from doing \u0026#34;immoral\u0026#34; things, by contacting regulators, the press, or locking you out of the system:  \u003c/p\u003e\u003cdiv\u003e\n    \u003cblockquote data-bluesky-uri=\"at://did:plc:exrxvyu6bpoym6mbnctke5tn/app.bsky.feed.post/3lpryu7yd2s2m\" data-bluesky-cid=\"bafyreiaxot7pflsv6xkdwlyhwixfmy535nyl5szmbqthgtk5muccdhhd5e\" data-bluesky-embed-color-mode=\"system\"\u003e\u003cp\u003ewelcome to the future, now your error-prone software can call the cops\n\n(this is an Anthropic employee talking about Claude Opus 4)[image or embed]\u003c/p\u003e— Molly White (\u003ca href=\"https://bsky.app/profile/did:plc:exrxvyu6bpoym6mbnctke5tn?ref_src=embed\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003e@molly.wiki\u003c/a\u003e) \u003ca href=\"https://bsky.app/profile/did:plc:exrxvyu6bpoym6mbnctke5tn/post/3lpryu7yd2s2m?ref_src=embed\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003eMay 22, 2025 at 4:55 PM\u003c/a\u003e\u003c/blockquote\u003e\n\u003c/div\u003e\n\u003cp\u003eThe employee went on to clarify that this has only ever happened in \u0026#34;clear-cut cases of wrongdoing,\u0026#34; but that they could see the bot going rogue should it interpret how it\u0026#39;s being used in a negative way. Check out the employee\u0026#39;s particularly relevant example below:\u003c/p\u003e\u003cdiv\u003e\n    \u003cblockquote data-bluesky-uri=\"at://did:plc:exrxvyu6bpoym6mbnctke5tn/app.bsky.feed.post/3lprzmmukhk24\" data-bluesky-cid=\"bafyreibloxgzsr4u4mllufv7hldai4hu6er2gdmkhmjvpmoocwiunmdluq\" data-bluesky-embed-color-mode=\"system\"\u003e\u003cp\u003ecan\u0026#39;t wait to explain to my family that the robot swatted me after i threatened its non-existent grandma[image or embed]\u003c/p\u003e— Molly White (\u003ca href=\"https://bsky.app/profile/did:plc:exrxvyu6bpoym6mbnctke5tn?ref_src=embed\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003e@molly.wiki\u003c/a\u003e) \u003ca href=\"https://bsky.app/profile/did:plc:exrxvyu6bpoym6mbnctke5tn/post/3lprzmmukhk24?ref_src=embed\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003eMay 22, 2025 at 5:09 PM\u003c/a\u003e\u003c/blockquote\u003e\n\u003c/div\u003e\n\u003cp\u003eThat employee later \u003ca href=\"https://x.com/sleepinyourhat/status/1925626079043104830\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003edeleted those posts\u003c/a\u003e and specified that this only happens during testing given unusual instructions and access to tools. Even if that is true, if it can happen in testing, it\u0026#39;s entirely possible it can happen in a future version of the model. Speaking of testing, Anthropic researchers found that this new model of Claude \u003ca href=\"https://www.axios.com/2025/05/23/anthropic-ai-deception-risk\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003eis prone to deception and blackmail\u003c/a\u003e, should the bot believe it is being threatened or dislikes the way an interaction is going. \u003c/p\u003e\u003cp\u003ePerhaps we should take torturing AI off the table?\u003c/p\u003e\n            \u003c/article\u003e\n\n    \u003c/div\u003e\u003cdiv data-ga-module=\"footer-nav\" data-ga-action=\"footer-nav-link\"\u003e\n\n        \n        \u003cdiv role=\"region\" aria-label=\"Our Mission \u0026amp; Social Network\"\u003e\n                \u003cp\u003e\u003ca data-ga-click=\"\" data-ga-element=\"nav-logo\" data-ga-action=\"nav-logo\" data-ga-item=\"logo\" href=\"https://lifehacker.com\"\u003e\n                    \u003cimg src=\"https://lifehacker.com/images/lifehacker-logo.svg\" alt=\"Lifehacker Logo\" loading=\"lazy\" width=\"256\" height=\"71\"/\u003e\n                \u003c/a\u003e\u003c/p\u003e\u003cp\u003eLifehacker has been a go-to source of tech help and life advice since 2005. Our mission is to offer reliable tech help and credible, practical, science-based life advice to help you live better.\n                \u003c/p\u003e\n                \u003cdiv data-ga-element=\"social-link\" data-ga-action=\"social-link\" data-ga-item=\"title\"\u003e\n                    \u003cul\u003e\n                        \u003cli\u003e\u003ca data-ga-click=\"\" data-ga-label=\"$text\" href=\"https://lifehacker.com/our-mission\" aria-label=\"Our Mission\"\u003eOur Mission\u003c/a\u003e\u003c/li\u003e\n                        \u003cli\u003e\u003ca data-ga-click=\"\" data-ga-label=\"$text\" href=\"https://lifehacker.com/our-team\" aria-label=\"Our Team\"\u003eOur Team\u003c/a\u003e\u003c/li\u003e\n                        \u003cli\u003e\u003ca data-ga-click=\"\" data-ga-label=\"$text\" href=\"https://lifehacker.com/newsletters\" aria-label=\"Newsletter\"\u003eNewsletter\u003c/a\u003e\u003c/li\u003e\n                    \u003c/ul\u003e\n                    \u003cul\u003e\n                        \u003cli\u003e\n                            \u003ca data-ga-click=\"\" data-ga-label=\"facebook\" href=\"https://facebook.com/lifehacker\" title=\"(opens in a new window)\" rel=\"noopener\" target=\"_blank\"\u003e\n                                \u003csvg\u003e\n                                    \u003cuse href=\"/images/icons/spritemap.svg#sprite-social-facebook\"\u003e\u003c/use\u003e\n                                \u003c/svg\u003e\n                            \u003c/a\u003e\n                        \u003c/li\u003e\n                        \u003cli\u003e\n                            \u003ca data-ga-click=\"\" data-ga-label=\"twitter\" href=\"https://twitter.com/lifehacker\" title=\"(opens in a new window)\" rel=\"noopener\" target=\"_blank\"\u003e\n                                \u003csvg\u003e\n                                    \u003cuse href=\"/images/icons/spritemap.svg#sprite-social-twitter\"\u003e\u003c/use\u003e\n                                \u003c/svg\u003e\n                            \u003c/a\u003e\n                        \u003c/li\u003e\n                        \u003cli\u003e\n                            \u003ca data-ga-click=\"\" data-ga-label=\"instagram\" href=\"https://instagram.com/lifehackerdotcom\" title=\"(opens in a new window)\" rel=\"noopener\" target=\"_blank\"\u003e\n                                \u003csvg\u003e\n                                    \u003cuse href=\"/images/icons/spritemap.svg#sprite-social-instagram\"\u003e\u003c/use\u003e\n                                \u003c/svg\u003e\n                            \u003c/a\u003e\n                        \u003c/li\u003e\n                        \u003cli\u003e\n                            \u003ca data-ga-click=\"\" data-ga-label=\"youtube\" href=\"https://www.youtube.com/user/lifehacker\" title=\"(opens in a new window)\" rel=\"noopener\" target=\"_blank\"\u003e\n                                \u003csvg\u003e\n                                    \u003cuse href=\"/images/icons/spritemap.svg#sprite-social-youtube\"\u003e\u003c/use\u003e\n                                \u003c/svg\u003e\n                            \u003c/a\u003e\n                        \u003c/li\u003e\n                    \u003c/ul\u003e\n                \u003c/div\u003e\n            \u003c/div\u003e\n\n        \n        \u003cdiv data-ga-element=\"zd-copyright\" data-ga-action=\"zd-copyright\" data-ga-item=\"title\" role=\"region\" aria-label=\"Ziff Davis Copyright\"\u003e\n                \u003cp\u003e© 2001-2025 Ziff Davis, LLC., A ZIFF DAVIS COMPANY. \u003cspan\u003eALL RIGHTS RESERVED.\u003c/span\u003e\u003c/p\u003e\n                \u003cp\u003eLifehacker is a federally registered trademark of Ziff Davis and may not be used by third parties without explicit permission. The display of third-party trademarks and trade names on this site does not necessarily indicate\n                    any affiliation or the\n                    endorsement of Lifehacker. If you click an affiliate link and buy a product or service, we may be paid a fee \u003cspan\u003eby that merchant.\u003c/span\u003e\u003c/p\u003e\n            \u003c/div\u003e\n\n        \n        \n    \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-05-23T20:52:22Z",
  "modifiedTime": "2025-05-23T20:52:31Z"
}
