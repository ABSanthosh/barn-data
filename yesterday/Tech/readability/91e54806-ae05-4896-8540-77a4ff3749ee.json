{
  "id": "91e54806-ae05-4896-8540-77a4ff3749ee",
  "title": "Normalizing Flows Are Capable Generative Models",
  "link": "https://machinelearning.apple.com/research/normalizing-flows",
  "description": "Comments",
  "author": "",
  "published": "Fri, 27 Jun 2025 20:50:10 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 2484,
  "excerpt": "Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density…",
  "siteName": "Apple Machine Learning Research",
  "favicon": "",
  "text": "AuthorsShuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, Josh SusskindNormalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We also propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. Putting these together, TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model. Figure 1: Samples at various resolutions generated by TarFlow. Figure 2: Model architecture of TarFlow.Related readings and updates.We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building…Read moreAutoregressive models for text sometimes generate repetitive and low-quality output because errors accumulate during the steps of generation. This issue is often attributed to exposure bias - the difference between how a model is trained and how it is used during inference. Denoising diffusion models provide an alternative approach in which a model can revisit and revise its output. However, they can be computationally expensive, and prior…Read more",
  "image": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv role=\"main\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eAuthors\u003c/span\u003eShuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, Josh Susskind\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp\u003eNormalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We also propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. Putting these together, TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://mlr.cdn-apple.com/media/guided_samples_007a300190.jpeg\" tabindex=\"-1\" target=\"_blank\"\u003e\u003cimg src=\"https://mlr.cdn-apple.com/media/guided_samples_007a300190.jpeg\" loading=\"lazy\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFigure 1: Samples at various resolutions generated by TarFlow.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://mlr.cdn-apple.com/media/modelv3_converted_c2dbc744fc.png\" tabindex=\"-1\" target=\"_blank\"\u003e\u003cimg src=\"https://mlr.cdn-apple.com/media/modelv3_converted_c2dbc744fc.png\" loading=\"lazy\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFigure 2: Model architecture of TarFlow.\u003c/p\u003e\u003c/div\u003e\u003csection\u003e\u003cp\u003e\u003ch2\u003eRelated readings and updates.\u003c/h2\u003e\u003c/p\u003e\u003cdiv\u003e\u003cdiv data-testid=\"card-starflow\"\u003e\u003cp\u003eWe present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building…\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://machinelearning.apple.com/research/starflow\" aria-label=\"Read more about STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis\"\u003eRead more\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"card-planner\"\u003e\u003cp\u003eAutoregressive models for text sometimes generate repetitive and low-quality output because errors accumulate during the steps of generation. This issue is often attributed to exposure bias - the difference between how a model is trained and how it is used during inference. Denoising diffusion models provide an alternative approach in which a model can revisit and revise its output. However, they can be computationally expensive, and prior…\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://machinelearning.apple.com/research/planner\" aria-label=\"Read more about PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model\"\u003eRead more\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": null,
  "modifiedTime": null
}
