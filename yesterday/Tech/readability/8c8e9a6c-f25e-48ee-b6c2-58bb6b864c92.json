{
  "id": "8c8e9a6c-f25e-48ee-b6c2-58bb6b864c92",
  "title": "DolphinGemma: How Google AI is helping decode dolphin communication",
  "link": "https://blog.google/technology/ai/dolphingemma/",
  "description": "Dolphin researchers are using Gemma and Google Pixel phones to try to decipher how dolphins talk to one another.",
  "author": "Dr. Thad StarnerGoogle DeepMind Research Scientist and Georgia Tech Professor",
  "published": "Mon, 14 Apr 2025 13:00:00 +0000",
  "source": "https://www.blog.google/rss/",
  "categories": [
    "AI"
  ],
  "byline": "Dr. Denise Herzing",
  "length": 8982,
  "excerpt": "Dolphin researchers are using Gemma and Google Pixel phones to try to decipher how dolphins talk to one another.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png?version=pr20250401-1731",
  "text": "Apr 14, 2025 [[read-time]] min read DolphinGemma, a large language model developed by Google, is helping scientists study how dolphins communicate — and hopefully find out what they're saying, too. Dr. Thad Starner Google DeepMind Research Scientist and Georgia Tech Professor For decades, understanding the clicks, whistles and burst pulses of dolphins has been a scientific frontier. What if we could not only listen to dolphins, but also understand the patterns of their complex communication well enough to generate realistic responses?Today, on National Dolphin Day, Google, in collaboration with researchers at Georgia Tech and the field research of the Wild Dolphin Project (WDP), is announcing progress on DolphinGemma: a foundational AI model trained to learn the structure of dolphin vocalizations and generate novel dolphin-like sound sequences. This approach in the quest for interspecies communication pushes the boundaries of AI and our potential connection with the marine world. Researching dolphin society for decadesUnderstanding any species requires deep context, and that's one of the many things the WDP provides. Since 1985, WDP has conducted the world's longest-running underwater dolphin research project, studying a specific community of wild Atlantic spotted dolphins (Stenella frontalis) in the Bahamas across generations. This non-invasive, \"In Their World, on Their Terms\" approach yields a rich, unique dataset: decades of underwater video and audio meticulously paired with individual dolphin identities, life histories and observed behaviors. A pod of Atlantic spotted dolphins, Stenella frontalis A primary focus for WDP is observing and analyzing the dolphins' natural communication and social interactions. Working underwater allows researchers to directly link sounds to specific behaviors in ways surface observation cannot. For decades, they have correlated sound types with behavioral contexts. Here are some examples:Signature whistles (unique names) that can be used by mothers and calves to reuniteBurst-pulse \"squawks\" often seen during fightsClick \"buzzes\" often used during courtship or chasing sharksKnowing the individual dolphins involved is crucial for accurate interpretation. The ultimate goal of this observational work is to understand the structure and potential meaning within these natural sound sequences — seeking patterns and rules that might indicate language. This long-term analysis of natural communication forms the bedrock of WDP's research and provides essential context for any AI analysis. Left: A mother spotted dolphin observes her calf while foraging. She will use her unique signature whistle to call the calf back after he is finished. Right: Spectrogram to visualize the whistle. Introducing DolphinGemmaAnalyzing dolphins' natural, complex communication is a monumental task, and WDP's vast, labeled dataset provides a unique opportunity for cutting-edge AI.Enter DolphinGemma. Developed by Google, this AI model makes use of specific Google audio technologies: the SoundStream tokenizer efficiently represents dolphin sounds, which are then processed by a model architecture suited for complex sequences. This ~400M parameter model is optimally-sized to run directly on the Pixel phones WDP uses in the field. Left: Whistles (left) and burst pulses (right) generated during early testing of DolphinGemma. This model builds upon insights from Gemma, Google’s collection of lightweight, state-of-the-art open models that are built from the same research and technology that powers our Gemini models. Trained extensively on WDP’s acoustic database of wild Atlantic spotted dolphins, DolphinGemma functions as an audio-in, audio-out model, processes sequences of natural dolphin sounds to identify patterns, structure and ultimately predict the likely subsequent sounds in a sequence, much like how large language models for human language predict the next word or token in a sentence.WDP is beginning to deploy DolphinGemma this field season with immediate potential benefits. By identifying recurring sound patterns, clusters and reliable sequences, the model can help researchers uncover hidden structures and potential meanings within the dolphins' natural communication — a task previously requiring immense human effort. Eventually, these patterns, augmented with synthetic sounds created by the researchers to refer to objects with which the dolphins like to play, may establish a shared vocabulary with the dolphins for interactive communication. Using Pixel phones to listen to and analyze dolphin soundsIn addition to analyzing natural communication, WDP is also pursuing a distinct, parallel path: exploring potential two-way interaction using technology in the ocean. This effort led to the development of the CHAT (Cetacean Hearing Augmentation Telemetry) system, in partnership with the Georgia Institute of Technology. CHAT is an underwater computer designed not to directly decipher the dolphins' complex natural language, but to establish a simpler, shared vocabulary.The concept first relies on associating novel, synthetic whistles (created by CHAT, distinct from natural dolphin sounds) with specific objects the dolphins enjoy, like sargassum, seagrass or scarves the researchers use. By demonstrating the system between humans, researchers hope the naturally curious dolphins will learn to mimic the whistles to request these items. Eventually, as more of the dolphins’ natural sounds are understood, they can also be added to the system. To enable two-way interaction, the CHAT system first needs to:Hear the mimic accurately amid ocean noise.Identify which whistle was mimicked in real-time.Inform the researcher (via bone-conducting headphones that work underwater) which object the dolphin \"requested.\"Enable the researcher to respond quickly by offering the correct object, reinforcing the connection.A Google Pixel 6 handled the high-fidelity analysis of dolphin sounds in real time. The upcoming generation, centered around a Google Pixel 9 (research slated for summer 2025), builds on this effort by integrating speaker/microphone functions and using the phone's advanced processing to run both deep learning models and template matching algorithms simultaneously. Left: Dr. Denise Herzing wearing “Chat Senior, 2012”, Right: Georgia Tech PhD Student Charles Ramey wearing “Chat Junior, 2025” Using Pixel smartphones dramatically reduces the need for custom hardware, improves system maintainability, lowers power consumption and shrinks the device's cost and size — crucial advantages for field research in the open ocean. Meanwhile, DolphinGemma’s predictive power can help CHAT anticipate and identify potential mimics earlier in the vocalization sequence, increasing the speed at which researchers can react to the dolphins and making interactions more fluid and reinforcing. A Google Pixel 9 inside the latest CHAT system hardware. Sharing DolphinGemma with the research communityRecognizing the value of collaboration in scientific discovery, we’re planning to share DolphinGemma as an open model this summer. While trained on Atlantic spotted dolphin sounds, we anticipate its potential utility for researchers studying other cetacean species, like bottlenose or spinner dolphins. Fine-tuning may be required for different species' vocalizations, and the open nature of the model facilitates this adaptation.By providing tools like DolphinGemma, we hope to give researchers worldwide the tools to mine their own acoustic datasets, accelerate the search for patterns and collectively deepen our understanding of these intelligent marine mammals.The journey to understanding dolphin communication is long, but the combination of dedicated field research by WDP, engineering expertise from Georgia Tech and the power of Google's technology is opening exciting new possibilities. We're not just listening anymore. We're beginning to understand the patterns within the sounds, paving the way for a future where the gap between human and dolphin communication might just get a little smaller.You can learn more about the Wild Dolphin Project on their website.",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_SocialExplainers_16x9_DolphinGem.width-1300.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\n\n    \n    \n\n\n\n\n\n    \n\n    \n      \n\n\u003cdiv data-analytics-module=\"{\n    \u0026#34;module_name\u0026#34;: \u0026#34;Hero Menu\u0026#34;,\n    \u0026#34;section_header\u0026#34;: \u0026#34;DolphinGemma: How Google AI is helping decode dolphin communication\u0026#34;\n  }\"\u003e\n  \n  \u003cdiv\u003e\n      \u003cdiv\u003e\n          \n            \u003cp\u003eApr 14, 2025\u003c/p\u003e\n          \n          \n            \u003cp data-reading-time-render=\"\"\u003e[[read-time]] min read\u003c/p\u003e\n          \n        \u003c/div\u003e\n      \n        \u003cp\u003e\n          DolphinGemma, a large language model developed by Google, is helping scientists study how dolphins communicate — and hopefully find out what they\u0026#39;re saying, too.\n        \u003c/p\u003e\n      \n    \u003c/div\u003e\n  \n  \u003cdiv\u003e\n      \n  \n    \u003cfigure\u003e\n        \u003cpicture\u003e\n            \n\n\n    \n\n    \n        \u003csource media=\"(max-resolution: 1.5dppx)\" sizes=\"122px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1517757973355.max-122x92.format-webp.webp 122w\"/\u003e\n    \n        \u003csource media=\"(min-resolution: 1.5dppx)\" sizes=\"244px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1517757973355.max-244x184.format-webp.webp 244w\"/\u003e\n    \n\n    \u003cimg src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1517757973355.max-244x184.format-webp.webp\" alt=\"thad headshot\" sizes=\" 122px,  244px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1517757973355.max-122x92.format-webp.webp 122w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1517757973355.max-244x184.format-webp.webp 244w\" data-target=\"image\" loading=\"lazy\"/\u003e\n    \n\n\n        \u003c/picture\u003e\n    \u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\n  \u003cp\u003eDr. Thad Starner\u003c/p\u003e\n  \n    \u003cp\u003e\n      Google DeepMind Research Scientist and Georgia Tech Professor\n    \u003c/p\u003e\n  \n  \n\u003c/div\u003e\n\n    \u003c/div\u003e\n\u003c/div\u003e\n\n    \n\n    \n      \n\n\n  \u003cuni-youtube-player-hero index=\"0\" thumbnail-alt=\"DolphinGemma text over a picture of dolphins\" component-title=\"DolphinGemma: How Google AI is helping decode dolphin communication\" video-id=\"T8GdEVVvXyE\" video-type=\"video\" image=\"DolphinGemma_SocialExplainers_16x9_DolphinGemma\" video-image-url-lazy=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_SocialExplainers_16x.width-100.format-webp.webp\" video-image-url-mobile=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_SocialExplainers_16x.width-700.format-webp.webp\" video-image-url-desktop=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_SocialExplainers_16.width-1000.format-webp.webp\"\u003e\n  \u003c/uni-youtube-player-hero\u003e\n\n\n    \n\n    \n    \u003cdiv data-reading-time=\"true\" data-component=\"uni-article-body\"\u003e\n\n            \n              \n\n\n\n\n\n\u003cuni-article-speakable page-title=\"DolphinGemma: How Google AI is helping decode dolphin communication\" listen-to-article=\"Listen to article\" data-date-modified=\"2025-04-14T17:08:29.525540+00:00\" data-tracking-ids=\"G-HGNBTNCHCQ,G-6NKTLKV14N\" data-voice-list=\"en.ioh-pngnat:Cyan,en.usb-pngnat:Lime\" data-script-src=\"https://www.gstatic.com/readaloud/player/web/api/js/api.js\"\u003e\u003c/uni-article-speakable\u003e\n\n            \n\n            \n            \n\n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;DolphinGemma: How Google AI is helping decode dolphin communication\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"6q2c0\"\u003eFor decades, understanding the clicks, whistles and burst pulses of dolphins has been a scientific frontier. What if we could not only listen to dolphins, but also understand the patterns of their complex communication well enough to generate realistic responses?\u003c/p\u003e\u003cp data-block-key=\"4o0o1\"\u003eToday, on National Dolphin Day, Google, in collaboration with researchers at Georgia Tech and the field research of the \u003ca href=\"https://www.wilddolphinproject.org/\"\u003eWild Dolphin Project\u003c/a\u003e (WDP), is announcing progress on DolphinGemma: a foundational AI model trained to learn the structure of dolphin vocalizations and generate novel dolphin-like sound sequences. This approach in the quest for interspecies communication pushes the boundaries of AI and our potential connection with the marine world.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;DolphinGemma: How Google AI is helping decode dolphin communication\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"6q2c0\"\u003eResearching dolphin society for decades\u003c/h2\u003e\u003cp data-block-key=\"ae8k9\"\u003eUnderstanding any species requires deep context, and that\u0026#39;s one of the many things the WDP provides. Since 1985, WDP has conducted the world\u0026#39;s longest-running underwater dolphin research project, studying a specific community of wild Atlantic spotted dolphins (Stenella frontalis) in the Bahamas across generations. This non-invasive, \u0026#34;In Their World, on Their Terms\u0026#34; approach yields a rich, unique dataset: decades of underwater video and audio meticulously paired with individual dolphin identities, life histories and observed behaviors.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Dolphins swimming in the water\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"DolphinGemma: How Google AI is helping decode dolphin communication\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"27106\"\u003eA pod of Atlantic spotted dolphins, Stenella frontalis\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Dolphins swimming in the water\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/dolphins.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/dolphins.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/dolphins.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;DolphinGemma: How Google AI is helping decode dolphin communication\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"iocbw\"\u003eA primary focus for WDP is observing and analyzing the dolphins\u0026#39; natural communication and social interactions. Working underwater allows researchers to directly link sounds to specific behaviors in ways surface observation cannot. For decades, they have correlated sound types with behavioral contexts. Here are some examples:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"fu0nf\"\u003eSignature whistles (unique names) that can be used by mothers and calves to reunite\u003c/li\u003e\u003cli data-block-key=\"b63q5\"\u003eBurst-pulse \u0026#34;squawks\u0026#34; often seen during fights\u003c/li\u003e\u003cli data-block-key=\"bseip\"\u003eClick \u0026#34;buzzes\u0026#34; often used during courtship or chasing sharks\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"2ar36\"\u003eKnowing the individual dolphins involved is crucial for accurate interpretation. The ultimate goal of this observational work is to understand the structure and potential meaning within these natural sound sequences — seeking patterns and rules that might indicate language. This long-term analysis of natural communication forms the bedrock of WDP\u0026#39;s research and provides essential context for any AI analysis.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"A split image: left, a dolphin touching the sandy seabed underwater; right, a spectrogram with bright vertical streaks indicating high-frequency sounds.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"DolphinGemma: How Google AI is helping decode dolphin communication\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"s704z\"\u003eLeft: A mother spotted dolphin observes her calf while foraging. She will use her unique signature whistle to call the calf back after he is finished. Right: Spectrogram to visualize the whistle.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"A split image: left, a dolphin touching the sandy seabed underwater; right, a spectrogram with bright vertical streaks indicating high-frequency sounds.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword1_RD3_V02.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword1_RD3_V02.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword1_RD3_V02.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;DolphinGemma: How Google AI is helping decode dolphin communication\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"iocbw\"\u003eIntroducing DolphinGemma\u003c/h2\u003e\u003cp data-block-key=\"2m8ks\"\u003eAnalyzing dolphins\u0026#39; natural, complex communication is a monumental task, and WDP\u0026#39;s vast, labeled dataset provides a unique opportunity for cutting-edge AI.\u003c/p\u003e\u003cp data-block-key=\"bgerj\"\u003eEnter DolphinGemma. Developed by Google, this AI model makes use of specific Google audio technologies: the SoundStream tokenizer efficiently represents dolphin sounds, which are then processed by a model architecture suited for complex sequences. This ~400M parameter model is optimally-sized to run directly on the Pixel phones WDP uses in the field.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Two spectrograms: left shows three arching sound patterns; right shows a more uniform sound pattern.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"DolphinGemma: How Google AI is helping decode dolphin communication\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"30sb2\"\u003eLeft: Whistles (left) and burst pulses (right) generated during early testing of DolphinGemma.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Two spectrograms: left shows three arching sound patterns; right shows a more uniform sound pattern.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword2_RD3_V01.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword2_RD3_V01.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword2_RD3_V01.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;DolphinGemma: How Google AI is helping decode dolphin communication\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"3p158\"\u003eThis model builds upon insights from \u003ca href=\"https://ai.google.dev/gemma\"\u003eGemma\u003c/a\u003e, Google’s collection of lightweight, state-of-the-art open models that are built from the same research and technology that powers our Gemini models. Trained extensively on WDP’s acoustic database of wild Atlantic spotted dolphins, DolphinGemma functions as an audio-in, audio-out model, processes sequences of natural dolphin sounds to identify patterns, structure and ultimately predict the likely subsequent sounds in a sequence, much like how large language models for human language predict the next word or token in a sentence.\u003c/p\u003e\u003cp data-block-key=\"a754e\"\u003eWDP is beginning to deploy DolphinGemma this field season with immediate potential benefits. By identifying recurring sound patterns, clusters and reliable sequences, the model can help researchers uncover hidden structures and potential meanings within the dolphins\u0026#39; natural communication — a task previously requiring immense human effort. Eventually, these patterns, augmented with synthetic sounds created by the researchers to refer to objects with which the dolphins like to play, may establish a shared vocabulary with the dolphins for interactive communication.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;DolphinGemma: How Google AI is helping decode dolphin communication\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"3p158\"\u003eUsing Pixel phones to listen to and analyze dolphin sounds\u003c/h2\u003e\u003cp data-block-key=\"7vd3s\"\u003eIn addition to analyzing natural communication, WDP is also pursuing a distinct, parallel path: exploring potential two-way interaction using technology in the ocean. This effort led to the development of the \u003ca href=\"https://www.wilddolphinproject.org/our-research/chat-research/\"\u003eCHAT\u003c/a\u003e (Cetacean Hearing Augmentation Telemetry) system, in partnership with the Georgia Institute of Technology. CHAT is an underwater computer designed not to directly decipher the dolphins\u0026#39; complex natural language, but to establish a simpler, shared vocabulary.\u003c/p\u003e\u003cp data-block-key=\"6avcn\"\u003eThe concept first relies on associating novel, synthetic whistles (created by CHAT, distinct from natural dolphin sounds) with specific objects the dolphins enjoy, like sargassum, seagrass or scarves the researchers use. By demonstrating the system between humans, researchers hope the naturally curious dolphins will learn to mimic the whistles to request these items. Eventually, as more of the dolphins’ natural sounds are understood, they can also be added to the system.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"10\" thumbnail-alt=\"CHAT explainer video\" video-id=\"YhopeQKbpZA\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;DolphinGemma: How Google AI is helping decode dolphin communication\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"3p158\"\u003eTo enable two-way interaction, the CHAT system first needs to:\u003c/p\u003e\u003col\u003e\u003cli data-block-key=\"9is9o\"\u003eHear the mimic accurately amid ocean noise.\u003c/li\u003e\u003cli data-block-key=\"5qsu4\"\u003eIdentify which whistle was mimicked in real-time.\u003c/li\u003e\u003cli data-block-key=\"48nl7\"\u003eInform the researcher (via bone-conducting headphones that work underwater) which object the dolphin \u0026#34;requested.\u0026#34;\u003c/li\u003e\u003cli data-block-key=\"2f5o\"\u003eEnable the researcher to respond quickly by offering the correct object, reinforcing the connection.\u003c/li\u003e\u003c/ol\u003e\u003cp data-block-key=\"c9b4\"\u003eA Google Pixel 6 handled the high-fidelity analysis of dolphin sounds in real time. The upcoming generation, centered around a Google Pixel 9 (research slated for summer 2025), builds on this effort by integrating speaker/microphone functions and using the phone\u0026#39;s advanced processing to run both deep learning models and template matching algorithms simultaneously.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Two portraits: left, a woman on a boat holding a device; right, a man indoors wearing headphones and holding a similar device.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"DolphinGemma: How Google AI is helping decode dolphin communication\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"pdphj\"\u003eLeft: Dr. Denise Herzing wearing “Chat Senior, 2012”, Right: Georgia Tech PhD Student Charles Ramey wearing “Chat Junior, 2025”\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Two portraits: left, a woman on a boat holding a device; right, a man indoors wearing headphones and holding a similar device.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Researchers_RD2_V01.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Researchers_RD2_V01.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Researchers_RD2_V01.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;DolphinGemma: How Google AI is helping decode dolphin communication\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"su54v\"\u003eUsing Pixel smartphones dramatically reduces the need for custom hardware, improves system maintainability, lowers power consumption and shrinks the device\u0026#39;s cost and size — crucial advantages for field research in the open ocean. Meanwhile, DolphinGemma’s predictive power can help CHAT anticipate and identify potential mimics earlier in the vocalization sequence, increasing the speed at which researchers can react to the dolphins and making interactions more fluid and reinforcing.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Pixel phone inside a case hooked up to cables\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"DolphinGemma: How Google AI is helping decode dolphin communication\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"qfo9j\"\u003eA Google Pixel 9 inside the latest CHAT system hardware.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Pixel phone inside a case hooked up to cables\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CHAT_Pixel.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CHAT_Pixel.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CHAT_Pixel.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;DolphinGemma: How Google AI is helping decode dolphin communication\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"su54v\"\u003eSharing DolphinGemma with the research community\u003c/h2\u003e\u003cp data-block-key=\"2ad01\"\u003eRecognizing the value of collaboration in scientific discovery, we’re planning to share DolphinGemma as an open model this summer. While trained on Atlantic spotted dolphin sounds, we anticipate its potential utility for researchers studying other cetacean species, like bottlenose or spinner dolphins. Fine-tuning may be required for different species\u0026#39; vocalizations, and the open nature of the model facilitates this adaptation.\u003c/p\u003e\u003cp data-block-key=\"40nps\"\u003eBy providing tools like DolphinGemma, we hope to give researchers worldwide the tools to mine their own acoustic datasets, accelerate the search for patterns and collectively deepen our understanding of these intelligent marine mammals.\u003c/p\u003e\u003cp data-block-key=\"e2jq7\"\u003eThe journey to understanding dolphin communication is long, but the combination of dedicated field research by WDP, engineering expertise from Georgia Tech and the power of Google\u0026#39;s technology is opening exciting new possibilities. We\u0026#39;re not just listening anymore. We\u0026#39;re beginning to understand the patterns within the sounds, paving the way for a future where the gap between human and dolphin communication might just get a little smaller.\u003c/p\u003e\u003cp data-block-key=\"esrl0\"\u003eYou can learn more about the\u003ca href=\"https://www.wilddolphinproject.org/\"\u003e Wild Dolphin Project\u003c/a\u003e on their website.\u003c/p\u003e\u003c/div\u003e\n  \n\n\n            \n            \n\n            \n              \n\n\n\n\n            \n          \u003c/div\u003e\n  \u003c/article\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-04-14T13:00:00Z",
  "modifiedTime": null
}
