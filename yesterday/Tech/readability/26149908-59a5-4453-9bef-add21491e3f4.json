{
  "id": "26149908-59a5-4453-9bef-add21491e3f4",
  "title": "Anthropic releases custom AI chatbot for classified spy work",
  "link": "https://arstechnica.com/ai/2025/06/anthropic-releases-custom-ai-chatbot-for-classified-spy-work/",
  "description": "\"Claude Gov\" is already handling classified information for the US government.",
  "author": "Benj Edwards",
  "published": "Fri, 06 Jun 2025 21:12:17 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "AI assistants",
    "AI confabulation",
    "AI ethics",
    "AI regulation",
    "Anthropic",
    "Artificial Intelligence",
    "ChatGPT",
    "chatgtp",
    "Claude",
    "GPT-4",
    "large language models",
    "machine learning",
    "microsoft",
    "national security",
    "openai"
  ],
  "byline": "Benj Edwards",
  "length": 1573,
  "excerpt": "“Claude Gov” is already handling classified information for the US government.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "On Thursday, Anthropic unveiled specialized AI models designed for US national security customers. The company released \"Claude Gov\" models that were built in response to direct feedback from government clients to handle operations such as strategic planning, intelligence analysis, and operational support. The custom models reportedly already serve US national security agencies, with access restricted to those working in classified environments. The Claude Gov models differ from Anthropic's consumer and enterprise offerings, also called Claude, in several ways. They reportedly handle classified material, \"refuse less\" when engaging with classified information, and are customized to handle intelligence and defense documents. The models also feature what Anthropic calls \"enhanced proficiency\" in languages and dialects critical to national security operations. Anthropic says the new models underwent the same \"safety testing\" as all Claude models. The company has been pursuing government contracts as it seeks reliable revenue sources, partnering with Palantir and Amazon Web Services in November to sell AI tools to defense customers. Anthropic is not the first company to offer specialized chatbot services for intelligence agencies. In 2024, Microsoft launched an isolated version of OpenAI's GPT-4 for the US intelligence community after 18 months of work. That system, which operated on a special government-only network without Internet access, became available to about 10,000 individuals in the intelligence community for testing and answering questions.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2025/06/claude_gov.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                      \n                      \n          \u003cp\u003eOn Thursday, Anthropic \u003ca href=\"https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers\"\u003eunveiled\u003c/a\u003e specialized AI models designed for US national security customers. The company released \u0026#34;Claude Gov\u0026#34; models that were built in response to direct feedback from government clients to handle operations such as strategic planning, intelligence analysis, and operational support. The custom models reportedly already serve US national security agencies, with access restricted to those working in classified environments.\u003c/p\u003e\n\u003cp\u003eThe Claude Gov models differ from Anthropic\u0026#39;s consumer and enterprise offerings, also called \u003ca href=\"https://arstechnica.com/ai/2025/05/anthropic-calls-new-claude-4-worlds-best-ai-coding-model/\"\u003eClaude\u003c/a\u003e, in several ways. They reportedly handle classified material, \u0026#34;refuse less\u0026#34; when engaging with classified information, and are customized to handle intelligence and defense documents. The models also feature what Anthropic calls \u0026#34;enhanced proficiency\u0026#34; in languages and dialects critical to national security operations.\u003c/p\u003e\n\u003cp\u003eAnthropic says the new models underwent the same \u0026#34;safety testing\u0026#34; as all Claude models. The company has been pursuing government contracts as it seeks reliable revenue sources, \u003ca href=\"https://arstechnica.com/ai/2024/11/safe-ai-champ-anthropic-teams-up-with-defense-giant-palantir-in-new-deal/\"\u003epartnering with Palantir\u003c/a\u003e and Amazon Web Services in November to sell AI tools to defense customers.\u003c/p\u003e\n\u003cp\u003eAnthropic is not the first company to offer specialized chatbot services for intelligence agencies. In 2024, Microsoft \u003ca href=\"https://arstechnica.com/information-technology/2024/05/microsoft-launches-ai-chatbot-for-spies/\"\u003elaunched\u003c/a\u003e an isolated version of OpenAI\u0026#39;s GPT-4 for the US intelligence community after 18 months of work. That system, which operated on a special government-only network without Internet access, became available to about 10,000 individuals in the intelligence community for testing and answering questions.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "2 min read",
  "publishedTime": "2025-06-06T21:12:17Z",
  "modifiedTime": "2025-06-06T21:12:17Z"
}
