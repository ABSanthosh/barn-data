{
  "id": "e1198668-2329-4e9a-aece-d3989a885f26",
  "title": "All the Biggest News and Features Announced During Google I/O 2025",
  "link": "https://lifehacker.com/tech/the-biggest-features-and-news-from-google-i-o-2025?utm_medium=RSS",
  "description": "Gemini took center stage.",
  "author": "Jake Peterson",
  "published": "Tue, 20 May 2025 21:30:00 +0000",
  "source": "https://lifehacker.com/rss",
  "categories": [
    "AI"
  ],
  "byline": "Jake Peterson",
  "length": 8814,
  "excerpt": "Gemini took center stage.",
  "siteName": "Lifehacker",
  "favicon": "https://lifehacker.com/images/android-chrome-384x384.png",
  "text": "Credit: Google It should have been obvious that Google I/O 2025 would be jammed-packed, considering the company felt the need to hold a separate event to cover all of its Android news. But color me shocked that Google pulled off a nearly two hour-long presentation, full of announcements and reveals, mostly about AI. Not all AI announcements are equal, of course. Some of the news was geared towards enterprise users, and some towards developers. But many of the features discussed are on their way to consumers' devices too, some as soon as today. These are the updates I'm going to focus on here—you can expect to try out these features today, in the coming weeks, or at some point in the near future.Gemini Live is coming to the iPhoneEarlier this year, Google rolled out Gemini Live for all Android users via the Gemini app. The feature lets you share your camera feed or screen with Gemini, so it can help answer questions about what you're seeing. As of today, Google is now bringing the feature out to iPhones with the Gemini app as well. As long as you have the app, you can share your camera and screen with the AI, no matter what platform you're on. AI Mode is the future of Google SearchGoogle has been testing AI Mode in Search since March. The feature essentially turns Google Search into more of a Gemini experience, allowing you to stack multiple questions into one complex request. According to Google, it's AI can handle breaking down your query and searching the web for the most relevant sources. The result, in theory, is a complete report answering all aspects of your search, including links to sources and images.AI Mode is rolling out for all users—not just testers—over the coming weeks. But it's not just the AI Mode experience that Google has been testing. The company also announced new AI Mode features at I/O. Cram multiple searches into oneFirst, there's Deep Search, which multiplies the number of searches AI Mode typically would make for your query and generates an \"expert-level fully-cited report\" for you. I would still fact check it thoroughly, seeing as AI has a habit of hallucinating. AI Mode is also getting Gemini Live access, so you can share your screen or camera in Search.Use \"Agent Mode\" as a real world personal assistantProject Mariner is also coming to AI Mode. Google says you'll have access to \"agentic capabilities,\" which basically means you can rely on the AI to complete tasks for you. For example, you'll be able to ask AI Mode to find you \"affordable tickets for this Saturday’s Reds game in the lower level,\" and not only will the bot do the searching for you, it'll fill out the necessary forms. Google says that functionality will apply to event tickets, restaurant reservations, and local appointments.You can see that in action with Agent Mode, which will theoretically be able to execute complex tasks on your behalf. We don't know a lot about how that will work yet, but we do have a clear example from the Google I/O stage. During the presentation, Alphabet CEO Sundar Pichai tasked Gemini's Agent Mode with finding an apartment with in-unit laundry, keeping to a certain budget. Gemini then got to work, opening the browser, pulling up Zillow, searching for apartments, and booking a tour. AI Mode will pull from your previous search history in order to deliver you more relevant results. That includes results that apply to your whereabouts—say, local recommendations for an upcoming trip—as well as preferences (if you tend to book outdoor dining spots, AI Mode may recommend outdoor dining when you ask to find dinner reservations). New Gemini features coming to WorkspaceGoogle announced a number of new Gemini features at I/O, some of which are coming to Workspace. One of the features Google focused on most was Personalized smart replies in Gmail. While Gmail has an AI-powered smart reply feature already, this one goes a step further, and bases its responses on all of your Google data. The goal is to generate a reply that sounds like you wrote it, and includes all the questions or comments you might reasonably have for the email in question. In practice, I'm not sure why I'd want to let AI do all of my communicating for me, but the feature will be available later this year, and for paid subscribers first.If you use Google Meet with a paid plan, expect to see live speech translation start to roll out today. The feature automatically dubs over speakers on a call in a target language, like an instant universal translator. Let's say you speak English and your meeting partner speaks Spanish: You hear them begin to speak in Spanish, before an AI voice takes over with the English translation. 'Try it on'Google doesn't want you returning the clothes you order online anymore. The company announced a new feature called \"try it on\" that uses AI to show you what you'd look like wearing whatever clothing item you're thinking about buying. This isn't a mere concept, either: Google is rolling out \"try it on\" today to Google Search lab users. If you want to learn more about the feature and how to use it, check out our full guide. Android XRAs the rumors suggested, Google talked a bit about Android XR, the company's software experience for glasses and headsets. Most of the news it shared was previously announced, but we did see some interesting features in action. For example when using one of the future glasses with Android XR built in, you'll be able to access a subtle HUD that can show you everything from photos to messages to Google Maps. (Personally, the main draw here for me would be AR Google Maps while walking around a new city.) On stage, we also saw a live demo of speech translation, which Android XR overlaying an English translation on screen as two presenters spoke in different languages. What do you think so far? While there's no true timeline on when you can try Android XR, Google's big news is that it is working with both Warby Parker and Gentle Monster on making glasses with the service built in. Veo 3, Imagen 4, and FlowGoogle unveiled two new AI generation models at I/O this year: Imagen 4 (images) and Veo 3 (video).Imagen 4 now generates higher-quality images with more detail than Imagen 3, Google's previous image generation model. However, the company specifically noted Imagen 4's improvements with text generation. If you ask the model to generate a poster, for example, Google says that the text will be both accurate to the request, as well as stylistic. Google kicked off the show with videos generated by Veo 3, so it's safe to say the company is quite proud of its video generation model. While the results are crisp, colorful, and occasionally jam-packed with elements, it definitely still suffers from the usual quirks and issues with AI-generated video. But the bigger story here is \"Flow,\" Google's new AI video editor. Flow uses Veo 3 to generate videos, which you can then assemble like any oother non-linear editor. You can use Imagen 4 to generate an element you want in a shot, then ask Flow to add it to the next clip. In addition to the ability to cut or expand a shot, you can control the camera movement of each shot independently. It's the most \"impressive\" this tech has seemed to me, but outside of a high-tech story board, I can't imagine the use for this. Maybe I'm in the minority, but I certainly don't want to watch AI-generated videos, even if they are created via tools similar to the ones human video creators use. Veo 3 is only available to Google AI Ultra subscribers, though Flow is available in limited capacity with Veo 2 to AI Pro subscribers. Two new Chrome featuresChrome users can look forward to two new features following Google I/O. First, Google is bringing Gemini directly to the browser—no need to open the Gemini site. Second, Chrome can now update your old passwords on your behalf. This feature is launching later this year, though you'll need to wait for the websites themselves to offer support.A new way to pay for AIFinally, Google is offering new subscriptions to access its AI features. Google AI Premium is now AI Pro, and remains largely the same, minus the new ability to access Flow and Gemini in Chrome. It still costs $20 per month.The new subscription is Google AI Ultra, which costs a whopping $250 a month. For that price, you get everything in Google AI Pro, but with the highest limits for all of the AI models, including Gemini, Flow, Whisk, and NotebookLM. You get access to Gemini 2.5 Pro Deep Think (the company's newest and most advanced reasoning model), Veo 3, Project Mariner, YouTube Premium, and 30TB of cloud storage. What a deal.",
  "image": "https://lifehacker.com/imagery/articles/01JVQK1KEMDKWGWN27CKAKWYRK/hero-image.fill.size_1200x675.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\" data-seamless-scroll-item=\"\" data-status=\"loaded\" data-content-slug=\"the-biggest-features-and-news-from-google-i-o-2025\" data-content-qualified-slug=\"/tech/the-biggest-features-and-news-from-google-i-o-2025\" data-content-toc-items=\"[{\u0026#34;text\u0026#34;:\u0026#34;Table of Contents\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;toc\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;Gemini Live is coming to the iPhone\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;gemini-live-is-coming-to-the-iphone\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;AI Mode is the future of Google Search\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;ai-mode-is-the-future-of-google-search\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;New Gemini features coming to Workspace\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;new-gemini-features-coming-to-workspace\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;\u0026#39;Try it on\u0026#39;\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;try-it-on\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;Android XR\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;android-xr\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;Veo 3, Imagen 4, and Flow\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;veo-3-imagen-4-and-flow\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;Two new Chrome features\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;two-new-chrome-features\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;A new way to pay for AI\u0026#34;,\u0026#34;anchor\u0026#34;:\u0026#34;a-new-way-to-pay-for-ai\u0026#34;}]\" data-max-scrolled-percentage=\"0\" data-index=\"0\" data-ga-module=\"content-body\"\u003e\n    \n                    \u003cp\u003e\u003cimg src=\"https://lifehacker.com/imagery/articles/01JVQK1KEMDKWGWN27CKAKWYRK/hero-image.fill.size_1248x702.v1747773637.jpg\" alt=\"google i/o 2025 logo\" width=\"1248\" height=\"702\" srcset=\"https://lifehacker.com/imagery/articles/01JVQK1KEMDKWGWN27CKAKWYRK/hero-image.fill.size_400x225.v1747773637.jpg 400w, https://lifehacker.com/imagery/articles/01JVQK1KEMDKWGWN27CKAKWYRK/hero-image.fill.size_800x450.v1747773637.jpg 800w, https://lifehacker.com/imagery/articles/01JVQK1KEMDKWGWN27CKAKWYRK/hero-image.fill.size_1248x702.v1747773637.jpg 1600w\" sizes=\"(max-width: 1280px) 100vw, 1280px\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: Google\u003c/span\u003e\n                            \u003c/p\u003e\n            \n            \n    \n    \n    \u003carticle data-autopogo=\"\"\u003e\n        \u003chr/\u003e\n        \u003cp\u003eIt should have been obvious that \u003ca href=\"https://lifehacker.com/tech/how-to-watch-and-what-to-expect-google-io-2025\" target=\"_blank\"\u003eGoogle I/O 2025\u003c/a\u003e would be jammed-packed, considering the company felt the need to hold a \u003ca href=\"https://lifehacker.com/tech/google-announcing-android-news-before-google-io-2025\" target=\"_blank\"\u003eseparate event to cover all of its Android news\u003c/a\u003e. But color me shocked that Google pulled off a nearly two hour-long presentation, full of announcements and reveals, mostly about AI.    \u003c/p\u003e\u003cp\u003eNot all AI announcements are equal, of course. Some of the news was geared towards enterprise users, and some towards developers. But many of the features discussed are on their way to consumers\u0026#39; devices too, some as soon as today. These are the updates I\u0026#39;m going to focus on here—you can expect to try out these features today, in the coming weeks, or at some point in the near future.\u003c/p\u003e\u003ch2 id=\"gemini-live-is-coming-to-the-iphone\"\u003eGemini Live is coming to the iPhone\u003c/h2\u003e\u003cp\u003eEarlier this year, Google rolled out \u003ca href=\"https://lifehacker.com/tech/gemini-live-can-now-see-your-phone-to-a-point\" target=\"_blank\"\u003eGemini Live\u003c/a\u003e for all Android users via the Gemini app. The feature lets you share your camera feed or screen with Gemini, so it can help answer questions about what you\u0026#39;re seeing. As of today, Google is now \u003ca href=\"https://lifehacker.com/tech/gemini-live-can-now-access-your-camera-and-see-your-screen-on-iphone\" target=\"_blank\"\u003ebringing the feature out to iPhones with the Gemini app\u003c/a\u003e as well. As long as you have the app, you can share your camera and screen with the AI, no matter what platform you\u0026#39;re on. \u003c/p\u003e\u003ch2 id=\"ai-mode-is-the-future-of-google-search\"\u003eAI Mode is the future of Google Search\u003c/h2\u003e\u003cp\u003eGoogle has been testing \u003ca href=\"https://lifehacker.com/tech/google-is-adding-more-ai-to-search\" target=\"_blank\"\u003eAI Mode in Search\u003c/a\u003e since March. The feature essentially turns Google Search into more of a Gemini experience, allowing you to stack multiple questions into one complex request. According to Google, it\u0026#39;s AI can handle breaking down your query and searching the web for the most relevant sources. The result, in theory, is a complete report answering all aspects of your search, including links to sources and images.\u003c/p\u003e\u003cp\u003eAI Mode is rolling out for all users—not just testers—\u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003eover the coming weeks\u003c/a\u003e. But it\u0026#39;s not just the AI Mode experience that Google has been testing. The company also announced new AI Mode features at I/O. \u003c/p\u003e\u003ch3\u003eCram multiple searches into one\u003c/h3\u003e\u003cp\u003eFirst, there\u0026#39;s \u003cstrong\u003eDeep Search\u003c/strong\u003e, which multiplies the number of searches AI Mode typically would make for your query and generates an \u0026#34;expert-level fully-cited report\u0026#34; for you. I would still fact check it thoroughly, seeing as \u003ca href=\"https://lifehacker.com/tech/ai-models-hallucinating-more\" target=\"_blank\"\u003eAI has a habit of hallucinating\u003c/a\u003e. AI Mode is also getting Gemini Live access, so you can share your screen or camera in Search.\u003c/p\u003e\u003ch3\u003eUse \u0026#34;Agent Mode\u0026#34; as a real world personal assistant\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://deepmind.google/technologies/project-mariner/\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003e\u003cstrong\u003eProject Mariner\u003c/strong\u003e\u003c/a\u003e is also coming to AI Mode. Google says you\u0026#39;ll have access to \u0026#34;agentic capabilities,\u0026#34; which basically means you can rely on the AI to complete tasks for you. For example, you\u0026#39;ll be able to ask AI Mode to find you \u0026#34;affordable tickets for this Saturday’s Reds game in the lower level,\u0026#34; and not only will the bot do the searching for you, it\u0026#39;ll fill out the necessary forms. Google says that functionality will apply to event tickets, restaurant reservations, and local appointments.\u003c/p\u003e\u003cp\u003eYou can see that in action with \u003ca href=\"https://lifehacker.com/tech/google-gemini-agent-mode-turns-ai-into-real-personal-assistant\" target=\"_blank\"\u003e\u003cstrong\u003eAgent Mode\u003c/strong\u003e\u003c/a\u003e, which will theoretically be able to execute complex tasks on your behalf. We don\u0026#39;t know a lot about how that will work yet, but we do have a clear example from the Google I/O stage. During the presentation, Alphabet CEO Sundar Pichai tasked Gemini\u0026#39;s Agent Mode with finding an apartment with in-unit laundry, keeping to a certain budget. Gemini then got to work, opening the browser, pulling up Zillow, searching for apartments, and booking a tour.     \u003c/p\u003e\u003cp\u003eAI Mode will pull from your previous search history in order to deliver you more relevant results. That includes results that apply to your whereabouts—say, local recommendations for an upcoming trip—as well as preferences (if you tend to book outdoor dining spots, AI Mode may recommend outdoor dining when you ask to find dinner reservations).  \u003c/p\u003e\u003ch2 id=\"new-gemini-features-coming-to-workspace\"\u003eNew Gemini features coming to Workspace\u003c/h2\u003e\u003cp\u003eGoogle announced \u003ca href=\"https://lifehacker.com/tech/seven-new-gemini-features-google-announced-at-io-2025\" target=\"_blank\"\u003ea number of new Gemini features at I/O\u003c/a\u003e, some of which are coming to Workspace. \u003c/p\u003e\u003cp\u003eOne of the features Google focused on most was \u003ca href=\"https://lifehacker.com/tech/gmail-new-personalized-smart-replies-will-try-to-write-more-like-you\" target=\"_blank\"\u003ePersonalized smart replies in Gmail\u003c/a\u003e. While Gmail has an AI-powered smart reply feature already, this one goes a step further, and bases its responses on all of your Google data. The goal is to generate a reply that sounds like you wrote it, and includes all the questions or comments you might reasonably have for the email in question. In practice, I\u0026#39;m not sure why I\u0026#39;d want to let AI do all of my communicating for me, but the feature will be available later this year, and for paid subscribers first.\u003c/p\u003e\u003cp\u003eIf you use Google Meet with a paid plan, expect to see \u003ca href=\"https://lifehacker.com/tech/google-announces-live-translation-yet-again-this-time-in-google-meet\" target=\"_blank\"\u003elive speech translation\u003c/a\u003e start to roll out today. The feature automatically dubs over speakers on a call in a target language, like an instant universal translator. Let\u0026#39;s say you speak English and your meeting partner speaks Spanish: You hear them begin to speak in Spanish, before an AI voice takes over with the English translation.  \u003c/p\u003e\u003ch2 id=\"try-it-on\"\u003e\u0026#39;Try it on\u0026#39;\u003c/h2\u003e\u003cp\u003eGoogle doesn\u0026#39;t want you returning the clothes you order online anymore. The company announced a new feature called \u0026#34;try it on\u0026#34; that uses AI to show you what you\u0026#39;d look like wearing whatever clothing item you\u0026#39;re thinking about buying. \u003c/p\u003e\u003cp\u003eThis isn\u0026#39;t a mere concept, either: Google is rolling out \u0026#34;try it on\u0026#34; today to Google Search lab users. If you want to learn more about the feature and how to use it, \u003ca href=\"https://lifehacker.com/tech/use-google-io-to-virtually-try-on-clothes-from-phone\" target=\"_blank\"\u003echeck out our full guide\u003c/a\u003e.  \u003c/p\u003e\u003ch2 id=\"android-xr\"\u003eAndroid XR\u003c/h2\u003e\u003cp\u003eAs the rumors suggested, Google talked a bit about \u003ca href=\"https://blog.google/products/android/android-xr-gemini-glasses-headsets/\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003eAndroid XR\u003c/a\u003e, the company\u0026#39;s software experience for glasses and headsets. Most of the news it shared was previously announced, but we did see some interesting features in action. \u003c/p\u003e\u003cp\u003eFor example when using one of the future glasses with Android XR built in, you\u0026#39;ll be able to access a subtle HUD that can show you everything from photos to messages to Google Maps. (Personally, the main draw here for me would be AR Google Maps while walking around a new city.) On stage, we also saw a live demo of speech translation, which Android XR overlaying an English translation on screen as two presenters spoke in different languages.\u003c/p\u003e\u003cdiv data-ga-click=\"\" data-ga-template=\"News\" data-ga-module=\"openweb_widget\" data-ga-element=\"openweb_scroll\" data-ga-item=\"openweb_scroll_midpage\" x-cloak=\"\" x-data=\"{ commentsCount: null, hasComments: false }\" x-init=\"commentsCount = await window.openweb.getMessagesCount(01JVQK1KEMDKWGWN27CKAKWYRK);\n     hasComments = commentsCount !== null \u0026amp;\u0026amp; commentsCount \u0026gt; 0\"\u003e\n            \n\n            \u003cp\u003e\u003cspan\u003e\n                What do you think so far?\n                \n            \u003c/span\u003e\n        \u003c/p\u003e\u003c/div\u003e\n\u003cp\u003eWhile there\u0026#39;s no true timeline on when you can try Android XR, Google\u0026#39;s big news is that it is working with both Warby Parker and Gentle Monster on making glasses with the service built in. \u003c/p\u003e\u003ch2 id=\"veo-3-imagen-4-and-flow\"\u003eVeo 3, Imagen 4, and Flow\u003c/h2\u003e\u003cp\u003eGoogle unveiled two new AI generation models at I/O this year: Imagen 4 (images) and Veo 3 (video).\u003c/p\u003e\u003cp\u003eImagen 4 now generates higher-quality images with more detail than Imagen 3, Google\u0026#39;s previous image generation model. However, the company specifically noted Imagen 4\u0026#39;s improvements with text generation. If you ask the model to generate a poster, for example, Google says that the text will be both accurate to the request, as well as stylistic. \u003c/p\u003e\u003cp\u003eGoogle kicked off the show with videos generated by Veo 3, so it\u0026#39;s safe to say the company is quite proud of its video generation model. While the results are crisp, colorful, and occasionally jam-packed with elements, it definitely still suffers from \u003ca href=\"https://lifehacker.com/tech/how-to-know-if-the-video-youre-watching-was-made-with-ai\" target=\"_blank\"\u003ethe usual quirks and issues with AI-generated video\u003c/a\u003e. But the bigger story here is \u0026#34;\u003ca href=\"https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003eFlow\u003c/a\u003e,\u0026#34; Google\u0026#39;s new AI video \u003cem\u003eeditor\u003c/em\u003e. Flow uses Veo 3 to generate videos, which you can then assemble like any oother non-linear editor. You can use Imagen 4 to generate an element you want in a shot, then ask Flow to add it to the next clip. In addition to the ability to cut or expand a shot, you can control the camera movement of each shot independently. \u003c/p\u003e\u003cp\u003eIt\u0026#39;s the most \u0026#34;impressive\u0026#34; this tech has seemed to me, but outside of a high-tech story board, I can\u0026#39;t imagine the use for this. Maybe I\u0026#39;m in the minority, but I certainly don\u0026#39;t want to watch AI-generated videos, even if they are created via tools similar to the ones human video creators use. \u003c/p\u003e\u003cp\u003eVeo 3 is only available to Google AI Ultra subscribers, though Flow is available in limited capacity with Veo 2 to AI Pro subscribers.  \u003c/p\u003e\u003ch2 id=\"two-new-chrome-features\"\u003eTwo new Chrome features\u003c/h2\u003e\u003cp\u003eChrome users can look forward to \u003ca href=\"https://lifehacker.com/tech/google-just-announced-two-new-chrome-features-at-i-o-2025\" target=\"_blank\"\u003etwo new features following Google I/O\u003c/a\u003e. First, Google is bringing Gemini directly to the browser—no need to open the Gemini site. Second, Chrome can now update your old passwords on your behalf. This feature is launching later this year, though you\u0026#39;ll need to wait for the websites themselves to offer support.\u003c/p\u003e\u003ch2 id=\"a-new-way-to-pay-for-ai\"\u003eA new way to pay for AI\u003c/h2\u003e\u003cp\u003eFinally, \u003ca href=\"https://blog.google/products/google-one/google-ai-ultra/\" target=\"_blank\" title=\"open in a new window\" rel=\"noopener\"\u003eGoogle is offering new subscriptions to access its AI features\u003c/a\u003e. Google AI Premium is now AI Pro, and remains largely the same, minus the new ability to access Flow and Gemini in Chrome. It still costs $20 per month.\u003c/p\u003e\u003cp\u003eThe new subscription is Google AI Ultra, which costs a whopping $250 a month. For that price, you get everything in Google AI Pro, but with the highest limits for all of the AI models, including Gemini, Flow, Whisk, and NotebookLM. You get access to Gemini 2.5 Pro Deep Think (the company\u0026#39;s newest and most advanced reasoning model), Veo 3, Project Mariner, YouTube Premium, and 30TB of cloud storage. What a deal.\u003c/p\u003e\n    \u003c/article\u003e\n\n    \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-05-20T21:30:00Z",
  "modifiedTime": "2025-05-20T21:30:00Z"
}
