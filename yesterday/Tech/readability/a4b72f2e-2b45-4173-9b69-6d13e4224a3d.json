{
  "id": "a4b72f2e-2b45-4173-9b69-6d13e4224a3d",
  "title": "Google’s Genie 2 “world model” reveal leaves more questions than answers",
  "link": "https://arstechnica.com/ai/2024/12/googles-genie-2-world-model-reveal-leaves-more-questions-than-answers/",
  "description": "Long-term persistence, real-time interactions remain huge hurdles for AI worlds.",
  "author": "Kyle Orland",
  "published": "Fri, 06 Dec 2024 23:09:04 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Gaming",
    "AI game design",
    "AI gaming",
    "Google Genie"
  ],
  "byline": "Kyle Orland",
  "length": 11017,
  "excerpt": "Long-term persistence, real-time interactions remain huge hurdles for AI worlds.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Making a command out of your wish? Long-term persistence, real-time interactions remain huge hurdles for AI worlds. A sample of some of the best-looking Genie 2 worlds Google wants to show off. Credit: Google Deepmind In March, Google showed off its first Genie AI model. After training on thousands of hours of 2D run-and-jump video games, the model could generate halfway-passable, interactive impressions of those games based on generic images or text descriptions. Nine months later, this week's reveal of the Genie 2 model expands that idea into the realm of fully 3D worlds, complete with controllable third- or first-person avatars. Google's announcement talks up Genie 2's role as a \"foundational world model\" that can create a fully interactive internal representation of a virtual environment. That could allow AI agents to train themselves in synthetic but realistic environments, Google says, forming an important stepping stone on the way to artificial general intelligence. But while Genie 2 shows just how much progress Google's Deepmind team has achieved in the last nine months, the limited public information about the model thus far leaves a lot of questions about how close we are to these foundational model worlds being useful for anything but some short but sweet demos. How long is your memory? Much like the original 2D Genie model, Genie 2 starts from a single image or text description and then generates subsequent frames of video based on both the previous frames and fresh input from the user (such as a movement direction or \"jump\"). Google says it trained on a \"large-scale video dataset\" to achieve this, but it doesn't say just how much training data was necessary compared to the 30,000 hours of footage used to train the first Genie. Short GIF demos on the Google DeepMind promotional page show Genie 2 being used to animate avatars ranging from wooden puppets to intricate robots to a boat on the water. Simple interactions shown in those GIFs demonstrate those avatars busting balloons, climbing ladders, and shooting exploding barrels without any explicit game engine describing those interactions. Those Genie 2-generated pyramids will still be there in 30 seconds. But in five minutes? Credit: Google Deepmind Perhaps the biggest advance claimed by Google here is Genie 2's \"long horizon memory.\" This feature allows the model to remember parts of the world as they come out of view and then render them accurately as they come back into the frame based on avatar movement. This kind of persistence has proven to be a persistent problem for video generation models like Sora, which OpenAI said in February \"do[es] not always yield correct changes in object state\" and can develop \"incoherencies... in long duration samples.\" The \"long horizon\" part of \"long horizon memory\" is perhaps a little overzealous here, though, as Genie 2 only \"maintains a consistent world for up to a minute,\" with \"the majority of examples shown lasting [10 to 20 seconds].\" Those are definitely impressive time horizons in the world of AI video consistency, but it's pretty far from what you'd expect from any other real-time game engine. Imagine entering a town in a Skyrim-style RPG, then coming back five minutes later to find that the game engine had forgotten what that town looks like and generated a completely different town from scratch instead. What are we prototyping, exactly? Perhaps for this reason, Google suggests Genie 2 as it stands is less useful for creating a complete game experience and more to \"rapidly prototype diverse interactive experiences\" or to turn \"concept art and drawings... into fully interactive environments.\" The ability to transform static \"concept art\" into lightly interactive \"concept videos\" could definitely be useful for visual artists brainstorming ideas for new game worlds. However, these kinds of AI-generated samples might be less useful for prototyping actual game designs that go beyond the visual. On Bluesky, British game designer Sam Barlow (Silent Hill: Shattered Memories, Her Story) points out how game designers often use a process called whiteboxing to lay out the structure of a game world as simple white boxes well before the artistic vision is set. The idea, he says, is to \"prove out and create a gameplay-first version of the game that we can lock so that art can come in and add expensive visuals to the structure. We build in lo-fi because it allows us to focus on these issues and iterate on them cheaply before we are too far gone to correct.\" Generating elaborate visual worlds using a model like Genie 2 before designing that underlying structure feels a bit like putting the cart before the horse. The process almost seems designed to generate generic, \"asset flip\"-style worlds with AI-generated visuals papered over generic interactions and architecture. As podcaster Ryan Zhao put it on Bluesky, \"The design process has gone wrong when what you need to prototype is 'what if there was a space.'\" Gotta go fast When Google revealed the first version of Genie earlier this year, it also published a detailed research paper outlining the specific steps taken behind the scenes to train the model and how that model generated interactive videos. No such research paper has been published detailing Genie 2's process, leaving us guessing at some important details. One of the most important of these details is model speed. The first Genie model generated its world at roughly one frame per second, a rate that was orders of magnitude slower than would be tolerably playable in real time. For Genie 2, Google only says that \"the samples in this blog post are generated by an undistilled base model, to show what is possible. We can play a distilled version in real-time with a reduction in quality of the outputs.\" Reading between the lines, it sounds like the full version of Genie 2 operates at something well below the real-time interactions implied by those flashy GIFs. It's unclear how much \"reduction in quality\" is necessary to get a diluted version of the model to real-time controls, but given the lack of examples presented by Google, we have to assume that reduction is significant. Oasis' AI-generated Minecraft clone shows great potential, but still has a lot of rough edges, so to speak. Credit: Oasis Real-time, interactive AI video generation isn't exactly a pipe dream. Earlier this year, AI model maker Decart and hardware maker Etched published the Oasis model, showing off a human-controllable, AI-generated video clone of Minecraft that runs at a full 20 frames per second. However, that 500 million parameter model was trained on millions of hours of footage of a single, relatively simple game, and focused exclusively on the limited set of actions and environmental designs inherent to that game. When Oasis launched, its creators fully admitted the model \"struggles with domain generalization,\" showing how \"realistic\" starting scenes had to be reduced to simplistic Minecraft blocks to achieve good results. And even with those limitations, it's not hard to find footage of Oasis degenerating into horrifying nightmare fuel after just a few minutes of play. What started as a realistic-looking soldier in this Genie 2 demo degenerates into this blobby mess just seconds later. Credit: Google Deepmind We can already see similar signs of degeneration in the extremely short GIFs shared by the Genie team, such as an avatar's dream-like fuzz during high-speed movement or NPCs that quickly fade into undifferentiated blobs at a short distance. That's not a great sign for a model whose \"long memory horizon\" is supposed to be a key feature. A learning crèche for other AI agents? From this image, Genie 2 could generate a useful training environment for an AI agent and a simple \"pick a door\" task. Credit: Google Deepmind Genie 2 seems to be using individual game frames as the basis for the animations in its model. But it also seems able to infer some basic information about the objects in those frames and craft interactions with those objects in the way a game engine might. Google's blog post shows how a SIMA agent inserted into a Genie 2 scene can follow simple instructions like \"enter the red door\" or \"enter the blue door,\" controlling the avatar via simple keyboard and mouse inputs. That could potentially make Genie 2 environment a great test bed for AI agents in various synthetic worlds. Google claims rather grandiosely that Genie 2 puts it on \"the path to solving a structural problem of training embodied agents safely while achieving the breadth and generality required to progress towards [artificial general intelligence].\" Whether or not that ends up being true, recent research shows that agent learning gained from foundational models can be effectively applied to real-world robotics. Using this kind of AI model to create worlds for other AI models to learn in might be the ultimate use case for this kind of technology. But when it comes to the dream of an AI model that can create generic 3D worlds that a human player could explore in real time, we might not be as close as it seems. Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once wrote a whole book about Minesweeper. 8 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/genie2.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n            \u003carticle data-id=\"2065398\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n    \u003cdiv\u003e\n      \u003cdiv\u003e\n        \u003cp\u003e\u003cspan\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"section-ai_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"section-ai_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#section-ai_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#section-ai_svg__b)\"\u003e\u003cpath d=\"M20 2.4c9.7 0 17.6 7.9 17.6 17.6S29.7 37.6 20 37.6 2.4 29.7 2.4 20 10.3 2.4 20 2.4M20 0C9 0 0 9 0 20s9 20 20 20 20-9 20-20S31 0 20 0\"\u003e\u003c/path\u003e\u003cpath d=\"M20 13q2.85 0 5.4.9c.7.2 1.4-.1 1.6-.9l1.4-5.5C26 5.9 23.1 4.9 20 4.9s-6 .9-8.4 2.6L13 13c.2.7.9 1.1 1.6.9Q17 13 20 13M8.9 18.3c.4-.8 1-1.5 1.7-2.1l-2.2-5.7C7 12.2 6 14.1 5.5 16.3l1.3 2.1c.5.8 1.7.8 2.2 0m24.3 0 1.3-2.1c-.5-2.2-1.5-4.1-2.9-5.8l-2.2 5.7c.7.6 1.3 1.3 1.7 2.1.5.8 1.6.9 2.2 0M23.2 20c0 1.8-1.5 3.2-3.2 3.2s-3.2-1.4-3.2-3.2 1.5-3.2 3.2-3.2 3.2 1.4 3.2 3.2\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n  \u003c/span\u003e\n  \u003cspan\u003e\n    Making a command out of your wish?\n  \u003c/span\u003e\n\u003c/p\u003e\n      \u003c/div\u003e\n\n      \n\n      \u003cp\u003e\n        Long-term persistence, real-time interactions remain huge hurdles for AI worlds.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\n    \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A sample of some of the best-looking Genie 2 worlds Google wants to show off.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/\" target=\"_blank\"\u003e\n          \n          Google Deepmind\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \n\u003cp\u003eIn March, Google \u003ca href=\"https://arstechnica.com/gadgets/2024/03/googles-genie-model-creates-interactive-2d-worlds-from-a-single-image/\"\u003eshowed off its first Genie AI model\u003c/a\u003e. After training on thousands of hours of 2D run-and-jump video games, the model could generate halfway-passable, interactive impressions of those games based on generic images or text descriptions.\u003c/p\u003e\n\u003cp\u003eNine months later, this week\u0026#39;s \u003ca href=\"https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/\"\u003ereveal of the Genie 2 model\u003c/a\u003e expands that idea into the realm of fully 3D worlds, complete with controllable third- or first-person avatars. Google\u0026#39;s announcement talks up Genie 2\u0026#39;s role as a \u0026#34;foundational world model\u0026#34; that can create a fully interactive internal representation of a virtual environment. That could allow AI agents to train themselves in synthetic but realistic environments, Google says, forming an important stepping stone on the way to artificial general intelligence.\u003c/p\u003e\n\u003cp\u003eBut while Genie 2 shows just how much progress Google\u0026#39;s Deepmind team has achieved in the last nine months, the limited public information about the model thus far leaves a lot of questions about how close we are to these foundational model worlds being useful for anything but some short but sweet demos.\u003c/p\u003e\n\u003ch2\u003eHow long is your memory?\u003c/h2\u003e\n\u003cp\u003eMuch like the original 2D Genie model, Genie 2 starts from a single image or text description and then generates subsequent frames of video based on both the previous frames and fresh input from the user (such as a movement direction or \u0026#34;jump\u0026#34;). Google says it trained on a \u0026#34;large-scale video dataset\u0026#34; to achieve this, but it doesn\u0026#39;t say just how much training data was necessary compared to the 30,000 hours of footage used to train the first Genie.\u003c/p\u003e\n\u003cp\u003eShort GIF demos on \u003ca href=\"https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/\"\u003ethe Google DeepMind promotional page\u003c/a\u003e show Genie 2 being used to animate avatars ranging from wooden puppets to intricate robots to a boat on the water. Simple interactions shown in those GIFs demonstrate those avatars busting balloons, climbing ladders, and shooting exploding barrels without any explicit game engine describing those interactions.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"576\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/pyramids-1024x576.png\" alt=\"\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/pyramids-1024x576.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/pyramids-640x360.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/pyramids-768x432.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/pyramids-384x216.png 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/pyramids-1152x648.png 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/pyramids-980x551.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/pyramids.png 1280w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      Those Genie 2-generated pyramids will still be there in 30 seconds. But in five minutes?\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/\" target=\"_blank\"\u003e\n          \n          Google Deepmind\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003ePerhaps the biggest advance claimed by Google here is Genie 2\u0026#39;s \u0026#34;long horizon memory.\u0026#34; This feature allows the model to remember parts of the world as they come out of view and then render them accurately as they come back into the frame based on avatar movement. This kind of persistence has proven to be a persistent problem for \u003ca href=\"https://arstechnica.com/information-technology/2024/02/openai-collapses-media-reality-with-sora-a-photorealistic-ai-video-generator/\"\u003evideo generation models like Sora\u003c/a\u003e, which OpenAI said in February \u0026#34;do[es] not always yield correct changes in object state\u0026#34; and can develop \u0026#34;incoherencies... in long duration samples.\u0026#34;\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eThe \u0026#34;long horizon\u0026#34; part of \u0026#34;long horizon memory\u0026#34; is perhaps a little overzealous here, though, as Genie 2 only \u0026#34;maintains a consistent world for up to a minute,\u0026#34; with \u0026#34;the majority of examples shown lasting [10 to 20 seconds].\u0026#34; Those are definitely impressive time horizons in the world of AI video consistency, but it\u0026#39;s pretty far from what you\u0026#39;d expect from any other real-time game engine. Imagine entering a town in a \u003cem\u003eSkyrim\u003c/em\u003e-style RPG, then coming back five minutes later to find that the game engine had forgotten what that town looks like and generated a completely different town from scratch instead.\u003c/p\u003e\n\u003ch2\u003eWhat are we prototyping, exactly?\u003c/h2\u003e\n\u003cp\u003ePerhaps for this reason, Google suggests Genie 2 as it stands is less useful for creating a complete game experience and more to \u0026#34;rapidly prototype diverse interactive experiences\u0026#34; or to turn \u0026#34;concept art and drawings... into fully interactive environments.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThe ability to transform static \u0026#34;concept art\u0026#34; into lightly interactive \u0026#34;concept videos\u0026#34; could definitely be useful for visual artists brainstorming ideas for new game worlds. However, these kinds of AI-generated samples might be less useful for prototyping actual game \u003cem\u003edesigns \u003c/em\u003ethat go beyond the visual.\u003c/p\u003e\n\n\n\u003cp\u003eOn Bluesky, British game designer Sam Barlow (\u003cem\u003eSilent Hill: Shattered Memories\u003c/em\u003e, \u003cem\u003eHer Story\u003c/em\u003e) \u003ca href=\"https://bsky.app/profile/mrsambarlow.bsky.social/post/3lclhskbxgk2r\"\u003epoints out\u003c/a\u003e how game designers often use a process called \u003ca href=\"https://www.gamedeveloper.com/design/white-boxing-your-game\"\u003ewhiteboxing\u003c/a\u003e to lay out the structure of a game world as simple white boxes well before the artistic vision is set. The idea, he says, is to \u0026#34;prove out and create a gameplay-first version of the game that we can lock so that art can come in and add expensive visuals to the structure. We build in lo-fi because it allows us to focus on these issues and iterate on them cheaply before we are too far gone to correct.\u0026#34;\u003c/p\u003e\n\u003cp\u003eGenerating elaborate visual worlds using a model like Genie 2 before designing that underlying structure feels a bit like putting the cart before the horse. The process almost seems designed to generate generic, \u003ca href=\"https://en.wikipedia.org/wiki/Asset_flip\"\u003e\u0026#34;asset flip\u0026#34;-style\u003c/a\u003e worlds with AI-generated visuals papered over generic interactions and architecture.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eAs podcaster Ryan Zhao \u003ca href=\"https://bsky.app/profile/insrtcoins.bsky.social/post/3lcl6epzwm22k\"\u003eput it on Bluesky\u003c/a\u003e, \u0026#34;The design process has gone wrong when what you need to prototype is \u0026#39;what if there was a space.\u0026#39;\u0026#34;\u003c/p\u003e\n\n\u003ch2\u003eGotta go fast\u003c/h2\u003e\n\u003cp\u003eWhen Google revealed the first version of Genie earlier this year, it also \u003ca href=\"https://arxiv.org/pdf/2402.15391v1\"\u003epublished a detailed research paper\u003c/a\u003e outlining the specific steps taken behind the scenes to train the model and how that model generated interactive videos. No such research paper has been published detailing Genie 2\u0026#39;s process, leaving us guessing at some important details.\u003c/p\u003e\n\u003cp\u003eOne of the most important of these details is model speed. The first Genie model generated its world at roughly one frame per second, a rate that was orders of magnitude slower than would be tolerably playable in real time. For Genie 2, Google only says that \u0026#34;the samples in this blog post are generated by an undistilled base model, to show what is possible. We can play a distilled version in real-time with a reduction in quality of the outputs.\u0026#34;\u003c/p\u003e\n\u003cp\u003eReading between the lines, it sounds like the full version of Genie 2 operates at something well below the real-time interactions implied by those flashy GIFs. It\u0026#39;s unclear how much \u0026#34;reduction in quality\u0026#34; is necessary to get a diluted version of the model to real-time controls, but given the lack of examples presented by Google, we have to assume that reduction is significant.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"800\" height=\"800\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/oasis.png\" alt=\"\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/oasis.png 800w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/oasis-640x640.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/oasis-300x300.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/oasis-768x768.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/oasis-500x500.png 500w\" sizes=\"auto, (max-width: 800px) 100vw, 800px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      Oasis\u0026#39; AI-generated \u003cem\u003eMinecraft\u003c/em\u003e clone shows great potential, but still has a lot of rough edges, so to speak.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://oasis-model.github.io/\" target=\"_blank\"\u003e\n          \n          Oasis\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eReal-time, interactive AI video generation isn\u0026#39;t exactly a pipe dream. Earlier this year, AI model maker \u003ca href=\"https://www.decart.ai/\"\u003eDecart\u003c/a\u003e and hardware maker \u003ca href=\"https://www.etched.com/\"\u003eEtched\u003c/a\u003e published \u003ca href=\"https://oasis-model.github.io/\"\u003ethe Oasis model\u003c/a\u003e, showing off a human-controllable, AI-generated video clone of \u003cem\u003eMinecraft\u003c/em\u003e that runs at a full 20 frames per second. However, that 500 million parameter model was trained on millions of hours of footage of a single, relatively simple game, and focused exclusively on the limited set of actions and environmental designs inherent to that game.\u003c/p\u003e\n\u003cp\u003eWhen Oasis launched, its creators fully admitted the model \u0026#34;struggles with domain generalization,\u0026#34; showing how \u0026#34;realistic\u0026#34; starting scenes \u003ca href=\"https://oasis-model.github.io/colloseum.webp\"\u003ehad to be reduced to simplistic \u003cem\u003eMinecraft\u003c/em\u003e blocks\u003c/a\u003e to achieve good results. And even with those limitations, it\u0026#39;s not hard to \u003ca href=\"https://www.forbes.com/sites/danidiplacido/2024/11/03/minecraft-is-finally-haunted-thanks-to-generative-ai/\"\u003efind footage\u003c/a\u003e of Oasis \u003ca href=\"https://www.youtube.com/watch?v=pWh4u2sXBhU\"\u003edegenerating into horrifying nightmare fuel\u003c/a\u003e after just a few minutes of play.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"576\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/blob-1024x576.png\" alt=\"\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/blob-1024x576.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/blob-640x360.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/blob-768x432.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/blob-384x216.png 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/blob-1152x648.png 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/blob-980x551.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/blob.png 1280w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      What started as a realistic-looking soldier in this Genie 2 demo degenerates into this blobby mess just seconds later.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/\" target=\"_blank\"\u003e\n          \n          Google Deepmind\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eWe can already see similar signs of degeneration in the extremely short GIFs shared by the Genie team, such as an avatar\u0026#39;s \u003ca href=\"https://deepmind.google/api/blob/website/media/long_video_1.mp4\"\u003edream-like fuzz\u003c/a\u003e during high-speed movement or NPCs that \u003ca href=\"https://deepmind.google/api/blob/website/media/npc_1.mp4\"\u003equickly fade into undifferentiated blobs\u003c/a\u003e at a short distance. That\u0026#39;s not a great sign for a model whose \u0026#34;long memory horizon\u0026#34; is supposed to be a key feature.\u003c/p\u003e\n\u003ch2\u003eA learning crèche for other AI agents?\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"616\" height=\"336\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/pickadoor.png\" alt=\"\" decoding=\"async\" loading=\"lazy\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      From this image, Genie 2 could generate a useful training environment for an AI agent and a simple \u0026#34;pick a door\u0026#34; task.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/\" target=\"_blank\"\u003e\n          \n          Google Deepmind\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eGenie 2 seems to be using individual game frames as the basis for the animations in its model. But it also seems able to infer some basic information about the objects in those frames and craft interactions with those objects in the way a game engine might.\u003c/p\u003e\n\u003cp\u003eGoogle\u0026#39;s blog post shows how \u003ca href=\"https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/\"\u003ea SIMA agent\u003c/a\u003e inserted into a Genie 2 scene can follow simple instructions like \u0026#34;enter the red door\u0026#34; or \u0026#34;enter the blue door,\u0026#34; controlling the avatar via simple keyboard and mouse inputs. That could potentially make Genie 2 environment a great test bed for AI agents in various synthetic worlds.\u003c/p\u003e\n\u003cp\u003eGoogle claims rather grandiosely that Genie 2 puts it on \u0026#34;the path to solving a structural problem of training embodied agents safely while achieving the breadth and generality required to progress towards [artificial general intelligence].\u0026#34; Whether or not that ends up being true, \u003ca href=\"https://arxiv.org/abs/2402.05741\"\u003erecent research\u003c/a\u003e shows that agent learning gained from foundational models can be effectively applied to real-world robotics.\u003c/p\u003e\n\u003cp\u003eUsing this kind of AI model to create worlds for other AI models to learn in might be the ultimate use case for this kind of technology. But when it comes to the dream of an AI model that can create generic 3D worlds that a human player could explore in real time, we might not be as close as it seems.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/kyle-orland/\"\u003e\u003cimg src=\"https://arstechnica.com/wp-content/uploads/2016/05/k.orland-13.jpg\" alt=\"Photo of Kyle Orland\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once \u003ca href=\"https://bossfightbooks.com/collections/books/products/minesweeper-by-kyle-orland\"\u003ewrote a whole book about \u003cem\u003eMinesweeper\u003c/em\u003e\u003c/a\u003e.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/ai/2024/12/googles-genie-2-world-model-reveal-leaves-more-questions-than-answers/#comments\" title=\"8 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    8 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n\n\n\n  \n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/information-technology/2024/12/new-broadcom-sales-plan-may-be-insignificant-in-deterring-vmware-migrations/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/12/GettyImages-1247615915-768x432-1733429422.jpg\" alt=\"Listing image for first story in Most Read: Broadcom reverses controversial plan in effort to cull VMware migrations\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2024-12-06T23:09:04Z",
  "modifiedTime": "2024-12-06T23:09:04Z"
}
