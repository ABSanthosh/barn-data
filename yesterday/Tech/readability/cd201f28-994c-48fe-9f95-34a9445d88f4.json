{
  "id": "cd201f28-994c-48fe-9f95-34a9445d88f4",
  "title": "Robot developers keep making it seem like housebots are imminent when they’re decades away",
  "link": "https://thenextweb.com/news/robot-developers-housebots-decades-away",
  "description": "The walking, talking, dancing Optimus robots at the recent Tesla demonstration generated huge excitement. But this turned to disappointment as it became apparent that much of what was happening was actually being controlled remotely by humans. As much as this might still be a fascinating glimpse of the future, it’s not the first time that robots have turned out to be a little too good to be true. Take Sophia, for instance, the robot created by Texas-based Hanson Robotics back in 2016. She was presented by the company as essentially an intelligent being, prompting numerous tech specialists to call this…This story continues at The Next Web",
  "author": "The Conversation",
  "published": "Mon, 28 Oct 2024 09:00:24 +0000",
  "source": "https://thenextweb.com/feed/",
  "categories": [
    "Insider",
    "Future of work"
  ],
  "byline": "The Conversation",
  "length": 6958,
  "excerpt": "Despite what Tesla may claim, housebots are still a long way away. Here's a look at what we need to get there.",
  "siteName": "TNW | Future-Of-Work",
  "favicon": "https://next.tnwcdn.com/assets/img/favicon/favicon-194x194.png",
  "text": "The walking, talking, dancing Optimus robots at the recent Tesla demonstration generated huge excitement. But this turned to disappointment as it became apparent that much of what was happening was actually being controlled remotely by humans. As much as this might still be a fascinating glimpse of the future, it’s not the first time that robots have turned out to be a little too good to be true. Take Sophia, for instance, the robot created by Texas-based Hanson Robotics back in 2016. She was presented by the company as essentially an intelligent being, prompting numerous tech specialists to call this out as well beyond our capabilities at the time. Similarly we’ve seen carefully choreographed videos of pre-scripted action sequences like Boston Dynamics’ Atlas gymnastics, the English-made Ameca robot “waking up”, and most recently Tesla’s Optimus in the factory. Obviously these are still impressive in different ways, but they’re nowhere near the complete sentient package. Let Optimus or Atlas loose in a random home and you’d see something very different. A humanoid robot capable of working in our homes needs to be capable of doing many different tasks, using our tools, navigating our environments and communicating with us like a human. If you thought this was just a year or two away, you’re going to be disappointed. Building robots able to interact and carry out complex tasks in our homes and streets is still a huge challenge. Designing them even to do one specific task well, such as opening a door, is phenomenally difficult. There are so many door handles with different shapes, weights and materials, not to mention the complexity of dealing with unforeseen circumstances such as a locked door or objects blocking the way. Developers have actually now created a door-opening robot, but robots that can deal with hundreds of everyday tasks are still some way off. Behind the curtain The Tesla demonstration’s “Wizard of Oz” remote operation technique is a commonly used control method in this field, giving researchers a benchmark against which to test their real advances. Known as telemetric control, this has been around for some time, and is becoming more advanced. One of the authors of this article, Carl Strathearn, was at a conference in Japan earlier this year, where a keynote speaker from one of the top robotics labs demonstrated an advanced telemetrics system. It allowed a single human to simultaneously operate many humanoid robots semi-autonomously, using pre-scripted movements, conversation prompts and computerised speech. Clearly, this is very useful technology. Telemetric systems are used to control robots working in dangerous environments, disability healthcare and even in outer space. But the reason why a human is still at the helm is because even the most advanced humanoid robots, such as Atlas, are not yet reliable enough to operate completely independently in the real world. Another major problem is what we can call social AI. Leading generative AI programs such as DeepMind’s Gemini and OpenAI’s GPT-4 Vision may be a foundation for creative autonomous AI systems for humanoid robots in the future. But we should not be misled into believing that such models mean that a robot is now capable of functioning well in the real world. Interpreting information and problem solving like a human requires much more than just recognising words, classifying objects and generating speech. It requires a deeper contextual understanding of people, objects and environments – in other words, common sense. To explore what is currently possible, we recently completed a research project called Common Sense Enhanced Language and Vision (CiViL). We equipped a robot called Euclid with commonsense knowledge as part of a generative AI vision and language system to assist people in preparing recipes. To do this, we had to create commonsense knowledge databases using real-world problem-solving examples enacted by students. Euclid could explain complicated steps in recipes, give suggestions when things went wrong, and even point people to locations in the kitchen where utensils and tools might typically be found. Yet there were still issues, such as what to do if someone has a bad allergic reaction while cooking. The problem is that it’s almost impossible to handle every possible scenario, yet that’s what true common sense entails. This fundamental aspect of AI has got somewhat lost in humanoid robots over the years. Generated speech, realistic facial expressions, telemetric controls, even the ability to play games such as “rock paper scissors” are all impressive. But the novelty soon wears off if the robots are not actually capable of doing anything useful on their own. This isn’t to say that significant progress isn’t being made toward autonomous humanoid robots. There’s impressive work going on into robotic nervous systems to give robots more senses for learning, for instance. It’s just not usually given the same amount of press attention as the big unveilings. The data deficit Another key challenge is the lack of real-world data to train AI systems, since online data doesn’t always accurately represent the real-world conditions necessary for training our robots well enough. We have yet to find an effective way of collecting this real-world data in large enough quantities to get good results. However, this may change soon if we can access it from technologies such as Alexa and Meta Ray-Bans. Nonetheless, the reality is that we’re still perhaps decades away from developing multimodal humanoid robots with advanced social AI that are capable of helping around the house. Maybe in the meantime we’ll be offered robots controlled remotely from a command centre. Will we want them, though? In the meantime, it’s also more important that we focus our efforts on creating robots for roles that can support people who urgently need help now. Examples would include healthcare, where there are long waiting lists and understaffed hospitals; and education, to offer a way for overanxious or severely ill children to participate in classrooms remotely. We also need better transparency, legislation and publicly available testing, so that everyone can tell fact from fiction and help build public trust for when the robots eventually do arrive. Carl Strathearn, Research Fellow, Computing, Edinburgh Napier University and Dimitra Gkatzia, Associate Professor in Computing, Edinburgh Napier University This article is republished from The Conversation under a Creative Commons license. Read the original article. Get the TNW newsletter Get the most important tech news in your inbox each week. Also tagged with",
  "image": "https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1\u0026fit=1280%2C640\u0026url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-3-1.jpg\u0026signature=83e3d40acd486750698a3a8fffde91e5",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                \u003carticle id=\"articleOutput\"\u003e\n                                                                        \u003cdiv\u003e\n                                \u003cfigure\u003e\n                                    \u003cimg alt=\"Robot developers keep making it seem like housebots are imminent when they’re decades away\" src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-3-1.jpg\u0026amp;signature=811ecfadaa793135234f4746eb54912f\" sizes=\"(max-width: 1023px) 100vw\n                                                   868px\" srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-3-1.jpg\u0026amp;signature=30199b20c668e54a5365a956b7485657 576w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-3-1.jpg\u0026amp;signature=2291531904632467bdf1aa8bcbc97f20 1152w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-3-1.jpg\u0026amp;signature=811ecfadaa793135234f4746eb54912f 1280w\" data-src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-3-1.jpg\u0026amp;signature=811ecfadaa793135234f4746eb54912f\" data-srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-3-1.jpg\u0026amp;signature=30199b20c668e54a5365a956b7485657 576w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-3-1.jpg\u0026amp;signature=2291531904632467bdf1aa8bcbc97f20 1152w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-3-1.jpg\u0026amp;signature=811ecfadaa793135234f4746eb54912f 1280w\"/\u003e\n\n                                    \n\n                                                                    \u003c/figure\u003e\n                            \u003c/div\u003e\n                        \n                                                    \n                            \n                                            \n                    \n                    \n\n                    \n                    \u003cdiv\u003e\n                        \u003cdiv id=\"article-main-content\"\u003e\n\u003cp\u003eThe walking, talking, dancing Optimus robots at the recent Tesla demonstration generated \u003ca href=\"https://x.com/HowThingsWork_/status/1844735472418931145\" target=\"_blank\" rel=\"nofollow noopener\"\u003ehuge excitement\u003c/a\u003e. But this turned to \u003ca href=\"https://gizmodo.com/elon-musks-beer-pouring-optimus-robots-are-not-autonomous-2000510899\" target=\"_blank\" rel=\"nofollow noopener\"\u003edisappointment\u003c/a\u003e as it \u003ca href=\"https://www.bloomberg.com/news/articles/2024-10-14/tesla-s-optimus-robots-were-remotely-operated-at-cybercab-event\" target=\"_blank\" rel=\"nofollow noopener\"\u003ebecame apparent\u003c/a\u003e that \u003ca href=\"https://fortune.com/2024/10/13/elon-musk-tesla-optimus-robot-tele-operated-robotaxi/\" target=\"_blank\" rel=\"nofollow noopener\"\u003emuch of\u003c/a\u003e what was happening was actually being controlled remotely by humans.\u003c/p\u003e\n\u003cp\u003eAs much as this might still be a fascinating glimpse of the future, it’s not the first time that robots have turned out to be a little too good to be true.\u003c/p\u003e\n\u003cp\u003eTake Sophia, for instance, the robot created by Texas-based Hanson Robotics back in 2016. She was presented by the company as essentially an intelligent being, prompting \u003ca href=\"https://www.theverge.com/2018/1/18/16904742/sophia-the-robot-ai-real-fake-yann-lecun-criticism\" target=\"_blank\" rel=\"nofollow noopener\"\u003enumerous tech specialists\u003c/a\u003e to \u003ca href=\"https://www.youtube.com/watch?v=CE4pGzmucig\" target=\"_blank\" rel=\"nofollow noopener\"\u003ecall this out\u003c/a\u003e as well beyond our capabilities at the time.\u003c/p\u003e\n\u003cp\u003eSimilarly we’ve seen carefully choreographed videos of pre-scripted action sequences like Boston Dynamics’ \u003ca href=\"https://www.cnet.com/tech/farewell-to-boston-dynamics-atlas-robot-hilarious-bloopers-and-triumphs-reel/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eAtlas gymnastics\u003c/a\u003e, the English-made \u003ca href=\"https://www.youtube.com/watch?v=o6nej26lAhM\" target=\"_blank\" rel=\"nofollow noopener\"\u003eAmeca robot “waking up”\u003c/a\u003e, and most recently Tesla’s Optimus \u003ca href=\"https://x.com/Tesla_Optimus/status/1846797392521167223\" target=\"_blank\" rel=\"nofollow noopener\"\u003ein the factory\u003c/a\u003e. Obviously these are still impressive in different ways, but they’re nowhere near the complete sentient package. Let Optimus or Atlas loose in a random home and you’d see something very different.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cp\u003e\u003ciframe srcdoc=\"\u0026lt;style\u0026gt;*{padding:0;margin:0;overflow:hidden}html,body{background:#000;height:100%}img{position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover;transition:opacity .1s cubic-bezier(0.4,0,1,1)}a:hover img+img{opacity:1!important}\u0026lt;/style\u0026gt;\u0026lt;a href=\u0026#39;https://www.youtube.com/embed/7f53c_C7yic?feature=oembed\u0026amp;autoplay=1\u0026amp;mute=1\u0026amp;modestbranding=1\u0026amp;iv_load_policy=3\u0026amp;theme=light\u0026amp;playsinline=1\u0026#39;\u0026gt;\u0026lt;img src=\u0026#39;https://img.youtube.com/vi/7f53c_C7yic/hqdefault.jpg\u0026#39;\u0026gt;\u0026lt;img src=\u0026#39;https://cdn0.tnwcdn.com/wp-content/themes/cyberdelia/assets/img/ytplaybtn.png\u0026#39; style=\u0026#39;top: 50%;left:50%;width:68px;height:48px;transform:translate3d(-50%,-50%,0)\u0026#39;\u0026gt;\u0026lt;img src=\u0026#39;https://cdn0.tnwcdn.com/wp-content/themes/cyberdelia/assets/img/ytplaybtn-hover.png\u0026#39; style=\u0026#39;top: 50%;left:50%;width:68px;height:48px;opacity:0;transform:translate3d(-50%,-50%,0)\u0026#39;\u0026gt;\u0026lt;/a\u0026gt;\" width=\"440\" height=\"260\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003c/figure\u003e\n\u003cp\u003eA humanoid robot capable of working in our homes needs to be capable of doing many different tasks, using our tools, navigating our environments and communicating with us like a human. If you thought this was just a year or two away, you’re going to be disappointed.\u003c/p\u003e\n\u003cp\u003eBuilding robots able to interact and carry out complex tasks in our homes and streets is still a huge challenge. Designing them even to do one specific task well, such as opening a door, is phenomenally difficult.\u003c/p\u003e\n\u003cp\u003eThere are so many door handles with different shapes, weights and materials, not to mention the complexity of dealing with unforeseen circumstances such as a locked door or objects blocking the way. \u003ca href=\"https://thenextweb.com/topic/software-developer\" target=\"_blank\" rel=\"noopener\"\u003eDevelopers\u003c/a\u003e have actually \u003ca href=\"https://www.newscientist.com/article/2415590-this-robot-can-figure-out-how-to-open-almost-any-door-on-its-own/\" target=\"_blank\" rel=\"nofollow noopener\"\u003enow created\u003c/a\u003e a door-opening robot, but robots that can deal with hundreds of everyday tasks are still some way off.\u003c/p\u003e\n\u003ch2\u003eBehind the curtain\u003c/h2\u003e\n\u003cp\u003eThe Tesla demonstration’s “Wizard of Oz” remote operation technique is a commonly used control method in this field, giving researchers a benchmark against which to test their real advances. Known as telemetric control, this has been around for some time, and is becoming more advanced.\u003c/p\u003e\n\u003cp\u003eOne of the authors of this article, Carl Strathearn, was at a conference in Japan earlier this year, where a keynote speaker from one of the top \u003ca href=\"https://thenextweb.com/topic/robotics\" target=\"_blank\" rel=\"noopener\"\u003erobotics\u003c/a\u003e labs demonstrated an advanced telemetrics system. It allowed a single human to simultaneously operate many humanoid robots semi-autonomously, using pre-scripted movements, conversation prompts and computerised speech.\u003c/p\u003e\n\u003cp\u003eClearly, this is very useful technology. Telemetric systems are used to control robots working in \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10741399/\" target=\"_blank\" rel=\"nofollow noopener\"\u003edangerous environments\u003c/a\u003e, \u003ca href=\"https://www.bbc.co.uk/news/technology-46466531\" target=\"_blank\" rel=\"nofollow noopener\"\u003edisability healthcare\u003c/a\u003e and even in \u003ca href=\"https://www.reuters.com/science/humanoid-robots-space-next-frontier-2023-12-27/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eouter space\u003c/a\u003e. But the reason why a human is still at the helm is because even the most advanced humanoid robots, such as Atlas, are not yet reliable enough to operate completely independently in the real world.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cp\u003e\u003ciframe srcdoc=\"\u0026lt;style\u0026gt;*{padding:0;margin:0;overflow:hidden}html,body{background:#000;height:100%}img{position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover;transition:opacity .1s cubic-bezier(0.4,0,1,1)}a:hover img+img{opacity:1!important}\u0026lt;/style\u0026gt;\u0026lt;a href=\u0026#39;https://www.youtube.com/embed/tF4DML7FIWk?feature=oembed\u0026amp;autoplay=1\u0026amp;mute=1\u0026amp;modestbranding=1\u0026amp;iv_load_policy=3\u0026amp;theme=light\u0026amp;playsinline=1\u0026#39;\u0026gt;\u0026lt;img src=\u0026#39;https://img.youtube.com/vi/tF4DML7FIWk/hqdefault.jpg\u0026#39;\u0026gt;\u0026lt;img src=\u0026#39;https://cdn0.tnwcdn.com/wp-content/themes/cyberdelia/assets/img/ytplaybtn.png\u0026#39; style=\u0026#39;top: 50%;left:50%;width:68px;height:48px;transform:translate3d(-50%,-50%,0)\u0026#39;\u0026gt;\u0026lt;img src=\u0026#39;https://cdn0.tnwcdn.com/wp-content/themes/cyberdelia/assets/img/ytplaybtn-hover.png\u0026#39; style=\u0026#39;top: 50%;left:50%;width:68px;height:48px;opacity:0;transform:translate3d(-50%,-50%,0)\u0026#39;\u0026gt;\u0026lt;/a\u0026gt;\" width=\"440\" height=\"260\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003c/figure\u003e\n\u003cp\u003eAnother major problem is what we can call social AI. Leading generative AI programs such as \u003ca href=\"https://www.technologyreview.com/2023/12/06/1084471/google-deepminds-new-gemini-model-looks-amazing-but-could-signal-peak-ai-hype/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eDeepMind’s Gemini\u003c/a\u003e and OpenAI’s \u003ca href=\"https://help.openai.com/en/articles/8555496-gpt-4-vision-api\" target=\"_blank\" rel=\"nofollow noopener\"\u003eGPT-4 Vision\u003c/a\u003e may be a foundation for creative autonomous AI systems for humanoid robots in the future. But we should not be misled into believing that such models mean that a robot is now capable of functioning well in the real world.\u003c/p\u003e\n\u003cp\u003eInterpreting information and problem solving like a human requires much more than just recognising words, classifying objects and generating speech. It requires a deeper contextual understanding of people, objects and environments – in other words, common sense.\u003c/p\u003e\n\u003cp\u003eTo explore what is currently possible, we recently completed a \u003ca href=\"https://cui.acm.org/workshops/HRI2023/pdfs/HRCI23_paper_5.pdf\" target=\"_blank\" rel=\"nofollow noopener\"\u003eresearch project\u003c/a\u003e called Common Sense Enhanced Language and Vision (CiViL). We equipped a robot called Euclid with commonsense knowledge as part of a generative AI vision and language system to assist people in preparing recipes. To do this, we had to create commonsense knowledge databases using real-world problem-solving examples enacted by students.\u003c/p\u003e\n\u003cp\u003eEuclid could explain complicated steps in recipes, give suggestions when things went wrong, and even point people to locations in the kitchen where utensils and tools might typically be found. Yet there were still issues, such as what to do if someone has a bad allergic reaction while cooking. The problem is that it’s almost impossible to handle every possible scenario, yet that’s what true common sense entails.\u003c/p\u003e\n\u003cp\u003eThis fundamental aspect of AI has got somewhat lost in humanoid robots over the years. Generated speech, realistic facial expressions, telemetric controls, even the ability to play games such as “rock paper scissors” are all impressive. But the novelty soon wears off if the robots are not actually capable of doing anything useful on their own.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cp\u003e\u003ciframe srcdoc=\"\u0026lt;style\u0026gt;*{padding:0;margin:0;overflow:hidden}html,body{background:#000;height:100%}img{position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover;transition:opacity .1s cubic-bezier(0.4,0,1,1)}a:hover img+img{opacity:1!important}\u0026lt;/style\u0026gt;\u0026lt;a href=\u0026#39;https://www.youtube.com/embed/3nxjjztQKtY?feature=oembed\u0026amp;autoplay=1\u0026amp;mute=1\u0026amp;modestbranding=1\u0026amp;iv_load_policy=3\u0026amp;theme=light\u0026amp;playsinline=1\u0026#39;\u0026gt;\u0026lt;img src=\u0026#39;https://img.youtube.com/vi/3nxjjztQKtY/hqdefault.jpg\u0026#39;\u0026gt;\u0026lt;img src=\u0026#39;https://cdn0.tnwcdn.com/wp-content/themes/cyberdelia/assets/img/ytplaybtn.png\u0026#39; style=\u0026#39;top: 50%;left:50%;width:68px;height:48px;transform:translate3d(-50%,-50%,0)\u0026#39;\u0026gt;\u0026lt;img src=\u0026#39;https://cdn0.tnwcdn.com/wp-content/themes/cyberdelia/assets/img/ytplaybtn-hover.png\u0026#39; style=\u0026#39;top: 50%;left:50%;width:68px;height:48px;opacity:0;transform:translate3d(-50%,-50%,0)\u0026#39;\u0026gt;\u0026lt;/a\u0026gt;\" width=\"440\" height=\"260\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003c/figure\u003e\n\u003cp\u003eThis isn’t to say that significant progress \u003ca href=\"https://humanoid-ai.github.io/HumanPlus.pdf\" target=\"_blank\" rel=\"nofollow noopener\"\u003eisn’t being made\u003c/a\u003e toward autonomous humanoid robots. There’s impressive work going on into \u003ca href=\"https://www.newscientist.com/article/2274084-artificial-nervous-system-senses-light-and-learns-to-catch-like-humans/\" target=\"_blank\" rel=\"nofollow noopener\"\u003erobotic nervous systems\u003c/a\u003e to give robots more senses for learning, for instance. It’s just not usually given the same amount of press attention as the big unveilings.\u003c/p\u003e\n\u003ch2\u003eThe data deficit\u003c/h2\u003e\n\u003cp\u003eAnother key challenge is the lack of real-world data to train AI systems, since online data doesn’t always accurately represent the real-world conditions necessary for training our robots well enough. We have yet to find an effective way of collecting this real-world data in large enough quantities to get good results. However, this may change soon if we can access it from technologies such as Alexa and \u003ca href=\"https://www.ray-ban.com/uk/ray-ban-meta-smart-glasses\" target=\"_blank\" rel=\"nofollow noopener\"\u003eMeta Ray-Bans\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNonetheless, the reality is that we’re still perhaps decades away from developing multimodal humanoid robots with advanced social AI that are capable of helping around the house. Maybe in the meantime we’ll be offered robots controlled remotely from a command centre. Will we want them, though?\u003c/p\u003e\n\u003cp\u003eIn the meantime, it’s also more important that we focus our efforts on creating robots for roles that can support people who urgently need help now. Examples would \u003ca href=\"https://fsi.stanford.edu/news/robots-may-be-right-prescription-struggling-nursing-homes\" target=\"_blank\" rel=\"nofollow noopener\"\u003einclude healthcare\u003c/a\u003e, where there are long waiting lists and understaffed hospitals; \u003ca href=\"https://schoolsweek.co.uk/how-robots-are-helping-anxious-pupils-go-back-to-school/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eand education\u003c/a\u003e, to offer a way for overanxious or severely ill children to participate in classrooms remotely. We also need better transparency, legislation and publicly available testing, so that everyone can tell fact from fiction and help build public trust for when the robots eventually do arrive.\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://counter.theconversation.com/content/241638/count.gif?distributor=republish-lightbox-basic\" alt=\"The Conversation\" width=\"1\" height=\"1\" srcset=\"\" data-old-src=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003ca href=\"https://theconversation.com/profiles/carl-strathearn-1196677\" target=\"_blank\" rel=\"nofollow noopener\"\u003eCarl Strathearn\u003c/a\u003e, Research Fellow, Computing, \u003ca href=\"https://theconversation.com/institutions/edinburgh-napier-university-696\" target=\"_blank\" rel=\"nofollow noopener\"\u003eEdinburgh Napier University\u003c/a\u003e and \u003ca href=\"https://theconversation.com/profiles/dimitra-gkatzia-340620\" target=\"_blank\" rel=\"nofollow noopener\"\u003eDimitra Gkatzia\u003c/a\u003e, Associate Professor in Computing, \u003ca href=\"https://theconversation.com/institutions/edinburgh-napier-university-696\" target=\"_blank\" rel=\"nofollow noopener\"\u003eEdinburgh Napier University\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThis article is republished from \u003ca href=\"https://theconversation.com\" target=\"_blank\" rel=\"nofollow noopener\"\u003eThe Conversation\u003c/a\u003e under a Creative Commons license. Read the \u003ca href=\"https://theconversation.com/robot-developers-keep-making-it-seem-like-housebots-are-imminent-when-theyre-decades-away-241638\" target=\"_blank\" rel=\"nofollow noopener\"\u003eoriginal article\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n                        \n\n                        \u003cdiv id=\"nl-container\"\u003e\n                                                        \u003ch2\u003eGet the TNW newsletter\u003c/h2\u003e\n                            \u003cp\u003eGet the most important tech news in your inbox each week.\u003c/p\u003e\n                            \n                        \u003c/div\u003e\n\n                        \n                                                    \u003ch2\u003eAlso tagged with\u003c/h2\u003e\n\n                            \u003cbr/\u003e\n\n                            \n                        \n                        \n\n                        \n                    \u003c/div\u003e\n                    \n\n                    \n                \u003c/article\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-10-28T09:00:24Z",
  "modifiedTime": "2024-10-22T13:05:02Z"
}
