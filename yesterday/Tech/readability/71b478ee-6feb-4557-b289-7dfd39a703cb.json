{
  "id": "71b478ee-6feb-4557-b289-7dfd39a703cb",
  "title": "Show HN: Real-time AI Voice Chat at ~500ms Latency",
  "link": "https://github.com/KoljaB/RealtimeVoiceChat",
  "description": "Comments",
  "author": "",
  "published": "Mon, 05 May 2025 20:17:32 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "KoljaB",
  "length": 9683,
  "excerpt": "Have a natural, spoken conversation with AI! Contribute to KoljaB/RealtimeVoiceChat development by creating an account on GitHub.",
  "siteName": "GitHub",
  "favicon": "https://github.githubassets.com/assets/apple-touch-icon-180x180-a80b8e11abe2.png",
  "text": "Real-Time AI Voice Chat üé§üí¨üß†üîä Have a natural, spoken conversation with an AI! This project lets you chat with a Large Language Model (LLM) using just your voice, receiving spoken responses in near real-time. Think of it as your own digital conversation partner. FastVoiceTalk_compressed_step3_h264.mp4 (early preview - first reasonably stable version) What's Under the Hood? A sophisticated client-server system built for low-latency interaction: üéôÔ∏è Capture: Your voice is captured by your browser. ‚û°Ô∏è Stream: Audio chunks are whisked away via WebSockets to a Python backend. ‚úçÔ∏è Transcribe: RealtimeSTT rapidly converts your speech to text. ü§î Think: The text is sent to an LLM (like Ollama or OpenAI) for processing. üó£Ô∏è Synthesize: The AI's text response is turned back into speech using RealtimeTTS. ‚¨ÖÔ∏è Return: The generated audio is streamed back to your browser for playback. üîÑ Interrupt: Jump in anytime! The system handles interruptions gracefully. Key Features ‚ú® Fluid Conversation: Speak and listen, just like a real chat. Real-Time Feedback: See partial transcriptions and AI responses as they happen. Low Latency Focus: Optimized architecture using audio chunk streaming. Smart Turn-Taking: Dynamic silence detection (turndetect.py) adapts to the conversation pace. Flexible AI Brains: Pluggable LLM backends (Ollama default, OpenAI support via llm_module.py). Customizable Voices: Choose from different Text-to-Speech engines (Kokoro, Coqui, Orpheus via audio_module.py). Web Interface: Clean and simple UI using Vanilla JS and the Web Audio API. Dockerized Deployment: Recommended setup using Docker Compose for easier dependency management. Technology Stack üõ†Ô∏è Backend: Python 3.x, FastAPI Frontend: HTML, CSS, JavaScript (Vanilla JS, Web Audio API, AudioWorklets) Communication: WebSockets Containerization: Docker, Docker Compose Core AI/ML Libraries: RealtimeSTT (Speech-to-Text) RealtimeTTS (Text-to-Speech) transformers (Turn detection, Tokenization) torch / torchaudio (ML Framework) ollama / openai (LLM Clients) Audio Processing: numpy, scipy Before You Dive In: Prerequisites üèä‚Äç‚ôÄÔ∏è This project leverages powerful AI models, which have some requirements: Operating System: Docker: Linux is recommended for the best GPU integration with Docker. Manual: The provided script (install.bat) is for Windows. Manual steps are possible on Linux/macOS but may require more troubleshooting (especially for DeepSpeed). üêç Python: 3.9 or higher (if setting up manually). üöÄ GPU: A powerful CUDA-enabled NVIDIA GPU is highly recommended, especially for faster STT (Whisper) and TTS (Coqui). Performance on CPU-only or weaker GPUs will be significantly slower. The setup assumes CUDA 12.1. Adjust PyTorch installation if you have a different CUDA version. Docker (Linux): Requires NVIDIA Container Toolkit. üê≥ Docker (Optional but Recommended): Docker Engine and Docker Compose v2+ for the containerized setup. üß† Ollama (Optional): If using the Ollama backend without Docker, install it separately and pull your desired models. The Docker setup includes an Ollama service. üîë OpenAI API Key (Optional): If using the OpenAI backend, set the OPENAI_API_KEY environment variable (e.g., in a .env file or passed to Docker). Getting Started: Installation \u0026 Setup ‚öôÔ∏è Clone the repository first: git clone https://github.com/KoljaB/RealtimeVoiceChat.git cd RealtimeVoiceChat Now, choose your adventure: üöÄ Option A: Docker Installation (Recommended for Linux/GPU) This is the most straightforward method, bundling the application, dependencies, and even Ollama into manageable containers. Build the Docker images: (This takes time! It downloads base images, installs Python/ML dependencies, and pre-downloads the default STT model.) (If you want to customize models/settings in code/*.py, do it before this step!) Start the services (App \u0026 Ollama): (Runs containers in the background. GPU access is configured in docker-compose.yml.) Give them a minute to initialize. (Crucial!) Pull your desired Ollama Model: (This is done after startup to keep the main app image smaller and allow model changes without rebuilding. Execute this command to pull the default model into the running Ollama container.) # Pull the default model (adjust if you configured a different one in server.py) docker compose exec ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M # (Optional) Verify the model is available docker compose exec ollama ollama list Stopping the Services: Restarting: Viewing Logs / Debugging: Follow app logs: docker compose logs -f app Follow Ollama logs: docker compose logs -f ollama Save logs to file: docker compose logs app \u003e app_logs.txt üõ†Ô∏è Option B: Manual Installation (Windows Script / venv) This method requires managing the Python environment yourself. It offers more direct control but can be trickier, especially regarding ML dependencies. B1) Using the Windows Install Script: Ensure you meet the prerequisites (Python, potentially CUDA drivers). Run the script. It attempts to create a venv, install PyTorch for CUDA 12.1, a compatible DeepSpeed wheel, and other requirements. (This opens a new command prompt within the activated virtual environment.) Proceed to the \"Running the Application\" section. B2) Manual Steps (Linux/macOS/Windows): Create \u0026 Activate Virtual Environment: python -m venv venv # Linux/macOS: source venv/bin/activate # Windows: .\\venv\\Scripts\\activate Upgrade Pip: python -m pip install --upgrade pip Navigate to Code Directory: Install PyTorch (Crucial Step - Match Your Hardware!): With NVIDIA GPU (CUDA 12.1 Example): # Verify your CUDA version! Adjust 'cu121' and the URL if needed. pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121 CPU Only (Expect Slow Performance): # pip install torch torchaudio torchvision Find other PyTorch versions: https://pytorch.org/get-started/previous-versions/ Install Other Requirements: pip install -r requirements.txt Note on DeepSpeed: The requirements.txt may include DeepSpeed. Installation can be complex, especially on Windows. The install.bat tries a precompiled wheel. If manual installation fails, you might need to build it from source or consult resources like deepspeedpatcher (use at your own risk). Coqui TTS performance benefits most from DeepSpeed. Running the Application ‚ñ∂Ô∏è If using Docker: Your application is already running via docker compose up -d! Check logs using docker compose logs -f app. If using Manual/Script Installation: Activate your virtual environment (if not already active): # Linux/macOS: source ../venv/bin/activate # Windows: ..\\venv\\Scripts\\activate Navigate to the code directory (if not already there): Start the FastAPI server: Accessing the Client (Both Methods): Open your web browser to http://localhost:8000 (or your server's IP if running remotely/in Docker on another machine). Grant microphone permissions when prompted. Click \"Start\" to begin chatting! Use \"Stop\" to end and \"Reset\" to clear the conversation. Configuration Deep Dive üîß Want to tweak the AI's voice, brain, or how it listens? Modify the Python files in the code/ directory. ‚ö†Ô∏è Important Docker Note: If using Docker, make any configuration changes before running docker compose build to ensure they are included in the image. TTS Engine \u0026 Voice (server.py, audio_module.py): Change START_ENGINE in server.py to \"coqui\", \"kokoro\", or \"orpheus\". Adjust engine-specific settings (e.g., voice model path for Coqui, speaker ID for Orpheus, speed) within AudioProcessor.__init__ in audio_module.py. LLM Backend \u0026 Model (server.py, llm_module.py): Set LLM_START_PROVIDER (\"ollama\" or \"openai\") and LLM_START_MODEL (e.g., \"hf.co/...\" for Ollama, model name for OpenAI) in server.py. Remember to pull the Ollama model if using Docker (see Installation Step A3). Customize the AI's personality by editing system_prompt.txt. STT Settings (transcribe.py): Modify DEFAULT_RECORDER_CONFIG to change the Whisper model (model), language (language), silence thresholds (silence_limit_seconds), etc. The default base.en model is pre-downloaded during the Docker build. Turn Detection Sensitivity (turndetect.py): Adjust pause duration constants within the TurnDetector.update_settings method. SSL/HTTPS (server.py): Set USE_SSL = True and provide paths to your certificate (SSL_CERT_PATH) and key (SSL_KEY_PATH) files. Docker Users: You'll need to adjust docker-compose.yml to map the SSL port (e.g., 443) and potentially mount your certificate files as volumes. Generating Local SSL Certificates (Windows Example w/ mkcert) Install Chocolatey package manager if you haven't already. Install mkcert: choco install mkcert Run Command Prompt as Administrator. Install a local Certificate Authority: mkcert -install Generate certs (replace your.local.ip): mkcert localhost 127.0.0.1 ::1 your.local.ip This creates .pem files (e.g., localhost+3.pem and localhost+3-key.pem) in the current directory. Update SSL_CERT_PATH and SSL_KEY_PATH in server.py accordingly. Remember to potentially mount these into your Docker container. Contributing ü§ù Got ideas or found a bug? Contributions are welcome! Feel free to open issues or submit pull requests. License üìú The core codebase of this project is released under the MIT License (see the LICENSE file for details). This project relies on external specific TTS engines (like Coqui XTTSv2) and LLM providers which have their own licensing terms. Please ensure you comply with the licenses of all components you use.",
  "image": "https://opengraph.githubassets.com/6c93a67f8af258960fff4c15b847a7470128e667b9de5145a824ea0f1e6566bc/KoljaB/RealtimeVoiceChat",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-hpc=\"true\"\u003e\u003carticle itemprop=\"text\"\u003e\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eReal-Time AI Voice Chat üé§üí¨üß†üîä\u003c/h2\u003e\u003ca id=\"user-content-real-time-ai-voice-chat-\" aria-label=\"Permalink: Real-Time AI Voice Chat üé§üí¨üß†üîä\" href=\"#real-time-ai-voice-chat-\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHave a natural, spoken conversation with an AI!\u003c/strong\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThis project lets you chat with a Large Language Model (LLM) using just your voice, receiving spoken responses in near real-time. Think of it as your own digital conversation partner.\u003c/p\u003e\n\u003cdetails open=\"\"\u003e\n  \u003csummary\u003e\n    \n    \u003cspan aria-label=\"Video description FastVoiceTalk_compressed_step3_h264.mp4\"\u003eFastVoiceTalk_compressed_step3_h264.mp4\u003c/span\u003e\n    \u003cspan\u003e\u003c/span\u003e\n  \u003c/summary\u003e\n\n  \u003cvideo src=\"https://private-user-images.githubusercontent.com/7604638/440153612-16cc29a7-bec2-4dd0-a056-d213db798d8f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY0OTQ0MzUsIm5iZiI6MTc0NjQ5NDEzNSwicGF0aCI6Ii83NjA0NjM4LzQ0MDE1MzYxMi0xNmNjMjlhNy1iZWMyLTRkZDAtYTA1Ni1kMjEzZGI3OThkOGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDZUMDExNTM1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MzkyNzdjNTRkMDM5MWY4YWI2NWY3NTFmNzJlOTVhN2RhOTczZGRmODlhYWU0N2RkMGYwOWM2MWJkN2Y3OTViMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.GVC8YoP925IjOVl3rSeyy3Aye3HY5NfFMHlYLKmYZZ0\" data-canonical-src=\"https://private-user-images.githubusercontent.com/7604638/440153612-16cc29a7-bec2-4dd0-a056-d213db798d8f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY0OTQ0MzUsIm5iZiI6MTc0NjQ5NDEzNSwicGF0aCI6Ii83NjA0NjM4LzQ0MDE1MzYxMi0xNmNjMjlhNy1iZWMyLTRkZDAtYTA1Ni1kMjEzZGI3OThkOGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDZUMDExNTM1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MzkyNzdjNTRkMDM5MWY4YWI2NWY3NTFmNzJlOTVhN2RhOTczZGRmODlhYWU0N2RkMGYwOWM2MWJkN2Y3OTViMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.GVC8YoP925IjOVl3rSeyy3Aye3HY5NfFMHlYLKmYZZ0\" controls=\"controls\" muted=\"muted\"\u003e\n\n  \u003c/video\u003e\n\u003c/details\u003e\n\n\u003cp dir=\"auto\"\u003e\u003cem\u003e(early preview - first reasonably stable version)\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eWhat\u0026#39;s Under the Hood?\u003c/h2\u003e\u003ca id=\"user-content-whats-under-the-hood\" aria-label=\"Permalink: What\u0026#39;s Under the Hood?\" href=\"#whats-under-the-hood\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eA sophisticated client-server system built for low-latency interaction:\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003eüéôÔ∏è \u003cstrong\u003eCapture:\u003c/strong\u003e Your voice is captured by your browser.\u003c/li\u003e\n\u003cli\u003e‚û°Ô∏è \u003cstrong\u003eStream:\u003c/strong\u003e Audio chunks are whisked away via WebSockets to a Python backend.\u003c/li\u003e\n\u003cli\u003e‚úçÔ∏è \u003cstrong\u003eTranscribe:\u003c/strong\u003e \u003ccode\u003eRealtimeSTT\u003c/code\u003e rapidly converts your speech to text.\u003c/li\u003e\n\u003cli\u003eü§î \u003cstrong\u003eThink:\u003c/strong\u003e The text is sent to an LLM (like Ollama or OpenAI) for processing.\u003c/li\u003e\n\u003cli\u003eüó£Ô∏è \u003cstrong\u003eSynthesize:\u003c/strong\u003e The AI\u0026#39;s text response is turned back into speech using \u003ccode\u003eRealtimeTTS\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e‚¨ÖÔ∏è \u003cstrong\u003eReturn:\u003c/strong\u003e The generated audio is streamed back to your browser for playback.\u003c/li\u003e\n\u003cli\u003eüîÑ \u003cstrong\u003eInterrupt:\u003c/strong\u003e Jump in anytime! The system handles interruptions gracefully.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eKey Features ‚ú®\u003c/h2\u003e\u003ca id=\"user-content-key-features-\" aria-label=\"Permalink: Key Features ‚ú®\" href=\"#key-features-\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eFluid Conversation:\u003c/strong\u003e Speak and listen, just like a real chat.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-Time Feedback:\u003c/strong\u003e See partial transcriptions and AI responses as they happen.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLow Latency Focus:\u003c/strong\u003e Optimized architecture using audio chunk streaming.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSmart Turn-Taking:\u003c/strong\u003e Dynamic silence detection (\u003ccode\u003eturndetect.py\u003c/code\u003e) adapts to the conversation pace.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFlexible AI Brains:\u003c/strong\u003e Pluggable LLM backends (Ollama default, OpenAI support via \u003ccode\u003ellm_module.py\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustomizable Voices:\u003c/strong\u003e Choose from different Text-to-Speech engines (Kokoro, Coqui, Orpheus via \u003ccode\u003eaudio_module.py\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWeb Interface:\u003c/strong\u003e Clean and simple UI using Vanilla JS and the Web Audio API.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDockerized Deployment:\u003c/strong\u003e Recommended setup using Docker Compose for easier dependency management.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eTechnology Stack üõ†Ô∏è\u003c/h2\u003e\u003ca id=\"user-content-technology-stack-Ô∏è\" aria-label=\"Permalink: Technology Stack üõ†Ô∏è\" href=\"#technology-stack-Ô∏è\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eBackend:\u003c/strong\u003e Python 3.x, FastAPI\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e HTML, CSS, JavaScript (Vanilla JS, Web Audio API, AudioWorklets)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCommunication:\u003c/strong\u003e WebSockets\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContainerization:\u003c/strong\u003e Docker, Docker Compose\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCore AI/ML Libraries:\u003c/strong\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ccode\u003eRealtimeSTT\u003c/code\u003e (Speech-to-Text)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eRealtimeTTS\u003c/code\u003e (Text-to-Speech)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003etransformers\u003c/code\u003e (Turn detection, Tokenization)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003etorch\u003c/code\u003e / \u003ccode\u003etorchaudio\u003c/code\u003e (ML Framework)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eollama\u003c/code\u003e / \u003ccode\u003eopenai\u003c/code\u003e (LLM Clients)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAudio Processing:\u003c/strong\u003e \u003ccode\u003enumpy\u003c/code\u003e, \u003ccode\u003escipy\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eBefore You Dive In: Prerequisites üèä‚Äç‚ôÄÔ∏è\u003c/h2\u003e\u003ca id=\"user-content-before-you-dive-in-prerequisites-Ô∏è\" aria-label=\"Permalink: Before You Dive In: Prerequisites üèä‚Äç‚ôÄÔ∏è\" href=\"#before-you-dive-in-prerequisites-Ô∏è\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThis project leverages powerful AI models, which have some requirements:\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eOperating System:\u003c/strong\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eDocker:\u003c/strong\u003e Linux is recommended for the best GPU integration with Docker.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eManual:\u003c/strong\u003e The provided script (\u003ccode\u003einstall.bat\u003c/code\u003e) is for Windows. Manual steps are possible on Linux/macOS but may require more troubleshooting (especially for DeepSpeed).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eüêç Python:\u003c/strong\u003e 3.9 or higher (if setting up manually).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eüöÄ GPU:\u003c/strong\u003e \u003cstrong\u003eA powerful CUDA-enabled NVIDIA GPU is \u003cem\u003ehighly recommended\u003c/em\u003e\u003c/strong\u003e, especially for faster STT (Whisper) and TTS (Coqui). Performance on CPU-only or weaker GPUs will be significantly slower.\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eThe setup assumes \u003cstrong\u003eCUDA 12.1\u003c/strong\u003e. Adjust PyTorch installation if you have a different CUDA version.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDocker (Linux):\u003c/strong\u003e Requires \u003ca href=\"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html\" rel=\"nofollow\"\u003eNVIDIA Container Toolkit\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eüê≥ Docker (Optional but Recommended):\u003c/strong\u003e Docker Engine and Docker Compose v2+ for the containerized setup.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eüß† Ollama (Optional):\u003c/strong\u003e If using the Ollama backend \u003cem\u003ewithout\u003c/em\u003e Docker, install it separately and pull your desired models. The Docker setup includes an Ollama service.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eüîë OpenAI API Key (Optional):\u003c/strong\u003e If using the OpenAI backend, set the \u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e environment variable (e.g., in a \u003ccode\u003e.env\u003c/code\u003e file or passed to Docker).\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eGetting Started: Installation \u0026amp; Setup ‚öôÔ∏è\u003c/h2\u003e\u003ca id=\"user-content-getting-started-installation--setup-Ô∏è\" aria-label=\"Permalink: Getting Started: Installation \u0026amp; Setup ‚öôÔ∏è\" href=\"#getting-started-installation--setup-Ô∏è\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eClone the repository first:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"git clone https://github.com/KoljaB/RealtimeVoiceChat.git\ncd RealtimeVoiceChat\"\u003e\u003cpre\u003egit clone https://github.com/KoljaB/RealtimeVoiceChat.git\n\u003cspan\u003ecd\u003c/span\u003e RealtimeVoiceChat\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eNow, choose your adventure:\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eüöÄ Option A: Docker Installation (Recommended for Linux/GPU)\u003c/strong\u003e\u003c/summary\u003e\n\u003cp dir=\"auto\"\u003eThis is the most straightforward method, bundling the application, dependencies, and even Ollama into manageable containers.\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBuild the Docker images:\u003c/strong\u003e\n\u003cem\u003e(This takes time! It downloads base images, installs Python/ML dependencies, and pre-downloads the default STT model.)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp dir=\"auto\"\u003e\u003cem\u003e(If you want to customize models/settings in \u003ccode\u003ecode/*.py\u003c/code\u003e, do it \u003cstrong\u003ebefore\u003c/strong\u003e this step!)\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eStart the services (App \u0026amp; Ollama):\u003c/strong\u003e\n\u003cem\u003e(Runs containers in the background. GPU access is configured in \u003ccode\u003edocker-compose.yml\u003c/code\u003e.)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp dir=\"auto\"\u003eGive them a minute to initialize.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e(Crucial!) Pull your desired Ollama Model:\u003c/strong\u003e\n\u003cem\u003e(This is done \u003cem\u003eafter\u003c/em\u003e startup to keep the main app image smaller and allow model changes without rebuilding. Execute this command to pull the default model into the running Ollama container.)\u003c/em\u003e\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"# Pull the default model (adjust if you configured a different one in server.py)\ndocker compose exec ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M\n\n# (Optional) Verify the model is available\ndocker compose exec ollama ollama list\"\u003e\u003cpre\u003e\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e Pull the default model (adjust if you configured a different one in server.py)\u003c/span\u003e\ndocker compose \u003cspan\u003eexec\u003c/span\u003e ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M\n\n\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e (Optional) Verify the model is available\u003c/span\u003e\ndocker compose \u003cspan\u003eexec\u003c/span\u003e ollama ollama list\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eStopping the Services:\u003c/strong\u003e\u003c/p\u003e\n\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRestarting:\u003c/strong\u003e\u003c/p\u003e\n\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eViewing Logs / Debugging:\u003c/strong\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eFollow app logs: \u003ccode\u003edocker compose logs -f app\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eFollow Ollama logs: \u003ccode\u003edocker compose logs -f ollama\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSave logs to file: \u003ccode\u003edocker compose logs app \u0026gt; app_logs.txt\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/details\u003e\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eüõ†Ô∏è Option B: Manual Installation (Windows Script / venv)\u003c/strong\u003e\u003c/summary\u003e\n\u003cp dir=\"auto\"\u003eThis method requires managing the Python environment yourself. It offers more direct control but can be trickier, especially regarding ML dependencies.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eB1) Using the Windows Install Script:\u003c/strong\u003e\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003eEnsure you meet the prerequisites (Python, potentially CUDA drivers).\u003c/li\u003e\n\u003cli\u003eRun the script. It attempts to create a venv, install PyTorch for CUDA 12.1, a compatible DeepSpeed wheel, and other requirements.\n\n\u003cem\u003e(This opens a new command prompt within the activated virtual environment.)\u003c/em\u003e\nProceed to the \u003cstrong\u003e\u0026#34;Running the Application\u0026#34;\u003c/strong\u003e section.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eB2) Manual Steps (Linux/macOS/Windows):\u003c/strong\u003e\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCreate \u0026amp; Activate Virtual Environment:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"python -m venv venv\n# Linux/macOS:\nsource venv/bin/activate\n# Windows:\n.\\venv\\Scripts\\activate\"\u003e\u003cpre\u003epython -m venv venv\n\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e Linux/macOS:\u003c/span\u003e\n\u003cspan\u003esource\u003c/span\u003e venv/bin/activate\n\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e Windows:\u003c/span\u003e\n.\u003cspan\u003e\\v\u003c/span\u003eenv\u003cspan\u003e\\S\u003c/span\u003ecripts\u003cspan\u003e\\a\u003c/span\u003ectivate\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eUpgrade Pip:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"python -m pip install --upgrade pip\"\u003e\u003cpre\u003epython -m pip install --upgrade pip\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eNavigate to Code Directory:\u003c/strong\u003e\u003c/p\u003e\n\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eInstall PyTorch (Crucial Step - Match Your Hardware!):\u003c/strong\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eWith NVIDIA GPU (CUDA 12.1 Example):\u003c/strong\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"# Verify your CUDA version! Adjust \u0026#39;cu121\u0026#39; and the URL if needed.\npip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121\"\u003e\u003cpre\u003e\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e Verify your CUDA version! Adjust \u0026#39;cu121\u0026#39; and the URL if needed.\u003c/span\u003e\npip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCPU Only (Expect Slow Performance):\u003c/strong\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"# pip install torch torchaudio torchvision\"\u003e\u003cpre\u003e\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e pip install torch torchaudio torchvision\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eFind other PyTorch versions:\u003c/em\u003e \u003ca href=\"https://pytorch.org/get-started/previous-versions/\" rel=\"nofollow\"\u003ehttps://pytorch.org/get-started/previous-versions/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eInstall Other Requirements:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"pip install -r requirements.txt\"\u003e\u003cpre\u003epip install -r requirements.txt\u003c/pre\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eNote on DeepSpeed:\u003c/strong\u003e The \u003ccode\u003erequirements.txt\u003c/code\u003e may include DeepSpeed. Installation can be complex, especially on Windows. The \u003ccode\u003einstall.bat\u003c/code\u003e tries a precompiled wheel. If manual installation fails, you might need to build it from source or consult resources like \u003ca href=\"https://github.com/erew123/deepspeedpatcher\"\u003edeepspeedpatcher\u003c/a\u003e (use at your own risk). Coqui TTS performance benefits most from DeepSpeed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/details\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eRunning the Application \u003cg-emoji alias=\"arrow_forward\"\u003e‚ñ∂Ô∏è\u003c/g-emoji\u003e\u003c/h2\u003e\u003ca id=\"user-content-running-the-application-Ô∏è\" aria-label=\"Permalink: Running the Application ‚ñ∂Ô∏è\" href=\"#running-the-application-Ô∏è\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eIf using Docker:\u003c/strong\u003e\nYour application is already running via \u003ccode\u003edocker compose up -d\u003c/code\u003e! Check logs using \u003ccode\u003edocker compose logs -f app\u003c/code\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eIf using Manual/Script Installation:\u003c/strong\u003e\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eActivate your virtual environment\u003c/strong\u003e (if not already active):\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"# Linux/macOS: source ../venv/bin/activate\n# Windows: ..\\venv\\Scripts\\activate\"\u003e\u003cpre\u003e\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e Linux/macOS: source ../venv/bin/activate\u003c/span\u003e\n\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e Windows: ..\\venv\\Scripts\\activate\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNavigate to the \u003ccode\u003ecode\u003c/code\u003e directory\u003c/strong\u003e (if not already there):\n\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStart the FastAPI server:\u003c/strong\u003e\n\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAccessing the Client (Both Methods):\u003c/strong\u003e\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003eOpen your web browser to \u003ccode\u003ehttp://localhost:8000\u003c/code\u003e (or your server\u0026#39;s IP if running remotely/in Docker on another machine).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrant microphone permissions\u003c/strong\u003e when prompted.\u003c/li\u003e\n\u003cli\u003eClick \u003cstrong\u003e\u0026#34;Start\u0026#34;\u003c/strong\u003e to begin chatting! Use \u0026#34;Stop\u0026#34; to end and \u0026#34;Reset\u0026#34; to clear the conversation.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eConfiguration Deep Dive üîß\u003c/h2\u003e\u003ca id=\"user-content-configuration-deep-dive-\" aria-label=\"Permalink: Configuration Deep Dive üîß\" href=\"#configuration-deep-dive-\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWant to tweak the AI\u0026#39;s voice, brain, or how it listens? Modify the Python files in the \u003ccode\u003ecode/\u003c/code\u003e directory.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e\u003cg-emoji alias=\"warning\"\u003e‚ö†Ô∏è\u003c/g-emoji\u003e Important Docker Note:\u003c/strong\u003e If using Docker, make any configuration changes \u003cem\u003ebefore\u003c/em\u003e running \u003ccode\u003edocker compose build\u003c/code\u003e to ensure they are included in the image.\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTTS Engine \u0026amp; Voice (\u003ccode\u003eserver.py\u003c/code\u003e, \u003ccode\u003eaudio_module.py\u003c/code\u003e):\u003c/strong\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eChange \u003ccode\u003eSTART_ENGINE\u003c/code\u003e in \u003ccode\u003eserver.py\u003c/code\u003e to \u003ccode\u003e\u0026#34;coqui\u0026#34;\u003c/code\u003e, \u003ccode\u003e\u0026#34;kokoro\u0026#34;\u003c/code\u003e, or \u003ccode\u003e\u0026#34;orpheus\u0026#34;\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eAdjust engine-specific settings (e.g., voice model path for Coqui, speaker ID for Orpheus, speed) within \u003ccode\u003eAudioProcessor.__init__\u003c/code\u003e in \u003ccode\u003eaudio_module.py\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLLM Backend \u0026amp; Model (\u003ccode\u003eserver.py\u003c/code\u003e, \u003ccode\u003ellm_module.py\u003c/code\u003e):\u003c/strong\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eSet \u003ccode\u003eLLM_START_PROVIDER\u003c/code\u003e (\u003ccode\u003e\u0026#34;ollama\u0026#34;\u003c/code\u003e or \u003ccode\u003e\u0026#34;openai\u0026#34;\u003c/code\u003e) and \u003ccode\u003eLLM_START_MODEL\u003c/code\u003e (e.g., \u003ccode\u003e\u0026#34;hf.co/...\u0026#34;\u003c/code\u003e for Ollama, model name for OpenAI) in \u003ccode\u003eserver.py\u003c/code\u003e. Remember to pull the Ollama model if using Docker (see Installation Step A3).\u003c/li\u003e\n\u003cli\u003eCustomize the AI\u0026#39;s personality by editing \u003ccode\u003esystem_prompt.txt\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSTT Settings (\u003ccode\u003etranscribe.py\u003c/code\u003e):\u003c/strong\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eModify \u003ccode\u003eDEFAULT_RECORDER_CONFIG\u003c/code\u003e to change the Whisper model (\u003ccode\u003emodel\u003c/code\u003e), language (\u003ccode\u003elanguage\u003c/code\u003e), silence thresholds (\u003ccode\u003esilence_limit_seconds\u003c/code\u003e), etc. The default \u003ccode\u003ebase.en\u003c/code\u003e model is pre-downloaded during the Docker build.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTurn Detection Sensitivity (\u003ccode\u003eturndetect.py\u003c/code\u003e):\u003c/strong\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eAdjust pause duration constants within the \u003ccode\u003eTurnDetector.update_settings\u003c/code\u003e method.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSSL/HTTPS (\u003ccode\u003eserver.py\u003c/code\u003e):\u003c/strong\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eSet \u003ccode\u003eUSE_SSL = True\u003c/code\u003e and provide paths to your certificate (\u003ccode\u003eSSL_CERT_PATH\u003c/code\u003e) and key (\u003ccode\u003eSSL_KEY_PATH\u003c/code\u003e) files.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDocker Users:\u003c/strong\u003e You\u0026#39;ll need to adjust \u003ccode\u003edocker-compose.yml\u003c/code\u003e to map the SSL port (e.g., 443) and potentially mount your certificate files as volumes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eGenerating Local SSL Certificates (Windows Example w/ mkcert)\u003c/strong\u003e\u003c/summary\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003eInstall Chocolatey package manager if you haven\u0026#39;t already.\u003c/li\u003e\n\u003cli\u003eInstall mkcert: \u003ccode\u003echoco install mkcert\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eRun Command Prompt \u003cem\u003eas Administrator\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eInstall a local Certificate Authority: \u003ccode\u003emkcert -install\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eGenerate certs (replace \u003ccode\u003eyour.local.ip\u003c/code\u003e): \u003ccode\u003emkcert localhost 127.0.0.1 ::1 your.local.ip\u003c/code\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eThis creates \u003ccode\u003e.pem\u003c/code\u003e files (e.g., \u003ccode\u003elocalhost+3.pem\u003c/code\u003e and \u003ccode\u003elocalhost+3-key.pem\u003c/code\u003e) in the current directory. Update \u003ccode\u003eSSL_CERT_PATH\u003c/code\u003e and \u003ccode\u003eSSL_KEY_PATH\u003c/code\u003e in \u003ccode\u003eserver.py\u003c/code\u003e accordingly. Remember to potentially mount these into your Docker container.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/details\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eContributing ü§ù\u003c/h2\u003e\u003ca id=\"user-content-contributing-\" aria-label=\"Permalink: Contributing ü§ù\" href=\"#contributing-\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eGot ideas or found a bug? Contributions are welcome! Feel free to open issues or submit pull requests.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eLicense üìú\u003c/h2\u003e\u003ca id=\"user-content-license-\" aria-label=\"Permalink: License üìú\" href=\"#license-\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe core codebase of this project is released under the \u003cstrong\u003eMIT License\u003c/strong\u003e (see the \u003ca href=\"https://github.com/KoljaB/RealtimeVoiceChat/blob/main/LICENSE\"\u003eLICENSE\u003c/a\u003e file for details).\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThis project relies on external specific TTS engines (like \u003ccode\u003eCoqui XTTSv2\u003c/code\u003e) and LLM providers which have their \u003cstrong\u003eown licensing terms\u003c/strong\u003e. Please ensure you comply with the licenses of all components you use.\u003c/p\u003e\n\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": null,
  "modifiedTime": null
}
