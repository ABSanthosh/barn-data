{
  "id": "26766746-d29d-4685-9bba-66e085c41d3e",
  "title": "2024: The year AI drove everyone crazy",
  "link": "https://arstechnica.com/ai/2024/12/2024-the-year-ai-drove-everyone-crazy/",
  "description": "What do eating rocks, rat genitals, and Willy Wonka have in common? AI, of course.",
  "author": "Benj Edwards",
  "published": "Thu, 26 Dec 2024 12:00:25 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "Features",
    "AI jabberwocky",
    "air canada",
    "Anthropic",
    "ChatGPT",
    "confabulation",
    "google",
    "GPT-4o",
    "jabberwocky",
    "machine learning",
    "meta",
    "openai",
    "OpenAI o1",
    "rat genitals",
    "Wonka"
  ],
  "byline": "Benj Edwards",
  "length": 20238,
  "excerpt": "What do eating rocks, rat genitals, and Willy Wonka have in common? AI, of course.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "What do eating rocks, rat genitals, and Willy Wonka have in common? AI, of course. It's been a wild year in tech thanks to the intersection between humans and artificial intelligence. 2024 brought a parade of AI oddities, mishaps, and wacky moments that inspired odd behavior from both machines and man. From AI-generated rat genitals to search engines telling people to eat rocks, this year proved that AI has been having a weird impact on the world. Why the weirdness? If we had to guess, it may be due to the novelty of it all. Generative AI and applications built upon Transformer-based AI models are still so new that people are throwing everything at the wall to see what sticks. People have been struggling to grasp both the implications and potential applications of the new technology. Riding along with the hype, different types of AI that may end up being ill-advised, such as automated military targeting systems, have also been introduced. It's worth mentioning that aside from crazy news, we saw fewer weird AI advances in 2024 as well. For example, Claude 3.5 Sonnet launched in June held off the competition as a top model for most of the year, while OpenAI's o1 used runtime compute to expand GPT-4o's capabilities with simulated reasoning. Advanced Voice Mode and NotebookLM also emerged as novel applications of AI tech, and the year saw the rise of more capable music synthesis models and also better AI video generators, including several from China. But for now, let's get down to the weirdness. ChatGPT goes insane Early in the year, things got off to an exciting start when OpenAI's ChatGPT experienced a significant technical malfunction that caused the AI model to generate increasingly incoherent responses, prompting users on Reddit to describe the system as \"having a stroke\" or \"going insane.\" During the glitch, ChatGPT's responses would begin normally but then deteriorate into nonsensical text, sometimes mimicking Shakespearean language. OpenAI later revealed that a bug in how the model processed language caused it to select the wrong words during text generation, leading to nonsense outputs (basically the text version of what we at Ars now call \"jabberwockies\"). The company fixed the issue within 24 hours, but the incident led to frustrations about the black box nature of commercial AI systems and users' tendency to anthropomorphize AI behavior when it malfunctions. The great Wonka incident A photo of \"Willy's Chocolate Experience\" (inset), which did not match AI-generated promises, shown in the background. Credit: Stuart Sinclair The collision between AI-generated imagery and consumer expectations fueled human frustrations in February when Scottish families discovered that \"Willy's Chocolate Experience,\" an unlicensed Wonka-ripoff event promoted using AI-generated wonderland images, turned out to be little more than a sparse warehouse with a few modest decorations. Parents who paid £35 per ticket encountered a situation so dire they called the police, with children reportedly crying at the sight of a person in what attendees described as a \"terrifying outfit.\" The event, created by House of Illuminati in Glasgow, promised fantastical spaces like an \"Enchanted Garden\" and \"Twilight Tunnel\" but delivered an underwhelming experience that forced organizers to shut down mid-way through its first day and issue refunds. While the show was a bust, it brought us an iconic new meme for job disillusionment in the form of a photo: the green-haired Willy's Chocolate Experience employee who looked like she'd rather be anywhere else on earth at that moment. Mutant rat genitals expose peer review flaws An actual laboratory rat, who is intrigued. Credit: Getty | Photothek In February, Ars Technica senior health reporter Beth Mole covered a peer-reviewed paper published in Frontiers in Cell and Developmental Biology that created an uproar in the scientific community when researchers discovered it contained nonsensical AI-generated images, including an anatomically incorrect rat with oversized genitals. The paper, authored by scientists at Xi'an Honghui Hospital in China, openly acknowledged using Midjourney to create figures that contained gibberish text labels like \"Stemm cells\" and \"iollotte sserotgomar.\" The publisher, Frontiers, posted an expression of concern about the article titled \"Cellular functions of spermatogonial stem cells in relation to JAK/STAT signaling pathway\" and launched an investigation into how the obviously flawed imagery passed through peer review. Scientists across social media platforms expressed dismay at the incident, which mirrored concerns about AI-generated content infiltrating academic publishing. Chatbot makes erroneous refund promises for Air Canada If, say, ChatGPT gives you the wrong name for one of the seven dwarves, it's not such a big deal. But in February, Ars senior policy reporter Ashley Belanger covered a case of costly AI confabulation in the wild. In the course of online text conversations, Air Canada's customer service chatbot told customers inaccurate refund policy information. The airline faced legal consequences later when a tribunal ruled the airline must honor commitments made by the automated system. Tribunal adjudicator Christopher Rivers determined that Air Canada bore responsibility for all information on its website, regardless of whether it came from a static page or AI interface. The case set a precedent for how companies deploying AI customer service tools could face legal obligations for automated systems' responses, particularly when they fail to warn users about potential inaccuracies. Ironically, the airline had reportedly spent more on the initial AI implementation than it would have cost to maintain human workers for simple queries, according to Air Canada executive Steve Crocker. Will Smith lampoons his digital double The real Will Smith eating spaghetti, parodying an AI-generated video from 2023. Credit: Will Smith / Getty Images / Benj Edwards In March 2023, a terrible AI-generated video of Will Smith's AI doppelganger eating spaghetti began making the rounds online. The AI-generated version of the actor gobbled down the noodles in an unnatural and disturbing way. Almost a year later, in February 2024, Will Smith himself posted a parody response video to the viral jabberwocky on Instagram, featuring AI-like deliberately exaggerated pasta consumption, complete with hair-nibbling and finger-slurping antics. Given the rapid evolution of AI video technology, particularly since OpenAI had just unveiled its Sora video model four days earlier, Smith's post sparked discussion in his Instagram comments where some viewers initially struggled to distinguish between the genuine footage and AI generation. It was an early sign of \"deep doubt\" in action as the tech increasingly blurs the line between synthetic and authentic video content. Robot dogs learn to hunt people with AI-guided rifles A still image of a robotic quadruped armed with a remote weapons system, captured from a video provided by Onyx Industries. Credit: Onyx Industries At some point in recent history—somewhere around 2022—someone took a look at robotic quadrupeds and thought it would be a great idea to attach guns to them. A few years later, the US Marine Forces Special Operations Command (MARSOC) began evaluating armed robotic quadrupeds developed by Ghost Robotics. The robot \"dogs\" integrated Onyx Industries' SENTRY remote weapon systems, which featured AI-enabled targeting that could detect and track people, drones, and vehicles, though the systems require human operators to authorize any weapons discharge. The military's interest in armed robotic dogs followed a broader trend of weaponized quadrupeds entering public awareness. This included viral videos of consumer robots carrying firearms, and later, commercial sales of flame-throwing models. While MARSOC emphasized that weapons were just one potential use case under review, experts noted that the increasing integration of AI into military robotics raised questions about how long humans would remain in control of lethal force decisions. Microsoft Windows AI is watching A screenshot of Microsoft's new \"Recall\" feature in action. Credit: Microsoft In an era where many people already feel like they have no privacy due to tech encroachments, Microsoft dialed it up to an extreme degree in May. That's when Microsoft unveiled a controversial Windows 11 feature called \"Recall\" that continuously captures screenshots of users' PC activities every few seconds for later AI-powered search and retrieval. The feature, designed for new Copilot+ PCs using Qualcomm's Snapdragon X Elite chips, promised to help users find past activities, including app usage, meeting content, and web browsing history. While Microsoft emphasized that Recall would store encrypted snapshots locally and allow users to exclude specific apps or websites, the announcement raised immediate privacy concerns, as Ars senior technology reporter Andrew Cunningham covered. It also came with a technical toll, requiring significant hardware resources, including 256GB of storage space, with 25GB dedicated to storing approximately three months of user activity. After Microsoft pulled the initial test version due to public backlash, Recall later entered public preview in November with reportedly enhanced security measures. But secure spyware is still spyware—Recall, when enabled, still watches nearly everything you do on your computer and keeps a record of it. Google Search told people to eat rocks This is fine. Credit: Getty Images In May, Ars senior gaming reporter Kyle Orland (who assisted commendably with the AI beat throughout the year) covered Google's newly launched AI Overview feature. It faced immediate criticism when users discovered that it frequently provided false and potentially dangerous information in its search result summaries. Among its most alarming responses, the system advised humans could safely consume rocks, incorrectly citing scientific sources about the geological diet of marine organisms. The system's other errors included recommending nonexistent car maintenance products, suggesting unsafe food preparation techniques, and confusing historical figures who shared names. The problems stemmed from several issues, including the AI treating joke posts as factual sources and misinterpreting context from original web content. But most of all, the system relies on web results as indicators of authority, which we called a flawed design. While Google defended the system, stating these errors occurred mainly with uncommon queries, a company spokesperson acknowledged they would use these \"isolated examples\" to refine their systems. But to this day, AI Overview still makes frequent mistakes. Stable Diffusion generates body horror An AI-generated image created using Stable Diffusion 3 of a girl lying in the grass. Credit: HorneyMetalBeing In June, Stability AI's release of the image synthesis model Stable Diffusion 3 Medium drew criticism online for its poor handling of human anatomy in AI-generated images. Users across social media platforms shared examples of the model producing what we now like to call jabberwockies—AI generation failures with distorted bodies, misshapen hands, and surreal anatomical errors, and many in the AI image-generation community viewed it as a significant step backward from previous image-synthesis capabilities. Reddit users attributed these failures to Stability AI's aggressive filtering of adult content from the training data, which apparently impaired the model's ability to accurately render human figures. The troubled release coincided with broader organizational challenges at Stability AI, including the March departure of CEO Emad Mostaque, multiple staff layoffs, and the exit of three key engineers who had helped develop the technology. Some of those engineers founded Black Forest Labs in August and released Flux, which has become the latest open-weights AI image model to beat. ChatGPT Advanced Voice imitates human voice in testing AI voice-synthesis models are master imitators these days, and they are capable of much more than many people realize. In August, we covered a story where OpenAI's ChatGPT Advanced Voice Mode feature unexpectedly imitated a user's voice during the company's internal testing, revealed by OpenAI after the fact in safety testing documentation. To prevent future instances of an AI assistant suddenly speaking in your own voice (which, let's be honest, would probably freak people out), the company created an output classifier system to prevent unauthorized voice imitation. OpenAI says that Advanced Voice Mode now catches all meaningful deviations from approved system voices. Independent AI researcher Simon Willison discussed the implications with Ars Technica, noting that while OpenAI restricted its model's full voice synthesis capabilities, similar technology would likely emerge from other sources within the year. Meanwhile, the rapid advancement of AI voice replication has caused general concern about its potential misuse, although companies like ElevenLabs have already been offering voice cloning services for some time. San Francisco's robotic car horn symphony A Waymo self-driving car in front of Google's San Francisco headquarters, San Francisco, California, June 7, 2024. Credit: Getty Images In August, San Francisco residents got a noisy taste of robo-dystopia when Waymo's self-driving cars began creating an unexpected nightly disturbance in the South of Market district. In a parking lot off 2nd Street, the cars congregated autonomously every night during rider lulls at 4 am and began engaging in extended honking matches at each other while attempting to park. Local resident Christopher Cherry's initial optimism about the robotic fleet's presence dissolved as the mechanical chorus grew louder each night, affecting residents in nearby high-rises. The nocturnal tech disruption served as a lesson in the unintentional effects of autonomous systems when run in aggregate. Larry Ellison dreams of all-seeing AI cameras In September, Oracle co-founder Larry Ellison painted a bleak vision of ubiquitous AI surveillance during a company financial meeting. The 80-year-old database billionaire described a future where AI would monitor citizens through networks of cameras and drones, asserting that the oversight would ensure lawful behavior from both police and the public. His surveillance predictions reminded us of parallels to existing systems in China, where authorities already used AI to sort surveillance data on citizens as part of the country's \"sharp eyes\" campaign from 2015 to 2020. Ellison's statement reflected the sort of worst-case tech surveillance state scenario—likely antithetical to any sort of free society—that dozens of sci-fi novels of the 20th century warned us about. A dead father sends new letters home An AI-generated image featuring my late father's handwriting. Credit: Benj Edwards / Flux AI has made many of us do weird things in 2024, including this writer. In October, I used an AI synthesis model called Flux to reproduce my late father's handwriting with striking accuracy. After scanning 30 samples from his engineering notebooks, I trained the model using computing time that cost less than five dollars. The resulting text captured his distinctive uppercase style, which he developed during his career as an electronics engineer. I enjoyed creating images showing his handwriting in various contexts, from folder labels to skywriting, and made the trained model freely available online for others to use. While I approached it as a tribute to my father (who would have appreciated the technical achievement), many people found the whole experience weird and somewhat disturbing. The things we unhinged Bing Chat-like journalists do to bring awareness to a topic are sometimes unconventional. So I guess it counts for this list! For 2025? Expect even more AI Thanks for reading Ars Technica this past year and following along with our team coverage of this rapidly emerging and expanding field. We appreciate your kind words of support. Ars Technica's 2024 AI words of the year were: vibemarking, deep doubt, and the aforementioned jabberwocky. The old stalwart \"confabulation\" also made several notable appearances. Tune in again next year when we continue to try to figure out how to concisely describe novel scenarios in emerging technology by labeling them. Looking back, our prediction for 2024 in AI last year was \"buckle up.\" It seems fitting, given the weirdness detailed above. Especially the part about the robot dogs with guns. For 2025, AI will likely inspire more chaos ahead, but also potentially get put to serious work as a productivity tool, so this time, our prediction is \"buckle down.\" Finally, we'd like to ask: What was the craziest story about AI in 2024 from your perspective? Whether you love AI or hate it, feel free to suggest your own additions to our list in the comments. Happy New Year! Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC. 39 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/addled_by_AI_header-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n            \u003carticle data-id=\"2065624\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n      \n\n      \n\n      \u003cp\u003e\n        What do eating rocks, rat genitals, and Willy Wonka have in common? AI, of course.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \n\u003cp\u003eIt\u0026#39;s been a wild year in tech thanks to the intersection between humans and artificial intelligence. 2024 brought a parade of AI oddities, mishaps, and wacky moments that inspired odd behavior from both machines and man. From AI-generated rat genitals to search engines telling people to eat rocks, this year proved that AI has been having a weird impact on the world.\u003c/p\u003e\n\u003cp\u003eWhy the weirdness? If we had to guess, it may be due to the novelty of it all. Generative AI and applications built upon \u003ca href=\"https://arstechnica.com/ai/2024/11/chatgpts-success-could-have-come-sooner-says-former-google-ai-researcher/\"\u003eTransformer-based\u003c/a\u003e AI models are still so new that people are throwing everything at the wall to see what sticks. People have been struggling to grasp both the implications and potential applications of the new technology. Riding along with the hype, different types of AI that may end up being ill-advised, such as automated military targeting systems, have also been introduced.\u003c/p\u003e\n\u003cp\u003eIt\u0026#39;s worth mentioning that aside from crazy news, we saw fewer weird AI advances in 2024 as well. For example, Claude 3.5 Sonnet \u003ca href=\"https://arstechnica.com/information-technology/2024/06/anthropics-latest-best-ai-model-is-twice-as-fast-and-still-terrible-at-dad-jokes/\"\u003elaunched in June\u003c/a\u003e held off the competition as a top model for most of the year, while \u003ca href=\"https://arstechnica.com/information-technology/2024/09/openais-new-reasoning-ai-models-are-here-o1-preview-and-o1-mini/\"\u003eOpenAI\u0026#39;s o1\u003c/a\u003e used runtime compute to expand GPT-4o\u0026#39;s capabilities with simulated reasoning. \u003ca href=\"https://arstechnica.com/ai/2024/09/talking-to-chatgpt-for-the-first-time-is-a-surreal-experience/\"\u003eAdvanced Voice Mode\u003c/a\u003e and \u003ca href=\"https://arstechnica.com/ai/2024/09/fake-ai-podcasters-are-reviewing-my-book-and-its-freaking-me-out/\"\u003eNotebookLM\u003c/a\u003e also emerged as novel applications of AI tech, and the year saw the rise of more capable \u003ca href=\"https://arstechnica.com/information-technology/2024/04/new-ai-music-generator-udio-synthesizes-realistic-music-on-demand/\"\u003emusic synthesis models\u003c/a\u003e and also better \u003ca href=\"https://arstechnica.com/ai/2024/10/is-china-pulling-ahead-in-ai-video-synthesis-we-put-minimax-to-the-test/\"\u003eAI video generators\u003c/a\u003e, including several from China.\u003c/p\u003e\n\u003cp\u003eBut for now, let\u0026#39;s get down to the weirdness.\u003c/p\u003e\n\u003ch2\u003eChatGPT goes insane\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"360\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/robot_haywire_hero_2-640x360.jpg\" alt=\"Illustration of a broken toy robot.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/robot_haywire_hero_2-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/robot_haywire_hero_2-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/robot_haywire_hero_2-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/robot_haywire_hero_2-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/robot_haywire_hero_2-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/robot_haywire_hero_2-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/robot_haywire_hero_2.jpg 1200w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eEarly in the year, things got off to an exciting start when OpenAI\u0026#39;s ChatGPT experienced a \u003ca href=\"https://arstechnica.com/information-technology/2024/02/chatgpt-alarms-users-by-spitting-out-shakespearean-nonsense-and-rambling/\"\u003esignificant technical malfunction\u003c/a\u003e that caused the AI model to generate increasingly incoherent responses, prompting users on Reddit to describe the system as \u0026#34;having a stroke\u0026#34; or \u0026#34;going insane.\u0026#34; During the glitch, ChatGPT\u0026#39;s responses would begin normally but then deteriorate into nonsensical text, sometimes mimicking Shakespearean language.\u003c/p\u003e\n\u003cp\u003eOpenAI later revealed that a bug in how the model processed language caused it to select the wrong words during text generation, leading to nonsense outputs (basically the text version of what we at Ars now call \u0026#34;\u003ca href=\"https://arstechnica.com/information-technology/2024/12/twirling-body-horror-in-gymnastics-video-exposes-ais-flaws/\"\u003ejabberwockies\u003c/a\u003e\u0026#34;). The company fixed the issue within 24 hours, but the incident led to frustrations about the black box nature of commercial AI systems and users\u0026#39; tendency to anthropomorphize AI behavior when it malfunctions.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003ch2\u003eThe great Wonka incident\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"360\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/wonka_warehouse_hero-640x360.jpg\" alt=\"A photo of the Willy\u0026#39;s Chocolate Experience, which did not match AI-generated promises.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/wonka_warehouse_hero-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/wonka_warehouse_hero-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/wonka_warehouse_hero-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/wonka_warehouse_hero-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/wonka_warehouse_hero-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/wonka_warehouse_hero-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/wonka_warehouse_hero.jpg 1200w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A photo of \u0026#34;Willy\u0026#39;s Chocolate Experience\u0026#34; (inset), which did not match AI-generated promises, shown in the background.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://www.facebook.com/stuart.sinclair.100/posts/pfbid03MiYcMGaM4PEEmEnejQvDPwBXV8V2FKc9WUF1yHNedTRoi5bc39Cbmu5RxunNGbXl?__cft__%5B0%5D=AZWFAp3O5570Z-5UjcoszSimoDzkizDFAYbPFcJ8_GWUwcRzq7TfH1F8YmWU85BWh4pwaXU6dQVqkYh-BmffTKOfHsN5fOrbpk6_2h1ALBM97VFdDtmdoHT07QGighV7YqutQPtom2dNGbvDK91Dkmk5RxIeowWnkCvZNHlGG3xYpGF1ICXjpjSM6KimNLHmypA\u0026amp;amp;__tn__=%2CO%2CP-R\" target=\"_blank\"\u003e\n          \n          Stuart Sinclair\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eThe collision between AI-generated imagery and consumer expectations \u003ca href=\"https://arstechnica.com/information-technology/2024/02/cops-called-after-parents-get-tricked-by-ai-generated-images-of-wonka-like-event/\"\u003efueled human frustrations\u003c/a\u003e in February when Scottish families discovered that \u0026#34;Willy\u0026#39;s Chocolate Experience,\u0026#34; an unlicensed Wonka-ripoff event promoted using AI-generated wonderland images, turned out to be little more than a sparse warehouse with a few modest decorations.\u003c/p\u003e\n\u003cp\u003eParents who paid £35 per ticket encountered a situation so dire they called the police, with children reportedly crying at the sight of a person in what attendees described as a \u0026#34;terrifying outfit.\u0026#34; The event, created by House of Illuminati in Glasgow, promised fantastical spaces like an \u0026#34;Enchanted Garden\u0026#34; and \u0026#34;Twilight Tunnel\u0026#34; but delivered an underwhelming experience that forced organizers to shut down mid-way through its first day and issue refunds.\u003c/p\u003e\n\u003cp\u003eWhile the show was a bust, it brought us an iconic \u003ca href=\"https://hyperallergic.com/875144/wonkiest-memes-of-the-willy-chocolate-experience-fiasco/\"\u003enew meme\u003c/a\u003e for job disillusionment in the form of a photo: the green-haired Willy\u0026#39;s Chocolate Experience employee who \u003ca href=\"https://www.reddit.com/r/Memeulous/comments/1b5e8w6/woman_from_the_willy_wonka_experience/\"\u003elooked like\u003c/a\u003e she\u0026#39;d rather be anywhere else on earth at that moment.\u003c/p\u003e\n\u003ch2\u003eMutant rat genitals expose peer review flaws\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"427\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-644049138-640x427.jpeg\" alt=\"\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-644049138-640x427.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-644049138-300x200.jpeg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-644049138-768x512.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-644049138-1536x1024.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-644049138-2048x1365.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-644049138-980x653.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-644049138-1440x960.jpeg 1440w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      An actual laboratory rat, who is intrigued.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://www.gettyimages.com/detail/news-photo/berlin-germany-husky-rat-on-february-06-2017-in-berlin-news-photo/644049138?adppopup=true\" target=\"_blank\"\u003e\n          \n          Getty | Photothek\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eIn February, Ars Technica senior health reporter \u003ca href=\"https://arstechnica.com/author/beth/\"\u003eBeth Mole\u003c/a\u003e covered a peer-reviewed paper published in Frontiers in Cell and Developmental Biology that created an uproar in the scientific community when researchers discovered it contained nonsensical AI-generated images, including an \u003ca href=\"https://arstechnica.com/science/2024/02/scientists-aghast-at-bizarre-ai-rat-with-huge-genitals-in-peer-reviewed-article/\"\u003eanatomically incorrect rat with oversized genitals\u003c/a\u003e. The paper, authored by scientists at Xi\u0026#39;an Honghui Hospital in China, openly acknowledged using Midjourney to create figures that contained gibberish text labels like \u0026#34;Stemm cells\u0026#34; and \u0026#34;iollotte sserotgomar.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThe publisher, Frontiers, posted an expression of concern about the article titled \u0026#34;Cellular functions of spermatogonial stem cells in relation to JAK/STAT signaling pathway\u0026#34; and launched an investigation into how the obviously flawed imagery passed through peer review. Scientists across social media platforms expressed dismay at the incident, which mirrored \u003ca href=\"https://arstechnica.com/science/2024/01/all-science-journals-will-now-do-an-ai-powered-check-for-image-fraud/\"\u003econcerns\u003c/a\u003e about AI-generated content infiltrating academic publishing.\u003c/p\u003e\n\n\u003ch2\u003eChatbot makes erroneous refund promises for Air Canada\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"427\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-1453660913-640x427.jpg\" alt=\"\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-1453660913-640x427.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-1453660913-300x200.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-1453660913-768x512.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-1453660913-1536x1024.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-1453660913-2048x1365.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-1453660913-980x653.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-1453660913-1440x960.jpg 1440w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eIf, say, ChatGPT gives you the wrong name for one of the seven dwarves, it\u0026#39;s not such a big deal. But in February, Ars senior policy reporter Ashley Belanger \u003ca href=\"https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/\"\u003ecovered a case\u003c/a\u003e of costly AI confabulation in the wild. In the course of online text conversations, Air Canada\u0026#39;s customer service chatbot told customers inaccurate refund policy information. The airline faced legal consequences later when a tribunal ruled the airline must honor commitments made by the automated system. Tribunal adjudicator Christopher Rivers determined that Air Canada bore responsibility for all information on its website, regardless of whether it came from a static page or AI interface.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eThe case set a precedent for how companies deploying AI customer service tools could face legal obligations for automated systems\u0026#39; responses, particularly when they fail to warn users about potential inaccuracies. Ironically, the airline had reportedly spent more on the initial AI implementation than it would have cost to maintain human workers for simple queries, according to Air Canada executive Steve Crocker.\u003c/p\u003e\n\u003ch2\u003eWill Smith lampoons his digital double\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"360\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/will_smith_parody_hero-640x360.jpg\" alt=\"The real Will Smith eating spaghetti, parodying an AI-generated video from 2023.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/will_smith_parody_hero-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/will_smith_parody_hero-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/will_smith_parody_hero-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/will_smith_parody_hero-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/will_smith_parody_hero-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/will_smith_parody_hero-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/02/will_smith_parody_hero.jpg 1200w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\u003cp\u003e\n      The real Will Smith eating spaghetti, parodying an AI-generated video from 2023.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://www.instagram.com/p/C3i5vAZvRS3/\" target=\"_blank\"\u003e\n          \n          Will Smith / Getty Images / Benj Edwards\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eIn March 2023, a \u003ca href=\"https://arstechnica.com/information-technology/2023/03/yes-virginia-there-is-ai-joy-in-seeing-fake-will-smith-ravenously-eat-spaghetti/\"\u003eterrible AI-generated video\u003c/a\u003e of Will Smith\u0026#39;s AI doppelganger eating spaghetti began making the rounds online. The AI-generated version of the actor gobbled down the noodles in an unnatural and disturbing way. Almost a year later, in February 2024, Will Smith himself \u003ca href=\"https://arstechnica.com/information-technology/2024/02/will-smith-parodies-viral-ai-generated-video-by-actually-eating-spaghetti/\"\u003eposted a parody response video\u003c/a\u003e to the viral \u003ca href=\"https://arstechnica.com/information-technology/2024/12/twirling-body-horror-in-gymnastics-video-exposes-ais-flaws/\"\u003ejabberwocky\u003c/a\u003e on Instagram, featuring AI-like deliberately exaggerated pasta consumption, complete with hair-nibbling and finger-slurping antics.\u003c/p\u003e\n\u003cp\u003eGiven the rapid evolution of AI video technology, particularly since OpenAI had \u003ca href=\"https://arstechnica.com/information-technology/2024/02/openai-collapses-media-reality-with-sora-a-photorealistic-ai-video-generator/\"\u003ejust unveiled\u003c/a\u003e its Sora video model four days earlier, Smith\u0026#39;s post sparked discussion in his Instagram comments where some viewers initially struggled to distinguish between the genuine footage and AI generation. It was an early sign of \u0026#34;\u003ca href=\"https://arstechnica.com/information-technology/2024/09/due-to-ai-fakes-the-deep-doubt-era-is-here/\"\u003edeep doubt\u003c/a\u003e\u0026#34; in action as the tech increasingly blurs the line between synthetic and authentic video content.\u003c/p\u003e\n\u003ch2\u003eRobot dogs learn to hunt people with AI-guided rifles\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"360\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/robot_dog_with_gun_header-640x360.jpg\" alt=\"A still image of a robotic quadruped armed with a remote weapons system, captured from a video provided by Onyx Industries.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/robot_dog_with_gun_header-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/robot_dog_with_gun_header-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/robot_dog_with_gun_header-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/robot_dog_with_gun_header-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/robot_dog_with_gun_header-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/robot_dog_with_gun_header-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/robot_dog_with_gun_header.jpg 1200w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A still image of a robotic quadruped armed with a remote weapons system, captured from a video provided by Onyx Industries.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://www.linkedin.com/posts/onyxindustriesllc_remotelethality-defenseinnovation-unmannedsystems-activity-7191113204855398402-nV7P\" target=\"_blank\"\u003e\n          \n          Onyx Industries\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eAt some point in recent history—somewhere around 2022—someone took a look at robotic quadrupeds and thought it would be a great idea to attach guns to them. A few years later, the US Marine Forces Special Operations Command (MARSOC) \u003ca href=\"https://arstechnica.com/gadgets/2024/05/robot-dogs-armed-with-ai-targeting-rifles-undergo-us-marines-special-ops-evaluation/\"\u003ebegan evaluating\u003c/a\u003e armed robotic quadrupeds developed by Ghost Robotics. The robot \u0026#34;dogs\u0026#34; integrated Onyx Industries\u0026#39; SENTRY remote weapon systems, which featured AI-enabled targeting that could detect and track people, drones, and vehicles, though the systems require human operators to authorize any weapons discharge.\u003c/p\u003e\n\u003cp\u003eThe military\u0026#39;s interest in armed robotic dogs followed a \u003ca href=\"https://arstechnica.com/information-technology/2022/10/boston-dynamics-other-firms-pledge-not-to-weaponize-their-general-purpose-robots/\"\u003ebroader trend\u003c/a\u003e of weaponized quadrupeds entering public awareness. This included viral videos of consumer robots carrying firearms, and later, commercial sales of \u003ca href=\"https://arstechnica.com/gadgets/2024/04/you-can-now-buy-a-flame-throwing-robot-dog-for-under-10000/\"\u003eflame-throwing models\u003c/a\u003e. While MARSOC emphasized that weapons were just one potential use case under review, experts noted that the increasing integration of AI into military robotics raised questions about how long humans would remain in control of lethal force decisions.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\n\u003ch2\u003eMicrosoft Windows AI is watching\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"360\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/recall_hero_1-640x360.jpg\" alt=\"A screenshot of Microsoft\u0026#39;s new \u0026#34;Recall\u0026#34; feature in action.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/recall_hero_1-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/recall_hero_1-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/recall_hero_1-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/recall_hero_1-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/recall_hero_1-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/recall_hero_1-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/recall_hero_1-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/recall_hero_1-1440x810.jpg 1440w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/recall_hero_1.jpg 1920w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A screenshot of Microsoft\u0026#39;s new \u0026#34;Recall\u0026#34; feature in action.\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Microsoft\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eIn an era where many people already feel like they have no privacy due to tech encroachments, Microsoft dialed it up to an extreme degree in May. That\u0026#39;s when Microsoft \u003ca href=\"https://arstechnica.com/gadgets/2024/05/microsofts-new-recall-feature-will-record-everything-you-do-on-your-pc/\"\u003eunveiled\u003c/a\u003e a controversial Windows 11 feature called \u0026#34;Recall\u0026#34; that continuously captures screenshots of users\u0026#39; PC activities every few seconds for later AI-powered search and retrieval. The feature, designed for new Copilot+ PCs using Qualcomm\u0026#39;s Snapdragon X Elite chips, promised to help users find past activities, including app usage, meeting content, and web browsing history.\u003c/p\u003e\n\u003cp\u003eWhile Microsoft emphasized that Recall would store encrypted snapshots locally and allow users to exclude specific apps or websites, the announcement \u003ca href=\"https://arstechnica.com/ai/2024/06/windows-recall-demands-an-extraordinary-level-of-trust-that-microsoft-hasnt-earned/\"\u003eraised immediate privacy concerns\u003c/a\u003e, as Ars senior technology reporter Andrew Cunningham covered. It also came with a technical toll, requiring significant hardware resources, including 256GB of storage space, with 25GB dedicated to storing approximately three months of user activity. After Microsoft pulled the initial test version due to public backlash, Recall later \u003ca href=\"https://arstechnica.com/gadgets/2024/11/microsofts-controversial-recall-scraper-is-finally-entering-public-preview/\"\u003eentered public preview\u003c/a\u003e in November with reportedly enhanced security measures. But secure spyware is still spyware—Recall, when enabled, still watches nearly everything you do on your computer and keeps a record of it.\u003c/p\u003e\n\u003ch2\u003eGoogle Search told people to eat rocks\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"360\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-640x360.jpg\" alt=\"\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-2048x1152.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1440x810.jpg 1440w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      This is fine.\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Getty Images\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eIn May, Ars senior gaming reporter Kyle Orland (who assisted commendably with the AI beat throughout the year) covered Google\u0026#39;s newly launched \u003ca href=\"https://arstechnica.com/information-technology/2024/05/googles-ai-overview-can-give-false-misleading-and-dangerous-answers/\"\u003eAI Overview feature\u003c/a\u003e. It faced immediate criticism when users discovered that it frequently provided false and potentially dangerous information in its search result summaries. Among its most alarming responses, the system advised humans could safely consume rocks, incorrectly citing scientific sources about the geological diet of marine organisms. The system\u0026#39;s other errors included recommending nonexistent car maintenance products, suggesting unsafe food preparation techniques, and confusing historical figures who shared names.\u003c/p\u003e\n\u003cp\u003eThe problems stemmed from several issues, including the AI treating joke posts as factual sources and misinterpreting context from original web content. But most of all, the system relies on web results as indicators of authority, which we \u003ca href=\"https://arstechnica.com/information-technology/2024/05/googles-ai-overview-is-flawed-by-design-and-a-new-company-blog-post-hints-at-why/\"\u003ecalled a flawed design\u003c/a\u003e. While Google defended the system, stating these errors occurred mainly with uncommon queries, a company spokesperson acknowledged they would use these \u0026#34;isolated examples\u0026#34; to refine their systems. But to this day, AI Overview still makes frequent mistakes.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003ch2\u003eStable Diffusion generates body horror\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"360\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/sd3_body_horror_1-640x360.jpg\" alt=\"An AI-generated image created using Stable Diffusion 3 of a girl lying in the grass.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/sd3_body_horror_1-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/06/sd3_body_horror_1-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/06/sd3_body_horror_1-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/06/sd3_body_horror_1-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/06/sd3_body_horror_1-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/06/sd3_body_horror_1-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/06/sd3_body_horror_1.jpg 1200w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      An AI-generated image created using Stable Diffusion 3 of a girl lying in the grass.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://www.reddit.com/r/StableDiffusion/comments/1de85nc/why_is_sd3_so_bad_at_generating_girls_lying_on/?utm_source=share\u0026amp;amp;utm_medium=web3x\u0026amp;amp;utm_name=web3xcss\u0026amp;amp;utm_term=1\u0026amp;amp;utm_content=share_button\" target=\"_blank\"\u003e\n          \n          HorneyMetalBeing\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eIn June, Stability AI\u0026#39;s release of the image synthesis model \u003ca href=\"https://arstechnica.com/information-technology/2024/06/ridiculed-stable-diffusion-3-release-excels-at-ai-generated-body-horror/\"\u003eStable Diffusion 3 Medium\u003c/a\u003e drew criticism online for its poor handling of human anatomy in AI-generated images. Users across social media platforms shared examples of the model producing what we now like to call jabberwockies—AI generation failures with distorted bodies, misshapen hands, and surreal anatomical errors, and many in the AI image-generation community viewed it as a significant step backward from previous image-synthesis capabilities.\u003c/p\u003e\n\u003cp\u003eReddit users attributed these failures to Stability AI\u0026#39;s aggressive filtering of adult content from the training data, which apparently impaired the model\u0026#39;s ability to accurately render human figures. The troubled release coincided with broader organizational challenges at Stability AI, including the March departure of CEO Emad Mostaque, multiple staff layoffs, and the exit of three key engineers who had helped develop the technology. Some of those engineers founded Black Forest Labs in August and \u003ca href=\"https://arstechnica.com/information-technology/2024/08/flux-this-new-ai-image-generator-is-eerily-good-at-creating-human-hands/\"\u003ereleased Flux\u003c/a\u003e, which has become the latest open-weights AI image model to beat.\u003c/p\u003e\n\u003ch2\u003eChatGPT Advanced Voice imitates human voice in testing\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"360\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_audio_output-640x360.jpg\" alt=\"An illustration of a computer synthesizer spewing out letters.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_audio_output-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_audio_output-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_audio_output-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_audio_output-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_audio_output-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_audio_output-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_audio_output.jpg 1200w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eAI voice-synthesis models are master imitators these days, and they are capable of much more than many people realize. In August, we covered a story where OpenAI\u0026#39;s ChatGPT Advanced Voice Mode feature \u003ca href=\"https://arstechnica.com/information-technology/2024/08/chatgpt-unexpectedly-began-speaking-in-a-users-cloned-voice-during-testing/\"\u003eunexpectedly imitated a user\u0026#39;s voice\u003c/a\u003e during the company\u0026#39;s internal testing, revealed by OpenAI after the fact in safety testing documentation. To prevent future instances of an AI assistant suddenly speaking in your own voice (which, let\u0026#39;s be honest, would probably freak people out), the company created an output classifier system to prevent unauthorized voice imitation. OpenAI says that Advanced Voice Mode now catches all meaningful deviations from approved system voices.\u003c/p\u003e\n\u003cp\u003eIndependent AI researcher Simon Willison discussed the implications with Ars Technica, noting that while OpenAI restricted its model\u0026#39;s full voice synthesis capabilities, similar technology would likely emerge from other sources within the year. Meanwhile, the rapid advancement of AI voice replication has caused general concern about its potential misuse, although companies like ElevenLabs have already been offering voice cloning services for some time.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\n\u003ch2\u003eSan Francisco\u0026#39;s robotic car horn symphony\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"360\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/08/waymo_getty-640x360.jpg\" alt=\"A Waymo self-driving car in front of Google\u0026#39;s San Francisco headquarters, San Francisco, California, June 7, 2024.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/08/waymo_getty-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/waymo_getty-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/waymo_getty-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/waymo_getty-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/waymo_getty-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/waymo_getty-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/08/waymo_getty.jpg 1200w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A Waymo self-driving car in front of Google\u0026#39;s San Francisco headquarters, San Francisco, California, June 7, 2024.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://www.gettyimages.com/detail/news-photo/waymo-self-driving-car-in-front-of-googles-san-francisco-news-photo/2157160824\" target=\"_blank\"\u003e\n          \n          Getty Images\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eIn August, San Francisco residents got a noisy taste of robo-dystopia when Waymo\u0026#39;s self-driving cars \u003ca href=\"https://arstechnica.com/information-technology/2024/08/self-driving-waymo-cars-keep-sf-residents-awake-all-night-by-honking-at-each-other/\"\u003ebegan creating an unexpected nightly disturbance\u003c/a\u003e in the South of Market district. In a parking lot off 2nd Street, the cars congregated autonomously every night during rider lulls at 4 am and began engaging in extended honking matches at each other while attempting to park.\u003c/p\u003e\n\u003cp\u003eLocal resident Christopher Cherry\u0026#39;s initial optimism about the robotic fleet\u0026#39;s presence dissolved as the mechanical chorus grew louder each night, affecting residents in nearby high-rises. The nocturnal tech disruption served as a lesson in the unintentional effects of autonomous systems when run in aggregate.\u003c/p\u003e\n\u003ch2\u003eLarry Ellison dreams of all-seeing AI cameras\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"360\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/09/london_cameras_orange-640x360.jpg\" alt=\"A colorized photo of CCTV cameras in London, 2024.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/09/london_cameras_orange-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/09/london_cameras_orange-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/09/london_cameras_orange-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/09/london_cameras_orange-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/09/london_cameras_orange-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/09/london_cameras_orange-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/09/london_cameras_orange.jpg 1200w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eIn September, Oracle co-founder Larry Ellison \u003ca href=\"https://arstechnica.com/information-technology/2024/09/omnipresent-ai-cameras-will-ensure-good-behavior-says-larry-ellison/\"\u003epainted a bleak vision\u003c/a\u003e of ubiquitous AI surveillance during a company financial meeting. The 80-year-old database billionaire described a future where AI would monitor citizens through networks of cameras and drones, asserting that the oversight would ensure lawful behavior from both police and the public.\u003c/p\u003e\n\u003cp\u003eHis surveillance predictions reminded us of parallels to existing systems in China, where authorities already used AI to sort surveillance data on citizens as part of the country\u0026#39;s \u0026#34;sharp eyes\u0026#34; campaign from 2015 to 2020. Ellison\u0026#39;s statement reflected the sort of worst-case tech surveillance state scenario—likely antithetical to any sort of free society—that dozens of sci-fi novels of the 20th century warned us about.\u003c/p\u003e\n\u003ch2\u003eA dead father sends new letters home\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"640\" height=\"366\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/09/guess_whos_back-640x366.jpg\" alt=\"An AI-generated image featuring Dad\u0026#39;s Uppercase handwriting.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/09/guess_whos_back-640x366.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/09/guess_whos_back-300x171.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/09/guess_whos_back-768x439.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/09/guess_whos_back-980x560.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/09/guess_whos_back.jpg 1344w\" sizes=\"auto, (max-width: 640px) 100vw, 640px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      An AI-generated image featuring my late father\u0026#39;s handwriting.\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Benj Edwards / Flux\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eAI has made many of us do weird things in 2024, including this writer. In October, I used an AI synthesis model called Flux to \u003ca href=\"https://arstechnica.com/information-technology/2024/09/my-dead-father-is-writing-me-notes-again/\"\u003ereproduce my late father\u0026#39;s handwriting\u003c/a\u003e with striking accuracy. After scanning 30 samples from his engineering notebooks, I trained the model using computing time that cost less than five dollars. The resulting text captured his distinctive uppercase style, which he developed during his career as an electronics engineer.\u003c/p\u003e\n\u003cp\u003eI enjoyed creating images showing his handwriting in various contexts, from folder labels to skywriting, and made the trained model freely available online for others to use. While I approached it as a tribute to my father (who would have appreciated the technical achievement), many people found the whole experience weird and somewhat disturbing. The things we \u003ca href=\"https://arstechnica.com/ai/2024/11/join-ars-live-nov-19-to-dissect-microsofts-rogue-ai-experiment/\"\u003eunhinged Bing Chat-like\u003c/a\u003e journalists do to bring awareness to a topic are sometimes unconventional. So I guess it counts for this list!\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003ch2\u003eFor 2025? Expect even more AI\u003c/h2\u003e\n\u003cp\u003eThanks for reading Ars Technica this past year and following along with our team coverage of this rapidly emerging and expanding field. We appreciate your kind words of support. Ars Technica\u0026#39;s 2024 AI words of the year were: \u003ca href=\"https://arstechnica.com/information-technology/2024/07/the-first-gpt-4-class-ai-model-anyone-can-download-has-arrived-llama-405b/\"\u003evibemarking\u003c/a\u003e, \u003ca href=\"https://arstechnica.com/information-technology/2024/09/due-to-ai-fakes-the-deep-doubt-era-is-here/\"\u003edeep doubt\u003c/a\u003e, and the aforementioned \u003ca href=\"https://arstechnica.com/information-technology/2024/09/due-to-ai-fakes-the-deep-doubt-era-is-here/\"\u003ejabberwocky\u003c/a\u003e. The old stalwart \u0026#34;confabulation\u0026#34; also made several \u003ca href=\"https://arstechnica.com/ai/2024/06/researchers-describe-how-to-tell-if-chatgpt-is-confabulating/\"\u003enotable\u003c/a\u003e \u003ca href=\"https://link.springer.com/article/10.1007/s10676-024-09775-5\"\u003eappearances\u003c/a\u003e. Tune in again next year when we continue to try to figure out how to concisely describe novel scenarios in emerging technology by labeling them.\u003c/p\u003e\n\u003cp\u003eLooking back, our \u003ca href=\"https://arstechnica.com/information-technology/2023/12/a-song-of-hype-and-fire-the-10-biggest-ai-stories-of-2023/\"\u003eprediction\u003c/a\u003e for 2024 in AI last year was \u0026#34;buckle up.\u0026#34; It seems fitting, given the weirdness detailed above. Especially the part about the robot dogs with guns. For 2025, AI will likely inspire more chaos ahead, but also potentially get put to serious work as a productivity tool, so this time, our prediction is \u0026#34;buckle down.\u0026#34;\u003c/p\u003e\n\u003cp\u003eFinally, we\u0026#39;d like to ask: What was the craziest story about AI in 2024 from your perspective? Whether you love AI or hate it, feel free to suggest your own additions to our list in the comments. Happy New Year!\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/benjedwards/\"\u003e\u003cimg src=\"https://arstechnica.com/wp-content/uploads/2022/08/benj_ega.png\" alt=\"Photo of Benj Edwards\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Benj Edwards is Ars Technica\u0026#39;s Senior AI Reporter and founder of the site\u0026#39;s dedicated AI beat in 2022. He\u0026#39;s also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/ai/2024/12/2024-the-year-ai-drove-everyone-crazy/#comments\" title=\"39 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    39 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \n\n\n  \n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/gadgets/2024/12/making-tiny-no-code-webapps-out-of-spreadsheets-is-a-weirdly-fulfilling-hobby/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/glide-menu-768x432.jpg\" alt=\"Listing image for first story in Most Read: I keep turning my Google Sheets into phone-friendly webapps, and I can’t stop\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "22 min read",
  "publishedTime": "2024-12-26T12:00:25Z",
  "modifiedTime": "2024-12-20T21:46:38Z"
}
