{
  "id": "ea644218-cb4f-441d-b2b9-52c61d73bfb3",
  "title": "New Lego-building AI creates models that actually stand up in real life",
  "link": "https://arstechnica.com/ai/2025/05/new-ai-model-generates-buildable-lego-creations-from-text-descriptions/",
  "description": "Carnegie Mellon \"LegoGPT\" system uses physics checks to ensure models don't collapse.",
  "author": "Benj Edwards",
  "published": "Fri, 09 May 2025 21:29:02 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "AI research",
    "Ava Pun",
    "Lego",
    "LegoGPT",
    "machine learning",
    "Toys"
  ],
  "byline": "Benj Edwards",
  "length": 7305,
  "excerpt": "Carnegie Mellon “LegoGPT” system uses physics checks to ensure models don’t collapse.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Skip to content Another brick in the wall Carnegie Mellon \"LegoGPT\" system uses physics checks to ensure models don't collapse. Several examples of shapes created by LegoGPT. Credit: Pun et al. On Thursday, researchers at Carnegie Mellon University unveiled LegoGPT, an AI model that creates physically stable Lego structures from text prompts. The new system not only designs Lego models that match text descriptions (prompts) but also ensures they can be built brick by brick in the real world, either by hand or with robotic assistance. \"To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions,\" the researchers wrote in their paper, which was posted on arXiv, \"and train an autoregressive large language model to predict the next brick to add via next-token prediction.\" This trained model generates Lego designs that match text prompts like \"a streamlined, elongated vessel\" or \"a classic-style car with a prominent front grille.\" The resulting designs are simple, using just a few brick types to create primitive shapes—but they stand up. As one Ars Technica staffer joked this morning upon seeing the research, \"It builds Lego like it's 1974.\" A LegoGPT demo video assembled by the research team. A LegoGPT demo video assembled by the research team. In the paper titled \"Generating Physically Stable and Buildable Lego Designs from Text,\" the research team led by Ava Pun explained that many existing 3D generation models focus on making diverse objects with detailed geometry, but these digital designs often can't be physically made. \"Without proper support, parts of the design can collapse, float, or remain disconnected,\" they wrote. Unlike previous attempts at autonomous Lego modeling, LegoGPT reportedly produces step-by-step instructions for building Lego creations that don't fall apart. You can see demos of the system in action on the project's website. How LegoGPT works To build LegoGPT, the Carnegie Mellon team repurposed the technology behind large language models (LLMs), similar to the kind that run ChatGPT, for \"next-brick prediction\" instead of next-word prediction. To do so, the team fine-tuned LLaMA-3.2-1B-Instruct, an instruction-following language model from Meta. The team then augmented the brick-predicting model with a separate software tool that can verify physical stability using mathematical models that simulate gravity and structural forces. To train the model, the team assembled a new dataset called \"StableText2Lego,\" which contained over 47,000 stable Lego structures paired with descriptive captions generated by a separate AI model, OpenAI's GPT-4o. Each structure underwent physics analysis to ensure it could be built in the real world. To build the Lego dataset, the team fed images rendered from 24 different viewpoints into GPT-4o and let that model write captions for each Lego structure, asking it to focus on geometric features while omitting color information. Credit: Pun et al. LegoGPT works by first generating a sequence of precisely placed Lego bricks. For each new brick in the sequence, the system makes sure it doesn't collide with existing bricks and that it fits within the building space. After completing a design, it uses the aforementioned mathematical models to verify that the model can stand upright without falling apart. If parts would collapse in real life, the system identifies the first unstable brick and backtracks, removing it and all subsequent bricks before trying a different approach. This \"physics-aware rollback\" method proved essential to the team's approach. Without it, only 24 percent of designs remained standing, compared to 98.8 percent with the full system. The LegoGPT system works in three parts, shown in this diagram. Credit: Pun et al. The researchers also expanded the system's abilities by adding texture and color options. For example, using an appearance prompt like \"Electric guitar in metallic purple,\" LegoGPT can generate a guitar model, with bricks assigned a purple color. Testing with robots and humans To prove their designs worked in real life, the researchers had robots assemble the AI-created Lego models. They used a dual-robot arm system with force sensors to pick up and place bricks according to the AI-generated instructions. Human testers also built some of the designs by hand, showing that the AI creates genuinely buildable models. \"Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing Lego designs that align closely with the input text prompts,\" the team noted in its paper. When tested against other AI systems for 3D creation, LegoGPT stands out through its focus on structural integrity. The team tested against several alternatives, including LLaMA-Mesh and other 3D generation models, and found its approach produced the highest percentage of stable structures. A video of two robot arms building a LegoGPT creation, provided by the researchers. A video of two robot arms building a LegoGPT creation, provided by the researchers. Still, there are some limitations. The current version of LegoGPT only works within a 20×20×20 building space and uses a mere eight standard brick types. \"Our method currently supports a fixed set of commonly used Lego bricks,\" the team acknowledged. \"In future work, we plan to expand the brick library to include a broader range of dimensions and brick types, such as slopes and tiles.\" The researchers also hope to scale up their training dataset to include more objects than the 21 categories currently available. Meanwhile, others can literally build on their work—the researchers released their dataset, code, and models on their project website and GitHub. Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC. 24 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/lego_header_3-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"app\"\u003e\n    \u003cp\u003e\u003ca href=\"#main\"\u003e\n  Skip to content\n\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cmain id=\"main\"\u003e\n            \u003carticle data-id=\"2094193\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n    \u003cdiv\u003e\n      \u003cdiv\u003e\n        \u003cp\u003e\u003cspan\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"section-ai_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"section-ai_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#section-ai_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#section-ai_svg__b)\"\u003e\u003cpath d=\"M20 2.4c9.7 0 17.6 7.9 17.6 17.6S29.7 37.6 20 37.6 2.4 29.7 2.4 20 10.3 2.4 20 2.4M20 0C9 0 0 9 0 20s9 20 20 20 20-9 20-20S31 0 20 0\"\u003e\u003c/path\u003e\u003cpath d=\"M20 13q2.85 0 5.4.9c.7.2 1.4-.1 1.6-.9l1.4-5.5C26 5.9 23.1 4.9 20 4.9s-6 .9-8.4 2.6L13 13c.2.7.9 1.1 1.6.9Q17 13 20 13M8.9 18.3c.4-.8 1-1.5 1.7-2.1l-2.2-5.7C7 12.2 6 14.1 5.5 16.3l1.3 2.1c.5.8 1.7.8 2.2 0m24.3 0 1.3-2.1c-.5-2.2-1.5-4.1-2.9-5.8l-2.2 5.7c.7.6 1.3 1.3 1.7 2.1.5.8 1.6.9 2.2 0M23.2 20c0 1.8-1.5 3.2-3.2 3.2s-3.2-1.4-3.2-3.2 1.5-3.2 3.2-3.2 3.2 1.4 3.2 3.2\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n  \u003c/span\u003e\n  \u003cspan\u003e\n    Another brick in the wall\n  \u003c/span\u003e\n\u003c/p\u003e\n      \u003c/div\u003e\n\n      \n\n      \u003cp\u003e\n        Carnegie Mellon \u0026#34;LegoGPT\u0026#34; system uses physics checks to ensure models don\u0026#39;t collapse.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\n    \u003cdiv\u003e\n    \n    \u003cp\u003e\n      Several examples of shapes created by LegoGPT.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://avalovelace1.github.io/LegoGPT/\" target=\"_blank\"\u003e\n          \n          Pun et al.\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \u003cp\u003eOn Thursday, researchers at Carnegie Mellon University \u003ca href=\"https://avalovelace1.github.io/LegoGPT/\"\u003eunveiled\u003c/a\u003e LegoGPT, an AI model that creates physically stable Lego structures from text prompts. The new system not only designs Lego models that match text descriptions (prompts) but also ensures they can be built brick by brick in the real world, either by hand or with robotic assistance.\u003c/p\u003e\n\u003cp\u003e\u0026#34;To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions,\u0026#34; the researchers wrote in their paper, which was \u003ca href=\"https://arxiv.org/pdf/2505.05469\"\u003eposted\u003c/a\u003e on arXiv, \u0026#34;and train an autoregressive large language model to predict the next brick to add via next-token prediction.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThis trained model generates Lego designs that match text prompts like \u0026#34;a streamlined, elongated vessel\u0026#34; or \u0026#34;a classic-style car with a prominent front grille.\u0026#34; The resulting designs are simple, using just a few brick types to create primitive shapes—but they stand up. As one Ars Technica staffer joked this morning upon seeing the research, \u0026#34;It builds Lego \u003ca href=\"https://www.toysperiod.com/lego-set-reference/by-decade/1970s/\"\u003elike it\u0026#39;s 1974\u003c/a\u003e.\u0026#34;\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cp\u003e\n    \u003cvideo id=\"video-2094193-1\" width=\"1280\" height=\"720\" preload=\"metadata\" controls=\"controls\"\u003e\u003csource type=\"video/mp4\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/05/Legogpt-White.mp4?_=1\"/\u003eA LegoGPT demo video assembled by the research team.\u003c/video\u003e\n  \u003c/p\u003e\n\n  \u003cfigcaption\u003e\n    \u003cspan\u003e\u003c/span\u003e\n    \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A LegoGPT demo video assembled by the research team.\n\n          \u003c/p\u003e\n  \u003c/div\u003e\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn the paper titled \u0026#34;Generating Physically Stable and Buildable Lego Designs from Text,\u0026#34; the research team led by \u003ca href=\"https://avapun.com/\"\u003eAva Pun\u003c/a\u003e explained that many existing 3D generation models focus on making diverse objects with detailed geometry, but these digital designs often can\u0026#39;t be physically made. \u0026#34;Without proper support, parts of the design can collapse, float, or remain disconnected,\u0026#34; they wrote.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eUnlike previous attempts at autonomous Lego modeling, LegoGPT reportedly produces step-by-step instructions for building Lego creations that don\u0026#39;t fall apart. You can \u003ca href=\"https://avalovelace1.github.io/LegoGPT/\"\u003esee demos\u003c/a\u003e of the system in action on the project\u0026#39;s website.\u003c/p\u003e\n\u003ch2\u003eHow LegoGPT works\u003c/h2\u003e\n\u003cp\u003eTo build LegoGPT, the Carnegie Mellon team repurposed the technology behind large language models (LLMs), similar to the kind that run ChatGPT, for \u0026#34;next-brick prediction\u0026#34; instead of next-word prediction. To do so, the team fine-tuned \u003ca href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\"\u003eLLaMA-3.2-1B-Instruct\u003c/a\u003e, an instruction-following language model from Meta.\u003c/p\u003e\n\u003cp\u003eThe team then augmented the brick-predicting model with a separate software tool that can verify physical stability using mathematical models that simulate gravity and structural forces.\u003c/p\u003e\n\u003cp\u003eTo train the model, the team assembled a new dataset called \u0026#34;StableText2Lego,\u0026#34; which contained over 47,000 stable Lego structures paired with descriptive captions generated by a separate AI model, OpenAI\u0026#39;s GPT-4o. Each structure underwent physics analysis to ensure it could be built in the real world.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"345\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/05/dataset-1024x345.jpg\" alt=\"To build the Lego dataset, the team fed images rendered from 24 different viewpoints into GPT-4o and let that model write captions for each LEGO structure, asking it to focus on geometric features while omitting color information.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/05/dataset-1024x345.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/dataset-640x216.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/dataset-768x259.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/dataset-1536x518.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/dataset-2048x690.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/dataset-980x330.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/dataset-1440x485.jpg 1440w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      To build the Lego dataset, the team fed images rendered from 24 different viewpoints into GPT-4o and let that model write captions for each Lego structure, asking it to focus on geometric features while omitting color information.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://avalovelace1.github.io/LegoGPT/\" target=\"_blank\"\u003e\n          \n          Pun et al.\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eLegoGPT works by first generating a sequence of precisely placed Lego bricks. For each new brick in the sequence, the system makes sure it doesn\u0026#39;t collide with existing bricks and that it fits within the building space. After completing a design, it uses the aforementioned mathematical models to verify that the model can stand upright without falling apart.\u003c/p\u003e\n\u003cp\u003eIf parts would collapse in real life, the system identifies the first unstable brick and backtracks, removing it and all subsequent bricks before trying a different approach. This \u0026#34;physics-aware rollback\u0026#34; method proved essential to the team\u0026#39;s approach. Without it, only 24 percent of designs remained standing, compared to 98.8 percent with the full system.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"459\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/05/pipeline-1024x459.jpg\" alt=\"The LegoGPT system works in three parts, shown in this diagram.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/05/pipeline-1024x459.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/pipeline-640x287.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/pipeline-768x344.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/pipeline-1536x689.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/pipeline-2048x918.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/pipeline-980x439.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/pipeline-1440x646.jpg 1440w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      The LegoGPT system works in three parts, shown in this diagram.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://avalovelace1.github.io/LegoGPT/\" target=\"_blank\"\u003e\n          \n          Pun et al.\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eThe researchers also expanded the system\u0026#39;s abilities by adding texture and color options. For example, using an appearance prompt like \u0026#34;Electric guitar in metallic purple,\u0026#34; LegoGPT can generate a guitar model, with bricks assigned a purple color.\u003c/p\u003e\n\u003ch2\u003eTesting with robots and humans\u003c/h2\u003e\n\u003cp\u003eTo prove their designs worked in real life, the researchers had robots assemble the AI-created Lego models. They used a dual-robot arm system with force sensors to pick up and place bricks according to the AI-generated instructions.\u003c/p\u003e\n\u003cp\u003eHuman testers also built some of the designs by hand, showing that the AI creates genuinely buildable models. \u0026#34;Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing Lego designs that align closely with the input text prompts,\u0026#34; the team noted in its paper.\u003c/p\u003e\n\u003cp\u003eWhen tested against other AI systems for 3D creation, LegoGPT stands out through its focus on structural integrity. The team tested against several alternatives, including \u003ca href=\"https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/\"\u003eLLaMA-Mesh\u003c/a\u003e and other 3D generation models, and found its approach produced the highest percentage of stable structures.\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cp\u003e\n    \u003cvideo id=\"video-2094193-2\" width=\"1280\" height=\"720\" preload=\"metadata\" controls=\"controls\"\u003e\u003csource type=\"video/mp4\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/05/guitar_8x.mp4?_=2\"/\u003eA video of two robot arms building a LegoGPT creation, provided by the researchers.\u003c/video\u003e\n  \u003c/p\u003e\n\n  \u003cfigcaption\u003e\n    \u003cspan\u003e\u003c/span\u003e\n    \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A video of two robot arms building a LegoGPT creation, provided by the researchers.\n\n          \u003c/p\u003e\n  \u003c/div\u003e\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eStill, there are some limitations. The current version of LegoGPT only works within a 20×20×20 building space and uses a mere eight standard brick types. \u0026#34;Our method currently supports a fixed set of commonly used Lego bricks,\u0026#34; the team acknowledged. \u0026#34;In future work, we plan to expand the brick library to include a broader range of dimensions and brick types, such as slopes and tiles.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThe researchers also hope to scale up their training dataset to include more objects than the 21 categories currently available. Meanwhile, others can literally build on their work—the researchers released their dataset, code, and models on their \u003ca href=\"https://avalovelace1.github.io/LegoGPT/\"\u003eproject website\u003c/a\u003e and \u003ca href=\"https://github.com/AvaLovelace1/LegoGPT/\"\u003eGitHub\u003c/a\u003e.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/benjedwards/\"\u003e\u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2022/08/benj_ega.png\" alt=\"Photo of Benj Edwards\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Benj Edwards is Ars Technica\u0026#39;s Senior AI Reporter and founder of the site\u0026#39;s dedicated AI beat in 2022. He\u0026#39;s also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/ai/2025/05/new-ai-model-generates-buildable-lego-creations-from-text-descriptions/#comments\" title=\"24 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    24 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \n\n\n  \n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/tech-policy/2025/05/bill-gates-accuses-elon-musk-of-killing-children-with-doge-led-usaid-cuts/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/05/gates-768x432.jpg\" alt=\"Listing image for first story in Most Read: Elon Musk is responsible for “killing the world’s poorest children,” says Bill Gates\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/main\u003e\n\n\n\n\n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-05-09T21:29:02Z",
  "modifiedTime": "2025-05-09T21:39:31Z"
}
