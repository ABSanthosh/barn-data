{
  "id": "1f1dcf66-8b4c-4ae8-8bc0-23efad4fcbe0",
  "title": "OpenAI releases new simulated reasoning models with full tool access",
  "link": "https://arstechnica.com/ai/2025/04/openai-releases-new-simulated-reasoning-models-with-full-tool-access/",
  "description": "New o3 model appears \"near-genius level,\" according to one doctor, but it still makes mistakes.",
  "author": "Benj Edwards",
  "published": "Wed, 16 Apr 2025 22:21:38 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "AI assistants",
    "Anthropic",
    "ChatGPT",
    "chatgtp",
    "Claude 3.7 Sonnet",
    "gemini",
    "Gemini 2.5 Pro",
    "google",
    "greg brockman",
    "large language models",
    "machine learning",
    "openai",
    "simulated reasoning",
    "SR models"
  ],
  "byline": "Benj Edwards",
  "length": 10493,
  "excerpt": "New o3 model appears “near-genius level,” according to one doctor, but it still makes mistakes.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "New o3 model appears \"near-genius level,\" according to one doctor, but it still makes mistakes. On Wednesday, OpenAI announced the release of two new models—o3 and o4-mini—that combine simulated reasoning capabilities with access to functions like web browsing and coding. These models mark the first time OpenAI's reasoning-focused models can use every ChatGPT tool simultaneously, including visual analysis and image generation. OpenAI announced o3 in December, and until now, only less capable derivative models named \"o3-mini\" and \"03-mini-high\" have been available. However, the new models replace their predecessors—o1 and o3-mini. OpenAI is rolling out access today for ChatGPT Plus, Pro, and Team users, with Enterprise and Edu customers gaining access next week. Free users can try o4-mini by selecting the \"Think\" option before submitting queries. OpenAI CEO Sam Altman tweeted that \"we expect to release o3-pro to the pro tier in a few weeks.\" For developers, both models are available starting today through the Chat Completions API and Responses API, though some organizations will need verification for access. \"These are the smartest models we've released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers,\" OpenAI claimed on its website. OpenAI says the models offer better cost efficiency than their predecessors, and each comes with a different intended use case: o3 targets complex analysis, while o4-mini, being a smaller version of its next-gen SR model \"o4\" (not yet released), optimizes for speed and cost-efficiency. OpenAI says o3 and o4-mini are multimodal, featuring the ability to \"think with images.\" Credit: OpenAI What sets these new models apart from OpenAI's other models (like GPT-4o and GPT-4.5) is their simulated reasoning capability, which uses a simulated step-by-step \"thinking\" process to solve problems. Additionally, the new models dynamically determine when and how to deploy aids to solve multistep problems. For example, when asked about future energy usage in California, the models can autonomously search for utility data, write Python code to build forecasts, generate visualizing graphs, and explain key factors behind predictions—all within a single query. OpenAI touts the new models' multimodal ability to incorporate images directly into their simulated reasoning process—not just analyzing visual inputs but actively \"thinking with\" them. This capability allows the models to interpret whiteboards, textbook diagrams, and hand-drawn sketches, even when images are blurry or of low quality. That said, the new releases continue OpenAI's tradition of selecting confusing product names that don't tell users much about each model's relative capabilities—for example, o3 is more powerful than o4-mini despite including a lower number. Then there's potential confusion with the firm's non-reasoning AI models. As Ars Technica contributor Timothy B. Lee noted today on X, \"It's an amazing branding decision to have a model called GPT-4o and another one called o4.\" Vibes and benchmarks All that aside, we know what you're thinking: What about the vibes? While we have not used 03 or o4-mini yet, frequent AI commentator and Wharton professor Ethan Mollick compared o3 favorably to Google's Gemini 2.5 Pro on Bluesky. \"After using them both, I think that Gemini 2.5 \u0026 o3 are in a similar sort of range (with the important caveat that more testing is needed for agentic capabilities),\" he wrote. \"Each has its own quirks \u0026 you will likely prefer one to another, but there is a gap between them \u0026 other models.\" During the livestream announcement for o3 and o4-mini today, OpenAI President Greg Brockman boldly claimed: \"These are the first models where top scientists tell us they produce legitimately good and useful novel ideas.\" Early user feedback seems to support this assertion, although until more third-party testing takes place, it's wise to be skeptical of the claims. On X, immunologist Dr. Derya Unutmaz said o3 appeared \"at or near genius level\" and wrote, \"It's generating complex incredibly insightful and based scientific hypotheses on demand! When I throw challenging clinical or medical questions at o3, its responses sound like they're coming directly from a top subspecialist physicians.\" OpenAI benchmark results for o3 and o4-mini SR models. Credit: OpenAI So the vibes seem on target, but what about numerical benchmarks? Here's an interesting one: OpenAI reports that o3 makes \"20 percent fewer major errors\" than o1 on difficult tasks, with particular strengths in programming, business consulting, and \"creative ideation.\" The company also reported state-of-the-art performance on several metrics. On the American Invitational Mathematics Examination (AIME) 2025, o4-mini achieved 92.7 percent accuracy. For programming tasks, o3 reached 69.1 percent accuracy on SWE-Bench Verified, a popular programming benchmark. The models also reportedly showed strong results on visual reasoning benchmarks, with o3 scoring 82.9 percent on MMMU (massive multi-disciplinary multimodal understanding), a college-level visual problem-solving test. OpenAI benchmark results for o3 and o4-mini SR models. Credit: OpenAI However, these benchmarks provided by OpenAI lack independent verification. One early evaluation of a pre-release o3 model by independent AI research lab Transluce found that the model exhibited recurring types of confabulations, such as claiming to run code locally or providing hardware specifications, and hypothesized this could be due to the model lacking access to its own reasoning processes from previous conversational turns. \"It seems that despite being incredibly powerful at solving math and coding tasks, o3 is not by default truthful about its capabilities,\" wrote Transluce in a tweet. Also, some evaluations from OpenAI include footnotes about methodology that bear consideration. For a \"Humanity's Last Exam\" benchmark result that measures expert-level knowledge across subjects (o3 scored 20.32 with no tools, but 24.90 with browsing and tools), OpenAI notes that browsing-enabled models could potentially find answers online. The company reports implementing domain blocks and monitoring to prevent what it calls \"cheating\" during evaluations. Even though early results seem promising overall, experts or academics who might try to rely on SR models for rigorous research should take the time to exhaustively determine whether the AI model actually produced an accurate result instead of assuming it is correct. And if you're operating the models outside your domain of knowledge, be careful accepting any results as accurate without independent verification. Pricing For ChatGPT subscribers, access to o3 and o4-mini is included with the subscription. On the API side (for developers who integrate the models into their apps), OpenAI has set o3's pricing at $10 per million input tokens and $40 per million output tokens, with a discounted rate of $2.50 per million for cached inputs. This represents a significant reduction from o1's pricing structure of $15/$60 per million input/output tokens—effectively a 33 percent price cut while delivering what OpenAI claims is improved performance. The more economical o4-mini costs $1.10 per million input tokens and $4.40 per million output tokens, with cached inputs priced at $0.275 per million tokens. This maintains the same pricing structure as its predecessor o3-mini, suggesting OpenAI is delivering improved capabilities without raising costs for its smaller reasoning model. Codex CLI OpenAI also introduced an experimental terminal application called Codex CLI, described as \"a lightweight coding agent you can run from your terminal.\" The open source tool connects the models to users' computers and local code. Alongside this release, the company announced a $1 million grant program offering API credits for projects using Codex CLI. A screenshot of OpenAI's new Codex CLI tool in action, taken from GitHub. Credit: OpenAI Codex CLI somewhat resembles Claude Code, an agent launched with Claude 3.7 Sonnet in February. Both are terminal-based coding assistants that operate directly from a console and can interact with local codebases. While Codex CLI connects OpenAI's models to users' computers and local code repositories, Claude Code was Anthropic's first venture into agentic tools, allowing Claude to search through codebases, edit files, write and run tests, and execute command line operations. Codex CLI is one more step toward OpenAI's goal of making autonomous agents that can execute multistep complex tasks on behalf of users. Let's hope all the vibe coding it produces isn't used in high-stakes applications without detailed human oversight. Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC. 30 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2025/04/digital_thinker_1-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n            \u003carticle data-id=\"2089454\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n      \n\n      \n\n      \u003cp\u003e\n        New o3 model appears \u0026#34;near-genius level,\u0026#34; according to one doctor, but it still makes mistakes.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \n\u003cp\u003eOn Wednesday, OpenAI \u003ca href=\"https://openai.com/index/introducing-o3-and-o4-mini/\"\u003eannounced\u003c/a\u003e the release of two new models—o3 and o4-mini—that combine simulated reasoning capabilities with access to functions like web browsing and coding. These models mark the first time OpenAI\u0026#39;s reasoning-focused models can use every ChatGPT tool simultaneously, including visual analysis and image generation.\u003c/p\u003e\n\u003cp\u003eOpenAI \u003ca href=\"https://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/\"\u003eannounced\u003c/a\u003e o3 in December, and until now, only less capable derivative models named \u0026#34;o3-mini\u0026#34; and \u0026#34;03-mini-high\u0026#34; have been available. However, the new models replace their predecessors—o1 and o3-mini.\u003c/p\u003e\n\u003cp\u003eOpenAI is rolling out access today for ChatGPT Plus, Pro, and Team users, with Enterprise and Edu customers gaining access next week. Free users can try o4-mini by selecting the \u0026#34;Think\u0026#34; option before submitting queries. OpenAI CEO Sam Altman \u003ca href=\"https://x.com/sama/status/1912558745013612888\"\u003etweeted\u003c/a\u003e that \u0026#34;we expect to release o3-pro to the pro tier in a few weeks.\u0026#34;\u003c/p\u003e\n\u003cp\u003eFor developers, both models are available starting today through the Chat Completions API and Responses API, though some organizations will need verification for access.\u003c/p\u003e\n\u003cp\u003e\u0026#34;These are the smartest models we\u0026#39;ve released to date, representing a step change in ChatGPT\u0026#39;s capabilities for everyone from curious users to advanced researchers,\u0026#34; OpenAI claimed on its website. OpenAI says the models offer better cost efficiency than their predecessors, and each comes with a different intended use case: o3 targets complex analysis, while o4-mini, being a smaller version of its next-gen SR model \u0026#34;o4\u0026#34; (not yet released), optimizes for speed and cost-efficiency.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"657\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_visual_reasoning-1024x657.jpg\" alt=\"OpenAI says o3 and o4-mini are multimodal, featuring the ability to \u0026#34;reason with images.\u0026#34;\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_visual_reasoning-1024x657.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_visual_reasoning-640x410.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_visual_reasoning-768x492.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_visual_reasoning-1536x985.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_visual_reasoning-980x628.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_visual_reasoning-1440x923.jpg 1440w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_visual_reasoning.jpg 1920w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      OpenAI says o3 and o4-mini are multimodal, featuring the ability to \u0026#34;think with images.\u0026#34;\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://openai.com/index/introducing-o3-and-o4-mini/\" target=\"_blank\"\u003e\n          \n          OpenAI\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\n\u003cp\u003eWhat sets these new models apart from OpenAI\u0026#39;s other models (like GPT-4o and GPT-4.5) is their simulated reasoning capability, which uses a simulated step-by-step \u0026#34;thinking\u0026#34; process to solve problems. Additionally, the new models dynamically determine when and how to deploy aids to solve multistep problems. For example, when asked about future energy usage in California, the models can autonomously search for utility data, write Python code to build forecasts, generate visualizing graphs, and explain key factors behind predictions—all within a single query.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eOpenAI touts the new models\u0026#39; multimodal ability to incorporate images directly into their simulated reasoning process—not just analyzing visual inputs but actively \u0026#34;thinking with\u0026#34; them. This capability allows the models to interpret whiteboards, textbook diagrams, and hand-drawn sketches, even when images are blurry or of low quality.\u003c/p\u003e\n\u003cp\u003eThat said, the new releases continue \u003ca href=\"https://arstechnica.com/ai/2025/04/when-is-4-1-greater-than-4-5-when-its-openais-newest-model/\"\u003eOpenAI\u0026#39;s tradition\u003c/a\u003e of selecting confusing product names that don\u0026#39;t tell users much about each model\u0026#39;s relative capabilities—for example, o3 is more powerful than o4-mini despite including a lower number. Then there\u0026#39;s potential confusion with the firm\u0026#39;s non-reasoning AI models. As Ars Technica contributor Timothy B. Lee \u003ca href=\"https://x.com/binarybits/status/1912565262349660580\"\u003enoted\u003c/a\u003e today on X, \u0026#34;It\u0026#39;s an amazing branding decision to have a model called GPT-4o and another one called o4.\u0026#34;\u003c/p\u003e\n\u003ch2\u003eVibes and benchmarks\u003c/h2\u003e\n\u003cp\u003eAll that aside, we know what you\u0026#39;re thinking: What about the vibes? While we have not used 03 or o4-mini yet, frequent AI commentator and Wharton professor Ethan Mollick \u003ca href=\"https://bsky.app/profile/emollick.bsky.social/post/3lmx7wozzdk26\"\u003ecompared\u003c/a\u003e o3 favorably to \u003ca href=\"https://arstechnica.com/ai/2025/03/google-says-the-new-gemini-2-5-pro-model-is-its-smartest-ai-yet/\"\u003eGoogle\u0026#39;s Gemini 2.5 Pro\u003c/a\u003e on Bluesky. \u0026#34;After using them both, I think that Gemini 2.5 \u0026amp; o3 are in a similar sort of range (with the important caveat that more testing is needed for agentic capabilities),\u0026#34; he wrote. \u0026#34;Each has its own quirks \u0026amp; you will likely prefer one to another, but there is a gap between them \u0026amp; other models.\u0026#34;\u003c/p\u003e\n\u003cp\u003eDuring the livestream announcement for o3 and o4-mini today, OpenAI President Greg Brockman boldly claimed: \u0026#34;These are the first models where top scientists tell us they produce legitimately good and useful novel ideas.\u0026#34;\u003c/p\u003e\n\u003cp\u003eEarly user feedback seems to support this assertion, although until more third-party testing takes place, it\u0026#39;s wise to be skeptical of the claims. On X, immunologist \u003ca href=\"https://scholar.google.com/citations?user=aND7Gh0AAAAJ\u0026amp;hl=en\"\u003eDr. Derya Unutmaz\u003c/a\u003e said o3 appeared \u0026#34;at or near genius level\u0026#34; and \u003ca href=\"https://x.com/DeryaTR_/status/1912558350794961168\"\u003ewrote\u003c/a\u003e, \u0026#34;It\u0026#39;s generating complex incredibly insightful and based scientific hypotheses on demand! When I throw challenging clinical or medical questions at o3, its responses sound like they\u0026#39;re coming directly from a top subspecialist physicians.\u0026#34;\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"970\" height=\"552\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3-coding.jpg\" alt=\"OpenAI benchmark results for o3 and o4-mini SR models.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3-coding.jpg 970w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3-coding-640x364.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3-coding-768x437.jpg 768w\" sizes=\"auto, (max-width: 970px) 100vw, 970px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      OpenAI benchmark results for o3 and o4-mini SR models.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://openai.com/index/introducing-o3-and-o4-mini/\" target=\"_blank\"\u003e\n          \n          OpenAI\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eSo the vibes seem on target, but what about numerical benchmarks? Here\u0026#39;s an interesting one: OpenAI reports that o3 makes \u0026#34;20 percent fewer major errors\u0026#34; than o1 on difficult tasks, with particular strengths in programming, business consulting, and \u0026#34;creative ideation.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThe company also reported state-of-the-art performance on several metrics. On the American Invitational Mathematics Examination (\u003ca href=\"https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination?srsltid=AfmBOoqMFrsf5REnSMJTX6MOZa9YEz8nVrGVho3WGGTrXsetAbhVVnrS\"\u003eAIME\u003c/a\u003e) 2025, o4-mini achieved 92.7 percent accuracy. For programming tasks, o3 reached 69.1 percent accuracy on \u003ca href=\"https://openai.com/index/introducing-swe-bench-verified/\"\u003eSWE-Bench Verified\u003c/a\u003e, a popular programming benchmark. The models also reportedly showed strong results on visual reasoning benchmarks, with o3 scoring 82.9 percent on MMMU (massive multi-disciplinary multimodal understanding), a college-level visual problem-solving test.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"858\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_benchmarks_1-1024x858.jpg\" alt=\"OpenAI benchmark results for o3 and o4-mini SR models.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_benchmarks_1-1024x858.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_benchmarks_1-640x536.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_benchmarks_1-768x643.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_benchmarks_1-980x821.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/o3_benchmarks_1.jpg 1120w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      OpenAI benchmark results for o3 and o4-mini SR models.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://openai.com/index/introducing-o3-and-o4-mini/\" target=\"_blank\"\u003e\n          \n          OpenAI\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eHowever, these benchmarks provided by OpenAI lack independent verification. One \u003ca href=\"https://transluce.org/investigating-o3-truthfulness\"\u003eearly evaluation\u003c/a\u003e of a pre-release o3 model by independent AI research lab \u003ca href=\"https://transluce.org/\"\u003eTransluce\u003c/a\u003e found that the model exhibited recurring types of confabulations, such as claiming to run code locally or providing hardware specifications, and hypothesized this could be due to the model lacking access to its own reasoning processes from previous conversational turns. \u0026#34;It seems that despite being incredibly powerful at solving math and coding tasks, o3 is not by default truthful about its capabilities,\u0026#34; \u003ca href=\"https://x.com/TransluceAI/status/1912552068717637980\"\u003ewrote\u003c/a\u003e Transluce in a tweet.\u003c/p\u003e\n\u003cp\u003eAlso, some evaluations from OpenAI include footnotes about methodology that bear consideration. For a \u0026#34;\u003ca href=\"https://agi.safe.ai/\"\u003eHumanity\u0026#39;s Last Exam\u003c/a\u003e\u0026#34; benchmark result that measures expert-level knowledge across subjects (o3 scored 20.32 with no tools, but 24.90 with browsing and tools), OpenAI notes that browsing-enabled models could potentially find answers online. The company reports implementing domain blocks and monitoring to prevent what it calls \u0026#34;cheating\u0026#34; during evaluations.\u003c/p\u003e\n\u003cp\u003eEven though early results seem promising overall, experts or academics who might try to rely on SR models for rigorous research should take the time to exhaustively determine whether the AI model actually produced an accurate result instead of assuming it is correct. And if you\u0026#39;re operating the models outside your domain of knowledge, be careful accepting any results as accurate without independent verification.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003ch2\u003ePricing\u003c/h2\u003e\n\u003cp\u003eFor ChatGPT subscribers, access to o3 and o4-mini is included with the subscription. On the API side (for developers who integrate the models into their apps), OpenAI has set o3\u0026#39;s pricing at $10 per million input tokens and $40 per million output tokens, with a discounted rate of $2.50 per million for cached inputs. This represents a significant reduction from o1\u0026#39;s pricing structure of $15/$60 per million input/output tokens—effectively a 33 percent price cut while delivering what OpenAI claims is improved performance.\u003c/p\u003e\n\u003cp\u003eThe more economical o4-mini costs $1.10 per million input tokens and $4.40 per million output tokens, with cached inputs priced at $0.275 per million tokens. This maintains the same pricing structure as its predecessor o3-mini, suggesting OpenAI is delivering improved capabilities without raising costs for its smaller reasoning model.\u003c/p\u003e\n\u003ch2\u003eCodex CLI\u003c/h2\u003e\n\u003cp\u003eOpenAI also introduced an experimental terminal application called \u003ca href=\"https://github.com/openai/codex\"\u003eCodex CLI\u003c/a\u003e, described as \u0026#34;a lightweight coding agent you can run from your terminal.\u0026#34; The open source tool connects the models to users\u0026#39; computers and local code. Alongside this release, the company announced a $1 million grant program offering API credits for projects using Codex CLI.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"589\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/codex_cli_screenshot-1024x589.jpg\" alt=\"A screenshot of OpenAI\u0026#39;s new Codex CLI tool in action, taken from GitHub.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/codex_cli_screenshot-1024x589.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/codex_cli_screenshot-640x368.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/codex_cli_screenshot-768x442.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/codex_cli_screenshot-980x564.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/codex_cli_screenshot.jpg 1038w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A screenshot of OpenAI\u0026#39;s new Codex CLI tool in action, taken from GitHub.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://github.com/openai/codex\" target=\"_blank\"\u003e\n          \n          OpenAI\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eCodex CLI somewhat resembles Claude Code, an agent \u003ca href=\"https://arstechnica.com/ai/2025/02/claude-3-7-sonnet-debuts-with-extended-thinking-to-tackle-complex-problems/\"\u003elaunched with Claude 3.7 Sonnet\u003c/a\u003e in February. Both are terminal-based coding assistants that operate directly from a console and can interact with local codebases. While Codex CLI connects OpenAI\u0026#39;s models to users\u0026#39; computers and local code repositories, Claude Code was Anthropic\u0026#39;s first venture into agentic tools, allowing Claude to search through codebases, edit files, write and run tests, and execute command line operations.\u003c/p\u003e\n\u003cp\u003eCodex CLI is one more step toward OpenAI\u0026#39;s goal of making autonomous agents that can execute multistep complex tasks on behalf of users. Let\u0026#39;s hope all the \u003ca href=\"https://arstechnica.com/ai/2025/03/is-vibe-coding-with-ai-gnarly-or-reckless-maybe-some-of-both/\"\u003evibe coding\u003c/a\u003e it produces isn\u0026#39;t used in high-stakes applications without detailed human oversight.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/benjedwards/\"\u003e\u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2022/08/benj_ega.png\" alt=\"Photo of Benj Edwards\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Benj Edwards is Ars Technica\u0026#39;s Senior AI Reporter and founder of the site\u0026#39;s dedicated AI beat in 2022. He\u0026#39;s also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/ai/2025/04/openai-releases-new-simulated-reasoning-models-with-full-tool-access/#comments\" title=\"30 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    30 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \n\n\n  \n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/security/2025/04/microsoft-is-putting-privacy-endangering-recall-back-into-windows-11/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/win11-recall-listing-768x432.jpg\" alt=\"Listing image for first story in Most Read: That groan you hear is users’ reaction to Recall going back into Windows\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2025-04-16T22:21:38Z",
  "modifiedTime": "2025-04-16T22:21:38Z"
}
