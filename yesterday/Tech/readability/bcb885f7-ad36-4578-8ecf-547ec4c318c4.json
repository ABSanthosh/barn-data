{
  "id": "bcb885f7-ad36-4578-8ecf-547ec4c318c4",
  "title": "Responsible AI: Our 2024 report and ongoing work",
  "link": "https://blog.google/technology/ai/responsible-ai-2024-report-ongoing-work/",
  "description": "We’re publishing our 2024 Responsible AI Progress Report and updating our Frontier Safety Framework and AI Principles.",
  "author": "Demis HassabisCEO and Co-FounderGoogle DeepMind",
  "published": "Tue, 04 Feb 2025 17:15:00 +0000",
  "source": "https://www.blog.google/rss/",
  "categories": [
    "AI"
  ],
  "byline": "James Manyika",
  "length": 8209,
  "excerpt": "We’re publishing our 2024 Responsible AI Progress Report and updating our Frontier Safety Framework and AI Principles.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png?version=pr20250121-1756",
  "text": "Feb 04, 2025 [[read-time]] min read Two years ago, we published our vision for advancing AI to serve society and propel innovation. A couple of weeks ago, we published the 2024 roundup of our tremendous progress towards that vision, from new state-of-the-art models empowering creativity to AI-enabled breakthroughs in biology, health research, and neuroscience.Being bold on AI also means being responsible from the start. That’s why our approach to AI has been consistently grounded in understanding and accounting for its broad implications for people. We were among the first organizations to publish AI principles in 2018 and have published an annual transparency report since 2019, and we consistently review our policies, practices and frameworks, and update them when the need arises. The 2024 Responsible AI Progress ReportOur 6th annual Responsible AI Progress Report details how we govern, map, measure and manage AI risk throughout the AI development lifecycle. The report highlights the progress we have made over the past year building out governance structures for our AI product launches.We are investing more than ever in both AI research and products that benefit people and society, and in AI safety and efforts to identify and address potential risks.Last year’s Report includes highlights from some of the over 300 research papers our teams have published on responsibility and safety topics, updates to our responsible AI policies, principles and frameworks, and key things we’ve learned from red teaming and evaluations that took place against safety, privacy, and security benchmarks. It also describes progress we’ve made on risk mitigation techniques across different gen AI launches — including better safety tuning and filters, security and privacy controls, the use of provenance technology in our products, and broad AI literacy education. Throughout 2024, we also supported the broader AI ecosystem through funding, tools, and standards development, as detailed in the report. An update to our Frontier Safety FrameworkAs AI development progresses, new capabilities may present new risks. That’s why we introduced the first iteration of our Frontier Safety Framework last year: a set of protocols to help us stay ahead of possible risks from powerful frontier AI models. Since then, we've collaborated with experts in industry, academia and government to deepen our understanding of the risks, the empirical evaluations to test for them, and the mitigations we can apply.We have also implemented the Framework in our Google DeepMind safety and governance processes for evaluating frontier models such as Gemini 2.0. Today we’re publishing an updated Frontier Safety Framework, which includes:Recommendations for Heightened Security: helping to identify where the strongest efforts to curb exfiltration risk are needed.Deployment Mitigations Procedure: focusing on preventing the misuse of critical capabilities in the systems we deploy.Deceptive Alignment Risk: addressing the risk of an autonomous system deliberately undermining human control.You can read more on the Google DeepMind blog. Updating AI PrinciplesSince we first published our AI Principles in 2018, the technology has evolved rapidly. Billions of people are using AI in their everyday lives. AI has become a general-purpose technology, and a platform which countless organizations and individuals use to build applications. It has moved from a niche research topic in the lab to a technology that is becoming as pervasive as mobile phones and the internet itself; one with numerous beneficial uses for society and people around the world, supported by a vibrant AI ecosystem of developers.Common baseline principles are an important part of this evolution. In addition to AI companies and academic institutions, we're encouraged by the progress we’ve seen on AI principles globally. The G7 and the International Organization for Standardization, as well as individual democratic nations, have all published frameworks to guide the safe development and use of AI. Increasingly, organizations and governments are able to look to these common standards as they consider how best to build, regulate, and deploy this evolving technology — our Responsible AI Progress Report, for example, is now based on the United States’ NIST Risk Management Framework. Our experience and research over recent years, along with threat intelligence, expertise, and best practices we’ve shared with other AI companies, have deepened our understanding of AI's potential and risks.There’s a global competition taking place for AI leadership within an increasingly complex geopolitical landscape. We believe democracies should lead in AI development, guided by core values like freedom, equality, and respect for human rights. And we believe that companies, governments, and organizations sharing these values should work together to create AI that protects people, promotes global growth, and supports national security.With that backdrop, we’re updating our own AI Principles to focus on three core tenets:Bold Innovation: We develop AI to assist, empower, and inspire people in almost every field of human endeavor, drive economic progress and improve lives, enable scientific breakthroughs, and help address humanity's biggest challenges.Responsible Development and Deployment: Because we understand that AI, as a still-emerging transformative technology, poses new complexities and risks, we consider it an imperative to pursue AI responsibly throughout the development and deployment lifecycle — from design to testing to deployment to iteration — learning as AI advances and uses evolve.Collaborative Progress, Together: We learn from others, and build technology that empowers others to harness AI positively.You can read our full AI Principles on AI.google.Guided by our AI Principles, we will continue to focus on AI research and applications that align with our mission, our scientific focus, and our areas of expertise, and stay consistent with widely accepted principles of international law and human rights — always evaluating specific work by carefully assessing whether the benefits substantially outweigh potential risks. We’ll also take into account whether our engagements require bespoke research and development or rely on general purpose, widely-available technology. These assessments are particularly important as AI is increasingly being developed by numerous organizations and governments for uses in fields like healthcare, science, robotics, cybersecurity, transportation, national security, energy, climate, and more.Of course, in addition to the Principles, we continue to have specific product policies and clear terms of use that contain prohibitions like illegal use of our services. The opportunity aheadWe recognize how quickly the underlying technology — and the debate around AI’s advancement, deployment, and uses — will continue to evolve, and we will continue to adapt and refine our approach as we all learn over time.As we see AGI, in particular, coming into sharper focus, the societal implications become incredibly profound. This isn't just about developing powerful AI; it's about building the most transformative technology in human history, using it to solve humanity’s biggest challenges, and ensuring that the right safeguards and governance are in place, for the benefit of the world. We’ll share our progress and findings about this journey, and expect to continue to evolve our thinking, as we move closer to AGI.As we move forward, we believe that the improvements we’ve made over the last year to our governance and other processes, our new Frontier Safety Framework, and our AI Principles position us well for the next phase of AI transformation. The opportunity of AI to assist and improve the lives of people around the world is what ultimately drives us in this work, and we will continue to pursue our bold, responsible, and collaborative approach to AI.",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/3a_SocialShare_K7eATvo.width-1300.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\n\n    \n    \n\n\n\n\n\n    \n\n    \n      \n\n\u003cdiv data-analytics-module=\"{\n    \u0026#34;module_name\u0026#34;: \u0026#34;Hero Menu\u0026#34;,\n    \u0026#34;section_header\u0026#34;: \u0026#34;Responsible AI: Our 2024 report and ongoing work\u0026#34;\n  }\"\u003e\n          \n            \u003cp\u003eFeb 04, 2025\u003c/p\u003e\n          \n          \n            \u003cp data-reading-time-render=\"\"\u003e[[read-time]] min read\u003c/p\u003e\n          \n        \u003c/div\u003e\n\n    \n\n    \n      \n\n    \n\n    \n    \u003cdiv data-reading-time=\"true\" data-component=\"uni-article-body\"\u003e\n\n            \n              \n\n\n\n\n\u003cgoogle-read-aloud-player data-analytics-module=\"{\n        \u0026#34;event\u0026#34;: \u0026#34;module_impression\u0026#34;,\n        \u0026#34;module_name\u0026#34;: \u0026#34;ai_audio\u0026#34;,\n        \u0026#34;section_header\u0026#34;: \u0026#34;Responsible AI: Our 2024 report and ongoing work\u0026#34;\n    }\" data-call-to-action-text=\"Listen to article\" data-date-modified=\"2025-02-04T20:23:57.615510+00:00\" data-progress-bar-style=\"half-wave\" data-api-key=\"AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac\" data-article-style=\"style9\" data-tracking-ids=\"G-HGNBTNCHCQ,G-6NKTLKV14N\" data-voice-list=\"en.ioh-pngnat:Cyan,en.usb-pngnat:Lime\" data-layout-style=\"style1\" data-highlight-mode=\"word-over-paragraph\" data-highlight-text-color=\"#000000\" data-highlight-word-background=\"#8AB4F8\" data-highlight-paragraph-background=\"#D2E3FC\" data-background=\"linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)\" data-foreground-color=\"#202124\" data-font=\"600 16px Google Sans, sans-serif\" data-box-shadow=\"0px 1px 3px 1px rgba(60, 64, 67, 0.15)\"\u003e\n\u003c/google-read-aloud-player\u003e\n\n\n\n            \n\n            \n            \n\n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Responsible AI: Our 2024 report and ongoing work\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"uxgce\"\u003eTwo years ago, we published \u003ca href=\"https://ai.google/advancing-ai/why-ai/\"\u003eour vision\u003c/a\u003e for advancing AI to serve society and propel innovation. A couple of weeks ago, we published the \u003ca href=\"https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/\"\u003e2024 roundup\u003c/a\u003e of our tremendous progress towards that vision, from new state-of-the-art models empowering creativity to AI-enabled breakthroughs in biology, health research, and neuroscience.\u003c/p\u003e\u003cp data-block-key=\"4u6fn\"\u003eBeing bold on AI also means being responsible from the start. That’s why our approach to AI has been consistently grounded in understanding and accounting for its broad implications for people. We were among the first organizations to publish AI principles in 2018 and have published an \u003ca href=\"https://ai.google/responsibility/principles/\"\u003eannual transparency report\u003c/a\u003e since 2019, and we consistently review our policies, practices and frameworks, and update them when the need arises.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Responsible AI: Our 2024 report and ongoing work\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"uxgce\"\u003eThe 2024 Responsible AI Progress Report\u003c/h2\u003e\u003cp data-block-key=\"al393\"\u003eOur 6th annual \u003ca href=\"https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\"\u003eResponsible AI Progress Report\u003c/a\u003e details how we \u003cb\u003egovern\u003c/b\u003e, \u003cb\u003emap\u003c/b\u003e, \u003cb\u003emeasure\u003c/b\u003e and \u003cb\u003emanage\u003c/b\u003e AI risk throughout the AI development lifecycle. The report highlights the progress we have made over the past year building out governance structures for our AI product launches.\u003c/p\u003e\u003cp data-block-key=\"75rbt\"\u003eWe are investing more than ever in both AI research and products that benefit people and society, and in AI safety and efforts to identify and address potential risks.\u003c/p\u003e\u003cp data-block-key=\"3a5u2\"\u003eLast year’s Report includes highlights from some of the over 300 research papers our teams have published on responsibility and safety topics, updates to our responsible AI policies, principles and frameworks, and key things we’ve learned from red teaming and evaluations that took place against safety, privacy, and security benchmarks. It also describes progress we’ve made on risk mitigation techniques across different gen AI launches — including better safety tuning and filters, security and privacy controls, the use of provenance technology in our products, and broad AI literacy education. Throughout 2024, we also supported the broader AI ecosystem through funding, tools, and standards development, as detailed in the report.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Responsible AI: Our 2024 report and ongoing work\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"uxgce\"\u003eAn update to our Frontier Safety Framework\u003c/h2\u003e\u003cp data-block-key=\"dfg29\"\u003eAs AI development progresses, new capabilities may present new risks. That’s why we \u003ca href=\"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/\"\u003eintroduced\u003c/a\u003e the first iteration of our Frontier Safety Framework last year: a set of protocols to help us stay ahead of possible risks from powerful frontier AI models. Since then, we\u0026#39;ve collaborated with experts in industry, academia and government to deepen our understanding of the risks, the empirical evaluations to test for them, and the mitigations we can apply.\u003c/p\u003e\u003cp data-block-key=\"1i9ek\"\u003eWe have also implemented the Framework in our Google DeepMind safety and governance processes for evaluating frontier models such as Gemini 2.0. Today we’re publishing an \u003ca href=\"https://deepmind.google/discover/blog/updating-the-frontier-safety-framework\"\u003eupdated Frontier Safety Framework\u003c/a\u003e, which includes:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"58ckn\"\u003e\u003cb\u003eRecommendations for Heightened Security:\u003c/b\u003e helping to identify where the strongest efforts to curb exfiltration risk are needed.\u003c/li\u003e\u003cli data-block-key=\"dc583\"\u003e\u003cb\u003eDeployment Mitigations Procedure:\u003c/b\u003e focusing on preventing the misuse of critical capabilities in the systems we deploy.\u003c/li\u003e\u003cli data-block-key=\"4oipn\"\u003e\u003cb\u003eDeceptive Alignment Risk:\u003c/b\u003e addressing the risk of an autonomous system deliberately undermining human control.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"cdkfu\"\u003eYou can read more on the \u003ca href=\"https://deepmind.google/discover/blog/updating-the-frontier-safety-framework\"\u003eGoogle DeepMind blog\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Responsible AI: Our 2024 report and ongoing work\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"uxgce\"\u003eUpdating AI Principles\u003c/h2\u003e\u003cp data-block-key=\"818mn\"\u003eSince we first published our AI Principles in 2018, the technology has evolved rapidly. Billions of people are using AI in their everyday lives. AI has become a general-purpose technology, and a platform which countless organizations and individuals use to build applications. It has moved from a niche research topic in the lab to a technology that is becoming as pervasive as mobile phones and the internet itself; one with numerous beneficial uses for society and people around the world, supported by a vibrant AI ecosystem of developers.\u003c/p\u003e\u003cp data-block-key=\"bn1un\"\u003eCommon baseline principles are an important part of this evolution. In addition to AI companies and academic institutions, we\u0026#39;re encouraged by the progress we’ve seen on AI principles globally. The \u003ca href=\"https://www.ey.com/en_gl/insights/ai/g7-ai-principles-and-code-of-conduct\"\u003eG7\u003c/a\u003e and the \u003ca href=\"https://www.iso.org/standard/81230.html\"\u003eInternational Organization for Standardization\u003c/a\u003e, as well as individual democratic nations, have all published frameworks to guide the safe development and use of AI. Increasingly, organizations and governments are able to look to these common standards as they consider how best to build, regulate, and deploy this evolving technology — our Responsible AI Progress Report, for example, is now based on the United States’ \u003ca href=\"https://www.nist.gov/itl/ai-risk-management-framework\"\u003eNIST Risk Management Framework\u003c/a\u003e. Our experience and research over recent years, along with threat intelligence, expertise, and best practices we’ve shared with other AI companies, have deepened our understanding of AI\u0026#39;s potential and risks.\u003c/p\u003e\u003cp data-block-key=\"4tj6g\"\u003eThere’s a global competition taking place for AI leadership within an increasingly complex geopolitical landscape. We believe democracies should lead in AI development, guided by core values like freedom, equality, and respect for human rights. And we believe that companies, governments, and organizations sharing these values should work together to create AI that protects people, promotes global growth, and supports national security.\u003c/p\u003e\u003cp data-block-key=\"5dac5\"\u003eWith that backdrop, we’re updating our own AI Principles to focus on three core tenets:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"303cm\"\u003e\u003cb\u003eBold Innovation:\u003c/b\u003e We develop AI to assist, empower, and inspire people in almost every field of human endeavor, drive economic progress and improve lives, enable scientific breakthroughs, and help address humanity\u0026#39;s biggest challenges.\u003cbr/\u003e\u003c/li\u003e\u003cli data-block-key=\"5c0oj\"\u003e\u003cb\u003eResponsible Development and Deployment:\u003c/b\u003e Because we understand that AI, as a still-emerging transformative technology, poses new complexities and risks, we consider it an imperative to pursue AI responsibly throughout the development and deployment lifecycle — from design to testing to deployment to iteration — learning as AI advances and uses evolve.\u003cbr/\u003e\u003c/li\u003e\u003cli data-block-key=\"ufc3\"\u003e\u003cb\u003eCollaborative Progress, Together:\u003c/b\u003e We learn from others, and build technology that empowers others to harness AI positively.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"6tkkq\"\u003eYou can read our full AI Principles on \u003ca href=\"https://ai.google/responsibility/principles/\"\u003eAI.google\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"3doem\"\u003eGuided by our AI Principles, we will continue to focus on AI research and applications that align with our mission, our scientific focus, and our areas of expertise, and stay consistent with widely accepted principles of international law and human rights — always evaluating specific work by carefully assessing whether the benefits substantially outweigh potential risks. We’ll also take into account whether our engagements require bespoke research and development or rely on general purpose, widely-available technology. These assessments are particularly important as AI is increasingly being developed by numerous organizations and governments for uses in fields like healthcare, science, robotics, cybersecurity, transportation, national security, energy, climate, and more.\u003c/p\u003e\u003cp data-block-key=\"72o3m\"\u003eOf course, in addition to the Principles, we continue to have specific product policies and clear terms of use that contain prohibitions like illegal use of our services.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Responsible AI: Our 2024 report and ongoing work\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"uxgce\"\u003eThe opportunity ahead\u003c/h2\u003e\u003cp data-block-key=\"6vt9m\"\u003eWe recognize how quickly the underlying technology — and the debate around AI’s advancement, deployment, and uses — will continue to evolve, and we will continue to adapt and refine our approach as we all learn over time.\u003c/p\u003e\u003cp data-block-key=\"2m6j7\"\u003eAs we see AGI, in particular, coming into sharper focus, the societal implications become incredibly profound. This isn\u0026#39;t just about developing powerful AI; it\u0026#39;s about building the most transformative technology in human history, using it to solve humanity’s biggest challenges, and ensuring that the right safeguards and governance are in place, for the benefit of the world. We’ll share our progress and findings about this journey, and expect to continue to evolve our thinking, as we move closer to AGI.\u003c/p\u003e\u003cp data-block-key=\"erlpp\"\u003eAs we move forward, we believe that the improvements we’ve made over the last year to our governance and other processes, our new Frontier Safety Framework, and our AI Principles position us well for the next phase of AI transformation. The opportunity of AI to assist and improve the lives of people around the world is what ultimately drives us in this work, and we will continue to pursue our bold, responsible, and collaborative approach to AI.\u003c/p\u003e\u003c/div\u003e\n  \n\n\n            \n            \n\n            \n              \n\n\n\n\n            \n          \u003c/div\u003e\n  \u003c/article\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2025-02-04T17:15:00Z",
  "modifiedTime": null
}
