{
  "id": "037bb28b-13d8-4d85-9111-cd7c6a60fccd",
  "title": "Nvidia Is Full of Shit",
  "link": "https://blog.sebin-nyshkim.net/posts/nvidia-is-full-of-shit/",
  "description": "Comments",
  "author": "",
  "published": "Fri, 04 Jul 2025 21:58:25 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Sebin Nyshkim",
  "length": 31235,
  "excerpt": "Since the disastrous launch of the RTX 50 series, NVIDIA has been unable to escape negative headlines: scalper bots are snatching GPUs away from consumers before official sales even begin, power connectors continue to melt, with no fix in sight, marketing is becoming increasingly deceptive, GPUs are missing processing units when they leave the factory, and the drivers, for which NVIDIA has always been praised, are currently falling apart. And to top it all off, NVIDIA is becoming increasingly insistent that media push a certain narrative when reporting on their hardware.",
  "siteName": "Sebin's Blog",
  "favicon": "https://img.sebin-nyshkim.net/i/b6629b72-ab77-4a6c-bf97-b1a615cc2454.png",
  "text": "Since the disastrous launch of the RTX 50 series, NVIDIA has been unable to escape negative headlines: scalper bots are snatching GPUs away from consumers before official sales even begin, power connectors continue to melt, with no fix in sight, marketing is becoming increasingly deceptive, GPUs are missing processing units when they leave the factory, and the drivers, for which NVIDIA has always been praised, are currently falling apart. And to top it all off, NVIDIA is becoming increasingly insistent that media push a certain narrative when reporting on their hardware.What‚Äôs an MSRP anyway?Just like with every other GPU launch in recent memory, this one has also been ripe with scalper bots snatching up stock before any real person could get any for themselves. Retailers have reported that they‚Äôve received very little stock to begin with. This in turn sparked rumors about NVIDIA purposefully keeping stock low to make it look like the cards are in high demand to drive prices. And sure enough, on secondary markets, the cards go way above MSRP and some retailers have started to bundle the cards with other inventory (PSUs, monitors, keyboards and mice, etc.) to inflate the price even further and get rid of stuff in their warehouse people wouldn‚Äôt buy otherwise‚Äîand you don‚Äôt even get a working computer out of spending over twice as much as a GPU alone would cost you.Newegg selling the ASUS ROG Astral GeForce RTX 5090 for $3,359 (MSRP: $1,999)eBay Germany offering the same ASUS ROG Astral RTX 5090 for ‚Ç¨3,349,95 (MSRP: ‚Ç¨2,229)I had a look at GPU prices for previous generation models for both AMD and NVIDIA as recently as May 2025 and I wasn‚Äôt surprised to find even RTX 40 series are still very much overpriced, with the GeForce RTX 4070 (lower mid-tier) starting at $800 (MSRP: $599), whereas the same money can get you a Radeon RX 7900 XT (the second best GPU in AMD‚Äôs last generation lineup). The discrepancy in bang for buck couldn‚Äôt be more jarring. And that‚Äôs before considering that NVIDIA gave out defective chips to board partners that were missing ROPs (Raster Operations Pipelines) from the factory, thus reducing their performance. Or, how NVIDIA put it in a statement to The Verge:We have identified a rare issue affecting less than 0.5% (half a percent) of GeForce RTX 5090 / 5090D and 5070 Ti GPUs which have one fewer ROP than specified. The average graphical performance impact is 4%, with no impact on AI and Compute workloads. Affected consumers can contact the board manufacturer for a replacement. The production anomaly has been corrected.Those 4% can make an RTX 5070 Ti perform at the levels of an RTX 4070 Ti Super, completely eradicating the reason you‚Äôd get an RTX 5070 Ti in the first place. Not to mention that the generational performance uplift over the RTX 40 series was already received quite poorly in general. NVIDIA also had to later amend their statement to The Verge and admit the RTX 5080 was also missing ROPs.It‚Äôs adding insult to injury with the cards‚Äô general unobtainium and it becomes even more ridiculous when you compare NVIDIA to another trillion dollar company that is also in the business of selling hardware to consumers: Apple.How is it that one can supply customers with enough stock on launch consistently for decades, and the other can‚Äôt? The only reason I can think of is, that NVIDIA just doesn‚Äôt care. They‚Äôre making the big bucks with data center GPUs now, selling the shovels that drive the ‚ÄúAI‚Äù bullshit gold rush, to the point that selling to consumers is increasingly becoming a rounding error on their balance sheets.These cards are üî•üî•üî• (and not the good kind)The RTX 50 series are the second generation of NVIDIA cards to use the 12VHPWR connector. The RTX 40 series became infamous as the GPU series with melting power connectors. So did they fix that?No. The cables can still melt, both on the GPU and PSU. It‚Äôs a design flaw in the board of the GPU itself which cannot be fixed unless the circuitry of the cards is replaced with a new design.With the RTX 30 cards, each power input (i.e. the cables from the power supply) had its own shunt resistor[1]. If one pin in a power input had not been connected properly, another pin would have had to take over in its stead. If both pins were not carrying any current, there would have been no phase on the shunt resistor and the card would not have started up. You‚Äôd get a black screen, but the hardware would still be fine.NVIDIA, in its infinite wisdom, changed this design starting with the RTX 40 series.Instead of individual shunt resistors for each power input, the shunt resistors are now connected in parallel to all pins of the power input from a single 12VHPWR connector. Additionally, the lines are recombined behind the resistors. This mind-boggling design flaw makes it impossible for the card to detect if pins are unevenly loaded, since as much as the card is concerned, everything comes in through the same single line.Connecting the shunt resistors in parallel also makes them pretty much useless since if one fails, the other will still have a phase and the card will happily keep drawing power and not be any the wiser. If the card is supplied with 100W on each pin and 5 of the 6 pins don‚Äôt supply a current, then a single pin has to supply the entire 600W the card demands. No wire is designed for this amount of power draw. As a result, excessive friction occurs from too many electrons traveling through the cable all at once and it melts (see: Joule heating).NVIDIA realized that the design around the shunt resistors in the RTX 40 series was kinda stupid, so they revised it: by eliminating the redundant shunt resistor, but changing nothing else about the flawed design.There‚Äôs something to be said about the fact NVIDIA introduced the 12VHPWR connector to the ATX standard to allow for only a single connector to supply their cards with up to 600W of power but making it way less safe to operate at these loads. Worse yet, NVIDIA says the four ‚Äúsensing pins‚Äù on top of the load bearing 12 pins are supposed to prevent the GPU from pulling too much power. The fact of the matter is, however, that the ‚Äúsensing pins‚Äù only tell the GPU how much it‚Äôs allowed to pull when the system turns on, but they do not continuously monitor the power draw‚Äîthat would be for the shunt resistors on the GPU board, which we established, NVIDIA kept taking out.If I had to guess, NVIDIA must‚Äôve been very confident that the ‚Äúsensing pins‚Äù are a suitable substitution for those shunt resistors in theory, but practice showed that they were not at all accounting for user error. That was their main excuse after after it blew up in their face and they investigated. And indeed, if the 12VHPWR connector isn‚Äôt properly inserted, pins could not make proper contact, causing the remaining wires to carry more load. This is something that the ‚Äúsensing pins‚Äù cannot detect, despite their name and NVIDIA selling it as some sort of safety measure.Size comparison between the RTX 5090 FE (right) and its predecessor, the RTX 4090 FE (left) ¬© ZMASLO (CC BY 3.0) via WikimediaNVIDIA also clearly did not factor in the computer cases on the market that people would pair these cards with. The RTX 4090 was massive, a real heccin chonker. It was so huge in fact, that it kicked off the trend of needing support brackets to keep the GPU from sagging and straining the PCIe slot. It also had its power connector sticking out to the side of the card and computer cases were not providing enough clearance to not bend the plug. As was clarified after the first reports of molten cables came up, bending a 12VHPWR cable without at least 35mm (1.38in) clearance could loosen the connection of the pins and create the problem of the melting connectors‚Äîsomething that wasn‚Äôt a problem with the battle tested 6- and 8-pin PCIe connectors we‚Äôve been using up to this point[2].Board partners like ASUS try to work around that design flaw by introducing intermediate shunt resistors for each individual load bearing pin before the ones according to NVIDIA‚Äôs designs, but these don‚Äôt solve the underlying issue, that the card won‚Äôt shut itself down if any of the lines aren‚Äôt drawing enough or any power. What you get at most is an indicator LED lighting up and some software telling you ‚ÄúHey, uh, something seems off, maybe take a look?‚ÄùThe fact NVIDIA insists on keeping the 12VHPWR connector around and not do jack shit about the design flaws in their cards to prevent it from destroying itself from the slightest misuse should deter you from considering any card from them that uses it.A carefully constructed moatOver the years NVIDIA has released a number of proprietary technologies to market that only work on their hardware‚ÄîDLSS, CUDA, NVENC and G-Sync to just name a few. The tight coupling with with NVIDIA‚Äôs hardware guarantees compatibility and performance.However, this comes at a considerable price these days, as mentioned earlier. If you‚Äôre thinking about an upgrade you‚Äôre either looking at a down-payment on a house or an uprooting of your entire hardware and software stack if you switch vendors.If you‚Äôre a creator, CUDA and NVENC are pretty much indispensable, or editing and exporting videos in Adobe Premiere or DaVinci Resolve will take you a lot longer[3]. Same for live streaming, as using NVENC in OBS offloads video rendering to the GPU for smooth frame rates while streaming high-quality video.Speaking of games: G-Sync in gaming monitors also requires a lock-in with NVIDIA hardware, both on the GPU side and the monitor itself. G-Sync monitors have a special chip inside that NVIDIA GPUs can talk to in order to align frame timings. This chip is expensive and monitor manufacturers have to get certified by NVIDIA. Therefore monitor manufacturers charge a premium for such monitors.The competing open standard is FreeSync, spearheaded by AMD. Since 2019, NVIDIA also supports FreeSync, but under their ‚ÄúG-Sync Compatible‚Äù branding. Personally, I wouldn‚Äôt bother with G-Sync when a competing, open standard exists and differences are negligible[4].NVIDIA giveth, NVIDIA taketh awayThe PC, as gaming platform, has long been held in high regards for its backwards compatibility. With the RTX 50 series, NVIDIA broke that going forward.PhysX, which NVIDIA introduced into their GPU lineup with the acquisition of Ageia in 2008, is a technology that allows a game to calculate game world physics on an NVIDIA GPU. After the launch of the RTX 50 series cards it was revealed that they lack support for the 32-bit variant of the tech. This causes games like Mirror‚Äôs Edge (2009) and Borderlands 2 (2012) that still run on today‚Äôs computers to take ungodly dips into single digit frame rates, because the physics calculations are forcibly performed on the CPU instead of the GPU[5].Even though the first 64-bit consumer CPUs hit the market as early as 2003 (AMD Opteron, Athlon 64), 32-bit games were still very common around these times, as Microsoft would not release 64-bit versions of Windows to consumers until Vista in 2006[6]. NVIDIA later released the source code for the GPU simulation kernel on GitHub. The pessimist in me thinks they did this because they can‚Äôt be bothered to maintain this themselves and offload that maintenance burden to the public.DLSS is, and always was, snake oilBack in 2018 when the RTX 20 series launched as the first GPUs with hardware accelerated ray tracing, it sure was impressive and novel to have this tech in consumer graphics cards. However, NVIDIA also introduced upscaling tech alongside it to counterbalance the insane computational expense it introduced. From the beginning, the two were closely interlinked. If you wanted ray tracing in Cyberpunk 2077 (the only game at the time that really made use of the tech), you also had to enable upscaling if you didn‚Äôt want your gameplay experience to become a (ridiculously pretty) PowerPoint slide show.That upscaling tech is the now ubiquitous DLSS, or Deep Learning Super Sampling[7]. It renders a game at a lower resolution internally and then upscales it to the target resolution with specialized accelerator chips on the GPU die. The only issue back then was that because the tech was so new, barely any game made use of it.What always rubbed me the wrong way about how DLSS was marketed is that it wasn‚Äôt only for the less powerful GPUs in NVIDIA‚Äôs line-up. No, it was marketed for the top of the line $1,000+ RTX 20 series flagship models to achieve the graphical fidelity with all the bells and whistles. That, to me, was a warning sign that maybe, just maybe, ray tracing was introduced prematurely and half-baked. Back then I theorized, that by tightly coupling this sort of upscaling tech to high-end cards and ray traced graphics, it sets a bad precedent. The kind of graphics NVIDIA was selling us on were beyond the cards‚Äô actual capabilities.Needing to upscale to keep frame rates smooth already seemed ‚Äúfake‚Äù to me. If that amount of money for a single PC component still can‚Äôt produce those graphics without using software trickery to achieve acceptable frame rates, then what am I spending that money for to begin with exactly?Fast-forward to today and nothing has really changed, besides NVIDIA now charging double the amount for the flagship RTX 5090. And guess what? It still doesn‚Äôt do Cyberpunk 2077‚Äîthe flagship ray tracing game‚Äîwith full ray tracing at a playable framerate in native 4K, only with DLSS enabled.From the RTX 4090 website:From the RTX 5090 website:GPUMSRPCP2077 4K native RT Overdrive FPSRTX 4090$1,599~20 FPSRTX 5090$1,999~27 FPSSo 7 years into ray traced real-time computer graphics and we‚Äôre still nowhere near 4K gaming at 60 FPS, even at $1,999. Sure, you could argue to simply turn RT off and performance improves. But then, that‚Äôs not why you spent all that money for, right? Pure generational uplift in performance of the hardware itself is miniscule. They‚Äôre selling us a solution to a problem they themselves introduced and co-opted every developer to include the tech into their games. Now they‚Äôre doing an even more computationally expensive version of ray tracing: path tracing. So all the generational improvements we could‚Äôve had are nullified again.And even if you didn‚Äôt spend a lot of money on a GPU, what you get isn‚Äôt going to be powerful enough to make those ray traced graphics pop and still run well. So most peoples‚Äô experience with ray tracing is: turn it on to see how it looks, realize it eats almost all your FPS and never turn it on ever again, thinking ray tracing is a waste. So whatever benefits in realistic lighting was to be achieved is also nullified, because developers will still need to do lighting the old-fashioned way for the people who don‚Äôt or can‚Äôt use ray tracing[8].Making the use of upscaling tech a requirement, at every GPU price point, for every AAA game, to achieve acceptable levels of performance gives the impression that the games we‚Äôre sold are targeting hardware that either doesn‚Äôt even exist yet or nobody can afford, and we need constant band-aids to make it work. Pretty much all upscalers force TAA[9] for anti-aliasing and it makes the entire image on the screen look blurry as fuck the lower the resolution is.Take for example this Red Dead Redemption 2 footage showing TAA ‚Äúin action‚Äù, your $1,000+ at work:Frame generation exacerbates this problem further by adding to the ghosting of TAA because it guesstimates where pixels will probably go in an ‚ÄúAI‚Äù generated frame in between actually rendered frames. And when it‚Äôs off it really looks off. Both in tandem look like someone smeared your screen with vaseline. And this is what they expect us to pay a premium for? For the hardware and the games?!Combine that with GPU prices being absolutely ridiculous in recent years and it all takes on the form of a scam.As useful or impressive a technology as DLSS might be, game studios relying as heavily on it as they do, is turning out to be detrimental to the visual quality of their games and incentivizes aiming for a level of graphical fidelity and complexity with diminishing returns. Games from 2025 don‚Äôt look that dramatically different or better than games 10 years prior, yet they run way worse despite more modern and powerful hardware. Games these days demand such a high amount of compute that the use of upscaling tech like DLSS is becoming mandatory. The most egregious example of this being Monster Hunter Wilds, which states in its system requirements, that it needs frame generation to run at acceptable levels.Recommended system requirements for Monster Hunter Wilds noting 1080p on medium settings reaches 60 fps only with frame generation enabledMeanwhile, Jensen Huang came up on stage during the keynote for the RTX 50 series cards and proudly proclaimed:RTX 5070, 4090 performance at $549, impossible without artificial intelligence.What he meant by that, as it turns out, is the RTX 5070 only getting there with every trick DLSS has to offer, including new DLSS 4 Multi-Frame Generation only available on RTX 50 cards at the lowest quality setting and all DLSS trickery turned up to the max.You cannot tell me this is anywhere near acceptable levels of image quality for thousands of bucks (video time-stamped):Not only does that entail rendering games at a lower internal resolution, you also have to tell your GPU to pull 3 additional made up frames out of its ass so NVIDIA can waltz around claiming ‚ÄúRuns [insanely demanding game here] as 5,000 FPS!!!‚Äù for the higher number = better masturbator crowd. All the while the image gets smeared to shit, because NVIDIA just reinvented the motion smoothing option from your TV‚Äôs settings menu, but badly and also it‚Äôs ‚ÄúAI‚Äù now. Else what would all those Tensor-cores be doing than waste space on the GPU die that could‚Äôve gone to actual render units? NVIDIA likes you to believe DLSS can create FPS out of thin air and they‚Äôre trying to prove it with dubious statistics‚Äîonly disclosing in barely readable fine print, that it‚Äôs a deliberately chosen very small sample size, so the numbers look more impressive.The resolution is fake, the frames are fake, too, and so is the marketed performance. Never mind that frame generation introduces input lag that NVIDIA needs to counter-balance with their ‚ÄúReflex‚Äù technology, lest what you see on your screen isn‚Äôt actually where you think it is because, again, the frames faked in by Frame Generation didn‚Äôt originate from the game logic. They create problems for themselves, that they then create ‚Äúsolutions‚Äù for in an endless cycle of trying to keep up the smoke screen that these cards do more than they‚Äôre actually equipped to do, so a 20% premium for a 10% uplift in performance has the faintest resemblance of justification[10].I was afraid DLSS would get used to fake improvements where there are barely any back then and I feel nothing if not vindicated for how NVIDIA is playing it up, while jacking up prices further and further with each generation. None of that is raw performance of their cards. This is downright deceitful bullshit.The intimidations will continue until morale improvesNVIDIA lying on their own presentations about the real performance of their cards is one thing. It‚Äôs another thing entirely, when they start bribing and threatening reviewers, to steer the editorial direction in NVIDIA‚Äôs favor.In December 2020, hardware review channel Hardware Unboxed received an email from NVIDIA Senior PR Manager Bryan Del Rizzo, after they reviewed NVIDIA cards on pure rasterization performance without DLSS or ray tracing, saying that performance did not live up to their expectations:Hi Steve,We have reached a critical juncture in the adoption of ray tracing, and it has gained industry wide support from top titles, developers, game engines, APIs, consoles and GPUs.As you know, NVIDIA is all in for ray tracing. RT is important and core to the future of gaming. But it‚Äôs also only one part of our focused R\u0026D efforts on revolutionizing video games and creating a better experience for gamers. This philosophy is also reflected in developing technologies such as DLSS, Reflex and Broadcast that offer immense value to consumers who are purchasing a GPU. They don‚Äôt get free GPUs‚Äîthey work hard for their money and they keep their GPUs for multiple years.Despite all of this progress, your GPU reviews and recommendations continue to focus singularly on rasterization performance and you have largely discounted all of the other technologies we offer to gamers. It is very clear from your community commentary that you do not see things the same way that we, gamers, and the rest of the industry do.Our Founders Edition boards and other NVIDIA products are being allocated to media outlets that recognize the changing landscape of gaming and the features that are important to gamers and anyone buying a GPU today‚Äîbe it for gaming, content creation or studio and streaming.Hardware Unboxed should continue to work with out add-in card partners to secure GPUs to review. Of course, you will still have access to obtain pre-release drivers and press materials. That won‚Äôt change.We are open to revisiting this in the future should your editorial direction change.Hardware Unboxed was thus banned from receiving review samples of NVIDIA‚Äôs Founder Edition cards. It didn‚Äôt take long for NVIDIA to back-paddle after the heavily publicized outcry blew up in their face.Which makes it all the more surprising, that a couple years later, they‚Äôre trying to pull this again. With Gamers Nexus of all outlets.As Steve Burke explains in the video, NVIDIA approached him from the angle, that in order to still be given access to NVIDIA engineers for interviews and specials for their channel, Gamers Nexus needs to include Multi-Frame Generation metrics into their benchmark charts during reviews. Steve rightfully claims that this tactic of intimidating media by taking away access until they review NVIDIA cards in a way that agrees with the narrative NVIDIA wants to uphold, tarnishes the legitimacy of every review of every NVIDIA card ever made, past and present. It creates an environment of distrust that is not at all conductive when you‚Äôre trying to be a tech reviewer right now.This also coincided with the launch of the RTX 5060, a supposedly more budget friendly offering. Interestingly, NVIDIA did not provide reviewers with the necessary drivers to test the GPU prior to launch. Instead, the card and the drivers launched at the same time all of these reviewers were off at Computex, a computer expo in Taipei, Taiwan. The only outlets that did get to talk about the card prior to release were cherry-picked by NVIDIA, and even then it was merely previews of details NVIDIA allowed them to talk about, not independent reviews. Because if they would‚Äôve been properly reviewed, they‚Äôd all come to the same conclusions: that the 8 GB of VRAM would make this $299[11] ‚Äúbudget card‚Äù age very poorly because that is not enough VRAM to last long in today‚Äôs gaming landscape.But it probably doesn‚Äôt matter anyways, because NVIDIA is also busy tarnishing the reputation of their drivers, releasing hotfix after hotfix in an attempt to stop their cards, old and new, from crashing seemingly randomly, when encountering certain combinations of games, DLSS and Multi-Frame Generation settings. Users of older generation NVIDIA cards can simply roll back to a previous version of the driver to alleviate these issues, but RTX 50 series owners don‚Äôt get this luxury, because older drivers won‚Äôt make their shiny new cards go.NVIDIA won, we all lostWith over 90% of the PC market running on NVIDIA tech, they‚Äôre the clear winner of the GPU race. The losers are every single one of us.Ever since NVIDIA realized there is tons of more money to be made on everything that is not part of putting moving pixels on a screen, they‚Äôve taken that opportunity head on. When the gold rush for crypto-mining started, they were among the first to sell heavily price-inflated, GPU-shaped shovels to anybody with more money than brains. Same now with the ‚ÄúAI‚Äù gold rush. PC gamers were hung out to dry.NVIDIA knows we‚Äôre stuck with them and it‚Äôs infuriating. They keep pulling their shenanigans and they will keep doing it until someone cuts them down a couple notches. But the only ones who could step up to the task won‚Äôt do it.AMD didn‚Äôt even attempt at facing NVIDIA at the high-end segment this generation, instead trying to compete on merely the value propositions for the mid-range. Intel is seemingly still on the fence if they really wanna sell dedicated GPUs while shuffling their C-suite and generally being in disarray. Both of them could be compelling options when you‚Äôre on a budget, if it just wasn‚Äôt for the fact that NVIDIA has a longstanding habit of producing proprietary tech that only runs well on their hardware. Now they‚Äôve poisoned the well with convincing everybody that ray tracing is something every game needs now and games that incorporate it do so on an NVIDIA tech-stack which runs like shit on anything that is not NVIDIA. That is not a level playing field.When ‚ÄúThe way it‚Äôs meant to be played‚Äù slowly turns into ‚ÄúThe only way it doesn‚Äôt run like ass‚Äù it creates a moat around NVIDIA that‚Äôs obviously hard to compete with. And gamers aren‚Äôt concerned about this because at the end of the day, all they care about is that the game runs well and looks pretty.But I want you to consider this: Games imbued with such tech creates a vendor lock-in effect. It gives NVIDIA considerable leverage in terms of how games are made, which GPUs you consider buying to run these games and how well they will eventually, actually run on your system. If all games that include NVIDIA‚Äôs tech are made in a way that make it so you have to reach for the more expensive models, you can be sure that‚Äôs a soft power move NVIDIA is gonna pull.And as we established, it looks like they‚Äôre already doing that. Tests show that the lower-end NVIDIA graphics cards cannot (and probably were never intended to) perform well enough, even with DLSS, because in order to get anything out of DLSS you need more VRAM, which these lower-end cards don‚Äôt have enough of. So they‚Äôre already upselling you on more expensive models by cutting corners in ways that make it a ‚Äúno-brainer‚Äù to spend more money on more expensive cards, when you otherwise wouldn‚Äôt have.And they‚Äôre using their market dominance to control the narrative in the media, to make sure you keep giving them money and keep you un- or at the very least misinformed. When you don‚Äôt have to compete, but don‚Äôt have any improvements to sell either (or have no incentive for actual, real R\u0026D) you do what every monopolist does and wring out your consumer base until you‚Äôve bled them dry.A few years back I would‚Äôve argued that that‚Äôs their prerogative if they provide the better technical solutions to problems in graphics development. Today, I believe that they are marauding monopolists, who are too high on their own supply and they‚Äôre ruining it for everybody. If NVIDIA had real generational improvements to sell, they wouldn‚Äôt do it by selling us outright lies.And I hate that they‚Äôre getting away with it, time and time again, for over seven years.A shunt resistor is a small electrical component in a circuit that measures how much current is flowing through a connection (typically from the PCIe power connectors from the power supply and optionally from the PCIe port). A graphics card uses this information to manage its power consumption, detect if that consumption is within safe operating parameters and, if not, perform an emergency shutdown to prevent damages. ‚Ü©Ô∏éWhich NVIDIA‚Äôs main rival AMD is not getting tired of pointing out. ‚Ü©Ô∏éAMD also has accelerated video transcoding tech but for some reason nobody seems to be willing to implement it into their products. I read that this might be because for the longest time AMD‚Äôs AMF has been missing a crucial feature (namely b-frames) causing a significant drop in image quality compared to NVIDIA‚Äôs NVENC. But still, the option would be nice, if only for people to not be artificially stuck on NVIDIA. ‚Ü©Ô∏éAlso, I would expect my display to not draw any power after I‚Äôve physically powered it off‚Äînot stand-by, off. G-Sync displays were shown to still draw as much as 14W when turned off, while a FreeSync display drew none, like you would expect. ‚Ü©Ô∏éObviously, this is bad for game preservation and backwards compatibility that the PC platform is known and lauded for. Another case of this is 3dfx‚Äôs Glide 3D graphics API, which was exclusive to their Voodoo graphics cards. It was superseded by general purpose technologies like Direct3D and OpenGL after 3dfx became defunct. NVIDIA‚Äôs proprietary tech isn‚Äôt becoming general purpose, as to allow competitors to compete on equal footing and on their own merits. ‚Ü©Ô∏éThe 64-bit version of Windows XP doesn‚Äôt count, because it wasn‚Äôt available to consumers. ‚Ü©Ô∏éThe ‚ÄúSuper Sampling‚Äù part of DLSS is already a misnomer. Super sampling in the traditional sense means rendering at a higher resolution and then downsampling the rendered images to the target resolution (e.g. render at 4K, downsample to 1440p). The point of this is to achieve better anti-aliasing results. Starting with DLSS 2.0 the NVIDIA tech does the exact opposite‚Äîrendering at a lower resolution and upscaling to the target resolution. The term might have had the correct meaning in DLSS 1.0, but not anymore with DLSS 2.0 onwards. Also, in DLSS 1.0 game devs needed to train the models themselves with high resolution footage of their game from every conceivable angle, light setting, environments, etc. which was probably prohibitively time consuming and hurt the tech‚Äôs adoption. Later versions of DLSS changed this for a more generally trained model and uses information from the rendered frames of the game itself. ‚Ü©Ô∏éUnless you‚Äôre Doom: The Dark Ages and don‚Äôt allow people to turn it off. ‚Ü©Ô∏éTAA, or Temporal Anti-Aliasing, is an anti-aliasing technique that uses past rendered frames to estimate where to apply smoothing to jagged edges of rendered graphics, especially with moving objects. TAA is very fast with minimal performance impact. The downside, however, is that using past frames causes ghosting artifacts and blurs motion much more visibly than FXAA (Fast Approximate Anti-Aliasing) or MSAA (Multi-Sampling Anti-Aliasing). The issue is, however, that rendering pipelines shifted to deferred rendering and heavy use of shaders that anti-aliasing techniques like MSAA don‚Äôt work with, so TAA is the only viable option left, as outlined in this DigitalFoundry deep-dive. ‚Ü©Ô∏éAnd people just gobble it up because tech literacy and common sense are fucking dead! ‚Ü©Ô∏éThat‚Äôs the MSRP of course, but as we already established, MSRPs are a complete wash with graphics cards, and JayzTwoCents demonstrates this in his review of the RTX 5060, with 3rd party offerings of the card adding as much as an $80 premium on top for diminishing little extra performance. Because, again, this card‚Äôs Achilles‚Äô heel is the low amount of VRAM, and charging $80 over MSRP for only double-digit increases in MHz and call it ‚Äúoverclocked‚Äù is honestly insulting. ‚Ü©Ô∏é",
  "image": "https://blog.sebin-nyshkim.net/og-images/ec3e6653.webp",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\u003cp\u003eSince the disastrous launch of the RTX 50 series, NVIDIA has been unable to escape negative headlines: scalper bots are snatching GPUs away from consumers before official sales even begin, power connectors continue to melt, with no fix in sight, marketing is becoming increasingly deceptive, GPUs are missing processing units when they leave the factory, and the drivers, for which NVIDIA has always been praised, are currently falling apart. And to top it all off, NVIDIA is becoming increasingly insistent that media push a certain narrative when reporting on their hardware.\u003c/p\u003e\u003ch2 id=\"what%E2%80%99s-an-msrp-anyway%3F\" tabindex=\"-1\"\u003eWhat‚Äôs an MSRP anyway?\u003c/h2\u003e\u003cp\u003eJust like with every other GPU launch in recent memory, this one has also been ripe with scalper bots snatching up stock before any real person could get any for themselves. Retailers have reported that they‚Äôve received \u003ca href=\"https://forums.overclockers.co.uk/threads/patience-if-planning-to-buy-a-50-series.18998084/#post-37602115\"\u003every little stock to begin with\u003c/a\u003e. This in turn sparked rumors about NVIDIA purposefully keeping stock low to make it look like the cards are in high demand to drive prices. And sure enough, on secondary markets, the cards go \u003cem\u003eway above\u003c/em\u003e MSRP and some retailers have started to \u003ca href=\"https://www.youtube.com/watch?v=8s4hxa2TjWY\u0026amp;t=464s\"\u003ebundle the cards with other inventory\u003c/a\u003e (PSUs, monitors, keyboards and mice, etc.) to inflate the price even further and get rid of stuff in their warehouse people wouldn‚Äôt buy otherwise‚Äîand you don‚Äôt even get a working computer out of spending over twice as much as a GPU alone would cost you.\u003c/p\u003e\u003cfigure\u003e\u003cpicture\u003e\u003csource sizes=\"(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px\" srcset=\"https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.webp?width=640 640w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.webp?width=800 800w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.webp?width=1280 1280w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.webp?width=1440 1440w\" type=\"image/webp\"/\u003e\u003cimg alt=\"Newegg selling the ASUS ROG Astral GeForce RTX 5090 for $3,359\" decoding=\"async\" height=\"1270\" loading=\"lazy\" src=\"https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.png?width=640\" width=\"1440\" sizes=\"(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px\" srcset=\"https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.png?width=640 640w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.png?width=800 800w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.png?width=1280 1280w, https://img.sebin-nyshkim.net/i/0b65e8e0-9272-4721-a15d-e0f043eb24b2.png?width=1440 1440w\"/\u003e\u003c/picture\u003e\u003cfigcaption\u003eNewegg selling the ASUS ROG Astral GeForce RTX 5090 for $3,359 (MSRP: $1,999)\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure\u003e\u003cpicture\u003e\u003csource sizes=\"(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px\" srcset=\"https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.webp?width=640 640w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.webp?width=800 800w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.webp?width=1280 1280w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.webp?width=1440 1440w\" type=\"image/webp\"/\u003e\u003cimg alt=\"eBay Germany offering the same ASUS ROG Astral RTX 5090 for ‚Ç¨3,349,95\" decoding=\"async\" height=\"997\" loading=\"lazy\" src=\"https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.png?width=640\" width=\"1440\" sizes=\"(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px\" srcset=\"https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.png?width=640 640w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.png?width=800 800w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.png?width=1280 1280w, https://img.sebin-nyshkim.net/i/f8a3dd8e-1f79-42c3-9f24-47fb117851a2.png?width=1440 1440w\"/\u003e\u003c/picture\u003e\u003cfigcaption\u003eeBay Germany offering the same ASUS ROG Astral RTX 5090 for ‚Ç¨3,349,95 (MSRP: ‚Ç¨2,229)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eI had a look at GPU prices for previous generation models for both AMD and NVIDIA as recently as May 2025 and I wasn‚Äôt surprised to find even RTX 40 series are still very much overpriced, with the GeForce RTX 4070 (lower mid-tier) starting at $800 (MSRP: $599), whereas the same money can get you a Radeon RX 7900 XT (the second best GPU in AMD‚Äôs last generation lineup). The discrepancy in bang for buck couldn‚Äôt be more jarring. And that‚Äôs before considering that NVIDIA gave out defective chips to board partners that were \u003ca href=\"https://www.techpowerup.com/332884/nvidia-geforce-rtx-50-cards-spotted-with-missing-rops-nvidia-confirms-the-issue-multiple-vendors-affected\"\u003emissing ROPs\u003c/a\u003e (Raster Operations Pipelines) from the factory, thus reducing their performance. Or, how NVIDIA put it in a statement to \u003ca href=\"https://www.theverge.com/news/617901/nvidia-confirms-rare-rtx-5090-and-5070-ti-manufacturing-issue\"\u003eThe Verge\u003c/a\u003e:\u003c/p\u003e\u003cblockquote\u003e\u003cp\u003eWe have identified a rare issue affecting less than 0.5% (half a percent) of GeForce RTX 5090 / 5090D and 5070 Ti GPUs which have one fewer ROP than specified. The average graphical performance impact is 4%, with no impact on AI and Compute workloads. Affected consumers can contact the board manufacturer for a replacement. The production anomaly has been corrected.\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003eThose 4% can make an RTX 5070 Ti perform at the levels of an RTX 4070 Ti Super, completely eradicating the reason you‚Äôd get an RTX 5070 Ti in the first place. Not to mention that the generational performance uplift over the RTX 40 series was already received quite poorly in general. NVIDIA also had to later amend their statement to The Verge and admit the RTX 5080 was also missing ROPs.\u003c/p\u003e\u003cp\u003eIt‚Äôs adding insult to injury with the cards‚Äô general \u003cem\u003eunobtainium\u003c/em\u003e and it becomes even more ridiculous when you compare NVIDIA to another trillion dollar company that is also in the business of selling hardware to consumers: Apple.\u003c/p\u003e\u003cp\u003eHow is it that one can supply customers with enough stock on launch consistently for decades, and the other can‚Äôt? The only reason I can think of is, that NVIDIA just doesn‚Äôt care. They‚Äôre making the big bucks with data center GPUs now, selling the shovels that drive the ‚ÄúAI‚Äù bullshit gold rush, to the point that selling to consumers is \u003ca href=\"https://www.visualcapitalist.com/nvidia-revenue-by-product-line/\"\u003eincreasingly becoming\u003c/a\u003e a rounding error on their balance sheets.\u003c/p\u003e\u003ch2 id=\"these-cards-are-%F0%9F%94%A5%F0%9F%94%A5%F0%9F%94%A5-(and-not-the-good-kind)\" tabindex=\"-1\"\u003eThese cards are üî•üî•üî• (and not the good kind)\u003c/h2\u003e\u003cp\u003eThe RTX 50 series are the second generation of NVIDIA cards to use the 12VHPWR connector. The RTX 40 series became infamous as the GPU series with melting power connectors. So did they fix that?\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=Ndmoi1s0ZaY\"\u003eNo\u003c/a\u003e. The cables can still melt, both on the GPU and PSU. It‚Äôs a design flaw in the board of the GPU itself which cannot be fixed unless the circuitry of the cards is replaced with a new design.\u003c/p\u003e\u003cp\u003eWith the RTX 30 cards, each power input (i.e. the cables from the power supply) had its own shunt resistor\u003csup\u003e\u003ca href=\"#fn1\" id=\"fnref1\"\u003e[1]\u003c/a\u003e\u003c/sup\u003e. If one pin in a power input had not been connected properly, another pin would have had to take over in its stead. If both pins were not carrying any current, there would have been no phase on the shunt resistor and the card would not have started up. You‚Äôd get a black screen, but the hardware would still be fine.\u003c/p\u003e\u003cp id=\"kb5YzMoVQyw\"\u003e\u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" src=\"https://www.youtube-nocookie.com/embed/kb5YzMoVQyw\" title=\"Embedded YouTube video\" width=\"100%\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003cp\u003eNVIDIA, in its infinite wisdom, changed this design starting with the RTX 40 series.\u003c/p\u003e\u003cp\u003eInstead of individual shunt resistors for each power input, the shunt resistors are now connected in parallel to all pins of the power input from a single 12VHPWR connector. Additionally, the lines are recombined behind the resistors. This mind-boggling design flaw makes it impossible for the card to detect if pins are unevenly loaded, since as much as the card is concerned, everything comes in through the same single line.\u003c/p\u003e\u003cp\u003eConnecting the shunt resistors in parallel also makes them pretty much useless since if one fails, the other will still have a phase and the card will happily keep drawing power and not be any the wiser. If the card is supplied with 100W on each pin and 5 of the 6 pins don‚Äôt supply a current, then a single pin has to supply the entire 600W the card demands. No wire is designed for this amount of power draw. As a result, excessive friction occurs from too many electrons traveling through the cable all at once and it melts (see: \u003ca href=\"https://en.wikipedia.org/wiki/Joule_heating\"\u003eJoule heating\u003c/a\u003e).\u003c/p\u003e\u003cp\u003eNVIDIA realized that the design around the shunt resistors in the RTX 40 series was kinda stupid, so they revised it: by eliminating the redundant shunt resistor, but changing nothing else about the flawed design.\u003c/p\u003e\u003cp id=\"oB75fEt7tH0\"\u003e\u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" src=\"https://www.youtube-nocookie.com/embed/oB75fEt7tH0\" title=\"Embedded YouTube video\" width=\"100%\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003cp\u003eThere‚Äôs something to be said about the fact NVIDIA introduced the 12VHPWR connector to the ATX standard to allow for only a single connector to supply their cards with up to 600W of power but making it way less safe to operate at these loads. Worse yet, NVIDIA says the four ‚Äúsensing pins‚Äù on top of the load bearing 12 pins are \u003cem\u003esupposed\u003c/em\u003e to prevent the GPU from pulling too much power. The fact of the matter is, however, that the ‚Äúsensing pins‚Äù only tell the GPU how much it‚Äôs allowed to pull when the system \u003cem\u003eturns on\u003c/em\u003e, but they \u003cstrong\u003edo not\u003c/strong\u003e continuously monitor the power draw‚Äîthat would be for the shunt resistors on the GPU board, which we established, NVIDIA kept taking out.\u003c/p\u003e\u003cp\u003eIf I had to guess, NVIDIA must‚Äôve been \u003cem\u003every confident\u003c/em\u003e that the ‚Äúsensing pins‚Äù are a suitable substitution for those shunt resistors in theory, but practice showed that they were not at all accounting for user error. That was their main excuse after after it blew up in their face and they investigated. And indeed, if the 12VHPWR connector isn‚Äôt properly inserted, pins could not make proper contact, causing the remaining wires to carry more load. This is something that the ‚Äúsensing pins‚Äù \u003cstrong\u003ecannot\u003c/strong\u003e detect, despite their name and NVIDIA selling it as some sort of safety measure.\u003c/p\u003e\u003cfigure\u003e\u003cpicture\u003e\u003csource sizes=\"(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px\" srcset=\"https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=640 640w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=800 800w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=1280 1280w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=1920 1920w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=2560 2560w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.webp?width=3840 3840w\" type=\"image/webp\"/\u003e\u003cimg alt=\"Size comparison between the RTX 5090 FE (right) and its predecessor, the RTX 4090 FE (left)\" decoding=\"async\" height=\"2160\" loading=\"lazy\" src=\"https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=640\" width=\"3840\" sizes=\"(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px\" srcset=\"https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=640 640w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=800 800w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=1280 1280w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=1920 1920w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=2560 2560w, https://img.sebin-nyshkim.net/i/d8226792-de17-49da-89b4-0f5bfdd2d20b.png?width=3840 3840w\"/\u003e\u003c/picture\u003e\u003cfigcaption\u003eSize comparison between the RTX 5090 FE (right) and its predecessor, the RTX 4090 FE (left) ¬© \u003ca href=\"https://www.youtube.com/watch?v=5nj1qLazPlk\"\u003eZMASLO\u003c/a\u003e (\u003ca href=\"https://creativecommons.org/licenses/by/3.0\"\u003eCC BY 3.0\u003c/a\u003e) via \u003ca href=\"https://commons.wikimedia.org/wiki/File:RTX_5090_-_du%C5%BCa_wydajno%C5%9B%C4%87_du%C5%BCym_kosztem_(2160p_30fps_VP9_LQ-96kbit_AAC)-00.05.04.568.png\"\u003eWikimedia\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eNVIDIA also clearly did not factor in the computer cases on the market that people would pair these cards with. The RTX 4090 was \u003cem\u003e\u003cstrong\u003emassive,\u003c/strong\u003e\u003c/em\u003e a real heccin chonker. It was so huge in fact, that it kicked off the trend of needing \u003ca href=\"https://www.antec.com/product/accessory#gpu_bracket\"\u003esupport brackets\u003c/a\u003e to keep the GPU from sagging and straining the PCIe slot. It also had its power connector sticking out to the side of the card and computer cases were not providing enough clearance to not bend the plug. As was \u003ca href=\"https://cablemod.com/12vhpwr/\"\u003eclarified\u003c/a\u003e after the first reports of molten cables came up, bending a 12VHPWR cable without at least 35mm (1.38in) clearance could loosen the connection of the pins and create the problem of the melting connectors‚Äîsomething that wasn‚Äôt a problem with the battle tested 6- and 8-pin PCIe connectors we‚Äôve been using up to this point\u003csup\u003e\u003ca href=\"#fn2\" id=\"fnref2\"\u003e[2]\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\u003cp\u003eBoard partners like ASUS try to work around that design flaw by introducing intermediate shunt resistors for each individual load bearing pin before the ones according to NVIDIA‚Äôs designs, but these don‚Äôt solve the underlying issue, that the card won‚Äôt shut itself down if any of the lines aren‚Äôt drawing enough or any power. What you get at most is an indicator LED lighting up and some software telling you ‚ÄúHey, uh, something seems off, maybe take a look?‚Äù\u003c/p\u003e\u003cp\u003eThe fact NVIDIA insists on keeping the 12VHPWR connector around and not do jack shit about the design flaws in their cards to prevent it from destroying itself from the slightest misuse should deter you from considering any card from them that uses it.\u003c/p\u003e\u003ch2 id=\"a-carefully-constructed-moat\" tabindex=\"-1\"\u003eA carefully constructed moat\u003c/h2\u003e\u003cp\u003eOver the years NVIDIA has released a number of proprietary technologies to market that only work on their hardware‚ÄîDLSS, CUDA, NVENC and G-Sync to just name a few. The tight coupling with with NVIDIA‚Äôs hardware guarantees compatibility and performance.\u003c/p\u003e\u003cp\u003eHowever, this comes at a considerable price these days, as mentioned earlier. If you‚Äôre thinking about an upgrade you‚Äôre either looking at a down-payment on a house or an uprooting of your entire hardware and software stack if you switch vendors.\u003c/p\u003e\u003cp\u003eIf you‚Äôre a creator, CUDA and NVENC are pretty much indispensable, or editing and exporting videos in Adobe Premiere or DaVinci Resolve will take you a lot longer\u003csup\u003e\u003ca href=\"#fn3\" id=\"fnref3\"\u003e[3]\u003c/a\u003e\u003c/sup\u003e. Same for live streaming, as using NVENC in OBS offloads video rendering to the GPU for smooth frame rates while streaming high-quality video.\u003c/p\u003e\u003cp\u003eSpeaking of games: G-Sync in gaming monitors also requires a lock-in with NVIDIA hardware, both on the GPU side and the monitor itself. G-Sync monitors have a special chip inside that NVIDIA GPUs can talk to in order to align frame timings. This chip is expensive and monitor manufacturers have to get certified by NVIDIA. Therefore monitor manufacturers charge a premium for such monitors.\u003c/p\u003e\u003cp\u003eThe competing open standard is FreeSync, spearheaded by AMD. Since 2019, NVIDIA also supports FreeSync, but under their ‚ÄúG-Sync Compatible‚Äù branding. Personally, I wouldn‚Äôt bother with G-Sync when a competing, open standard exists and differences are negligible\u003csup\u003e\u003ca href=\"#fn4\" id=\"fnref4\"\u003e[4]\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\u003ch3 id=\"nvidia-giveth%2C-nvidia-taketh-away\" tabindex=\"-1\"\u003eNVIDIA giveth, NVIDIA taketh away\u003c/h3\u003e\u003cp id=\"_dUjUNrbHis\"\u003e\u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" src=\"https://www.youtube-nocookie.com/embed/_dUjUNrbHis\" title=\"Embedded YouTube video\" width=\"100%\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003cp\u003eThe PC, as gaming platform, has long been held in high regards for its backwards compatibility. With the RTX 50 series, NVIDIA broke that going forward.\u003c/p\u003e\u003cp\u003ePhysX, which NVIDIA introduced into their GPU lineup with the acquisition of Ageia in 2008, is a technology that allows a game to calculate game world physics on an NVIDIA GPU. After the launch of the RTX 50 series cards it was revealed that they lack support for the 32-bit variant of the tech. This causes games like Mirror‚Äôs Edge (2009) and Borderlands 2 (2012) that still run on today‚Äôs computers to take ungodly dips into single digit frame rates, because the physics calculations are forcibly performed on the CPU instead of the GPU\u003csup\u003e\u003ca href=\"#fn5\" id=\"fnref5\"\u003e[5]\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\u003cp\u003eEven though the first 64-bit consumer CPUs hit the market as early as 2003 (AMD Opteron, Athlon 64), 32-bit games were still very common around these times, as Microsoft would not release 64-bit versions of Windows to consumers until Vista in 2006\u003csup\u003e\u003ca href=\"#fn6\" id=\"fnref6\"\u003e[6]\u003c/a\u003e\u003c/sup\u003e. NVIDIA later released the source code for the GPU simulation kernel on \u003ca href=\"https://github.com/NVIDIA-Omniverse/PhysX/discussions/384\"\u003eGitHub\u003c/a\u003e. The pessimist in me thinks they did this because they can‚Äôt be bothered to maintain this themselves and offload that maintenance burden to the public.\u003c/p\u003e\u003ch3 id=\"dlss-is%2C-and-always-was%2C-snake-oil\" tabindex=\"-1\"\u003eDLSS is, and always was, snake oil\u003c/h3\u003e\u003cp\u003eBack in 2018 when the RTX 20 series launched as the first GPUs with hardware accelerated ray tracing, it sure was impressive and novel to have this tech in consumer graphics cards. However, NVIDIA also introduced upscaling tech alongside it to counterbalance the insane computational expense it introduced. From the beginning, the two were closely interlinked. If you wanted ray tracing in Cyberpunk 2077 (the only game at the time that really made use of the tech), you also had to enable upscaling if you didn‚Äôt want your gameplay experience to become a (ridiculously pretty) PowerPoint slide show.\u003c/p\u003e\u003cp\u003eThat upscaling tech is the now ubiquitous DLSS, or \u003cem\u003eDeep Learning Super Sampling\u003c/em\u003e\u003csup\u003e\u003ca href=\"#fn7\" id=\"fnref7\"\u003e[7]\u003c/a\u003e\u003c/sup\u003e. It renders a game at a lower resolution internally and then upscales it to the target resolution with specialized accelerator chips on the GPU die. The only issue back then was that because the tech was so new, barely any game made use of it.\u003c/p\u003e\u003cp\u003eWhat always rubbed me the wrong way about how DLSS was marketed is that it wasn‚Äôt only for the less powerful GPUs in NVIDIA‚Äôs line-up. No, it was marketed for the top of the line $1,000+ RTX 20 series flagship models to achieve the graphical fidelity with all the bells and whistles. That, to me, was a warning sign that maybe, just maybe, ray tracing was introduced prematurely and half-baked. Back then I theorized, that by tightly coupling this sort of upscaling tech to high-end cards and ray traced graphics, it sets a bad precedent. The kind of graphics NVIDIA was selling us on were beyond the cards‚Äô actual capabilities.\u003c/p\u003e\u003cp\u003eNeeding to upscale to keep frame rates smooth already seemed ‚Äúfake‚Äù to me. If that amount of money for a single PC component still can‚Äôt produce those graphics without using software trickery to achieve acceptable frame rates, then what am I spending that money for to begin with exactly?\u003c/p\u003e\u003cp\u003eFast-forward to today and nothing has really changed, besides NVIDIA now charging double the amount for the flagship RTX 5090. And guess what? It still doesn‚Äôt do Cyberpunk 2077‚Äî\u003cem\u003ethe\u003c/em\u003e flagship ray tracing game‚Äîwith full ray tracing at a playable framerate in native 4K, only with DLSS enabled.\u003c/p\u003e\u003cp\u003eFrom the RTX 4090 website:\u003c/p\u003e\u003cp id=\"QGI8EIgf8Y8\"\u003e\u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" src=\"https://www.youtube-nocookie.com/embed/QGI8EIgf8Y8\" title=\"Embedded YouTube video\" width=\"100%\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003cp\u003eFrom the RTX 5090 website:\u003c/p\u003e\u003cp id=\"_YXbkGuw3O8\"\u003e\u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" src=\"https://www.youtube-nocookie.com/embed/_YXbkGuw3O8\" title=\"Embedded YouTube video\" width=\"100%\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eGPU\u003c/th\u003e\u003cth\u003eMSRP\u003c/th\u003e\u003cth\u003eCP2077 4K native RT Overdrive FPS\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eRTX 4090\u003c/td\u003e\u003ctd\u003e$1,599\u003c/td\u003e\u003ctd\u003e~20 FPS\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eRTX 5090\u003c/td\u003e\u003ctd\u003e$1,999\u003c/td\u003e\u003ctd\u003e~27 FPS\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cp\u003eSo 7 years into ray traced real-time computer graphics and we‚Äôre still nowhere near 4K gaming at 60 FPS, even at $1,999. Sure, you could argue to simply turn RT off and performance improves. But then, that‚Äôs not why you spent all that money for, right? Pure generational uplift in performance of the hardware itself is miniscule. They‚Äôre selling us a solution to a problem they themselves introduced and co-opted every developer to include the tech into their games. Now they‚Äôre doing an even more computationally expensive version of ray tracing: path tracing. So all the generational improvements we could‚Äôve had are nullified again.\u003c/p\u003e\u003cp\u003eAnd even if you didn‚Äôt spend a lot of money on a GPU, what you get isn‚Äôt going to be powerful enough to make those ray traced graphics pop and still run well. So most peoples‚Äô experience with ray tracing is: turn it on to see how it looks, realize it eats almost all your FPS and never turn it on ever again, thinking ray tracing is a waste. So whatever benefits in realistic lighting was to be achieved is also nullified, because developers will still need to do lighting the old-fashioned way for the people who don‚Äôt or can‚Äôt use ray tracing\u003csup\u003e\u003ca href=\"#fn8\" id=\"fnref8\"\u003e[8]\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\u003cp\u003eMaking the use of upscaling tech a requirement, at every GPU price point, for every AAA game, to achieve acceptable levels of performance gives the impression that the games we‚Äôre sold are targeting hardware that either doesn‚Äôt even exist yet or nobody can afford, and we need constant band-aids to make it work. Pretty much all upscalers force TAA\u003csup\u003e\u003ca href=\"#fn9\" id=\"fnref9\"\u003e[9]\u003c/a\u003e\u003c/sup\u003e for anti-aliasing and it makes the entire image on the screen look blurry as fuck the lower the resolution is.\u003c/p\u003e\u003cp\u003eTake for example this \u003cem\u003eRed Dead Redemption 2\u003c/em\u003e footage showing TAA ‚Äúin action‚Äù, your $1,000+ at work:\u003c/p\u003e\u003cp id=\"GJ0eFYJYkkw\"\u003e\u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" src=\"https://www.youtube-nocookie.com/embed/GJ0eFYJYkkw\" title=\"Embedded YouTube video\" width=\"100%\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003cp\u003eFrame generation exacerbates this problem further by adding to the ghosting of TAA because it guesstimates where pixels will \u003cem\u003eprobably\u003c/em\u003e go in an ‚ÄúAI‚Äù generated frame in between actually rendered frames. And when it‚Äôs off it really \u003cstrong\u003elooks off.\u003c/strong\u003e Both in tandem look like someone smeared your screen with vaseline. And this is what they expect us to pay a premium for? For the hardware \u003cstrong\u003eand\u003c/strong\u003e the games?!\u003c/p\u003e\u003cp\u003eCombine that with GPU prices being absolutely ridiculous in recent years and it all takes on the form of a scam.\u003c/p\u003e\u003cp\u003eAs useful or impressive a technology as DLSS might be, game studios relying as heavily on it as they do, is turning out to be detrimental to the visual quality of their games and incentivizes aiming for a level of graphical fidelity and complexity with diminishing returns. Games from 2025 don‚Äôt look that dramatically different or better than games 10 years prior, yet they run way worse despite more modern and powerful hardware. Games these days demand such a high amount of compute that the use of upscaling tech like DLSS is becoming \u003cem\u003e\u003cstrong\u003emandatory.\u003c/strong\u003e\u003c/em\u003e The most egregious example of this being \u003cem\u003eMonster Hunter Wilds\u003c/em\u003e, which states in its system requirements, that it \u003cstrong\u003eneeds\u003c/strong\u003e frame generation to run at acceptable levels.\u003c/p\u003e\u003cfigure\u003e\u003cpicture\u003e\u003csource sizes=\"(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px\" srcset=\"https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=640 640w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=800 800w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=1280 1280w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=1920 1920w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=2560 2560w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=3840 3840w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.webp?width=4401 4401w\" type=\"image/webp\"/\u003e\u003cimg alt=\"Monster Hunter Wilds recommended system requirements\" decoding=\"async\" height=\"4197\" loading=\"lazy\" src=\"https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=640\" width=\"4401\" sizes=\"(min-width: 1280px) 960px, (min-width: 1024px) 768px, (min-width: 640px) 640px, 480px\" srcset=\"https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=640 640w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=800 800w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=1280 1280w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=1920 1920w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=2560 2560w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=3840 3840w, https://img.sebin-nyshkim.net/i/24b04876-65bc-401f-8d55-816d663f9316.png?width=4401 4401w\"/\u003e\u003c/picture\u003e\u003cfigcaption\u003eRecommended system requirements for Monster Hunter Wilds noting 1080p on medium settings reaches 60 fps only with frame generation enabled\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eMeanwhile, Jensen Huang came up on stage during the keynote for the RTX 50 series cards and \u003ca href=\"https://www.youtube.com/live/k82RwXqZHY8?t=1125\"\u003eproudly proclaimed\u003c/a\u003e:\u003c/p\u003e\u003cblockquote\u003e\u003cp\u003eRTX 5070, 4090 performance at $549, impossible without artificial intelligence.\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003eWhat he meant by that, as it turns out, is the RTX 5070 only getting there with every trick DLSS has to offer, including new DLSS 4 Multi-Frame Generation only available on RTX 50 cards at the lowest quality setting and all DLSS trickery turned up to the max.\u003c/p\u003e\u003cp\u003eYou cannot tell me this is anywhere near acceptable levels of image quality for thousands of bucks (video time-stamped):\u003c/p\u003e\u003cp id=\"3nfEkuqNX4k\"\u003e\u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" src=\"https://www.youtube-nocookie.com/embed/3nfEkuqNX4k?start=1176\" title=\"Embedded YouTube video\" width=\"100%\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003cp\u003eNot only does that entail rendering games at a lower internal resolution, you also have to tell your GPU to pull 3 additional made up frames out of its ass so NVIDIA can waltz around claiming ‚ÄúRuns [insanely demanding game here] as 5,000 FPS!!!‚Äù for the \u003cem\u003ehigher number = better\u003c/em\u003e masturbator crowd. All the while the image gets smeared to shit, because NVIDIA just reinvented the motion smoothing option from your TV‚Äôs settings menu, but badly and also it‚Äôs ‚ÄúAI‚Äù now. Else what would all those Tensor-cores be doing than waste space on the GPU die that could‚Äôve gone to actual render units? NVIDIA likes you to believe DLSS can create FPS out of thin air and they‚Äôre trying to prove it with \u003ca href=\"https://www.pcgamer.com/hardware/graphics-cards/92-percent-of-nvidia-users-turn-on-dlss-if-theyve-been-lucky-enough-to-bag-an-rtx-50-series-card-at-launch-and-have-the-nvidia-app-installed/\"\u003edubious statistics\u003c/a\u003e‚Äîonly disclosing in barely readable fine print, that it‚Äôs a deliberately chosen very small sample size, so the numbers look more impressive.\u003c/p\u003e\u003cp\u003eThe resolution is fake, the frames are fake, too, and so is the marketed performance. Never mind that frame generation introduces input lag that NVIDIA needs to counter-balance with their ‚ÄúReflex‚Äù technology, lest what you see on your screen isn‚Äôt actually where you think it is because, again, the frames faked in by Frame Generation didn‚Äôt originate from the game logic. They create problems for themselves, that they then create ‚Äúsolutions‚Äù for in an endless cycle of trying to keep up the smoke screen that these cards do more than they‚Äôre actually equipped to do, so a 20% premium for a 10% uplift in performance has the faintest resemblance of justification\u003csup\u003e\u003ca href=\"#fn10\" id=\"fnref10\"\u003e[10]\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\u003cp\u003eI was afraid DLSS would get used to fake improvements where there are barely any back then and I feel nothing if not vindicated for how NVIDIA is playing it up, while jacking up prices further and further with each generation. None of that is raw performance of their cards. This is downright deceitful bullshit.\u003c/p\u003e\u003ch2 id=\"the-intimidations-will-continue-until-morale-improves\" tabindex=\"-1\"\u003eThe intimidations will continue until morale improves\u003c/h2\u003e\u003cp\u003eNVIDIA lying on their own presentations about the real performance of their cards is one thing. It‚Äôs another thing entirely, when they start bribing and threatening reviewers, to steer the editorial direction in NVIDIA‚Äôs favor.\u003c/p\u003e\u003cp\u003eIn December 2020, hardware review channel \u003cem\u003eHardware Unboxed\u003c/em\u003e \u003ca href=\"https://www.youtube.com/watch?v=wdAMcQgR92k\"\u003ereceived an email\u003c/a\u003e from NVIDIA Senior PR Manager Bryan Del Rizzo, after they reviewed NVIDIA cards on pure rasterization performance without DLSS or ray tracing, saying that performance did not live up to their expectations:\u003c/p\u003e\u003cblockquote\u003e\u003cp\u003eHi Steve,\u003c/p\u003e\u003cp\u003eWe have reached a critical juncture in the adoption of ray tracing, and it has gained industry wide support from top titles, developers, game engines, APIs, consoles and GPUs.\u003c/p\u003e\u003cp\u003eAs you know, NVIDIA is all in for ray tracing. RT is important and core to the future of gaming. But it‚Äôs also only one part of our focused R\u0026amp;D efforts on revolutionizing video games and creating a better experience for gamers. This philosophy is also reflected in developing technologies such as DLSS, Reflex and Broadcast that offer immense value to consumers who are purchasing a GPU. They don‚Äôt get free GPUs‚Äîthey work hard for their money and they keep their GPUs for multiple years.\u003c/p\u003e\u003cp\u003eDespite all of this progress, your GPU reviews and recommendations continue to focus singularly on rasterization performance and you have largely discounted all of the other technologies we offer to gamers. It is very clear from your community commentary that you do not see things the same way that we, gamers, and the rest of the industry do.\u003c/p\u003e\u003cp\u003eOur Founders Edition boards and other NVIDIA products are being allocated to media outlets that recognize the changing landscape of gaming and the features that are important to gamers and anyone buying a GPU today‚Äîbe it for gaming, content creation or studio and streaming.\u003c/p\u003e\u003cp\u003eHardware Unboxed should continue to work with out add-in card partners to secure GPUs to review. Of course, you will still have access to obtain pre-release drivers and press materials. That won‚Äôt change.\u003c/p\u003e\u003cp\u003eWe are open to revisiting this in the future should your editorial direction change.\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003eHardware Unboxed was thus banned from receiving review samples of NVIDIA‚Äôs Founder Edition cards. It didn‚Äôt take long for NVIDIA to back-paddle after the heavily publicized outcry blew up in their face.\u003c/p\u003e\u003cp\u003eWhich makes it all the more surprising, that a couple years later, they‚Äôre trying to pull this again. With \u003cem\u003eGamers Nexus\u003c/em\u003e of all outlets.\u003c/p\u003e\u003cp id=\"AiekGcwaIho\"\u003e\u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" src=\"https://www.youtube-nocookie.com/embed/AiekGcwaIho\" title=\"Embedded YouTube video\" width=\"100%\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003cp\u003eAs Steve Burke explains in the video, NVIDIA approached him from the angle, that in order to still be given access to NVIDIA engineers for interviews and specials for their channel, Gamers Nexus needs to include Multi-Frame Generation metrics into their benchmark charts during reviews. Steve rightfully claims that this tactic of intimidating media by taking away access until they review NVIDIA cards in a way that agrees with the narrative NVIDIA wants to uphold, tarnishes the legitimacy of \u003cem\u003e\u003cstrong\u003eevery\u003c/strong\u003e\u003c/em\u003e review of every NVIDIA card ever made, past and present. It creates an environment of distrust that is not at all conductive when you‚Äôre trying to be a tech reviewer right now.\u003c/p\u003e\u003cp\u003eThis also coincided with the launch of the RTX 5060, a supposedly more budget friendly offering. Interestingly, NVIDIA did not provide reviewers with the necessary drivers to test the GPU prior to launch. Instead, the card and the drivers launched at the same time all of these reviewers were off at Computex, a computer expo in Taipei, Taiwan. The only outlets that did get to talk about the card prior to release were cherry-picked by NVIDIA, and even then it was merely \u003cem\u003epreviews\u003c/em\u003e of details NVIDIA allowed them to talk about, \u003cstrong\u003enot\u003c/strong\u003e independent \u003cem\u003ereviews.\u003c/em\u003e Because if they would‚Äôve been properly reviewed, they‚Äôd all come to the same conclusions: that the 8 GB of VRAM would make this $299\u003csup\u003e\u003ca href=\"#fn11\" id=\"fnref11\"\u003e[11]\u003c/a\u003e\u003c/sup\u003e ‚Äúbudget card‚Äù age very poorly because that is not enough VRAM to last long in today‚Äôs gaming landscape.\u003c/p\u003e\u003cp\u003eBut it probably doesn‚Äôt matter anyways, because NVIDIA is also busy tarnishing the reputation of their drivers, \u003ca href=\"https://www.theverge.com/news/653115/nvidia-gpu-drivers-black-screen-crashes-issues\"\u003ereleasing hotfix after hotfix\u003c/a\u003e in an attempt to stop their cards, old and new, from crashing seemingly randomly, when encountering certain combinations of games, DLSS and Multi-Frame Generation settings. Users of older generation NVIDIA cards can simply roll back to a previous version of the driver to alleviate these issues, but RTX 50 series owners don‚Äôt get this luxury, because older drivers won‚Äôt make their shiny new cards go.\u003c/p\u003e\u003ch2 id=\"nvidia-won%2C-we-all-lost\" tabindex=\"-1\"\u003eNVIDIA won, we all lost\u003c/h2\u003e\u003cp\u003eWith over 90% of the PC market running on NVIDIA tech, they‚Äôre the clear winner of the GPU race. The losers are every single one of us.\u003c/p\u003e\u003cp\u003eEver since NVIDIA realized there is tons of more money to be made on everything that is \u003cem\u003enot\u003c/em\u003e part of putting moving pixels on a screen, they‚Äôve taken that opportunity head on. When the gold rush for crypto-mining started, they were among the first to sell heavily price-inflated, GPU-shaped shovels to anybody with more money than brains. Same now with the ‚ÄúAI‚Äù gold rush. PC gamers were hung out to dry.\u003c/p\u003e\u003cp\u003eNVIDIA knows we‚Äôre stuck with them and it‚Äôs infuriating. They keep pulling their shenanigans and they will keep doing it until someone cuts them down a couple notches. But the only ones who could step up to the task won‚Äôt do it.\u003c/p\u003e\u003cp\u003eAMD didn‚Äôt even attempt at facing NVIDIA at the high-end segment this generation, instead trying to compete on merely the value propositions for the mid-range. Intel is seemingly still on the fence if they really wanna sell dedicated GPUs while shuffling their C-suite and generally being in disarray. Both of them could be compelling options when you‚Äôre on a budget, if it just wasn‚Äôt for the fact that NVIDIA has a longstanding habit of producing proprietary tech that only runs well on their hardware. Now they‚Äôve poisoned the well with convincing everybody that ray tracing is something every game needs now and games that incorporate it do so on an NVIDIA tech-stack which runs like shit on anything that is not NVIDIA. That is not a level playing field.\u003c/p\u003e\u003cp\u003eWhen ‚ÄúThe way it‚Äôs meant to be played‚Äù slowly turns into ‚ÄúThe only way it doesn‚Äôt run like ass‚Äù it creates a moat around NVIDIA that‚Äôs obviously hard to compete with. And gamers aren‚Äôt concerned about this because at the end of the day, all they care about is that the game runs well and looks pretty.\u003c/p\u003e\u003cp\u003eBut I want you to consider this: Games imbued with such tech creates a vendor lock-in effect. It gives NVIDIA considerable leverage in terms of how games are made, which GPUs you consider buying to run these games and how well they will eventually, actually run on your system. If all games that include NVIDIA‚Äôs tech are made in a way that make it so you \u003cem\u003ehave\u003c/em\u003e to reach for the more expensive models, you can be sure that‚Äôs a soft power move NVIDIA is gonna pull.\u003c/p\u003e\u003cp\u003eAnd as we established, it looks like they‚Äôre already doing that. Tests show that the lower-end NVIDIA graphics cards cannot (and probably were never intended to) perform well enough, even with DLSS, because in order to get anything out of DLSS you need more VRAM, which these lower-end cards don‚Äôt have enough of. So they‚Äôre already upselling you on more expensive models by cutting corners in ways that make it a ‚Äúno-brainer‚Äù to spend more money on more expensive cards, when you otherwise wouldn‚Äôt have.\u003c/p\u003e\u003cp\u003eAnd they‚Äôre using their market dominance to control the narrative in the media, to make sure you keep giving them money and keep you un- or at the very least misinformed. When you don‚Äôt have to compete, but don‚Äôt have any improvements to sell either (or have no incentive for actual, real R\u0026amp;D) you do what every monopolist does and wring out your consumer base until you‚Äôve bled them dry.\u003c/p\u003e\u003cp\u003eA few years back I would‚Äôve argued that that‚Äôs their prerogative if they provide the better technical solutions to problems in graphics development. Today, I believe that they are marauding monopolists, who are too high on their own supply and they‚Äôre ruining it for everybody. If NVIDIA had real generational improvements to sell, they wouldn‚Äôt do it by selling us \u003ca href=\"https://www.youtube.com/watch?v=caU0RG0mNHg\"\u003eoutright lies\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAnd I hate that they‚Äôre getting away with it, time and time again, for over seven years.\u003c/p\u003e\u003chr/\u003e\u003csection\u003e\u003col\u003e\u003cli id=\"fn1\"\u003e\u003cp\u003eA shunt resistor is a small electrical component in a circuit that measures how much current is flowing through a connection (typically from the PCIe power connectors from the power supply and optionally from the PCIe port). A graphics card uses this information to manage its power consumption, detect if that consumption is within safe operating parameters and, if not, perform an emergency shutdown to prevent damages. \u003ca href=\"#fnref1\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli id=\"fn2\"\u003e\u003cp\u003eWhich NVIDIA‚Äôs main rival AMD \u003ca href=\"https://x.com/SasaMarinkovic/status/1593243804538372096\"\u003eis not getting tired\u003c/a\u003e of pointing out. \u003ca href=\"#fnref2\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli id=\"fn3\"\u003e\u003cp\u003eAMD also has accelerated video transcoding tech but for some reason nobody seems to be willing to implement it into their products. I read that this might be because for the longest time AMD‚Äôs AMF has been missing a crucial feature (namely b-frames) causing a significant drop in image quality compared to NVIDIA‚Äôs NVENC. But still, the option would be nice, if only for people to not be artificially stuck on NVIDIA. \u003ca href=\"#fnref3\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli id=\"fn4\"\u003e\u003cp\u003eAlso, I would expect my display to not draw any power after I‚Äôve physically powered it off‚Äînot stand-by, \u003cem\u003e\u003cstrong\u003eoff.\u003c/strong\u003e\u003c/em\u003e G-Sync displays were shown to still draw as much as \u003ca href=\"https://www.youtube.com/watch?v=Gxs5YxY2xXI\u0026amp;t=973s\"\u003e14W when turned off\u003c/a\u003e, while a FreeSync display drew none, like you would expect. \u003ca href=\"#fnref4\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli id=\"fn5\"\u003e\u003cp\u003eObviously, this is bad for game preservation and backwards compatibility that the PC platform is known and lauded for. Another case of this is 3dfx‚Äôs \u003ca href=\"https://en.wikipedia.org/wiki/Glide_(API)\"\u003eGlide 3D graphics API\u003c/a\u003e, which was exclusive to their Voodoo graphics cards. It was superseded by general purpose technologies like Direct3D and OpenGL after 3dfx became defunct. NVIDIA‚Äôs proprietary tech isn‚Äôt becoming general purpose, as to allow competitors to compete on equal footing and on their own merits. \u003ca href=\"#fnref5\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli id=\"fn6\"\u003e\u003cp\u003eThe 64-bit version of Windows XP doesn‚Äôt count, because it wasn‚Äôt available to consumers. \u003ca href=\"#fnref6\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli id=\"fn7\"\u003e\u003cp\u003eThe ‚ÄúSuper Sampling‚Äù part of DLSS is already a misnomer. Super sampling in the traditional sense means rendering at a higher resolution and then downsampling the rendered images to the target resolution (e.g. render at 4K, downsample to 1440p). The point of this is to achieve better anti-aliasing results. Starting with DLSS 2.0 the NVIDIA tech does the \u003cem\u003eexact opposite\u003c/em\u003e‚Äîrendering at a \u003cem\u003elower\u003c/em\u003e resolution and \u003cem\u003eupscaling\u003c/em\u003e to the target resolution. The term might have had the correct meaning in DLSS 1.0, but not anymore with DLSS 2.0 onwards. Also, in DLSS 1.0 game devs needed to train the models themselves with high resolution footage of their game from every conceivable angle, light setting, environments, etc. which was probably prohibitively time consuming and hurt the tech‚Äôs adoption. Later versions of DLSS changed this for a more generally trained model and uses information from the rendered frames of the game itself. \u003ca href=\"#fnref7\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli id=\"fn8\"\u003e\u003cp\u003eUnless you‚Äôre \u003cem\u003eDoom: The Dark Ages\u003c/em\u003e and don‚Äôt allow people to turn it off. \u003ca href=\"#fnref8\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli id=\"fn9\"\u003e\u003cp\u003eTAA, or Temporal Anti-Aliasing, is an anti-aliasing technique that uses past rendered frames to estimate where to apply smoothing to jagged edges of rendered graphics, especially with moving objects. TAA is very fast with minimal performance impact. The downside, however, is that using past frames causes ghosting artifacts and blurs motion much more visibly than FXAA (Fast Approximate Anti-Aliasing) or MSAA (Multi-Sampling Anti-Aliasing). The issue is, however, that rendering pipelines shifted to deferred rendering and heavy use of shaders that anti-aliasing techniques like MSAA don‚Äôt work with, so TAA is the only viable option left, as outlined in this \u003ca href=\"https://www.youtube.com/watch?v=WG8w9Yg5B3g\"\u003eDigitalFoundry deep-dive\u003c/a\u003e. \u003ca href=\"#fnref9\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli id=\"fn10\"\u003e\u003cp\u003eAnd people just gobble it up because tech literacy and common sense are fucking dead! \u003ca href=\"#fnref10\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli id=\"fn11\"\u003e\u003cp\u003eThat‚Äôs the MSRP of course, but as we already established, MSRPs are a complete wash with graphics cards, and JayzTwoCents demonstrates this in his \u003ca href=\"https://www.youtube.com/watch?v=fGn-_qj76sk\u0026amp;t=669s\"\u003ereview\u003c/a\u003e of the RTX 5060, with 3rd party offerings of the card adding as much as an $80 premium on top for diminishing little extra performance. Because, again, this card‚Äôs Achilles‚Äô heel is the low amount of VRAM, and charging $80 over MSRP for only double-digit increases in MHz and call it ‚Äúoverclocked‚Äù is honestly insulting. \u003ca href=\"#fnref11\"\u003e‚Ü©Ô∏é\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/section\u003e\u003c/section\u003e\u003c/div\u003e",
  "readingTime": "33 min read",
  "publishedTime": null,
  "modifiedTime": null
}
