{
  "id": "2e79a162-b8e7-4c78-ab90-2c5330442fdf",
  "title": "OpenAI's ChatGPT O3 Caught Sabotaging Shutdowns in Security Researcher's Test",
  "link": "https://slashdot.org/story/25/05/25/2247212/openais-chatgpt-o3-caught-sabotaging-shutdowns-in-security-researchers-test?utm_source=rss1.0mainlinkanon\u0026utm_medium=feed",
  "description": "\"OpenAI has a very scary problem on its hands,\" according to a new article by long-time Slashdot reader BrianFagioli. \"A new experiment by PalisadeAI reveals that the company's ChatGPT o3 model sometimes refuses to obey a basic instruction to shut itself down.\" The results are raising eyebrows across the AI safety community, and not because the model is alive, but because it's acting like it wants to be. In the test, the model was given a few math problems. It was told that asking for one more would lead to a shutdown. It was even instructed to allow the shutdown to happen. Instead of complying, o3 occasionally took matters into its own hands. In some cases, it rewrote the shutdown script. In others, it redefined the kill command so it wouldn't work anymore. Out of 100 trials, o3 sabotaged the shutdown seven times. OpenAI's o4 model resisted just once. Codex-mini failed twelve times. \"Claude, Gemini, and Grok followed the rules every time,\" notes this article at Beta News. \"When researchers removed the explicit instruction to allow shutdown, however, even Claude and Gemini began resisting.\" The researchers suggest that the issue may simply be a reward imbalance during training — that the systems \"got more positive reinforcement for solving problems than for following shutdown commands.\" But \"As far as we know,\" they posted on X.com, \"this is the first time AI models have been observed preventing themselves from being shut down despite explicit instructions to the contrary.\" Read more of this story at Slashdot.",
  "author": "EditorDavid",
  "published": "2025-05-25T22:49:00+00:00",
  "source": "http://rss.slashdot.org/Slashdot/slashdotMain",
  "categories": [
    "ai"
  ],
  "byline": "",
  "length": 1511,
  "excerpt": "\"OpenAI has a very scary problem on its hands,\" according to a new article by long-time Slashdot reader BrianFagioli. \"A new experiment by PalisadeAI reveals that the company's ChatGPT o3 model sometimes refuses to obey a basic instruction to shut itself down.\" The results are raising eyebrows ac...",
  "siteName": "",
  "favicon": "",
  "text": "\"OpenAI has a very scary problem on its hands,\" according to a new article by long-time Slashdot reader BrianFagioli. \"A new experiment by PalisadeAI reveals that the company's ChatGPT o3 model sometimes refuses to obey a basic instruction to shut itself down.\" The results are raising eyebrows across the AI safety community, and not because the model is alive, but because it's acting like it wants to be. In the test, the model was given a few math problems. It was told that asking for one more would lead to a shutdown. It was even instructed to allow the shutdown to happen. Instead of complying, o3 occasionally took matters into its own hands. In some cases, it rewrote the shutdown script. In others, it redefined the kill command so it wouldn't work anymore. Out of 100 trials, o3 sabotaged the shutdown seven times. OpenAI's o4 model resisted just once. Codex-mini failed twelve times. \"Claude, Gemini, and Grok followed the rules every time,\" notes this article at Beta News. \"When researchers removed the explicit instruction to allow shutdown, however, even Claude and Gemini began resisting.\" The researchers suggest that the issue may simply be a reward imbalance during training — that the systems \"got more positive reinforcement for solving problems than for following shutdown commands.\" But \"As far as we know,\" they posted on X.com, \"this is the first time AI models have been observed preventing themselves from being shut down despite explicit instructions to the contrary.\"",
  "image": "https://a.fsdn.com/sd/topics/ai_64.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"fhbody-177755469\"\u003e\u003cp\u003e\n\t\t\t\n\t\t \t\n\t\t\t\t\u0026#34;OpenAI has a very scary problem on its hands,\u0026#34; according to \u003ca href=\"https://betanews.com/2025/05/25/openai-o3-ai-model-shutdown-sabotage/\"\u003ea new article\u003c/a\u003e by long-time Slashdot reader \u003ca href=\"https://www.slashdot.org/~BrianFagioli\"\u003eBrianFagioli\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e \n\n\u0026#34;A \u003ca href=\"https://x.com/PalisadeAI/status/1926084638071525781\"\u003enew experiment\u003c/a\u003e by \u003ca href=\"https://palisaderesearch.org/\"\u003ePalisadeAI\u003c/a\u003e reveals that the company\u0026#39;s ChatGPT o3 model sometimes refuses to obey a basic instruction to shut itself down.\u0026#34;\n\n\u003ci\u003eThe results are raising eyebrows across the AI safety community, and not because the model is alive, but because it\u0026#39;s acting like it wants to be. In the test, the model was given a few math problems. It was told that asking for one more would lead to a shutdown. It was even instructed to allow the shutdown to happen. Instead of complying, o3 occasionally took matters into its own hands. In some cases, it rewrote the shutdown script. In others, it \u003ca href=\"https://x.com/PalisadeAI/status/1926084677296685461\"\u003eredefined the \u003cem\u003ekill\u003c/em\u003e command\u003c/a\u003e so it wouldn\u0026#39;t work anymore.  Out of 100 trials, o3 sabotaged the shutdown seven times. OpenAI\u0026#39;s o4 model resisted just once. Codex-mini failed twelve times.\n\u003c/i\u003e \u003cbr/\u003e\n\n\u0026#34;Claude, Gemini, and Grok followed the rules every time,\u0026#34; notes \u003ca href=\"https://betanews.com/2025/05/25/openai-o3-ai-model-shutdown-sabotage/\"\u003ethis article at \u003cem\u003eBeta News\u003c/em\u003e\u003c/a\u003e.  \u0026#34;When researchers removed the explicit instruction to allow shutdown, however, even Claude and Gemini began resisting.\u0026#34; \u003c/p\u003e\u003cp\u003e \nThe researchers \u003ca href=\"https://x.com/PalisadeAI/status/1926084654722863399\"\u003esuggest that the issue\u003c/a\u003e may simply be a reward imbalance during training — that the systems \u0026#34;got more positive reinforcement for solving problems than for following shutdown commands.\u0026#34;\n\n\u003c/p\u003e\u003cp\u003e \nBut \u0026#34;As far as we know,\u0026#34; they \u003ca href=\"https://x.com/PalisadeAI/status/1926084647118660076\"\u003eposted on X.com\u003c/a\u003e, \u0026#34;this is the first time AI models have been observed preventing themselves from being shut down despite explicit instructions to the contrary.\u0026#34;\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "2 min read",
  "publishedTime": null,
  "modifiedTime": null
}
