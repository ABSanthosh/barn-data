{
  "id": "1b9231ec-5755-4a6e-8ea5-bab0dce27b9a",
  "title": "Understanding Reasoning LLMs",
  "link": "https://magazine.sebastianraschka.com/p/understanding-reasoning-llms",
  "description": "Comments",
  "author": "",
  "published": "Thu, 06 Feb 2025 21:34:12 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Sebastian Raschka, PhD",
  "length": 27106,
  "excerpt": "Methods and Strategies for Building and Refining Reasoning Models",
  "siteName": "Ahead of AI",
  "favicon": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fca9744ac-726d-4947-a030-91ea0ab38e23%2Fapple-touch-icon-1024x1024.png",
  "text": "This article describes the four main approaches to building reasoning models, or how we can enhance LLMs with reasoning capabilities. I hope this provides valuable insights and helps you navigate the rapidly evolving literature and hype surrounding this topic.In 2024, the LLM field saw increasing specialization. Beyond pre-training and fine-tuning, we witnessed the rise of specialized applications, from RAGs to code assistants. I expect this trend to accelerate in 2025, with an even greater emphasis on domain- and application-specific optimizations (i.e., \"specializations\").Stages 1-3 are the common steps to developing LLMs. Stage 4 specializes LLMs for specific use cases.The development of reasoning models is one of these specializations. This means we refine LLMs to excel at complex tasks that are best solved with intermediate steps, such as puzzles, advanced math, and coding challenges. However, this specialization does not replace other LLM applications. Because transforming an LLM into a reasoning model also introduces certain drawbacks, which I will discuss later.To give you a brief glimpse of what's covered below, in this article, I will:Explain the meaning of \"reasoning model\"Discuss the advantages and disadvantages of reasoning modelsOutline the methodology behind DeepSeek R1Describe the four main approaches to building and improving reasoning modelsShare thoughts on the LLM landscape following the DeepSeek V3 and R1 releasesProvide tips for developing reasoning models on a tight budgetI hope you find this article useful as AI continues its rapid development this year!If you work in AI (or machine learning in general), you are probably familiar with vague and hotly debated definitions. The term \"reasoning models\" is no exception. Eventually, someone will define it formally in a paper, only for it to be redefined in the next, and so on.In this article, I define \"reasoning\" as the process of answering questions that require complex, multi-step generation with intermediate steps. For example, factual question-answering like \"What is the capital of France?\" does not involve reasoning. In contrast, a question like \"If a train is moving at 60 mph and travels for 3 hours, how far does it go?\" requires some simple reasoning. For instance, it requires recognizing the relationship between distance, speed, and time before arriving at the answer.A regular LLM may only provide a short answer (as shown on the left), whereas reasoning models typically include intermediate steps that reveal part of the thought process. (Note that many LLMs who have not been specifically developed for reasoning tasks can also provide intermediate reasoning steps in their answers.)Most modern LLMs are capable of basic reasoning and can answer questions like, \"If a train is moving at 60 mph and travels for 3 hours, how far does it go?\" So, today, when we refer to reasoning models, we typically mean LLMs that excel at more complex reasoning tasks, such as solving puzzles, riddles, and mathematical proofs.Additionally, most LLMs branded as reasoning models today include a \"thought\" or \"thinking\" process as part of their response. Whether and how an LLM actually \"thinks\" is a separate discussion.Intermediate steps in reasoning models can appear in two ways. First, they may be explicitly included in the response, as shown in the previous figure. Second, some reasoning LLMs, such as OpenAI's o1, run multiple iterations with intermediate steps that are not shown to the user.\"Reasoning\" is used at two different levels: 1) processing the input and generating via multiple intermediate steps and 2) providing some sort of reasoning as part of the response to the user.Now that we have defined reasoning models, we can move on to the more interesting part: how to build and improve LLMs for reasoning tasks. However, before diving into the technical details, it is important to consider when reasoning models are actually needed.When do we need a reasoning model? Reasoning models are designed to be good at complex tasks such as solving puzzles, advanced math problems, and challenging coding tasks. However, they are not necessary for simpler tasks like summarization, translation, or knowledge-based question answering. In fact, using reasoning models for everything can be inefficient and expensive. For instance, reasoning models are typically more expensive to use, more verbose, and sometimes more prone to errors due to \"overthinking.\" Also here the simple rule applies: Use the right tool (or type of LLM) for the task.The key strengths and limitations of reasoning models are summarized in the figure below.The key strengths and weaknesses of reasoning models.Before discussing four main approaches to building and improving reasoning models in the next section, I want to briefly outline the DeepSeek R1 pipeline, as described in the DeepSeek R1 technical report. This report serves as both an interesting case study and a blueprint for developing reasoning LLMs.Note that DeepSeek did not release a single R1 reasoning model but instead introduced three distinct variants: DeepSeek-R1-Zero, DeepSeek-R1, and DeepSeek-R1-Distill.Based on the descriptions in the technical report, I have summarized the development process of these models in the diagram below.Development process of DeepSeeks three different reasoning models that are discussed in the DeepSeek R1 technical report.Next, let's briefly go over the process shown in the diagram above. More details will be covered in the next section, where we discuss the four main approaches to building and improving reasoning models.(1) DeepSeek-R1-Zero: This model is based on the 671B pre-trained DeepSeek-V3 base model released in December 2024. The research team trained it using reinforcement learning (RL) with two types of rewards. This approach is referred to as \"cold start\" training because it did not include a supervised fine-tuning (SFT) step, which is typically part of reinforcement learning with human feedback (RLHF).(2) DeepSeek-R1: This is DeepSeek's flagship reasoning model, built upon DeepSeek-R1-Zero. The team further refined it with additional SFT stages and further RL training, improving upon the \"cold-started\" R1-Zero model.(3) DeepSeek-R1-Distill*: Using the SFT data generated in the previous steps, the DeepSeek team fine-tuned Qwen and Llama models to enhance their reasoning abilities. While not distillation in the traditional sense, this process involved training smaller models (Llama 8B and 70B, and Qwen 1.5Bâ€“30B) on outputs from the larger DeepSeek-R1 671B model.In this section, I will outline the key techniques currently used to enhance the reasoning capabilities of LLMs and to build specialized reasoning models such as DeepSeek-R1, OpenAI's o1 \u0026 o3, and others.Note: The exact workings of o1 and o3 remain unknown outside of OpenAI. However, they are rumored to leverage a combination of both inference and training techniques.One way to improve an LLM's reasoning capabilities (or any capability in general) is inference-time scaling. This term can have multiple meanings, but in this context, it refers to increasing computational resources during inference to improve output quality.A rough analogy is how humans tend to generate better responses when given more time to think through complex problems. Similarly, we can apply techniques that encourage the LLM to \"think\" more while generating an answer. (Although, whether LLMs actually \"think\" is a different discussion.)One straightforward approach to inference-time scaling is clever prompt engineering. A classic example is chain-of-thought (CoT) prompting, where phrases like \"think step by step\" are included in the input prompt. This encourages the model to generate intermediate reasoning steps rather than jumping directly to the final answer, which can often (but not always) lead to more accurate results on more complex problems. (Note that it doesn't make sense to employ this strategy for simpler knowledge-based questions, like \"What is the capital of France\", which is again a good rule of thumb to find out whether a reasoning model makes sense on your given input query.)An example of classic CoT prompting from the 2022 Large Language Models are Zero-Shot Reasoners paper (https://arxiv.org/abs/2205.11916).The aforementioned CoT approach can be seen as inference-time scaling because it makes inference more expensive through generating more output tokens.Another approach to inference-time scaling is the use of voting and search strategies. One simple example is majority voting where we have the LLM generate multiple answers, and we select the correct answer by majority vote. Similarly, we can use beam search and other search algorithms to generate better responses.I highly recommend the Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters paper that I described in my previous Noteworthy AI Research Papers of 2024 (Part Two) article (https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2) for more details on these different strategies.Different search-based methods rely on a process-reward-based model to select the best answer. Annotated figure from the LLM Test-Time Compute paper, https://arxiv.org/abs/2408.03314The DeepSeek R1 technical report states that its models do not use inference-time scaling. However, this technique is often implemented at the application layer on top of the LLM, so it is possible that DeepSeek applies it within their app.I suspect that OpenAI's o1 and o3 models use inference-time scaling, which would explain why they are relatively expensive compared to models like GPT-4o. In addition to inference-time scaling, o1 and o3 were likely trained using RL pipelines similar to those used for DeepSeek R1. More on reinforcement learning in the next two sections below.One of my personal highlights from the DeepSeek R1 paper is their discovery that reasoning emerges as a behavior from pure reinforcement learning (RL). Let's explore what this means in more detail.As outlined earlier, DeepSeek developed three types of R1 models. The first, DeepSeek-R1-Zero, was built on top of the DeepSeek-V3 base model, a standard pre-trained LLM they released in December 2024. Unlike typical RL pipelines, where supervised fine-tuning (SFT) is applied before RL, DeepSeek-R1-Zero was trained exclusively with reinforcement learning without an initial SFT stage as highlighted in the diagram below.The development process of DeepSeek-R1-Zero model.Still, this RL process is similar to the commonly used RLHF approach, which is typically applied to preference-tune LLMs. (I covered RLHF in more detail in my article, LLM Training: RLHF and Its Alternatives.) However, as mentioned above, the key difference in DeepSeek-R1-Zero is that they skipped the supervised fine-tuning (SFT) stage for instruction tuning. This is why they refer to it as \"pure\" RL. (Although, RL in the context of LLMs differs significantly from traditional RL, which is a topic for another time.)For rewards, instead of using a reward model trained on human preferences, they employed two types of rewards: an accuracy reward and a format reward.The accuracy reward uses the LeetCode compiler to verify coding answers and a deterministic system to evaluate mathematical responses.The format reward relies on an LLM judge to ensure responses follow the expected format, such as placing reasoning steps inside \u003cthink\u003e tags.Surprisingly, this approach was enough for the LLM to develop basic reasoning skills. The researchers observed an \"Aha!\" moment, where the model began generating reasoning traces as part of its responses despite not being explicitly trained to do so, as shown in the figure below.A figure from the DeepSeek R1 technical report (https://arxiv.org/abs/2501.12948) showing the emergence of the \"Aha\" moment.While R1-Zero is not a top-performing reasoning model, it does demonstrate reasoning capabilities by generating intermediate \"thinking\" steps, as shown in the figure above. This confirms that it is possible to develop a reasoning model using pure RL, and the DeepSeek team was the first to demonstrate (or at least publish) this approach.Next, let's look at the development of DeepSeek-R1, DeepSeekâ€™s flagship reasoning model, which serves as a blueprint for building reasoning models. This model improves upon DeepSeek-R1-Zero by incorporating additional supervised fine-tuning (SFT) and reinforcement learning (RL) to improve its reasoning performance.Note that it is actually common to include an SFT stage before RL, as seen in the standard RLHF pipeline. OpenAI's o1 was likely developed using a similar approach.The development process of DeepSeek-R1 model.As shown in the diagram above, the DeepSeek team used DeepSeek-R1-Zero to generate what they call \"cold-start\" SFT data. The term \"cold start\" refers to the fact that this data was produced by DeepSeek-R1-Zero, which itself had not been trained on any supervised fine-tuning (SFT) data.Using this cold-start SFT data, DeepSeek then trained the model via instruction fine-tuning, followed by another reinforcement learning (RL) stage. This RL stage retained the same accuracy and format rewards used in DeepSeek-R1-Zeroâ€™s RL process. However, they added a consistency reward to prevent language mixing, which occurs when the model switches between multiple languages within a response.The RL stage was followed by another round of SFT data collection. In this phase, the most recent model checkpoint was used to generate 600K Chain-of-Thought (CoT) SFT examples, while an additional 200K knowledge-based SFT examples were created using the DeepSeek-V3 base model.These 600K + 200K SFT samples were then used for another round of RL. In this stage, they again used rule-based methods for accuracy rewards for math and coding questions, while human preference labels used for other question types.The final model, DeepSeek-R1 has a noticeable performance boost over DeepSeek-R1-Zero thanks to the additional SFT and RL stages, as shown in the table below.Benchmark comparison of OpenAI A1 and DeepSeek R1 models. Annotated figure from the DeepSeek-R1 technical report (https://arxiv.org/abs/2501.12948).So far, we have covered three key approaches to building and improving reasoning models:1. Inference-time scaling, a technique that improves reasoning capabilities without training or otherwise modifying the underlying model.2. Pure reinforcement learning (RL) as in DeepSeek-R1-Zero, which showed that reasoning can emerge as a learned behavior without supervised fine-tuning.3. Supervised fine-tuning (SFT) plus RL, which led to DeepSeek-R1, DeepSeekâ€™s flagship reasoning model.So, whatâ€™s left? Model \"distillation.\"Surprisingly, DeepSeek also released smaller models trained via a process they call distillation. However, in the context of LLMs, distillation does not necessarily follow the classical knowledge distillation approach used in deep learning. Traditionally, in knowledge distillation (as briefly described in Chapter 6 of my Machine Learning Q and AI book), a smaller student model is trained on both the logits of a larger teacher model and a target dataset.Instead, here distillation refers to instruction fine-tuning smaller LLMs, such as Llama 8B and 70B and Qwen 2.5 models (0.5B to 32B), on an SFT dataset generated by larger LLMs. Specifically, these larger LLMs are DeepSeek-V3 and an intermediate checkpoint of DeepSeek-R1. In fact, the SFT data used for this distillation process is the same dataset that was used to train DeepSeek-R1, as described in the previous section.To clarify this process, I have highlighted the distillation portion in the diagram below.The development process of DeepSeek-R1-Distill models.Why did they develop these distilled models? In my opinion, there are two key reasons:1. Smaller models are more efficient. This means they are cheaper to run, but they also can run on lower-end hardware, which makes these especially interesting for many researchers and tinkerers like me.2. A case study in pure SFT. These distilled models serve as an interesting benchmark, showing how far pure supervised fine-tuning (SFT) can take a model without reinforcement learning.The table below compares the performance of these distilled models against other popular models, as well as DeepSeek-R1-Zero and DeepSeek-R1.Benchmark comparison of distilled versus non-distilled models. Annotated figure from the DeepSeek-R1 technical report (https://arxiv.org/abs/2501.12948).As we can see, the distilled models are noticeably weaker than DeepSeek-R1, but they are surprisingly strong relative to DeepSeek-R1-Zero, despite being orders of magnitude smaller. It's also interesting to note how well these models perform compared to o1 mini (I suspect o1-mini itself might be a similarly distilled version of o1).Before wrapping up this section with a conclusion, thereâ€™s one more interesting comparison worth mentioning. The DeepSeek team tested whether the emergent reasoning behavior seen in DeepSeek-R1-Zero could also appear in smaller models. To investigate this, they applied the same pure RL approach from DeepSeek-R1-Zero directly to Qwen-32B.The results of this experiment are summarized in the table below, where QwQ-32B-Preview serves as a reference reasoning model based on Qwen 2.5 32B developed by the Qwen team (I think the training details were never disclosed). This comparison provides some additional insights into whether pure RL alone can induce reasoning capabilities in models much smaller than DeepSeek-R1-Zero.Benchmark comparison distillation and RL on a smaller 32B model. Annotated figure from the DeepSeek-R1 technical report (https://arxiv.org/abs/2501.12948).Interestingly, the results suggest that distillation is far more effective than pure RL for smaller models. This aligns with the idea that RL alone may not be sufficient to induce strong reasoning abilities in models of this scale, whereas SFT on high-quality reasoning data can be a more effective strategy when working with small models.For completeness, it would have been useful to see additional comparisons in the table:1. Qwen-32B trained with SFT + RL, similar to how DeepSeek-R1 was developed. This would help determine how much improvement can be made, compared to pure RL and pure SFT, when RL is combined with SFT.2. DeepSeek-V3 trained with pure SFT, similar to how the distilled models were created. This would allow for a direct comparison to see how effective RL + SFT is over pure SFT.In this section, we explored four different strategies for building and improving reasoning models:1. Inference-time scaling requires no additional training but increases inference costs, making large-scale deployment more expensive as the number or users or query volume grows. Still, it remains a no-brainer for improving the performance of already strong models. I strongly suspect that o1 leverages inference-time scaling, which helps explain why it is more expensive on a per-token basis compared to DeepSeek-R1.2. Pure RL is interesting for research purposes because it provides insights into reasoning as an emergent behavior. However, in practical model development, RL + SFT is the preferred approach as it leads to stronger reasoning models. I strongly suspect that o1 was trained using RL + SFT as well. More precisely, I believe o1 starts from a weaker, smaller base model than DeepSeek-R1 but compensates with RL + SFT and inference-time scaling.3. As mentioned above, RL + SFT is the key approach for building high-performance reasoning models. DeepSeek-R1 is a nice blueprint showing how this can be done.4. Distillation is an attractive approach, especially for creating smaller, more efficient models. However, the limitation is that distillation does not drive innovation or produce the next generation of reasoning models. For instance, distillation always depends on an existing, stronger model to generate the supervised fine-tuning (SFT) data.One interesting aspect I expect to see next is to combine RL + SFT (approach 3) with inference-time scaling (approach 1). This is likely what OpenAI o1 is doing, except it's probably based on a weaker base model than DeepSeek-R1, which explains why DeepSeek-R1 performs so well while remaining relatively cheap at inference time.In recent weeks, many people have asked for my thoughts on the DeepSeek-R1 models. In short, I think they are an awesome achievement. As a research engineer, I particularly appreciate the detailed technical report, which provides insights into their methodology that I can learn from.One of the most fascinating takeaways is how reasoning emerged as a behavior from pure RL. And it's impressive that DeepSeek has open-sourced their models under a permissive open-source MIT license, which has even fewer restrictions than Meta's Llama models.How does it compare to o1?Is DeepSeek-R1 better than o1? Iâ€™d say itâ€™s roughly in the same ballpark. However, what stands out is that DeepSeek-R1 is more efficient at inference time. This suggests that DeepSeek likely invested more heavily in the training process, while OpenAI may have relied more on inference-time scaling for o1.That said, it's difficult to compare o1 and DeepSeek-R1 directly because OpenAI has not disclosed much about o1. For instance, we donâ€™t know:Is o1 also a Mixture of Experts (MoE)?How large is o1?Could o1 just be a slightly refined version of GPT-4o with minimal RL + SFT and only extensive inference-time scaling?Without knowing these details, a direct comparison remains an apples-to-oranges comparison.The cost of training DeepSeek-R1Another point of discussion has been the cost of developing DeepSeek-R1. Some have mentioned a ~$6 million training cost, but they likely conflated DeepSeek-V3 (the base model released in December last year) and DeepSeek-R1.The $6 million estimate is based on an assumed $2 per GPU hour and the number of GPU hours required for the final training run of DeepSeek-V3, which was originally discussed back in December 2024.However, the DeepSeek team has never disclosed the exact GPU hours or development cost for R1, so any cost estimates remain pure speculation.Either way, ultimately, DeepSeek-R1 is a major milestone in open-weight reasoning models, and its efficiency at inference time makes it an interesting alternative to OpenAIâ€™s o1.Developing a DeepSeek-R1-level reasoning model likely requires hundreds of thousands to millions of dollars, even when starting with an open-weight base model like DeepSeek-V3. This can feel discouraging for researchers or engineers working with limited budgets.The good news: Distillation can go a long wayFortunately, model distillation offers a more cost-effective alternative. The DeepSeek team demonstrated this with their R1-distilled models, which achieve surprisingly strong reasoning performance despite being significantly smaller than DeepSeek-R1. However, even this approach isnâ€™t entirely cheap. Their distillation process used 800K SFT samples, which requires substantial compute.Interestingly, just a few days before DeepSeek-R1 was released, I came across an article about Sky-T1, a fascinating project where a small team trained an open-weight 32B model using only 17K SFT samples. The total cost? Just $450, which is less than the registration fee for most AI conferences.This example highlights that while large-scale training remains expensive, smaller, targeted fine-tuning efforts can still yield impressive results at a fraction of the cost.Figure from the \"Sky-T1: Train your own O1 preview model within $450\" article, https://novasky-ai.github.io/posts/sky-t1/According to their benchmarks, Sky-T1 performs roughly on par with o1, which is impressive given its low training cost.Pure RL on a budget: TinyZeroWhile Sky-T1 focused on model distillation, I also came across some interesting work in the \"pure RL\" space. One notable example is TinyZero, a 3B parameter model that replicates the DeepSeek-R1-Zero approach (side note: it costs less than $30 to train).Surprisingly, even at just 3B parameters, TinyZero exhibits some emergent self-verification abilities, which supports the idea that reasoning can emerge through pure RL, even in small models.The TinyZero repository mentions that a research report is still work in progress, and Iâ€™ll definitely be keeping an eye out for further details.A figure from the TinyZero repository (https://github.com/Jiayi-Pan/TinyZero) showing that the model is capable of self-verification. (It would have been interesting to see the response of the base model in comparison.)The two projects mentioned above demonstrate that interesting work on reasoning models is possible even with limited budgets. While both approaches replicate methods from DeepSeek-R1, one focusing on pure RL (TinyZero) and the other on pure SFT (Sky-T1), it would be fascinating to explore how these ideas can be extended further.Beyond Traditional SFT: Journey LearningOne particularly interesting approach I came across last year is described in the paper O1 Replication Journey: A Strategic Progress Report â€“ Part 1. Despite its title, the paper does not actually replicate o1. Instead, it introduces an different way to improve the distillation (pure SFT) process.The key idea in the paper is \"journey learning\" as an alternative to \"shortcut learning.\"Shortcut learning refers to the traditional approach in instruction fine-tuning, where models are trained using only correct solution paths.Journey learning, on the other hand, also includes incorrect solution paths, allowing the model to learn from mistakes.This approach is kind of related to the self-verification abilities observed in TinyZeroâ€™s pure RL training, but it focuses on improving the model entirely through SFT. By exposing the model to incorrect reasoning paths and their corrections, journey learning may also reinforce self-correction abilities, potentially making reasoning models more reliable this way.Journey learning, as opposed to traditional shortcut learning, includes wrong solutions paths in the SFT data. Annotated figure from the O1 Replication Journey: A Strategic Progress Report â€“ Part 1 (https://arxiv.org/abs/2410.18982)This could be an exciting direction for future work, particularly for low-budget reasoning model development, where RL-based approaches may be computationally impractical.Anyways, a lot of interesting work is currently happening on the reasoning model front, and I'm sure we will see a lot more exciting work in the upcoming months!This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my Build a Large Language Model (From Scratch) book. (I am confident that you'll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)If you read the book and have a few minutes to spare, I'd really appreciate a brief review. It helps us authors a lot!Your support means a great deal! Thank you!Discussion about this post",
  "image": "https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle\u003e\u003cdiv dir=\"auto\"\u003e\u003cp\u003eThis article describes the four main approaches to building reasoning models, or how we can enhance LLMs with reasoning capabilities. I hope this provides valuable insights and helps you navigate the rapidly evolving literature and hype surrounding this topic.\u003c/p\u003e\u003cp\u003eIn 2024, the LLM field saw increasing specialization. Beyond pre-training and fine-tuning, we witnessed the rise of specialized applications, from RAGs to code assistants. I expect this trend to accelerate in 2025, with an even greater emphasis on domain- and application-specific optimizations (i.e., \u0026#34;specializations\u0026#34;).\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png\" width=\"1456\" height=\"755\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/d6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:755,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:false,\u0026#34;topImage\u0026#34;:true,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cem\u003eStages 1-3 are the common steps to developing LLMs. Stage 4 specializes LLMs for specific use cases.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eThe development of reasoning models is one of these specializations. This means we refine LLMs to excel at complex tasks that are best solved with intermediate steps, such as puzzles, advanced math, and coding challenges. However, this specialization does not replace other LLM applications. Because transforming an LLM into a reasoning model also introduces certain drawbacks, which I will discuss later.\u003c/p\u003e\u003cp\u003eTo give you a brief glimpse of what\u0026#39;s covered below, in this article, I will:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eExplain the meaning of \u0026#34;reasoning model\u0026#34;\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDiscuss the advantages and disadvantages of reasoning models\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOutline the methodology behind DeepSeek R1\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDescribe the four main approaches to building and improving reasoning models\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eShare thoughts on the LLM landscape following the DeepSeek V3 and R1 releases\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eProvide tips for developing reasoning models on a tight budget\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eI hope you find this article useful as AI continues its rapid development this year!\u003c/p\u003e\u003cp\u003eIf you work in AI (or machine learning in general), you are probably familiar with vague and hotly debated definitions. The term \u0026#34;reasoning models\u0026#34; is no exception. Eventually, someone will define it formally in a paper, only for it to be redefined in the next, and so on.\u003c/p\u003e\u003cp\u003eIn this article, I define \u0026#34;reasoning\u0026#34; as the process of answering questions that require complex, multi-step generation with intermediate steps. For example, factual question-answering like \u0026#34;What is the capital of France?\u0026#34; does not involve reasoning. In contrast, a question like \u0026#34;If a train is moving at 60 mph and travels for 3 hours, how far does it go?\u0026#34; requires some simple reasoning. For instance, it requires recognizing the relationship between distance, speed, and time before arriving at the answer.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04124450-9742-4c2e-899b-10b041404ad0_1450x830.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04124450-9742-4c2e-899b-10b041404ad0_1450x830.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04124450-9742-4c2e-899b-10b041404ad0_1450x830.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04124450-9742-4c2e-899b-10b041404ad0_1450x830.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04124450-9742-4c2e-899b-10b041404ad0_1450x830.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04124450-9742-4c2e-899b-10b041404ad0_1450x830.png\" width=\"632\" height=\"361.7655172413793\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/04124450-9742-4c2e-899b-10b041404ad0_1450x830.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:830,\u0026#34;width\u0026#34;:1450,\u0026#34;resizeWidth\u0026#34;:632,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04124450-9742-4c2e-899b-10b041404ad0_1450x830.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04124450-9742-4c2e-899b-10b041404ad0_1450x830.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04124450-9742-4c2e-899b-10b041404ad0_1450x830.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04124450-9742-4c2e-899b-10b041404ad0_1450x830.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cem\u003eA regular LLM may only provide a short answer (as shown on the left), whereas reasoning models typically include intermediate steps that reveal part of the thought process. (Note that many LLMs who have not been specifically developed for reasoning tasks can also provide intermediate reasoning steps in their answers.)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eMost modern LLMs are capable of basic reasoning and can answer questions like, \u0026#34;If a train is moving at 60 mph and travels for 3 hours, how far does it go?\u0026#34; So, today, when we refer to reasoning models, we typically mean LLMs that excel at more complex reasoning tasks, such as solving puzzles, riddles, and mathematical proofs.\u003c/p\u003e\u003cp\u003eAdditionally, most LLMs branded as reasoning models today include a \u0026#34;thought\u0026#34; or \u0026#34;thinking\u0026#34; process as part of their response. Whether and how an LLM actually \u0026#34;thinks\u0026#34; is a separate discussion.\u003c/p\u003e\u003cp\u003eIntermediate steps in reasoning models can appear in two ways. First, they may be explicitly included in the response, as shown in the previous figure. Second, some reasoning LLMs, such as OpenAI\u0026#39;s o1, run multiple iterations with intermediate steps that are not shown to the user.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png\" width=\"1456\" height=\"767\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:767,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cem\u003e\u0026#34;Reasoning\u0026#34; is used at two different levels: 1) processing the input and generating via multiple intermediate steps and 2) providing some sort of reasoning as part of the response to the user.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eNow that we have defined reasoning models, we can move on to the more interesting part: how to build and improve LLMs for reasoning tasks. However, before diving into the technical details, it is important to consider when reasoning models are actually needed.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhen do we need a reasoning model?\u003c/strong\u003e\u003cspan\u003e Reasoning models are designed to be good at complex tasks such as solving puzzles, advanced math problems, and challenging coding tasks. However, they are not necessary for simpler tasks like summarization, translation, or knowledge-based question answering. In fact, using reasoning models for everything can be inefficient and expensive. For instance, reasoning models are typically more expensive to use, more verbose, and sometimes more prone to errors due to \u0026#34;overthinking.\u0026#34; Also here the simple rule applies: Use the right tool (or type of LLM) for the task.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eThe key strengths and limitations of reasoning models are summarized in the figure below.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png\" width=\"1352\" height=\"524\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:524,\u0026#34;width\u0026#34;:1352,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cem\u003eThe key strengths and weaknesses of reasoning models.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eBefore discussing four main approaches to building and improving reasoning models in the next section, I want to briefly outline the DeepSeek R1 pipeline, as described in the\u003c/span\u003e\u003ca href=\"https://arxiv.org/abs/2501.12948\" rel=\"\"\u003e DeepSeek R1 technical report\u003c/a\u003e\u003cspan\u003e. This report serves as both an interesting case study and a blueprint for developing reasoning LLMs.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eNote that DeepSeek did not release a single R1 reasoning model but instead introduced three distinct variants: DeepSeek-R1-Zero, DeepSeek-R1, and DeepSeek-R1-Distill.\u003c/p\u003e\u003cp\u003eBased on the descriptions in the technical report, I have summarized the development process of these models in the diagram below.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png\" width=\"1456\" height=\"1162\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/e9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:1162,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:273496,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d62ee0-577f-43c0-9f21-3c5dfb2ca72d_1468x1172.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eDevelopment process of DeepSeeks three different reasoning models that are discussed in the DeepSeek R1 technical report.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eNext, let\u0026#39;s briefly go over the process shown in the diagram above. More details will be covered in the next section, where we discuss the four main approaches to building and improving reasoning models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e(1) DeepSeek-R1-Zero: \u003c/strong\u003e\u003cspan\u003eThis model is based on the 671B pre-trained DeepSeek-V3 base model released in December 2024. The research team trained it using reinforcement learning (RL) with two types of rewards. This approach is referred to as \u0026#34;cold start\u0026#34; training because it did not include a supervised fine-tuning (SFT) step, which is typically part of reinforcement learning with human feedback (RLHF).\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e(2) DeepSeek-R1: \u003c/strong\u003e\u003cspan\u003eThis is DeepSeek\u0026#39;s flagship reasoning model, built upon DeepSeek-R1-Zero. The team further refined it with additional SFT stages and further RL training, improving upon the \u0026#34;cold-started\u0026#34; R1-Zero model.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e(3) DeepSeek-R1-Distill*: \u003c/strong\u003e\u003cspan\u003eUsing the SFT data generated in the previous steps, the DeepSeek team fine-tuned Qwen and Llama models to enhance their reasoning abilities. While not distillation in the traditional sense, this process involved training smaller models (Llama 8B and 70B, and Qwen 1.5Bâ€“30B) on outputs from the larger DeepSeek-R1 671B model.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eIn this section, I will outline the key techniques currently used to enhance the reasoning capabilities of LLMs and to build specialized reasoning models such as DeepSeek-R1, OpenAI\u0026#39;s o1 \u0026amp; o3, and others.\u003c/p\u003e\u003cp\u003eNote: The exact workings of o1 and o3 remain unknown outside of OpenAI. However, they are rumored to leverage a combination of both inference and training techniques.\u003c/p\u003e\u003cp\u003eOne way to improve an LLM\u0026#39;s reasoning capabilities (or any capability in general) is inference-time scaling. This term can have multiple meanings, but in this context, it refers to increasing computational resources during inference to improve output quality.\u003c/p\u003e\u003cp\u003eA rough analogy is how humans tend to generate better responses when given more time to think through complex problems. Similarly, we can apply techniques that encourage the LLM to \u0026#34;think\u0026#34; more while generating an answer. (Although, whether LLMs actually \u0026#34;think\u0026#34; is a different discussion.)\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOne straightforward approach to inference-time scaling is clever prompt engineering. A classic example is \u003c/span\u003e\u003cem\u003echain-of-thought (CoT) prompting\u003c/em\u003e\u003cspan\u003e, where phrases like \u0026#34;think step by step\u0026#34; are included in the input prompt. This encourages the model to generate intermediate reasoning steps rather than jumping directly to the final answer, which can often (but not always) lead to more accurate results on more complex problems. (Note that it doesn\u0026#39;t make sense to employ this strategy for simpler knowledge-based questions, like \u0026#34;What is the capital of France\u0026#34;, which is again a good rule of thumb to find out whether a reasoning model makes sense on your given input query.)\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png\" width=\"1456\" height=\"381\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:381,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cem\u003eAn example of classic CoT prompting from the 2022 Large Language Models are Zero-Shot Reasoners paper (https://arxiv.org/abs/2205.11916).\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eThe aforementioned CoT approach can be seen as inference-time scaling because it makes inference more expensive through generating more output tokens.\u003c/p\u003e\u003cp\u003eAnother approach to inference-time scaling is the use of voting and search strategies. One simple example is majority voting where we have the LLM generate multiple answers, and we select the correct answer by majority vote. Similarly, we can use beam search and other search algorithms to generate better responses.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eI highly recommend the \u003c/span\u003e\u003cem\u003e\u003ca href=\"https://arxiv.org/abs/2408.03314\" rel=\"\"\u003eScaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters\u003c/a\u003e\u003c/em\u003e\u003cspan\u003e paper that I described in my previous Noteworthy AI Research Papers of 2024 (Part Two) article (https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2) for more details on these different strategies.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png\" width=\"1456\" height=\"836\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:836,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cem\u003eDifferent search-based methods rely on a process-reward-based model to select the best answer. Annotated figure from the LLM Test-Time Compute paper, https://arxiv.org/abs/2408.03314\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eThe DeepSeek R1 technical report states that its models do not use inference-time scaling. However, this technique is often implemented at the application layer on top of the LLM, so it is possible that DeepSeek applies it within their app.\u003c/p\u003e\u003cp\u003eI suspect that OpenAI\u0026#39;s o1 and o3 models use inference-time scaling, which would explain why they are relatively expensive compared to models like GPT-4o. In addition to inference-time scaling, o1 and o3 were likely trained using RL pipelines similar to those used for DeepSeek R1. More on reinforcement learning in the next two sections below.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOne of my personal highlights from the \u003c/span\u003e\u003ca href=\"https://arxiv.org/abs/2501.12948\" rel=\"\"\u003eDeepSeek R1 paper\u003c/a\u003e\u003cspan\u003e is their discovery that reasoning emerges as a behavior from pure reinforcement learning (RL). Let\u0026#39;s explore what this means in more detail.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eAs outlined earlier, DeepSeek developed three types of R1 models. The first, \u003c/span\u003e\u003cstrong\u003eDeepSeek-R1-Zero\u003c/strong\u003e\u003cspan\u003e, was built on top of the DeepSeek-V3 base model, a standard pre-trained LLM they released in December 2024. Unlike typical RL pipelines, where supervised fine-tuning (SFT) is applied before RL, DeepSeek-R1-Zero was trained \u003c/span\u003e\u003cstrong\u003eexclusively\u003c/strong\u003e\u003cspan\u003e with reinforcement learning without an initial SFT stage as highlighted in the diagram below.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png\" width=\"1438\" height=\"1156\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:1156,\u0026#34;width\u0026#34;:1438,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:275867,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8d5351-f882-474a-b8b1-1a27d7c41176_1438x1156.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eThe development process of DeepSeek-R1-Zero model.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eStill, this RL process is similar to the commonly used RLHF approach, which is typically applied to preference-tune LLMs. (I covered RLHF in more detail in my article,\u003c/span\u003e\u003ca href=\"https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives\" rel=\"\"\u003e \u003c/a\u003e\u003cem\u003e\u003ca href=\"https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives\" rel=\"\"\u003eLLM Training: RLHF and Its Alternatives\u003c/a\u003e\u003c/em\u003e\u003cspan\u003e.) However, as mentioned above, the key difference in \u003c/span\u003e\u003cem\u003eDeepSeek-R1-Zero\u003c/em\u003e\u003cspan\u003e is that they skipped the supervised fine-tuning (SFT) stage for instruction tuning. This is why they refer to it as \u0026#34;pure\u0026#34; RL. (Although, RL in the context of LLMs differs significantly from traditional RL, which is a topic for another time.)\u003c/span\u003e\u003c/p\u003e\u003cp\u003eFor rewards, instead of using a reward model trained on human preferences, they employed two types of rewards: an accuracy reward and a format reward.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eThe \u003c/span\u003e\u003cstrong\u003eaccuracy reward\u003c/strong\u003e\u003cspan\u003e uses the LeetCode compiler to verify coding answers and a deterministic system to evaluate mathematical responses.\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eThe \u003c/span\u003e\u003cstrong\u003eformat reward\u003c/strong\u003e\u003cspan\u003e relies on an LLM judge to ensure responses follow the expected format, such as placing reasoning steps inside \u0026lt;think\u0026gt; tags.\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSurprisingly, this approach was enough for the LLM to develop basic reasoning skills. The researchers observed an \u0026#34;Aha!\u0026#34; moment, where the model began generating reasoning traces as part of its responses despite not being explicitly trained to do so, as shown in the figure below.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png\" width=\"1456\" height=\"927\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:927,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cem\u003eA figure from the DeepSeek R1 technical report (https://arxiv.org/abs/2501.12948) showing the emergence of the \u0026#34;Aha\u0026#34; moment.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eWhile R1-Zero is not a top-performing reasoning model, it does demonstrate reasoning capabilities by generating intermediate \u0026#34;thinking\u0026#34; steps, as shown in the figure above. This confirms that it is possible to develop a reasoning model using pure RL, and the DeepSeek team was the first to demonstrate (or at least publish) this approach.\u003c/p\u003e\u003cp\u003eNext, let\u0026#39;s look at the development of DeepSeek-R1, DeepSeekâ€™s flagship reasoning model, which serves as a blueprint for building reasoning models. This model improves upon DeepSeek-R1-Zero by incorporating additional supervised fine-tuning (SFT) and reinforcement learning (RL) to improve its reasoning performance.\u003c/p\u003e\u003cp\u003eNote that it is actually common to include an SFT stage before RL, as seen in the standard RLHF pipeline. OpenAI\u0026#39;s o1 was likely developed using a similar approach.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png\" width=\"1456\" height=\"1157\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:1157,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:290154,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8793ca00-f680-4327-bd87-1e69e60c8fa5_1460x1160.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eThe development process of DeepSeek-R1 model.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eAs shown in the diagram above, the DeepSeek team used DeepSeek-R1-Zero to generate what they call \u0026#34;cold-start\u0026#34; SFT data. The term \u0026#34;cold start\u0026#34; refers to the fact that this data was produced by DeepSeek-R1-Zero, which itself had not been trained on any supervised fine-tuning (SFT) data.\u003c/p\u003e\u003cp\u003eUsing this cold-start SFT data, DeepSeek then trained the model via instruction fine-tuning, followed by another reinforcement learning (RL) stage. This RL stage retained the same accuracy and format rewards used in DeepSeek-R1-Zeroâ€™s RL process. However, they added a consistency reward to prevent language mixing, which occurs when the model switches between multiple languages within a response.\u003c/p\u003e\u003cp\u003eThe RL stage was followed by another round of SFT data collection. In this phase, the most recent model checkpoint was used to generate 600K Chain-of-Thought (CoT) SFT examples, while an additional 200K knowledge-based SFT examples were created using the DeepSeek-V3 base model.\u003c/p\u003e\u003cp\u003eThese 600K + 200K SFT samples were then used for another round of RL. In this stage, they again used rule-based methods for accuracy rewards for math and coding questions, while human preference labels used for other question types.\u003c/p\u003e\u003cp\u003eThe final model, DeepSeek-R1 has a noticeable performance boost over DeepSeek-R1-Zero thanks to the additional SFT and RL stages, as shown in the table below.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png\" width=\"1456\" height=\"459\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/f7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:459,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eBenchmark comparison of OpenAI A1 and DeepSeek R1 models. Annotated figure from the DeepSeek-R1 technical report (https://arxiv.org/abs/2501.12948).\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eSo far, we have covered three key approaches to building and improving reasoning models:\u003c/p\u003e\u003cp\u003e1. Inference-time scaling, a technique that improves reasoning capabilities without training or otherwise modifying the underlying model.\u003c/p\u003e\u003cp\u003e2. Pure reinforcement learning (RL) as in DeepSeek-R1-Zero, which showed that reasoning can emerge as a learned behavior without supervised fine-tuning.\u003c/p\u003e\u003cp\u003e3. Supervised fine-tuning (SFT) plus RL, which led to DeepSeek-R1, DeepSeekâ€™s flagship reasoning model.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSo, whatâ€™s left? Model \u0026#34;distillation.\u0026#34;\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSurprisingly, DeepSeek also released smaller models trained via a process they call \u003c/span\u003e\u003cem\u003edistillation\u003c/em\u003e\u003cspan\u003e. However, in the context of LLMs, distillation does not necessarily follow the classical knowledge distillation approach used in deep learning. Traditionally, in knowledge distillation (as briefly described in Chapter 6 of my \u003c/span\u003e\u003ca href=\"https://amzn.to/40YYowg\" rel=\"\"\u003eMachine Learning Q and AI\u003c/a\u003e\u003cspan\u003e book), a smaller student model is trained on both the logits of a larger teacher model and a target dataset.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eInstead, here distillation refers to instruction fine-tuning smaller LLMs, such as Llama 8B and 70B and Qwen 2.5 models (0.5B to 32B), on an SFT dataset generated by larger LLMs. Specifically, these larger LLMs are DeepSeek-V3 and an intermediate checkpoint of DeepSeek-R1. In fact, the SFT data used for this distillation process is the same dataset that was used to train DeepSeek-R1, as described in the previous section.\u003c/p\u003e\u003cp\u003eTo clarify this process, I have highlighted the distillation portion in the diagram below.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png\" width=\"1430\" height=\"1142\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/be12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:1142,\u0026#34;width\u0026#34;:1430,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:278189,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe12b7ce-22d6-464e-887c-5eee7776f82b_1430x1142.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eThe development process of DeepSeek-R1-Distill models.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eWhy did they develop these distilled models? In my opinion, there are two key reasons:\u003c/p\u003e\u003cp\u003e1. Smaller models are more efficient. This means they are cheaper to run, but they also can run on lower-end hardware, which makes these especially interesting for many researchers and tinkerers like me.\u003c/p\u003e\u003cp\u003e2. A case study in pure SFT. These distilled models serve as an interesting benchmark, showing how far pure supervised fine-tuning (SFT) can take a model without reinforcement learning.\u003c/p\u003e\u003cp\u003eThe table below compares the performance of these distilled models against other popular models, as well as DeepSeek-R1-Zero and DeepSeek-R1.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png\" width=\"1168\" height=\"604\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/ebc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:604,\u0026#34;width\u0026#34;:1168,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eBenchmark comparison of distilled versus non-distilled models. Annotated figure from the DeepSeek-R1 technical report (https://arxiv.org/abs/2501.12948).\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eAs we can see, the distilled models are noticeably weaker than DeepSeek-R1, but they are surprisingly strong relative to DeepSeek-R1-Zero, despite being orders of magnitude smaller. It\u0026#39;s also interesting to note how well these models perform compared to o1 mini (I suspect o1-mini itself might be a similarly distilled version of o1).\u003c/p\u003e\u003cp\u003eBefore wrapping up this section with a conclusion, thereâ€™s one more interesting comparison worth mentioning. The DeepSeek team tested whether the emergent reasoning behavior seen in DeepSeek-R1-Zero could also appear in smaller models. To investigate this, they applied the same pure RL approach from DeepSeek-R1-Zero directly to Qwen-32B.\u003c/p\u003e\u003cp\u003eThe results of this experiment are summarized in the table below, where QwQ-32B-Preview serves as a reference reasoning model based on Qwen 2.5 32B developed by the Qwen team (I think the training details were never disclosed). This comparison provides some additional insights into whether pure RL alone can induce reasoning capabilities in models much smaller than DeepSeek-R1-Zero.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png\" width=\"1448\" height=\"408\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:408,\u0026#34;width\u0026#34;:1448,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eBenchmark comparison distillation and RL on a smaller 32B model. Annotated figure from the DeepSeek-R1 technical report (https://arxiv.org/abs/2501.12948).\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eInterestingly, the results suggest that distillation is far more effective than pure RL for smaller models. This aligns with the idea that RL alone may not be sufficient to induce strong reasoning abilities in models of this scale, whereas SFT on high-quality reasoning data can be a more effective strategy when working with small models.\u003c/p\u003e\u003cp\u003eFor completeness, it would have been useful to see additional comparisons in the table:\u003c/p\u003e\u003cp\u003e1. Qwen-32B trained with SFT + RL, similar to how DeepSeek-R1 was developed. This would help determine how much improvement can be made, compared to pure RL and pure SFT, when RL is combined with SFT.\u003c/p\u003e\u003cp\u003e2. DeepSeek-V3 trained with pure SFT, similar to how the distilled models were created. This would allow for a direct comparison to see how effective RL + SFT is over pure SFT.\u003c/p\u003e\u003cp\u003eIn this section, we explored four different strategies for building and improving reasoning models:\u003c/p\u003e\u003cp\u003e1. Inference-time scaling requires no additional training but increases inference costs, making large-scale deployment more expensive as the number or users or query volume grows. Still, it remains a no-brainer for improving the performance of already strong models. I strongly suspect that o1 leverages inference-time scaling, which helps explain why it is more expensive on a per-token basis compared to DeepSeek-R1.\u003c/p\u003e\u003cp\u003e2. Pure RL is interesting for research purposes because it provides insights into reasoning as an emergent behavior. However, in practical model development, RL + SFT is the preferred approach as it leads to stronger reasoning models. I strongly suspect that o1 was trained using RL + SFT as well. More precisely, I believe o1 starts from a weaker, smaller base model than DeepSeek-R1 but compensates with RL + SFT and inference-time scaling.\u003c/p\u003e\u003cp\u003e3. As mentioned above, RL + SFT is the key approach for building high-performance reasoning models. DeepSeek-R1 is a nice blueprint showing how this can be done.\u003c/p\u003e\u003cp\u003e4. Distillation is an attractive approach, especially for creating smaller, more efficient models. However, the limitation is that distillation does not drive innovation or produce the next generation of reasoning models. For instance, distillation always depends on an existing, stronger model to generate the supervised fine-tuning (SFT) data.\u003c/p\u003e\u003cp\u003eOne interesting aspect I expect to see next is to combine RL + SFT (approach 3) with inference-time scaling (approach 1). This is likely what OpenAI o1 is doing, except it\u0026#39;s probably based on a weaker base model than DeepSeek-R1, which explains why DeepSeek-R1 performs so well while remaining relatively cheap at inference time.\u003c/p\u003e\u003cp\u003eIn recent weeks, many people have asked for my thoughts on the DeepSeek-R1 models. In short, I think they are an awesome achievement. As a research engineer, I particularly appreciate the detailed technical report, which provides insights into their methodology that I can learn from.\u003c/p\u003e\u003cp\u003eOne of the most fascinating takeaways is how reasoning emerged as a behavior from pure RL. And it\u0026#39;s impressive that DeepSeek has open-sourced their models under a permissive open-source MIT license, which has even fewer restrictions than Meta\u0026#39;s Llama models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow does it compare to o1?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIs DeepSeek-R1 better than o1? Iâ€™d say itâ€™s roughly in the same ballpark. However, what stands out is that DeepSeek-R1 is more efficient at inference time. This suggests that DeepSeek likely invested more heavily in the training process, while OpenAI may have relied more on inference-time scaling for o1.\u003c/p\u003e\u003cp\u003eThat said, it\u0026#39;s difficult to compare o1 and DeepSeek-R1 directly because OpenAI has not disclosed much about o1. For instance, we donâ€™t know:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eIs o1 also a Mixture of Experts (MoE)?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eHow large is o1?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCould o1 just be a slightly refined version of GPT-4o with minimal RL + SFT and only extensive inference-time scaling?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWithout knowing these details, a direct comparison remains an apples-to-oranges comparison.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eThe cost of training DeepSeek-R1\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAnother point of discussion has been the cost of developing DeepSeek-R1. Some have mentioned a ~$6 million training cost, but they likely conflated DeepSeek-V3 (the base model released in December last year) and DeepSeek-R1.\u003c/p\u003e\u003cp\u003eThe $6 million estimate is based on an assumed $2 per GPU hour and the number of GPU hours required for the final training run of DeepSeek-V3, which was originally discussed back in December 2024.\u003c/p\u003e\u003cp\u003eHowever, the DeepSeek team has never disclosed the exact GPU hours or development cost for R1, so any cost estimates remain pure speculation.\u003c/p\u003e\u003cp\u003eEither way, ultimately, DeepSeek-R1 is a major milestone in open-weight reasoning models, and its efficiency at inference time makes it an interesting alternative to OpenAIâ€™s o1.\u003c/p\u003e\u003cp\u003eDeveloping a DeepSeek-R1-level reasoning model likely requires hundreds of thousands to millions of dollars, even when starting with an open-weight base model like DeepSeek-V3. This can feel discouraging for researchers or engineers working with limited budgets.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eThe good news: Distillation can go a long way\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFortunately, model distillation offers a more cost-effective alternative. The DeepSeek team demonstrated this with their R1-distilled models, which achieve surprisingly strong reasoning performance despite being significantly smaller than DeepSeek-R1. However, even this approach isnâ€™t entirely cheap. Their distillation process used 800K SFT samples, which requires substantial compute.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eInterestingly, just a few days before DeepSeek-R1 was released, I came across \u003c/span\u003e\u003ca href=\"https://novasky-ai.github.io/posts/sky-t1/\" rel=\"\"\u003ean article about Sky-T1\u003c/a\u003e\u003cspan\u003e, a fascinating project where a small team trained an open-weight 32B model using only 17K SFT samples. The total cost? Just $450, which is less than the registration fee for most AI conferences.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eThis example highlights that while large-scale training remains expensive, smaller, targeted fine-tuning efforts can still yield impressive results at a fraction of the cost.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png\" width=\"1364\" height=\"570\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:570,\u0026#34;width\u0026#34;:1364,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eFigure from the \u0026#34;Sky-T1: Train your own O1 preview model within $450\u0026#34; article, https://novasky-ai.github.io/posts/sky-t1/\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eAccording to their benchmarks, Sky-T1 performs roughly on par with o1, which is impressive given its low training cost.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePure RL on a budget: TinyZero\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eWhile Sky-T1 focused on model distillation, I also came across some interesting work in the \u0026#34;pure RL\u0026#34; space. One notable example is \u003c/span\u003e\u003ca href=\"https://github.com/Jiayi-Pan/TinyZero/\" rel=\"\"\u003eTinyZero\u003c/a\u003e\u003cspan\u003e, a 3B parameter model that replicates the DeepSeek-R1-Zero approach (side note: it costs less than $30 to train).\u003c/span\u003e\u003c/p\u003e\u003cp\u003eSurprisingly, even at just 3B parameters, TinyZero exhibits some emergent self-verification abilities, which supports the idea that reasoning can emerge through pure RL, even in small models.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eThe \u003c/span\u003e\u003ca href=\"https://github.com/Jiayi-Pan/TinyZero/\" rel=\"\"\u003eTinyZero repository\u003c/a\u003e\u003cspan\u003e mentions that a research report is still work in progress, and Iâ€™ll definitely be keeping an eye out for further details.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png\" width=\"1456\" height=\"869\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:869,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eA figure from the TinyZero repository (https://github.com/Jiayi-Pan/TinyZero) showing that the model is capable of self-verification. (It would have been interesting to see the response of the base model in comparison.)\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eThe two projects mentioned above demonstrate that interesting work on reasoning models is possible even with limited budgets. While both approaches replicate methods from DeepSeek-R1, one focusing on pure RL (TinyZero) and the other on pure SFT (Sky-T1), it would be fascinating to explore how these ideas can be extended further.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBeyond Traditional SFT: Journey Learning\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOne particularly interesting approach I came across last year is described in the paper\u003c/span\u003e\u003ca href=\"https://arxiv.org/abs/2410.18982\" rel=\"\"\u003e \u003c/a\u003e\u003cem\u003e\u003ca href=\"https://arxiv.org/abs/2410.18982\" rel=\"\"\u003eO1 Replication Journey: A Strategic Progress Report â€“ Part 1\u003c/a\u003e\u003c/em\u003e\u003cspan\u003e. Despite its title, the paper does not actually replicate o1. Instead, it introduces an different way to improve the distillation (pure SFT) process.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eThe key idea in the paper is \u0026#34;journey learning\u0026#34; as an alternative to \u0026#34;shortcut learning.\u0026#34;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eShortcut learning refers to the traditional approach in instruction fine-tuning, where models are trained using only correct solution paths.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eJourney learning, on the other hand, also includes incorrect solution paths, allowing the model to learn from mistakes.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis approach is kind of related to the self-verification abilities observed in TinyZeroâ€™s pure RL training, but it focuses on improving the model entirely through SFT. By exposing the model to incorrect reasoning paths and their corrections, journey learning may also reinforce self-correction abilities, potentially making reasoning models more reliable this way.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png\" width=\"1456\" height=\"707\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:707,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:null,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:null,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eJourney learning, as opposed to traditional shortcut learning, includes wrong solutions paths in the SFT data. Annotated figure from the O1 Replication Journey: A Strategic Progress Report â€“ Part 1 (https://arxiv.org/abs/2410.18982)\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eThis could be an exciting direction for future work, particularly for low-budget reasoning model development, where RL-based approaches may be computationally impractical.\u003c/p\u003e\u003cp\u003eAnyways, a lot of interesting work is currently happening on the reasoning model front, and I\u0026#39;m sure we will see a lot more exciting work in the upcoming months!\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cspan\u003eThis magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my \u003c/span\u003e\u003ca href=\"https://amzn.to/4fqvn0D\" rel=\"\"\u003eBuild a Large Language Model (From Scratch) book\u003c/a\u003e\u003cspan\u003e. (I am confident that you\u0026#39;ll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)\u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cspan\u003eIf you read the book and have a few minutes to spare, I\u0026#39;d really appreciate a \u003c/span\u003e\u003ca href=\"https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167\" rel=\"\"\u003ebrief review\u003c/a\u003e\u003cspan\u003e. It helps us authors a lot!\u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eYour support means a great deal! Thank you!\u003c/strong\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e\u003cdiv id=\"discussion\"\u003e\u003ch4\u003eDiscussion about this post\u003c/h4\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "29 min read",
  "publishedTime": "2025-02-05T12:11:39Z",
  "modifiedTime": null
}
