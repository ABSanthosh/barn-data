{
  "id": "0b0797bc-4b49-4cfb-8a0d-c6cde3cdee81",
  "title": "Compiling LLMs into a MegaKernel: A path to low-latency inference",
  "link": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17",
  "description": "Comments",
  "author": "",
  "published": "Thu, 19 Jun 2025 19:20:54 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Zhihao Jia",
  "length": 12026,
  "excerpt": "TL;DR: We developed a compiler that automatically transforms LLM inference into a single megakernel — a fused GPU kernel that performs all necessary computation and communication in one launch. This…",
  "siteName": "Medium",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "TL;DR: We developed a compiler that automatically transforms LLM inference into a single megakernel — a fused GPU kernel that performs all necessary computation and communication in one launch. This end-to-end GPU fusion approach reduces LLM inference latency by 1.2-6.7x. Our compiler is easy to use — you can compile your LLM into a high-performance megakernel with just a few dozen lines of Python.What’s the key idea? Traditional LLM systems often rely on sequences of GPU kernel launches and external communication calls, resulting in underutilized hardware. Our compiler automatically fuses these operations — spanning multiple layers, iterations, and GPUs — into a megakernel. This design eliminates launch overhead, enables fine-grained software pipelining, and overlaps computation with communication across GPUs.Team members: Xinhao Cheng, Bohan Hou, Yingyi Huang, Jianan Ji, Jinchen Jiang, Hongyi Jin, Ruihang Lai, Shengjie Lin, Xupeng Miao, Gabriele Oliaro, Zihao Ye, Zhihao Zhang, Yilong Zhao, Tianqi Chen, Zhihao JiaSoftware: https://github.com/mirage-project/mirage/tree/mpkOne of the most effective ways to reduce latency in LLM inference is to fuse all computation and communication into a single megakernel — also known as a persistent kernel. In this design, the system launches just one GPU kernel to execute the entire model — from layer-by-layer computation to inter-GPU communication — without interruption. This approach offers several key performance advantages:Eliminates kernel launch overhead, even in multi-GPU settings, by avoiding repeated kernel invocations;Enables software pipelining across layers, allowing the kernel to begin loading data for the next layer while computing the current one;Overlaps computation and communication, as a megakernel can simultaneously execute compute operations and inter-GPU communication to hide latency.Despite these advantages, compiling an LLM into a megakernel is highly challenging. Existing high-level ML frameworks — such as PyTorch, Triton, and TVM — do not natively support end-to-end megakernel generation. Additionally, modern LLM systems are built from a diverse collection of specialized kernel libraries: NCCL or NVSHMEM for communication, FlashInfer or FlashAttention for efficient attention, and CUDA or Triton for custom computation. This fragmentation makes it difficult to consolidate the entire inference pipeline into a single, unified kernel.Can we automate this process through compilation? Motivated by this question, our team from CMU, UW, Berkeley, NVIDIA, and Tsinghua developed Mirage Persistent Kernel (MPK) — a compiler and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers.Why MPK?A key advantage of MPK is extremely low latency for LLM inference by eliminating kernel launch overhead and maximally overlapping computation, data loading, and inter-GPU communication across layers.Figure 1. Comparing LLM decoding latency between MPK and existing systems. We used a 39-token prompt and generated 512 tokens without speculative decoding.Figure 1 illustrates a performance comparison between MPK and existing LLM inference systems on both single- and multi-GPU configurations. On a single NVIDIA A100 40GB GPU, MPK reduces per-token decoding latency from 14.5 ms — as achieved by optimized systems like vLLM and SGLang — to 12.5 ms, approaching the theoretical lower bound of 10 ms (based on loading 16 GB of weights with 1.6 TB/s memory bandwidth).Beyond single-GPU optimization, MPK fuses computation and inter-GPU communication into a single megakernel. This design enables MPK to maximally overlap computation and communication. As a result, the performance improvements of MPK over current systems increase with the number of GPUs, making it particularly effective for multi-GPU deployments.What’s Next?The rest of this blog dives deeper into how MPK works:Part 1 introduces the MPK compiler, which transforms an LLM’s computation graph into an optimized task graph;Part 2 covers the MPK runtime, which executes this task graph within a megakernel to achieve high throughput and low latency.Part 1. The Compiler: Transforming an LLM into a Fine-Grained Task GraphThe computation performed by a large language model (LLM) is typically represented as a computation graph, where each node corresponds to a compute operation (e.g., matrix multiplication, attention) or a collective communication primitive (e.g., all-reduce), and edges denote data dependencies between operations. In existing systems, each operator is generally executed via a dedicated GPU kernel. However, this kernel-per-operator execution model often fails to exploit pipelining opportunities, since dependencies are enforced at a coarse granularity — across entire kernels — rather than the actual data units.The computation of an LLM is generally represented as a computation graph, where each node is a compute operator (e.g., matrix multiplication, attention) or a collective communication primitive (e.g., allreduce), and edges denote data dependencies between operators. Existing systems generally launch a dedicated GPU kernel for each operator. However, this kernel-per-operator approach often fails to exploit pipelining opportunities, since dependencies are enforced at a coarse granularity — across entire kernels — rather than the actual data units.Consider a typical example: an allreduce operation following a matrix multiplication. In existing kernel-per-operator systems, the allreduce kernel must wait until the entire matmul kernel completes. In reality, though, each chunk of data for the allreduce only depends on a portion of the matmul output. This mismatch between logical and actual data dependencies limits the potential for overlapping computation and communication.Figure 2. The MPK compiler transforms an LLM’s computation graph (defined in PyTorch) into an optimized, fine-grained task graph that exposes maximum parallelism. The right-hand side illustrates an alternative — but suboptimal — task graph that introduces unnecessary data dependencies and global barriers, limiting pipelining opportunities across layers.To address this issue, MPK introduces a compiler that automatically transforms the LLM’s computation graph into a fine-grained task graph. This task graph explicitly captures dependencies at the sub-kernel level, enabling more aggressive pipelining across layers.In an MPK task graph:Each task (shown as a rectangle in Figure 2) represents a unit of computation or communication assigned to a single GPU streaming multiprocessor (SM).Each event (shown as a circle) represents a synchronization point between tasks.Each task has an outgoing edge to a triggering event, which is activated once all associated tasks complete.Each tasks also has an incoming edge from a dependent event, indicating the task can start execution as soon as the event is activated.Task graphs allow MPK to uncover pipelining opportunities that would be missed in computation graphs. For example, MPK can construct an optimized task graph where each allreduce task depends only on the corresponding matmul task that produces its input — enabling partial execution and overlap.In addition to generating an optimized task graph, MPK also automatically generates high-performance CUDA implementations for each task using the Mirage kernel superoptimizer. This ensures that each task runs efficiently on a GPU SM. (For more about the kernel superoptimizer, see this post.)Part 2. The Runtime: Executing a Task Graph in a MegaKernelMPK includes an on-GPU runtime system that executes the task graph entirely within a single GPU megakernel, allowing for fine-grained control over task execution and scheduling without any kernel launches during inference.To achieve this, MPK statically partitions all streaming multiprocessors (SMs) on a GPU into two roles: workers and schedulers. The number of worker and scheduler SMs is fixed at kernel launch time and matches the total number of physical SMs, avoiding any dynamic context switching overhead.WorkersEach worker operates on an SM and maintains a dedicated task queue. It follows a simple but efficient execution loop:Fetch the next task from its queue.Execute the task (e.g., matrix multiplication, attention, or inter-GPU data transfers).Notify the triggering event upon task completion.Repeat.This design ensures that workers remain fully utilized while enabling task execution to proceed asynchronously across layers and operations.SchedulersScheduling decisions are handled by MPK’s distributed schedulers, each of which runs on a single warp. Because each SM can accommodate multiple warps, up to four schedulers can run concurrently per SM. Each scheduler maintains a queue of activated events. It continuously:Dequeues activated events whose dependencies are satisfied (i.e., all prerequisite tasks have completed).Launches the set of tasks that depend on the activated event.This decentralized scheduling mechanism minimizes coordination overhead while enabling scalable execution across SMs.Figure 3. The MPK runtime executes a task graph in a megakernel.Event-Driven ExecutionFigure 3 illustrates MPK’s execution timeline. Each rectangle represents a task running on a worker; each circle represents an event. As a task completes, it increments the counter for its corresponding triggering event. When the event counter reaches a pre-defined threshold, the event is considered activated and is enqueued into a scheduler’s event queue. The scheduler then launches any downstream tasks that depend on this event.This design allows for fine-grained software pipelining and overlap between computation and communication. For example:Matmul tasks can execute in parallel with attention tasks from different layers.Allreduce communication can begin as soon as partial matmul results are available.Because all scheduling and task transitions occur within a single kernel context, the overhead between tasks is extremely low — typically just 1–2 microseconds — enabling efficient execution of multi-layer, multi-GPU LLM workloads.Looking ForwardOur vision for MPK is to make megakernel compilation both easy to use and highly performant. Currently you can compile an LLM into a megakernel with just a few dozen lines of Python code — mainly to specify the megakernel’s inputs and outputs. We’re excited about this direction, and there’s still much more to explore. Some of the key areas we’re actively working on include:Support for modern GPU architectures. One of our next milestones is extending MPK to support next-generation architectures such as NVIDIA Blackwell. A major challenge lies in integrating warp specialization — a key optimization for newer GPUs — with MPK’s megakernel execution model.Handling workload dynamism. MPK currently builds a static task graph, which limits its ability to handle dynamic workloads such as mixture-of-experts (MoE) models. We’re developing new compilation strategies that allow MPK to support dynamic control flow and conditional execution inside megakernels.Advanced scheduling and task assignment: MPK unlocks a new level of fine-grained scheduling at the task level. While our current implementation uses simple round-robin scheduling to distribute tasks across SMs, we see exciting opportunities in advanced scheduling policies — such as priority-aware or throughput-optimized strategies — for use cases like latency-SLO-driven serving or hybrid batching.We believe MPK represents a foundational shift in how LLM inference workloads are compiled and executed on GPUs, and we’re eager to collaborate with the community to push this vision forward.Want to Learn More?To learn more about MPK and explore our code and documentation, please visit our project website: https://github.com/mirage-project/mirage.We welcome feedback, contributions, and collaborations from the community!",
  "image": "https://miro.medium.com/v2/resize:fit:841/1*xW52lnX0OeobpeA2_BvX1w.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv tabindex=\"-1\" aria-hidden=\"false\"\u003e\u003ca rel=\"noopener follow\" href=\"https://zhihaojia.medium.com/?source=post_page---byline--cf7840913c17---------------------------------------\" data-discover=\"true\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Zhihao Jia\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*R0UgBeMN9mRGFYbdrrwFgg.jpeg\" width=\"32\" height=\"32\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"2160\"\u003e\u003cstrong\u003eTL;DR:\u003c/strong\u003e We developed a compiler that automatically transforms LLM inference into a single megakernel — a fused GPU kernel that performs all necessary computation and communication in one launch. This end-to-end GPU fusion approach reduces LLM inference latency by 1.2-6.7x. Our compiler is easy to use — you can compile your LLM into a high-performance megakernel with just a few dozen lines of Python.\u003c/p\u003e\u003cp id=\"10a5\"\u003e\u003cstrong\u003eWhat’s the key idea? \u003c/strong\u003eTraditional LLM systems often rely on sequences of GPU kernel launches and external communication calls, resulting in underutilized hardware. Our compiler automatically fuses these operations — spanning multiple layers, iterations, and GPUs — into a megakernel. This design eliminates launch overhead, enables fine-grained software pipelining, and overlaps computation with communication across GPUs.\u003c/p\u003e\u003cp id=\"6354\"\u003e\u003cstrong\u003eTeam members\u003c/strong\u003e: \u003ca href=\"https://xinhaoc.github.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eXinhao Cheng\u003c/a\u003e, \u003ca href=\"https://spectrometerhbh.github.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBohan Hou\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/avery-huang-6585b4189/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eYingyi Huang\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/jiananji/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJianan Ji\u003c/a\u003e, \u003ca href=\"https://github.com/undefined-c0der\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJinchen Jiang\u003c/a\u003e, \u003ca href=\"https://github.com/jinhongyii\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHongyi Jin\u003c/a\u003e, \u003ca href=\"https://ruihanglai.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRuihang Lai\u003c/a\u003e, \u003ca href=\"https://github.com/linsj20\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eShengjie Lin\u003c/a\u003e, \u003ca href=\"https://hsword.github.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eXupeng Miao\u003c/a\u003e, \u003ca href=\"https://www.gabrieleoliaro.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGabriele Oliaro\u003c/a\u003e, \u003ca href=\"https://homes.cs.washington.edu/~zhye/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eZihao Ye\u003c/a\u003e, \u003ca href=\"https://jackfram.github.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eZhihao Zhang\u003c/a\u003e, \u003ca href=\"https://happierpig.github.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eYilong Zhao\u003c/a\u003e, \u003ca href=\"https://tqchen.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTianqi Chen\u003c/a\u003e, \u003ca href=\"https://www.cs.cmu.edu/~zhihaoj2/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eZhihao Jia\u003c/a\u003e\u003c/p\u003e\u003cp id=\"46cc\"\u003e\u003cstrong\u003eSoftware\u003c/strong\u003e: \u003ca href=\"https://github.com/mirage-project/mirage/tree/mpk\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://github.com/mirage-project/mirage/tree/mpk\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"875a\"\u003eOne of the most effective ways to reduce latency in LLM inference is to fuse all computation and communication into a single \u003cstrong\u003emegakernel\u003c/strong\u003e\u003cem\u003e — \u003c/em\u003ealso known as a \u003cstrong\u003epersistent kernel\u003c/strong\u003e. In this design, the system launches just \u003cstrong\u003eone\u003c/strong\u003e GPU kernel to execute the entire model — from layer-by-layer computation to inter-GPU communication — without interruption. This approach offers several key performance advantages:\u003c/p\u003e\u003col\u003e\u003cli id=\"c1d5\"\u003e\u003cstrong\u003eEliminates kernel launch overhead\u003c/strong\u003e, even in multi-GPU settings,\u003cstrong\u003e \u003c/strong\u003eby avoiding repeated kernel invocations;\u003c/li\u003e\u003cli id=\"86ec\"\u003e\u003cstrong\u003eEnables software pipelining\u003c/strong\u003e \u003cstrong\u003eacross layers\u003c/strong\u003e, allowing the kernel to begin loading data for the next layer while computing the current one;\u003c/li\u003e\u003cli id=\"e675\"\u003e\u003cstrong\u003eOverlaps computation and communication\u003c/strong\u003e, as a megakernel can simultaneously execute compute operations and inter-GPU communication to hide latency.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"27b4\"\u003eDespite these advantages, compiling an LLM into a megakernel is highly challenging. Existing high-level ML frameworks — such as \u003ca href=\"https://pytorch.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePyTorch\u003c/a\u003e, \u003ca href=\"https://github.com/triton-lang/triton\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTriton\u003c/a\u003e, and \u003ca href=\"https://tvm.apache.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTVM\u003c/a\u003e — do not natively support end-to-end megakernel generation. Additionally, modern LLM systems are built from a diverse collection of specialized kernel libraries: \u003ca href=\"https://github.com/NVIDIA/nccl\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNCCL\u003c/a\u003e or \u003ca href=\"https://developer.nvidia.com/nvshmem\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNVSHMEM\u003c/a\u003e for communication, \u003ca href=\"https://github.com/flashinfer-ai/flashinfer\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFlashInfer\u003c/a\u003e or \u003ca href=\"https://github.com/Dao-AILab/flash-attention\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFlashAttention\u003c/a\u003e for efficient attention, and CUDA or \u003ca href=\"https://github.com/triton-lang/triton\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTriton\u003c/a\u003e for custom computation. This fragmentation makes it difficult to consolidate the entire inference pipeline into a single, unified kernel.\u003c/p\u003e\u003cp id=\"d94b\"\u003e\u003cstrong\u003eCan we automate this process through compilation?\u003c/strong\u003e Motivated by this question, our team from CMU, UW, Berkeley, NVIDIA, and Tsinghua developed \u003ca href=\"https://github.com/mirage-project/mirage\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003eMirage Persistent Kernel\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e (MPK) \u003c/strong\u003e— a compiler and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers.\u003c/p\u003e\u003ch2 id=\"7694\"\u003eWhy MPK?\u003c/h2\u003e\u003cp id=\"6a29\"\u003eA key advantage of MPK is extremely low latency for LLM inference by eliminating kernel launch overhead and maximally overlapping computation, data loading, and inter-GPU communication across layers.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 1. Comparing LLM decoding latency between MPK and existing systems. We used a 39-token prompt and generated 512 tokens without speculative decoding.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"fa3d\"\u003eFigure 1 illustrates a performance comparison between MPK and existing LLM inference systems on both single- and multi-GPU configurations. On a single NVIDIA A100 40GB GPU, MPK reduces per-token decoding latency from \u003cstrong\u003e14.5 ms\u003c/strong\u003e — as achieved by optimized systems like vLLM and SGLang — to \u003cstrong\u003e12.5 ms\u003c/strong\u003e, approaching the theoretical lower bound of \u003cstrong\u003e10 ms\u003c/strong\u003e (based on loading 16 GB of weights with 1.6 TB/s memory bandwidth).\u003c/p\u003e\u003cp id=\"7d3d\"\u003eBeyond single-GPU optimization, MPK fuses computation and inter-GPU communication into a single megakernel. This design enables MPK to maximally overlap computation and communication. As a result, the performance improvements of MPK over current systems \u003cstrong\u003eincrease with the number of GPUs\u003c/strong\u003e, making it particularly effective for multi-GPU deployments.\u003c/p\u003e\u003ch2 id=\"8502\"\u003eWhat’s Next?\u003c/h2\u003e\u003cp id=\"9785\"\u003eThe rest of this blog dives deeper into how MPK works:\u003c/p\u003e\u003cul\u003e\u003cli id=\"e145\"\u003e\u003cstrong\u003ePart 1\u003c/strong\u003e introduces the \u003cstrong\u003eMPK compiler\u003c/strong\u003e, which transforms an LLM’s computation graph into an optimized task graph;\u003c/li\u003e\u003cli id=\"e481\"\u003e\u003cstrong\u003ePart 2\u003c/strong\u003e covers the \u003cstrong\u003eMPK runtime\u003c/strong\u003e, which executes this task graph within a megakernel to achieve high throughput and low latency.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"23b0\"\u003ePart 1. The Compiler: Transforming an LLM into a Fine-Grained Task Graph\u003c/h2\u003e\u003cp id=\"8b68\"\u003eThe computation performed by a large language model (LLM) is typically represented as a \u003cstrong\u003ecomputation graph\u003c/strong\u003e, where each node corresponds to a compute operation (e.g., matrix multiplication, attention) or a collective communication primitive (e.g., all-reduce), and edges denote data dependencies between operations. In existing systems, each operator is generally executed via a dedicated GPU kernel. However, this \u003cstrong\u003ekernel-per-operator execution model\u003c/strong\u003e often fails to exploit pipelining opportunities, since dependencies are enforced at a coarse granularity — across entire kernels — rather than the actual data units.\u003c/p\u003e\u003cp id=\"390e\"\u003eThe computation of an LLM is generally represented as a computation graph, where each node is a compute operator (e.g., matrix multiplication, attention) or a collective communication primitive (e.g., allreduce), and edges denote data dependencies between operators. Existing systems generally launch a dedicated GPU kernel for each operator. However, this kernel-per-operator approach often fails to exploit pipelining opportunities, since dependencies are enforced at a coarse granularity — across entire kernels — rather than the actual data units.\u003c/p\u003e\u003cp id=\"e233\"\u003eConsider a typical example: an allreduce operation following a matrix multiplication. In existing kernel-per-operator systems, the allreduce kernel must wait until the entire matmul kernel completes. In reality, though, each chunk of data for the allreduce only depends on a portion of the matmul output. This mismatch between logical and actual data dependencies limits the potential for overlapping computation and communication.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 2. The MPK compiler transforms an LLM’s computation graph (defined in PyTorch) into an optimized, fine-grained task graph that exposes maximum parallelism. The right-hand side illustrates an alternative — but suboptimal — task graph that introduces unnecessary data dependencies and global barriers, limiting pipelining opportunities across layers.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"8df6\"\u003eTo address this issue, MPK introduces a compiler that automatically transforms the LLM’s computation graph into a fine-grained \u003cstrong\u003etask graph\u003c/strong\u003e. This task graph explicitly captures dependencies at the sub-kernel level, enabling more aggressive pipelining across layers.\u003c/p\u003e\u003cp id=\"f7fd\"\u003eIn an MPK task graph:\u003c/p\u003e\u003cul\u003e\u003cli id=\"6ebd\"\u003eEach \u003cstrong\u003etask\u003c/strong\u003e (shown as a rectangle in Figure 2) represents a unit of computation or communication assigned to a single GPU streaming multiprocessor (SM).\u003c/li\u003e\u003cli id=\"743a\"\u003eEach \u003cstrong\u003eevent\u003c/strong\u003e (shown as a circle) represents a synchronization point between tasks.\u003c/li\u003e\u003cli id=\"3f9f\"\u003eEach task has an outgoing edge to a \u003cem\u003etriggering event\u003c/em\u003e, which is activated once all associated tasks complete.\u003c/li\u003e\u003cli id=\"f6a3\"\u003eEach tasks also has an incoming edge from a \u003cem\u003edependent event\u003c/em\u003e, indicating the task can start execution as soon as the event is activated.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"03f0\"\u003eTask graphs allow MPK to uncover pipelining opportunities that would be missed in computation graphs. For example, MPK can construct an optimized task graph where each allreduce task depends only on the corresponding matmul task that produces its input — enabling partial execution and overlap.\u003c/p\u003e\u003cp id=\"2e61\"\u003eIn addition to generating an optimized task graph, MPK also automatically generates \u003cstrong\u003ehigh-performance CUDA implementations\u003c/strong\u003e for each task using the \u003ca href=\"https://github.com/mirage-project/mirage\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMirage kernel superoptimizer\u003c/a\u003e. This ensures that each task runs efficiently on a GPU SM. (For more about the kernel superoptimizer, see \u003ca rel=\"noopener\" href=\"https://zhihaojia.medium.com/generating-fast-gpu-kernels-without-programming-in-cuda-triton-3fdd4900d9bc\" data-discover=\"true\"\u003ethis post\u003c/a\u003e.)\u003c/p\u003e\u003ch2 id=\"0ca9\"\u003ePart 2. The Runtime: Executing a Task Graph in a MegaKernel\u003c/h2\u003e\u003cp id=\"2ca7\"\u003eMPK includes an on-GPU runtime system that \u003cstrong\u003eexecutes the task graph entirely within a single GPU megakernel\u003c/strong\u003e, allowing for fine-grained control over task execution and scheduling without any kernel launches during inference.\u003c/p\u003e\u003cp id=\"f7b7\"\u003eTo achieve this, MPK statically partitions all streaming multiprocessors (SMs) on a GPU into two roles: \u003cstrong\u003eworkers\u003c/strong\u003e and \u003cstrong\u003eschedulers\u003c/strong\u003e. The number of worker and scheduler SMs is fixed at kernel launch time and matches the total number of physical SMs, avoiding any dynamic context switching overhead.\u003c/p\u003e\u003ch2 id=\"2d6b\"\u003eWorkers\u003c/h2\u003e\u003cp id=\"03e1\"\u003eEach \u003cstrong\u003eworker\u003c/strong\u003e operates on an SM and maintains a dedicated task queue. It follows a simple but efficient execution loop:\u003c/p\u003e\u003col\u003e\u003cli id=\"02f3\"\u003eFetch the next task from its queue.\u003c/li\u003e\u003cli id=\"2086\"\u003eExecute the task (e.g., matrix multiplication, attention, or inter-GPU data transfers).\u003c/li\u003e\u003cli id=\"8c9c\"\u003eNotify the triggering event upon task completion.\u003c/li\u003e\u003cli id=\"4f74\"\u003eRepeat.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"d04d\"\u003eThis design ensures that workers remain fully utilized while enabling task execution to proceed asynchronously across layers and operations.\u003c/p\u003e\u003ch2 id=\"4a27\"\u003eSchedulers\u003c/h2\u003e\u003cp id=\"3ca6\"\u003eScheduling decisions are handled by MPK’s \u003cstrong\u003edistributed schedulers\u003c/strong\u003e, each of which runs on a \u003cstrong\u003esingle warp\u003c/strong\u003e. Because each SM can accommodate multiple warps, up to four schedulers can run concurrently per SM. Each scheduler maintains a queue of activated events. It continuously:\u003c/p\u003e\u003col\u003e\u003cli id=\"44c8\"\u003eDequeues activated events whose dependencies are satisfied (i.e., all prerequisite tasks have completed).\u003c/li\u003e\u003cli id=\"660c\"\u003eLaunches the set of tasks that depend on the activated event.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"73af\"\u003eThis decentralized scheduling mechanism minimizes coordination overhead while enabling scalable execution across SMs.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 3. The MPK runtime executes a task graph in a megakernel.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"f558\"\u003eEvent-Driven Execution\u003c/h2\u003e\u003cp id=\"e99f\"\u003eFigure 3 illustrates MPK’s execution timeline. Each rectangle represents a task running on a worker; each circle represents an event. As a task completes, it increments the counter for its corresponding triggering event. When the event counter reaches a pre-defined threshold, the event is considered activated and is enqueued into a scheduler’s event queue. The scheduler then launches any downstream tasks that depend on this event.\u003c/p\u003e\u003cp id=\"a7de\"\u003eThis design allows for \u003cstrong\u003efine-grained software pipelining\u003c/strong\u003e and \u003cstrong\u003eoverlap between computation and communication\u003c/strong\u003e. For example:\u003c/p\u003e\u003cul\u003e\u003cli id=\"8c54\"\u003eMatmul tasks can execute in parallel with attention tasks from different layers.\u003c/li\u003e\u003cli id=\"23bc\"\u003eAllreduce communication can begin as soon as partial matmul results are available.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"e020\"\u003eBecause all scheduling and task transitions occur within a single kernel context, \u003cstrong\u003ethe overhead between tasks is extremely low\u003c/strong\u003e — typically just \u003cstrong\u003e1–2 microseconds\u003c/strong\u003e — enabling efficient execution of multi-layer, multi-GPU LLM workloads.\u003c/p\u003e\u003ch2 id=\"c9c6\"\u003eLooking Forward\u003c/h2\u003e\u003cp id=\"4435\"\u003eOur vision for MPK is to make \u003cstrong\u003emegakernel compilation both easy to use and highly performant\u003c/strong\u003e. Currently you can compile an LLM into a megakernel with just a few dozen lines of Python code — mainly to specify the megakernel’s inputs and outputs. We’re excited about this direction, and there’s still much more to explore. Some of the key areas we’re actively working on include:\u003c/p\u003e\u003cul\u003e\u003cli id=\"6415\"\u003e\u003cstrong\u003eSupport for modern GPU architectures. \u003c/strong\u003eOne of our next milestones is extending MPK to support next-generation architectures such as \u003cstrong\u003eNVIDIA Blackwell\u003c/strong\u003e. A major challenge lies in integrating warp specialization — a key optimization for newer GPUs — with MPK’s megakernel execution model.\u003c/li\u003e\u003cli id=\"248f\"\u003e\u003cstrong\u003eHandling workload dynamism. \u003c/strong\u003eMPK currently builds a static task graph, which limits its ability to handle dynamic workloads such as \u003cstrong\u003emixture-of-experts (MoE)\u003c/strong\u003e models. We’re developing new compilation strategies that allow MPK to support dynamic control flow and conditional execution inside megakernels.\u003c/li\u003e\u003cli id=\"7752\"\u003e\u003cstrong\u003eAdvanced scheduling and task assignment:\u003c/strong\u003e MPK unlocks a new level of \u003cstrong\u003efine-grained scheduling\u003c/strong\u003e at the task level. While our current implementation uses simple round-robin scheduling to distribute tasks across SMs, we see exciting opportunities in \u003cstrong\u003eadvanced scheduling policies\u003c/strong\u003e — such as priority-aware or throughput-optimized strategies — for use cases like latency-SLO-driven serving or hybrid batching.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"e507\"\u003eWe believe MPK represents a foundational shift in how LLM inference workloads are compiled and executed on GPUs, and we’re eager to collaborate with the community to push this vision forward.\u003c/p\u003e\u003ch2 id=\"9c4d\"\u003eWant to Learn More?\u003c/h2\u003e\u003cp id=\"5561\"\u003eTo learn more about MPK and explore our code and documentation, please visit our project website: \u003ca href=\"https://github.com/mirage-project/mirage\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://github.com/mirage-project/mirage\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"1a48\"\u003eWe welcome feedback, contributions, and collaborations from the community!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2025-06-19T03:22:47.828Z",
  "modifiedTime": null
}
