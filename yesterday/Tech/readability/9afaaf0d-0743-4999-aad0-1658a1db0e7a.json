{
  "id": "9afaaf0d-0743-4999-aad0-1658a1db0e7a",
  "title": "Hear a podcast discussion about Gemini’s multimodal capabilities.",
  "link": "https://blog.google/products/gemini/release-notes-podcast-gemini-multimodal/",
  "description": "The latest episode of the Google AI: Release Notes podcast focuses on how Gemini was built from the ground up as a multimodal model — meaning a model that works with tex…",
  "author": "",
  "published": "Mon, 07 Jul 2025 11:44:00 +0000",
  "source": "https://www.blog.google/rss/",
  "categories": [
    "Gemini Models"
  ],
  "byline": "",
  "length": 661,
  "excerpt": "The latest episode of the Google AI: Release Notes podcast focuses on how Gemini was built from the ground up as a multimodal model — meaning a model that works with text, images, video and documents.Host Logan Kilpatrick chats with Anirudh Baddepudi, the product lead for Gemini's multimodal vision capabilities. They discuss how Gemini understands and reasons about images, video and documents, the future of product experiences when \"everything is vision\" and how these capabilities are creating new ways for developers and users to use Gemini.Watch the full conversation below, or listen to the Google AI: Release Notes podcast on Apple Podcasts or Spotify.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png?version=pr20250625-1614",
  "text": "The latest episode of the Google AI: Release Notes podcast focuses on how Gemini was built from the ground up as a multimodal model — meaning a model that works with text, images, video and documents.Host Logan Kilpatrick chats with Anirudh Baddepudi, the product lead for Gemini's multimodal vision capabilities. They discuss how Gemini understands and reasons about images, video and documents, the future of product experiences when \"everything is vision\" and how these capabilities are creating new ways for developers and users to use Gemini.Watch the full conversation below, or listen to the Google AI: Release Notes podcast on Apple Podcasts or Spotify.",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Release_Notes_Podcast_Images_and_YouTube_Th.max-1440x810.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv slot=\"uni-short-post-description-slot\"\u003e\u003cp data-block-key=\"plvll\"\u003eThe latest episode of the Google AI: Release Notes podcast focuses on how Gemini was built from the ground up as a multimodal model — meaning a model that works with text, images, video and documents.\u003c/p\u003e\u003cp data-block-key=\"be1qh\"\u003eHost Logan Kilpatrick chats with Anirudh Baddepudi, the product lead for Gemini\u0026#39;s multimodal vision capabilities. They discuss how Gemini understands and reasons about images, video and documents, the future of product experiences when \u0026#34;everything is vision\u0026#34; and how these capabilities are creating new ways for developers and users to use Gemini.\u003c/p\u003e\u003cp data-block-key=\"5i2b2\"\u003eWatch the full conversation below, or listen to the Google AI: Release Notes podcast on \u003ca href=\"https://goo.gle/3Bm7QzQ\"\u003eApple Podcasts\u003c/a\u003e or \u003ca href=\"https://goo.gle/3ZL3ADl\"\u003eSpotify\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "Less than 1 min",
  "publishedTime": "2025-07-07T11:44:00Z",
  "modifiedTime": null
}
