{
  "id": "6916c15a-fa77-42de-8e40-b1d5a7df94b0",
  "title": "Ask a techspert: What is inference?",
  "link": "https://blog.google/technology/ai/ask-a-techspert-what-is-inference/",
  "description": "Learn more about AI and inference from Google experts.",
  "author": "Molly McHugh-JohnsonContributorThe Keyword",
  "published": "Mon, 23 Jun 2025 17:30:00 +0000",
  "source": "https://www.blog.google/rss/",
  "categories": [
    "Gemini Models",
    "Google Cloud",
    "AI"
  ],
  "byline": "Molly McHugh-Johnson",
  "length": 6596,
  "excerpt": "Learn more about AI and inference from Google experts.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png?version=pr20250611-1635",
  "text": "In April, we introduced Ironwood, our seventh-generation Tensor Processing Unit (TPU) designed to power the age of generative AI inference. TPUs — which are chips that power AI systems — are not new to Google, but this latest generation is different: It’s meant to take AI systems beyond being responsive and help them be proactive instead. And it will accomplish this thanks to inference, which is the process that allows AI systems to use models to make knowledge-based outputs. To better understand this next era of AI computing, I asked senior product manager Niranjan Hira and distinguished engineer Fenghui Zhang to give me a crash course in inference.I know what the word inference means — that, based on the information you’re given, you can come to some sort of conclusion. Is that in any way what it means when we’re talking about AI?Niranjan: Kind of, yes. It’s an oversimplification, but I think it's easiest to understand inference as pattern matching. In the broadest sense when we’re talking about generative AI and inference, what we’re asking is: Can AI models match patterns to predict what you want? For example, if I said “peanut butter and ____” and asked an American audience to fill in the blank, they’d probably say “jelly.” That's a good example of inference for speech patterns, and that’s something that AI inference can do, but it goes way beyond that.Fenghui: Inference in general is the way we actually use the model to do something useful. First, we have to train the model: An AI model will contain the model parameters, model architecture and configuration, which is the code that it needs to execute tasks — and these things combine to carry out the functionality. So inference is what allows us to actually take all that and use it.What kinds of AI models use inference?Fenghui: Deep learning AI like language models, image generation models and audio models all use inference because they’re making predictions for what’s going to “happen” based on what they’ve learned from past data patterns.Niranjan: Recommendation models use inference, too.What’s an example of a recommendation model?Fenghui: Most ads models are recommendation models, and the model that recommends YouTube videos to you. These are “traditional” (sometimes called “classical”) AI — not generative AI such as LLMs and image or video generation models — that have been using inference for ages.So inference isn’t new to AI, it’s just gotten better as AI has gotten more capable?Fenghui: Yes. And inference isn’t just what allows AI models to predict. It’s what allows them to classify, too. The model can label things based on how it’s learned. Here’s a famous example: Many years ago, we gave an AI model a picture and asked if it could identify a cat in the image. Using data — and inference — it was able to teach itself what a cat is and what it looks like and correctly identify the cat.I remember that!Fenghui: That was an example of a model using inference.Niranjan: More recently, do you remember a couple of years ago when people were talking about AI-created images that basically just ignored the laws of physics? People’s hands, for example, were often not depicted correctly. Models today do a much better job of that. They’re better at physics and texture, among other things. And the same thing goes for text translation. For instance: Language translation used to be statistical. It was usable but it wasn’t exactly right, and it certainly wasn’t conversational. But statistical translation led us to generative AI translation, which, today, lots of people feel comfortable using, even in their customer-facing products. We’re still using the process called inference, but the underlying AI and our computation capacity have improved dramatically. Can you measure how well inference works?Fenghui: We can when we measure how well a model performs at certain tasks. We also use inference to evaluate and train models to make them better. So while we train the model, we keep running inference to try to improve the model quality simultaneously.And because of training setups like this, you’re seeing inference levels get better and better by industry benchmarks, I would think.Niranjan: Yes. But there's also the question of human perception — how much have we all noticed these things getting better? And in general, it’s quite a lot. Something else we really care about at Google when we work on inference is privacy: We are careful about what we need to store for these experiences to work. What are some examples of Google AI where we can see improved inference?Fenghui: One of the best inference use cases we have at Google is AI Overviews. You type a query into Search and a very complex system farms it out to a bunch of models to try and get results back. It’s using inference to understand your query and to know what answer you want, and in the end, it summarizes what it learns into something very useful. Inference is also critical to a lot of the agentic work we’re doing. With agents, in addition to asking an AI model to deliver information based on its inference, you can make it do things for you. This is sort of an extension of inference as we formerly understood it.So inference is getting better at using data, or knowledge, to offer answers and even take action. How else is it changing?Fenghui: Well, one thing that’s super important is the cost. We're trying to make inference as affordable as possible. Let's say we’re trying to make a smaller, more affordable version of Gemini available to people. We would work on the model’s inference to find ways to change the computation paradigm, or the code that makes up the model, without changing the semantics, or the principal task it’s supposed to do, in order to reduce the cost. It’s basically making a smaller, more efficient version of the model so more people can access its capabilities.How do we bring the cost down?Fenghui: One way is to optimize the hardware. That’s why we have Ironwood coming out this year. It’s optimized for inference because of its inference-first design: more compute power and memory, and optimization for certain numeric types. Software-wise, we’re improving our compilers and our frameworks. Over time, we want AI inference to be more efficient. We want better quality, but we want a smaller footprint that costs less to be helpful.",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/InferenceHero_v3.width-1300.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-reading-time=\"true\" data-component=\"uni-article-body\"\u003e\n\n            \n              \n\n\n\n\n\n\u003cuni-article-speakable page-title=\"Ask a techspert: What is inference?\" listen-to-article=\"Listen to article\" data-date-modified=\"2025-06-23T17:30:01.036451+00:00\" data-tracking-ids=\"G-HGNBTNCHCQ,G-6NKTLKV14N\" data-voice-list=\"en.ioh-pngnat:Cyan,en.usb-pngnat:Lime\" data-script-src=\"https://www.gstatic.com/readaloud/player/web/api/js/api.js\"\u003e\u003c/uni-article-speakable\u003e\n\n            \n\n            \n            \n\n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Ask a techspert: What is inference?\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"k6o0t\"\u003eIn April, we introduced Ironwood, our seventh-generation Tensor Processing Unit (TPU) designed to power the age of generative AI inference. \u003ca href=\"https://blog.google/technology/ai/difference-cpu-gpu-tpu-trillium/\"\u003eTPUs — which are chips that power AI systems — are not new to Google\u003c/a\u003e, but this latest generation is different: It’s meant to take AI systems beyond being responsive and help them be proactive instead. And it will accomplish this thanks to inference, which is the process that allows AI systems to use models to make knowledge-based outputs.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\n\u003cuni-related-content-tout title=\"Ironwood: The first Google TPU for the …\" cta=\"See more\" summary=\"We’re introducing Ironwood, our seventh-generation Tensor Processing Unit (TPU) designed to power the age of generative AI inference.\" hideimage=\"False\" eyebrow=\"Related Article\" image-alt-text=\"\" role=\"none\" fullurl=\"https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/\" pagetype=\"articlepage\" isarticlepage=\"\" data-ga4-related-article=\"{\n  \u0026#34;event\u0026#34;: \u0026#34;article_lead_click\u0026#34;,\n  \u0026#34;link_text\u0026#34;: \u0026#34;Ironwood: The first Google TPU for the age of inference\u0026#34;,\n  \u0026#34;link_type\u0026#34;: \u0026#34;internal\u0026#34;,\n  \u0026#34;full_url\u0026#34;: \u0026#34;https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/\u0026#34;,\n  \u0026#34;title\u0026#34;: \u0026#34;Ironwood: The first Google TPU for the age of inference\u0026#34;,\n  \u0026#34;author\u0026#34; : \u0026#34;Amin Vahdat\u0026#34;,\n  \u0026#34;slug\u0026#34;: \u0026#34;ironwood-tpu-age-of-inference\u0026#34;,\n  \u0026#34;position\u0026#34;: \u0026#34;1 of 1\u0026#34;,\n  \u0026#34;click_location\u0026#34;: \u0026#34;undefined\u0026#34;,\n  \u0026#34;primary_tag\u0026#34;: \u0026#34;Products - Google Cloud\u0026#34;,\n  \u0026#34;secondary_tags\u0026#34;: \u0026#34;AI\u0026#34;,\n  \u0026#34;published_date\u0026#34;: \u0026#34;2025-04-09|12:00\u0026#34;,\n  \u0026#34;hero_media_type\u0026#34;: \u0026#34;mp4\u0026#34;,\n  \u0026#34;days_since_published\u0026#34;: \u0026#34;75\u0026#34;,\n  \u0026#34;content_category\u0026#34;: \u0026#34;Announcement\u0026#34;,\n  \u0026#34;word_count\u0026#34;: \u0026#34;953\u0026#34;,\n  \u0026#34;has_audio\u0026#34;: \u0026#34;no\u0026#34;,\n  \u0026#34;has_video\u0026#34;: \u0026#34;yes\u0026#34;,\n  \u0026#34;has_image\u0026#34;: \u0026#34;yes\u0026#34;,\n  \u0026#34;has_carousel\u0026#34;: \u0026#34;no\u0026#34;\n}\"\u003e\n  \n    \u003cdiv slot=\"rct-image-slot\"\u003e\n      \n      \n        \n    \u003cfigure\u003e\n        \u003cpicture\u003e\n            \n\n\n    \n\n    \n        \u003csource media=\"(max-resolution: 1.5dppx)\" sizes=\"300px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Social-Share_1920x1080.width-300.format-webp.webp 300w\"/\u003e\n    \n        \u003csource media=\"(min-resolution: 1.5dppx)\" sizes=\"600px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Social-Share_1920x1080.width-600.format-webp.webp 600w\"/\u003e\n    \n\n    \u003cimg src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Social-Share_1920x1080.width-600.format-webp.webp\" alt=\"TPUv7_Social-Share_1920x1080\" sizes=\" 300px,  600px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Social-Share_1920x1080.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Social-Share_1920x1080.width-600.format-webp.webp 600w\" data-target=\"image\" loading=\"lazy\"/\u003e\n    \n\n\n        \u003c/picture\u003e\n    \u003c/figure\u003e\n\n\n      \n    \u003c/div\u003e\n  \n\u003c/uni-related-content-tout\u003e\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Ask a techspert: What is inference?\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"k6o0t\"\u003eTo better understand this next era of AI computing, I asked senior product manager Niranjan Hira and distinguished engineer Fenghui Zhang to give me a crash course in inference.\u003c/p\u003e\u003cp data-block-key=\"745pm\"\u003e\u003cb\u003eI know what the word inference means — that, based on the information you’re given, you can come to some sort of conclusion. Is that in any way what it means when we’re talking about AI?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"4r3mc\"\u003e\u003cb\u003eNiranjan:\u003c/b\u003e Kind of, yes. It’s an oversimplification, but I think it\u0026#39;s easiest to understand inference as pattern matching. In the broadest sense when we’re talking about generative AI and inference, what we’re asking is: Can AI models match patterns to predict what you want? For example, if I said “peanut butter and ____” and asked an American audience to fill in the blank, they’d probably say “jelly.” That\u0026#39;s a good example of inference for speech patterns, and that’s something that AI inference can do, but it goes way beyond that.\u003c/p\u003e\u003cp data-block-key=\"7ifv0\"\u003e\u003cb\u003eFenghui:\u003c/b\u003e Inference in general is the way we actually use the model to do something useful. First, we have to train the model: An AI model will contain the model parameters, model architecture and configuration, which is the code that it needs to execute tasks — and these things combine to carry out the functionality. So inference is what allows us to actually take all that and use it.\u003c/p\u003e\u003cp data-block-key=\"8jip7\"\u003e\u003cb\u003eWhat kinds of AI models use inference?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"emuv4\"\u003e\u003cb\u003eFenghui\u003c/b\u003e: Deep learning AI like language models, image generation models and audio models all use inference because they’re making predictions for what’s going to “happen” based on what they’ve learned from past data patterns.\u003c/p\u003e\u003cp data-block-key=\"ces6f\"\u003e\u003cb\u003eNiranjan:\u003c/b\u003e Recommendation models use inference, too.\u003c/p\u003e\u003cp data-block-key=\"2nprt\"\u003e\u003cb\u003eWhat’s an example of a recommendation model?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"3ilbk\"\u003e\u003cb\u003eFenghui:\u003c/b\u003e Most ads models are recommendation models, and the model that recommends YouTube videos to you. These are “traditional” (sometimes called “classical”) AI — not generative AI such as LLMs and image or video generation models — that have been using inference for ages.\u003c/p\u003e\u003cp data-block-key=\"dn4gh\"\u003e\u003cb\u003eSo inference isn’t new to AI, it’s just gotten better as AI has gotten more capable?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"2a12h\"\u003e\u003cb\u003eFenghui\u003c/b\u003e: Yes. And inference isn’t just what allows AI models to predict. It’s what allows them to classify, too. The model can label things based on how it’s learned. Here’s a famous example: \u003ca href=\"https://blog.google/technology/ai/using-large-scale-brain-simulations-for/\"\u003eMany years ago\u003c/a\u003e, we gave an AI model a picture and asked if it could identify a cat in the image. Using data — and inference — it was able to teach itself what a cat is and what it looks like and correctly identify the cat.\u003c/p\u003e\u003cp data-block-key=\"9dvhm\"\u003e\u003cb\u003eI remember that!\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"61vul\"\u003e\u003cb\u003eFenghui\u003c/b\u003e: That was an example of a model using inference.\u003c/p\u003e\u003cp data-block-key=\"ibor\"\u003e\u003cb\u003eNiranjan:\u003c/b\u003e More recently, do you remember a couple of years ago when people were talking about AI-created images that basically just ignored the laws of physics? People’s hands, for example, were often not depicted correctly. Models today do a much better job of that. They’re better at physics and texture, among other things. And the same thing goes for text translation. For instance: Language translation used to be statistical. It was usable but it wasn’t exactly right, and it certainly wasn’t conversational. But statistical translation led us to generative AI translation, which, today, lots of people feel comfortable using, even in their customer-facing products. We’re still using the process called inference, but the underlying AI and our computation capacity have improved dramatically.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"An illustration of a hand placing a red puzzle piece, surrounded by other puzzle pieces and dashed lines.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"Ask a techspert: What is inference?\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"An illustration of a hand placing a red puzzle piece, surrounded by other puzzle pieces and dashed lines.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/InferenceInline.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/InferenceInline.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/InferenceInline.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Ask a techspert: What is inference?\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"k6o0t\"\u003e\u003cb\u003eCan you measure how well inference works?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"3svaq\"\u003e\u003cb\u003eFenghui\u003c/b\u003e: We can when we measure how well a model performs at certain tasks. We also use inference to evaluate and train models to make them better. So while we train the model, we keep running inference to try to improve the model quality simultaneously.\u003c/p\u003e\u003cp data-block-key=\"ft160\"\u003e\u003cb\u003eAnd because of training setups like this, you’re seeing inference levels get better and better by industry benchmarks, I would think.\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"8jn0g\"\u003e\u003cb\u003eNiranjan\u003c/b\u003e: Yes. But there\u0026#39;s also the question of human perception — how much have we all noticed these things getting better? And in general, it’s quite a lot. Something else we really care about at Google when we work on inference is privacy: We are careful about what we need to store for these experiences to work.\u003cbr/\u003e \u003cb\u003eWhat are some examples of Google AI where we can see improved inference?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"a5ran\"\u003e\u003cb\u003eFenghui\u003c/b\u003e: One of the best inference use cases we have at Google is \u003ca href=\"https://blog.google/products/search/ai-overview-expansion-may-2025-update/\"\u003eAI Overviews.\u003c/a\u003e You type a query into Search and a very complex system farms it out to a bunch of models to try and get results back. It’s using inference to understand your query and to know what answer you want, and in the end, it summarizes what it learns into something very useful. Inference is also critical to a lot of the \u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/#agentic-capabilities\"\u003eagentic work we’re doing\u003c/a\u003e. With agents, in addition to asking an AI model to deliver information based on its inference, you can make it do things for you. This is sort of an extension of inference as we formerly understood it.\u003cbr/\u003e\u003c/p\u003e\u003cp data-block-key=\"6gsmc\"\u003e\u003cb\u003eSo inference is getting better at using data, or knowledge, to offer answers and even take action. How else is it changing?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"1ss2r\"\u003e\u003cb\u003eFenghui\u003c/b\u003e: Well, one thing that’s super important is the cost. We\u0026#39;re trying to make inference as affordable as possible. Let\u0026#39;s say we’re trying to make a smaller, more affordable version of Gemini available to people. We would work on the model’s inference to find ways to change the computation paradigm, or the code that makes up the model, without changing the semantics, or the principal task it’s supposed to do, in order to reduce the cost. It’s basically making a smaller, more efficient version of the model so more people can access its capabilities.\u003c/p\u003e\u003cp data-block-key=\"aaim\"\u003e\u003cb\u003eHow do we bring the cost down?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"f6nvd\"\u003e\u003cb\u003eFenghui\u003c/b\u003e: One way is to optimize the hardware. That’s why we have Ironwood coming out this year. It’s optimized for inference because of its inference-first design: more compute power and memory, and optimization for certain numeric types. Software-wise, we’re improving our compilers and our frameworks. Over time, we want AI inference to be more efficient. We want better quality, but we want a smaller footprint that costs less to be helpful.\u003c/p\u003e\u003c/div\u003e\n  \n\n\n            \n            \n\n            \n              \n\n\n\n\n            \n          \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-06-23T17:30:00Z",
  "modifiedTime": null
}
