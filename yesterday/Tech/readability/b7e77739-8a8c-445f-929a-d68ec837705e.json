{
  "id": "b7e77739-8a8c-445f-929a-d68ec837705e",
  "title": "Bolt3D: Generating 3D Scenes in Seconds",
  "link": "https://szymanowiczs.github.io/bolt3d",
  "description": "Comments",
  "author": "",
  "published": "Wed, 19 Mar 2025 22:30:56 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 6214,
  "excerpt": "¹Google Research ²VGG, University of Oxford ³Google DeepMind",
  "siteName": "",
  "favicon": "",
  "text": "¹Google Research      ²VGG, University of Oxford       ³Google DeepMind TL;DR: Feed-forward 3D scene generation in 6.25s on a single GPU. How it works Given one or more input images, we generate multi-view Splatter Images. To do so, we first generate the scene appearance and geometry using a multi-view diffusion model. Then, Splatter Images are regressed using a Gaussian Head. 3D Gaussians from multiple Splatter Images are combined to form the 3D scene. An animated diagram briefly describing the method. On the left, an input image is shown. Next are the rotating Splatter Images. On the right is the full 3D scene. Interactive Viewer Click on the images below to render 3D scenes in real-time in your browser. Result Gallery Variable number of input views Bolt3D can accept variable number of input images. Our model adheres to conditioning when it is available and generates unobserved scene regions without any reprojection or inpainting mechanisms. 1 input view 1-view reconstruction 2 input views 2-view reconstruction Geometry VAE The key to generating high-quality 3D scenes with a latent diffusion model is our Geometry VAE, capable of compressing pointmaps with high accuracy. We find empirically that our VAE with a transformer decoder is more appropriate for autoencoding pointmaps than a VAE with a convolutional decoder or a VAE pre-trained for autoencoding images. Below we visualize colored point clouds using (1) Pointmaps from data, (2) Pointmaps autoencoded with our VAE, (3) Pointmaps autoencoded with a VAE with a convolutional decoder and (4) Pointmaps autoencoded with a pre-trained Image VAE. Data Our AE Conv. AE Image AE Comparison to other methods Compare the renders of our method Bolt3D (right) with feed-forward and optimization-based methods (left). Our method gives feed-forward 3D reconstruction models generative capabilities, and significantly reduces inference cost compared to optimization-based methods. Try selecting different methods and scenes! Acknowledgements We would like to express our deepest gratitude to Ben Poole for helpful suggestions, guidance, and contributions. We also thank George Kopanas, Sander Dieleman, Matthew Burruss, Matthew Levine, Peter Hedman, Songyou Peng, Rundi Wu, Alex Trevithick, Daniel Duckworth, Hadi Alzayer, David Charatan, Jiapeng Tang and Akshay Krishnan for valuable discussions and insights. Finally, we extend our gratitude to Shlomi Fruchter, Kevin Murphy, Mohammad Babaeizadeh, Han Zhang and Amir Hertz for training the base text-to-image latent diffusion model. Website template is borrowed from CAT3D and CAT4D. BibTeX @article{szymanowicz2025bolt3d, title={{Bolt3D: Generating 3D Scenes in Seconds}}, author={Szymanowicz, Stanislaw and Zhang, Jason Y. and Srinivasan, Pratul and Gao, Ruiqi and Brussee, Arthur and Holynski, Aleksander and Martin-Brualla, Ricardo and Barron, Jonathan T. and Henzler, Philipp}, journal={arXiv:2503.14445}, year={2025} }",
  "image": "https://szymanowiczs.github.io/bolt3d/img/rep_img.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n        \u003cheader role=\"banner\"\u003e\n            \n\n            \u003cdiv\u003e\n                \n                \n\n                \u003cp\u003e\n                    ¹Google Research      ²VGG, University of Oxford\n                          ³Google DeepMind\n                \u003c/p\u003e\n            \u003c/div\u003e\n\n            \n        \u003c/header\u003e\n\n\n        \u003cmain role=\"main\"\u003e\n\n            \n\n\n            \u003cdiv\u003e\n                \u003cp\u003e\n                    \u003ch6\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e: Feed-forward 3D scene\n                        generation in 6.25s on a single GPU.\u003c/h6\u003e\n                \u003c/p\u003e\n            \u003c/div\u003e\u003cdiv\u003e\n                    \u003ch3\u003e How it works \u003c/h3\u003e\n                    \u003cp\u003e\n                        Given one or more input images, we generate multi-view Splatter Images.\n                        To do so, we first generate the scene appearance and geometry using a multi-view diffusion\n                        model.\n                        Then, Splatter Images are regressed using a Gaussian Head.\n                        3D Gaussians from multiple Splatter Images are combined to form the 3D scene.\n                    \u003c/p\u003e\n                    \u003cvideo muted=\"\" loop=\"\" autoplay=\"\" src=\"https://szymanowiczs.github.io/bolt3d/videos/howitworks_website.m4v\" width=\"65%\"\u003e\n                        An animated diagram briefly describing the method. On the left, an input image is shown. Next\n                        are the rotating Splatter Images. On the right is the full 3D scene.\n                    \u003c/video\u003e\n                \u003c/div\u003e\u003cdiv id=\"viewer\"\u003e\n                        \u003cbr/\u003e\n\n                        \n\n                        \u003cdiv id=\"viewer-content\"\u003e\n                            \u003ch2\u003e\n                                Interactive Viewer\u003cbr/\u003e\n                            \u003c/h2\u003e\n                            \u003ccanvas id=\"brush_canvas\"\u003e\u003c/canvas\u003e\n\n\n                            \u003cp\u003e\n                                Click on the images below to render 3D scenes in real-time in your browser.\n                            \u003c/p\u003e\n\n                            \n                        \u003c/div\u003e\n\n                        \u003cvideo id=\"viewer-fallback-video\" loop=\"\" playsinline=\"\" autoplay=\"\" muted=\"\"\u003e\u003c/video\u003e\n                        \u003cbr/\u003e\n                    \u003c/div\u003e\n\n            \u003cp\u003e\n                \u003ch2\u003e\n                    Result Gallery\u003cbr/\u003e\n                \u003c/h2\u003e\n            \u003c/p\u003e\n\n            \n\n            \u003cbr/\u003e\n            \u003cdiv id=\"variable_input_views\"\u003e\n                        \u003ch3\u003e Variable number of input views\n                        \u003c/h3\u003e\n                        \u003cp\u003e\n                            Bolt3D can accept variable number of input images.\n                            Our model adheres to conditioning when it is available and generates unobserved\n                            scene regions without any reprojection or inpainting mechanisms.\n                        \u003c/p\u003e\n\n                        \u003ctable\u003e\n                            \u003ctbody\u003e\u003ctr\u003e\n                                \u003cth\u003e1 input view\u003c/th\u003e\n                                \u003cth\u003e1-view reconstruction\u003c/th\u003e\n                                \u003cth\u003e2 input views\u003c/th\u003e\n                                \u003cth\u003e2-view reconstruction\u003c/th\u003e\n                            \u003c/tr\u003e\n                            \u003ctr\u003e\n                                \u003ctd rowspan=\"1\" colspan=\"4\"\u003e\n                                    \u003cvideo id=\"variable_input_video\" width=\"100%\" loop=\"\" playsinline=\"\" autoplay=\"\" muted=\"\" controls=\"\"\u003e\n                                        \u003csource src=\"https://szymanowiczs.github.io/bolt3d/videos/single_vs_2_view.mp4\"/\u003e\n                                    \u003c/video\u003e\n                                \u003c/td\u003e\n                            \u003c/tr\u003e\n                        \u003c/tbody\u003e\u003c/table\u003e\n                        \u003cbr/\u003e\n                    \u003c/div\u003e\n            \u003cbr/\u003e\n\n            \u003cdiv id=\"geometry_vae-ours\"\u003e\n                        \u003ch3\u003e Geometry VAE\u003c/h3\u003e\n                        \u003cp\u003e\n                            The key to generating high-quality 3D scenes with a latent diffusion model is our Geometry\n                            VAE,\n                            capable of compressing pointmaps with high accuracy.\n                            We find empirically that our VAE with a transformer decoder is more appropriate for\n                            autoencoding pointmaps\n                            than a VAE with a convolutional decoder or a VAE pre-trained for autoencoding images.\n                            Below we visualize colored point clouds using (1) Pointmaps from data, (2) Pointmaps\n                            autoencoded with our VAE,\n                            (3) Pointmaps autoencoded with a VAE with a convolutional decoder and (4) Pointmaps\n                            autoencoded with a pre-trained Image VAE.\n                        \u003c/p\u003e\n\n                        \u003ctable\u003e\n                            \u003ctbody\u003e\u003ctr\u003e\n                                \u003cth\u003eData\u003c/th\u003e\n                                \u003cth\u003eOur AE\u003c/th\u003e\n                                \u003cth\u003eConv. AE\u003c/th\u003e\n                                \u003cth\u003eImage AE\u003c/th\u003e\n                            \u003c/tr\u003e\n                            \u003ctr\u003e\n                                \u003ctd rowspan=\"1\" colspan=\"4\"\u003e\n                                    \u003cvideo id=\"geometry_ours_video\" width=\"100%\" loop=\"\" playsinline=\"\" autoplay=\"\" muted=\"\" controls=\"\"\u003e\n                                        \u003csource src=\"https://szymanowiczs.github.io/bolt3d/videos/geometry_vae/render_053.m4v\"/\u003e\n                                    \u003c/video\u003e\n                                \u003c/td\u003e\n                            \u003c/tr\u003e\n                        \u003c/tbody\u003e\u003c/table\u003e\n                        \u003cbr/\u003e\n                    \u003c/div\u003e\n            \u003cdiv\u003e\n                    \u003ch3\u003e Comparison to other methods\u003c/h3\u003e\n                    \u003cp\u003e\n                        Compare the renders of our method Bolt3D (right) with feed-forward and optimization-based\n                        methods (left).\n                        Our method gives feed-forward 3D reconstruction models generative capabilities,\n                        and significantly reduces inference cost compared to optimization-based methods.\n                        Try selecting different methods and scenes!\n                    \u003c/p\u003e\n\n                    \n                \u003c/div\u003e\n            \u003cbr/\u003e\n            \u003cdiv\u003e\n                \u003ch4\u003e\n                    Acknowledgements\n                \u003c/h4\u003e\n\n                \u003cdiv\u003e\u003cp\u003e\n                    We would like to express our deepest gratitude to Ben Poole for helpful suggestions, guidance, and\n                    contributions.\n                    We also thank George Kopanas, Sander Dieleman, Matthew Burruss, Matthew Levine, Peter Hedman,\n                    Songyou Peng, Rundi Wu, Alex\n                    Trevithick, Daniel Duckworth, Hadi Alzayer, David Charatan,\n                    Jiapeng Tang and Akshay Krishnan for valuable discussions and insights.\n                    Finally, we extend our gratitude to Shlomi Fruchter, Kevin Murphy, Mohammad Babaeizadeh, Han Zhang\n                    and\n                    Amir Hertz for training the base text-to-image latent diffusion model.\n                    Website template is borrowed from \u003ca href=\"https://cat3d.github.io/\"\u003eCAT3D\u003c/a\u003e and \u003ca href=\"https://cat-4d.github.io/\"\u003eCAT4D\u003c/a\u003e.\n                    \u003c/p\u003e\u003c/div\u003e\n            \u003c/div\u003e\n\n            \u003cdiv id=\"BibTeX\"\u003e\n                    \u003ch2\u003eBibTeX\u003c/h2\u003e\n                    \u003cpre\u003e\u003ccode\u003e@article{szymanowicz2025bolt3d,\ntitle={{Bolt3D: Generating 3D Scenes in Seconds}},\nauthor={Szymanowicz, Stanislaw and Zhang, Jason Y. and Srinivasan, Pratul\n     and Gao, Ruiqi and Brussee, Arthur and Holynski, Aleksander and\n     Martin-Brualla, Ricardo and Barron, Jonathan T. and Henzler, Philipp},\njournal={arXiv:2503.14445},\nyear={2025}\n}\u003c/code\u003e\u003c/pre\u003e\n                \u003c/div\u003e\n\n            \u003c/main\u003e\n    \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": null,
  "modifiedTime": null
}
