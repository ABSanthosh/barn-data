{
  "id": "99bea224-7461-4487-9bd0-0def71a5cfe6",
  "title": "Google reveals quantum computing chip with ‘breakthrough’ achievements",
  "link": "https://www.theverge.com/2024/12/9/24317382/google-willow-quantum-computing-chip-breakthrough",
  "description": "",
  "author": "Emma Roth",
  "published": "2024-12-09T18:05:54-05:00",
  "source": "https://www.theverge.com/rss/index.xml",
  "categories": null,
  "byline": "Emma Roth",
  "length": 2247,
  "excerpt": "Google’s quantum computing lab achieved a major milestone with its new processor Willow, which reduces errors and has more powerful performance.",
  "siteName": "The Verge",
  "favicon": "https://www.theverge.com/icons/android_chrome_512x512.png",
  "text": "Google’s quantum computing lab just achieved a major milestone. On Monday, the company revealed that its new quantum computing chip, Willow, is capable of performing a computing challenge in less than five minutes — a process Google says would take one of the world’s fastest supercomputers 10 septillion years, or longer than the age of the universe.That’s a big jump from 2019 when Google announced its quantum processor could complete a mathematical equation in three minutes, as opposed to 10,000 years on a supercomputer. IBM disputed the claim at the time.Along with more powerful performance, researchers also found a way to reduce errors, something Google calls “one of the greatest challenges in quantum computing.” Instead of bits, which represent either 1 or 0, quantum computing uses qubits, a unit that can exist in multiple states at the same time, such as 1, 0, and anything in between.As noted by Google, qubits are prone to errors because they “have a tendency to rapidly exchange information with their environment.” However, Google’s researchers discovered a way to reduce errors by introducing more qubits to a system and were able to correct them in real time. Their findings were published in Nature.“This historic accomplishment is known in the field as ‘below threshold’ — being able to drive errors down while scaling up the number of qubits,” Google Quantum AI founder Hartmut Neven writes on Google’s blog. “You must demonstrate being below threshold to show real progress on error correction, and this has been an outstanding challenge since quantum error correction was introduced by Peter Shor in 1995.”Willow, which has 105 qubits, “now has best-in-class performance,” according to Neven. Microsoft, Amazon, and IBM are working on quantum computing systems of their own.Google’s next goal is to perform a first “useful, beyond-classical” computation that is both “relevant to a real-world application” and one that typical computers can’t achieve. Going forward, Neven says quantum technology will be “indispensable” for collecting AI training data, eventually helping to “discover new medicines, designing more efficient batteries for electric cars, and accelerating progress in fusion and new energy alternatives.”",
  "image": "https://cdn.vox-cdn.com/thumbor/WOGSUCVNOlwPL1f_OFB5QC44Oo0=/0x0:2544x1372/1200x628/filters:focal(1187x746:1188x747)/cdn.vox-cdn.com/uploads/chorus_asset/file/25782341/google_willow_chip.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eGoogle’s quantum computing lab just achieved a major milestone. On Monday,\u003ca href=\"https://blog.google/technology/research/google-willow-quantum-chip/\"\u003e the company revealed\u003c/a\u003e that its new quantum computing chip, Willow, is capable of performing a computing challenge in less than five minutes — a process Google says would take one of the world’s fastest supercomputers 10 septillion years, or longer than the age of the universe.\u003c/p\u003e\u003cp\u003eThat’s a big jump from 2019 when Google announced its \u003ca href=\"https://www.theverge.com/2019/10/23/20928294/google-quantum-supremacy-sycamore-computer-qubit-milestone\"\u003equantum processor could complete\u003c/a\u003e a mathematical equation in three minutes, as opposed to 10,000 years on a supercomputer. IBM \u003ca href=\"https://www.cbsnews.com/news/google-quantum-computing-quantum-supremacy-claim-by-google-disputed-by-ibm/\"\u003edisputed the claim\u003c/a\u003e at the time.\u003c/p\u003e\u003cp\u003eAlong with more powerful performance, researchers also found a way to reduce errors, something Google calls “one of the greatest challenges in quantum computing.” Instead of bits, which represent either 1 or 0, quantum computing uses qubits, \u003ca href=\"https://www.ibm.com/topics/qubit#:~:text=A%20classical%20bit%20can%20only%20exist%20in%20either%20a%200%20position%20or%20a%201%20position.%20Qubits%2C%20however%2C%20can%20also%20occupy%20a%20third%20state%20known%20as%20a%20superposition.%20A%20superposition%20represents%200%2C%201%2C%20and%20all%20the%20positions%20in%20between%20taken%20at%20once%2C%20for%20a%20total%20of%20three%20separate%20positions.\"\u003ea unit that can exist in multiple states\u003c/a\u003e at the same time, such as 1, 0, and anything in between.\u003c/p\u003e\u003cp\u003eAs noted by Google, qubits are prone to errors because they “have a tendency to rapidly exchange information with their environment.” However, Google’s researchers discovered a way to reduce errors by introducing more qubits to a system and were able to correct them in real time. Their findings were \u003ca href=\"https://www.nature.com/articles/s41586-024-08449-y\"\u003epublished in \u003cem\u003eNature\u003c/em\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e“This historic accomplishment is known in the field as ‘below threshold’ — being able to drive errors down while scaling up the number of qubits,” Google Quantum AI founder Hartmut Neven writes on Google’s blog. “You must demonstrate being below threshold to show real progress on error correction, and this has been an outstanding challenge since \u003ca href=\"https://journals.aps.org/pra/abstract/10.1103/PhysRevA.52.R2493\"\u003equantum error correction\u003c/a\u003e was introduced by Peter Shor in 1995.”\u003c/p\u003e\u003cp\u003eWillow, which has 105 qubits, “now has best-in-class performance,” according to Neven. \u003ca href=\"https://www.theverge.com/2024/4/8/24120103/microsoft-says-its-cracked-the-code-on-an-important-quantum-computing-problem\"\u003eMicrosoft\u003c/a\u003e, \u003ca href=\"https://www.theverge.com/2024/5/16/24158365/quantum-computing-network-aws-amazon-harvard\"\u003eAmazon\u003c/a\u003e, and \u003ca href=\"https://www.theverge.com/23988271/ibm-quantum-heron-system-two-jerry-chow-qubits\"\u003eIBM\u003c/a\u003e are working on quantum computing systems of their own.\u003c/p\u003e\u003cp\u003eGoogle’s next goal is to perform a first “useful, beyond-classical” computation that is both “relevant to a real-world application” and one that typical computers can’t achieve. Going forward, Neven says quantum technology will be “indispensable” for collecting AI training data, eventually helping to “discover new medicines, designing more efficient batteries for electric cars, and accelerating progress in fusion and new energy alternatives.”\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2024-12-09T23:05:54.316Z",
  "modifiedTime": "2024-12-09T23:05:54.316Z"
}
