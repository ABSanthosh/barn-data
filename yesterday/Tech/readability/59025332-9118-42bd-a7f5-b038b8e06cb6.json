{
  "id": "59025332-9118-42bd-a7f5-b038b8e06cb6",
  "title": "ChatGPT’s agent can now do deep research for you",
  "link": "https://www.theverge.com/news/604902/chagpt-deep-research-ai-agent",
  "description": "OpenAI has revealed another new agentic feature for ChatGPT called deep research, which it says can operate autonomously to “plan and execute a multi-step trajectory to find the data it needs, backtracking and reacting to real-time information where necessary.” Instead of simply generating text, it shows a summary of its process in a sidebar, with […]",
  "author": "Richard Lawler",
  "published": "2025-02-02T19:11:45-05:00",
  "source": "https://www.theverge.com/rss/index.xml",
  "categories": [
    "AI",
    "News",
    "OpenAI",
    "Tech"
  ],
  "byline": "Richard Lawler",
  "length": 2550,
  "excerpt": "More accurate, and more resource intensive.",
  "siteName": "The Verge",
  "favicon": "https://www.theverge.com/static-assets/icons/android-chrome-512x512.png",
  "text": "OpenAI has revealed another new agentic feature for ChatGPT called deep research, which it says can operate autonomously to “plan and execute a multi-step trajectory to find the data it needs, backtracking and reacting to real-time information where necessary.”Instead of simply generating text, it shows a summary of its process in a sidebar, with citations and a summary showing the process used for reference.Image: OpenAIUsers can ask questions using text, images, and additional files like PDFs or spreadsheets to add context, and then it will take “anywhere from 5 to 30 minutes” to develop a response provided in the chat window, with promises that in the future it will also be able to include embedded images and charts. OpenAI also notes limitations for deep research, saying it can “sometimes hallucinate” and make up facts, struggle with telling the difference between authoritative info and rumors, and register how certain it should rate a response.Developing ways for generative AI tools to be more useful and worth paying for is the future companies like OpenAI have promised for agents, and it claims that deep research is capable of operating at the level of a research analyst. The demo video included here begins with a request for info on changes in the retail industry over the last three years, with a response that includes bullet points and tables.This feature closely follows OpenAI’s launch of Operator, a tool that can use a web browser to complete tasks for you, and is similar to the Project Mariner research prototype Google showed off in December. Google’s tool is not available to the public yet, but deep research is launching “with a version optimized for Pro users today.”OpenAI is offering up to 100 queries per month for those paying the $200 monthly fee and “limited access” promised for Plus, Team, and eventually, Enterprise users, calling the ability “very compute intensive,” requiring more inference compute the longer it takes to research something. It also says that all paid users will get higher rate limits in the future when a faster, more cost-effective version is available.A press release says that the model powering deep research scored a new high for accuracy on an AI benchmark dubbed “Humanity’s Last Exam,” which asks for responses to expert-level questions. The OpenAI deep research model reached an accuracy of 26.6 percent with browsing and python tools enabled, well above GPT-4o’s 3.3 percent, and the next highest scorer, its o3-mini (high) model evaluated only on text, at 13 percent.",
  "image": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/STK155_OPEN_AI_CVirginia__A.jpg?quality=90\u0026strip=all\u0026crop=0%2C10.732984293194%2C100%2C78.534031413613\u0026w=1200",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"zephr-anchor\"\u003e\u003cp\u003eOpenAI \u003ca href=\"https://www.youtube.com/watch?v=jv-lpIsnLOo\"\u003ehas revealed\u003c/a\u003e another new agentic feature for ChatGPT called deep research, which it says can operate autonomously to “plan and execute a multi-step trajectory to find the data it needs, backtracking and reacting to real-time information where necessary.”\u003c/p\u003e\u003cp\u003eInstead of simply generating text, it shows a summary of its process in a sidebar, with citations and a summary showing the process used for reference.\u003c/p\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e\u003ca href=\"https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0,0,100,100\" data-pswp-height=\"2160\" data-pswp-width=\"3841\" target=\"_blank\" rel=\"noreferrer\"\u003e\u003cimg alt=\"OpenAI deep research sidebar on ChatGPT\" data-chromatic=\"ignore\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" sizes=\"(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px\" srcset=\"https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=2400 2400w\" src=\"https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/deep-research-sidebar.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=2400\"/\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003ccite\u003eImage: OpenAI\u003c/cite\u003e\u003c/p\u003e\u003c/div\u003e\u003cp\u003eUsers can ask questions using text, images, and additional files like PDFs or spreadsheets to add context, and then it will take “anywhere from 5 to 30 minutes” to develop a response provided in the chat window, with promises that in the future it will also be able to include embedded images and charts. OpenAI also notes limitations for deep research, saying it can “sometimes hallucinate” and make up facts, struggle with telling the difference between authoritative info and rumors, and register how certain it should rate a response.\u003c/p\u003e\u003cp\u003eDeveloping ways for generative AI tools to be more useful and worth paying for is the future companies like OpenAI \u003ca href=\"https://www.theverge.com/2024/10/10/24266333/ai-agents-assistants-openai-google-deepmind-bots\"\u003ehave promised for agents\u003c/a\u003e, and it claims that deep research is capable of operating at the level of a research analyst. The demo video included here begins with a request for info on changes in the retail industry over the last three years, with a response that includes bullet points and tables.\u003c/p\u003e\u003cp\u003eThis feature closely follows OpenAI’s \u003ca href=\"https://www.theverge.com/2025/1/23/24350395/openai-chatgpt-operator-agent-control-computer\"\u003elaunch of Operator\u003c/a\u003e, a tool that can use a web browser to complete tasks for you, and is similar to the Project Mariner research prototype Google showed off in December. Google’s tool is not available to the public yet, but deep research is launching “with a version optimized for Pro users today.”\u003c/p\u003e\u003cp\u003eOpenAI is offering up to 100 queries per month for those paying the $200 monthly fee and “limited access” promised for Plus, Team, and eventually, Enterprise users, calling the ability “very compute intensive,” requiring more inference compute the longer it takes to research something. It also says that all paid users will get higher rate limits in the future when a faster, more cost-effective version is available.\u003c/p\u003e\u003cp\u003eA press release says that the model powering deep research scored a new high for accuracy on an AI benchmark dubbed “\u003ca href=\"https://agi.safe.ai/\"\u003eHumanity’s Last Exam\u003c/a\u003e,” which asks for responses to expert-level questions. The OpenAI deep research model reached an accuracy of 26.6 percent with browsing and python tools enabled, well above GPT-4o’s 3.3 percent, and the next highest scorer, its o3-mini (high) model evaluated only on text, at 13 percent.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2025-02-03T00:03:30Z",
  "modifiedTime": "2025-02-03T00:11:45Z"
}
