{
  "id": "ca0186a6-5e12-42b2-9244-f2b043690bc6",
  "title": "CodeSOD: The First 10,000",
  "link": "https://thedailywtf.com/articles/the-first-10-000",
  "description": "Alicia recently inherited a whole suite of home-grown enterprise applications. Like a lot of these kinds of systems, it needs to do batch processing. She went tracking down a mysterious IllegalStateException only to find this query causing the problem: select * from data_import where id \u003e 10000 The query itself is fine, but the code calling it checks to see if this query returned any rows- if it did, the code throws the IllegalStateException. First, of course, this should be a COUNT(*) query- no need to actually return rows here. But also… what? Why do we fail if there are any transactions with an ID greater than 10000? Why on Earth would we care? Well, the next query it runs is this: update data_import set id=id+10000 Oh. Oh no. Oh nooooo. Are they… are they using the ID to also represent some state information about the status of the record? It sure seems like it! The program then starts INSERTing data, using a counter which starts at 1. Once all the new data is added, the program then does: delete from data_import where id \u003e 10000 All this is done within a single method, with no transactions and no error handling. And yes, this is by design. You see, if anything goes wrong during the inserts, then the old records don't get deleted, so we can see that processing failed and correct it. And since the IDs are sequential and always start at 1, we can easily find which row caused the problem. Who needs logging or any sort of exception handling- just check your IDs. The underlying reason why this started failing was because the inbound data started trying to add more than 10,000 rows, which meant the INSERTs started failing (since we already had rows there for this). Alicia wanted to fix this and clean up the process, but too many things depended on it working in this broken fashion. Instead, her boss implemented a quick and easy fix: they changed \"10000\" to \"100000\". [Advertisement] Picking up NuGet is easy. Getting good at it takes time. Download our guide to learn the best practice of NuGet for the Enterprise.",
  "author": "Remy Porter",
  "published": "Wed, 13 Nov 2024 06:30:00 GMT",
  "source": "http://syndication.thedailywtf.com/TheDailyWtf",
  "categories": [
    "CodeSOD"
  ],
  "byline": "Remy Porter",
  "length": 2156,
  "excerpt": "Alicia recently inherited a whole suite of home-grown enterprise applications. Like a lot of these kinds of systems, it needs to do batch processing. She went tracking down a mysterious IllegalStateException only to find this query causing the problem: select * from data_import where id \u003e 10000",
  "siteName": "The Daily WTF",
  "favicon": "",
  "text": "by in CodeSOD on 2024-11-13 Edit Remy PorterComputers were a mistake, which is why I'm trying to shoot them into space. Editor-in-Chief for TDWTF. Alicia recently inherited a whole suite of home-grown enterprise applications. Like a lot of these kinds of systems, it needs to do batch processing. She went tracking down a mysterious IllegalStateException only to find this query causing the problem: select * from data_import where id \u003e 10000 The query itself is fine, but the code calling it checks to see if this query returned any rows- if it did, the code throws the IllegalStateException. First, of course, this should be a COUNT(*) query- no need to actually return rows here. But also… what? Why do we fail if there are any transactions with an ID greater than 10000? Why on Earth would we care? Well, the next query it runs is this: update data_import set id=id+10000 Oh. Oh no. Oh nooooo. Are they… are they using the ID to also represent some state information about the status of the record? It sure seems like it! The program then starts INSERTing data, using a counter which starts at 1. Once all the new data is added, the program then does: delete from data_import where id \u003e 10000 All this is done within a single method, with no transactions and no error handling. And yes, this is by design. You see, if anything goes wrong during the inserts, then the old records don't get deleted, so we can see that processing failed and correct it. And since the IDs are sequential and always start at 1, we can easily find which row caused the problem. Who needs logging or any sort of exception handling- just check your IDs. The underlying reason why this started failing was because the inbound data started trying to add more than 10,000 rows, which meant the INSERTs started failing (since we already had rows there for this). Alicia wanted to fix this and clean up the process, but too many things depended on it working in this broken fashion. Instead, her boss implemented a quick and easy fix: they changed \"10000\" to \"100000\".",
  "image": "https://s3.amazonaws.com/remy.jetpackshark.com/remy-thumb.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv itemscope=\"\" itemtype=\"http://schema.org/Article\" id=\"article-page\"\u003e\n\n        \n        \u003cp\u003e\u003cspan\u003eby \n            in \u003ca itemprop=\"articleSection\" href=\"https://thedailywtf.com/series/code-sod\"\u003eCodeSOD\u003c/a\u003e\n            on \u003cspan itemprop=\"datePublished\" content=\"2024-11-13\"\u003e2024-11-13\u003c/span\u003e\n            \u003ca href=\"https://thedailywtf.com/admin/article/edit/10780\" rel=\"nofollow\"\u003eEdit\u003c/a\u003e\n        \u003c/span\u003e\u003c/p\u003e\n        \u003cdiv itemprop=\"author\" itemscope=\"\" itemtype=\"http://schema.org/Person\"\u003e\n            \u003cp\u003e\u003cimg itemprop=\"image\" src=\"https://s3.amazonaws.com/remy.jetpackshark.com/remy-thumb.jpg\"/\u003e\n            \u003ca itemprop=\"name\" href=\"https://thedailywtf.com/authors/remy-porter\"\u003eRemy Porter\u003c/a\u003e\u003c/p\u003e\u003cp itemprop=\"description\"\u003eComputers were a mistake, which is why I\u0026#39;m trying to shoot them into space. Editor-in-Chief for TDWTF.\u003c/p\u003e\n        \u003c/div\u003e\n        \u003cdiv itemprop=\"articleBody\"\u003e\n            \u003cp\u003e\u003cstrong\u003eAlicia\u003c/strong\u003e recently inherited a whole suite of home-grown enterprise applications. Like a lot of these kinds of systems, it needs to do batch processing. She went tracking down a mysterious \u003ccode\u003eIllegalStateException\u003c/code\u003e only to find this query causing the problem:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u003cspan\u003eselect\u003c/span\u003e \u003cspan\u003e*\u003c/span\u003e \u003cspan\u003efrom\u003c/span\u003e data_import \u003cspan\u003ewhere\u003c/span\u003e id \u003cspan\u003e\u0026gt;\u003c/span\u003e \u003cspan\u003e10000\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe query itself is fine, but the code calling it checks to see if this query returned any rows- if it did, the code throws the \u003ccode\u003eIllegalStateException\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFirst, of course, this should be a \u003ccode\u003eCOUNT(*)\u003c/code\u003e query- no need to actually return rows here. But also… what? Why do we fail if there are any transactions with an ID greater than 10000? \u003cem\u003eWhy on Earth would we care\u003c/em\u003e?\u003c/p\u003e\n\u003cp\u003eWell, the \u003cem\u003enext\u003c/em\u003e query it runs is this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u003cspan\u003eupdate\u003c/span\u003e data_import \u003cspan\u003eset\u003c/span\u003e id\u003cspan\u003e=\u003c/span\u003eid\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e10000\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOh. Oh no. Oh nooooo. Are they… are they using the ID to also represent some state information about the status of the record? It sure seems like it!\u003c/p\u003e\n\u003cp\u003eThe program then starts \u003ccode\u003eINSERT\u003c/code\u003eing data, using a counter which starts at 1. Once all the new data is added, the program then does:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u003cspan\u003edelete\u003c/span\u003e \u003cspan\u003efrom\u003c/span\u003e data_import \u003cspan\u003ewhere\u003c/span\u003e id \u003cspan\u003e\u0026gt;\u003c/span\u003e \u003cspan\u003e10000\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll this is done within a single method, with no transactions and no error handling. And yes, this is by design. You see, if anything goes wrong during the inserts, then the old records don\u0026#39;t get deleted, so we can see that processing failed and correct it. And since the IDs are sequential and always start at 1, we can easily find \u003cem\u003ewhich\u003c/em\u003e row caused the problem. Who needs logging or any sort of exception handling- just check your IDs.\u003c/p\u003e\n\u003cp\u003eThe underlying reason why this started failing was because the inbound data started trying to add more than 10,000 rows, which meant the \u003ccode\u003eINSERT\u003c/code\u003es started failing (since we already had rows there for this). Alicia wanted to fix this and clean up the process, but too many things depended on it working in this broken fashion. Instead, her boss implemented a quick and easy fix: they changed \u0026#34;10000\u0026#34; to \u0026#34;100000\u0026#34;.\u003c/p\u003e\n\n\n        \u003c/div\u003e\n\n        \n    \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2024-11-13T06:30:00Z",
  "modifiedTime": null
}
