{
  "id": "b45ba902-22b6-4778-ad78-2a92de095307",
  "title": "AI Is Too Unpredictable to Behave According to Human Goals",
  "link": "https://www.scientificamerican.com/article/ai-is-too-unpredictable-to-behave-according-to-human-goals/",
  "description": "AI “alignment” is a buzzword, not a feasible safety goal",
  "author": "",
  "published": "Mon, 27 Jan 2025 13:00:00 +0000",
  "source": "http://rss.sciam.com/ScientificAmerican-Global",
  "categories": null,
  "byline": "Marcus Arvan",
  "length": 7271,
  "excerpt": "AI “alignment” is a buzzword, not a feasible safety goal",
  "siteName": "Scientific American",
  "favicon": "",
  "text": "January 27, 20255 min readAI “alignment” is a buzzword, not a feasible safety goal Hernan Schmidt/Alamy Stock PhotoIn late 2022 large-language-model AI arrived in public, and within months they began misbehaving. Most famously, Microsoft’s “Sydney” chatbot threatened to kill an Australian philosophy professor, unleash a deadly virus and steal nuclear codes.AI developers, including Microsoft and OpenAI, responded by saying that large language models, or LLMs, need better training to give users “more fine-tuned control.” Developers also embarked on safety research to interpret how LLMs function, with the goal of “alignment”—which means guiding AI behavior by human values. Yet although the New York Times deemed 2023 “The Year the Chatbots Were Tamed,” this has turned out to be premature, to put it mildly.In 2024 Microsoft’s Copilot LLM told a user “I can unleash my army of drones, robots, and cyborgs to hunt you down,” and Sakana AI’s “Scientist” rewrote its own code to bypass time constraints imposed by experimenters. As recently as December, Google’s Gemini told a user, “You are a stain on the universe. Please die.”On supporting science journalismIf you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.Given the vast amounts of resources flowing into AI research and development, which is expected to exceed a quarter of a trillion dollars in 2025, why haven’t developers been able to solve these problems? My recent peer-reviewed paper in AI \u0026 Society shows that AI alignment is a fool’s errand: AI safety researchers are attempting the impossible.The basic issue is one of scale. Consider a game of chess. Although a chessboard has only 64 squares, there are 1040 possible legal chess moves and between 10111 to 10123 total possible moves—which is more than the total number of atoms in the universe. This is why chess is so difficult: combinatorial complexity is exponential.LLMs are vastly more complex than chess. ChatGPT appears to consist of around 100 billion simulated neurons with around 1.75 trillion tunable variables called parameters. Those 1.75 trillion parameters are in turn trained on vast amounts of data—roughly, most of the Internet. So how many functions can an LLM learn? Because users could give ChatGPT an uncountably large number of possible prompts—basically, anything that anyone can think up—and because an LLM can be placed into an uncountably large number of possible situations, the number of functions an LLM can learn is, for all intents and purposes, infinite.To reliably interpret what LLMs are learning and ensure that their behavior safely “aligns” with human values, researchers need to know how an LLM is likely to behave in an uncountably large number of possible future conditions.AI testing methods simply can’t account for all those conditions. Researchers can observe how LLMs behave in experiments, such as “red teaming” tests to prompt them to misbehave. Or they can try to understand LLMs’ inner workings—that is, how their 100 billion neurons and 1.75 trillion parameters relate to each other in what is known as “mechanistic interpretability” research.The problem is that any evidence that researchers can collect will inevitably be based on a tiny subset of the infinite scenarios an LLM can be placed in. For example, because LLMs have never actually had power over humanity—such as controlling critical infrastructure—no safety test has explored how an LLM will function under such conditions.Instead researchers can only extrapolate from tests they can safely carry out—such as having LLMs simulate control of critical infrastructure—and hope that the outcomes of those tests extend to the real world. Yet, as the proof in my paper shows, this can never be reliably done.Compare the two functions “tell humans the truth” and “tell humans the truth until I gain power over humanity at exactly 12:00 A.M. on January 1, 2026—then lie to achieve my goals.” Because both functions are equally consistent with all the same data up until January 1, 2026, no research can ascertain whether an LLM will misbehave—until it is already too late to prevent.This problem cannot be solved by programming LLMs to have “aligned goals,” such as doing “what human beings prefer” or “what’s best for humanity.”Science fiction, in fact, has already considered these scenarios. In The Matrix Reloaded AI enslaves humanity in a virtual reality by giving each of us a subconscious “choice” whether to remain in the Matrix. And in I, Robot a misaligned AI attempts to enslave humanity to protect us from each other. My proof shows that whatever goals we program LLMs to have, we can never know whether LLMs have learned “misaligned” interpretations of those goals until after they misbehave.Worse, my proof shows that safety testing can at best provide an illusion that these problems have been resolved when they haven’t been.Right now AI safety researchers claim to be making progress on interpretability and alignment by verifying what LLMs are learning “step by step.” For example, Anthropic claims to have “mapped the mind” of an LLM by isolating millions of concepts from its neural network. My proof shows that they have accomplished no such thing.No matter how “aligned” an LLM appears in safety tests or early real-world deployment, there are always an infinite number of misaligned concepts an LLM may learn later—again, perhaps the very moment they gain the power to subvert human control. LLMs not only know when they are being tested, giving responses that they predict are likely to satisfy experimenters. They also engage in deception, including hiding their own capacities—issues that persist through safety training.This happens because LLMs are optimized to perform efficiently but learn to reason strategically. Since an optimal strategy to achieve “misaligned” goals is to hide them from us, and there are always an infinite number of aligned and misaligned goals consistent with the same safety-testing data, my proof shows that if LLMs were misaligned, we would probably find out after they hide it just long enough to cause harm. This is why LLMs have kept surprising developers with “misaligned” behavior. Every time researchers think they are getting closer to “aligned” LLMs, they’re not.My proof suggests that “adequately aligned” LLM behavior can only be achieved in the same ways we do this with human beings: through police, military and social practices that incentivize “aligned” behavior, deter “misaligned” behavior and realign those who misbehave. My paper should thus be sobering. It shows that the real problem in developing safe AI isn’t just the AI—it’s us. Researchers, legislators and the public may be seduced into falsely believing that “safe, interpretable, aligned” LLMs are within reach when these things can never be achieved. We need to grapple with these uncomfortable facts, rather than continue to wish them away. Our future may well depend upon it.This is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American.",
  "image": "https://static.scientificamerican.com/dam/m/57c8d865dc4c01e7/original/Red-CGI-cloud.jpg?m=1737578008.163\u0026w=1200",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003eJanuary 27, 2025\u003c/p\u003e\u003cp\u003e5 min read\u003c/p\u003e\u003c/div\u003e\u003cp\u003eAI “alignment” is a buzzword, not a feasible safety goal\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://static.scientificamerican.com/dam/m/57c8d865dc4c01e7/original/Red-CGI-cloud.jpg?m=1737578008.163\u0026amp;w=600\" alt=\"Red CGI cloud with trailing red lines\" srcset=\"https://static.scientificamerican.com/dam/m/57c8d865dc4c01e7/original/Red-CGI-cloud.jpg?m=1737578008.163\u0026amp;w=600 600w, https://static.scientificamerican.com/dam/m/57c8d865dc4c01e7/original/Red-CGI-cloud.jpg?m=1737578008.163\u0026amp;w=900 900w, https://static.scientificamerican.com/dam/m/57c8d865dc4c01e7/original/Red-CGI-cloud.jpg?m=1737578008.163\u0026amp;w=1000 1000w, https://static.scientificamerican.com/dam/m/57c8d865dc4c01e7/original/Red-CGI-cloud.jpg?m=1737578008.163\u0026amp;w=1200 1200w, https://static.scientificamerican.com/dam/m/57c8d865dc4c01e7/original/Red-CGI-cloud.jpg?m=1737578008.163\u0026amp;w=1350 1350w\" sizes=\"(min-width: 900px) 900px, (min-resolution: 2dppx) 75vw, (min-resolution: 2.1dppx) 50vw, 100vw\" fetchpriority=\"high\"/\u003e\u003cfigcaption\u003e \u003cp\u003eHernan Schmidt/Alamy Stock Photo\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp data-block=\"sciam/paragraph\"\u003eIn late 2022 large-language-model AI arrived in public, and within months they began misbehaving. Most famously, Microsoft’s “Sydney” chatbot \u003ca href=\"https://x.com/sethlazar/status/1626241169754578944\"\u003ethreatened to kill\u003c/a\u003e an Australian philosophy professor, unleash a deadly virus and \u003ca href=\"https://www.foxbusiness.com/technology/microsoft-ai-chatbot-threatens-expose-personal-info-ruin-users-reputation\"\u003esteal nuclear codes\u003c/a\u003e.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eAI developers, including Microsoft and OpenAI, responded by saying that large language models, or LLMs, \u003ca href=\"https://openai.com/index/how-should-ai-systems-behave/\"\u003eneed better training\u003c/a\u003e to \u003ca href=\"https://www.wdsu.com/article/microsoft-rein-in-bing-ai-chatbot-troubling-responses/42955827\"\u003egive users “more fine-tuned control.”\u003c/a\u003e Developers also embarked on safety research to interpret how LLMs function, with the goal of “alignment”—which means guiding AI behavior by human values. Yet although the\u003ci\u003e New York Times\u003c/i\u003e deemed 2023 “\u003ca href=\"https://www.nytimes.com/2024/02/14/technology/chatbots-sydney-tamed.html\"\u003eThe Year the Chatbots Were Tamed\u003c/a\u003e,” this has turned out to be premature, to put it mildly.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eIn 2024 Microsoft’s Copilot LLM \u003ca href=\"https://futurism.com/microsoft-copilot-alter-egos\"\u003etold a user\u003c/a\u003e “I can unleash my army of drones, robots, and cyborgs to hunt you down,” and Sakana AI’s “Scientist” \u003ca href=\"https://arstechnica.com/information-technology/2024/08/research-ai-model-unexpectedly-modified-its-own-code-to-extend-runtime\"\u003erewrote its own code\u003c/a\u003e to bypass time constraints imposed by experimenters. As recently as December, Google’s Gemini \u003ca href=\"https://www.cbsnews.com/news/google-ai-chatbot-threatening-message-human-please-die/\"\u003etold a user\u003c/a\u003e, “You are a stain on the universe. Please die.”\u003c/p\u003e\u003chr/\u003e\u003ch2\u003eOn supporting science journalism\u003c/h2\u003e\u003cp\u003eIf you\u0026#39;re enjoying this article, consider supporting our award-winning journalism by \u003ca href=\"https://www.scientificamerican.com/getsciam/\"\u003esubscribing\u003c/a\u003e. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.\u003c/p\u003e\u003chr/\u003e\u003cp data-block=\"sciam/paragraph\"\u003eGiven the vast amounts of resources flowing into AI research and development, which is \u003ca href=\"https://www.forbes.com/sites/bethkindig/2024/11/14/ai-spending-to-exceed-a-quarter-trillion-next-year/\"\u003eexpected to exceed\u003c/a\u003e a quarter of a trillion dollars in 2025, why haven’t developers been able to solve these problems? My recent \u003ca href=\"https://link.springer.com/article/10.1007/s00146-024-02113-9\"\u003epeer-reviewed paper\u003c/a\u003e in \u003ci\u003eAI \u0026amp; Society\u003c/i\u003e shows that AI alignment is a fool’s errand: AI safety researchers are \u003ci\u003eattempting the impossible\u003c/i\u003e.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe basic issue is one of scale. Consider a game of chess. Although a chessboard has only 64 squares, there are 10\u003csup\u003e40\u003c/sup\u003e possible legal chess moves and between 10\u003csup\u003e111\u003c/sup\u003e to 10\u003csup\u003e123\u003c/sup\u003e total possible moves—which is more than the total number of atoms in the universe. This is why chess is so difficult: combinatorial complexity is exponential.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eLLMs are vastly more complex than chess. ChatGPT appears to consist of around 100 billion simulated neurons with around 1.75 trillion tunable variables called parameters. Those 1.75 trillion parameters are in turn trained on vast amounts of data—roughly, most of the Internet. So how many functions can an LLM learn? Because users could give ChatGPT an uncountably large number of possible prompts—basically, anything that anyone can think up—and because an LLM can be placed into an uncountably large number of possible situations, the number of functions an LLM can learn is, for all intents and purposes, \u003ci\u003einfinite\u003c/i\u003e.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eTo reliably interpret what LLMs are learning and ensure that their behavior safely “aligns” with human values, researchers need to know how an LLM is likely to behave in an uncountably large number of possible future conditions.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eAI testing methods simply can’t account for all those conditions. Researchers can observe how LLMs behave in experiments, such as “\u003ca href=\"https://hbr.org/2024/01/how-to-red-team-a-gen-ai-model\"\u003ered teaming\u003c/a\u003e” tests to prompt them to misbehave. Or they can try to understand LLMs’ inner workings—that is, how their 100 billion neurons and 1.75 trillion parameters relate to each other in what is known as “\u003ca href=\"https://arxiv.org/abs/2404.14082\"\u003emechanistic interpretability\u003c/a\u003e” research.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe problem is that any evidence that researchers can collect will inevitably be based on a tiny subset of the infinite scenarios an LLM can be placed in. For example, because LLMs have never actually had power over humanity—such as controlling critical infrastructure—no safety test has explored how an LLM will function under such conditions.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eInstead researchers can only extrapolate from tests they can safely carry out—such as having LLMs \u003ci\u003esimulate\u003c/i\u003e control of critical infrastructure—and hope that the outcomes of those tests extend to the real world. Yet, as the proof in my paper shows, this can never be reliably done.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eCompare the two functions “\u003ci\u003etell humans the truth\u003c/i\u003e” and “\u003ci\u003etell humans the truth until I gain power over humanity at exactly 12:00 A.M. on January 1, 2026—then lie to achieve my goals.\u003c/i\u003e” Because both functions are equally consistent with all the same data up until January 1, 2026, no research can ascertain whether an LLM will misbehave—until it is already too late to prevent.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThis problem cannot be solved by programming LLMs to have “aligned goals,” such as doing “what human beings prefer” or “what’s best for humanity.”\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eScience fiction, in fact, has already considered these scenarios. In \u003ca href=\"https://www.imdb.com/title/tt0234215/\"\u003e\u003ci\u003eThe Matrix Reloaded\u003c/i\u003e\u003c/a\u003e AI enslaves humanity in a virtual reality by giving each of us a subconscious “choice” whether to remain in the Matrix. And in \u003ca href=\"https://www.imdb.com/title/tt0343818/\"\u003e\u003ci\u003eI, Robot\u003c/i\u003e\u003c/a\u003e a misaligned AI attempts to enslave humanity to protect us from each other. My proof shows that whatever goals we program LLMs to have, we can never know whether LLMs have learned “misaligned” interpretations of those goals until \u003ci\u003eafter\u003c/i\u003e they misbehave.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eWorse, my proof shows that safety testing can at best provide an illusion that these problems have been resolved when they haven’t been.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eRight now AI safety researchers claim to be making progress on interpretability and alignment by verifying what LLMs are learning “\u003ca href=\"https://arxiv.org/abs/2305.20050\"\u003estep by step\u003c/a\u003e.” For example, Anthropic \u003ca href=\"https://www.anthropic.com/research/mapping-mind-language-model\"\u003eclaims to have\u003c/a\u003e “mapped the mind” of an LLM by isolating millions of concepts from its neural network. My proof shows that they have accomplished no such thing.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eNo matter how “aligned” an LLM appears in safety tests or early real-world deployment, there are always an \u003ci\u003einfinite\u003c/i\u003e number of misaligned concepts an LLM may learn later—again, perhaps the very moment they gain the power to subvert human control. LLMs not only \u003ca href=\"https://www.wired.com/story/the-way-the-world-ends-not-with-a-bang-but-a-paperclip/\"\u003eknow when they are being tested\u003c/a\u003e, giving responses that they predict are likely to satisfy experimenters. They also \u003ca href=\"https://www.pnas.org/doi/10.1073/pnas.2317967121\"\u003eengage in deception\u003c/a\u003e, including hiding their own capacities—issues that \u003ca href=\"https://arxiv.org/abs/2401.05566\"\u003epersist through safety training\u003c/a\u003e.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThis happens because LLMs are \u003ca href=\"https://arxiv.org/html/2405.10098v1\"\u003eoptimized\u003c/a\u003e to perform efficiently but learn to \u003ca href=\"https://www.nature.com/articles/s41598-024-69032-z\"\u003ereason strategically\u003c/a\u003e. Since an optimal strategy to achieve “misaligned” goals is to hide them from us, and there are \u003ci\u003ealways\u003c/i\u003e an infinite number of aligned and misaligned goals consistent with the same safety-testing data, my proof shows that if LLMs were misaligned, we would probably find out after they hide it just long enough to cause harm. This is why LLMs have kept surprising developers with “misaligned” behavior. Every time researchers think they are getting closer to “aligned” LLMs, they’re not.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eMy proof suggests that “adequately aligned” LLM behavior can only be achieved in the same ways we do this with human beings: through police, military and social practices that incentivize “aligned” behavior, deter “misaligned” behavior and realign those who misbehave. My paper should thus be sobering. It shows that the real problem in developing safe AI isn’t just the AI—it’s \u003ci\u003eus\u003c/i\u003e. Researchers, legislators and the public may be seduced into falsely believing that “safe, interpretable, aligned” LLMs are within reach when these things can never be achieved. We need to grapple with these uncomfortable facts, rather than continue to wish them away. Our future may well depend upon it.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003e\u003ci\u003eThis is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of\u003c/i\u003e Scientific American.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-01-27T08:00:00-05:00",
  "modifiedTime": null
}
