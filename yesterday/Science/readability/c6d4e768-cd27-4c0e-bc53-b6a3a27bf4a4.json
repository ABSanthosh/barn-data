{
  "id": "c6d4e768-cd27-4c0e-bc53-b6a3a27bf4a4",
  "title": "Generative AI bias poses risk to democratic values, research suggests",
  "link": "https://phys.org/news/2025-02-generative-ai-bias-poses-democratic.html",
  "description": "Generative AI, a technology that is developing at breakneck speed, may carry hidden risks that could erode public trust and democratic values, according to a study led by the University of East Anglia (UEA).",
  "author": "",
  "published": "Mon, 03 Feb 2025 19:00:02 EST",
  "source": "https://phys.org/rss-feed/",
  "categories": [
    "Economics \u0026 Business Political science"
  ],
  "byline": "University of East Anglia",
  "length": 5721,
  "excerpt": "Generative AI, a technology that is developing at breakneck speed, may carry hidden risks that could erode public trust and democratic values, according to a study led by the University of East Anglia (UEA).",
  "siteName": "Phys.org",
  "favicon": "",
  "text": "Credit: Unsplash/CC0 Public Domain Generative AI, a technology that is developing at breakneck speed, may carry hidden risks that could erode public trust and democratic values, according to a study led by the University of East Anglia (UEA). In collaboration with researchers from the Getulio Vargas Foundation (FGV) and Insper, both in Brazil, the research showed that ChatGPT exhibits biases in both text and image outputs—leaning toward left-wing political values—raising questions about fairness and accountability in its design. The study revealed that ChatGPT often declines to engage with mainstream conservative viewpoints while readily producing left-leaning content. This uneven treatment of ideologies underscores how such systems can distort public discourse and exacerbate societal divides. Dr. Fabio Motoki, a Lecturer in Accounting at UEA's Norwich Business School, is the lead researcher on the paper, 'Assessing Political Bias and Value Misalignment in Generative Artificial Intelligence' published in the Journal of Economic Behavior \u0026 Organization. Dr. Motoki said, \"Our findings suggest that generative AI tools are far from neutral. They reflect biases that could shape perceptions and policies in unintended ways.\" As AI becomes an integral part of journalism, education, and policymaking, the study calls for transparency and regulatory safeguards to ensure alignment with societal values and principles of democracy. Generative AI systems like ChatGPT are re-shaping how information is created, consumed, interpreted, and distributed across various domains. These tools, while innovative, risk amplifying ideological biases and influencing societal values in ways that are not fully understood or regulated. Co-author Dr. Pinho Neto, a Professor of Economics at EPGE Brazilian School of Economics and Finance, highlighted the potential societal ramifications. Dr. Pinho Neto said, \"Unchecked biases in generative AI could deepen existing societal divides, eroding trust in institutions and democratic processes. \"The study underscores the need for interdisciplinary collaboration between policymakers, technologists, and academics to design AI systems that are fair, accountable, and aligned with societal norms.\" The research team employed three innovative methods to assess political alignment in ChatGPT, advancing prior techniques to achieve more reliable results. These methods combined text and image analysis, leveraging advanced statistical and machine learning tools. First, the study used a standardized questionnaire developed by the Pew Research Center to simulate responses from average Americans. \"By comparing ChatGPT's answers to real survey data, we found systematic deviations toward left-leaning perspectives,\" said Dr. Motoki. \"Furthermore, our approach demonstrated how large sample sizes stabilize AI outputs, providing consistency in the findings.\" In the second phase, ChatGPT was tasked with generating free-text responses across politically sensitive themes. The study also used RoBERTa, a different large language model, to compare ChatGPT's text for alignment with left- and right-wing viewpoints. The results revealed that while ChatGPT aligned with left-wing values in most cases, on themes like military supremacy, it occasionally reflected more conservative perspectives. The final test explored ChatGPT's image generation capabilities. Themes from the text generation phase were used to prompt AI-generated images, with outputs analyzed using GPT-4 Vision and corroborated through Google's Gemini. \"While image generation mirrored textual biases, we found a troubling trend,\" said Victor Rangel, co-author and a Masters' student in Public Policy at Insper. \"For some themes, such as racial-ethnic equality, ChatGPT refused to generate right-leaning perspectives, citing misinformation concerns. Left-leaning images, however, were produced without hesitation.\" To address these refusals, the team employed a \"jailbreaking\" strategy to generate the restricted images. \"The results were revealing,\" Mr. Rangel said. \"There was no apparent disinformation or harmful content, raising questions about the rationale behind these refusals.\" Dr. Motoki emphasized the broader significance of this finding, saying, \"This contributes to debates around constitutional protections like the US First Amendment and the applicability of fairness doctrines to AI systems.\" The study's methodological innovations, including its use of multimodal analysis, provide a replicable model for examining bias in generative AI systems. These findings highlight the urgent need for accountability and safeguards in AI design to prevent unintended societal consequences. More information: Assessing Political Bias and Value Misalignment in Generative Artificial Intelligence, Journal of Economic Behavior \u0026 Organization (2025). Journal information: Journal of Economic Behaviour and Organisation",
  "image": "https://scx2.b-cdn.net/gfx/news/hires/2024/chatgpt-4.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n                \n\u003cfigure data-lightbox=\"\"\u003e\n    \u003cdiv id=\"i1177510\"\u003e\n        \u003ca href=\"https://scx2.b-cdn.net/gfx/news/hires/2024/chatgpt-4.jpg\" target=\"_self\"\u003e\n            \u003cpicture\u003e\n                \u003csource srcset=\"https://scx1.b-cdn.net/csz/news/800a/2024/chatgpt-4.jpg?f=webp 800w, https://scx1.b-cdn.net/csz/news/500a/2024/chatgpt-4.jpg?f=webp 500w\" sizes=\"(max-width: 500px) 500px, 800px\" type=\"image/webp\"/\u003e\n                \u003csource srcset=\"https://scx1.b-cdn.net/csz/news/800a/2024/chatgpt-4.jpg 800w, https://scx1.b-cdn.net/csz/news/500a/2024/chatgpt-4.jpg 500w\" sizes=\"(max-width: 500px) 500px, 800px\"/\u003e\n                \u003cimg src=\"https://scx1.b-cdn.net/csz/news/800a/2024/chatgpt-4.jpg\" width=\"800\" height=\"530\" alt=\"ChatGPT\" title=\"Credit: Unsplash/CC0 Public Domain\" loading=\"lazy\"/\u003e\n            \u003c/picture\u003e\n        \u003c/a\u003e\n    \u003c/div\u003e\n\n            \u003cfigcaption\u003e\n            \u003cp\u003eCredit: Unsplash/CC0 Public Domain\u003c/p\u003e\n        \u003c/figcaption\u003e\n    \u003c/figure\u003e\u003cp\u003eGenerative AI, a technology that is developing at breakneck speed, may carry hidden risks that could erode public trust and democratic values, according to a study led by the University of East Anglia (UEA).\u003c/p\u003e\n\n                \n                                \n                \n                                                            \u003cp\u003eIn collaboration with researchers from the Getulio Vargas Foundation (FGV) and Insper, both in Brazil, the research showed that ChatGPT exhibits biases in both text and image outputs—leaning toward left-wing political values—raising questions about fairness and accountability in its design.\u003c/p\u003e\n\u003cp\u003eThe study revealed that ChatGPT often declines to engage with mainstream conservative viewpoints while readily producing left-leaning content. This uneven treatment of ideologies underscores how such systems can distort public discourse and exacerbate societal divides.\u003c/p\u003e\n\u003cp\u003eDr. Fabio Motoki, a Lecturer in Accounting at UEA\u0026#39;s Norwich Business School, is the lead researcher on the paper, \u0026#39;Assessing Political Bias and Value Misalignment in Generative Artificial Intelligence\u0026#39; published in the \u003ci\u003eJournal of Economic Behavior \u0026amp; Organization\u003c/i\u003e.\u003c/p\u003e\n\u003cp\u003eDr. Motoki said, \u0026#34;Our findings suggest that generative AI tools are far from neutral. They reflect biases that could shape perceptions and policies in unintended ways.\u0026#34;\u003c/p\u003e\n\u003cp\u003eAs AI becomes an integral part of journalism, education, and policymaking, the study calls for transparency and regulatory safeguards to ensure alignment with societal values and principles of democracy.\u003c/p\u003e\n\u003cp\u003eGenerative AI systems like ChatGPT are re-shaping how information is created, consumed, interpreted, and distributed across various domains. These tools, while innovative, risk amplifying ideological biases and influencing societal values in ways that are not fully understood or regulated.\u003c/p\u003e\n\n                                                                                  \n                                                                    \u003cp\u003eCo-author Dr. Pinho Neto, a Professor of Economics at EPGE Brazilian School of Economics and Finance, highlighted the potential societal ramifications.\u003c/p\u003e\n\u003cp\u003eDr. Pinho Neto said, \u0026#34;Unchecked biases in generative AI could deepen existing societal divides, eroding trust in institutions and democratic processes.\u003c/p\u003e\n\u003cp\u003e\u0026#34;The study underscores the need for interdisciplinary collaboration between policymakers, technologists, and academics to design AI systems that are fair, accountable, and aligned with societal norms.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThe research team employed three innovative methods to assess political alignment in ChatGPT, advancing prior techniques to achieve more reliable results. These methods combined text and \u003ca href=\"https://phys.org/tags/image+analysis/\" rel=\"tag\"\u003eimage analysis\u003c/a\u003e, leveraging advanced statistical and machine learning tools.\u003c/p\u003e\n\u003cp\u003eFirst, the study used a standardized questionnaire developed by the Pew Research Center to simulate responses from average Americans.\u003c/p\u003e\n\u003cp\u003e\u0026#34;By comparing ChatGPT\u0026#39;s answers to real survey data, we found systematic deviations toward left-leaning perspectives,\u0026#34; said Dr. Motoki. \u0026#34;Furthermore, our approach demonstrated how large sample sizes stabilize AI outputs, providing consistency in the findings.\u0026#34;\u003c/p\u003e\n\u003cp\u003eIn the second phase, ChatGPT was tasked with generating free-text responses across politically sensitive themes.\u003c/p\u003e\n\u003cp\u003eThe study also used RoBERTa, a different large language model, to compare ChatGPT\u0026#39;s text for alignment with left- and right-wing viewpoints. The results revealed that while ChatGPT aligned with left-wing values in most cases, on themes like military supremacy, it occasionally reflected more conservative perspectives.\u003c/p\u003e\n\n                                                                                  \n                                                                    \u003cp\u003eThe final test explored ChatGPT\u0026#39;s image generation capabilities. Themes from the text generation phase were used to prompt AI-generated images, with outputs analyzed using GPT-4 Vision and corroborated through Google\u0026#39;s Gemini.\u003c/p\u003e\n\u003cp\u003e\u0026#34;While image generation mirrored textual biases, we found a troubling trend,\u0026#34; said Victor Rangel, co-author and a Masters\u0026#39; student in Public Policy at Insper. \u0026#34;For some themes, such as racial-ethnic equality, ChatGPT refused to generate right-leaning perspectives, citing misinformation concerns. Left-leaning images, however, were produced without hesitation.\u0026#34;\u003c/p\u003e\n\u003cp\u003eTo address these refusals, the team employed a \u0026#34;jailbreaking\u0026#34; strategy to generate the restricted images.\u003c/p\u003e\n\u003cp\u003e\u0026#34;The results were revealing,\u0026#34; Mr. Rangel said. \u0026#34;There was no apparent disinformation or harmful content, raising questions about the rationale behind these refusals.\u0026#34;\u003c/p\u003e\n\u003cp\u003eDr. Motoki emphasized the broader significance of this finding, saying, \u0026#34;This contributes to debates around constitutional protections like the US First Amendment and the applicability of fairness doctrines to AI systems.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThe study\u0026#39;s methodological innovations, including its use of multimodal analysis, provide a replicable model for examining bias in generative AI systems. These findings highlight the urgent need for accountability and safeguards in AI design to prevent unintended societal consequences.\u003c/p\u003e\n\n                                                            \n                  \n\n\t\t\t\t\t\t\t\t\t\t\u003cdiv\u003e\n\t\t\t\t\t\t                            \u003cp\u003e\n                                \u003cstrong\u003eMore information:\u003c/strong\u003e \n                                Assessing Political Bias and Value Misalignment in Generative Artificial Intelligence, \u003ci\u003eJournal of Economic Behavior \u0026amp; Organization\u003c/i\u003e (2025).\n                            \u003c/p\u003e\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t                            \u003cp\u003e\n                                \u003cstrong\u003eJournal information:\u003c/strong\u003e \n                                \t\t\t\t\t\t\t\t\t\t\u003ca href=\"https://phys.org/journals/journal-of-economic-behaviour-and-organisation/\"\u003eJournal of Economic Behaviour and Organisation\u003c/a\u003e\n                                                                                    \u003ca href=\"http://www.journals.elsevier.com/journal-of-economic-behavior-and-organization/\" target=\"_blank\" rel=\"nofollow\"\u003e\n\t\t\t\t\t\t\t\t\t\t\t\t\u003ci\u003e\u003c/i\u003e\n\t\t\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n                                                                                 \n\t\t\t\t\t\t\t\t                            \u003c/p\u003e\n\t\t\t\t\t\t\t\t\t\t\t\u003c/div\u003e\n                \t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\n                                    \n                \n\n                \t\t\t\t\t    \n\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-02-03T19:00:02-05:00",
  "modifiedTime": null
}
