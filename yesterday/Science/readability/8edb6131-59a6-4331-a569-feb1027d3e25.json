{
  "id": "8edb6131-59a6-4331-a569-feb1027d3e25",
  "title": "This New Algorithm for Sorting Books or Files Is Close to Perfection",
  "link": "https://www.wired.com/story/new-book-sorting-algorithm-almost-reaches-perfection/",
  "description": "The library sorting problem is used across computer science for organizing far more than just books. A new solution is less than a page-width away from the theoretical ideal.",
  "author": "Steve Nadis",
  "published": "Sun, 16 Feb 2025 12:00:00 +0000",
  "source": "https://www.wired.com/feed/category/science/latest/rss",
  "categories": [
    "Science",
    "Science / Physics and Math",
    "Book Smart"
  ],
  "byline": "Steve Nadis",
  "length": 9749,
  "excerpt": "The library sorting problem is used across computer science for organizing far more than just books. A new solution is less than a page-width away from the theoretical ideal.",
  "siteName": "WIRED",
  "favicon": "",
  "text": "If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more. Please also consider subscribing to WIREDThe original version of this story appeared in Quanta Magazine.Computer scientists often deal with abstract problems that are hard to comprehend, but an exciting new algorithm matters to anyone who owns books and at least one shelf. The algorithm addresses something called the library sorting problem (more formally, the “list labeling” problem). The challenge is to devise a strategy for organizing books in some kind of sorted order—alphabetically, for instance—that minimizes how long it takes to place a new book on the shelf.Imagine, for example, that you keep your books clumped together, leaving empty space on the far right of the shelf. Then, if you add a book by Isabel Allende to your collection, you might have to move every book on the shelf to make room for it. That would be a time-consuming operation. And if you then get a book by Douglas Adams, you’ll have to do it all over again. A better arrangement would leave unoccupied spaces distributed throughout the shelf—but how, exactly, should they be distributed?This problem was introduced in a 1981 paper, and it goes beyond simply providing librarians with organizational guidance. That’s because the problem also applies to the arrangement of files on hard drives and in databases, where the items to be arranged could number in the billions. An inefficient system means significant wait times and major computational expense. Researchers have invented some efficient methods for storing items, but they’ve long wanted to determine the best possible way.Last year, in a study that was presented at the Foundations of Computer Science conference in Chicago, a team of seven researchers described a way to organize items that comes tantalizingly close to the theoretical ideal. The new approach combines a little knowledge of the bookshelf’s past contents with the surprising power of randomness.“It’s a very important problem,” said Seth Pettie, a computer scientist at the University of Michigan, because many of the data structures we rely upon today store information sequentially. He called the new work “extremely inspired [and] easily one of my top three favorite papers of the year.”Narrowing BoundsSo how does one measure a well-sorted bookshelf? A common way is to see how long it takes to insert an individual item. Naturally, that depends on how many items there are in the first place, a value typically denoted by n. In the Isabel Allende example, when all the books have to move to accommodate a new one, the time it takes is proportional to n. The bigger the n, the longer it takes. That makes this an “upper bound” to the problem: It will never take longer than a time proportional to n to add one book to the shelf.The authors of the 1981 paper that ushered in this problem wanted to know if it was possible to design an algorithm with an average insertion time much less than n. And indeed, they proved that one could do better. They created an algorithm that was guaranteed to achieve an average insertion time proportional to (log n)2. This algorithm had two properties: It was “deterministic,” meaning that its decisions did not depend on any randomness, and it was also “smooth,” meaning that the books must be spread evenly within subsections of the shelf where insertions (or deletions) are made. The authors left open the question of whether the upper bound could be improved even further. For over four decades, no one managed to do so.However, the intervening years did see improvements to the lower bound. While the upper bound specifies the maximum possible time needed to insert a book, the lower bound gives the fastest possible insertion time. To find a definitive solution to a problem, researchers strive to narrow the gap between the upper and lower bounds, ideally until they coincide. When that happens, the algorithm is deemed optimal—inexorably bounded from above and below, leaving no room for further refinement.In 2004, a team of researchers found that the best any algorithm could do for the library sorting problem—in other words, the ultimate lower bound—was log n. This result pertained to the most general version of the problem, applying to any algorithm of any type. Two of the same authors had already secured a result for a more specific version of the problem in 1990, showing that for any smooth algorithm, the lower bound is significantly higher: (log n)2. And in 2012, another team proved the same lower bound, (log n)2, for any deterministic algorithm that does not use randomness at all.These results showed that for any smooth or deterministic algorithm, you could not achieve an average insertion time better than (log n)2, which was the same as the upper bound established in the 1981 paper. In other words, to improve that upper bound, researchers would need to devise a different kind of algorithm. “If you’re going to do better, you have to be randomized and non-smooth,” said Michael Bender, a computer scientist at Stony Brook University.Michael Bender went after the library sorting problem using an approach that didn’t necessarily make intuitive sense. Photograph: Courtesy of Michael BenderBut getting rid of smoothness, which requires items to be spread apart more or less evenly, seemed like a mistake. (Remember the problems that arose from our initial example—the non-smooth configuration where all the books were clumped together on the left-hand side of the shelf.) And it also was not obvious how leaving things to random chance—essentially a coin toss—would help matters. “Intuitively, it wasn’t clear that was a direction that made sense,” Bender said.Nevertheless, in 2022, Bender and five colleagues decided to try out a randomized, non-smooth algorithm anyway, just to see whether it might offer any advantages.A Secret HistoryIronically, progress came from another restriction. There are sound privacy or security reasons why you may want to use an algorithm that’s blind to the history of the bookshelf. “If I had 50 Shades of Grey on my bookshelf and took it off,” said William Kuszmaul of Carnegie Mellon University, nobody would be able to tell.William Kuszmaul, Bender and, others lowered the upper bound on the library sorting problem practically down to the ideal. Photograph: Rose SilverIn a 2022 paper, Bender, Kuszmaul, and four coauthors created just such an algorithm—one that was “history independent,” non-smooth, and randomized—which finally reduced the 1981 upper bound, bringing the average insertion time down to (log n)1.5.Kuszmaul remembers being surprised that a tool normally used to ensure privacy could confer other benefits. “It’s as if you used cryptography to make your algorithm faster,” he said. “Which just seems kind of strange.”Helen Xu of the Georgia Institute of Technology, who was not part of this research team, was also impressed.  She said that the idea of using history independence for reasons other than security may have implications for many other types of problems.Closing the GapBender, Kuszmaul, and others made an even bigger improvement with last year’s paper. They again broke the record, lowering the upper bound to (log n) times (log log n)3—equivalent to (log n)1.000…1. In other words, they came exceedingly close to the theoretical limit, the ultimate lower bound of log n.Once again, their approach was non-smooth and randomized, but this time their algorithm relied on a limited degree of history dependence. It looked at past trends to plan for future events, but only up to a point. Suppose, for instance, you’ve been getting a lot of books by authors whose last name starts with N—Nabokov, Neruda, Ng. The algorithm extrapolates from that and assumes more are probably coming, so it’ll leave a little extra space in the N section. But reserving too much space could lead to trouble if a bunch of A-name authors start pouring in. “The way we made it a good thing was by being strategically random about how much history to look at when we make our decisions,” Bender said.The result built on and transformed their previous work. It “uses randomness in a completely different way than the 2022 paper,” Pettie said.These papers collectively represent “a significant improvement” on the theory side, said Brian Wheatman, a computer scientist at the University of Chicago. “And on the applied side, I think they have the potential for a big improvement as well.”Xu agrees. “In the past few years, there’s been interest in using data structures based on list labeling for storing and processing dynamic graphs,” she said. These advances would almost certainly make things faster.Meanwhile, there’s more for theorists to contemplate. “We know that we can almost do log n,” Bender said, “[but] there’s still this tiny gap”—the diminutive log log n term that stands in the way of a complete solution. “We don’t know if the right thing to do is to lower the upper bound or raise the lower bound.”Pettie, for one, doesn’t expect the lower bound to change. “Usually in these situations, when you see a gap this close, and one of the bounds looks quite natural and the other looks unnatural, then the natural one is the right answer,” he said. It’s much more likely that any future improvements will affect the upper bound, bringing it all the way down to log n. “But the world’s full of weird surprises.”Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.",
  "image": "https://media.wired.com/photos/67a37fad81ee88784f01942a/191:100/w_1280,c_limit/LibrarySortingProblem-crKristinaArmitage-Lede.mp4",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eIf you buy something using links in our stories, we may earn a commission. This helps support our journalism. \u003ca href=\"https://www.wired.com/2015/11/affiliate-link-policy/\"\u003eLearn more\u003c/a\u003e. Please also consider \u003ca href=\"https://subscribe.wired.com/subscribe/splits/wired/WIR_SELF?source=HCL_WIR_EDIT_HARDCODED_0_COMMERCE_AFFILIATE_ZZ\"\u003esubscribing to WIRED\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"ArticlePageChunks\"\u003e\u003cdiv data-journey-hook=\"client-content\" data-testid=\"BodyWrapper\"\u003e\u003cp\u003e\u003cem\u003e\u003cspan\u003eThe original version\u003c/span\u003e of\u003c/em\u003e \u003ca href=\"https://www.quantamagazine.org/new-book-sorting-algorithm-almost-reaches-perfection-20250124/\"\u003e\u003cem\u003ethis story\u003c/em\u003e\u003c/a\u003e \u003cem\u003eappeared in \u003ca href=\"https://www.quantamagazine.org\"\u003eQuanta Magazine\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eComputer scientists often deal with abstract problems that are hard to comprehend, but an exciting new algorithm matters to anyone who owns books and at least one shelf. The algorithm addresses something called the library sorting problem (more formally, the “list labeling” problem). The challenge is to devise a strategy for organizing books in some kind of sorted order—alphabetically, for instance—that minimizes how long it takes to place a new book on the shelf.\u003c/p\u003e\u003cp\u003eImagine, for example, that you keep your books clumped together, leaving empty space on the far right of the shelf. Then, if you add a book by Isabel Allende to your collection, you might have to move every book on the shelf to make room for it. That would be a time-consuming operation. And if you then get a book by Douglas Adams, you’ll have to do it all over again. A better arrangement would leave unoccupied spaces distributed throughout the shelf—but how, exactly, should they be distributed?\u003c/p\u003e\u003cp\u003eThis problem was introduced in a \u003ca data-offer-url=\"https://link.springer.com/chapter/10.1007/3-540-10843-2_34\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://link.springer.com/chapter/10.1007/3-540-10843-2_34\u0026#34;}\" href=\"https://link.springer.com/chapter/10.1007/3-540-10843-2_34\" rel=\"nofollow noopener\" target=\"_blank\"\u003e1981 paper\u003c/a\u003e, and it goes beyond simply providing librarians with organizational guidance. That’s because the problem also applies to the arrangement of files on hard drives and in databases, where the items to be arranged could number in the billions. An inefficient system means significant wait times and major computational expense. Researchers have invented some efficient methods for storing items, but they’ve long wanted to determine the best possible way.\u003c/p\u003e\u003cp\u003eLast year, in \u003ca data-offer-url=\"https://arxiv.org/abs/2405.00807\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://arxiv.org/abs/2405.00807\u0026#34;}\" href=\"https://arxiv.org/abs/2405.00807\" rel=\"nofollow noopener\" target=\"_blank\"\u003ea study\u003c/a\u003e that was presented at the Foundations of Computer Science conference in Chicago, a team of seven researchers described a way to organize items that comes tantalizingly close to the theoretical ideal. The new approach combines a little knowledge of the bookshelf’s past contents with the surprising power of randomness.\u003c/p\u003e\u003cp\u003e“It’s a very important problem,” said \u003ca href=\"https://web.eecs.umich.edu/~pettie/\" target=\"_blank\"\u003eSeth Pettie\u003c/a\u003e, a computer scientist at the University of Michigan, because many of the data structures we rely upon today store information sequentially. He called the new work “extremely inspired [and] easily one of my top three favorite papers of the year.”\u003c/p\u003e\u003ch2\u003eNarrowing Bounds\u003c/h2\u003e\u003cp\u003eSo how does one measure a well-sorted bookshelf? A common way is to see how long it takes to insert an individual item. Naturally, that depends on how many items there are in the first place, a value typically denoted by \u003cem\u003en\u003c/em\u003e. In the Isabel Allende example, when all the books have to move to accommodate a new one, the time it takes is proportional to \u003cem\u003en\u003c/em\u003e. The bigger the \u003cem\u003en\u003c/em\u003e, the longer it takes. That makes this an “upper bound” to the problem: It will never take longer than a time proportional to \u003cem\u003en\u003c/em\u003e to add one book to the shelf.\u003c/p\u003e\u003cp\u003eThe authors of the 1981 paper that ushered in this problem wanted to know if it was possible to design an algorithm with an average insertion time much less than \u003cem\u003en\u003c/em\u003e. And indeed, they proved that one could do better. They created an algorithm that was guaranteed to achieve an average insertion time proportional to (log \u003cem\u003en\u003c/em\u003e)\u003csup\u003e2\u003c/sup\u003e. This algorithm had two properties: It was “deterministic,” meaning that its decisions did not depend on any randomness, and it was also “smooth,” meaning that the books must be spread evenly within subsections of the shelf where insertions (or deletions) are made. The authors left open the question of whether the upper bound could be improved even further. For over four decades, no one managed to do so.\u003c/p\u003e\u003cp\u003eHowever, the intervening years did see improvements to the lower bound. While the upper bound specifies the maximum possible time needed to insert a book, the lower bound gives the fastest possible insertion time. To find a definitive solution to a problem, researchers strive to narrow the gap between the upper and lower bounds, ideally until they coincide. When that happens, the algorithm is deemed optimal—inexorably bounded from above and below, leaving no room for further refinement.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-journey-hook=\"client-content\" data-testid=\"BodyWrapper\"\u003e\u003cp\u003eIn 2004, a team of researchers found that the \u003ca data-offer-url=\"https://epubs.siam.org/doi/abs/10.1137/S0895480100315808?journalCode=sjdmec\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://epubs.siam.org/doi/abs/10.1137/S0895480100315808?journalCode=sjdmec\u0026#34;}\" href=\"https://epubs.siam.org/doi/abs/10.1137/S0895480100315808?journalCode=sjdmec\" rel=\"nofollow noopener\" target=\"_blank\"\u003ebest any algorithm could do\u003c/a\u003e for the library sorting problem—in other words, the ultimate lower bound—was log \u003cem\u003en\u003c/em\u003e. This result pertained to the most general version of the problem, applying to any algorithm of any type. Two of the same authors had already secured a result for a more specific version of the problem in 1990, showing that for any smooth algorithm, \u003ca data-offer-url=\"https://link.springer.com/chapter/10.1007/3-540-52846-6_87\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://link.springer.com/chapter/10.1007/3-540-52846-6_87\u0026#34;}\" href=\"https://link.springer.com/chapter/10.1007/3-540-52846-6_87\" rel=\"nofollow noopener\" target=\"_blank\"\u003ethe lower bound is significantly higher\u003c/a\u003e: (log \u003cem\u003en\u003c/em\u003e)\u003csup\u003e2\u003c/sup\u003e. And in 2012, another team \u003ca data-offer-url=\"https://dl.acm.org/doi/abs/10.1145/2213977.2214083\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://dl.acm.org/doi/abs/10.1145/2213977.2214083\u0026#34;}\" href=\"https://dl.acm.org/doi/abs/10.1145/2213977.2214083\" rel=\"nofollow noopener\" target=\"_blank\"\u003eproved the same lower bound\u003c/a\u003e, (log \u003cem\u003en\u003c/em\u003e)\u003csup\u003e2\u003c/sup\u003e, for any deterministic algorithm that does not use randomness at all.\u003c/p\u003e\u003cp\u003eThese results showed that for any smooth or deterministic algorithm, you could not achieve an average insertion time better than (log \u003cem\u003en\u003c/em\u003e)\u003csup\u003e2\u003c/sup\u003e, which was the same as the upper bound established in the 1981 paper. In other words, to improve that upper bound, researchers would need to devise a different kind of algorithm. “If you’re going to do better, you have to be randomized and non-smooth,” said \u003ca data-offer-url=\"https://www.cs.stonybrook.edu/people/faculty/michaelbender\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://www.cs.stonybrook.edu/people/faculty/michaelbender\u0026#34;}\" href=\"https://www.cs.stonybrook.edu/people/faculty/michaelbender\" rel=\"nofollow noopener\" target=\"_blank\"\u003eMichael Bender\u003c/a\u003e, a computer scientist at Stony Brook University.\u003c/p\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e\u003cp\u003eMichael Bender went after the library sorting problem using an approach that didn’t necessarily make intuitive sense.\u003c/p\u003e\n\u003c/span\u003e\u003cspan\u003ePhotograph: Courtesy of Michael Bender\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003cp\u003eBut getting rid of smoothness, which requires items to be spread apart more or less evenly, seemed like a mistake. (Remember the problems that arose from our initial example—the non-smooth configuration where all the books were clumped together on the left-hand side of the shelf.) And it also was not obvious how leaving things to random chance—essentially a coin toss—would help matters. “Intuitively, it wasn’t clear that was a direction that made sense,” Bender said.\u003c/p\u003e\u003cp\u003eNevertheless, in 2022, Bender and five colleagues decided to try out a randomized, non-smooth algorithm anyway, just to see whether it might offer any advantages.\u003c/p\u003e\u003ch2\u003eA Secret History\u003c/h2\u003e\u003cp\u003eIronically, progress came from another restriction. There are sound privacy or security reasons why you may want to use an algorithm that’s blind to the history of the bookshelf. “If I had \u003cem\u003e50 Shades of Grey\u003c/em\u003e on my bookshelf and took it off,” said \u003ca data-offer-url=\"https://csd.cmu.edu/people/faculty/william-kuszmaul\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://csd.cmu.edu/people/faculty/william-kuszmaul\u0026#34;}\" href=\"https://csd.cmu.edu/people/faculty/william-kuszmaul\" rel=\"nofollow noopener\" target=\"_blank\"\u003eWilliam Kuszmaul\u003c/a\u003e of Carnegie Mellon University, nobody would be able to tell.\u003c/p\u003e\u003cdiv data-testid=\"GenericCallout\"\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e\u003cp\u003eWilliam Kuszmaul, Bender and, others lowered the upper bound on the library sorting problem practically down to the ideal.\u003c/p\u003e\n\u003c/span\u003e\u003cspan\u003ePhotograph: Rose Silver\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eIn a 2022 paper, Bender, Kuszmaul, and four coauthors created just such an algorithm—one that was “history independent,” non-smooth, and randomized—which finally \u003ca data-offer-url=\"https://arxiv.org/abs/2203.02763\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://arxiv.org/abs/2203.02763\u0026#34;}\" href=\"https://arxiv.org/abs/2203.02763\" rel=\"nofollow noopener\" target=\"_blank\"\u003ereduced the 1981 upper bound\u003c/a\u003e, bringing the average insertion time down to (log \u003cem\u003en\u003c/em\u003e)\u003csup\u003e1.5\u003c/sup\u003e.\u003c/p\u003e\u003cp\u003eKuszmaul remembers being surprised that a tool normally used to ensure privacy could confer other benefits. “It’s as if you used cryptography to make your algorithm faster,” he said. “Which just seems kind of strange.”\u003c/p\u003e\u003c/div\u003e\u003cdiv data-journey-hook=\"client-content\" data-testid=\"BodyWrapper\"\u003e\u003cp\u003e\u003ca data-offer-url=\"https://www.cc.gatech.edu/people/helen-xu\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://www.cc.gatech.edu/people/helen-xu\u0026#34;}\" href=\"https://www.cc.gatech.edu/people/helen-xu\" rel=\"nofollow noopener\" target=\"_blank\"\u003eHelen Xu\u003c/a\u003e of the Georgia Institute of Technology, who was not part of this research team, was also impressed.  She said that the idea of using history independence for reasons other than security may have implications for many other types of problems.\u003c/p\u003e\u003ch2\u003eClosing the Gap\u003c/h2\u003e\u003cp\u003eBender, Kuszmaul, and others made an even bigger improvement with last year’s paper. They again broke the record, lowering the upper bound to (log \u003cem\u003en\u003c/em\u003e) times (log log \u003cem\u003en\u003c/em\u003e)\u003csup\u003e3\u003c/sup\u003e—equivalent to (log \u003cem\u003en\u003c/em\u003e)\u003csup\u003e1.000…1\u003c/sup\u003e. In other words, they came exceedingly close to the theoretical limit, the ultimate lower bound of log \u003cem\u003en\u003c/em\u003e.\u003c/p\u003e\u003cp\u003eOnce again, their approach was non-smooth and randomized, but this time their algorithm relied on a limited degree of history dependence. It looked at past trends to plan for future events, but only up to a point. Suppose, for instance, you’ve been getting a lot of books by authors whose last name starts with N—Nabokov, Neruda, Ng. The algorithm extrapolates from that and assumes more are probably coming, so it’ll leave a little extra space in the N section. But reserving too much space could lead to trouble if a bunch of A-name authors start pouring in. “The way we made it a good thing was by being strategically random about how much history to look at when we make our decisions,” Bender said.\u003c/p\u003e\u003cp\u003eThe result built on and transformed their previous work. It “uses randomness in a completely different way than the 2022 paper,” Pettie said.\u003c/p\u003e\u003cp\u003eThese papers collectively represent “a significant improvement” on the theory side, said \u003ca href=\"https://computerscience.uchicago.edu/people/brian-wheatman/\" target=\"_blank\"\u003eBrian Wheatman\u003c/a\u003e, a computer scientist at the University of Chicago. “And on the applied side, I think they have the potential for a big improvement as well.”\u003c/p\u003e\u003cp\u003eXu agrees. “In the past few years, there’s been interest in using data structures based on list labeling for storing and processing dynamic graphs,” she said. These advances would almost certainly make things faster.\u003c/p\u003e\u003cp\u003eMeanwhile, there’s more for theorists to contemplate. “We know that we can almost do log \u003cem\u003en\u003c/em\u003e,” Bender said, “[but] there’s still this tiny gap”—the diminutive log log \u003cem\u003en\u003c/em\u003e term that stands in the way of a complete solution. “We don’t know if the right thing to do is to lower the upper bound or raise the lower bound.”\u003c/p\u003e\u003cp\u003ePettie, for one, doesn’t expect the lower bound to change. “Usually in these situations, when you see a gap this close, and one of the bounds looks quite natural and the other looks unnatural, then the natural one is the right answer,” he said. It’s much more likely that any future improvements will affect the upper bound, bringing it all the way down to log \u003cem\u003en.\u003c/em\u003e “But the world’s full of weird surprises.”\u003c/p\u003e\u003chr/\u003e\u003cp\u003e\u003ca href=\"https://www.quantamagazine.org/new-book-sorting-algorithm-almost-reaches-perfection-20250124/\"\u003e\u003cem\u003eOriginal story\u003c/em\u003e\u003c/a\u003e \u003cem\u003ereprinted with permission from\u003c/em\u003e \u003ca href=\"https://www.quantamagazine.org\"\u003eQuanta Magazine\u003c/a\u003e, \u003cem\u003ean editorially independent publication of the\u003c/em\u003e \u003ca href=\"https://www.simonsfoundation.org\"\u003e\u003cem\u003eSimons Foundation\u003c/em\u003e\u003c/a\u003e \u003cem\u003ewhose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-02-16T07:00:00-05:00",
  "modifiedTime": "2025-02-16T12:00:00Z"
}
