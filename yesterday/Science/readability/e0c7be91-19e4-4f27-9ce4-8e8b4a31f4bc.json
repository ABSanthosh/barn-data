{
  "id": "e0c7be91-19e4-4f27-9ce4-8e8b4a31f4bc",
  "title": "AI Is Eating Data Center Power Demand—and It’s Only Getting Worse",
  "link": "https://www.wired.com/story/new-research-energy-electricity-artificial-intelligence-ai/",
  "description": "A new analysis of AI hardware being produced and how it is being used attempts to estimate the vast amount of electricity being consumed by AI.",
  "author": "Molly Taft",
  "published": "Thu, 22 May 2025 18:09:41 +0000",
  "source": "https://www.wired.com/feed/category/science/latest/rss",
  "categories": [
    "Science",
    "Science / Environment",
    "Power Hungry"
  ],
  "byline": "Molly Taft",
  "length": 6324,
  "excerpt": "A new analysis of AI hardware being produced and how it is being used attempts to estimate the vast amount of electricity being consumed by AI.",
  "siteName": "WIRED",
  "favicon": "",
  "text": "AI’s energy use already represents as much as 20 percent of global data-center power demand, research published Thursday in the journal Joule shows. That demand from AI, the research states, could double by the end of this year, comprising nearly half of all total data-center electricity consumption worldwide, excluding the electricity used for bitcoin mining.The new research is published in a commentary by Alex de Vries-Gao, the founder of Digiconomist, a research company that evaluates the environmental impact of technology. De Vries-Gao started Digiconomist in the late 2010s to explore the impact of bitcoin mining, another extremely energy-intensive activity, would have on the environment. Looking at AI, he says, has grown more urgent over the past few years because of the widespread adoption of ChatGPT and other large language models that use massive amounts of energy. According to his research, worldwide AI energy demand is now set to surpass demand from bitcoin mining by the end of this year.“The money that bitcoin miners had to get to where they are today is peanuts compared to the money that Google and Microsoft and all these big tech companies are pouring in [to AI],” he says. “This is just escalating a lot faster, and it’s a much bigger threat.”The development of AI is already having an impact on Big Tech’s climate goals. Tech giants have acknowledged in recent sustainability reports that AI is largely responsible for driving up their energy use. Google’s greenhouse gas emissions, for instance, have increased 48 percent since 2019, complicating the company’s goals of reaching net zero by 2030.“As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute,” Google’s 2024 sustainability report reads.Last month, the International Energy Agency released a report finding that data centers made up 1.5 percent of global energy use in 2024—around 415 terrawatt-hours, a little less than the yearly energy demand of Saudi Arabia. This number is only set to get bigger: Data centers’ electricity consumption has grown four times faster than overall consumption in recent years, while the amount of investment in data centers has nearly doubled since 2022, driven largely by massive expansions to account for new AI capacity. Overall, the IEA predicted that data center electricity consumption will grow to more than 900 TWh by the end of the decade.But there’s still a lot of unknowns about the share that AI, specifically, takes up in that current configuration of electricity use by data centers. Data centers power a variety of services—like hosting cloud services and providing online infrastructure—that aren’t necessarily linked to the energy-intensive activities of AI. Tech companies, meanwhile, largely keep the energy expenditure of their software and hardware private.Some attempts to quantify AI’s energy consumption have started from the user side: calculating the amount of electricity that goes into a single ChatGPT search, for instance. De Vries-Gao decided to look, instead, at the supply chain, starting from the production side to get a more global picture.The high computing demands of AI, De Vries-Gao says, creates a natural “bottleneck” in the current global supply chain around AI hardware, particularly around the Taiwan Semiconductor Manufacturing Company (TSMC), the undisputed leader in producing key hardware that can handle these needs. Companies like Nvidia outsource the production of their chips to TSMC, which also produces chips for other companies like Google and AMD. (Both TSMC and Nvidia declined to comment for this article.)De Vries-Gao used analyst estimates, earnings call transcripts, and device details to put together an approximate estimate of TSMC’s production capacity. He then looked at publicly available electricity consumption profiles of AI hardware and estimates on utilization rates of that hardware—which can vary based on what it’s being used for—to arrive at a rough figure of just how much of global data-center demand is taken up by AI. De Vries-Gao calculates that without increased production, AI will consume up to 82 terrawatt-hours of electricity this year—roughly around the same as the annual electricity consumption of a country like Switzerland. If production capacity for AI hardware doubles this year, as analysts have projected it will, demand could increase at a similar rate, representing almost half of all data center demand by the end of the year.Despite the amount of publicly available information used in the paper, a lot of what De Vries-Gao is doing is peering into a black box: We simply don’t know certain factors that affect AI’s energy consumption, like the utilization rates of every piece of AI hardware in the world or what machine learning activities they’re being used for, let alone how the industry might develop in the future.Sasha Luccioni, an AI and energy researcher and the climate lead at open-source machine-learning platform Hugging Face, cautioned about leaning too hard on some of the conclusions of the new paper, given the amount of unknowns at play. Luccioni, who was not involved in this research, says that when it comes to truly calculating AI’s energy use, disclosure from tech giants is crucial.“It’s because we don’t have the information that [researchers] have to do this,” she says. “That’s why the error bar is so huge.”And tech companies do keep this information. In 2022, Google published a paper on machine learning and electricity use, noting that machine learning was “10%–15% of Google’s total energy use” from 2019 to 2021, and predicted that with best practices, “by 2030 total carbon emissions from training will reduce.” However, since that paper—which was released before Google Gemini’s debut in 2023—Google has not provided any more detailed information about how much electricity ML uses. (Google declined to comment for this story.)“You really have to deep-dive into the semiconductor supply chain to be able to make any sensible statement about the energy demand of AI,” De Vries-Gao says. “If these big tech companies were just publishing the same information that Google was publishing three years ago, we would have a pretty good indicator” of AI’s energy use.",
  "image": "https://media.wired.com/photos/682de990e697c0df119ce1f1/191:100/w_1280,c_limit/GettyImages-2161394212.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-testid=\"ArticlePageChunks\"\u003e\u003cdiv data-journey-hook=\"client-content\" data-testid=\"BodyWrapper\"\u003e\u003cp\u003e\u003cspan\u003eAI’s energy use\u003c/span\u003e already represents as much as 20 percent of global data-center power demand, \u003ca data-offer-url=\"https://www.cell.com/joule/fulltext/S2542-4351(25)00142-4\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://www.cell.com/joule/fulltext/S2542-4351(25)00142-4\u0026#34;}\" href=\"https://www.cell.com/joule/fulltext/S2542-4351(25)00142-4\" rel=\"nofollow noopener\" target=\"_blank\"\u003eresearch published Thursday\u003c/a\u003e in the journal Joule shows. That demand from \u003ca href=\"https://wired.com/tag/artificial-intelligence/\"\u003eAI\u003c/a\u003e, the research states, could double by the end of this year, comprising nearly half of all total data-center \u003ca href=\"https://www.wired.com/tag/electricity/\"\u003eelectricity\u003c/a\u003e consumption worldwide, excluding the electricity used for \u003ca href=\"https://www.wired.com/tag/bitcoin/\"\u003ebitcoin\u003c/a\u003e mining.\u003c/p\u003e\u003cp\u003eThe new research is published in a commentary by Alex de Vries-Gao, the founder of Digiconomist, a research company that evaluates the environmental impact of technology. De Vries-Gao started Digiconomist in the late 2010s to explore the impact of bitcoin mining, another extremely energy-intensive activity, would have on the environment. Looking at AI, he says, has grown more urgent over the past few years because of the widespread adoption of ChatGPT and other large language models that use massive amounts of energy. According to his research, worldwide AI energy demand is now set to surpass demand from bitcoin mining by the end of this year.\u003c/p\u003e\u003cp\u003e“The money that bitcoin miners had to get to where they are today is peanuts compared to the money that \u003ca href=\"https://www.wired.com/tag/google/\"\u003eGoogle\u003c/a\u003e and \u003ca href=\"https://www.wired.com/tag/microsoft/\"\u003eMicrosoft\u003c/a\u003e and all these big tech companies are pouring in [to AI],” he says. “This is just escalating a lot faster, and it’s a much bigger threat.”\u003c/p\u003e\u003cp\u003eThe development of AI is already having an impact on Big Tech’s climate goals. Tech giants have \u003ca href=\"https://www.wired.com/story/ai-energy-demands-water-impact-internet-hyper-consumption-era/\"\u003eacknowledged in recent sustainability reports\u003c/a\u003e that AI is largely responsible for driving up their energy use. Google’s greenhouse gas emissions, for instance, have increased 48 percent since 2019, complicating the company’s goals of reaching net zero by 2030.\u003c/p\u003e\u003cp\u003e“As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute,” Google’s 2024 sustainability report \u003ca data-offer-url=\"https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf\u0026#34;}\" href=\"https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf\" rel=\"nofollow noopener\" target=\"_blank\"\u003ereads\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eLast month, the International Energy Agency released a \u003ca data-offer-url=\"https://www.iea.org/reports/energy-and-ai/executive-summary\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://www.iea.org/reports/energy-and-ai/executive-summary\u0026#34;}\" href=\"https://www.iea.org/reports/energy-and-ai/executive-summary\" rel=\"nofollow noopener\" target=\"_blank\"\u003ereport\u003c/a\u003e finding that data centers made up 1.5 percent of global energy use in 2024—around 415 terrawatt-hours, a little less than the yearly energy demand of Saudi Arabia. This number is only set to get bigger: Data centers’ electricity consumption has grown four times faster than overall consumption in recent years, while the amount of investment in data centers has nearly doubled since 2022, driven largely by massive expansions to account for new AI capacity. Overall, the IEA predicted that data center electricity consumption will grow to more than 900 TWh by the end of the decade.\u003c/p\u003e\u003cp\u003eBut there’s still a lot of unknowns about the share that AI, specifically, takes up in that current configuration of electricity use by data centers. Data centers power a variety of services—like hosting cloud services and providing online infrastructure—that aren’t necessarily linked to the energy-intensive activities of AI. Tech companies, meanwhile, largely keep the energy expenditure of their software and hardware private.\u003c/p\u003e\u003cp\u003eSome attempts to quantify AI’s energy consumption have started from the user side: calculating the amount of electricity that goes into a single ChatGPT search, for instance. De Vries-Gao decided to look, instead, at the supply chain, starting from the production side to get a more global picture.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-journey-hook=\"client-content\" data-testid=\"BodyWrapper\"\u003e\u003cp\u003eThe high computing demands of AI, De Vries-Gao says, creates a natural “bottleneck” in the current global supply chain around AI hardware, particularly around the Taiwan Semiconductor Manufacturing Company (TSMC), the undisputed leader in producing key hardware that can handle these needs. Companies like Nvidia outsource the production of their chips to TSMC, which also produces chips for other companies like Google and AMD. (Both TSMC and Nvidia declined to comment for this article.)\u003c/p\u003e\u003cp\u003eDe Vries-Gao used analyst estimates, earnings call transcripts, and device details to put together an approximate estimate of TSMC’s production capacity. He then looked at publicly available electricity consumption profiles of AI hardware and estimates on utilization rates of that hardware—which can vary based on what it’s being used for—to arrive at a rough figure of just how much of global data-center demand is taken up by AI. De Vries-Gao calculates that without increased production, AI will consume up to 82 terrawatt-hours of electricity this year—roughly around the same as the annual electricity consumption of a country like Switzerland. If production capacity for AI hardware doubles this year, as analysts have projected it will, demand could increase at a similar rate, representing almost half of all data center demand by the end of the year.\u003c/p\u003e\u003cp\u003eDespite the amount of publicly available information used in the paper, a lot of what De Vries-Gao is doing is peering into a black box: We simply don’t know certain factors that affect AI’s energy consumption, like the utilization rates of every piece of AI hardware in the world or what machine learning activities they’re being used for, let alone how the industry might develop in the future.\u003c/p\u003e\u003cp\u003eSasha Luccioni, an AI and energy researcher and the climate lead at open-source machine-learning platform Hugging Face, cautioned about leaning too hard on some of the conclusions of the new paper, given the amount of unknowns at play. Luccioni, who was not involved in this research, \u003ca data-offer-url=\"https://www.linkedin.com/feed/update/urn:li:activity:7329218056809779201/\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://www.linkedin.com/feed/update/urn:li:activity:7329218056809779201/\u0026#34;}\" href=\"https://www.linkedin.com/feed/update/urn:li:activity:7329218056809779201/\" rel=\"nofollow noopener\" target=\"_blank\"\u003esays\u003c/a\u003e that when it comes to truly calculating AI’s energy use, disclosure from tech giants is crucial.\u003c/p\u003e\u003cp\u003e“It’s because we don’t have the information that [researchers] have to do this,” she says. “That’s why the error bar is so huge.”\u003c/p\u003e\u003cp\u003eAnd tech companies \u003cem\u003edo\u003c/em\u003e keep this information. In 2022, Google published a paper on machine learning and electricity use, \u003ca data-offer-url=\"https://www.techrxiv.org/doi/full/10.36227/techrxiv.19139645.v1\" data-event-click=\"{\u0026#34;element\u0026#34;:\u0026#34;ExternalLink\u0026#34;,\u0026#34;outgoingURL\u0026#34;:\u0026#34;https://www.techrxiv.org/doi/full/10.36227/techrxiv.19139645.v1\u0026#34;}\" href=\"https://www.techrxiv.org/doi/full/10.36227/techrxiv.19139645.v1\" rel=\"nofollow noopener\" target=\"_blank\"\u003enoting\u003c/a\u003e that machine learning was “10%–15% of Google’s total energy use” from 2019 to 2021, and predicted that with best practices, “by 2030 total carbon emissions from training will reduce.” However, since that paper—which was released before Google Gemini’s debut in 2023—Google has not provided any more detailed information about how much electricity ML uses. (Google declined to comment for this story.)\u003c/p\u003e\u003cp\u003e“You really have to deep-dive into the semiconductor supply chain to be able to make any sensible statement about the energy demand of AI,” De Vries-Gao says. “If these big tech companies were just publishing the same information that Google was publishing three years ago, we would have a pretty good indicator” of AI’s energy use.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-05-22T14:09:41.416-04:00",
  "modifiedTime": "2025-05-22T18:09:41.416Z"
}
