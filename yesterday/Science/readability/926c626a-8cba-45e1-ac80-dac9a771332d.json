{
  "id": "926c626a-8cba-45e1-ac80-dac9a771332d",
  "title": "Human-like artificial intelligence may face greater blame for moral violations",
  "link": "https://www.sciencedaily.com/releases/2024/12/241218174943.htm",
  "description": "In a new study, participants tended to assign greater blame to artificial intelligences (AIs) involved in real-world moral transgressions when they perceived the AIs as having more human-like minds.",
  "author": "",
  "published": "Wed, 18 Dec 2024 17:49:43 EST",
  "source": "https://www.sciencedaily.com/rss/all.xml",
  "categories": null,
  "byline": "",
  "length": 2466,
  "excerpt": "In a new study, participants tended to assign greater blame to artificial intelligences (AIs) involved in real-world moral transgressions when they perceived the AIs as having more human-like minds.",
  "siteName": "ScienceDaily",
  "favicon": "",
  "text": "In a new study, participants tended to assign greater blame to artificial intelligences (AIs) involved in real-world moral transgressions when they perceived the AIs as having more human-like minds. Minjoo Joo of Sookmyung Women's University in Seoul, Korea, presents these findings in the open-access journal PLOS ONE on December 18, 2024. Prior research has revealed a tendency of people to blame AI for various moral transgressions, such as in cases of an autonomous vehicle hitting a pedestrian or decisions that caused medical or military harm. Additional research suggests that people tend to assign more blame to AIs perceived to be capable of awareness, thinking, and planning. People may be more likely to attribute such capacities to AIs they perceive as having human-like minds that can experience conscious feelings. On the basis of that earlier research, Joo hypothesized that AIs perceived as having human-like minds may receive a greater share of blame for a given moral transgression. To test this idea, Joo conducted several experiments in which participants were presented with various real-world instances of moral transgressions involving AIs -- such as racist auto-tagging of photos -- and were asked questions to evaluate their mind perception of the AI involved, as well as the extent to which they assigned blame to the AI, its programmer, the company behind it, or the government. In some cases, AI mind perception was manipulated by describing a name, age, height, and hobby for the AI. Across the experiments, participants tended to assign considerably more blame to an AI when they perceived it as having a more human-like mind. In these cases, when participants were asked to distribute relative blame, they tended to assign less blame to the involved company. But when asked to rate the level of blame independently for each agent, there was no reduction in blame assigned to the company. These findings suggest that AI mind perception is a critical factor contributing to blame attribution for transgressions involving AI. Additionally, Joo raises concerns about the potentially harmful consequences of misusing AIs as scapegoats and calls for further research on AI blame attribution. The author adds: \"Can AIs be held accountable for moral transgressions? This research shows that perceiving AI as human-like increases blame toward AI while reducing blame on human stakeholders, raising concerns about using AI as a moral scapegoat.\"",
  "image": "https://www.sciencedaily.com/images/scidaily-icon.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp id=\"first\"\u003eIn a new study, participants tended to assign greater blame to artificial intelligences (AIs) involved in real-world moral transgressions when they perceived the AIs as having more human-like minds. Minjoo Joo of Sookmyung Women\u0026#39;s University in Seoul, Korea, presents these findings in the open-access journal \u003cem\u003ePLOS ONE\u003c/em\u003e on December 18, 2024.\u003c/p\u003e\u003cdiv id=\"text\"\u003e\n\u003cp\u003ePrior research has revealed a tendency of people to blame AI for various moral transgressions, such as in cases of an autonomous vehicle hitting a pedestrian or decisions that caused medical or military harm. Additional research suggests that people tend to assign more blame to AIs perceived to be capable of awareness, thinking, and planning. People may be more likely to attribute such capacities to AIs they perceive as having human-like minds that can experience conscious feelings.\u003c/p\u003e\n\u003cp\u003eOn the basis of that earlier research, Joo hypothesized that AIs perceived as having human-like minds may receive a greater share of blame for a given moral transgression.\u003c/p\u003e\n\u003cp\u003eTo test this idea, Joo conducted several experiments in which participants were presented with various real-world instances of moral transgressions involving AIs -- such as racist auto-tagging of photos -- and were asked questions to evaluate their mind perception of the AI involved, as well as the extent to which they assigned blame to the AI, its programmer, the company behind it, or the government. In some cases, AI mind perception was manipulated by describing a name, age, height, and hobby for the AI.\u003c/p\u003e\n\u003cp\u003eAcross the experiments, participants tended to assign considerably more blame to an AI when they perceived it as having a more human-like mind. In these cases, when participants were asked to distribute relative blame, they tended to assign less blame to the involved company. But when asked to rate the level of blame independently for each agent, there was no reduction in blame assigned to the company.\u003c/p\u003e\n\u003cp\u003eThese findings suggest that AI mind perception is a critical factor contributing to blame attribution for transgressions involving AI. Additionally, Joo raises concerns about the potentially harmful consequences of misusing AIs as scapegoats and calls for further research on AI blame attribution.\u003c/p\u003e\n\u003cp\u003eThe author adds: \u0026#34;Can AIs be held accountable for moral transgressions? This research shows that perceiving AI as human-like increases blame toward AI while reducing blame on human stakeholders, raising concerns about using AI as a moral scapegoat.\u0026#34;\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": null,
  "modifiedTime": null
}
