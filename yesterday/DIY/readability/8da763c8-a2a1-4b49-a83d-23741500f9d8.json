{
  "id": "8da763c8-a2a1-4b49-a83d-23741500f9d8",
  "title": "Will it Run Llama 2? Now DOS Can",
  "link": "https://hackaday.com/2025/04/19/will-it-run-llama-2-now-dos-can/",
  "description": "Will a 486 run Crysis? No, of course not. Will it run a large language model (LLM)? Given the huge buildout of compute power to do just that, many people …read more",
  "author": "Tyler August",
  "published": "Sat, 19 Apr 2025 11:00:23 +0000",
  "source": "https://hackaday.com/blog/feed/",
  "categories": [
    "Artificial Intelligence",
    "Retrocomputing",
    "ai",
    "dos 6.22",
    "i386",
    "llama.2c",
    "LLM",
    "retrocomputing"
  ],
  "byline": "",
  "length": 1804,
  "excerpt": "Will a 486 run Crysis? No, of course not. Will it run a large language model (LLM)? Given the huge buildout of compute power to do just that, many people would scoff at the very notion. But [Yeo Kh…",
  "siteName": "Hackaday",
  "favicon": "https://hackaday.com/wp-content/themes/hackaday-2/img/hackaday-logo_1024x1024.png?v=3",
  "text": "Skip to content Will a 486 run Crysis? No, of course not. Will it run a large language model (LLM)? Given the huge buildout of compute power to do just that, many people would scoff at the very notion. But [Yeo Kheng Meng] is not many people. He has set up various DOS computers to run a stripped down version of the Llama 2 LLM, originally from Meta. More specifically, [Yeo Kheng Meng] is implementing [Andreq Karpathy]’s Llama2.c library, which we have seen here before, running on Windows 98. Llama2.c is a wonderful bit of programming that lets one inference a trained Llama2 model in only seven hundred lines of C. It it is seven hundred lines of modern C, however, so porting to DOS 6.22 and the outdated i386 architecture took some doing. [Yeo Kheng Meng] documents that work, and benchmarks a few retrocomputers. As painful as it may be to say — yes, a 486 or a Pentium 1 can now be counted as “retro”. The models are not large, of course, with TinyStories-trained  260 kB model churning out a blistering 2.08 tokens per second on a generic 486 box. Newer machines can run larger models faster, of course. Ironically a Pentium M Thinkpad T24 (was that really 21 years ago?) is able to run a larger 110 Mb model faster than [Yeo Kheng Meng]’s modern Ryzen 5 desktop. Not because the Pentium M is going blazing fast, mind you, but because a memory allocation error prevented that model from running on the modern CPU. Slow and steady finishes the race, it seems. This port will run on any 32-bit i386 hardware, which leaves the 16-bit regime as the next challenge. If one of you can get an Llama 2 hosted locally on an 286 or a 68000-based machine, then we may have to stop asking “Does it run DOOM?” and start asking “Will it run an LLM?”",
  "image": "https://hackaday.com/wp-content/uploads/2025/04/dosllam2-usage.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"page\"\u003e\n    \n\n    \u003cp\u003e\u003ca href=\"#content\"\u003eSkip to content\u003c/a\u003e\u003c/p\u003e\n\n    \u003cdiv id=\"content\"\u003e\n        \u003cmain id=\"main\" role=\"main\"\u003e\n\n        \n            \n\u003carticle itemscope=\"\" itemtype=\"http://schema.org/Article\" id=\"post-771750\"\u003e\n    \n\n    \u003cdiv itemprop=\"articleBody\"\u003e\n        \u003cp\u003eWill a 486 run \u003cem\u003eCrysis\u003c/em\u003e? No, of course not. Will it run a large language model (LLM)? Given the huge buildout of compute power to do just that, many people would scoff at the very notion. But [Yeo Kheng Meng] \u003ca href=\"https://yeokhengmeng.com/2025/04/llama2-llm-on-dos/\" target=\"_blank\"\u003eis not many people.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHe has set up various DOS computers to run a stripped down version of the \u003ca href=\"https://hackaday.com/2023/03/22/why-llama-is-a-big-deal/\"\u003eLlama 2 LLM, originally from Meta\u003c/a\u003e. More specifically, [Yeo Kheng Meng] is implementing [Andreq Karpathy]’s Llama2.c library, which we have seen here before, \u003ca href=\"https://hackaday.com/2025/01/13/modern-ai-on-vintage-hardware-llama-2-runs-on-windows-98/\"\u003erunning on Windows 98\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hackaday.com/wp-content/uploads/2023/03/Llama_thumbnail.png\" target=\"_blank\"\u003e\u003cimg decoding=\"async\" data-attachment-id=\"582238\" data-permalink=\"https://hackaday.com/2025/04/19/will-it-run-llama-2-now-dos-can/llama_thumbnail/\" data-orig-file=\"https://hackaday.com/wp-content/uploads/2023/03/Llama_thumbnail.png\" data-orig-size=\"1200,1200\" data-comments-opened=\"1\" data-image-meta=\"{\u0026#34;aperture\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;credit\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;camera\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;caption\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;created_timestamp\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;copyright\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;focal_length\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;iso\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;shutter_speed\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;orientation\u0026#34;:\u0026#34;0\u0026#34;}\" data-image-title=\"Llama_thumbnail\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://hackaday.com/wp-content/uploads/2023/03/Llama_thumbnail.png?w=400\" data-large-file=\"https://hackaday.com/wp-content/uploads/2023/03/Llama_thumbnail.png?w=625\" src=\"https://hackaday.com/wp-content/uploads/2023/03/Llama_thumbnail.png?w=400\" alt=\"\" width=\"300\" height=\"300\" srcset=\"https://hackaday.com/wp-content/uploads/2023/03/Llama_thumbnail.png 1200w, https://hackaday.com/wp-content/uploads/2023/03/Llama_thumbnail.png?resize=250,250 250w, https://hackaday.com/wp-content/uploads/2023/03/Llama_thumbnail.png?resize=400,400 400w, https://hackaday.com/wp-content/uploads/2023/03/Llama_thumbnail.png?resize=625,625 625w\" sizes=\"(max-width: 300px) 100vw, 300px\"/\u003e\u003c/a\u003eLlama2.c is a wonderful bit of programming that lets one inference a trained Llama2 model in only seven hundred lines of C. It it is seven hundred lines of \u003cem\u003emodern\u003c/em\u003e C, however, so porting to DOS 6.22 and the outdated i386 architecture took some doing. [Yeo Kheng Meng] documents that work, and benchmarks a few retrocomputers. As painful as it may be to say — yes, a 486 or a Pentium 1 can now be counted as “retro”.\u003c/p\u003e\n\u003cp\u003eThe models are not large, of course, with TinyStories-trained  260 kB model churning out a blistering 2.08 tokens per second on a generic 486 box. Newer machines can run larger models faster, of course. Ironically a Pentium M Thinkpad T24 (was that \u003cem\u003ereally\u003c/em\u003e 21 years ago?) is able to run a larger 110 Mb model faster than [Yeo Kheng Meng]’s modern Ryzen 5 desktop. Not because the Pentium M is going blazing fast, mind you, but because a memory allocation error prevented that model from running on the modern CPU. Slow and steady finishes the race, it seems.\u003c/p\u003e\n\u003cp\u003eThis port will run on any 32-bit i386 hardware, which leaves the 16-bit regime as the next challenge. If one of you can get an Llama 2 hosted locally on an 286 or a 68000-based machine, then we may have to stop asking \u003ca href=\"https://hackaday.com/2025/02/27/what-game-should-replace-doom-as-the-meme-port-of-choice/\"\u003e“Does it run \u003cem\u003eDOOM\u003c/em\u003e?”\u003c/a\u003e and start asking “Will it run an LLM?”\u003c/p\u003e\n\n\u003cp\u003e\u003ciframe title=\"DOS Llama 2 local LLM client (ft. Thinkpad T42 and Toshiba Satellite 315CDT)\" width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/4241obgG_QI?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\t            \u003c/div\u003e\n    \u003cul\u003e\n    \t\t\t\t\t\u003cli\u003e\n    \t\t\t\t\u003ca href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fhackaday.com%2F2025%2F04%2F19%2Fwill-it-run-llama-2-now-dos-can%2F\" target=\"_blank\"\u003e\n    \t\t\t\t\t\u003ci\u003e\u003cimg src=\"https://hackaday.com/wp-content/themes/hackaday-2/img/share_face.png\"/\u003e \u003c/i\u003e\n    \t\t\t\t\t\t\t\t\t\u003c/a\u003e\n    \t\t\t\u003c/li\u003e\n    \t\t\t\t\t\u003cli\u003e\n                        \u003ca href=\"https://twitter.com/intent/tweet?text=Will%20It%20Run%20Llama%202?%20Now%20DOS%20Can%20via%20@hackaday\u0026amp;url=https://hackaday.com/2025/04/19/will-it-run-llama-2-now-dos-can/\" target=\"_blank\"\u003e\n    \t\t\t\t\t\u003ci\u003e\u003cimg src=\"https://hackaday.com/wp-content/themes/hackaday-2/img/share_twitter.png\"/\u003e\u003c/i\u003e\n    \t\t\t\t\t\t\t\t\t\u003c/a\u003e\n    \t\t\t\u003c/li\u003e\n    \t\t\t\t\t\u003cli\u003e\n    \t\t\t\t\u003ca href=\"https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fhackaday.com%2F2025%2F04%2F19%2Fwill-it-run-llama-2-now-dos-can%2F\" target=\"_blank\"\u003e\n    \t\t\t\t\t\u003ci\u003e\u003cimg src=\"https://hackaday.com/wp-content/themes/hackaday-2/img/share_in.png\"/\u003e\u003c/i\u003e\n    \t\t\t\t\t\t\t\t\t\u003c/a\u003e\n    \t\t\t\u003c/li\u003e\n    \t\t\t\t\t\u003cli\u003e\n                \u003ca href=\"mailto:?subject=Will+It+Run+Llama+2%3F+Now+DOS+Can | Hackaday\u0026amp;body=https%3A%2F%2Fhackaday.com%2F2025%2F04%2F19%2Fwill-it-run-llama-2-now-dos-can%2F\"\u003e\n    \t\t\t\t\t\u003ci\u003e\u003cimg src=\"https://hackaday.com/wp-content/themes/hackaday-2/img/share_mail1.png\"/\u003e\u003c/i\u003e\n    \t\t\t\t\t\t\t\t\t\u003c/a\u003e\n    \t\t\t\u003c/li\u003e\n    \t\t\t\u003c/ul\u003e\n    \n\u003c/article\u003e\n\n            \t\n\t\n            \n\n            \n\n\n        \n        \n\n        \n        \n\n        \n        \u003c/main\u003e\n    \u003c/div\u003e\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2025-04-19T11:00:23Z",
  "modifiedTime": "2025-04-19T05:54:42Z"
}
