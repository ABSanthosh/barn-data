{
  "id": "06bdf501-269a-4311-8aa1-14390d5f82f3",
  "title": "More Details On Why DeepSeek is a Big Deal",
  "link": "https://hackaday.com/2025/02/03/more-details-on-why-deepseek-is-a-big-deal/",
  "description": "The DeepSeek large language models (LLM) have been making headlines lately, and for more than one reason. IEEE Spectrum has an article that sums everything up very nicely. We shared …read more",
  "author": "Donald Papp",
  "published": "Tue, 04 Feb 2025 00:00:00 +0000",
  "source": "https://hackaday.com/blog/feed/",
  "categories": [
    "Artificial Intelligence",
    "Machine Learning",
    "ai",
    "Deepseek",
    "LLM",
    "reasoning"
  ],
  "byline": "",
  "length": 2615,
  "excerpt": "The DeepSeek large language models (LLM) have been making headlines lately, and for more than one reason. IEEE Spectrum has an article that sums everything up very nicely. We shared the way DeepSee…",
  "siteName": "Hackaday",
  "favicon": "https://hackaday.com/wp-content/themes/hackaday-2/img/hackaday-logo_1024x1024.png?v=3",
  "text": "Skip to content The DeepSeek large language models (LLM) have been making headlines lately, and for more than one reason. IEEE Spectrum has an article that sums everything up very nicely. We shared the way DeepSeek made a splash when it came onto the AI scene not long ago, and this is a good opportunity to go into a few more details of why this has been such a big deal. For one thing, DeepSeek (there’s actually two flavors, -V3 and -R1, more on them in a moment) punches well above its weight. DeepSeek is the product of an innovative development process, and freely available to use or modify. It is also indirectly highlighting the way companies in this space like to label their LLM offerings as “open” or “free”, but stop well short of actually making them open source. The DeepSeek-V3 LLM was developed in China and reportedly cost less than 6 million USD to train. This was possible thanks to developing DualPipe, a highly optimized and scalable method of training the system despite limitations due to export restrictions on Nvidia hardware. Details are in the technical paper for DeepSeek-V3. There’s also DeepSeek-R1, a chain-of-thought “reasoning” model which handily provides its thought process enclosed within easily-parsed \u003cthink\u003e and \u003c/think\u003e pseudo-tags that are included in its responses. A model like this takes an iterative step-by-step approach to formulating responses, and benefits from prompts that provide a clear goal the LLM can aim for. The way DeepSeek-R1 was created was itself novel. Its training started with supervised fine-tuning (SFT) which is a human-led, intensive process as a “cold start” which eventually handed off to a more automated reinforcement learning (RL) process with a rules-based reward system. The result avoided problems that come from relying too much on RL, while minimizing the human effort of SFT. Technical details on the process of training DeepSeek-R1 are here. DeepSeek-V3 and -R1 are freely available in the sense that one can access the full-powered models online or via an app, or download distilled models for local use on more limited hardware. It is free and open as in accessible, but not open source because not everything needed to replicate the work is actually released. Like with most LLMs, the training data and actual training code used are not available. What is released and making waves of its own are the technical details of how researchers produced what they did, and that means there are efforts to try to make an actually open source version. Keep an eye out for Open-R1!",
  "image": "https://hackaday.com/wp-content/uploads/2022/06/ai_feat.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"page\"\u003e\n    \n\n    \u003cp\u003e\u003ca href=\"#content\"\u003eSkip to content\u003c/a\u003e\u003c/p\u003e\n\n    \u003cdiv id=\"content\"\u003e\n        \u003cmain id=\"main\" role=\"main\"\u003e\n\n        \n            \n\u003carticle itemscope=\"\" itemtype=\"http://schema.org/Article\" id=\"post-757435\"\u003e\n    \n\n    \u003cdiv itemprop=\"articleBody\"\u003e\n        \u003cp\u003eThe DeepSeek large language models (LLM) have been making headlines lately, and for more than one reason. \u003ca href=\"https://spectrum.ieee.org/deepseek\" target=\"_blank\"\u003eIEEE Spectrum has an article that sums everything up very nicely\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe shared the way \u003ca href=\"https://hackaday.com/2025/01/27/new-open-source-deepseek-v3-language-model-making-waves/\"\u003eDeepSeek made a splash\u003c/a\u003e when it came onto the AI scene not long ago, and this is a good opportunity to go into a few more details of why this has been such a big deal.\u003c/p\u003e\n\u003cp\u003eFor one thing, DeepSeek (there’s actually two flavors, -V3 and -R1, more on them in a moment) punches well above its weight. DeepSeek is the product of an innovative development process, and freely available to use or modify. It is also indirectly highlighting the way companies in this space like to label their LLM offerings as “open” or “free”, but stop well short of actually making them open source.\u003c/p\u003e\n\u003cp\u003eThe DeepSeek-V3 LLM was developed in China and reportedly cost less than 6 million USD to train. This was possible thanks to developing \u003cstrong\u003eDualPipe\u003c/strong\u003e, a highly optimized and scalable method of training the system despite limitations due to export restrictions on Nvidia hardware. \u003ca href=\"https://arxiv.org/html/2412.19437v1\" target=\"_blank\"\u003eDetails are in the technical paper for DeepSeek-V3\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThere’s also \u003ca href=\"https://github.com/deepseek-ai/DeepSeek-R1/tree/main\" target=\"_blank\"\u003eDeepSeek-R1\u003c/a\u003e, a chain-of-thought “reasoning” model which handily provides its thought process enclosed within easily-parsed \u003ccode\u003e\u0026lt;think\u0026gt;\u003c/code\u003e and \u003ccode\u003e\u0026lt;/think\u0026gt;\u003c/code\u003e pseudo-tags that are included in its responses. A model like this takes an iterative step-by-step approach to formulating responses, and benefits from prompts that provide a clear goal the LLM can aim for. The way DeepSeek-R1 was created was itself novel. Its training started with supervised fine-tuning (SFT) which is a human-led, intensive process as a “cold start” which eventually handed off to a more automated reinforcement learning (RL) process with a rules-based reward system. The result avoided problems that come from relying too much on RL, while minimizing the human effort of SFT. Technical \u003ca href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\" target=\"_blank\"\u003edetails on the process of training DeepSeek-R1 are here\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eDeepSeek-V3 and -R1 are freely available in the sense that one can access the full-powered models online or via an app, or download distilled models for local use on more limited hardware. It is free and open as in accessible, but not \u003cem\u003eopen source\u003c/em\u003e because not everything needed to replicate the work is actually released. Like with most LLMs, the training data and actual training code used are not available.\u003c/p\u003e\n\u003cp\u003eWhat \u003cem\u003eis\u003c/em\u003e released and making waves of its own are the technical details of how researchers produced what they did, and that means there are efforts to try to make an actually open source version. Keep an eye out for \u003ca href=\"https://huggingface.co/blog/open-r1\" target=\"_blank\"\u003eOpen-R1\u003c/a\u003e!\u003c/p\u003e\n\t            \u003c/div\u003e\n    \u003cul\u003e\n    \t\t\t\t\t\u003cli\u003e\n    \t\t\t\t\u003ca href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fhackaday.com%2F2025%2F02%2F03%2Fmore-details-on-why-deepseek-is-a-big-deal%2F\" target=\"_blank\"\u003e\n    \t\t\t\t\t\u003ci\u003e\u003cimg src=\"https://hackaday.com/wp-content/themes/hackaday-2/img/share_face.png\"/\u003e \u003c/i\u003e\n    \t\t\t\t\t\t\t\t\t\u003c/a\u003e\n    \t\t\t\u003c/li\u003e\n    \t\t\t\t\t\u003cli\u003e\n                        \u003ca href=\"https://twitter.com/intent/tweet?text=More%20Details%20On%20Why%20DeepSeek%20Is%20A%20Big%20Deal%20via%20@hackaday\u0026amp;url=https://hackaday.com/2025/02/03/more-details-on-why-deepseek-is-a-big-deal/\" target=\"_blank\"\u003e\n    \t\t\t\t\t\u003ci\u003e\u003cimg src=\"https://hackaday.com/wp-content/themes/hackaday-2/img/share_twitter.png\"/\u003e\u003c/i\u003e\n    \t\t\t\t\t\t\t\t\t\u003c/a\u003e\n    \t\t\t\u003c/li\u003e\n    \t\t\t\t\t\u003cli\u003e\n    \t\t\t\t\u003ca href=\"https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fhackaday.com%2F2025%2F02%2F03%2Fmore-details-on-why-deepseek-is-a-big-deal%2F\" target=\"_blank\"\u003e\n    \t\t\t\t\t\u003ci\u003e\u003cimg src=\"https://hackaday.com/wp-content/themes/hackaday-2/img/share_in.png\"/\u003e\u003c/i\u003e\n    \t\t\t\t\t\t\t\t\t\u003c/a\u003e\n    \t\t\t\u003c/li\u003e\n    \t\t\t\t\t\u003cli\u003e\n                \u003ca href=\"mailto:?subject=More+Details+On+Why+DeepSeek+Is+A+Big+Deal | Hackaday\u0026amp;body=https%3A%2F%2Fhackaday.com%2F2025%2F02%2F03%2Fmore-details-on-why-deepseek-is-a-big-deal%2F\"\u003e\n    \t\t\t\t\t\u003ci\u003e\u003cimg src=\"https://hackaday.com/wp-content/themes/hackaday-2/img/share_mail1.png\"/\u003e\u003c/i\u003e\n    \t\t\t\t\t\t\t\t\t\u003c/a\u003e\n    \t\t\t\u003c/li\u003e\n    \t\t\t\u003c/ul\u003e\n    \n\u003c/article\u003e\n\n            \t\n\t\n            \n\n            \n\n\n        \n        \n\n        \n        \n\n        \n        \u003c/main\u003e\n    \u003c/div\u003e\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-02-04T00:00:00Z",
  "modifiedTime": "2025-02-03T19:27:09Z"
}
