{
  "id": "98037e0a-bcab-483f-87dc-68ddb6699969",
  "title": "Apple sued over 2022 dropping of CSAM detection features",
  "link": "https://appleinsider.com/articles/24/12/08/apple-sued-over-2022-dropping-of-csam-detection-features?utm_medium=rss",
  "description": "A victim of childhood sexual abuse is suing Apple over its 2022 dropping of a previously-announced plan to scan images stored in iCloud for child sexual abuse material.Apple has retained nudity detection in images, but dropped some CSAM protection features in 2022.Apple originally introduced a plan in late 2021 to protect users from child sexual abuse material (CSAM) by scanning uploaded images on-device using a hashtag system. It would also warn users before sending or receiving photos with algorithically-detected nudity.The nudity-detection feature, called Communication Safety, is still in place today. However, Apple dropped its plan for CSAM detection after backlash from privacy experts, child safety groups, and governments. Continue Reading on AppleInsider | Discuss on our Forums",
  "author": "Charles Martin",
  "published": "Sun, 08 Dec 2024 21:46:32 +0000",
  "source": "https://appleinsider.com/rss/news/",
  "categories": null,
  "byline": "Charles Martin",
  "length": 4033,
  "excerpt": "A victim of childhood sexual abuse is suing Apple over its 2022 dropping of a previously-announced plan to scan images stored in iCloud for child sexual abuse material.",
  "siteName": "AppleInsider",
  "favicon": "https://photos5.appleinsider.com/v10/images/favicon-ai-144.png",
  "text": "Apple has retained nudity detection in images, but dropped some CSAM protection features in 2022. A victim of childhood sexual abuse is suing Apple over its 2022 dropping of a previously-announced plan to scan images stored in iCloud for child sexual abuse material. Apple originally introduced a plan in late 2021 to protect users from child sexual abuse material (CSAM) by scanning uploaded images on-device using a hashtag system. It would also warn users before sending or receiving photos with algorithically-detected nudity. The nudity-detection feature, called Communication Safety, is still in place today. However, Apple dropped its plan for CSAM detection after backlash from privacy experts, child safety groups, and governments. A 27-year-old woman, who was a victim of sexual abuse as a child by a relative, is suing Apple using a court-allowed pseudonym for stopping the CSAM-detecting feature. She says she previously received a law-enforcement notice that the images of her abuse were being stored on iCloud via a MacBook seized in Vermont when the feature was active. In her lawsuit, she says Apple broke its promise to protect victims like her when it eliminated the CSAM-scanning feature from iCloud. By doing so, she says that Apple has allowed that material to be shared extensively. Therefore, Apple is selling \"defective products that harmed a class of customers\" like herself. More victims join lawsuit The woman's lawsuit against Apple demands changes to Apple practices, and potential compensation to a group of up to 2,680 other eligible victims, according to one of the woman's lawyers. The lawsuit notes that CSAM-scanning features used by Google and Meta's Facebook catch far more illegal material than Apple's anti-nudity feature does. Under current law, victims of child sexual abuse can be compensated at a minimum amount of $150,000. If all of the potential plaintiffs in the woman's lawsuit were to win compensation, damages could exceed $1.2 billion for Apple if it is found liable. In a related case, attorneys acting on behalf of a nine-year-old CSAM victim sued Apple in a North Carolina court in August. In that case, the girl says strangers sent her CSAM videos through iCloud links, and \"encouraged her to film and upload\" similar videos, according to The New York Times, which reported on both cases. Apple filed a motion to dismiss the North Carolina case, noting that Section 230 of the federal code protects it from liability for material uploaded to iCloud by its users. It also said that it was protected from product liability claims because iCloud isn't a standalone product. Court rulings soften Section 230 protection Recent court rulings, however, could work against Apple's claims to avoid liability. The US Court of Appeals for the Ninth Circuit has determined that such defenses can only apply to active content moderation, rather than as a blanket protection from possible liability. Apple spokesman Fred Sainz said in response to the new lawsuit that Apple believes \"child sexual abuse material is abhorrent, and we are committed to fighting the ways predators put children at risk.\" Sainz added that \"we are urgently and actively innovating to combat these crimes without compromising the security and privacy of all our users.\" He pointed to the expansion of the nudity-detecting features to its Messages app, along with the ability for users to report harmful material to Apple. The woman behind the lawsuit and her lawyer, Margaret Mabie, do not agree that Apple has done enough. In preparation for the case, Mabie dug through law enforcement reports and other documents to find cases related to her clients' images and Apple's products. Mabie eventually built a list of more than 80 examples of the images being shared. One of the people sharing the images was a Bay Area man who was caught with more than 2,000 illegal images and videos stored in iCloud, the Times noted.",
  "image": "https://photos5.appleinsider.com/gallery/61979-128378-CSAM-features-xl.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"article-hero\" aria-labelledby=\"hero-cap\" role=\"figure\"\u003e\n                          \u003cp id=\"hero-cap\" title=\"Apple has retained nudity detection in images, but dropped some CSAM protection features in 2022.\"\u003eApple has retained nudity detection in images, but dropped some CSAM protection features in 2022.\u003c/p\u003e\n                                    \u003cp\u003e\u003ca href=\"https://photos5.appleinsider.com/gallery/61979-128378-CSAM-features-xl.jpg\"\u003e\n              \u003cimg fetchpriority=\"high\" src=\"https://photos5.appleinsider.com/gallery/61979-128378-CSAM-features-xl.jpg\" alt=\"\"/\u003e\n            \u003c/a\u003e\n                      \u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\n          \u003cp\u003eA victim of childhood sexual abuse is suing Apple over its 2022 dropping of a previously-announced plan to scan images stored in \u003ca href=\"https://appleinsider.com/inside/icloud\" title=\"iCloud\" data-kpt=\"1\"\u003eiCloud\u003c/a\u003e for child sexual abuse material.\n\u003c/p\u003e\u003cp\u003eApple originally introduced a plan \u003ca href=\"https://appleinsider.com/articles/21/08/05/apple-expanding-child-safety-features-across-imessage-siri-icloud-photos\"\u003ein late 2021\u003c/a\u003e to protect users from child sexual abuse material (CSAM) by scanning uploaded images on-device using a hashtag system. It would also warn users before sending or receiving photos with algorithically-detected nudity.\n\u003c/p\u003e\u003cp\u003eThe nudity-detection feature, called Communication Safety, is still in place today. However, Apple dropped its plan for CSAM detection \u003ca href=\"https://appleinsider.com/articles/22/12/07/apples-plan-to-scan-iphone-photos-for-child-abuse-material-is-dead\"\u003eafter backlash\u003c/a\u003e from privacy experts, child safety groups, and governments.\n\u003c/p\u003e\u003cp\u003eA 27-year-old woman, who was a victim of sexual abuse as a child by a relative, is suing Apple using a court-allowed pseudonym for stopping the CSAM-detecting feature. She says she previously received a law-enforcement notice that the images of her abuse were being stored on iCloud via a MacBook seized in Vermont when the feature was active.\n\u003c/p\u003e\u003cp\u003eIn her lawsuit, she says Apple broke its promise to protect victims like her when it eliminated the CSAM-scanning feature from iCloud. By doing so, she says that Apple has allowed that material to be shared extensively.\n\u003c/p\u003e\u003cp\u003eTherefore, Apple is selling \u0026#34;defective products that harmed a class of customers\u0026#34; like herself.\n\u003c/p\u003e\u003cp\u003e\u003ch2 data-anchor=\"more-victims-join-lawsuit\" id=\"more-victims-join-lawsuit\"\u003eMore victims join lawsuit\u003c/h2\u003e\n\u003c/p\u003e\u003cp\u003eThe woman\u0026#39;s lawsuit against Apple demands changes to Apple practices, and potential compensation to a group of up to 2,680 other eligible victims, according to one of the woman\u0026#39;s lawyers. The lawsuit notes that CSAM-scanning features used by Google and Meta\u0026#39;s Facebook catch far more illegal material than Apple\u0026#39;s anti-nudity feature does.\n\u003c/p\u003e\u003cp\u003eUnder current law, victims of child sexual abuse can be compensated at a minimum amount of $150,000. If all of the potential plaintiffs in the woman\u0026#39;s lawsuit were to win compensation, damages could exceed $1.2 billion for Apple if it is found liable.\n\u003c/p\u003e\u003cp\u003eIn a related case, attorneys acting on behalf of a nine-year-old CSAM victim sued Apple in a North Carolina court \u003ca href=\"https://appleinsider.com/articles/24/08/15/apple-accused-of-using-privacy-to-excuse-ignoring-child-abuse-material-on-icloud\"\u003ein August\u003c/a\u003e. In that case, the girl says strangers sent her CSAM videos through iCloud links, and \u0026#34;encouraged her to film and upload\u0026#34; similar videos, according to \u003cem\u003eThe New York Times\u003c/em\u003e, which \u003ca href=\"https://www.nytimes.com/apple-child-sexual-abuse-material-lawsuit.html\"\u003ereported on both cases.\n\u003c/a\u003e\u003c/p\u003e\u003cp\u003eApple filed a motion to dismiss the North Carolina case, noting that Section 230 of the federal code protects it from liability for material uploaded to iCloud by its users. It also said that it was protected from product liability claims because iCloud isn\u0026#39;t a standalone product.\n\u003c/p\u003e\u003cp\u003e\u003ch2 data-anchor=\"court-rulings-soften-section-230-protection\" id=\"court-rulings-soften-section-230-protection\"\u003eCourt rulings soften Section 230 protection\u003c/h2\u003e\n\u003c/p\u003e\u003cp\u003eRecent court rulings, however, could work against Apple\u0026#39;s claims to avoid liability. The US Court of Appeals for the Ninth Circuit has \u003ca href=\"https://jolt.law.harvard.edu/digest/does-v-reddit-ninth-circuits-narrow-reading-of-the-sex-trafficking-exception-further-complicates-debate-over-section-230-immunity\"\u003edetermined\u003c/a\u003e that such defenses can only apply to active content moderation, rather than as a blanket protection from possible liability.\n\u003c/p\u003e\u003cp\u003eApple spokesman Fred Sainz said in response to the new lawsuit that Apple believes \u0026#34;child sexual abuse material is abhorrent, and we are committed to fighting the ways predators put children at risk.\u0026#34;\n\u003c/p\u003e\u003cp\u003eSainz added that \u0026#34;we are urgently and actively innovating to combat these crimes without compromising the security and privacy of all our users.\u0026#34;\n\u003c/p\u003e\u003cp\u003eHe pointed to the expansion of the nudity-detecting features to \u003ca href=\"https://appleinsider.com/articles/21/11/09/new-ios-15-beta-includes-messages-feature-that-detects-nudity-sent-to-kids\"\u003eits Messages app\u003c/a\u003e, along with the ability for users to report harmful material to Apple.\n\u003c/p\u003e\u003cp\u003eThe woman behind the lawsuit and her lawyer, Margaret Mabie, do not agree that Apple has done enough. In preparation for the case, Mabie dug through law enforcement reports and other documents to find cases related to her clients\u0026#39; images and Apple\u0026#39;s products.\n\u003c/p\u003e\u003cp\u003eMabie eventually built a list of more than 80 examples of the images being shared. One of the people sharing the images was a Bay Area man who was caught with more than 2,000 illegal images and videos stored in iCloud, the \u003cem\u003eTimes\u003c/em\u003e noted.\u003c/p\u003e\n\n        \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-12-08T21:46:32Z",
  "modifiedTime": null
}
