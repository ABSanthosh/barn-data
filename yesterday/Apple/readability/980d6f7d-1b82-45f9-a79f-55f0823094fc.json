{
  "id": "980d6f7d-1b82-45f9-a79f-55f0823094fc",
  "title": "Apple just gave developers access to its new local AI models, here’s how they perform",
  "link": "https://9to5mac.com/2025/06/11/how-do-apple-new-local-models-compare/",
  "description": "One of the very first announcements on this year’s WWDC was that for the first time, third‑party developers will get to tap directly into Apple’s on‑device AI with the new Foundation Models framework. But how do these models actually compare against what’s already out there? more…",
  "author": "Marcus Mendes",
  "published": "Wed, 11 Jun 2025 21:25:35 +0000",
  "source": "https://9to5mac.com/feed",
  "categories": [
    "News"
  ],
  "byline": "Marcus Mendes",
  "length": 3192,
  "excerpt": "With the new Foundation Models framework, third-party apps will be able to leverage the same on-device possibilities that Apple will enjoy.",
  "siteName": "9to5Mac",
  "favicon": "https://9to5mac.com/wp-content/uploads/sites/6/2019/10/cropped-cropped-mac1-1.png?w=192",
  "text": "One of the very first announcements on this year’s WWDC was that for the first time, third‑party developers will get to tap directly into Apple’s on‑device AI with the new Foundation Models framework. But how do these models actually compare against what’s already out there? With the new Foundation Models framework, third-party developers can now build on the same on-device AI stack used by Apple’s native apps. In other words, this means that developers will now be able to integrate AI features like summarizing documents, pulling key info from user text, or even generating structured content, entirely offline, with zero API cost. But how good are Apple’s models, really? Competitive where it counts Based on Apple’s own human evaluations, the answer is: pretty solid, especially when you consider the balance (which some might call ‘tradeoff’) between size, speed, and efficiency. In Apple’s testing, its ~3B parameter on-device model outperformed similar lightweight vision-language models like InternVL-2.5 and Qwen-2.5-VL-3B in image tasks, winning over 46% and 50% of prompts, respectively. And in text, it held its ground against larger models like Gemma-3-4B, even edging ahead in some international English locales and multilingual evaluations (Portuguese, French, Japanese, etc.). In other words, Apple’s new local models seem set to deliver consistent results for many real-world uses without resorting to the cloud or requiring data to leave the device. When it comes to Apple’s server model (which won’t be accessible by third-party developers like the local models), it compared favorably to LLaMA-4-Scout and even outperformed Qwen-2.5-VL-32B in image understanding. That said, GPT-4o still comfortably leads the pack overall. The “free and offline” part really matters The real story here isn’t just that Apple’s new models are better. It’s that they’re built in. With the Foundation Models framework, developers no longer need to bundle heavy language models in their apps for offline processing. That means leaner app sizes and no need to fall back on the cloud for most tasks. The result? A more private experience for users, and no API costs for developers, savings that can ultimately benefit everyone. Apple says the models are optimized for structured outputs using a Swift-native “guided generation” system, which allows developers to constrain model responses directly into app logic. For apps in education, productivity, and communication, this could be a game-changer, offering the benefits of LLMs without the latency, cost, or privacy tradeoffs. Ultimately, Apple’s models aren’t the most powerful in the world, but they don’t need to be. They’re good, they’re fast, and now they’re available to every developer for free, on-device, and offline. That might not make for the same headlines as more powerful models will, but in practice, it could lead to a wave of genuinely useful AI features in third-party iOS apps that don’t require the cloud. And for Apple, that may very well be the point. Add 9to5Mac to your Google News feed.  FTC: We use income earning auto affiliate links. More.",
  "image": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/06/wwdc-2025-14.27.11.jpg?resize=1200%2C628\u0026quality=82\u0026strip=all\u0026ssl=1",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\n\u003cfigure\u003e\n\t\u003cimg width=\"1600\" height=\"894\" src=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/06/wwdc-2025-14.27.11.jpg?quality=82\u0026amp;strip=all\u0026amp;w=1600\" alt=\"\" srcset=\"https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/06/wwdc-2025-14.27.11.jpg?w=320\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 320w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/06/wwdc-2025-14.27.11.jpg?w=640\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 640w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/06/wwdc-2025-14.27.11.jpg?w=1024\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 1024w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/06/wwdc-2025-14.27.11.jpg?w=1500\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 1500w\" decoding=\"async\" fetchpriority=\"high\"/\u003e\u003c/figure\u003e\n\n\u003cp\u003eOne of the very first announcements on this year’s WWDC was that for the first time, third‑party developers will get to \u003ca href=\"https://9to5mac.com/2025/06/09/apple-third-party-developers-apple-intelligence-models/\"\u003etap directly into\u003c/a\u003e Apple’s on‑device AI with the new Foundation Models framework. But how do these models actually compare against what’s already out there?\u003c/p\u003e\n\n\n\n\u003cp\u003eWith the new Foundation Models framework, third-party developers can now build on the same on-device AI stack used by Apple’s native apps.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn other words, this means that developers will now be able to integrate AI features like summarizing documents, pulling key info from user text, or even generating structured content, entirely offline, with zero API cost.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut how good are Apple’s models, really?\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-competitive-where-it-counts\"\u003eCompetitive where it counts\u003c/h2\u003e\n\n\n\n\u003cp\u003eBased on Apple’s own \u003ca href=\"https://machinelearning.apple.com/research/apple-foundation-models-2025-updates\"\u003ehuman evaluations\u003c/a\u003e, the answer is: pretty solid, especially when you consider the balance (which some might call ‘tradeoff’) between size, speed, and efficiency.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn Apple’s testing, its ~3B parameter on-device model outperformed similar lightweight vision-language models like InternVL-2.5 and Qwen-2.5-VL-3B in image tasks, winning over 46% and 50% of prompts, respectively.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" height=\"405\" width=\"1024\" src=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-image-benchmark.jpg?quality=82\u0026amp;strip=all\u0026amp;w=1024\" alt=\"\" srcset=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-image-benchmark.jpg 1500w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-image-benchmark.jpg?resize=155,61 155w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-image-benchmark.jpg?resize=655,259 655w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-image-benchmark.jpg?resize=768,304 768w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-image-benchmark.jpg?resize=1024,405 1024w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-image-benchmark.jpg?resize=350,138 350w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-image-benchmark.jpg?resize=140,55 140w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-image-benchmark.jpg?resize=150,59 150w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAnd in text, it held its ground against larger models like Gemma-3-4B, even edging ahead in some international English locales and multilingual evaluations (Portuguese, French, Japanese, etc.).\u003c/p\u003e\n\n\n\n\u003cp\u003eIn other words, Apple’s new local models seem set to deliver consistent results for many real-world uses without resorting to the cloud or requiring data to leave the device.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" height=\"875\" width=\"1024\" src=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-text-benchmark.jpg?quality=82\u0026amp;strip=all\u0026amp;w=1024\" alt=\"\" srcset=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-text-benchmark.jpg 1500w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-text-benchmark.jpg?resize=152,130 152w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-text-benchmark.jpg?resize=655,560 655w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-text-benchmark.jpg?resize=768,656 768w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-text-benchmark.jpg?resize=1024,875 1024w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-text-benchmark.jpg?resize=350,299 350w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-text-benchmark.jpg?resize=140,120 140w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-text-benchmark.jpg?resize=1170,1000 1170w, https://9to5mac.com/wp-content/uploads/sites/6/2025/06/apple-models-text-benchmark.jpg?resize=150,128 150w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhen it comes to Apple’s server model (which won’t be accessible by third-party developers like the local models), it compared favorably to LLaMA-4-Scout and even outperformed Qwen-2.5-VL-32B in image understanding. That said, GPT-4o still comfortably leads the pack overall.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-free-and-offline-part-really-matters\"\u003eThe “free and offline” part really matters\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe real story here isn’t just that Apple’s new models are better. It’s that they’re built in. With the Foundation Models framework, developers no longer need to bundle heavy language models in their apps for offline processing. That means leaner app sizes and no need to fall back on the cloud for most tasks.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe result? A more private experience for users, and no API costs for developers, savings that can ultimately benefit everyone.\u003c/p\u003e\n\n\n\n\u003cp\u003eApple says the models are optimized for structured outputs using a Swift-native “guided generation” system, which allows developers to constrain model responses directly into app logic. For apps in education, productivity, and communication, this could be a game-changer, offering the benefits of LLMs without the latency, cost, or privacy tradeoffs.\u003c/p\u003e\n\n\n\n\u003cp\u003eUltimately, Apple’s models aren’t the most powerful in the world, but they don’t need to be. They’re good, they’re fast, and now they’re available to every developer for free, on-device, and offline.\u003c/p\u003e\n\n\n\n\u003cp\u003eThat might not make for the same headlines as more powerful models will, but in practice, it could lead to a wave of genuinely useful AI features in third-party iOS apps that don’t require the cloud. And for Apple, that may very well be the point.\u003c/p\u003e\n\t\u003cp\u003e\n\t\t\u003ca target=\"_blank\" rel=\"nofollow\" href=\"https://news.google.com/publications/CAAqBggKMLOFATDAGg?hl=en-US\u0026amp;gl=US\u0026amp;ceid=US:en\"\u003e\n\t\t\t\u003cem\u003eAdd 9to5Mac to your Google News feed.\u003c/em\u003e \n\t\t\t\t\t\u003c/a\u003e\n\t\u003c/p\u003e\n\t\u003cdiv\u003e\u003cp\u003e\u003cem\u003eFTC: We use income earning auto affiliate links.\u003c/em\u003e \u003ca href=\"https://9to5mac.com/about/#affiliate\"\u003eMore.\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://bit.ly/43RjEnY\"\u003e\u003cimg src=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/06/iMazing_-WWDC25_9to5mac_750x150@2x.webp\" alt=\"\" width=\"1024\" height=\"205\"/\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-06-11T21:25:35Z",
  "modifiedTime": "2025-06-11T21:25:37Z"
}
