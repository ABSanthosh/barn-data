{
  "id": "0f302905-e9a6-4d4f-a75d-d1788f6a9f4d",
  "title": "Apple Is Using Differential Privacy to Improve Apple Intelligence",
  "link": "https://www.macstories.net/news/apple-is-using-differential-privacy-to-improve-apple-intelligence/",
  "description": "Apple has been using differential privacy for nearly ten years to collect its users data in a way that isn’t traceable back to an individual. As Apple explains in a recent post on its Machine Learning Research site: This approach works by randomly polling participating devices for whether they’ve seen a particular fragment, and devices […]",
  "author": "John Voorhees",
  "published": "Tue, 15 Apr 2025 19:08:23 +0000",
  "source": "https://www.macstories.net/feed",
  "categories": [
    "news",
    "AI",
    "artificial intelligence",
    "Privacy"
  ],
  "byline": "",
  "length": 2833,
  "excerpt": "Apple has been using differential privacy for nearly ten years to collect its users data in a way that isn’t traceable back to an individual. As Apple explains in a recent post on its Machine Learning Research site: This approach works by randomly polling participating devices for whether they’ve seen a particular fragment, and devices",
  "siteName": "",
  "favicon": "https://www.macstories.net/app/themes/macstories4/images/apple-touch-icon-152x152-precomposed.png",
  "text": "Apple has been using differential privacy for nearly ten years to collect its users data in a way that isn’t traceable back to an individual. As Apple explains in a recent post on its Machine Learning Research site: This approach works by randomly polling participating devices for whether they’ve seen a particular fragment, and devices respond anonymously with a noisy signal. By noisy, we mean that devices may provide the true signal of whether a fragment was seen or a randomly selected signal for an alternative fragment or no matches at all. By calibrating how often devices send randomly selected responses, we ensure that hundreds of people using the same term are needed before the word can be discoverable. The company has used the technique to analyze everything from the popularity of emoji to what words to suggest with QuickType. Now, Apple is using differential privacy to mine the data of users who have opted into sharing device analytics to improve Apple Intelligence. So far, the technique’s use has been limited to improving Genmoji, but in upcoming OS releases, it will be used for “Image Playground, Image Wand, Memories Creation and Writing Tools in Apple Intelligence, as well as in Visual Intelligence,” too. The report explains that: Building on our many years of experience using techniques like differential privacy, as well as new techniques like synthetic data generation, we are able to improve Apple Intelligence features while protecting user privacy for users who opt in to the device analytics program. These techniques allow Apple to understand overall trends, without learning information about any individual, like what prompts they use or the content of their emails. As we continue to advance the state of the art in machine learning and AI to enhance our product experiences, we remain committed to developing and implementing cutting-edge techniques to protect user privacy. For Genmoji, this means collecting data on the most popular prompts used to create the emoji-like images. Apple explains that written content is more challenging but that it can use an LLM to generate synthetic data like emails. The synthetic data is then sent to users’ devices who have opted into device analytics to determine which data matches actual user data most closely and frequently, again using differential privacy to prevent individual device identification. Using differential privacy to improve Apple Intelligence without directly scraping user data is clever, but it does make me wonder why something similar wasn’t used to generate Apple’s large language models that were trained on the contents of the Internet. Perhaps that’s not possible at the scale of an LLM, or maybe that initial model needs a level of precision that differential privacy doesn’t offer, but I think it’s fair to ask.",
  "image": "https://56243e3f6f46fe44a301-deabeb5f3878e3553d0b065ea974f9bf.ssl.cf1.rackcdn.com/256px.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle id=\"content\"\u003e\n\n    \n\n    \u003cdiv\u003e\n                        \u003cp id=\"p1\"\u003eApple has been using \u003ca href=\"https://www.macstories.net/linked/apples-differential-privacy-and-your-data/\" rel=\"noopener noreferrer\"\u003edifferential privacy\u003c/a\u003e for nearly ten years to collect its users data in a way that isn’t traceable back to an individual. \u003ca href=\"https://machinelearning.apple.com/research/differential-privacy-aggregate-trends\" rel=\"noopener noreferrer\"\u003eAs Apple explains\u003c/a\u003e in a recent post on its Machine Learning Research site:\u003c/p\u003e\n\u003cblockquote id=\"blockquote2\"\u003e\u003cp\u003e\n  This approach works by randomly polling participating devices for whether they’ve seen a particular fragment, and devices respond anonymously with a noisy signal. By noisy, we mean that devices may provide the true signal of whether a fragment was seen or a randomly selected signal for an alternative fragment or no matches at all. By calibrating how often devices send randomly selected responses, we ensure that hundreds of people using the same term are needed before the word can be discoverable.\n\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp id=\"p3\"\u003eThe company has used the technique to analyze everything \u003ca href=\"https://www.macstories.net/linked/apple-shares-differential-privacy-insights-for-emoji-and-quicktype-keyboard/\" rel=\"noopener noreferrer\"\u003efrom the popularity of emoji to what words to suggest with QuickType\u003c/a\u003e.\u003c/p\u003e\n\u003cp id=\"p4\"\u003eNow, Apple is using \u003ca href=\"https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf\" rel=\"noopener noreferrer\"\u003edifferential privacy\u003c/a\u003e to mine the data of users who have opted into sharing device analytics to improve Apple Intelligence. So far, the technique’s use has been limited to improving Genmoji, but in upcoming OS releases, it will be used for “Image Playground, Image Wand, Memories Creation and Writing Tools in Apple Intelligence, as well as in Visual Intelligence,” too.\u003c/p\u003e\n\u003cp id=\"p5\"\u003e\u003ca href=\"https://machinelearning.apple.com/research/differential-privacy-aggregate-trends\" rel=\"noopener noreferrer\"\u003eThe report explains that\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote id=\"blockquote6\"\u003e\u003cp\u003e\n  Building on our many years of experience using techniques like differential privacy, as well as new techniques like synthetic data generation, we are able to improve Apple Intelligence features while protecting user privacy for users who opt in to the device analytics program. These techniques allow Apple to understand overall trends, without learning information about any individual, like what prompts they use or the content of their emails. As we continue to advance the state of the art in machine learning and AI to enhance our product experiences, we remain committed to developing and implementing cutting-edge techniques to protect user privacy.\n\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp id=\"p7\"\u003eFor Genmoji, this means collecting data on the most popular prompts used to create the emoji-like images. Apple explains that written content is more challenging but that it can use an LLM to generate synthetic data like emails. The synthetic data is then sent to users’ devices who have opted into device analytics to determine which data matches actual user data most closely and frequently, again using differential privacy to prevent individual device identification.\u003c/p\u003e\n\u003cp id=\"p8\"\u003eUsing differential privacy to improve Apple Intelligence without directly scraping user data is clever, but it does make me wonder why something similar wasn’t used to generate Apple’s large language models that were trained on the contents of the Internet. Perhaps that’s not possible at the scale of an LLM, or maybe that initial model needs a level of precision that differential privacy doesn’t offer, but I think it’s fair to ask.\u003c/p\u003e\n            \u003c/div\u003e\n\n    \n        \n\n    \n    \n    \n\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-04-15T19:08:23-04:00",
  "modifiedTime": null
}
