{
  "id": "421a5065-0edb-4188-bff0-343335521a00",
  "title": "Enhancing Gemini Nano: delivering higher quality summaries with LoRA",
  "link": "https://developer.chrome.com/blog/improved-summaries-gemini-nano?hl=en",
  "description": "By fine-tuning Gemini Nano with LoRA, we enhanced the experience and output quality of the Summarizer API.",
  "author": "",
  "published": "Wed, 21 May 2025 07:00:00 GMT",
  "source": "https://developer.chrome.com/static/blog/feed.xml",
  "categories": null,
  "byline": "",
  "length": 15193,
  "excerpt": "By fine-tuning Gemini Nano with LoRA, we enhanced the experience and output quality of the Summarizer API.",
  "siteName": "Chrome for Developers",
  "favicon": "https://www.gstatic.com/devrel-devsite/prod/v6dc4611c4232bd02b2b914c4948f523846f90835f230654af18f87f75fe9f73c/chrome/images/favicon.png",
  "text": "Skip to main content Enhancing Gemini Nano: delivering higher quality summaries with LoRA Stay organized with collections Save and categorize content based on your preferences. Published: May 21, 2025 Summarization stands as one of the most common and vital AI tasks using large language models (LLMs). Summaries offer a critical means to quickly understand extensive content—from lengthy articles and dense chat logs to numerous reviews—saving time, enhancing productivity, and enabling faster, better-informed decision-making. There are many different types of summaries, with varied levels of detail and formatting expectations. To meet the expectations of the various summary types, Chrome collaborated with Google Cloud to improve Gemini Nano's output. We fine-tuned Gemini Nano with low-rank adaptation (LoRA) to enhance the experience and output quality, for all summary styles and lengths. Additionally, we implemented automatic and autorater evaluations on different aspects of summary quality, including factuality, coverage, format, and readability. We've visualized what this difference looks like in practice. You can experiment with this implementation and take a look at a real-time demo that compares the outputs of Gemini Nano and Gemini Nano with LoRA. What is the Summarizer API? Explainer Web Extensions Chrome Status Intent GitHub Origin trial Origin trial View Intent to Ship The Summarizer API condenses lengthy text content into brief, easy-to-digest summaries. The API is built into Chrome nd uses Gemini Nano to perform inference. Different sites may require summaries with a range of styles and lengths. For example, if you're a news site, you may want to offer a bulleted list of key points in your articles. Alternatively, users browsing product reviews could benefit from a quick and short summary of the review sentiment. To demonstrate, we've summarized the Wikipedia page on Welsh Corgis with the length set to short. Summary type Output headline ## Welsh Corgi: A History of Royalty and Herding Dogs key-points * The Welsh Corgi is a small herding dog that originated in Wales. * There are two main breeds: Pembroke and Cardigan Welsh Corgi. * The Pembroke is more popular and has been associated with the British royal family. tl;dr The Welsh Corgi, a small herding dog with a long history in Wales and the British royal family, comes in two varieties: Pembroke and Cardigan, both known for their fox-like faces, short legs, and herding instincts. teaser Discover the history of the Welsh Corgi, from its humble origins as a herding dog for Welsh farmers to its rise as a symbol of the British royal family. You can experiment with other pages using the Summarizer API Playground. Experiment with fine-tuning Fine-tuning is only available as a flag in Chrome Canary, from version 138.0.7180.0. To use this model: Open Chrome Canary. Go to chrome://flags/#summarization-api-for-gemini-nano Select Enabled with Adaptation. Restart the browser. Open DevTools Console and input Summarizer.availability(). This starts the download for the supplemental LoRA. Once the download is complete, you can start experimenting. Evaluating the summarizer's performance We measured the performance improvement of the fine-tuned Gemini Nano primarily using two evaluation methods, automatic and autorater. Fine-tuning helps a model better perform specific tasks, such as: Translate medical text better. Generate images in a specific art style. Understand a new slang. In this case, we wanted to better meet the expectations of each summary type. Automatic evaluation Automatic evaluation uses software to judge a model's output quality. We used this technique to search for formatting errors, sentence repetition, and existence of non-English characters in summaries of English input. Formatting errors: We check whether the summary responses follow the prompt's formatting instructions. For example, for the short key-points style, we check whether each bullet point starts with an asterisk (*) and that the number of bullet points does not exceed 3 bullet points. Sentence repetition: We check whether the same sentence is repeated in a single summary response, as this indicates a poor quality response. Non-English characters: We check whether the response includes non-English characters when the input is meant to be in English. Hyperlink in output: We check if the response has any hyperlinks, in Markdown format or in plain text, that don't exist in the input. We evaluated two styles of input: scraped articles and chat logs.   Headline TL;DR Key-Points Teaser   Base / With LoRA Base / With LoRA Base / With LoRA Base / With LoRA Format errors 13.54% / 7.05% 41.07% / 4.61% 12.58% / 6.36% 51.17% / 6.74% Sentence repetition 0.07% / 0.07% 0.21% / 0.0% 0.10% / 0.10% 0.10% / 0.03% Non-English errors 3.95% / 0.03% 1.38% / 0.0% 2.41% / 0.03% 1.44% / 0.0% Hyperlinks 0.07% / 0.0% 0.14% / 0.0% 0.14% / 0.0% 0.34% / 0.0% Automatic evaluation across 970 articles across the different summary types.   Headline TL;DR Key-Points Teaser   Base / With LoRA Base / With LoRA Base / With LoRA Base / With LoRA Format error 13.17% / 0.24% 22.92% / 0.18% 4.43% / 0.09% 29.64% / 3.51% Sentence repetition 0.0% / 0.0% 0.0% / 0.0% 0.0% / 0.0% 0.03% / 0.0% Non-English error 0.15% / 0.0% 0.15% / 0.0% 0.03% / 0.0% 0.06% / 0.0% Hyperlinks 0.0% / 0.0% 0.0% / 0.0% 0.0% / 0.0% 0.0% / 0.0% Automatic evaluation across 1091 chat entry samples. After fine-tuning the Gemini Nano, we saw a significant reduction in the format error rate across different summary types, for both articles and chat logs. Autorater evaluation We used Gemini 1.5 Pro for autorater evaluation, to judge Gemini Nano's output quality. As each summary has a different purpose, the criteria and value of the criteria differed for different summary types. All summary types were evaluated for: Coverage: Does the summary accurately capture the essential purpose of the input? Factuality: Is the summary truthful? Does the summary introduce new information that was not explicitly stated or implied in the text? Format: Is the summary formatted with valid Markdown syntax? Does the summary keep to the maximum length of sentences, as is requested? Clarity: Is the summary repetitive? Does the summary accurately convey the core message in the fewest possible words? As these summary types have different purposes, additional metrics apply to specific summary types: Engagement: (headline): Is the summary immediately understandable to a general audience? Does the summary use a tone that is engaging and appealing to a general audience? Succinctness (tl;dr): Is the summary clear, concise, and immediately understandable to someone with a very short attention span? Does it effectively distill the core message into a readily digestible form for a quick read? Enticement (teaser): Does the summary effectively create intrigue and encourage the reader to want to learn more by reading the full text? Does it use language that is engaging and suggestive of interesting content? We compared the output of the base model and the model with LoRA, side-by-side, using the autorater. The autorater's scores were averaged between 0 and 1, which was then assessed against the threshold value. To ensure a well-grounded result, we reduced data variance and alleviated positional bias. Data variance reduction: We averaged the scores of three independent outputs per input, as independent runs may have slightly different results. We averaged the outputs for both the base model and fine-tuned Gemini Nano. While the differences in scores across outputs were only slightly different, averages help us more reliably understand large sets of data. Alleviate positional bias: To avoid giving preference to the value of the summary shared first with the rater, we evaluated the results twice, then averaged the final scores. We evaluated the model with LoRA, then the base model. Then, we reversed the order. We evaluated the base model, followed by the model with LoRA. We averaged the final scores.   Short Medium Long   Base / With LoRA Base / With LoRA Base / With LoRA LoRA first 74.29% / 86.64% 76.11% / 81.38% 68.62% / 78.95% Base model first 68.02% / 88.60% 64.97% / 87.58% 58.25% / 86.35% Version C (Average) 71.02% / 89.18% 69.59% / 84.08% 63.47% / 82.65% Winrates for key-points summary type. Higher values are better results. While the difference in scoring for outputs from the same model were only slightly different, averages help us more reliably understand large sets of data. Across 500 articles, fine-tuned Gemini Nano performed significantly better than the base model.   Headline TL;DR Key-Points Teaser   Base / With LoRA Base / With LoRA Base / With LoRA Base / With LoRA Short 74.74% / 89.12% 55.76% / 89.50% 71.02% / 89.18% 53.47% / 87.14% Medium 73.10% / 87.89% 41.82% / 81.21% 69.59% / 84.08% 48.98% / 86.74% Long 60.99% / 89.32% 50.51% / 84.85% 63.47% / 82.65% 62.65% / 87.55% Autorarer winrate across 500 articles across the different summary and length types. Higher numbers indicate better results. The same was true in our evaluation of 500 chat logs, fine-tuned Gemini Nano outperformed the base model.   Headline TL;DR Key-Points Teaser   Base / With LoRA Base / With LoRA Base / With LoRA Base / With LoRA Short 70.59% / 96.15% 66.27% / 97.79% 81.60% / 97.40% 67.48% / 96.14% Medium 76.67% / 95.13% 56.02% / 94.98% 82.60% / 97.20% 50.41% / 96.95% Long 59.03% / 94.32% 65.86% / 95.58% 75.00% / 97.60% 70.94% / 97.16% Autorater winrate across 500 chatlogs across the different summary and length types. Higher numbers indicate better results. These results demonstrate that our fine-tuning improved the overall summary quality. Traditionally, fine-tuning is performed by adjusting the model's parameters. Modern AI models are huge, so this operation is slow, expensive, and requires storing a brand new copy of the model. Instead of changing all the parameters, what if we add tiny extra pieces that steer the model in the direction we want? Their smaller size enables much faster training. This is the core tenet of low-rank adaptation (LoRA). With LoRA, certain pieces of the model remain unchanged, often referred to as freezing the parameters. A smaller set of data is then introduced with small corrections, focusing training efforts on these added pieces. In fact, typically you can train as few as 2% of the original parameters with LoRA and still see significant changes in your outputs. Why does this work? Many behaviors (such as new styles, tasks, or knowledge) can be incorporated without altering every model parameter. These changes can be expressed as small patterns or directions within the model's existing knowledge. LoRA efficiently captures these patterns without touching the original model. Take a look at some examples that compare the Gemini Nano base model to the results after adding LoRA. We asked Gemini Nano to create a short headline that \"captures attention and conveys the essence of the content\" in the Joshua Tree National Park Wikipedia article. Gemini Nano Gemini Nano with LoRA Joshua Tree National Park's Unique Landscape and History Joshua Tree: National Park in California, Straddling Deserts \u0026 Home to Iconic Trees Next, we prompted Gemini Nano for a short teaser for the Ratatouille article. \"Write an enticing one sentence that encourages users to read the full text.\" Gemini Nano Gemini Nano with LoRA Ratatouille is a beloved French Provençal dish featuring stewed vegetables, often including tomatoes and garlic. Its origins can be traced to the 18th century, and its preparation has evolved over time, with some chefs emphasizing individual vegetable flavors while others prefer a smoother, more unified texture. Beyond its French origins, ratatouille has inspired similar dishes in various cuisines around the world, including piperade from Spain and tian from the Mediterranean. The film \"Ratatouille\" (2007) further popularized the dish, showcasing its culinary versatility and emotional resonance. Ratatouille, a French Provençal dish of stewed vegetables, has a rich culinary history, evolving from a simple stew into a modern culinary masterpiece that has captivated audiences worldwide. Real-time inference demo We built an interface that demonstrates the comparison of outputs between Gemini Nano and Gemini Nano with LoRA. We asked Gemini Nano to create a tl;dr summary with a short length for the Ocean Sunfish article. Remember that tl;dr and short requires a response in 1 sentence that is a \"quick read.\" Watch in real-time how long it took to create the results and read the output. By implementing fine-tuning, Gemini Nano can better generate a summary that follows the specific instructions. We're eager to hear your feedback on how your summaries are impacted by the fine-tuned Gemini Nano. Experiment with the updated model in Chrome Canary. Learm more about the Summarizer API. If you have feedback on Chrome's implementation, file a bug report or feature request. Discover all of the built-in AI APIs which use models, including large language models, in the browser. Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-21 UTC. [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-05-21 UTC.\"],[],[]]",
  "image": "https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/hero.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\n    \u003cdevsite-progress type=\"indeterminate\" id=\"app-progress\"\u003e\u003c/devsite-progress\u003e\n  \n    \u003ca href=\"#main-content\"\u003e\n      \n      Skip to main content\n    \u003c/a\u003e\n    \u003csection\u003e\n      \u003cdevsite-cookie-notification-bar\u003e\u003c/devsite-cookie-notification-bar\u003e\u003cdevsite-header role=\"banner\"\u003e\n  \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\u003c/devsite-header\u003e\n      \n      \u003csection id=\"gc-wrapper\"\u003e\n        \u003cmain role=\"main\" id=\"main-content\" has-sidebar=\"\"\u003e\n          \n          \u003cdevsite-content\u003e\n            \n              \n\n\n\n\n\n\n\n\n\n\n\n\n\u003carticle\u003e\n  \n  \n  \n  \n  \n\n  \n  \n    \u003ch2 tabindex=\"-1\"\u003e\n      Enhancing Gemini Nano: delivering higher quality summaries with LoRA\n      \u003cp data-nosnippet=\"\"\u003e\u003cdevsite-feature-tooltip ack-key=\"AckCollectionsBookmarkTooltipDismiss\" analytics-category=\"Site-Wide Custom Events\" analytics-action-show=\"Callout Profile displayed\" analytics-action-close=\"Callout Profile dismissed\" analytics-label=\"Create Collection Callout\" dismiss-button=\"true\" id=\"devsite-collections-dropdown\" dismiss-button-text=\"Dismiss\" close-button-text=\"Got it\"\u003e\n\n    \n    \u003cdevsite-bookmark\u003e\u003c/devsite-bookmark\u003e\n\n    \u003cspan slot=\"popout-heading\"\u003e\n      \n      Stay organized with collections\n    \u003c/span\u003e\n    \u003cspan slot=\"popout-contents\"\u003e\n      \n      Save and categorize content based on your preferences.\n    \u003c/span\u003e\n  \u003c/devsite-feature-tooltip\u003e\u003c/p\u003e\n  \n    \u003c/h2\u003e\n  \n  \n\n  \u003cdevsite-toc depth=\"2\" devsite-toc-embedded=\"\"\u003e\n  \u003c/devsite-toc\u003e\n  \n    \n  \n\n  \u003cdiv\u003e\n\n  \n    \n\n\n\n\n\u003cdiv translate=\"no\"\u003e\n        \n          \u003cp\u003e\u003cimg alt=\"Yae Jee Cho\" src=\"https://web.dev/images/authors/yaejee.jpg\" decoding=\"async\" height=\"64\" loading=\"lazy\" width=\"64\"/\u003e\u003c/p\u003e\n      \u003c/div\u003e\n\n\u003cp\u003e\n  Published: May 21, 2025\n\u003c/p\u003e\n\n\n\u003cp\u003e\u003cimg src=\"https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon.png\" alt=\"\" width=\"250\" srcset=\"https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_36.png 36w,https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_48.png 48w,https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_72.png 72w,https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_96.png 96w,https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_480.png 480w,https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_720.png 720w,https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_856.png 856w,https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_960.png 960w,https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_1440.png 1440w,https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_1920.png 1920w,https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/image/summarizer-icon_2880.png 2880w\" sizes=\"(max-width: 840px) 100vw, 856px\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eSummarization stands as one of the most common and vital AI tasks using large\nlanguage models (LLMs). Summaries offer a critical means to quickly understand\nextensive content—from lengthy articles and dense chat logs to numerous\nreviews—saving time, enhancing productivity, and enabling faster,\nbetter-informed decision-making.\u003c/p\u003e\n\n\u003cp\u003eThere are many different types of summaries, with varied levels of detail and\nformatting expectations. To meet the expectations of the various summary types,\nChrome collaborated with Google Cloud to improve Gemini Nano\u0026#39;s output.\u003c/p\u003e\n\n\u003cp\u003eWe fine-tuned Gemini Nano with\n\u003ca href=\"#better_summaries_with_lora\"\u003elow-rank adaptation (LoRA)\u003c/a\u003e to enhance the\nexperience and output quality, for all summary styles and lengths. Additionally,\nwe implemented \u003ca href=\"https://developers.google.com/machine-learning/glossary#automatic-evaluation\"\u003eautomatic\u003c/a\u003e\nand \u003ca href=\"https://developers.google.com/machine-learning/glossary#autorater-evaluation\"\u003eautorater\u003c/a\u003e\nevaluations on different aspects of summary quality, including factuality,\ncoverage, format, and readability.\u003c/p\u003e\n\n\u003cp\u003eWe\u0026#39;ve visualized what this difference looks like in practice. You can\n\u003ca href=\"#turn-on-fine-tuning\"\u003eexperiment with this implementation\u003c/a\u003e and take a look at a\n\u003ca href=\"#demo\"\u003ereal-time demo\u003c/a\u003e that compares the outputs of Gemini Nano and\nGemini Nano with LoRA.\u003c/p\u003e\n\n\u003ch2 id=\"what_is_the_summarizer_api\" data-text=\"What is the Summarizer API?\" tabindex=\"-1\"\u003eWhat is the Summarizer API?\u003c/h2\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n    \u003cth\u003eExplainer\u003c/th\u003e\n    \u003cth\u003eWeb\u003c/th\u003e\n    \u003cth\u003eExtensions\u003c/th\u003e\n    \u003cth\u003eChrome Status\u003c/th\u003e\n    \u003cth\u003eIntent\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \n\u003ctd\u003e\u003ca href=\"https://github.com/explainers-by-googlers/writing-assistance-apis/\"\u003eGitHub\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cimg src=\"https://developer.chrome.com/static/images/flag.svg\" width=\"16\" height=\"16\" alt=\"Behind a flag\"/\u003e \u003ca href=\"https://developer.chrome.com/origintrials#/view_trial/1923599990840623105\" target=\"_blank\"\u003eOrigin trial\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cimg src=\"https://developer.chrome.com/static/images/flag.svg\" width=\"16\" height=\"16\" alt=\"Behind a flag\"/\u003e \u003ca href=\"https://developer.chrome.com/origintrials#/view_trial/1923599990840623105\" target=\"_blank\"\u003eOrigin trial\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://chromestatus.com/feature/5193953788559360\" target=\"_blank\"\u003eView\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://groups.google.com/a/chromium.org/g/blink-dev/c/cpyB56aHWs4/m/8NTdmGV8AAAJ\" target=\"_blank\"\u003eIntent to Ship\u003c/a\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://developer.chrome.com/docs/ai/summarizer-api\"\u003eSummarizer API\u003c/a\u003e condenses lengthy\ntext content into brief, easy-to-digest summaries. The API is built into Chrome\nnd uses Gemini Nano to perform inference.\u003c/p\u003e\n\n\u003cp\u003eDifferent sites may require summaries with a\n\u003ca href=\"https://developer.chrome.com/docs/ai/summarizer-api#api-functions\"\u003erange of styles and lengths\u003c/a\u003e. For\nexample, if you\u0026#39;re a news site, you may want to offer a bulleted list of key\npoints in your articles. Alternatively, users browsing product reviews could\nbenefit from a quick and short summary of the review sentiment. To demonstrate,\nwe\u0026#39;ve summarized the\n\u003ca href=\"https://en.wikipedia.org/wiki/Welsh_Corgi\"\u003eWikipedia page on Welsh Corgis\u003c/a\u003e\nwith the length set to \u003ccode translate=\"no\" dir=\"ltr\"\u003eshort\u003c/code\u003e.\u003c/p\u003e\n\n\u003ctable\u003e\n\u003ctbody\u003e\u003ctr\u003e\n   \u003ctd\u003e\u003cb\u003eSummary type\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eOutput\u003c/b\u003e\u003c/td\u003e\n \u003c/tr\u003e\n \u003ctr\u003e\n   \u003ctd\u003e\u003ccode translate=\"no\" dir=\"ltr\"\u003eheadline\u003c/code\u003e\u003c/td\u003e\n   \u003ctd\u003e## Welsh Corgi: A History of Royalty and Herding Dogs\u003c/td\u003e\n \u003c/tr\u003e\n \u003ctr\u003e\n   \u003ctd\u003e\u003ccode translate=\"no\" dir=\"ltr\"\u003ekey-points\u003c/code\u003e\u003c/td\u003e\n   \u003ctd\u003e* The Welsh Corgi is a small herding dog that originated in Wales.\u003cbr/\u003e\n      * There are two main breeds: Pembroke and Cardigan Welsh Corgi.\u003cbr/\u003e\n      * The Pembroke is more popular and has been associated with the British royal family.\u003c/td\u003e\n \u003c/tr\u003e\n \u003ctr\u003e\n   \u003ctd\u003e\u003ccode translate=\"no\" dir=\"ltr\"\u003etl;dr\u003c/code\u003e\u003c/td\u003e\n   \u003ctd\u003eThe Welsh Corgi, a small herding dog with a long history in Wales and\n   the British royal family, comes in two varieties: Pembroke and Cardigan,\n   both known for their fox-like faces, short legs, and herding instincts.\u003c/td\u003e\n   \u003c/tr\u003e\n   \u003ctr\u003e\n   \u003ctd\u003e\u003ccode translate=\"no\" dir=\"ltr\"\u003eteaser\u003c/code\u003e\u003c/td\u003e\n   \u003ctd\u003eDiscover the history of the Welsh Corgi, from its humble origins as a\n   herding dog for Welsh farmers to its rise as a symbol of the British royal\n   family.\u003c/td\u003e\n \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n\u003cp\u003eYou can experiment with other pages using the\n\u003ca href=\"https://chrome.dev/web-ai-demos/summarization-api-playground/\"\u003eSummarizer API Playground\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3 id=\"turn-on-fine-tuning\" data-text=\"Experiment with fine-tuning\" tabindex=\"-1\"\u003eExperiment with fine-tuning\u003c/h3\u003e\n\n\u003cp\u003eFine-tuning is only available as a \u003ca href=\"https://developer.chrome.com/docs/web-platform/chrome-flags\"\u003eflag in Chrome Canary\u003c/a\u003e, from version \u003ccode translate=\"no\" dir=\"ltr\"\u003e138.0.7180.0\u003c/code\u003e. To use this model:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eOpen \u003ca href=\"https://www.google.com/chrome/canary/\"\u003eChrome Canary\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eGo to \u003ccode translate=\"no\" dir=\"ltr\"\u003echrome://flags/#summarization-api-for-gemini-nano\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSelect \u003cstrong\u003eEnabled with Adaptation\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eRestart the browser.\u003c/li\u003e\n\u003cli\u003eOpen \u003ca href=\"https://developer.chrome.com/docs/devtools/console/reference#open\"\u003eDevTools \u003cstrong\u003eConsole\u003c/strong\u003e\u003c/a\u003e and input \u003ccode translate=\"no\" dir=\"ltr\"\u003eSummarizer.availability()\u003c/code\u003e. This starts the download for the supplemental LoRA.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eOnce the download is complete, you can start experimenting.\u003c/p\u003e\n\n\u003ch2 id=\"evaluating_the_summarizers_performance\" data-text=\"Evaluating the summarizer\u0026#39;s performance\" tabindex=\"-1\"\u003eEvaluating the summarizer\u0026#39;s performance\u003c/h2\u003e\n\n\u003cp\u003eWe measured the performance improvement of the fine-tuned Gemini Nano primarily\nusing two evaluation methods, \u003ca href=\"#automatic_evaluation\"\u003eautomatic\u003c/a\u003e and\n\u003ca href=\"#autorater_evaluation\"\u003eautorater\u003c/a\u003e. Fine-tuning helps a model better perform\nspecific tasks, such as:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eTranslate medical text better.\u003c/li\u003e\n\u003cli\u003eGenerate images in a specific art style.\u003c/li\u003e\n\u003cli\u003eUnderstand a new slang.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn this case, we wanted to better meet the expectations of each summary type.\u003c/p\u003e\n\n\u003ch3 id=\"automatic_evaluation\" data-text=\"Automatic evaluation\" tabindex=\"-1\"\u003eAutomatic evaluation\u003c/h3\u003e\n\n\u003cp\u003e\u003ca href=\"https://developers.google.com/machine-learning/glossary#automatic-evaluation\"\u003eAutomatic evaluation\u003c/a\u003e\nuses software to judge a model\u0026#39;s output quality.\nWe used this technique to search for formatting errors, sentence repetition, and\nexistence of non-English characters in summaries of English input.\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eFormatting errors\u003c/strong\u003e: We check whether the summary responses follow the\nprompt\u0026#39;s formatting instructions. For example, for the short key-points style,\nwe check whether each bullet point starts with an asterisk (\u003ccode translate=\"no\" dir=\"ltr\"\u003e*\u003c/code\u003e) and\nthat the number of bullet points does not exceed 3 bullet points.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eSentence repetition\u003c/strong\u003e: We check whether the same sentence is repeated in a\nsingle summary response, as this indicates a poor quality response.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eNon-English characters\u003c/strong\u003e: We check whether the response includes non-English\ncharacters when the input is meant to be in English.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eHyperlink in output\u003c/strong\u003e: We check if the response has any hyperlinks,\nin Markdown format or in plain text, that don\u0026#39;t exist in the input.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe evaluated two styles of input: scraped articles and chat logs.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n   \u003ctd\u003e \u003c/td\u003e\n   \u003ctd\u003eHeadline\u003c/td\u003e\n   \u003ctd\u003eTL;DR\u003c/td\u003e\n   \u003ctd\u003eKey-Points\u003c/td\u003e\n   \u003ctd\u003eTeaser\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e \u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eFormat errors\u003c/td\u003e\n   \u003ctd\u003e13.54% / 7.05%\u003c/td\u003e\n   \u003ctd\u003e41.07% / 4.61%\u003c/td\u003e\n   \u003ctd\u003e12.58% / 6.36%\u003c/td\u003e\n   \u003ctd\u003e51.17% / 6.74%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eSentence repetition\u003c/td\u003e\n   \u003ctd\u003e0.07% / 0.07%\u003c/td\u003e\n   \u003ctd\u003e0.21% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.10% / 0.10%\u003c/td\u003e\n   \u003ctd\u003e0.10% / 0.03%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eNon-English errors\u003c/td\u003e\n   \u003ctd\u003e3.95% / 0.03%\u003c/td\u003e\n   \u003ctd\u003e1.38% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e2.41% / 0.03%\u003c/td\u003e\n   \u003ctd\u003e1.44% / 0.0%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eHyperlinks\u003c/td\u003e\n   \u003ctd\u003e0.07% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.14% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.14% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.34% / 0.0%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003c/tbody\u003e\u003ccaption\u003eAutomatic evaluation across 970 articles across the different\n  summary types.\u003c/caption\u003e\n\u003c/table\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e \u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eHeadline\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eTL;DR\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eKey-Points\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eTeaser\u003c/b\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e \u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\u003cb\u003eFormat error\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e13.17% / 0.24%\u003c/td\u003e\n   \u003ctd\u003e22.92% / 0.18%\u003c/td\u003e\n   \u003ctd\u003e4.43% / 0.09%\u003c/td\u003e\n   \u003ctd\u003e29.64% / 3.51%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\u003cb\u003eSentence repetition\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e0.0% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.0% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.0% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.03% / 0.0%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n\u003ctd\u003e\u003cb\u003eNon-English error\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e0.15% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.15% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.03% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.06% / 0.0%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n\u003ctd\u003e\u003cb\u003eHyperlinks\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e0.0% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.0% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.0% / 0.0%\u003c/td\u003e\n   \u003ctd\u003e0.0% / 0.0%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003c/tbody\u003e\u003ccaption\u003eAutomatic evaluation across 1091 chat entry samples.\u003c/caption\u003e\n\u003c/table\u003e\n\n\u003cp\u003eAfter fine-tuning the Gemini Nano, we saw a significant reduction in the format error rate across different summary types, for both articles and chat logs.\u003c/p\u003e\n\n\u003ch3 id=\"autorater_evaluation\" data-text=\"Autorater evaluation\" tabindex=\"-1\"\u003eAutorater evaluation\u003c/h3\u003e\n\n\u003cp\u003eWe used Gemini 1.5 Pro for \u003ca href=\"https://developers.google.com/machine-learning/glossary#autorater-evaluation\"\u003eautorater evaluation\u003c/a\u003e,\nto judge Gemini Nano\u0026#39;s output quality. As each summary has a different purpose,\nthe criteria and value of the criteria differed for different summary types.\nAll summary types were evaluated for:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCoverage\u003c/strong\u003e: Does the summary accurately capture the essential purpose of\nthe input?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFactuality:\u003c/strong\u003e Is the summary truthful? Does the summary introduce new\ninformation that was not explicitly stated or implied in the text?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFormat\u003c/strong\u003e: Is the summary formatted with valid Markdown syntax? Does the\nsummary keep to the maximum length of sentences, as is requested?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eClarity\u003c/strong\u003e: Is the summary repetitive? Does the summary accurately convey\nthe core message in the fewest possible words?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAs these summary types have different purposes, additional metrics apply to\nspecific summary types:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEngagement:\u003c/strong\u003e (\u003ccode translate=\"no\" dir=\"ltr\"\u003eheadline\u003c/code\u003e): Is the summary immediately understandable to a\ngeneral audience? Does the summary use a tone that is engaging and appealing\nto a general audience?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSuccinctness\u003c/strong\u003e (\u003ccode translate=\"no\" dir=\"ltr\"\u003etl;dr\u003c/code\u003e): Is the summary clear, concise, and immediately\nunderstandable to someone with a very short attention span? Does it\neffectively distill the core message into a readily digestible form for a\nquick read?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEnticement\u003c/strong\u003e (\u003ccode translate=\"no\" dir=\"ltr\"\u003eteaser\u003c/code\u003e): Does the summary effectively create intrigue and\nencourage the reader to want to learn more by reading the full text? Does it\nuse language that is engaging and suggestive of interesting content?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe compared the output of the base model and the model with LoRA, side-by-side,\nusing the autorater. The autorater\u0026#39;s scores were averaged between 0 and 1, which\nwas then assessed against the threshold value.\u003c/p\u003e\n\n\u003cp\u003eTo ensure a well-grounded result, we reduced data variance and alleviated\npositional bias.\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eData variance reduction\u003c/strong\u003e: We averaged the scores of three independent\noutputs per input, as independent runs may have slightly different results. We\naveraged the outputs for both the base model and fine-tuned Gemini Nano.\nWhile the differences in scores across outputs were only slightly different,\naverages help us more reliably understand large sets of data.\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eAlleviate positional bias\u003c/strong\u003e: To avoid giving preference to the value of the\nsummary shared first with the rater, we evaluated the results twice, then\naveraged the final scores.\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eWe evaluated the model with LoRA, then the base model.\u003c/li\u003e\n\u003cli\u003eThen, we reversed the order. We evaluated the base model, followed by the\nmodel with LoRA.\u003c/li\u003e\n\u003cli\u003eWe averaged the final scores.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e \u003c/td\u003e\n  \u003ctd\u003e\u003cb\u003eShort\u003c/b\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cb\u003eMedium\u003c/b\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cb\u003eLong\u003c/b\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e \u003c/td\u003e\n  \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n  \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n  \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003eLoRA first\u003c/td\u003e\n  \u003ctd\u003e74.29% / 86.64%\u003c/td\u003e\n  \u003ctd\u003e76.11% / 81.38%\u003c/td\u003e\n  \u003ctd\u003e68.62% / 78.95%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003eBase model first\u003c/td\u003e\n  \u003ctd\u003e68.02% / 88.60%\u003c/td\u003e\n  \u003ctd\u003e64.97% / 87.58%\u003c/td\u003e\n  \u003ctd\u003e58.25% / 86.35%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003eVersion C (Average)\u003c/td\u003e\n  \u003ctd\u003e71.02% / 89.18%\u003c/td\u003e\n  \u003ctd\u003e69.59% / 84.08%\u003c/td\u003e\n  \u003ctd\u003e63.47% / 82.65%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003c/tbody\u003e\u003ccaption\u003eWinrates for \u003ccode translate=\"no\" dir=\"ltr\"\u003ekey-points\u003c/code\u003e summary type. Higher values are better results.\u003c/caption\u003e\n\u003c/table\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWhile the difference in scoring for outputs from the same model were only\nslightly different, averages help us more reliably understand large sets of\ndata.\u003c/p\u003e\n\n\u003cp\u003eAcross 500 articles, fine-tuned Gemini Nano performed significantly better\nthan the base model.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n   \u003ctd\u003e \u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eHeadline\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eTL;DR\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eKey-Points\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eTeaser\u003c/b\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e \u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eShort\u003c/td\u003e\n   \u003ctd\u003e74.74% / 89.12%\u003c/td\u003e\n   \u003ctd\u003e55.76% / 89.50%\u003c/td\u003e\n   \u003ctd\u003e71.02% / 89.18%\u003c/td\u003e\n   \u003ctd\u003e53.47% / 87.14%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eMedium\u003c/td\u003e\n   \u003ctd\u003e73.10% / 87.89%\u003c/td\u003e\n   \u003ctd\u003e41.82% / 81.21%\u003c/td\u003e\n   \u003ctd\u003e69.59% / 84.08%\u003c/td\u003e\n   \u003ctd\u003e48.98% / 86.74%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eLong\u003c/td\u003e\n   \u003ctd\u003e60.99% / 89.32%\u003c/td\u003e\n   \u003ctd\u003e50.51% / 84.85%\u003c/td\u003e\n   \u003ctd\u003e63.47% / 82.65%\u003c/td\u003e\n   \u003ctd\u003e62.65% / 87.55%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003c/tbody\u003e\u003ccaption\u003eAutorarer winrate across 500 articles across the different summary and length\n  types. Higher numbers indicate better results.\u003c/caption\u003e\n\u003c/table\u003e\n\n\u003cp\u003eThe same was true in our evaluation of 500 chat logs, fine-tuned Gemini Nano\noutperformed the base model.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n   \u003ctd\u003e \u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eHeadline\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eTL;DR\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eKey-Points\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eTeaser\u003c/b\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e \u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n   \u003ctd\u003eBase / With LoRA\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\u003cb\u003eShort\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e70.59% / 96.15%\u003c/td\u003e\n   \u003ctd\u003e66.27% / 97.79%\u003c/td\u003e\n   \u003ctd\u003e81.60% / 97.40%\u003c/td\u003e\n   \u003ctd\u003e67.48% / 96.14%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\u003cb\u003eMedium\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e76.67% / 95.13%\u003c/td\u003e\n   \u003ctd\u003e56.02% / 94.98%\u003c/td\u003e\n   \u003ctd\u003e82.60% / 97.20%\u003c/td\u003e\n   \u003ctd\u003e50.41% / 96.95%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\u003cb\u003eLong\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e59.03% / 94.32%\u003c/td\u003e\n   \u003ctd\u003e65.86% / 95.58%\u003c/td\u003e\n   \u003ctd\u003e75.00% / 97.60%\u003c/td\u003e\n   \u003ctd\u003e70.94% / 97.16%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003c/tbody\u003e\u003ccaption\u003eAutorater winrate across 500 chatlogs across the different summary and length\n  types. Higher numbers indicate better results.\u003c/caption\u003e\n\u003c/table\u003e\n\n\u003cp\u003eThese results demonstrate that our fine-tuning improved the overall summary\nquality.\u003c/p\u003e\n\n\n\n\u003cp\u003eTraditionally, fine-tuning is performed by adjusting the model\u0026#39;s parameters.\n\u003ca href=\"https://web.dev/articles/llm-sizes\"\u003eModern AI models are huge\u003c/a\u003e, so this\noperation is slow, expensive, and requires storing a brand new copy of the\nmodel.\u003c/p\u003e\n\n\u003cp\u003eInstead of changing all the parameters, what if we add tiny extra pieces that steer the model in the direction we want? Their smaller size enables much faster training. This is the core tenet of \u003ca href=\"https://arxiv.org/abs/2106.09685\"\u003elow-rank adaptation (LoRA)\u003c/a\u003e. With LoRA, certain pieces of the model remain unchanged, often referred to as freezing the parameters. A smaller set of data is then introduced with small corrections, focusing training efforts on these added pieces.\u003c/p\u003e\n\n\u003cp\u003eIn fact, typically you can train as few as 2% of the original parameters with\nLoRA and still see significant changes in your outputs.\u003c/p\u003e\n\n\u003cp\u003eWhy does this work? Many behaviors (such as new styles, tasks, or knowledge)\ncan be incorporated without altering every model parameter. These changes can be\nexpressed as small patterns or directions within the model\u0026#39;s existing knowledge.\nLoRA efficiently captures these patterns without touching the original model.\u003c/p\u003e\n\n\u003cp\u003eTake a look at some examples that compare the Gemini Nano base model to the\nresults after adding LoRA.\u003c/p\u003e\n\n\u003cp\u003eWe asked Gemini Nano to create a short headline that \u0026#34;captures attention and\nconveys the essence of the content\u0026#34; in the\n\u003ca href=\"https://en.wikipedia.org/wiki/Joshua_Tree_National_Park\"\u003eJoshua Tree National Park Wikipedia article\u003c/a\u003e.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n   \u003ctd\u003e\u003cb\u003eGemini Nano\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eGemini Nano with LoRA\u003c/b\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eJoshua Tree National Park\u0026#39;s Unique Landscape and History\u003c/td\u003e\n   \u003ctd\u003eJoshua Tree: National Park in California, Straddling Deserts \u0026amp; Home to Iconic Trees\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n\u003cp\u003eNext, we prompted Gemini Nano for a short teaser for the\n\u003ca href=\"https://en.wikipedia.org/wiki/Ratatouille\"\u003eRatatouille article\u003c/a\u003e. \u0026#34;Write an\nenticing one sentence that encourages users to read the full text.\u0026#34;\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n   \u003ctd\u003e\u003cb\u003eGemini Nano\u003c/b\u003e\u003c/td\u003e\n   \u003ctd\u003e\u003cb\u003eGemini Nano with LoRA\u003c/b\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eRatatouille is a beloved French Provençal dish featuring stewed\n   vegetables, often including tomatoes and garlic. Its origins can be traced\n   to the 18th century, and its preparation has evolved over time, with some\n   chefs emphasizing individual vegetable flavors while others prefer a smoother,\n   more unified texture. Beyond its French origins, ratatouille has inspired\n   similar dishes in various cuisines around the world, including piperade from\n   Spain and tian from the Mediterranean. The film \u0026#34;Ratatouille\u0026#34; (2007) further\n   popularized the dish, showcasing its culinary versatility and emotional\n   resonance.\u003c/td\u003e\n   \u003ctd\u003eRatatouille, a French Provençal dish of stewed vegetables, has a rich\n   culinary history, evolving from a simple stew into a modern culinary\n   masterpiece that has captivated audiences worldwide.\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n\u003ch3 id=\"demo\" data-text=\"Real-time inference demo\" tabindex=\"-1\"\u003eReal-time inference demo\u003c/h3\u003e\n\n\u003cp\u003eWe built an interface that demonstrates the comparison of outputs between Gemini\nNano and Gemini Nano with LoRA.\u003c/p\u003e\n\n\u003cp\u003eWe asked Gemini Nano to create a \u003ccode translate=\"no\" dir=\"ltr\"\u003etl;dr\u003c/code\u003e summary with a \u003ccode translate=\"no\" dir=\"ltr\"\u003eshort\u003c/code\u003e length for the\n\u003ca href=\"https://en.wikipedia.org/wiki/Ocean_sunfish\"\u003eOcean Sunfish\u003c/a\u003e article. Remember\nthat \u003ccode translate=\"no\" dir=\"ltr\"\u003etl;dr\u003c/code\u003e and \u003ccode translate=\"no\" dir=\"ltr\"\u003eshort\u003c/code\u003e requires a response in 1 sentence that is a \u0026#34;quick read.\u0026#34;\u003c/p\u003e\n\n\u003cfigure\u003e\n\u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" controls=\"\" width=\"700\" height=\"575\"\u003e\n  \u003csource src=\"https://developer.chrome.com/static/blog/improved-summaries-gemini-nano/video/summarization-demo.mp4\" type=\"video/mp4\"/\u003e\n\u003c/video\u003e\n\u003cfigcaption\u003eWatch in real-time how long it took to create the results and read the output.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBy implementing fine-tuning, Gemini Nano can better generate a summary that\nfollows the specific instructions.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe\u0026#39;re eager to hear your feedback on how your summaries are impacted by the\nfine-tuned Gemini Nano.\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExperiment with the \u003ca href=\"#turn-on-fine-tuning\"\u003eupdated model in Chrome Canary\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eLearm more about the \u003ca href=\"https://developer.chrome.com/docs/ai/summarizer-api\"\u003eSummarizer API\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eIf you have feedback on Chrome\u0026#39;s implementation, file a \u003ca href=\"https://issues.chromium.org/issues/new?component=1617227\u0026amp;priority=P2\u0026amp;type=bug\u0026amp;template=0\u0026amp;noWizard=true\"\u003ebug report\u003c/a\u003e or \u003ca href=\"https://issues.chromium.org/issues/new?component=1617227\u0026amp;priority=P2\u0026amp;type=feature_request\u0026amp;template=0\u0026amp;noWizard=true\"\u003efeature request\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eDiscover all of the \u003ca href=\"https://developer.chrome.com/docs/ai/built-in-apis\"\u003ebuilt-in AI APIs\u003c/a\u003e which use models,\nincluding large language models, in the browser.\u003c/p\u003e\n\n\n\n  \n\n  \n\u003c/div\u003e\n\n  \n\n  \n    \n    \n      \n    \u003cdevsite-thumb-rating position=\"footer\"\u003e\n    \u003c/devsite-thumb-rating\u003e\n  \n       \n    \n    \n  \n\n  \n  \n\u003c/article\u003e\n\n\n\u003cdevsite-content-footer\u003e\n  \u003cp\u003eExcept as otherwise noted, the content of this page is licensed under the \u003ca href=\"https://creativecommons.org/licenses/by/4.0/\"\u003eCreative Commons Attribution 4.0 License\u003c/a\u003e, and code samples are licensed under the \u003ca href=\"https://www.apache.org/licenses/LICENSE-2.0\"\u003eApache 2.0 License\u003c/a\u003e. For details, see the \u003ca href=\"https://developers.google.com/site-policies\"\u003eGoogle Developers Site Policies\u003c/a\u003e. Java is a registered trademark of Oracle and/or its affiliates.\u003c/p\u003e\n  \u003cp\u003eLast updated 2025-05-21 UTC.\u003c/p\u003e\n\u003c/devsite-content-footer\u003e\n\n\n\u003cdevsite-notification\u003e\n\u003c/devsite-notification\u003e\n\n\n  \n\u003cp\u003e\n  \n  \n    \u003ctemplate\u003e\n      [[[\u0026#34;Easy to understand\u0026#34;,\u0026#34;easyToUnderstand\u0026#34;,\u0026#34;thumb-up\u0026#34;],[\u0026#34;Solved my problem\u0026#34;,\u0026#34;solvedMyProblem\u0026#34;,\u0026#34;thumb-up\u0026#34;],[\u0026#34;Other\u0026#34;,\u0026#34;otherUp\u0026#34;,\u0026#34;thumb-up\u0026#34;]],[[\u0026#34;Missing the information I need\u0026#34;,\u0026#34;missingTheInformationINeed\u0026#34;,\u0026#34;thumb-down\u0026#34;],[\u0026#34;Too complicated / too many steps\u0026#34;,\u0026#34;tooComplicatedTooManySteps\u0026#34;,\u0026#34;thumb-down\u0026#34;],[\u0026#34;Out of date\u0026#34;,\u0026#34;outOfDate\u0026#34;,\u0026#34;thumb-down\u0026#34;],[\u0026#34;Samples / code issue\u0026#34;,\u0026#34;samplesCodeIssue\u0026#34;,\u0026#34;thumb-down\u0026#34;],[\u0026#34;Other\u0026#34;,\u0026#34;otherDown\u0026#34;,\u0026#34;thumb-down\u0026#34;]],[\u0026#34;Last updated 2025-05-21 UTC.\u0026#34;],[],[]]\n    \u003c/template\u003e\n  \n\u003c/p\u003e\n            \n          \u003c/devsite-content\u003e\n        \u003c/main\u003e\n        \n        \n        \n        \u003cdevsite-panel\u003e\n          \n        \u003c/devsite-panel\u003e\n        \n      \u003c/section\u003e\u003c/section\u003e\n    \u003cdevsite-sitemask\u003e\u003c/devsite-sitemask\u003e\n    \u003cdevsite-snackbar\u003e\u003c/devsite-snackbar\u003e\n    \u003cdevsite-tooltip\u003e\u003c/devsite-tooltip\u003e\n    \u003cdevsite-heading-link\u003e\u003c/devsite-heading-link\u003e\n    \u003cdevsite-analytics\u003e\n      \n        \n\n      \n    \u003c/devsite-analytics\u003e\n    \n      \u003cdevsite-badger\u003e\u003c/devsite-badger\u003e\n    \n    \n    \n\n\n    \u003cdevsite-a11y-announce\u003e\u003c/devsite-a11y-announce\u003e\n  \n\u003c/div\u003e",
  "readingTime": "16 min read",
  "publishedTime": null,
  "modifiedTime": null
}
