{
  "id": "1470e503-3844-4119-8670-af29868fbfa8",
  "title": "Bump’s Journey into Delightful Experiences on Android with Jetpack Compose",
  "link": "https://medium.com/androiddevelopers/bumps-journey-into-delightful-experiences-on-android-with-jetpack-compose-b2a1f8048018?source=rss----95b274b437c2---4",
  "description": "",
  "author": "Android Developers",
  "published": "Wed, 12 Mar 2025 16:04:53 GMT",
  "source": "https://medium.com/feed/androiddevelopers",
  "categories": [
    "jetpack-compose",
    "media3",
    "developer-tools",
    "app-development",
    "case-study"
  ],
  "byline": "Android Developers",
  "length": 18748,
  "excerpt": "This is a guest post from amo Engineer, Cyril Mottier, on how they’ve leveraged Android to create magic in their Android app. At amo, we are redefining what it means to build social applications. Our…",
  "siteName": "Android Developers",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "This is a guest post from amo Engineer, Cyril Mottier, on how they’ve leveraged Android to create magic in their Android app.At amo, we are redefining what it means to build social applications. Our mission is to create a new kind of social company, one that prioritizes high-quality, thoughtfully designed mobile experiences. One of our flagship applications, Bump, puts your friends on the map — whether you’re checking in on your crew or making moves to meet up.Our app leverages multiplatform technologies for its foundation. At the core lies a shared Rust-based library that powers all of our iOS and Android apps. This library, managed by our backend engineers, is responsible for persistence and networking. The library exposes its APIs as Kotlin Flow. In addition to making everything reactive and realtime-enabled by default, it integrates effortlessly with Jetpack Compose, the technology we use to build our UI. This architecture ensures a consistent and high-performance experience across platforms. It also allows mobile engineers to spend more time on the user experience where they can focus on crafting innovative and immersive user interactions.In this post, we will explore how we leverage the Android SDK, Jetpack Compose, the Kotlin programming language, and Google Play Services to build unique, delightful experiences in Bump. Our goal is to break mental barriers and show what’s truly possible on Android — often in a very few lines of code. We want to inspire engineers to think beyond conventional UI paradigms and explore new ways to create magical moments for users. By the end of this article, you’ll have a deeper understanding of how to harness Android’s capabilities to build experiences that feel like magic.At amo, we ship, learn, and iterate rapidly across our feature set. That means that the design of some of the features highlighted in this article has already changed, or will do so in the coming weeks and months.Designing for the senses vs designing for the eyesGreat touch-based UX isn’t just about flashy visuals. It’s about delivering meaningful feedback through graphics, haptics, sounds, and more. Said differently, it is about designing for all senses, not just for the eyes. We take this very seriously when designing applications and always take all of these potential dimensions into account.One example is our in-app notification center. The notification center is a visual entry point accessible from anywhere in the app which shows all of your notifications from the entire amo suite of apps. It can be moved anywhere on the screen. Its style also changes regularly thanks to some in-residence or external artists. But styling doesn’t stop at the visual level, we also style it at the audio level: when it is dragged around, a short and repeating sound is played.Turn the sound on for the full experienceTo make it fun and joyful, we pushed this even further to let the user be a DJ. The volume, the speed and the pitch of the audio change depending on where the user drags it. It is a “be your own DJ” moment. The implementation of this experience can be split in two parts. The first part deals with the audio and the second part handles the rendering of the entry point and its interactions (dragging, tapping, etc.).Let’s first dive into the code handling the audio. It consists of a Composable requiring an URL pointing to the music, a flag indicating whether it should play or not (true only when dragging) and a two-dimensional offset: X axis controls the volume, Y axis controls the playback speed \u0026 pitch.@Composablefun GalaxyGateAccessPointMusicPlayer( musicUrl: String, isActive: Boolean, offset: Offset,) { val audioPlayer = rememberAudioPlayer( uri = Uri.parse(musicUrl), ) LaunchedEffect(audioPlayer, isActive) { if (isActive) { audioPlayer.play(isLooped = true) } else { audioPlayer.pause() } } SideEffect { audioPlayer.setSpeedPitch( speed = 0.75f + offset.y * 0.5f, pitch = offset.x + 0.5f ) audioPlayer.setVolume( (1.0f - ((offset.x - 0.5f) * 2f).coerceIn(0f, 1f)), (1.0f - ((0.5f - offset.x) * 2f).coerceIn(0f, 1f)), ) }}@Composablefun rememberAudioPlayer( uri: Uri,): AudioPlayer { val context = LocalContext.current val lifecycle = LocalLifecycleOwner.current.lifecycle return remember(context, lifecycle, uri) { DefaultAudioPlayer( context = context, lifecycle = lifecycle, uri = uri, ) }}DefaultAudioPlayer is just an in-house wrapper around ExoPlayer provided by Jetpack Media3 that deals with initialization, lifecycle management, fading when starting/stopping music, etc. It exposes 2 methods setSpeedPitch and setVolume delegating to the underlying ExoPlayer.By combining gesture readings with audio pitch, speed and volume, we added delight and surprise when users didn’t expect it.Highly Unique Experiences with Animations and ShadersWe named our application “Bump” as a nod to its core feature: people close to each other can “bump” their phones together. If they are not registered as friends on the app, bumping will automatically send a friend request. And if they are, a mesmerizing animation triggers. We also notify mutual friends that they “bumped” and they should join.This Bump feature is central to the app’s experience. It stands out in its interaction, functionality, and the unique value it provides. To express its importance, we wanted the feature to have a distinctive visual appeal. Here’s a glimpse of how it currently looks in the app:There is a lot going on in this video but it can be summarized to 3 animations: the “wave” animation when the device detects a local bump/shake, the animation showing the two friends bumping and lastly a “ring pulsing” animation to finish. While the second animation is plain Compose, the two others are custom. Creating such custom effects involved venturing into what is often considered “unknown territory” in Android development: custom shaders. While daunting at first, it’s actually quite accessible and unlocks immense creative potential for truly unique experiences.Simply put, shaders are highly parallelizable code segments. Each shader runs once per pixel per frame. This might sound intense, but this is precisely where GPUs excel. In Android 13, shaders have been integrated as first-class citizens with AGSL shaders and RuntimeShader for Views and Compose.Since our app requires a minimum of API 30 (Android 11), we opted for a more traditional approach using a custom OpenGL renderer.We extract a Bitmap of the view we want to apply the effect to, pass it to the OpenGL renderer, and run the shader. While this method ensures retro-compatibility, its main drawback is that it operates on a snapshot of the view hierarchy throughout the animation. As a result, any changes occurring on the view during the animation aren’t reflected on screen until the animation concludes. Keen observers might notice a slight glitch at the end of the animation when the screenshot is removed, and normal rendering resumes.Bringing Friends to Life on the MapIn our apps, profile pictures are a bit different. Instead of static images, you record a live profile picture, a transparent, animated boomerang-like cutout. This approach feels more personal, as it literally brings your friends to life on-screen. You see them smiling or making faces, rather than just viewing curated or filtered photos from their camera roll.From a product perspective, this feature involves two key phases: the recording and the rendering. Before diving into these specific areas, let’s discuss the format we use for data transport between mobile devices (Android \u0026 iOS) and the server. To optimize bandwidth and decoding time, we chose the H.265 HEVC format in an MP4 container and perform the face detection on device. Most modern devices have hardware decoders, making decoding incredibly fast. Since cross-platform videos with transparency aren’t widely supported or optimized, we developed a custom in-house solution. Our videos consist of two “planes”:The original video on topA mask video at the bottomWe haven’t yet optimized this process. Currently, we don’t pre-apply the mask to the top plane. Doing so could reduce the final encoded video size by replacing the original background with a plain color.This format is fairly effective. For instance, the video above is only 64KB. Once we aligned all mobile platforms on the format for our animated profile pictures, we began implementing it.Recording a Live Profile PictureThe first step is capturing the video, which is handled by Jetpack CameraX. To provide users with visual feedback, we also utilize ML Kit Face Detection. Initially, we attempted to map detected facial expressions (such as eyes closed or smiling) to a 3D model rendered with Filament. However, achieving real-time performance proved too challenging for the timeframe we had. We instead decided to detect the face contour and to move a default avatar image on the screen accordingly.Once the recording is complete, Jetpack CameraX provides a video file containing the recorded sequence. This marks the beginning of the second step. The video is decoded frame by frame, and each frame is processed using ML Kit Selfie Segmentation. This API computes the face contour from the input image (our frames) and produces an output mask of the same size. Next, a composite image is generated, with the original video frame on top and the mask frame at the bottom. These composite frames are then fed into an H.265 video encoder. Once all frames are processed, the video meets the specifications described earlier and is ready to be sent to our servers.While the process could be improved with better interframe selfie segmentation, usage of depth sensors, or more advanced AI techniques, it performs well and has been successfully running in production for over a year.Rendering Your Friends on the MapPlaying back animated profile pictures presented another challenge. The main difficulty arose from what seemed like a simple product requirement: displaying 10+ real-time moving profile pictures simultaneously on the screen, animating in a back-and-forth loop (similar to boomerang videos). Video decoders, especially hardware ones, excel at decoding videos forward. However, they struggle with reverse playback. Additionally, decoding is computationally intensive. While decoding a single video is manageable, decoding 10+ videos in parallel is not. Our requirement was akin to wanting to watch 10+ movies simultaneously on your favorite streaming app, all in reverse mode. This is an uncommon and unique use case.We overcame this challenge by trading computational needs for increased memory consumption. Instead of continuously decoding video, we opted to store all frames of the animation in memory. The video is a 30fps, 2.5-second video with a resolution of 256x320 pixels and transparency. This results in a memory consumption of approximately 24MB per video. A queue-based system handling decoding requests sequentially can manage this efficiently. For each request, we:Decode the video frame by frame using Jetpack Media3 Transformer APIsFor each frame:Apply the lower part of the video as a mask to the upper part.Append the generated Bitmap to the list of frames.Upon completing this process, we obtain a List\u003cBitmap\u003e containing all the ordered, transformed (mask-applied) frames of the video. To animate the profile picture in a boomerang manner, we simply run a linear, infinite transition. This transition starts from the first frame, proceeds to the last frame, and then returns to the first frame, repeating this cycle indefinitely.@Immutableclass MovingCutout( val duration: Int, val bitmaps: List\u003cImageBitmap\u003e,) : Cutout@Composablefun rememberMovingCutoutPainter(cutout: MovingCutout): Painter { val state = rememberUpdatedState(newValue = cutout) val infiniteTransition = rememberInfiniteTransition(label = \"MovingCutoutTransition\") val currentBitmap by infiniteTransition.animateValue( initialValue = cutout.bitmaps.first(), targetValue = cutout.bitmaps.last(), typeConverter = state.VectorConverter, animationSpec = infiniteRepeatable( animation = tween(cutout.duration, easing = LinearEasing), repeatMode = RepeatMode.Reverse ), label = \"MovingCutoutFrame\" ) return remember(cutout) { // A custom BitmapPainter implementation to allow delegation when getting // 1. Intrinsic size // 2. Current Bitmap CallbackBitmapPainter( getIntrinsicSize = { with(cutout.bitmaps[0]) { Size(width.toFloat(), height.toFloat()) } }, getImageBitmap = { currentBitmap } ) }}private val State\u003cMovingCutout\u003e.VectorConverter: TwoWayConverter\u003cImageBitmap, AnimationVector1D\u003e get() = TwoWayConverter( convertToVector = { AnimationVector1D(value.bitmaps.indexOf(it).toFloat()) }, convertFromVector = { value.bitmaps[it.value.roundToInt()] } )Beyond Default: Crafting Powerful Map InteractionsAs a map-based social app, Bump relies heavily on the Google Maps Android SDK. While the framework provides default interactions, we wanted to push the boundaries of what’s possible. Specifically, users want to zoom in and out quickly. Although Google Maps offers pinch-to-zoom and double-tap gestures, these have limitations. Pinch-to-zoom requires two fingers, and double-tap doesn’t cover the full zoom range.For a better user experience, we’ve added our own gestures. One particularly useful feature is edge zoom, which allows rapid zooming in and out using a single finger. Simply swipe up or down from the left or right edge of the screen. Swiping down to the bottom zooms out completely, while swiping up to the top zooms in fully.Like Google Maps gestures, there are no visual cues for this feature, but it’s acceptable for a power gesture. We provide visual and haptic feedback to help users remember it. Currently, this is achieved with a glue-like effect that follows the finger, as shown below:Implementing this feature involves two tasks: detecting edge zoom gestures and rendering the visual effect. Thanks to Jetpack Compose’s versatility, this can be done in just a few lines of code. We use the draggable2D Modifier to detect drags, which triggers an onDragUpdate callback to update the Google Maps camera and triggers a recomposition by updating a point variable.@Composablefun EdgeZoomGestureDetector( side: EdgeZoomSide, onDragStarted: () -\u003e Unit, onDragUpdate: (Float) -\u003e Unit, onDragStopped: () -\u003e Unit, modifier: Modifier = Modifier, curveSize: Dp = 160.dp,) { var heightPx by remember { mutableIntStateOf(Int.MAX_VALUE) } var point by remember { mutableStateOf(Offset.Zero) } val draggableState = rememberDraggable2DState { delta -\u003e point = when (side) { EdgeZoomSide.Start -\u003e point + delta EdgeZoomSide.End -\u003e point + Offset(-delta.x, delta.y) } onDragUpdate(delta.y / heightPx) } val curveSizePx = with(LocalDensity.current) { curveSize.toPx() } Box( modifier = modifier .fillMaxHeight() .onPlaced { heightPx = it.size.height } .draggable2D( state = draggableState, onDragStarted = { point = it onDragStarted() }, onDragStopped = { point = point.copy(x = 0f) onDragStopped() }, ) .drawWithCache { val path = Path() onDrawBehind { path.apply { reset() val x = point.x.coerceAtMost(curveSizePx / 2f) val y = point.y val top = y - (curveSizePx - x) val bottom = y + (curveSizePx - x) moveTo(0f, top) cubicTo( 0f, top + (y - top) / 2f, x, top + (y - top) / 2f, x, y ) cubicTo( x, y + (bottom - y) / 2f, 0f, y + (bottom - y) / 2f, 0f, bottom, ) } scale(side.toXScale(), 1f) { drawPath(path, Palette.black) } } } )}enum class EdgeZoomSide(val alignment: Alignment) { Start(Alignment.CenterStart), End(Alignment.CenterEnd),}private fun EdgeZoomSide.toXScale(): Float = when (this) { EdgeZoomSide.Start -\u003e 1f EdgeZoomSide.End -\u003e -1f}The drawing part is handled by the drawBehind Modifier, which creates a Path consisting of two simple cubic curves, emulating a Gaussian curve. Before rendering it, the path is flipped on the X axis based on the screen side.This effect looks nice but it also feels static, instantly following the finger without any animation effect. To improve this, we added spring-based animation. By extracting the computation of x (representing the tip of the Gaussian curve) from drawBehind into an animatable state, we achieve a smoother visual effect:val x by animateFloatAsState( targetValue = point.x.coerceAtMost(curveSizePx / 2f), label = \"animated-curve-width\",)This creates a visually appealing effect that feels natural. However, we wanted to engage other senses too, so we introduced haptic feedback to mimic the feel of a toothed wheel on an old safe. Using Kotlin Flow and LaunchedEffect and snapshotFlow this was implemented in a very few lines of code:val haptic = LocalHapticFeedback.currentLaunchedEffect(heightPx, slotCount) { val slotHeight = heightPx / slotCount snapshotFlow { (point.y / slotHeight).toInt() } .drop(1) // Drop the initial \"tick\" .collect { haptic.performHapticFeedback(HapticFeedbackType.SegmentTick) }}ConclusionBump is filled with many other innovative features. We invite you to explore the product further to discover more of these gems. Overall, the entire Android ecosystem — including the platform, developer tools, Jetpack Compose, Google Play Services — provided most of the necessary building blocks. It offered the flexibility needed to design and implement these unique interactions. Thanks to Android, creating a standout product is just a matter of passion, time, and more than a few lines of code!",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*SFQ6X7A3SbzEQL6l8S0nhg.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca rel=\"noopener follow\" href=\"https://medium.com/@AndroidDev?source=post_page---byline--b2a1f8048018---------------------------------------\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Android Developers\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*VglQS9HKgUvUuAX36Np5qQ.png\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/androiddevelopers?source=post_page---byline--b2a1f8048018---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Android Developers\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*4Tg6pPzer7cIarYaszIKaQ.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cp id=\"5ad4\"\u003e\u003cem\u003eThis is a guest post from amo Engineer, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/cyrilmottier/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eCyril Mottier\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, on how they’ve leveraged Android to create magic in their Android app.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"f52b\"\u003eAt \u003ca href=\"https://amo.co/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eamo\u003c/a\u003e, we are redefining what it means to build social applications. Our mission is to create a new kind of social company, one that prioritizes high-quality, thoughtfully designed mobile experiences. One of our flagship applications, \u003ca href=\"https://play.google.com/store/apps/details?id=co.amo.android.location\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBump\u003c/a\u003e, puts your friends on the map — whether you’re checking in on your crew or making moves to meet up.\u003c/p\u003e\u003cp id=\"adea\"\u003eOur app leverages multiplatform technologies for its foundation. At the core lies a shared Rust-based library that powers all of our iOS and Android apps. This library, managed by our backend engineers, is responsible for persistence and networking. The library exposes its APIs as Kotlin Flow. In addition to making everything reactive and realtime-enabled by default, it integrates effortlessly with Jetpack Compose, the technology we use to build our UI. This architecture ensures a consistent and high-performance experience across platforms. It also allows mobile engineers to spend more time on the user experience where they can focus on crafting innovative and immersive user interactions.\u003c/p\u003e\u003cp id=\"aaf9\"\u003eIn this post, we will explore how we leverage the Android SDK, \u003ca href=\"https://developer.android.com/compose\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJetpack Compose\u003c/a\u003e, the Kotlin programming language, and Google Play Services to build unique, delightful experiences in Bump. Our goal is to break mental barriers and show what’s truly possible on Android — often in a very few lines of code. We want to inspire engineers to think beyond conventional UI paradigms and explore new ways to create magical moments for users. By the end of this article, you’ll have a deeper understanding of how to harness Android’s capabilities to build experiences that feel like magic.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"4f1b\"\u003eAt amo, we ship, learn, and iterate rapidly across our feature set. That means that the design of some of the features highlighted in this article has already changed, or will do so in the coming weeks and months.\u003c/p\u003e\u003c/blockquote\u003e\u003ch2 id=\"a64e\"\u003eDesigning for the senses vs designing for the eyes\u003c/h2\u003e\u003cp id=\"1978\"\u003eGreat touch-based UX isn’t just about flashy visuals. It’s about delivering meaningful feedback through graphics, haptics, sounds, and more. Said differently, it is about designing for all senses, not just for the eyes. We take this very seriously when designing applications and always take all of these potential dimensions into account.\u003c/p\u003e\u003cp id=\"7a14\"\u003eOne example is our in-app notification center. The notification center is a visual entry point accessible from anywhere in the app which shows all of your notifications from the entire amo suite of apps. It can be moved anywhere on the screen. Its style also changes regularly thanks to some in-residence or external artists. But styling doesn’t stop at the visual level, we also style it at the audio level: when it is dragged around, a short and repeating sound is played.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eTurn the sound on for the full experience\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"c473\"\u003eTo make it fun and joyful, we pushed this even further to let the user be a DJ. The volume, the speed and the pitch of the audio change depending on where the user drags it. It is a “be your own DJ” moment. The implementation of this experience can be split in two parts. The first part deals with the audio and the second part handles the rendering of the entry point and its interactions (dragging, tapping, etc.).\u003c/p\u003e\u003cp id=\"0b90\"\u003eLet’s first dive into the code handling the audio. It consists of a Composable requiring an URL pointing to the music, a flag indicating whether it should play or not (true only when dragging) and a two-dimensional offset: X axis controls the volume, Y axis controls the playback speed \u0026amp; pitch.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"311c\"\u003e@Composable\u003cbr/\u003efun GalaxyGateAccessPointMusicPlayer(\u003cbr/\u003e    musicUrl: String,\u003cbr/\u003e    isActive: Boolean,\u003cbr/\u003e    offset: Offset,\u003cbr/\u003e) {\u003cbr/\u003e    val audioPlayer = rememberAudioPlayer(\u003cbr/\u003e        uri = Uri.parse(musicUrl),\u003cbr/\u003e    )\u003cbr/\u003e    LaunchedEffect(audioPlayer, isActive) {\u003cbr/\u003e        if (isActive) {\u003cbr/\u003e            audioPlayer.play(isLooped = true)\u003cbr/\u003e        } else {\u003cbr/\u003e            audioPlayer.pause()\u003cbr/\u003e        }\u003cbr/\u003e    }\u003cp\u003e    SideEffect {\u003cbr/\u003e        audioPlayer.setSpeedPitch(\u003cbr/\u003e            speed = 0.75f + offset.y * 0.5f,\u003cbr/\u003e            pitch = offset.x + 0.5f\u003cbr/\u003e        )\u003cbr/\u003e        audioPlayer.setVolume(\u003cbr/\u003e            (1.0f - ((offset.x - 0.5f) * 2f).coerceIn(0f, 1f)),\u003cbr/\u003e            (1.0f - ((0.5f - offset.x) * 2f).coerceIn(0f, 1f)),\u003cbr/\u003e        )\u003cbr/\u003e    }\u003cbr/\u003e}\u003c/p\u003e\u003cp\u003e@Composable\u003cbr/\u003efun rememberAudioPlayer(\u003cbr/\u003e    uri: Uri,\u003cbr/\u003e): AudioPlayer {\u003cbr/\u003e    val context = LocalContext.current\u003cbr/\u003e    val lifecycle = LocalLifecycleOwner.current.lifecycle\u003cbr/\u003e    return remember(context, lifecycle, uri) {\u003cbr/\u003e        DefaultAudioPlayer(\u003cbr/\u003e            context = context,\u003cbr/\u003e            lifecycle = lifecycle,\u003cbr/\u003e            uri = uri,\u003cbr/\u003e        )\u003cbr/\u003e    }\u003cbr/\u003e}\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"ce65\"\u003e\u003ccode\u003eDefaultAudioPlayer\u003c/code\u003e is just an in-house wrapper around \u003ccode\u003eExoPlayer\u003c/code\u003e provided by \u003ca href=\"https://developer.android.com/media/media3\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJetpack Media3 \u003c/a\u003ethat deals with initialization, lifecycle management, fading when starting/stopping music, etc. It exposes 2 methods \u003ccode\u003esetSpeedPitch\u003c/code\u003e and \u003ccode\u003esetVolume\u003c/code\u003e delegating to the underlying \u003ccode\u003eExoPlayer\u003c/code\u003e.\u003c/p\u003e\u003cp id=\"1271\"\u003eBy combining gesture readings with audio pitch, speed and volume, we added delight and surprise when users didn’t expect it.\u003c/p\u003e\u003ch2 id=\"eb4e\"\u003eHighly Unique Experiences with Animations and Shaders\u003c/h2\u003e\u003cp id=\"0c06\"\u003eWe named our application “Bump” as a nod to its core feature: people close to each other can “bump” their phones together. If they are not registered as friends on the app, bumping will automatically send a friend request. And if they are, a mesmerizing animation triggers. We also notify mutual friends that they “bumped” and they should join.\u003c/p\u003e\u003cp id=\"c3e8\"\u003eThis Bump feature is central to the app’s experience. It stands out in its interaction, functionality, and the unique value it provides. To express its importance, we wanted the feature to have a distinctive visual appeal. Here’s a glimpse of how it currently looks in the app:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"497a\"\u003eThere is a lot going on in this video but it can be summarized to 3 animations: the “wave” animation when the device detects a local bump/shake, the animation showing the two friends bumping and lastly a “ring pulsing” animation to finish. While the second animation is plain Compose, the two others are custom. Creating such custom effects involved venturing into what is often considered “unknown territory” in Android development: custom shaders. While daunting at first, it’s actually quite accessible and unlocks immense creative potential for truly unique experiences.\u003c/p\u003e\u003cp id=\"afdf\"\u003eSimply put, shaders are highly parallelizable code segments. Each shader runs once per pixel per frame. This might sound intense, but this is precisely where GPUs excel. In Android 13, shaders have been integrated as first-class citizens with \u003ca href=\"https://developer.android.com/develop/ui/compose/graphics/draw/brush#advanced-example\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAGSL shaders and \u003c/a\u003e\u003ccode\u003e\u003ca href=\"https://developer.android.com/develop/ui/compose/graphics/draw/brush#advanced-example\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRuntimeShader\u003c/a\u003e\u003c/code\u003e for Views and Compose.\u003c/p\u003e\u003cp id=\"ece5\"\u003eSince our app requires a minimum of API 30 (Android 11), we opted for a more traditional approach using a custom OpenGL renderer.\u003c/p\u003e\u003cp id=\"3802\"\u003eWe extract a \u003ccode\u003eBitmap\u003c/code\u003e of the view we want to apply the effect to, pass it to the OpenGL renderer, and run the shader. While this method ensures retro-compatibility, its main drawback is that it operates on a snapshot of the view hierarchy throughout the animation. As a result, any changes occurring on the view during the animation aren’t reflected on screen until the animation concludes. Keen observers might notice a slight glitch at the end of the animation when the screenshot is removed, and normal rendering resumes.\u003c/p\u003e\u003ch2 id=\"a5e3\"\u003eBringing Friends to Life on the Map\u003c/h2\u003e\u003cp id=\"0ffb\"\u003eIn our apps, profile pictures are a bit different. Instead of static images, you record a live profile picture, a transparent, animated boomerang-like cutout. This approach feels more personal, as it literally brings your friends to life on-screen. You see them smiling or making faces, rather than just viewing curated or filtered photos from their camera roll.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"b38f\"\u003eFrom a product perspective, this feature involves two key phases: the recording and the rendering. Before diving into these specific areas, let’s discuss the format we use for data transport between mobile devices (Android \u0026amp; iOS) and the server. To optimize bandwidth and decoding time, we chose the \u003ca href=\"https://developer.android.com/media/platform/supported-formats\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eH.265 HEVC\u003c/a\u003e format in an MP4 container and perform the face detection on device. Most modern devices have hardware decoders, making decoding incredibly fast. Since cross-platform videos with transparency aren’t widely supported or optimized, we developed a custom in-house solution. Our videos consist of two “planes”:\u003c/p\u003e\u003cul\u003e\u003cli id=\"48de\"\u003eThe original video on top\u003c/li\u003e\u003cli id=\"8fd3\"\u003eA mask video at the bottom\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cblockquote\u003e\u003cp id=\"2803\"\u003eWe haven’t yet optimized this process. Currently, we don’t pre-apply the mask to the top plane. Doing so could reduce the final encoded video size by replacing the original background with a plain color.\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"d991\"\u003eThis format is fairly effective. For instance, the video above is only 64KB. Once we aligned all mobile platforms on the format for our animated profile pictures, we began implementing it.\u003c/p\u003e\u003ch2 id=\"7e65\"\u003eRecording a Live Profile Picture\u003c/h2\u003e\u003cp id=\"61ff\"\u003eThe first step is capturing the video, which is handled by \u003ca href=\"https://developer.android.com/media/camera/camerax\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJetpack CameraX\u003c/a\u003e. To provide users with visual feedback, we also utilize \u003ca href=\"https://developers.google.com/ml-kit/vision/face-detection\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eML Kit Face Detection\u003c/a\u003e. Initially, we attempted to map detected facial expressions (such as eyes closed or smiling) to a 3D model rendered with \u003ca href=\"https://github.com/google/filament\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFilament\u003c/a\u003e. However, achieving real-time performance proved too challenging for the timeframe we had. We instead decided to detect the face contour and to move a default avatar image on the screen accordingly.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"8cb7\"\u003eOnce the recording is complete, Jetpack CameraX provides a video file containing the recorded sequence. This marks the beginning of the second step. The video is decoded frame by frame, and each frame is processed using \u003ca href=\"https://developers.google.com/ml-kit/vision/selfie-segmentation\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eML Kit Selfie Segmentation\u003c/a\u003e. This API computes the face contour from the input image (our frames) and produces an output mask of the same size. Next, a composite image is generated, with the original video frame on top and the mask frame at the bottom. These composite frames are then fed into an H.265 video encoder. Once all frames are processed, the video meets the specifications described earlier and is ready to be sent to our servers.\u003c/p\u003e\u003cp id=\"22d1\"\u003eWhile the process could be improved with better interframe selfie segmentation, usage of depth sensors, or more advanced AI techniques, it performs well and has been successfully running in production for over a year.\u003c/p\u003e\u003ch2 id=\"44c8\"\u003eRendering Your Friends on the Map\u003c/h2\u003e\u003cp id=\"32fa\"\u003ePlaying back animated profile pictures presented another challenge. The main difficulty arose from what seemed like a simple product requirement: displaying 10+ real-time moving profile pictures simultaneously on the screen, animating in a back-and-forth loop (similar to boomerang videos). Video decoders, especially hardware ones, excel at decoding videos forward. However, they struggle with reverse playback. Additionally, decoding is computationally intensive. While decoding a single video is manageable, decoding 10+ videos in parallel is not. Our requirement was akin to wanting to watch 10+ movies simultaneously on your favorite streaming app, all in reverse mode. This is an uncommon and unique use case.\u003c/p\u003e\u003cp id=\"076e\"\u003eWe overcame this challenge by trading computational needs for increased memory consumption. Instead of continuously decoding video, we opted to store all frames of the animation in memory. The video is a 30fps, 2.5-second video with a resolution of 256x320 pixels and transparency. This results in a memory consumption of approximately 24MB per video. A queue-based system handling decoding requests sequentially can manage this efficiently. For each request, we:\u003c/p\u003e\u003col\u003e\u003cli id=\"3e1d\"\u003eDecode the video frame by frame using Jetpack Media3 Transformer APIs\u003c/li\u003e\u003cli id=\"0782\"\u003eFor each frame:\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli id=\"8e14\"\u003eApply the lower part of the video as a mask to the upper part.\u003c/li\u003e\u003cli id=\"4288\"\u003eAppend the generated Bitmap to the list of frames.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"d1e6\"\u003eUpon completing this process, we obtain a \u003ccode\u003eList\u0026lt;Bitmap\u0026gt;\u003c/code\u003e containing all the ordered, transformed (mask-applied) frames of the video. To animate the profile picture in a boomerang manner, we simply run a linear, infinite transition. This transition starts from the first frame, proceeds to the last frame, and then returns to the first frame, repeating this cycle indefinitely.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"39c4\"\u003e@Immutable\u003cbr/\u003eclass MovingCutout(\u003cbr/\u003e    val duration: Int,\u003cbr/\u003e    val bitmaps: List\u0026lt;ImageBitmap\u0026gt;,\u003cbr/\u003e) : Cutout\u003cp\u003e@Composable\u003cbr/\u003efun rememberMovingCutoutPainter(cutout: MovingCutout): Painter {\u003cbr/\u003e    val state = rememberUpdatedState(newValue = cutout)\u003cbr/\u003e    val infiniteTransition = rememberInfiniteTransition(label = \u0026#34;MovingCutoutTransition\u0026#34;)\u003cbr/\u003e    val currentBitmap by infiniteTransition.animateValue(\u003cbr/\u003e        initialValue = cutout.bitmaps.first(),\u003cbr/\u003e        targetValue = cutout.bitmaps.last(),\u003cbr/\u003e        typeConverter = state.VectorConverter,\u003cbr/\u003e        animationSpec = infiniteRepeatable(\u003cbr/\u003e            animation = tween(cutout.duration, easing = LinearEasing),\u003cbr/\u003e            repeatMode = RepeatMode.Reverse\u003cbr/\u003e        ),\u003cbr/\u003e        label = \u0026#34;MovingCutoutFrame\u0026#34;\u003cbr/\u003e    )\u003cbr/\u003e    return remember(cutout) {\u003cbr/\u003e        // A custom BitmapPainter implementation to allow delegation when getting\u003cbr/\u003e        //  1. Intrinsic size\u003cbr/\u003e        //  2. Current Bitmap\u003cbr/\u003e        CallbackBitmapPainter( \u003cbr/\u003e            getIntrinsicSize = {\u003cbr/\u003e                with(cutout.bitmaps[0]) { Size(width.toFloat(), height.toFloat()) }\u003cbr/\u003e            },\u003cbr/\u003e            getImageBitmap = { currentBitmap }\u003cbr/\u003e        )\u003cbr/\u003e    }\u003cbr/\u003e}\u003c/p\u003e\u003cp\u003eprivate val State\u0026lt;MovingCutout\u0026gt;.VectorConverter: TwoWayConverter\u0026lt;ImageBitmap, AnimationVector1D\u0026gt;\u003cbr/\u003e    get() = TwoWayConverter(\u003cbr/\u003e        convertToVector = { AnimationVector1D(value.bitmaps.indexOf(it).toFloat()) },\u003cbr/\u003e        convertFromVector = { value.bitmaps[it.value.roundToInt()] }\u003cbr/\u003e    )\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003ch2 id=\"22fe\"\u003eBeyond Default: Crafting Powerful Map Interactions\u003c/h2\u003e\u003cp id=\"c1cc\"\u003eAs a map-based social app, Bump relies heavily on the \u003ca href=\"https://github.com/googlemaps/android-maps-compose\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGoogle Maps Android SDK\u003c/a\u003e. While the framework provides default interactions, we wanted to push the boundaries of what’s possible. Specifically, users want to zoom in and out quickly. Although Google Maps offers pinch-to-zoom and double-tap gestures, these have limitations. Pinch-to-zoom requires two fingers, and double-tap doesn’t cover the full zoom range.\u003c/p\u003e\u003cp id=\"1d0a\"\u003eFor a better user experience, we’ve added our own gestures. One particularly useful feature is edge zoom, which allows rapid zooming in and out using a single finger. Simply swipe up or down from the left or right edge of the screen. Swiping down to the bottom zooms out completely, while swiping up to the top zooms in fully.\u003c/p\u003e\u003cp id=\"88a2\"\u003eLike Google Maps gestures, there are no visual cues for this feature, but it’s acceptable for a power gesture. We provide visual and haptic feedback to help users remember it. Currently, this is achieved with a glue-like effect that follows the finger, as shown below:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"3ee9\"\u003eImplementing this feature involves two tasks: detecting edge zoom gestures and rendering the visual effect. Thanks to Jetpack Compose’s versatility, this can be done in just a few lines of code. We use the \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/compose/foundation/gestures/package-summary#(androidx.compose.ui.Modifier).draggable2D(androidx.compose.foundation.gestures.Draggable2DState,kotlin.Boolean,androidx.compose.foundation.interaction.MutableInteractionSource,kotlin.Boolean,kotlin.Function1,kotlin.Function1,kotlin.Boolean)\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edraggable2D\u003c/a\u003e\u003c/code\u003e Modifier to detect drags, which triggers an \u003ccode\u003eonDragUpdate\u003c/code\u003e callback to update the Google Maps camera and triggers a recomposition by updating a point variable.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"acbd\"\u003e@Composable\u003cbr/\u003efun EdgeZoomGestureDetector(\u003cbr/\u003e    side: EdgeZoomSide,\u003cbr/\u003e    onDragStarted: () -\u0026gt; Unit,\u003cbr/\u003e    onDragUpdate: (Float) -\u0026gt; Unit,\u003cbr/\u003e    onDragStopped: () -\u0026gt; Unit,\u003cbr/\u003e    modifier: Modifier = Modifier,\u003cbr/\u003e    curveSize: Dp = 160.dp,\u003cbr/\u003e) {\u003cbr/\u003e    var heightPx by remember { mutableIntStateOf(Int.MAX_VALUE) }\u003cbr/\u003e    var point by remember { mutableStateOf(Offset.Zero) }\u003cbr/\u003e    val draggableState = rememberDraggable2DState { delta -\u0026gt;\u003cbr/\u003e        point = when (side) {\u003cbr/\u003e            EdgeZoomSide.Start -\u0026gt; point + delta\u003cbr/\u003e            EdgeZoomSide.End -\u0026gt; point + Offset(-delta.x, delta.y)\u003cbr/\u003e        }\u003cbr/\u003e        onDragUpdate(delta.y / heightPx)\u003cbr/\u003e    }\u003cbr/\u003e    val curveSizePx = with(LocalDensity.current) { curveSize.toPx() }\u003cp\u003e        Box(\u003cbr/\u003e        modifier = modifier\u003cbr/\u003e            .fillMaxHeight()\u003cbr/\u003e            .onPlaced {\u003cbr/\u003e                heightPx = it.size.height\u003cbr/\u003e            }\u003cbr/\u003e            .draggable2D(\u003cbr/\u003e                state = draggableState,\u003cbr/\u003e                onDragStarted = {\u003cbr/\u003e                    point = it\u003cbr/\u003e                    onDragStarted()\u003cbr/\u003e                },\u003cbr/\u003e                onDragStopped = {\u003cbr/\u003e                    point = point.copy(x = 0f)\u003cbr/\u003e                    onDragStopped()\u003cbr/\u003e                },\u003cbr/\u003e            )\u003cbr/\u003e            .drawWithCache {\u003cbr/\u003e                val path = Path()\u003cbr/\u003e                onDrawBehind {\u003cbr/\u003e                    path.apply {\u003cbr/\u003e                        reset()\u003cbr/\u003e                        val x = point.x.coerceAtMost(curveSizePx / 2f)\u003cbr/\u003e                        val y = point.y\u003cbr/\u003e                        val top = y - (curveSizePx - x)\u003cbr/\u003e                        val bottom = y + (curveSizePx - x)\u003cbr/\u003e                        moveTo(0f, top)\u003cbr/\u003e                        cubicTo(\u003cbr/\u003e                            0f, top + (y - top) / 2f,\u003cbr/\u003e                            x, top + (y - top) / 2f,\u003cbr/\u003e                            x, y\u003cbr/\u003e                        )\u003cbr/\u003e                        cubicTo(\u003cbr/\u003e                            x, y + (bottom - y) / 2f,\u003cbr/\u003e                            0f, y + (bottom - y) / 2f,\u003cbr/\u003e                            0f, bottom,\u003cbr/\u003e                        )\u003cbr/\u003e                    }\u003c/p\u003e\u003cp\u003e                    scale(side.toXScale(), 1f) {\u003cbr/\u003e                        drawPath(path, Palette.black)\u003cbr/\u003e                    }\u003cbr/\u003e                }\u003cbr/\u003e            }\u003cbr/\u003e    )\u003cbr/\u003e}\u003c/p\u003e\u003cp\u003eenum class EdgeZoomSide(val alignment: Alignment) {\u003cbr/\u003e    Start(Alignment.CenterStart),\u003cbr/\u003e    End(Alignment.CenterEnd),\u003cbr/\u003e}\u003c/p\u003e\u003cp\u003eprivate fun EdgeZoomSide.toXScale(): Float = when (this) {\u003cbr/\u003e    EdgeZoomSide.Start -\u0026gt; 1f\u003cbr/\u003e    EdgeZoomSide.End -\u0026gt; -1f\u003cbr/\u003e}\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"aa2c\"\u003eThe drawing part is handled by the \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/compose/ui/draw/package-summary#(androidx.compose.ui.Modifier).drawBehind(kotlin.Function1)\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edrawBehind\u003c/a\u003e\u003c/code\u003e Modifier, which creates a \u003ccode\u003ePath\u003c/code\u003e consisting of two simple cubic curves, emulating a Gaussian curve. Before rendering it, the path is flipped on the X axis based on the screen side.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"9f09\"\u003eThis effect looks nice but it also feels static, instantly following the finger without any animation effect. To improve this, we added spring-based animation. By extracting the computation of \u003ccode\u003ex\u003c/code\u003e (representing the tip of the Gaussian curve) from \u003ccode\u003edrawBehind\u003c/code\u003e into an animatable state, we achieve a smoother visual effect:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"bee2\"\u003eval x by animateFloatAsState(\u003cbr/\u003e    targetValue = point.x.coerceAtMost(curveSizePx / 2f),\u003cbr/\u003e    label = \u0026#34;animated-curve-width\u0026#34;,\u003cbr/\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"4150\"\u003eThis creates a visually appealing effect that feels natural. However, we wanted to engage other senses too, so we introduced haptic feedback to mimic the feel of a toothed wheel on an old safe. Using Kotlin \u003ccode\u003eFlow\u003c/code\u003e and \u003ccode\u003eLaunchedEffect\u003c/code\u003e and \u003ccode\u003esnapshotFlow\u003c/code\u003e this was implemented in a very few lines of code:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"392e\"\u003eval haptic = LocalHapticFeedback.current\u003cbr/\u003eLaunchedEffect(heightPx, slotCount) {\u003cbr/\u003e    val slotHeight = heightPx / slotCount\u003cbr/\u003e    snapshotFlow { (point.y / slotHeight).toInt() }\u003cbr/\u003e        .drop(1) // Drop the initial \u0026#34;tick\u0026#34;\u003cbr/\u003e        .collect {\u003cbr/\u003e            haptic.performHapticFeedback(HapticFeedbackType.SegmentTick)\u003cbr/\u003e        }\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003ch2 id=\"73b0\"\u003eConclusion\u003c/h2\u003e\u003cp id=\"9552\"\u003eBump is filled with many other innovative features. We invite you to explore the product further to discover more of these gems. Overall, the entire Android ecosystem — including the platform, developer tools, Jetpack Compose, Google Play Services — provided most of the necessary building blocks. It offered the flexibility needed to design and implement these unique interactions. Thanks to Android, creating a standout product is just a matter of passion, time, and more than a few lines of code!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "20 min read",
  "publishedTime": "2025-03-12T16:04:53.498Z",
  "modifiedTime": null
}
