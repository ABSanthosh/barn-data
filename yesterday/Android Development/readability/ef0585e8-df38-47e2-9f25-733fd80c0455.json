{
  "id": "ef0585e8-df38-47e2-9f25-733fd80c0455",
  "title": "Building delightful Android camera and media experiences",
  "link": "http://android-developers.googleblog.com/2025/05/building-delightful-android-camera-media-experiences.html",
  "description": "",
  "author": "Android Developers",
  "published": "2025-05-07T14:00:00.000-07:00",
  "source": "http://feeds.feedburner.com/blogspot/hsDu",
  "categories": [
    "#Media",
    "Camera",
    "Featured",
    "Media"
  ],
  "byline": "",
  "length": 20241,
  "excerpt": "The Camera \u0026 Media team shares learnings from creating sample code and demos, highlighting Jetpack libraries for future-proof apps, Media3, and more.",
  "siteName": "Android Developers Blog",
  "favicon": "",
  "text": "Posted by Donovan McMurray, Mayuri Khinvasara Khabya, Mozart Louis, and Nevin Mital – Developer Relations Engineers Hello Android Developers! We are the Android Developer Relations Camera \u0026 Media team, and we’re excited to bring you something a little different today. Over the past several months, we’ve been hard at work writing sample code and building demos that showcase how to take advantage of all the great potential Android offers for building delightful user experiences. Some of these efforts are available for you to explore now, and some you’ll see later throughout the year, but for this blog post we thought we’d share some of the learnings we gathered while going through this exercise. Grab your favorite Android plush or rubber duck, and read on to see what we’ve been up to! Future-proof your app with Jetpack Nevin MitalOne of our focuses for the past several years has been improving the developer tools available for video editing on Android. This led to the creation of the Jetpack Media3 Transformer APIs, which offer solutions for both single-asset and multi-asset video editing preview and export. Today, I’d like to focus on the Composition demo app, a sample app that showcases some of the multi-asset editing experiences that Transformer enables. I started by adding a custom video compositor to demonstrate how you can arrange input video sequences into different layouts for your final composition, such as a 2x2 grid or a picture-in-picture overlay. You can customize this by implementing a VideoCompositorSettings and overriding the getOverlaySettings method. This object can then be set when building your Composition with setVideoCompositorSettings. Here is an example for the 2x2 grid layout: object : VideoCompositorSettings { ... override fun getOverlaySettings(inputId: Int, presentationTimeUs: Long): OverlaySettings { return when (inputId) { 0 -\u003e { // First sequence is placed in the top left StaticOverlaySettings.Builder() .setScale(0.5f, 0.5f) .setOverlayFrameAnchor(0f, 0f) // Middle of overlay .setBackgroundFrameAnchor(-0.5f, 0.5f) // Top-left section of background .build() } 1 -\u003e { // Second sequence is placed in the top right StaticOverlaySettings.Builder() .setScale(0.5f, 0.5f) .setOverlayFrameAnchor(0f, 0f) // Middle of overlay .setBackgroundFrameAnchor(0.5f, 0.5f) // Top-right section of background .build() } 2 -\u003e { // Third sequence is placed in the bottom left StaticOverlaySettings.Builder() .setScale(0.5f, 0.5f) .setOverlayFrameAnchor(0f, 0f) // Middle of overlay .setBackgroundFrameAnchor(-0.5f, -0.5f) // Bottom-left section of background .build() } 3 -\u003e { // Fourth sequence is placed in the bottom right StaticOverlaySettings.Builder() .setScale(0.5f, 0.5f) .setOverlayFrameAnchor(0f, 0f) // Middle of overlay .setBackgroundFrameAnchor(0.5f, -0.5f) // Bottom-right section of background .build() } else -\u003e { StaticOverlaySettings.Builder().build() } } } } Since getOverlaySettings also provides a presentation time, we can even animate the layout, such as in this picture-in-picture example: Next, I spent some time migrating the Composition demo app to use Jetpack Compose. With complicated editing flows, it can help to take advantage of as much screen space as is available, so I decided to use the supporting pane adaptive layout. This way, the user can fine-tune their video creation on the preview screen, and export options are only shown at the same time on a larger display. Below, you can see how the UI dynamically adapts to the screen size on a foldable device, when switching from the outer screen to the inner screen and vice versa. What’s great is that by using Jetpack Media3 and Jetpack Compose, these features also carry over seamlessly to other devices and form factors, such as the new Android XR platform. Right out-of-the-box, I was able to run the demo app in Home Space with the 2D UI I already had. And with some small updates, I was even able to adapt the UI specifically for XR with features such as multiple panels, and to take further advantage of the extra space, an Orbiter with playback controls for the editing preview. What’s great is that by using Jetpack Media3 and Jetpack Compose, these features also carry over seamlessly to other devices and form factors, such as the new Android XR platform. Right out-of-the-box, I was able to run the demo app in Home Space with the 2D UI I already had. And with some small updates, I was even able to adapt the UI specifically for XR with features such as multiple panels, and to take further advantage of the extra space, an Orbiter with playback controls for the editing preview. Orbiter( position = OrbiterEdge.Bottom, offset = EdgeOffset.inner(offset = MaterialTheme.spacing.standard), alignment = Alignment.CenterHorizontally, shape = SpatialRoundedCornerShape(CornerSize(28.dp)) ) { Row (horizontalArrangement = Arrangement.spacedBy(MaterialTheme.spacing.mini)) { // Playback control for rewinding by 10 seconds FilledTonalIconButton({ viewModel.seekBack(10_000L) }) { Icon( painter = painterResource(id = R.drawable.rewind_10), contentDescription = \"Rewind by 10 seconds\" ) } // Playback control for play/pause FilledTonalIconButton({ viewModel.togglePlay() }) { Icon( painter = painterResource(id = R.drawable.rounded_play_pause_24), contentDescription = if(viewModel.compositionPlayer.isPlaying) { \"Pause preview playback\" } else { \"Resume preview playback\" } ) } // Playback control for forwarding by 10 seconds FilledTonalIconButton({ viewModel.seekForward(10_000L) }) { Icon( painter = painterResource(id = R.drawable.forward_10), contentDescription = \"Forward by 10 seconds\" ) } } } Jetpack libraries unlock premium functionality incrementally Donovan McMurrayNot only do our Jetpack libraries have you covered by working consistently across existing and future devices, but they also open the doors to advanced functionality and custom behaviors to support all types of app experiences. In a nutshell, our Jetpack libraries aim to make the common case very accessible and easy, and it has hooks for adding more custom features later. We’ve worked with many apps who have switched to a Jetpack library, built the basics, added their critical custom features, and actually saved developer time over their estimates. Let’s take a look at CameraX and how this incremental development can supercharge your process. // Set up CameraX app with preview and image capture. // Note: setting the resolution selector is optional, and if not set, // then a default 4:3 ratio will be used. val aspectRatioStrategy = AspectRatioStrategy( AspectRatio.RATIO_16_9, AspectRatioStrategy.FALLBACK_RULE_NONE) var resolutionSelector = ResolutionSelector.Builder() .setAspectRatioStrategy(aspectRatioStrategy) .build() private val previewUseCase = Preview.Builder() .setResolutionSelector(resolutionSelector) .build() private val imageCaptureUseCase = ImageCapture.Builder() .setResolutionSelector(resolutionSelector) .setCaptureMode(ImageCapture.CAPTURE_MODE_MINIMIZE_LATENCY) .build() val useCaseGroupBuilder = UseCaseGroup.Builder() .addUseCase(previewUseCase) .addUseCase(imageCaptureUseCase) cameraProvider.unbindAll() camera = cameraProvider.bindToLifecycle( this, // lifecycleOwner CameraSelector.DEFAULT_BACK_CAMERA, useCaseGroupBuilder.build(), ) After setting up the basic structure for CameraX, you can set up a simple UI with a camera preview and a shutter button. You can use the CameraX Viewfinder composable which displays a Preview stream from a CameraX SurfaceRequest. // Create preview Box( Modifier .background(Color.Black) .fillMaxSize(), contentAlignment = Alignment.Center, ) { surfaceRequest?.let { CameraXViewfinder( modifier = Modifier.fillMaxSize(), implementationMode = ImplementationMode.EXTERNAL, surfaceRequest = surfaceRequest, ) } Button( onClick = onPhotoCapture, shape = CircleShape, colors = ButtonDefaults.buttonColors(containerColor = Color.White), modifier = Modifier .height(75.dp) .width(75.dp), ) } fun onPhotoCapture() { // Not shown: defining the ImageCapture.OutputFileOptions for // your saved images imageCaptureUseCase.takePicture( outputOptions, ContextCompat.getMainExecutor(context), object : ImageCapture.OnImageSavedCallback { override fun onError(exc: ImageCaptureException) { val msg = \"Photo capture failed.\" Toast.makeText(context, msg, Toast.LENGTH_SHORT).show() } override fun onImageSaved(output: ImageCapture.OutputFileResults) { val savedUri = output.savedUri if (savedUri != null) { // Do something with the savedUri if needed } else { val msg = \"Photo capture failed.\" Toast.makeText(context, msg, Toast.LENGTH_SHORT).show() } } }, ) } You’re already on track for a solid camera experience, but what if you wanted to add some extra features for your users? Adding filters and effects are easy with CameraX’s Media3 effect integration, which is one of the new features introduced in CameraX 1.4.0. Here’s how simple it is to add a black and white filter from Media3’s built-in effects. val media3Effect = Media3Effect( application, PREVIEW or IMAGE_CAPTURE, ContextCompat.getMainExecutor(application), {}, ) media3Effect.setEffects(listOf(RgbFilter.createGrayscaleFilter())) useCaseGroupBuilder.addEffect(media3Effect) The Media3Effect object takes a Context, a bitwise representation of the use case constants for targeted UseCases, an Executor, and an error listener. Then you set the list of effects you want to apply. Finally, you add the effect to the useCaseGroupBuilder we defined earlier. (Left) Our camera app with no filter applied.  (Right) Our camera app after the createGrayscaleFilter was added. There are many other built-in effects you can add, too! See the Media3 Effect documentation for more options, like brightness, color lookup tables (LUTs), contrast, blur, and many other effects. To take your effects to yet another level, it’s also possible to define your own effects by implementing the GlEffect interface, which acts as a factory of GlShaderPrograms. You can implement a BaseGlShaderProgram’s drawFrame() method to implement a custom effect of your own. A minimal implementation should tell your graphics library to use its shader program, bind the shader program's vertex attributes and uniforms, and issue a drawing command. Jetpack libraries meet you where you are and your app’s needs. Whether that be a simple, fast-to-implement, and reliable implementation, or custom functionality that helps the critical user journeys in your app stand out from the rest, Jetpack has you covered! Jetpack offers a foundation for innovative AI Features Mayuri Khinvasara KhabyaJust as Donovan demonstrated with CameraX for capture, Jetpack Media3 provides a reliable, customizable, and feature-rich solution for playback with ExoPlayer. The AI Samples app builds on this foundation to delight users with helpful and enriching AI-driven additions. In today's rapidly evolving digital landscape, users expect more from their media applications. Simply playing videos is no longer enough. Developers are constantly seeking ways to enhance user experiences and provide deeper engagement. Leveraging the power of Artificial Intelligence (AI), particularly when built upon robust media frameworks like Media3, offers exciting opportunities. Let’s take a look at some of the ways we can transform the way users interact with video content: Empowering Video Understanding: The core idea is to use AI, specifically multimodal models like the Gemini Flash and Pro models, to analyze video content and extract meaningful information. This goes beyond simply playing a video; it's about understanding what's in the video and making that information readily accessible to the user. Actionable Insights: The goal is to transform raw video into summaries, insights, and interactive experiences. This allows users to quickly grasp the content of a video and find specific information they need or learn something new! Accessibility and Engagement: AI helps make videos more accessible by providing features like summaries, translations, and descriptions. It also aims to increase user engagement through interactive features. A Glimpse into AI-Powered Video Journeys The following example demonstrates potential video journies enhanced by artificial intelligence. This sample integrates several components, such as ExoPlayer and Transformer from Media3; the Firebase SDK (leveraging Vertex AI on Android); and Jetpack Compose, ViewModel, and StateFlow. The code will be available soon on Github. (Left) Video summarization   (Right) Thumbnails timestamps and HDR frame extraction There are two experiences in particular that I’d like to highlight: HDR Thumbnails: AI can help identify key moments in the video that could make for good thumbnails. With those timestamps, you can use the new ExperimentalFrameExtractor API from Media3 to extract HDR thumbnails from videos, providing richer visual previews. Text-to-Speech: AI can be used to convert textual information derived from the video into spoken audio, enhancing accessibility. On Android you can also choose to play audio in different languages and dialects thus enhancing personalization for a wider audience. Using the right AI solution Currently, only cloud models support video inputs, so we went ahead with a cloud-based solution.Iintegrating Firebase in our sample empowers the app to: Generate real-time, concise video summaries automatically. Produce comprehensive content metadata, including chapter markers and relevant hashtags. Facilitate seamless multilingual content translation. So how do you actually interact with a video and work with Gemini to process it? First, send your video as an input parameter to your prompt: val promptData = \"Summarize this video in the form of top 3-4 takeaways only. Write in the form of bullet points. Don't assume if you don't know\" val generativeModel = Firebase.vertexAI.generativeModel(\"gemini-2.0-flash\") _outputText.value = OutputTextState.Loading viewModelScope.launch(Dispatchers.IO) { try { val requestContent = content { fileData(videoSource.toString(), \"video/mp4\") text(prompt) } val outputStringBuilder = StringBuilder() generativeModel.generateContentStream(requestContent).collect { response -\u003e outputStringBuilder.append(response.text) _outputText.value = OutputTextState.Success(outputStringBuilder.toString()) } _outputText.value = OutputTextState.Success(outputStringBuilder.toString()) } catch (error: Exception) { _outputText.value = error.localizedMessage?.let { OutputTextState.Error(it) } } } Notice there are two key components here: FileData: This component integrates a video into the query. Prompt: This asks the user what specific assistance they need from AI in relation to the provided video. Of course, you can finetune your prompt as per your requirements and get the responses accordingly. In conclusion, by harnessing the capabilities of Jetpack Media3 and integrating AI solutions like Gemini through Firebase, you can significantly elevate video experiences on Android. This combination enables advanced features like video summaries, enriched metadata, and seamless multilingual translations, ultimately enhancing accessibility and engagement for users. As these technologies continue to evolve, the potential for creating even more dynamic and intelligent video applications is vast. Go above-and-beyond with specialized APIs Mozart LouisAndroid 16 introduces the new audio PCM Offload mode which can reduce the power consumption of audio playback in your app, leading to longer playback time and increased user engagement. Eliminating the power anxiety greatly enhances the user experience. Oboe is Android’s premiere audio api that developers are able to use to create high performance, low latency audio apps. A new feature is being added to the Android NDK and Android 16 called Native PCM Offload playback. Offload playback helps save battery life when playing audio. It works by sending a large chunk of audio to a special part of the device's hardware (a DSP). This allows the CPU of the device to go into a low-power state while the DSP handles playing the sound. This works with uncompressed audio (like PCM) and compressed audio (like MP3 or AAC), where the DSP also takes care of decoding. This can result in significant power saving while playing back audio and is perfect for applications that play audio in the background or while the screen is off (think audiobooks, podcasts, music etc). We created the sample app PowerPlay to demonstrate how to implement these features using the latest NDK version, C++ and Jetpack Compose. Here are the most important parts! First order of business is to assure the device supports audio offload of the file attributes you need. In the example below, we are checking if the device support audio offload of stereo, float PCM file with a sample rate of 48000Hz. val format = AudioFormat.Builder() .setEncoding(AudioFormat.ENCODING_PCM_FLOAT) .setSampleRate(48000) .setChannelMask(AudioFormat.CHANNEL_OUT_STEREO) .build() val attributes = AudioAttributes.Builder() .setContentType(AudioAttributes.CONTENT_TYPE_MUSIC) .setUsage(AudioAttributes.USAGE_MEDIA) .build() val isOffloadSupported = if (Build.VERSION.SDK_INT \u003e= Build.VERSION_CODES.Q) { AudioManager.isOffloadedPlaybackSupported(format, attributes) } else { false } if (isOffloadSupported) { player.initializeAudio(PerformanceMode::POWER_SAVING_OFFLOADED) } Once we know the device supports audio offload, we can confidently set the Oboe audio streams’ performance mode to the new performance mode option, PerformanceMode::POWER_SAVING_OFFLOADED. // Create an audio stream AudioStreamBuilder builder; builder.setChannelCount(mChannelCount); builder.setDataCallback(mDataCallback); builder.setFormat(AudioFormat::Float); builder.setSampleRate(48000); builder.setErrorCallback(mErrorCallback); builder.setPresentationCallback(mPresentationCallback); builder.setPerformanceMode(PerformanceMode::POWER_SAVING_OFFLOADED); builder.setFramesPerDataCallback(128); builder.setSharingMode(SharingMode::Exclusive); builder.setSampleRateConversionQuality(SampleRateConversionQuality::Medium); Result result = builder.openStream(mAudioStream); Now when audio is played back, it will be offloading audio to the DSP, helping save power when playing back audio. There is more to this feature that will be covered in a future blog post, fully detailing out all of the new available APIs that will help you optimize your audio playback experience! What’s next Of course, we were only able to share the tip of the iceberg with you here, so to dive deeper into the samples, check out the following links: Jetpack Media3 Composition Demo app SociaLite AI Samples PowerPlay Hopefully these examples have inspired you to explore what new and fascinating experiences you can build on Android. Tune in to our session at Google I/O in a couple weeks to learn even more about use-cases supported by solutions like Jetpack CameraX and Jetpack Media3!",
  "image": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2nT4w2e_EWOqk6E4zMVVYhUuQYgsukdfAKL_cEixIoRr7aKCFBjRB-et5zPB_lAASNgixHGvZLmevsjbQ8y75oN6bTPp1-ZQabGrF8umyNnlK-SGgzFXI9kHoqH4aUcDqsiXlEam9VkGf1y0Z4xWy0wlwHmU0c54toszEmZEj95E8-Nrh1zrBxO6fsp0/w1200-h630-p-k-no-nu/android-media-evergreen-option-2.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cmeta content=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2nT4w2e_EWOqk6E4zMVVYhUuQYgsukdfAKL_cEixIoRr7aKCFBjRB-et5zPB_lAASNgixHGvZLmevsjbQ8y75oN6bTPp1-ZQabGrF8umyNnlK-SGgzFXI9kHoqH4aUcDqsiXlEam9VkGf1y0Z4xWy0wlwHmU0c54toszEmZEj95E8-Nrh1zrBxO6fsp0/s1600/android-media-evergreen-option-2.png\" name=\"twitter:image\"/\u003e\n\u003cp\u003e\n\n\u003cem\u003ePosted by Donovan McMurray, Mayuri Khinvasara Khabya, Mozart Louis, and Nevin Mital – Developer Relations Engineers\u003c/em\u003e\n\n\u003ca href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhP3GdxA9YnWdmWaV-WBmA1Iwmrkg0IiuG6HeLa7AQdMcJKnYSjAHlilcdXI0FKvsPU_9JPai55RXpC1P1MoqUow9hplafIccCV_AVAxASuvdxSAlaVICsK_PG73CFWx_6HCrACTZGmxDyQtlvN-ncB7z2JInOSRhQC-NqrCfCtfqeUdhHZhj7HaEsCKbU/s1600/0025-AfD-Android-New-Blog-Header-4209x1253%20%281%29.png\"\u003e\u003cimg data-original-height=\"800\" data-original-width=\"100%\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhP3GdxA9YnWdmWaV-WBmA1Iwmrkg0IiuG6HeLa7AQdMcJKnYSjAHlilcdXI0FKvsPU_9JPai55RXpC1P1MoqUow9hplafIccCV_AVAxASuvdxSAlaVICsK_PG73CFWx_6HCrACTZGmxDyQtlvN-ncB7z2JInOSRhQC-NqrCfCtfqeUdhHZhj7HaEsCKbU/s1600/0025-AfD-Android-New-Blog-Header-4209x1253%20%281%29.png\"/\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003eHello Android Developers!\u003c/p\u003e\n\n\u003cp\u003eWe are the Android Developer Relations Camera \u0026amp; Media team, and we’re excited to bring you something a little different today. Over the past several months, we’ve been hard at work writing sample code and building demos that showcase how to take advantage of all the great potential Android offers for building delightful user experiences.\u003c/p\u003e\n\n\u003cp\u003eSome of these efforts are available for you to explore now, and some you’ll see later throughout the year, but for this blog post we thought we’d share some of the learnings we gathered while going through this exercise.\u003c/p\u003e \n  \n\u003cp\u003eGrab your favorite Android plush or rubber duck, and read on to see what we’ve been up to!\u003c/p\u003e\n\n\u003ch2\u003e\u003cspan\u003eFuture-proof your app with Jetpack\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eNevin Mital\u003c/em\u003e\u003c/p\u003e\u003cp\u003eOne of our focuses for the past several years has been improving the developer tools available for video editing on Android. This led to the creation of the \u003ca href=\"https://developer.android.com/media/media3/transformer\" target=\"_blank\"\u003eJetpack Media3 Transformer\u003c/a\u003e APIs, which offer solutions for both single-asset and multi-asset video editing preview and export. Today, I’d like to focus on the \u003ca href=\"https://github.com/androidx/media/tree/main/demos/composition\" target=\"_blank\"\u003eComposition demo app\u003c/a\u003e, a sample app that showcases some of the multi-asset editing experiences that Transformer enables.\u003c/p\u003e\n\n\u003cp\u003eI started by adding a custom video compositor to demonstrate how you can arrange input video sequences into different layouts for your final composition, such as a 2x2 grid or a picture-in-picture overlay. You can customize this by implementing a \u003cspan\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/media3/effect/VideoCompositorSettings\" target=\"_blank\"\u003eVideoCompositorSettings\u003c/a\u003e\u003c/span\u003e and overriding the \u003cspan\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/media3/effect/VideoCompositorSettings#getOverlaySettings%28int,long%29\" target=\"_blank\"\u003egetOverlaySettings\u003c/a\u003e\u003c/span\u003e method. This object can then be set when building your Composition with \u003cspan\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/media3/transformer/Composition.Builder#setVideoCompositorSettings%28androidx.media3.common.VideoCompositorSettings%29\" target=\"_blank\"\u003esetVideoCompositorSettings\u003c/a\u003e\u003c/span\u003e.\u003c/p\u003e\n\n\u003cp\u003eHere is an example for the 2x2 grid layout:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003eobject : VideoCompositorSettings {\n  ...\n\n  \u003cspan\u003eoverride\u003c/span\u003e \u003cspan\u003efun\u003c/span\u003e \u003cspan\u003egetOverlaySettings\u003c/span\u003e(inputId: Int, presentationTimeUs: Long): OverlaySettings {\n    \u003cspan\u003ereturn\u003c/span\u003e \u003cspan\u003ewhen\u003c/span\u003e (inputId) {\n      \u003cspan\u003e0\u003c/span\u003e -\u0026gt; { \u003cspan\u003e// First sequence is placed in the top left\u003c/span\u003e\n        StaticOverlaySettings.Builder()\n          .setScale(\u003cspan\u003e0.5f\u003c/span\u003e, \u003cspan\u003e0.5f\u003c/span\u003e)\n          .setOverlayFrameAnchor(\u003cspan\u003e0f\u003c/span\u003e, \u003cspan\u003e0f\u003c/span\u003e) \u003cspan\u003e// Middle of overlay\u003c/span\u003e\n          .setBackgroundFrameAnchor(-\u003cspan\u003e0.5f\u003c/span\u003e, \u003cspan\u003e0.5f\u003c/span\u003e) \u003cspan\u003e// Top-left section of background\u003c/span\u003e\n          .build()\n      }\n\n      \u003cspan\u003e1\u003c/span\u003e -\u0026gt; { \u003cspan\u003e// Second sequence is placed in the top right\u003c/span\u003e\n        StaticOverlaySettings.Builder()\n          .setScale(\u003cspan\u003e0.5f\u003c/span\u003e, \u003cspan\u003e0.5f\u003c/span\u003e)\n          .setOverlayFrameAnchor(\u003cspan\u003e0f\u003c/span\u003e, \u003cspan\u003e0f\u003c/span\u003e) \u003cspan\u003e// Middle of overlay\u003c/span\u003e\n          .setBackgroundFrameAnchor(\u003cspan\u003e0.5f\u003c/span\u003e, \u003cspan\u003e0.5f\u003c/span\u003e) \u003cspan\u003e// Top-right section of background\u003c/span\u003e\n          .build()\n      }\n\n      \u003cspan\u003e2\u003c/span\u003e -\u0026gt; { \u003cspan\u003e// Third sequence is placed in the bottom left\u003c/span\u003e\n        StaticOverlaySettings.Builder()\n          .setScale(\u003cspan\u003e0.5f\u003c/span\u003e, \u003cspan\u003e0.5f\u003c/span\u003e)\n          .setOverlayFrameAnchor(\u003cspan\u003e0f\u003c/span\u003e, \u003cspan\u003e0f\u003c/span\u003e) \u003cspan\u003e// Middle of overlay\u003c/span\u003e\n          .setBackgroundFrameAnchor(-\u003cspan\u003e0.5f\u003c/span\u003e, -\u003cspan\u003e0.5f\u003c/span\u003e) \u003cspan\u003e// Bottom-left section of background\u003c/span\u003e\n          .build()\n      }\n\n      \u003cspan\u003e3\u003c/span\u003e -\u0026gt; { \u003cspan\u003e// Fourth sequence is placed in the bottom right\u003c/span\u003e\n        StaticOverlaySettings.Builder()\n          .setScale(\u003cspan\u003e0.5f\u003c/span\u003e, \u003cspan\u003e0.5f\u003c/span\u003e)\n          .setOverlayFrameAnchor(\u003cspan\u003e0f\u003c/span\u003e, \u003cspan\u003e0f\u003c/span\u003e) \u003cspan\u003e// Middle of overlay\u003c/span\u003e\n          .setBackgroundFrameAnchor(\u003cspan\u003e0.5f\u003c/span\u003e, -\u003cspan\u003e0.5f\u003c/span\u003e) \u003cspan\u003e// Bottom-right section of background\u003c/span\u003e\n          .build()\n      }\n\n      \u003cspan\u003eelse\u003c/span\u003e -\u0026gt; {\n        StaticOverlaySettings.Builder().build()\n      }\n    }\n  }\n}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eSince \u003cspan\u003egetOverlaySettings\u003c/span\u003e also provides a presentation time, we can even animate the layout, such as in this picture-in-picture example:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"moving image of picture in picture on a mobile device\" height=\"640\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1gvIAiDyYJgdqCELpwbfE7HgTBldURiqYbYM6_5Yfjxmqjw5tHEbhgRSnMnF5ZqMRef40zRpT-s2AOiwT7y9Qar8YjAMVzSKnu0av8C77GoV41C3N3DG1NzphXK-n8mYywc6tvoaqx1W0oODb5hFZWJ0-HOZIuh4eUwA53Uwa_wZTqGeAnwbXDEYojuk/w288-h640/camera-media-picture-in-picture.gif\" width=\"288\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eNext, I spent some time migrating the Composition demo app to use Jetpack Compose. With complicated editing flows, it can help to take advantage of as much screen space as is available, so I decided to use the \u003ca href=\"https://developer.android.com/develop/ui/compose/layouts/adaptive/build-a-supporting-pane-layout\" target=\"_blank\"\u003esupporting pane adaptive layout\u003c/a\u003e. This way, the user can fine-tune their video creation on the preview screen, and export options are only shown at the same time on a larger display. Below, you can see how the UI dynamically adapts to the screen size on a foldable device, when switching from the outer screen to the inner screen and vice versa.\u003c/p\u003e\n\n\u003cp\u003eWhat’s great is that by using Jetpack Media3 and Jetpack Compose, these features also carry over seamlessly to other devices and form factors, such as the new Android XR platform. Right out-of-the-box, I was able to run the demo app in \u003ca href=\"https://developer.android.com/develop/xr/jetpack-xr-sdk/transition-home-space-to-full-space\" target=\"_blank\"\u003eHome Space\u003c/a\u003e with the 2D UI I already had. And with some small updates, I was even able to adapt the UI specifically for XR with features such as multiple panels, and to take further advantage of the extra space, an Orbiter with playback controls for the editing preview.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"moving image of suportive pane adaptive layout\" height=\"640\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRA-GywGhx63aaahWp9Wx6bE_bN_4qk28omhH6z7sINSMHCwtzjgeBBg6WB-xIlyeCU__9qjEU-D_0I8TkLewth438hvHOlhKawfjhaGVpqN-CPzlRKjZJ1YUdKrdigwZJw0HfDP3ogaMRxNWCOEP4LGRsf5IxjYSQ4lBJLSNSI__8_aa88o6usLAbiCg/w331-h640/camera-media-supporting-pane-adaptive-layout.gif\" width=\"331\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eWhat’s great is that by using Jetpack Media3 and Jetpack Compose, these features also carry over seamlessly to other devices and form factors, such as the new Android XR platform. Right out-of-the-box, I was able to run the demo app in \u003ca href=\"https://developer.android.com/develop/xr/jetpack-xr-sdk/transition-home-space-to-full-space\" target=\"_blank\"\u003eHome Space\u003c/a\u003e with the 2D UI I already had. And with some small updates, I was even able to adapt the UI specifically for XR with features such as multiple panels, and to take further advantage of the extra space, an Orbiter with playback controls for the editing preview.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"moving image of sequential composition preview in Android XR\" height=\"640\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWt5myPF6tGegayCwASmxtZisL0FIV3EHrIt6Ju50gjnil13EhWs2ijA73IPdQWQQe4jSikN7vhvlsrCLA863WiIngRbIrdZwxmKo2RKbGwgQBavqaadktj1gbdXTnkhfyzbrsms5ln7PuPbkr5_vABKs_JFUGOTT-q68U_XLABNvy8phTJ413zAd8vAs/w640-h640/camera-media-android-xr.gif\" width=\"640\"/\u003e\u003c/p\u003e\u003cbr/\u003e\n\n\u003cdiv\u003e\u003cpre\u003eOrbiter(\n  position = OrbiterEdge.Bottom,\n  offset = EdgeOffset.inner(offset = MaterialTheme.spacing.standard),\n  alignment = Alignment.CenterHorizontally,\n  shape = SpatialRoundedCornerShape(CornerSize(\u003cspan\u003e28.d\u003c/span\u003ep))\n) {\n  Row (horizontalArrangement = Arrangement.spacedBy(MaterialTheme.spacing.mini)) {\n    \u003cspan\u003e// Playback control for rewinding by 10 seconds\u003c/span\u003e\n    FilledTonalIconButton({ viewModel.seekBack(\u003cspan\u003e10\u003c/span\u003e_000L) }) {\n      Icon(\n        painter = painterResource(id = R.drawable.rewind_10),\n        contentDescription = \u003cspan\u003e\u0026#34;Rewind by 10 seconds\u0026#34;\u003c/span\u003e\n      )\n    }\n    \u003cspan\u003e// Playback control for play/pause\u003c/span\u003e\n    FilledTonalIconButton({ viewModel.togglePlay() }) {\n      Icon(\n        painter = painterResource(id = R.drawable.rounded_play_pause_24),\n        contentDescription = \n            \u003cspan\u003eif\u003c/span\u003e(viewModel.compositionPlayer.isPlaying) {\n                \u003cspan\u003e\u0026#34;Pause preview playback\u0026#34;\u003c/span\u003e\n            } \u003cspan\u003eelse\u003c/span\u003e {\n                \u003cspan\u003e\u0026#34;Resume preview playback\u0026#34;\u003c/span\u003e\n            }\n      )\n    }\n    \u003cspan\u003e// Playback control for forwarding by 10 seconds\u003c/span\u003e\n    FilledTonalIconButton({ viewModel.seekForward(\u003cspan\u003e10\u003c/span\u003e_000L) }) {\n      Icon(\n        painter = painterResource(id = R.drawable.forward_10),\n        contentDescription = \u003cspan\u003e\u0026#34;Forward by 10 seconds\u0026#34;\u003c/span\u003e\n      )\n    }\n  }\n}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003ch2\u003e\u003cspan\u003eJetpack libraries unlock premium functionality incrementally\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eDonovan McMurray\u003c/em\u003e\u003c/p\u003e\u003cp\u003eNot only do our Jetpack libraries have you covered by working consistently across existing and future devices, but they also open the doors to advanced functionality and custom behaviors to support all types of app experiences. In a nutshell, our Jetpack libraries aim to make the common case very accessible and easy, and it has hooks for adding more custom features later.\u003c/p\u003e \n\n\u003cp\u003eWe’ve worked with many apps who have switched to a Jetpack library, built the basics, added their critical custom features, and actually saved developer time over their estimates. Let’s take a look at CameraX and how this incremental development can supercharge your process.\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e// Set up CameraX app with preview and image capture.\u003c/span\u003e\n\u003cspan\u003e// Note: setting the resolution selector is optional, and if not set,\u003c/span\u003e\n\u003cspan\u003e// then a default 4:3 ratio will be used.\u003c/span\u003e\n\u003cspan\u003eval\u003c/span\u003e aspectRatioStrategy = AspectRatioStrategy(\n  AspectRatio.RATIO_16_9, AspectRatioStrategy.FALLBACK_RULE_NONE)\n\u003cspan\u003evar\u003c/span\u003e resolutionSelector = ResolutionSelector.Builder()\n  .setAspectRatioStrategy(aspectRatioStrategy)\n  .build()\n\n\u003cspan\u003eprivate\u003c/span\u003e \u003cspan\u003eval\u003c/span\u003e previewUseCase = Preview.Builder()\n  .setResolutionSelector(resolutionSelector)\n  .build()\n\u003cspan\u003eprivate\u003c/span\u003e \u003cspan\u003eval\u003c/span\u003e imageCaptureUseCase = ImageCapture.Builder()\n  .setResolutionSelector(resolutionSelector)\n  .setCaptureMode(ImageCapture.CAPTURE_MODE_MINIMIZE_LATENCY)\n  .build()\n\n\u003cspan\u003eval\u003c/span\u003e useCaseGroupBuilder = UseCaseGroup.Builder()\n  .addUseCase(previewUseCase)\n  .addUseCase(imageCaptureUseCase)\n\ncameraProvider.unbindAll()\n\ncamera = cameraProvider.bindToLifecycle(\n  \u003cspan\u003ethis\u003c/span\u003e,  \u003cspan\u003e// lifecycleOwner\u003c/span\u003e\n  CameraSelector.DEFAULT_BACK_CAMERA,\n  useCaseGroupBuilder.build(),\n)\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eAfter setting up the basic structure for CameraX, you can set up a simple UI with a camera preview and a shutter button. You can use the CameraX Viewfinder composable which displays a Preview stream from a CameraX SurfaceRequest.\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e// Create preview\u003c/span\u003e\nBox(\n  Modifier\n    .background(Color.Black)\n    .fillMaxSize(),\n  contentAlignment = Alignment.Center,\n) {\n  surfaceRequest?.let {\n    CameraXViewfinder(\n      modifier = Modifier.fillMaxSize(),\n      implementationMode = ImplementationMode.EXTERNAL,\n      surfaceRequest = surfaceRequest,\n     )\n  }\n  Button(\n    onClick = onPhotoCapture,\n    shape = CircleShape,\n    colors = ButtonDefaults.buttonColors(containerColor = Color.White),\n    modifier = Modifier\n      .height(\u003cspan\u003e75.d\u003c/span\u003ep)\n      .width(\u003cspan\u003e75.d\u003c/span\u003ep),\n  )\n}\n\n\u003cspan\u003efun\u003c/span\u003e \u003cspan\u003eonPhotoCapture\u003c/span\u003e() {\n  \u003cspan\u003e// Not shown: defining the ImageCapture.OutputFileOptions for\u003c/span\u003e\n  \u003cspan\u003e// your saved images\u003c/span\u003e\n  imageCaptureUseCase.takePicture(\n    outputOptions,\n    ContextCompat.getMainExecutor(context),\n    object : ImageCapture.OnImageSavedCallback {\n      \u003cspan\u003eoverride\u003c/span\u003e \u003cspan\u003efun\u003c/span\u003e \u003cspan\u003eonError\u003c/span\u003e(exc: ImageCaptureException) {\n        \u003cspan\u003eval\u003c/span\u003e msg = \u003cspan\u003e\u0026#34;Photo capture failed.\u0026#34;\u003c/span\u003e\n        Toast.makeText(context, msg, Toast.LENGTH_SHORT).show()\n      }\n\n      \u003cspan\u003eoverride\u003c/span\u003e \u003cspan\u003efun\u003c/span\u003e \u003cspan\u003eonImageSaved\u003c/span\u003e(output: ImageCapture.OutputFileResults) {\n        \u003cspan\u003eval\u003c/span\u003e savedUri = output.savedUri\n        \u003cspan\u003eif\u003c/span\u003e (savedUri != \u003cspan\u003enull\u003c/span\u003e) {\n          \u003cspan\u003e// Do something with the savedUri if needed\u003c/span\u003e\n        } \u003cspan\u003eelse\u003c/span\u003e {\n          \u003cspan\u003eval\u003c/span\u003e msg = \u003cspan\u003e\u0026#34;Photo capture failed.\u0026#34;\u003c/span\u003e\n          Toast.makeText(context, msg, Toast.LENGTH_SHORT).show()\n        }\n      }\n    },\n  )\n}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eYou’re already on track for a solid camera experience, but what if you wanted to add some extra features for your users? Adding filters and effects are easy with CameraX’s Media3 effect integration, which is \u003ca href=\"https://android-developers.googleblog.com/2024/12/whats-new-in-camerax-140-and-jetpack-compose-support.html\" target=\"_blank\"\u003eone of the new features introduced in CameraX 1.4.0\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eHere’s how simple it is to add a black and white filter from Media3’s built-in effects.\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003eval\u003c/span\u003e media3Effect = Media3Effect(\n  application,\n  PREVIEW or IMAGE_CAPTURE,\n  ContextCompat.getMainExecutor(application),\n  {},\n)\nmedia3Effect.setEffects(listOf(RgbFilter.createGrayscaleFilter()))\nuseCaseGroupBuilder.addEffect(media3Effect)\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://developer.android.com/reference/androidx/camera/media3/effect/Media3Effect\" target=\"_blank\"\u003eMedia3Effect\u003c/a\u003e object takes a \u003ca href=\"https://developer.android.com/reference/android/content/Context\" target=\"_blank\"\u003eContext\u003c/a\u003e, a bitwise representation of the \u003ca href=\"https://developer.android.com/reference/androidx/camera/core/CameraEffect#constants_1\" target=\"_blank\"\u003euse case constants\u003c/a\u003e for targeted \u003ca href=\"https://developer.android.com/reference/androidx/camera/core/UseCase\" target=\"_blank\"\u003eUseCases\u003c/a\u003e, an \u003ca href=\"https://developer.android.com/reference/java/util/concurrent/Executor.html\" target=\"_blank\"\u003eExecutor\u003c/a\u003e, and an error listener. Then you set the list of effects you want to apply. Finally, you add the effect to the \u003cspan\u003euseCaseGroupBuilder\u003c/span\u003e we defined earlier.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"moving image of sequential composition preview in Android XR\" height=\"640\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicrINuUuJVTYNdcqv2L433Iwt689pZhDgwqBB-s0vIqsMQpPOpfeKjnUjF7uo-D6Re98OOY6IRTqLRUCKoNKj1U7MqIMgE2GijeAUDa_nQIE0Vv9eM6IXZ_2L0Rp4qX9DfOldEe3KaOdWRaKbQ2qHbxQc9sOARlrTRwHugkRoFudYDjwDZPvhVzv8LcUk/s1600/black-and-white-filter-media3-built-in-effects.png\" width=\"640\"/\u003e\u003c/p\u003e\u003cimgcaption\u003e\u003ccenter\u003e\u003cem\u003e(Left) Our camera app with no filter applied. \u003c/em\u003e\u003c/center\u003e\u003ccenter\u003e\u003cem\u003e (Right) Our camera app after the createGrayscaleFilter was added.\n\u003c/em\u003e\u003c/center\u003e\u003c/imgcaption\u003e\n\n\u003cp\u003eThere are many other built-in effects you can add, too! See the Media3 \u003ca href=\"https://developer.android.com/reference/androidx/media3/common/Effect\" target=\"_blank\"\u003eEffect\u003c/a\u003e documentation for more options, like brightness, color lookup tables (LUTs), contrast, blur, and many other effects.\u003c/p\u003e\n\n\u003cp\u003eTo take your effects to yet another level, it’s also possible to define your own effects by implementing the \u003cspan\u003e\u003ca href=\"https://developer.android.com/reference/androidx/media3/effect/GlEffect\" target=\"_blank\"\u003eGlEffect\u003c/a\u003e\u003c/span\u003e interface, which acts as a factory of \u003cspan\u003e\u003ca href=\"https://developer.android.com/reference/androidx/media3/effect/GlShaderProgram\" target=\"_blank\"\u003eGlShaderPrograms\u003c/a\u003e\u003c/span\u003e. You can implement a \u003cspan\u003e\u003ca href=\"https://developer.android.com/reference/androidx/media3/effect/BaseGlShaderProgram\" target=\"_blank\"\u003eBaseGlShaderProgram\u003c/a\u003e\u003c/span\u003e’s \u003cspan\u003e\u003ca href=\"https://developer.android.com/reference/androidx/media3/effect/BaseGlShaderProgram\" target=\"_blank\"\u003edrawFrame()\u003c/a\u003e\u003c/span\u003e method to implement a custom effect of your own. A minimal implementation should tell your graphics library to use its shader program, bind the shader program\u0026#39;s vertex attributes and uniforms, and issue a drawing command.\u003c/p\u003e\n\n\u003cp\u003eJetpack libraries meet you where you are and your app’s needs. Whether that be a simple, fast-to-implement, and reliable implementation, or custom functionality that helps the critical user journeys in your app stand out from the rest, Jetpack has you covered!\u003c/p\u003e\n\n\u003ch2\u003e\u003cspan\u003eJetpack offers a foundation for innovative AI Features\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ci\u003eMayuri Khinvasara Khabya\u003c/i\u003e\u003c/p\u003e\u003cp\u003eJust as Donovan demonstrated with CameraX for capture, Jetpack \u003ca href=\"https://developer.android.com/media\" target=\"_blank\"\u003eMedia3\u003c/a\u003e provides a reliable, customizable, and feature-rich solution for playback with ExoPlayer. The AI Samples app builds on this foundation to delight users with helpful and enriching AI-driven additions.\u003c/p\u003e\n\n\u003cp\u003eIn today\u0026#39;s rapidly evolving digital landscape, users expect more from their media applications. Simply playing videos is no longer enough. Developers are constantly seeking ways to enhance user experiences and provide deeper engagement. Leveraging the power of Artificial Intelligence (AI), particularly when built upon robust media frameworks like Media3, offers exciting opportunities. Let’s take a look at some of the ways we can transform the way users interact with video content:\u003c/p\u003e\n\u003cul\u003e\u003cul\u003e\n\u003cli\u003e\u003cb\u003eEmpowering Video Understanding:\u003c/b\u003e The core idea is to use AI, specifically multimodal models like the Gemini Flash and Pro models, to analyze video content and extract meaningful information. This goes beyond simply playing a video; it\u0026#39;s about understanding what\u0026#39;s in the video and making that information readily accessible to the user.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\n\u003cli\u003e\u003cb\u003eActionable Insights:\u003c/b\u003e The goal is to transform raw video into summaries, insights, and interactive experiences. This allows users to quickly grasp the content of a video and find specific information they need or learn something new!\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\n\u003cli\u003e\u003cb\u003eAccessibility and Engagement:\u003c/b\u003e AI helps make videos more accessible by providing features like summaries, translations, and descriptions. It also aims to increase user engagement through interactive features.\u003c/li\u003e\n\u003c/ul\u003e\u003c/ul\u003e\n\n\u003ch3\u003e\u003cspan\u003eA Glimpse into AI-Powered Video Journeys\u003c/span\u003e\u003c/h3\u003e\n\n\u003cp\u003eThe following example demonstrates potential video journies enhanced by artificial intelligence. This sample integrates several components, such as ExoPlayer and Transformer from Media3; the Firebase SDK (leveraging Vertex AI on Android); and Jetpack Compose, ViewModel, and StateFlow. The code will be available soon on \u003ca href=\"https://github.com/android/ai-samples/tree/main/ai-catalog\" target=\"_blank\"\u003eGithub\u003c/a\u003e.\u003c/p\u003e\n\n\n\u003cp\u003e\u003cimg alt=\"moving images of examples of AI-powered video journeys\" height=\"624\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjej2Wp0E2-hX0G8cwNcEtEDuhRbcMuAMLk3_u1Og7XNr-jIgAbidX3JMMlEiimyKDlG1lzbNoq9eMM_czZv5ug5rvhwzSw8JC5M0wokDNwUHINEpqX9ZqDaR0EzRnu8bb58qzSXBG2xd0XGNFk4d_lIkC4Sf8fGgQEFdF31oDws1uE8y5m5J4Wxl54q8U/w640-h624/camera-media-video-assist-gemini.gif\" width=\"640\"/\u003e\u003c/p\u003e\u003cimgcaption\u003e\u003ccenter\u003e\u003cem\u003e(Left) Video summarization  \u003c/em\u003e\u003c/center\u003e\u003ccenter\u003e\u003cem\u003e (Right) Thumbnails  timestamps and HDR frame extraction\u003c/em\u003e\u003c/center\u003e\u003c/imgcaption\u003e\n\n\u003cp\u003eThere are two experiences in particular that I’d like to highlight:\u003c/p\u003e\n\u003cul\u003e\u003cul\u003e\n\u003cli\u003e\u003cb\u003eHDR Thumbnails:\u003c/b\u003e AI can help identify key moments in the video that could make for good thumbnails. With those timestamps, you can use the new \u003cspan\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/media3/transformer/ExperimentalFrameExtractor\" target=\"_blank\"\u003eExperimentalFrameExtractor\u003c/a\u003e\u003c/span\u003e API from Media3 to extract HDR thumbnails from videos, providing richer visual previews.\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eText-to-Speech:\u003c/b\u003e AI can be used to convert textual information derived from the video into spoken audio, enhancing accessibility. On Android you can also choose to play audio in different languages and dialects thus enhancing personalization for a wider audience.\u003c/li\u003e\n\u003c/ul\u003e\u003c/ul\u003e\n\n\u003ch3\u003e\u003cspan\u003eUsing the right AI solution\u003c/span\u003e\u003c/h3\u003e\n\n\u003cp\u003eCurrently, only cloud models support video inputs, so we went ahead with  a cloud-based solution.Iintegrating Firebase in our sample empowers the app to:\u003c/p\u003e\n\u003cul\u003e\u003cul\u003e\n\u003cli\u003eGenerate real-time, concise video summaries automatically.\u003c/li\u003e\n\u003cli\u003eProduce comprehensive content metadata, including chapter markers and relevant hashtags.\u003c/li\u003e\n\u003cli\u003eFacilitate seamless multilingual content translation.\u003c/li\u003e\n\u003c/ul\u003e\u003c/ul\u003e\n\n\u003cp\u003eSo how do you actually interact with a video and work with Gemini to process it? First, send your video as an input parameter to your prompt:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003eval\u003c/span\u003e promptData =\n\u003cspan\u003e\u0026#34;Summarize this video in the form of top 3-4 takeaways only. Write in the form of bullet points. Don\u0026#39;t assume if you don\u0026#39;t know\u0026#34;\u003c/span\u003e\n\n\u003cspan\u003eval\u003c/span\u003e generativeModel = Firebase.vertexAI.generativeModel(\u003cspan\u003e\u0026#34;gemini-2.0-flash\u0026#34;\u003c/span\u003e)\n_outputText.value = OutputTextState.Loading\n\nviewModelScope.launch(Dispatchers.IO) {\n    \u003cspan\u003etry\u003c/span\u003e {\n        \u003cspan\u003eval\u003c/span\u003e requestContent = content {\n            fileData(videoSource.toString(), \u003cspan\u003e\u0026#34;video/mp4\u0026#34;\u003c/span\u003e)\n            text(prompt)\n        }\n        \u003cspan\u003eval\u003c/span\u003e outputStringBuilder = StringBuilder()\n\n        generativeModel.generateContentStream(requestContent).collect { response -\u0026gt;\n            outputStringBuilder.append(response.text)\n            _outputText.value = OutputTextState.Success(outputStringBuilder.toString())\n        }\n\n        _outputText.value = OutputTextState.Success(outputStringBuilder.toString())\n\n    } \u003cspan\u003ecatch\u003c/span\u003e (error: Exception) {\n        _outputText.value = error.localizedMessage?.let { OutputTextState.Error(it) }\n    }\n}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eNotice there are two key components here:\u003c/p\u003e\n\u003cul\u003e\u003cul\u003e\n\u003cli\u003e\u003cb\u003eFileData:\u003c/b\u003e This component integrates a video into the query.\n\u003c/li\u003e\u003cli\u003e\u003cb\u003ePrompt:\u003c/b\u003e This asks the user what specific assistance they need from AI in relation to the provided video.\n\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e \n\n\u003cp\u003eOf course, you can finetune your prompt as per your requirements and get the responses accordingly.\u003c/p\u003e\n\n\u003cp\u003eIn conclusion, by harnessing the capabilities of Jetpack Media3 and integrating AI solutions like Gemini through Firebase, you can significantly elevate video experiences on Android. This combination enables advanced features like video summaries, enriched metadata, and seamless multilingual translations, ultimately enhancing accessibility and engagement for users. As these technologies continue to evolve, the potential for creating even more dynamic and intelligent video applications is vast.\u003c/p\u003e\n\n\u003ch2\u003e\u003cspan\u003eGo above-and-beyond with specialized APIs\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ci\u003eMozart Louis\u003c/i\u003e\u003c/p\u003e\u003cp\u003eAndroid 16 introduces the new audio PCM Offload mode which can reduce the power consumption of audio playback in your app, leading to longer playback time and increased user engagement. Eliminating the power anxiety greatly enhances the user experience.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://github.com/google/oboe\" target=\"_blank\"\u003eOboe\u003c/a\u003e is Android’s premiere audio api that developers are able to use to create high performance, low latency audio apps. A new feature is being added to the Android NDK and Android 16 called Native PCM Offload playback.\u003c/p\u003e\n\n\u003cp\u003eOffload playback helps save battery life when playing audio. It works by sending a large chunk of audio to a special part of the device\u0026#39;s hardware (a DSP). This allows the CPU of the device to go into a low-power state while the DSP handles playing the sound. This works with uncompressed audio (like PCM) and compressed audio (like MP3 or AAC), where the DSP also takes care of decoding.\u003c/p\u003e\n\n\u003cp\u003eThis can result in significant power saving while playing back audio and is perfect for applications that play audio in the background or while the screen is off (think audiobooks, podcasts, music etc).\u003c/p\u003e\n\n\u003cp\u003eWe created the \u003ca href=\"https://github.com/google/oboe/tree/powerplay-sample\" target=\"_blank\"\u003esample app PowerPlay\u003c/a\u003e to demonstrate how to implement these features using the latest NDK version, C++ and Jetpack Compose.\u003c/p\u003e\n\n\u003cp\u003eHere are the most important parts!\u003c/p\u003e\n\n\u003cp\u003eFirst order of business is to assure the device supports audio offload of the file attributes you need. In the example below, we are checking if the device support audio offload of stereo, float PCM file with a sample rate of 48000Hz.\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e       \u003cspan\u003eval\u003c/span\u003e format = AudioFormat.Builder()\n            .setEncoding(AudioFormat.ENCODING_PCM_FLOAT)\n            .setSampleRate(\u003cspan\u003e48000\u003c/span\u003e)\n            .setChannelMask(AudioFormat.CHANNEL_OUT_STEREO)\n            .build()\n\n        \u003cspan\u003eval\u003c/span\u003e attributes =\n            AudioAttributes.Builder()\n                .setContentType(AudioAttributes.CONTENT_TYPE_MUSIC)\n                .setUsage(AudioAttributes.USAGE_MEDIA)\n                .build()\n       \n        \u003cspan\u003eval\u003c/span\u003e isOffloadSupported = \n            \u003cspan\u003eif\u003c/span\u003e (Build.VERSION.SDK_INT \u0026gt;= Build.VERSION_CODES.Q) {\n                AudioManager.isOffloadedPlaybackSupported(format, attributes)\n            } \u003cspan\u003eelse\u003c/span\u003e {\n                \u003cspan\u003efalse\u003c/span\u003e\n            }\n\n        \u003cspan\u003eif\u003c/span\u003e (isOffloadSupported) {\n            player.initializeAudio(PerformanceMode::POWER_SAVING_OFFLOADED)\n        }\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eOnce we know the device supports audio offload, we can confidently set the Oboe audio streams’ performance mode to the new performance mode option, \u003cspan\u003e\u003ca href=\"https://github.com/google/oboe/blob/powerplay-sample/include/oboe/Definitions.h#L293\" target=\"_blank\"\u003ePerformanceMode::POWER_SAVING_OFFLOADED\u003c/a\u003e\u003c/span\u003e.\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e// Create an audio stream\u003c/span\u003e\n        AudioStreamBuilder builder;\n        builder.setChannelCount(mChannelCount);\n        builder.setDataCallback(mDataCallback);\n        builder.setFormat(AudioFormat::Float);\n        builder.setSampleRate(\u003cspan\u003e48000\u003c/span\u003e);\n\n        builder.setErrorCallback(mErrorCallback);\n        builder.setPresentationCallback(mPresentationCallback);\n        builder.setPerformanceMode(PerformanceMode::POWER_SAVING_OFFLOADED);\n        builder.setFramesPerDataCallback(\u003cspan\u003e128\u003c/span\u003e);\n        builder.setSharingMode(SharingMode::Exclusive);\n           builder.setSampleRateConversionQuality(SampleRateConversionQuality::Medium);\n        Result result = builder.openStream(mAudioStream);\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eNow when audio is played back, it will be offloading audio to the DSP, helping save power when playing back audio.\u003c/p\u003e\n\n\u003cp\u003eThere is more to this feature that will be covered in a future blog post, fully detailing out all of the new available APIs that will help you optimize your audio playback experience!\u003c/p\u003e\n\n\u003ch2\u003e\u003cspan\u003eWhat’s next\u003c/span\u003e\u003c/h2\u003e\n\n\u003cp\u003eOf course, we were only able to share the tip of the iceberg with you here, so to dive deeper into the samples, check out the following links:\u003c/p\u003e\n\u003cul\u003e\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/androidx/media/tree/main/demos/composition\" target=\"_blank\"\u003eJetpack Media3 Composition Demo app\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/android/socialite\" target=\"_blank\"\u003eSociaLite\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/android/ai-samples/\" target=\"_blank\"\u003eAI Samples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/google/oboe/tree/powerplay-sample/samples/powerplay\" target=\"_blank\"\u003ePowerPlay\u003c/a\u003e\u003c/li\u003e  \n\u003c/ul\u003e\u003c/ul\u003e\n\n\u003cp\u003eHopefully these examples have inspired you to explore what new and fascinating experiences you can build on Android. Tune in to \u003ca href=\"https://io.google/2025/explore/technical-session-19\" target=\"_blank\"\u003eour session at Google I/O\u003c/a\u003e in a couple weeks to learn even more about use-cases supported by solutions like Jetpack CameraX and Jetpack Media3!\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "22 min read",
  "publishedTime": null,
  "modifiedTime": null
}
