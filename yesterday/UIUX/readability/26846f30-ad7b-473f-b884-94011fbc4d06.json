{
  "id": "26846f30-ad7b-473f-b884-94011fbc4d06",
  "title": "How to read LLM benchmarks",
  "link": "https://uxdesign.cc/how-to-read-llm-benchmarks-ffc01959b2a8?source=rss----138adf9c44c---4",
  "description": "",
  "author": "Gautham Srinivas",
  "published": "Mon, 16 Dec 2024 09:32:58 GMT",
  "source": "https://uxdesign.cc/feed",
  "categories": [
    "ai",
    "artificial-intelligence",
    "machine-learning",
    "llm",
    "ux"
  ],
  "byline": "Gautham Srinivas",
  "length": 5848,
  "excerpt": "Every once in a while, there’s an announcement about a new model. It’s always better than its predecessor and it’s also better than all of the other frontier models in the market. The announcement…",
  "siteName": "UX Collective",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*dn6MbbIIlwobt0jnUcrt_Q.png",
  "text": "And why you shouldn’t trust them blindlySource: Anthropic’s Claude 3.5 Sonnet blog postDisclaimer: The opinions stated here are my own, not necessarily those of my employer.Every once in a while, there’s an announcement about a new model. It’s always better than its predecessor and it’s also better than all of the other frontier models in the market. The announcement comes with a table that looks like this:The high-level message delivered here is “we are better than everyone else at almost everything”.But how exactly is this claim made? What do these numbers mean? Can you take them at face value? Let’s break it down.Why BenchmarksImagine you’re selling a car and you want to claim that it’s the best car on the market. However, potential buyers look for different features — some want the safest car, while others want the fastest. To convince the broadest audience to choose your car, you compare it against competitors using universally understood data: Safety rating, Fuel efficiency, 0-to-60 time, etc.LLM Benchmarks serve a similar purpose. They are standardized tests and datasets designed to evaluate the performance of models across various tasks. They provide metrics and criteria to compare different models, ensuring consistency and objectivity in assessments.How Benchmarks WorkEach Benchmark evaluates a capability that the LLM might be used for. HumanEval, for example, tests the model’s ability to write code. It consists of a set of 164 programming challenges (ex: finding a substring within a large string) and uses unit tests to check the functional correctness of generated code.Another example is “Reasoning”, which can be defined in different ways. For the purposes of benchmarking, it’s defined as the ability to answer hard, complex questions that require step-by-step deduction and analyzing data. Here’s an example: “Two quantum states with energies E1 and E2 have a lifetime of 10^-9 sec and 10^-8 sec, respectively. We want to clearly distinguish these two energy levels. Which one of the following options could be their energy difference so that they be clearly resolved?” This question will sound hard unless you’re a physicist (or if you enjoy devouring books about quantum mechanics for some reason). It was from the GPQA benchmark, which has 448 such questions across different fields. Models receive a score based on how many questions they answer correctly.Other tests include Language understanding (MMLU) and Math problem solving (MATH). They are similar tests with other types of questions. But within each test, it’s the same set of questions that every model is evaluated against. This is how consistency is maintained (not unlike the idea of humans taking standardized tests).CoT \u0026 Few-shotZooming into the same Claude tableFew-shot (like “3-shot”) refers to the amount of examples that were given to the model to better understand the task. 0-shot means no examples were given. CoT refers to Chain-of-Thought, where the model is asked to explain its reasoning process. CoT and examples can help improve response quality for certain tasks, which is why they are separately highlighted in benchmark results. Here’s an example I got from ChatGPT to explain CoT:The problem with these BenchmarksLack of transparencyWe don’t know how a model was trained. We don’t know how the benchmark tests were run. Then how can we say for sure that the model was not trained on the testing data? This issue is called contamination, which is a common problem in Machine Learning.The sheer amount of data that LLMs are trained on has made it impossibly hard to detect or avoid this problem. It’s the human equivalent of finding out all the questions before the day of the exam.Do these exams really measure ability or intelligence?It’s likely that a significant portion of benchmark results can be explained by the ability of LLMs to memorize vast amounts of data. Then, are they really more capable than humans just because they got a better score? Here’s another example — There was news last year of ChatGPT acing the LSAT. It’s a pretty impressive feat, except when you think about (a) The fact that the LSAT usually contains questions from previous years and (b) How LSAT questions from previous years are freely available all over the internet. If ChatGPT aced the LSAT after seeing the questions before the test, would you still replace your lawyer with it?How to choose for yourselfIf you’re a developer or a team trying to use AI in your product, you need to construct your own evaluation. This evaluation needs to be focused on the use cases that matter to you. The dataset needs to be customized based on your requirements.An example — If you want to automate customer service for your business, build a dataset of questions that your customers might ask. Then, build a system to prompt any LLM with these questions and score the answers they give you. Run this activity between the models you are considering and then make your choice.If you’re an individual user looking to decide if you need to switch between Claude and ChatGPT, you can build a set of your most commonly used prompts (“write a cover letter”, “generate an image of…”, etc) and compare the different responses before making your decision. Even if the results aren’t statistically significant (unless you ask a lot of questions and repeat this process multiple times), it’s a controllable system that can explain your decision making. IMO, it’s much better than opaque benchmarking processes run by companies that are selling models to you.Further reading on running evaluations can be found here. Better writing on the problems with benchmarks can be found here and here.Disclaimer: The opinions stated here are my own, not necessarily those of my employer.Please consider subscribing to my substack if you liked this article. Thank you!",
  "image": "https://miro.medium.com/v2/resize:fit:1200/0*bTQUpb5miXxjlDI_.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"2e16\"\u003eAnd why you shouldn’t trust them blindly\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/@gauthamsrinivas?source=post_page---byline--ffc01959b2a8--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Gautham Srinivas\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*JEFaAw4XgeIymGcxVVpRRw.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://uxdesign.cc/?source=post_page---byline--ffc01959b2a8--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"UX Collective\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*mDhF9X4VO0rCrJvWFatyxg.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cfigure\u003e\u003cfigcaption\u003eSource: Anthropic’s Claude \u003ca href=\"https://www.anthropic.com/news/claude-3-5-sonnet\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e3.5 Sonnet blog post\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"90aa\"\u003e\u003cem\u003eDisclaimer: The opinions stated here are my own, not necessarily those of my employer.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"b024\"\u003eEvery once in a while, there’s an announcement about a new model. It’s always better than its predecessor and it’s also better than all of the other frontier models in the market. The announcement comes with a table that looks like this:\u003c/p\u003e\u003cp id=\"fe2b\"\u003eThe high-level message delivered here is “\u003cstrong\u003e\u003cem\u003ewe are better than everyone else at almost everything\u003c/em\u003e\u003c/strong\u003e”.\u003c/p\u003e\u003cp id=\"4951\"\u003eBut how exactly is this claim made? What do these numbers mean? Can you take them at face value? Let’s break it down.\u003c/p\u003e\u003ch2 id=\"be9f\"\u003eWhy Benchmarks\u003c/h2\u003e\u003cp id=\"709c\"\u003eImagine you’re selling a car and you want to claim that it’s the best car on the market. However, potential buyers look for different features — some want the safest car, while others want the fastest. To convince the broadest audience to choose your car, you compare it against competitors using universally understood data: Safety rating, Fuel efficiency, 0-to-60 time, etc.\u003c/p\u003e\u003cp id=\"9712\"\u003eLLM Benchmarks serve a similar purpose. They are \u003cstrong\u003estandardized tests and datasets designed to evaluate the performance of models across various tasks\u003c/strong\u003e. They provide metrics and criteria to compare different models, ensuring consistency and objectivity in assessments.\u003c/p\u003e\u003ch2 id=\"0454\"\u003eHow Benchmarks Work\u003c/h2\u003e\u003cp id=\"4a68\"\u003eEach Benchmark evaluates a capability that the LLM might be used for. \u003ca href=\"https://klu.ai/glossary/humaneval-benchmark\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHumanEval\u003c/a\u003e, for example, tests the model’s ability to \u003cstrong\u003ewrite code\u003c/strong\u003e. It consists of a set of 164 programming challenges (ex: finding a substring within a large string) and uses unit tests to check the functional correctness of generated code.\u003c/p\u003e\u003cp id=\"9089\"\u003eAnother example is “\u003cstrong\u003eReasoning\u003c/strong\u003e”, which can be defined in different ways. For the purposes of benchmarking, it’s defined as the ability to answer hard, complex questions that require step-by-step deduction and analyzing data. Here’s an example: \u003cem\u003e“Two quantum states with energies E1 and E2 have a lifetime of 10^-9 sec and 10^-8 sec, respectively. We want to clearly distinguish these two energy levels. Which one of the following options could be their energy difference so that they be clearly resolved?”\u003c/em\u003e This question will sound hard unless you’re a physicist (or if you enjoy devouring books about quantum mechanics for some reason). It was from the \u003ca href=\"https://arxiv.org/abs/2311.12022\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGPQA benchmark\u003c/a\u003e, which has 448 such questions across different fields. Models receive a score based on how many questions they answer correctly.\u003c/p\u003e\u003cp id=\"bd40\"\u003eOther tests include Language understanding (\u003ca href=\"https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMMLU\u003c/a\u003e) and Math problem solving (\u003ca href=\"https://paperswithcode.com/sota/math-word-problem-solving-on-math\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMATH\u003c/a\u003e). They are similar tests with other types of questions. But within each test, \u003cstrong\u003eit’s the same set of questions that every model is evaluated against\u003c/strong\u003e. This is how consistency is maintained (not unlike the idea of humans taking standardized tests).\u003c/p\u003e\u003ch2 id=\"c0e1\"\u003eCoT \u0026amp; Few-shot\u003c/h2\u003e\u003cfigure\u003e\u003cfigcaption\u003eZooming into the same Claude table\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"9a88\"\u003e\u003ca href=\"https://arxiv.org/abs/2203.04291\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFew-shot\u003c/a\u003e (like “3-shot”) refers to the amount of examples that were given to the model to better understand the task. 0-shot means no examples were given. CoT refers to \u003ca href=\"https://arxiv.org/abs/2201.11903\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eChain-of-Thought\u003c/a\u003e, where the model is asked to explain its reasoning process. \u003cstrong\u003eCoT and examples can help improve response quality for certain tasks\u003c/strong\u003e, which is why they are separately highlighted in benchmark results. Here’s an example I got from ChatGPT to explain CoT:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"78b2\"\u003eThe problem with these Benchmarks\u003c/h2\u003e\u003cp id=\"5145\"\u003e\u003cstrong\u003eLack of transparency\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"3613\"\u003eWe don’t know how a model was trained. We don’t know how the benchmark tests were run. Then how can we say for sure that the model was not trained on the testing data? \u003cstrong\u003eThis issue is called \u003c/strong\u003e\u003ca href=\"https://www.holisticai.com/blog/overview-of-data-contamination\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003econtamination\u003c/strong\u003e\u003c/a\u003e, which is a common problem in Machine Learning.\u003c/p\u003e\u003cp id=\"1e46\"\u003eThe sheer amount of data that LLMs are trained on has made it impossibly hard to detect or avoid this problem. It’s the human equivalent of finding out all the questions before the day of the exam.\u003c/p\u003e\u003cp id=\"7b71\"\u003e\u003cstrong\u003eDo these exams really measure ability or intelligence?\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"dbfa\"\u003eIt’s likely that a significant portion of benchmark results can be explained by the ability of LLMs to memorize vast amounts of data. Then, are they \u003cem\u003ereally\u003c/em\u003e more capable than humans just because they got a better score? Here’s another example — There was news last year of \u003ca href=\"https://www.firstpost.com/world/chatgpt-aces-lsat-gre-but-fails-english-literature-12303682.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eChatGPT acing the LSAT\u003c/a\u003e. It’s a pretty impressive feat, except when you think about (a) The fact that the LSAT \u003ca href=\"https://www.reddit.com/r/LSAT/comments/16bmoxv/does_the_lsat_ever_reuse_questions/?captcha=1\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eusually contains questions from previous years\u003c/a\u003e and (b) How LSAT questions from previous years are freely available all over the internet. If ChatGPT aced the LSAT after seeing the questions before the test, would you still replace your lawyer with it?\u003c/p\u003e\u003ch2 id=\"4254\"\u003eHow to choose for yourself\u003c/h2\u003e\u003cp id=\"2b46\"\u003eIf you’re a developer or a team trying to use AI in your product, \u003cstrong\u003eyou need to construct your own evaluation\u003c/strong\u003e. \u003cstrong\u003eThis evaluation needs to be focused on the use cases that matter to you. The dataset needs to be customized based on your requirements.\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"7be9\"\u003eAn example — If you want to automate customer service for your business, build a dataset of questions that your customers might ask. Then, build a system to prompt any LLM with these questions and score the answers they give you. Run this activity between the models you are considering and then make your choice.\u003c/p\u003e\u003cp id=\"47bf\"\u003eIf you’re an individual user looking to decide if you need to switch between Claude and ChatGPT, you can build a set of your most commonly used prompts (\u003cem\u003e“write a cover letter”, “generate an image of…”, \u003c/em\u003eetc) and compare the different responses before making your decision. Even if the results aren’t statistically significant (unless you ask a lot of questions and repeat this process multiple times), it’s a controllable system that can explain your decision making. IMO, \u003cstrong\u003eit’s much better than opaque benchmarking processes run by companies that are selling models to you.\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"c522\"\u003eFurther reading on running evaluations can be found \u003ca href=\"https://huggingface.co/learn/cookbook/en/rag_evaluation\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e. Better writing on the problems with benchmarks can be found \u003ca href=\"https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e and \u003ca href=\"https://x.com/emilymbender/status/1636090914346274816\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"54a6\"\u003e\u003cstrong\u003e\u003cem\u003eDisclaimer: The opinions stated here are my own, not necessarily those of my employer.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"ea43\"\u003e\u003ca href=\"https://gauthamsk.substack.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003ePlease consider subscribing to my substack if you liked this article. Thank you!\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-11-24T07:17:29.074Z",
  "modifiedTime": null
}
