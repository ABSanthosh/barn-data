{
  "id": "b874128f-0f72-482c-b50a-a6875db7b178",
  "title": "AI is reshaping UI — have you noticed the biggest change yet?",
  "link": "https://uxdesign.cc/ai-is-reshaping-ui-have-you-noticed-the-biggest-change-yet-ee80efcbf8a5?source=rss----138adf9c44c---4",
  "description": "",
  "author": "Tetiana Sydorenko",
  "published": "Thu, 20 Feb 2025 12:21:14 GMT",
  "source": "https://uxdesign.cc/feed",
  "categories": [
    "ux",
    "ui-design",
    "artificial-intelligence",
    "ui",
    "ai"
  ],
  "byline": "Tetiana Sydorenko",
  "length": 8604,
  "excerpt": "The way we interact with software is anything but static. Sometimes it’s a gentle evolution, other times a jarring leap. Today, a growing wave of design pioneers, including Vitaly Friedman, Emily…",
  "siteName": "UX Collective",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*dn6MbbIIlwobt0jnUcrt_Q.png",
  "text": "AI is reshaping interactions as we know them, driving a new UI paradigm. Let’s break down how.Goodbye commands, hello intentThe way we interact with software is anything but static. Sometimes it’s a gentle evolution, other times a jarring leap. Today, a growing wave of design pioneers, including Vitaly Friedman, Emily Campbell and Greg Nudelman are dissecting emerging patterns within AI applications, mapping out the landscape that refuses to stand still. At first glance, this might seem like yet another hype cycle, the kind of breathless enthusiasm that surrounds every new tech trend. But take a step back, and a deeper transformation becomes apparent: our interactions with digital systems are not just changing; they are shifting in their very essence.Imagine the transition from film cameras to digital photography — suddenly, users no longer had to understand exposure times or carefully ration film. They simply clicked a button, and the device handled the rest.AI is bringing a similar shift to UI design, moving us away from rigid, step-by-step processes and toward fluid, intuitive workflows. The very nature of the interaction is shifting, and as Jakob Nielsen recently stressed in his article, this evolution demands our full attention. He articulates a crucial insight:“With the new AI systems, the user no longer tells the computer what to do. Rather, the user tells the computer what outcome they want”.This isn’t just a technological evolution — it’s a philosophical one. It challenges long-held assumptions about control, agency, and human-machine collaboration. Where once we meticulously dictated every step, we now define intentions and let AI determine the best path forward. This transformation is as profound as the move from command-line interfaces to graphical user interfaces, and for UI designers, it represents both an opportunity and a challenge.Tapping, swiping, asking: How interaction is evolvingBut before we dive into how AI is reshaping interaction, it’s important to reflect on what has defined our most intuitive interfaces so far. In 1985, Edwin Hutchins, James Hollan, and Don Norman published a seminal paper on direct manipulation interfaces. Norman later defined some of the most widely accepted design principles in “The Design of Everyday Things”, while Hutchins pioneered the concept of Distributed Cognition. But in 1985, they, along with Hollan, captured a pivotal moment in design history when direct manipulation was emerging as a dominant strategy.Direct manipulation is an interaction style in which users act on displayed objects of interest using physical, incremental, and reversible actions whose effects are immediately visible on the screen. NN/gBut what does this mean in simple terms? Say, you need to move a file from one folder to another — this is a classic example of direct manipulation — you see the file, grab it, and move it exactly where you want it to go.You start by recognizing your goal (1). Then, you locate the file in its current folder and decide to drag it to the new location (2). You click and hold the file, move it across the screen, and drop it into the target folder (3).If you accidentally drop it in the wrong place, you immediately see the result, adjust your approach, and drag it again until it lands where you intended. This kind of interaction feels intuitive because it minimizes cognitive effort — the system responds in real-time to your actions, reinforcing a sense of direct engagement and control.The smoother this process, the more natural and satisfying the interaction feels.Moving a file on MacOS using direct manipulation involves dragging that file from the source folder and moving it into the destination folder. SourceWhile reducing distance improves usability, what truly defines direct manipulation is engagement. The authors write:“The systems that best exemplify direct manipulation all give the qualitative feeling that one is directly engaged with control of the objects — not with the programs, not with the computer, but with the semantic objects of our goals and intentions.”Direct manipulation has remained a foundational design principle for decades. However, as we transition into AI-driven systems, we must consider how these principles evolve — and when they give way to goal-oriented interactions.Now, think of Windows Photos’ AI-powered ‘Erase’ feature. Say you take a picture of your dog, but there’s an unwanted leash in the shot. Instead of manually selecting pixels and meticulously editing them out, as you would have done a decade ago, you simply select the leash and let the AI handle the rest. The system understands your goal — remove the leash — and executes the best possible solution.Windows Photos, sourceThis interaction still involves some level of manipulation, as you must indicate the object to be erased, but the difference is that you are refining a request rather than directly altering pixels. You are no longer meticulously editing every detail; you are collaborating with the system to achieve a desired outcome. This shift marks a fundamental evolution in UI design.Desolda, along with fellow researchers, captured this dynamic in a model based on Norman’s Gulf of Execution and Gulf of Evaluation. Unlike straightforward direct manipulation — such as dragging a file between folders, where actions unfold step by step — AI interactions demand a more fluid, iterative process. Users articulate their goals, but instead of executing every step manually, they collaborate with the system, refining inputs and guiding the AI as it interprets, adjusts, and responds dynamically.The continued relevance of direct manipulationAI may be reshaping the way we interact with technology, but direct manipulation isn’t going anywhere. Even in an era of intent-based interfaces, users will still need to engage with AI systems, guiding them with the right inputs to translate human goals into machine-readable instructions. Designing AI experiences isn’t about replacing direct manipulation — it’s about enhancing it, layering new interaction models on top of well-established patterns to make interactions smoother, more intuitive, and ultimately, more powerful.To design seamless AI experiences, we need to recognize and build on familiar patterns.For instance, in many AI applications, an open-ended prompt field acts as an icebreaker, helping users get the conversation started. Built upon the familiar input field pattern, which has been a standard UI component for decades, this method now serves a new role. Whether it’s typing a question into ChatGPT or instructing a design tool to generate a layout, this approach provides flexibility while guiding user intent in an intuitive and approachable way.Open input pattern examples, SourceThis approach isn’t limited to interaction patterns — it extends into UX frameworks as well.For example, Evan Sunwall introduced ‘Promptframes’ as a way to complement traditional wireframes by integrating prompt writing and generative AI into the design process. The goal is to increase content fidelity and accelerate user testing by incorporating AI-powered content generation earlier in the workflow. Yet, this concept is built upon the foundation of wireframes, reinforcing the importance of understanding traditional UX structures to effectively design for AI-driven experiences.Final thoughtsThe best interface experiences are the ones users don’t notice. They don’t demand your attention or make you think about how to use them — they just work, letting users focus on what they came to do. AI, when done right, follows this same principle. It doesn’t need neon “powered by AI” labels; it should weave itself so seamlessly into the user journey that it feels like a natural extension of intent.Take Netflix’s recommender system. It doesn’t interrupt your experience to remind you it’s using advanced algorithms. It doesn’t ask you to configure a dozen settings. Instead, it quietly learns, adapts, and presents suggestions that feel effortless — so much so that you rarely stop to think about the system behind it. That’s what AI-driven interaction should be: not a feature you have to wrestle with, but an invisible assistant that refines itself around your needs.As we move toward intent-driven systems, this is the bar designers should aim for. AI should reduce friction, not add complexity. It should empower users, not overwhelm them with unnecessary choices. The best AI isn’t the one that demands attention — it’s the one that disappears into the flow of what you were trying to accomplish in the first place.",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*ED3ed-LZjgaeZQWNZ3a49w.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"2282\"\u003eAI is reshaping interactions as we know them, driving a new UI paradigm. Let’s break down how.\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://tetiana-sydorenko.medium.com/?source=post_page---byline--ee80efcbf8a5---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Tetiana Sydorenko\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*hFZeAfBxhwsmw-eUYoxQtw.png\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://uxdesign.cc/?source=post_page---byline--ee80efcbf8a5---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"UX Collective\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*mDhF9X4VO0rCrJvWFatyxg.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003ch2 id=\"20b2\"\u003eGoodbye commands, hello intent\u003c/h2\u003e\u003cp id=\"d822\"\u003eThe way we interact with software is anything but static. Sometimes it’s a gentle evolution, other times a jarring leap. Today, a growing wave of design pioneers, including \u003ca href=\"https://www.interaction-design.org/master-classes/design-patterns-for-ai-ux?srsltid=AfmBOoqlyw60eOAQQiKPaMAxzTAb7xuxh0_cYtt03gyxIn4onpB9mpNn\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVitaly Friedman\u003c/a\u003e, \u003ca href=\"https://www.shapeof.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEmily Campbell\u003c/a\u003e and \u003ca href=\"https://www.uxforai.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGreg Nudelman\u003c/a\u003e are dissecting emerging patterns within AI applications, mapping out the landscape that refuses to stand still. At first glance, this might seem like yet another hype cycle, the kind of breathless enthusiasm that surrounds every new tech trend. But take a step back, and a deeper transformation becomes apparent: our interactions with digital systems are not just changing; they are shifting in their very essence.\u003c/p\u003e\u003cp id=\"4e28\"\u003eImagine the transition from film cameras to digital photography — suddenly, users no longer had to understand exposure times or carefully ration film. They simply clicked a button, and the device handled the rest.\u003c/p\u003e\u003cp id=\"1c74\"\u003eAI is bringing a similar shift to UI design, moving us away from rigid, step-by-step processes and toward fluid, intuitive workflows. The very nature of the interaction is shifting, and as \u003ca href=\"https://www.nngroup.com/articles/ai-paradigm/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJakob Nielsen recently stressed in his article\u003c/a\u003e, this evolution demands our full attention. He articulates a crucial insight:\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"79b1\"\u003e“With the new AI systems, the user no longer tells the computer what to do. Rather, the user tells the computer what outcome they want”.\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"7da6\"\u003eThis isn’t just a technological evolution — it’s a philosophical one. It challenges long-held assumptions about control, agency, and human-machine collaboration. Where once we meticulously dictated every step, we now define intentions and let AI determine the best path forward. This transformation is as profound as the move from \u003ca href=\"https://www.coursera.org/articles/command-line-interface\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecommand-line interfaces\u003c/a\u003e to \u003ca href=\"https://www.interaction-design.org/literature/topics/graphical-user-interfaces\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003egraphical user interfaces\u003c/a\u003e, and for UI designers, it represents both an opportunity and a challenge.\u003c/p\u003e\u003ch2 id=\"ca73\"\u003eTapping, swiping, asking: How interaction is evolving\u003c/h2\u003e\u003cp id=\"cff7\"\u003eBut before we dive into how AI is reshaping interaction, it’s important to reflect on what has defined our most intuitive interfaces so far. In 1985, \u003ca href=\"https://www.interaction-design.org/literature/author/edwin-hutchins?srsltid=AfmBOooCr1H23XG0v5-cw2OOAZLUW-cNfk63vVeuFLOldPnOZkZpxxTI\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEdwin Hutchins\u003c/a\u003e, \u003ca href=\"https://www.semanticscholar.org/author/James-Hollan/1698170\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJames Hollan\u003c/a\u003e, and \u003ca href=\"https://www.interaction-design.org/literature/topics/don-norman?srsltid=AfmBOoqhgaBrCVz6nc61GENO1IHf0WrzQJszLL2KMeamh817Wu0nyfPa\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDon Norman\u003c/a\u003e published \u003ca href=\"https://www.lri.fr/~mbl/ENS/FONDIHM/2013/papers/Hutchins-HCI-85.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ea seminal paper on direct manipulation interfaces\u003c/a\u003e. Norman later defined some of the most widely accepted design principles in “\u003ca href=\"https://www.amazon.com/Design-Everyday-Things-Revised-Expanded/dp/0465050654\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eThe Design of Everyday Things”\u003c/a\u003e, while Hutchins pioneered the concept of Distributed Cognition. But in 1985, they, along with Hollan, captured a pivotal moment in design history when\u003cstrong\u003e direct manipulation\u003c/strong\u003e was emerging as a dominant strategy.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"7311\"\u003e\u003cstrong\u003eDirect manipulation\u003c/strong\u003e is an interaction style in which users act on displayed objects of interest using physical, incremental, and reversible actions whose effects are immediately visible on the screen.\u003cem\u003e \u003c/em\u003e\u003ca href=\"https://www.nngroup.com/articles/direct-manipulation/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eNN/g\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"2d2c\"\u003eBut what does this mean in simple terms? Say, you need to move a file from one folder to another — this is a classic example of direct manipulation — you see the file, grab it, and move it exactly where you want it to go.\u003c/p\u003e\u003cp id=\"5335\"\u003eYou start by recognizing your goal (1). Then, you locate the file in its current folder and decide to drag it to the new location (2). You click and hold the file, move it across the screen, and drop it into the target folder (3).\u003c/p\u003e\u003cp id=\"430c\"\u003eIf you accidentally drop it in the wrong place, you immediately see the result, adjust your approach, and drag it again until it lands where you intended. This kind of interaction feels intuitive because it minimizes cognitive effort — the system responds in real-time to your actions, reinforcing a sense of direct engagement and control.\u003c/p\u003e\u003cp id=\"87b3\"\u003eThe smoother this process, the more natural and satisfying the interaction feels.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eMoving a file on MacOS using direct manipulation involves dragging that file from the source folder and moving it into the destination folder. \u003c/em\u003e\u003ca href=\"https://www.nngroup.com/articles/direct-manipulation/#:~:text=Direct%20manipulation%20(DM)%20is%20an,immediately%20visible%20on%20the%20screen.\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eSource\u003c/em\u003e\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"d880\"\u003eWhile reducing distance improves usability, what truly defines direct manipulation is engagement. The authors write:\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"baac\"\u003e\u003cem\u003e“The systems that best exemplify direct manipulation all give the qualitative feeling that one is directly engaged with control of the objects — not with the programs, not with the computer, but with the semantic objects of our goals and intentions.”\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"4d06\"\u003eDirect manipulation has remained a foundational design principle for decades. However, as we transition into AI-driven systems, we must consider how these principles evolve — and when they give way to goal-oriented interactions.\u003c/p\u003e\u003cp id=\"3e8f\"\u003eNow, think of \u003ca href=\"https://blogs.windows.com/windows-insider/2024/02/22/windows-photos-gets-generative-erase-and-recent-ai-editing-features-now-available-on-arm64-devices-and-windows-10/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eWindows Photos’ AI-powered ‘Erase’ feature\u003c/a\u003e. Say you take a picture of your dog, but there’s an unwanted leash in the shot. Instead of manually selecting pixels and meticulously editing them out, as you would have done a decade ago, you simply select the leash and let the AI handle the rest. The system understands your goal — remove the leash — and executes the best possible solution.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eWindows Photos, \u003ca href=\"http://blogs.windows.com/windows-insider/2024/02/22/windows-photos-gets-generative-erase-and-recent-ai-editing-features-now-available-on-arm64-devices-and-windows-10/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esource\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"4df5\"\u003eThis interaction still involves some level of manipulation, as you must indicate the object to be erased, but the difference is that you are refining a request rather than directly altering pixels. You are no longer meticulously editing every detail; you are collaborating with the system to achieve a desired outcome. This shift marks a fundamental evolution in UI design.\u003c/p\u003e\u003cp id=\"021d\"\u003eDesolda, along with fellow researchers, \u003ca href=\"https://pdf.sciencedirectassets.com/271219/1-s2.0-S0933365724X00081/1-s2.0-S0933365724001751/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG4aCXVzLWVhc3QtMSJIMEYCIQDhWv4qg%2BB94rKjxi2RYo3%2Fxsd%2FJJ%2F7T1F86LzkxkVoIAIhAN%2BY3YLJkceDIc9j0ZILQ69W1iTx%2BKFkTkk0WEpvqMENKrwFCJf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1IgwIBcX2N1XElrWsvMAqkAU42MEqZVTKmpa2JVAtPQCIecRVOGjLSXakTN03588AUq6XH6Ig3XRVKxrOEbaQ%2BEI%2BF8O2jaIgbv%2FWiKNwcW%2BjeihYKmOLNr4yzunn%2FxqU1InMMu6YmnE9nFnHsZrJa%2BSYE%2Fd%2Bq%2B6b%2BPYAsEkuaolke0GiVQRTSgwPz18xcG9hvnAUWwJ5tTgdEIhcojSvNxLE22Bstyv4n3ghTorZEkUoooKUlcx3r79Cmtz8Vv0NDixyykBeNrisc5Uf0rkqJ%2BnY0S%2F7sB7i0ipiscT8k16y0dDnG0cYdnBt7%2FqS2EmajoapWROFwS6xKfoXYAuk%2BqIBISfxFcTCvZRLsL1mAgi53U866CQSXKRjM7b%2Bf5breLtf%2B6GwN2q8u23%2BmUQLCNlalSHhgIRFx%2BEgQeHMzUP%2Ftw9L2Q%2FIR8AlTKKgYkEsP1xIE3HMR4YqCyBuDHrymFxXH28VidUe%2FcyLcqqwPnuTx%2B6HDnXK9HE%2BD2GrjQxKAzNhr%2F3jT73k8vQY3SUrffUqECkBph%2FRbsafPqU%2BwI155B1EryatMWvittJl4cgmAdadooHoJtKS9SmbGIeZU6pQ8drWFWFHCSikxLCm5LdkXG4XrbYvlgZlBDSKQfePEOk1HPaTxn1XI0vh7HJi1WVktSar5VAVBqRfCUQ0tAwNrh421HTV0WMlrRbbpvW6IsKHcdlZ9NeIShVULu7wJ%2BjGGwPD8aKTjduvmTg5HgbDGb1hyBsyMOpGTklg7%2B9Db%2FVaBGedKsjb%2FMCWzwf1yZiMtsNcz%2BgLqHL0C3wxWqRqjf26PvT6YTBwt6Rx1ELlCMBhxoHMxKwB3X%2FAWW6aRCOX5TGJiiKqSQoX3kdFLqyFvU0gb3k94sx981JkwKSJjTD09dO9BjqwASrN6DFyYDsVA8gJfhvaY6fqXyBloshA279HDQq8VTtKEUzTUl7FDGTlup%2FdHnNSc3OTm2FchLUcK6gqwisqSac7H12fRfqojpK%2BlvTmev3IdrWmE4pSC3NTIPAadmuS7RPSx1xp1pdHNkqfeS4sbrT4D%2FZWaEVm6pl30235n4HHpDMmlCktshiZciYpJT29MeSaCFI1MWbvqyGSQnoUNbqgC%2BW8WrYSmQ5GnKB%2B8o21\u0026amp;X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026amp;X-Amz-Date=20250218T221818Z\u0026amp;X-Amz-SignedHeaders=host\u0026amp;X-Amz-Expires=300\u0026amp;X-Amz-Credential=ASIAQ3PHCVTY3UVEDXKN%2F20250218%2Fus-east-1%2Fs3%2Faws4_request\u0026amp;X-Amz-Signature=6ccea7117aed1b2c2cff6f76c0b97909fa53611895266359b01b99408d4c8dd4\u0026amp;hash=eedce0e95a85e0b4b3d98f7fc142013d0c0409ff4feed67190e2413b1d3ee6ea\u0026amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61\u0026amp;pii=S0933365724001751\u0026amp;tid=spdf-47125350-406e-4990-aca9-7f7dd20dff4b\u0026amp;sid=30075f051c83594f44795fc06d3adb9a5e29gxrqb\u0026amp;type=client\u0026amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t\u0026amp;ua=19155857505e56505458\u0026amp;rr=914164697d0f3516\u0026amp;cc=cz\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecaptured this dynamic\u003c/a\u003e in a model based on \u003ca href=\"https://app.uxcel.com/courses/ux-design-psychology/the-nature-of-user-errors-902/the-gulfs-of-evaluation-and-execution-8093\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNorman’s Gulf of Execution and Gulf of Evaluation\u003c/a\u003e. Unlike straightforward direct manipulation — such as dragging a file between folders, where actions unfold step by step — AI interactions demand a more fluid, iterative process. Users articulate their goals, but instead of executing every step manually, they collaborate with the system, refining inputs and guiding the AI as it interprets, adjusts, and responds dynamically.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"558e\"\u003eThe continued relevance of direct manipulation\u003c/h2\u003e\u003cp id=\"b937\"\u003eAI may be reshaping the way we interact with technology, but direct manipulation isn’t going anywhere. Even in an era of intent-based interfaces, users will still need to engage with AI systems, guiding them with the right inputs to translate \u003ca href=\"https://pair.withgoogle.com/guidebook-v2/chapters/user-needs/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehuman goals\u003c/a\u003e into machine-readable instructions. Designing AI experiences isn’t about replacing direct manipulation — it’s about enhancing it, layering new interaction models on top of well-established patterns to make interactions smoother, more intuitive, and ultimately, more powerful.\u003c/p\u003e\u003cp id=\"beaf\"\u003eTo design seamless AI experiences, we need to recognize and build on \u003ca href=\"https://www.interaction-design.org/literature/topics/ui-design-patterns?srsltid=AfmBOor8VB9z7Pl0PxMk0ELWYh3lwgcMwVyRbUdVwXJ5GhEpOQCZT7Tx\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003efamiliar patterns\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"a689\"\u003eFor instance, in many AI applications, an \u003ca href=\"https://www.shapeof.ai/patterns/open-input\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eopen-ended prompt field\u003c/a\u003e acts as an icebreaker, helping users get the conversation started. Built upon the familiar input field pattern, which has been a standard UI component for decades, this method now serves a new role. Whether it’s typing a question into ChatGPT or instructing a design tool to generate a layout, this approach provides flexibility while guiding user intent in an intuitive and approachable way.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eOpen input pattern examples, \u003ca href=\"https://www.shapeof.ai/patterns/open-input\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSource\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"d968\"\u003eThis approach isn’t limited to interaction patterns — it extends into UX frameworks as well.\u003c/p\u003e\u003cp id=\"b636\"\u003eFor example, \u003ca href=\"https://www.nngroup.com/articles/promptframes/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEvan Sunwall introduced ‘Promptframes’\u003c/a\u003e as a way to complement traditional \u003ca href=\"https://www.interaction-design.org/literature/topics/wireframe?srsltid=AfmBOor2b9qCG4iwzCm4Gah5RJrySRtbsJzBb0swed_fMylcRtQIKjUB\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ewireframes\u003c/a\u003e by integrating prompt writing and generative AI into the design process. The goal is to increase content fidelity and accelerate user testing by incorporating AI-powered content generation earlier in the workflow. Yet, this concept is built upon the foundation of wireframes, reinforcing the importance of understanding traditional UX structures to effectively design for AI-driven experiences.\u003c/p\u003e\u003ch2 id=\"62da\"\u003eFinal thoughts\u003c/h2\u003e\u003cp id=\"bf14\"\u003e\u003ca rel=\"noopener\" target=\"_blank\" href=\"https://uxdesign.cc/it-s-not-the-interface-that-makes-the-experience-fb16f8e29299\"\u003eThe best interface experiences are the ones users don’t notice\u003c/a\u003e. They don’t demand your attention or make you think about how to use them — they just work, letting users focus on what they came to do. AI, when done right, follows this same principle. It doesn’t need \u003ca rel=\"noopener\" target=\"_blank\" href=\"https://uxdesign.cc/the-way-we-design-ai-looks-like-nostalgia-already-aa5dfc01d4c7\"\u003eneon “powered by AI” labels\u003c/a\u003e; it should weave itself so seamlessly into the user journey that it feels like a natural extension of intent.\u003c/p\u003e\u003cp id=\"08e8\"\u003eTake \u003ca href=\"https://dl.acm.org/doi/10.1145/2843948\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNetflix’s recommender system\u003c/a\u003e. It doesn’t interrupt your experience to remind you it’s using advanced algorithms. It doesn’t ask you to configure a dozen settings. Instead, it quietly learns, adapts, and presents suggestions that feel effortless — so much so that you rarely stop to think about the system behind it. That’s what AI-driven interaction should be: not a feature you have to wrestle with, but an invisible assistant that refines itself around your needs.\u003c/p\u003e\u003cp id=\"dea4\"\u003eAs we move toward intent-driven systems, this is the bar designers should aim for. AI should reduce friction, not add complexity. It should empower users, not overwhelm them with unnecessary choices. The best AI isn’t the one that demands attention — it’s the one that disappears into the flow of what you were trying to accomplish in the first place.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2025-02-20T12:21:14.364Z",
  "modifiedTime": null
}
