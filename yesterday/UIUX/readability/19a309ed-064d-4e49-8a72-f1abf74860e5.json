{
  "id": "19a309ed-064d-4e49-8a72-f1abf74860e5",
  "title": "A practitioner’s journal on navigating UX in the age of AI",
  "link": "https://uxdesign.cc/a-practitioners-journal-on-navigating-ux-in-the-age-of-ai-97f0a11e8319?source=rss----138adf9c44c---4",
  "description": "",
  "author": "Jonathan Montalvo",
  "published": "Tue, 03 Jun 2025 11:18:58 GMT",
  "source": "https://uxdesign.cc/feed",
  "categories": [
    "ux",
    "user-experience",
    "ai",
    "product-design",
    "agentic-ai"
  ],
  "byline": "Jonathan Montalvo",
  "length": 18423,
  "excerpt": "I sat there, somewhere between awe and anxiety, reflecting on my journey as a designer — where I started, what I’ve learned, and what still feels unknown. I’ve watched the confidence of seasoned…",
  "siteName": "UX Collective",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*dn6MbbIIlwobt0jnUcrt_Q.png",
  "text": "A practical and personal look at how technology is reshaping the role — and responsibility — of designers.Image source: Lummi.ai — by Ricardo MatosIt started with a late-night scroll — headlines about AI, layoffs, and a new kind of design future.I sat there, somewhere between awe and anxiety, reflecting on my journey as a designer — where I started, what I’ve learned, and what still feels unknown. I’ve watched the confidence of seasoned peers collide with the hesitation of those just starting out. And somewhere in that tension, I felt the need to write.This isn’t just an article — it’s a map of my thoughts. A journal entry from the edge of the change I feel will impact us all.As the world grows more digital by the second, screens continue to shape nearly every interaction we have — whether we’re working, socializing, learning, or relaxing. In this shifting landscape, User Experience Design (UX) has become more than just a function of product development. It’s the connective tissue between humans, technology, and business.But as the boundaries of design blur with the rise of AI, autonomous agents, and emerging modalities like voice and ambient computing — I find myself asking: What does the future hold for the UX profession?Today: the current state of UXAccording to the UX Trends 2025 report, the state of UX today reflects a maturing discipline that is increasingly foundational to how digital products are built, scaled, and evolved. UX has evolved beyond screens and interfaces — it now, or should guide strategy, inform systems thinking, and shape how teams build for people in complex environments.Personalized at ScalePersonalization today goes far beyond saving preferences or tailoring content. With machine learning models embedded in design systems, we’re delivering experiences that adapt in real time — based on behavior, context, and predictive signals.My personal SpotifyFrom adaptive onboarding flows to intelligent feature delivery, UX is becoming deeply anticipatory. These systems aren’t just responding to users — they’re learning from them, identifying patterns, and proactively shaping interactions to feel more seamless and relevant. Take Spotify, for example. Its recommendation engine continuously adapts to listening habits, surfacing new music and curating playlists that evolve with the user. This is personalization at scale — powered by AI, refined through behavior, and executed in milliseconds.Designing for these experiences means moving beyond static personas to dynamic behaviors. It’s UX informed by insight, powered by data, and driven by systems that evolve alongside the user.Deep CollaborationGone are the days of siloed design handoffs. UX today thrives in environments where designers, PMs, and engineers work together from the outset — not just to ship, but to discover. The UX Trends 2025 report identifies this as a sign of UX maturity: integrated, problem-solving teams aligned on user value and business impact.This vision aligns with Marty Cagan’s idea of empowered product teams — cross-functional groups that own outcomes, not just deliverables. In his view, design isn’t just a support role — it’s central to shaping and solving meaningful product challenges.Image source: Maze, Marty Cagan — diagram of the product trioBut while the trend report highlights the presence of collaboration, Cagan pushes us to consider the quality of it. He cautions that collaboration isn’t consensus. Empowered teams are trusted to make decisions, take risks, and challenge each other with alignment, not uniformity.In his book Inspired, he states that great products emerge when teams operate with clarity, context, and autonomy. UX isn’t just at the table, it’s helping to define the table’s purpose.Ethical \u0026 Inclusive by DefaultWith greater influence comes greater responsibility — and yes, I’m paraphrasing Spider-Man’s Uncle Ben. Ethical design is now a core expectation, not just in theory but in practice. Teams are being asked to address everything from algorithmic bias and exclusionary defaults to manipulative patterns and misinformation. Accessibility, privacy, and psychological safety are no longer side considerations — they’re product requirements.Concept \u0026 illustration by Trina Moore PervallInclusive design goes beyond compliance. It’s about designing for edge cases and historically underserved users — understanding how race, gender, ability, language, and socioeconomic status intersect with technology. The best teams prioritize co-creation, inviting diverse perspectives into research and testing processes to build more equitable experiences from the ground up.As systems become more automated and autonomous, the ethical stakes are higher. Designers must now think in terms of unintended consequences, system-level harms, and long-term trust. Ethical frameworks are evolving into operational practices: checklists, red-team reviews, bias audits, and accountability rituals.As highlighted in the UX Trends 2025 report, ethical and inclusive design isn’t a layer to add, it’s the lens through which the entire product experience must be shaped. It’s the benchmark for quality in today’s experience-driven world.Image source: McKinsey \u0026 Company — What is an AI agent?The shift happening now: toolchains, layoffs \u0026 the AI resetWhile the future often feels theoretical, today’s designers are already navigating dramatic shifts in how work is done.AI agents in the wildEarly AI agents are already reshaping workflows. AI agents are a system that can reason, plan, and take actions on its own based on information it’s given to manage workflows, use external tools, and adapt as things change. Tools like n8n.io let teams automate multi-step processes with ease — from generating content and summarizing research to integrating APIs and triggering reminders. These aren’t science fiction sidekicks. They’re real, available, and starting to augment creative work.Let’s not confuse AI agents with automation. I recently watched a breakdown by Kevin Hutson from Futurepedia about AI agents and he mentioned, “Automation is more like predefined fixed steps whereas AI agents are a more dynamic and flexible system capable of reasoning.”In his video he noted that agents rely on three key components:The Brain (LLM) — A Large Language Model like ChatGPT, Claude, Gemini, etc., handles the reasoning, planning, and language generation.Memory — This gives the agent the ability to remember past interactions and use that context to make better decisions. It might remember previous steps in a conversation or pull from stored memory sources like documents or a vector database.Tools — How the agent interacts with the world — searching the web or pulling info from a document, taking action by sending emails, updating databases or creating calendar events, and the orchestration of calling other agents, triggering workflow or fusing actions together.Designers today are experimenting with agents that run user tests, market research, flag accessibility issues, or build prototypes from written prompts. The question is no longer if agents will assist us, but how we design around them.The continued rise of No-Code \u0026 Vibe CodingNo-code platforms like Webflow, Framer, and now Figma Make are enabling designers to bypass engineers and bring their visions to life faster. Combined with AI-assisted creation, “vibe coding” is emerging — a practice where intuition meets prompt engineering to generate layouts, content, and flows.Vibe coding toolkitVibe coding is more than just a new buzzword — it’s a shift in mindset. Other tools like Bolt and Lovable, which are more dedicated vibe coding tools, allow creators to work through feeling, tone, and energy. Designers can describe the vibe of an experience — playful, serious, minimal, bold — and use AI tools to manifest that intention instantly in visual form. It’s rapid, intuitive, and deeply aligned with how many creatives actually think.This isn’t just about speed. It’s about shifting power. Designers can now experiment more freely, iterate faster, and own more of the production process. And that freedom changes how we think — focusing us more on expression and good taste, where designers must be able to clearly articulate the language of design in a tasteful way that expresses to a machine how a product needs to feel, flow, and function.The AI reset \u0026 the new team structureAccording to Forbes, the “AI reset” is prompting mass layoffs across tech — not simply for cost savings, but to recalibrate for a leaner, more AI-native workforce. Organizations are reducing headcount, rethinking team structures, and placing more emphasis on toolchains than headcount.Image source: Getty via Forbes articleDesign teams are being asked to do more with less. This shift demands designers who are not only visually skilled but operationally savvy — comfortable with automation, systems thinking, and rapid prototyping.“We’re not just designing products anymore. We’re designing the way work itself happens.”Tomorrow: the near future of UXGoogle I/O 2025 revealed numerous innovations that serve as early indicators of upcoming UX trends. The new product releases signal a major evolution in human interaction methods with AI systems which work across different platforms and devices. Designers must understand these signals and use them to develop experiences that are usable and inclusive to all humanity.AI-powered personalizationPicture user interfaces that understand your behaviors so well that they can predict what you’ll do next. The merging of AI into everyday tools enables unprecedented levels of personalization. For instance, Google’s Gemini AI assistant delivers “Personalized Smart Replies” within Gmail that replicate users’ writing styles to improve communication efficiency. Customization at this level improves user engagement by producing interactions that appear instinctive and straightforward.From a UX standpoint, this raises deeper questions: At what point does personalization become manipulation? How can we build user experiences that evolve based on individual needs while ensuring they maintain both transparency and user control?Voice, gesture \u0026 multimodal UXThe traditional screen-based interface is evolving. Devices now possess enhanced functionality to process and react to voice commands along with gestures and visual signals. The launch of “Gemini Live” demonstrates this shift. Through this feature users can immediately engage their surroundings by using their camera to have back-and-forth conversations with AI about visible objects.Image Source: GoogleThis ushers in new UX challenges: We must create conversational and gestural feedback loops that match the clarity and assurance provided by a click. What strategies can we implement to guarantee accessibility for every form of user interaction?Systems over screensSeamless integration across platforms and devices holds the key to the future of UX. Through its “AI Mode” search function, Google replaces conventional search result formats with interactive chatbot experiences that enable users to gather information more intuitively and faster.Google launched “Stitch”, their vibe coding tool that allows application development through natural language inputs and image prompts to connect design and development processes. I’m curious about how engineering teams will be affected by these developments.Main fuctions of StitchThe development of these tools shows a continuing shift in the industry toward AI-enhanced design methods that distribute UX design beyond individual UIs to multiple surfaces and contextual moments. With AI serving as the foundational layer for daily interactions, designers now need to focus on orchestrating systems rather than building them as they create invisible architectures which deliver tangible results.Google’s new advancements reveal how the future will unfold. These new product announcements suggest that future UX design will extend beyond traditional clicks to create immersive experiences throughout users’ environments.Beyond: UX in the next 5–10 yearsDesigners who thrive in the next decade will be those who lean into ambiguity, shape emerging tech with empathy, and hold the line on what makes technology human.AI agents as experience partnersThe rise of intelligent agents will mean users don’t just interact with interfaces — they partner with them. From personal schedulers to shopping assistants and creative collaborators, agents will carry out tasks with increasing autonomy.In the next 5–10 years, we can expect a shift from isolated, task-specific agents to fully integrated, multi-agent ecosystems. These systems will allow AI agents to collaborate with one another — negotiating schedules, delegating tasks, and even learning from shared user behavior across platforms. Imagine a project management AI coordinating with your personal calendar agent, your team’s research agent, and a marketing analytics agent — all working together to keep you focused and productive. Wow! My brain hurts just thinking about this.Image source: n8n.io | n8n research agent workflowMass adoption of AI agents will likely become a workforce norm. According to Salesforce, AI agent usage is expected to rise dramatically by 327%, with projected productivity gains of up to 30%. For professionals, this won’t just be a competitive advantage — it will be a basic expectation. Much like we’re expected to know how to use email or spreadsheets today, fluency in managing and co-working with AI agents will become standard.For UX designers, this means creating not just user flows — but agent frameworks. We’ll need to shape how agents communicate with each other, how they surface decisions to users, and how trust, context, and boundaries are preserved across every interaction.This challenges us to design experiences that are relational, not transactional. How do we build trust with a digital entity? What does a good “agent personality” look like? How do we give users control without overwhelming them?As economist Richard Baldwin said, “AI won’t take your job — but someone who knows how to use AI will.” This is where designers must thrive: crafting agents that build, collaborate, and accelerate workflows. The human advantage is you.The non-interface futureScreens may disappear, but design will persist. As highlighted in this LinkedIn piece by Muhammad Zeeshan Asghar , the next decade of UX may be shaped more by how we feel and perceive than by what we tap and scroll. From neural interfaces to ambient computing, interactions will increasingly be mediated through space, sensation, and subtle context-aware cues.Image source: Toptal | multimodal realityThis transition challenges designers to think beyond visible surfaces. We’ll need to become choreographers of the invisible — designing feedback loops that feel intuitive without relying on screens, buttons, or clicks. The future UX layer may live in our environments, wearables, or neural signals — but it will still require intention, clarity, and a human touch.Designers as ethical orchestratorsAs automation takes on the labor, the designer’s role will become increasingly strategic. We’ll be called to shape not just the user journey, but the moral architecture behind it. Our work will determine what AI does, who it serves, and where the boundaries are drawn.Made with ChatGPT by authorAs outlined in this article by Design Bootcamp, ethical UX design in the AI era isn’t just about preventing harm — it’s about anticipating unintended consequences, designing for edge cases, and embedding human values into algorithms that scale. It’s no longer enough to ask, “can we build this?” The more important question is: “should we?”This requires deeper collaboration with data scientists, ethicists, and product leaders. It demands transparency in how AI decisions are made and fairness in who they impact. As AI grows more autonomous, designers must champion user trust, agency, and dignity in every experience.Ethical design won’t be a slide in the deck — it will be the foundation.The future won’t be led by those who simply adapt to AI, but by those who guide it — with intention, responsibility, and humanity.OpenAI’s acquisition of io: a paradigm shift in UXIn a landmark move, OpenAI has acquired io, the AI hardware startup founded by former Apple design chief Jony Ive for $6.5 billion. This acquisition signifies OpenAI’s strategic entry into the consumer hardware market, aiming to develop AI-native devices that transcend traditional screens and interfaces.Image source: OpenAI—Sam \u0026 Jony introduce ioThe collaboration between OpenAI and Ive’s design firm, LoveFrom, is set to redefine user interactions with technology. By integrating advanced AI capabilities with iconic industrial design, the partnership aspires to create devices that are contextually aware, ambient, and seamlessly integrated into users’ lives.This move underscores a shift toward experiences that are not just user-friendly but profoundly user-centric — emphasizing empathy, ethical design, and utility that feels almost invisible. As reported by the New York Post, the deal hints at an entirely new category of interaction design that is screen-optional, multimodal, and shaped by intent rather than input.For UX professionals, this development signals a broader transformation: we are not just designing software; we are designing intelligent, embodied experiences. This calls for a hybrid skill set that blends behavioral science, interaction design, and systems thinking in ways never before required.UX is entering a new era — not defined by tools, but by tensions. Between speed and depth. Automation and intention. Power and empathy. The next frontier of design isn’t about control. It’s about conversation. Between human and machine, intent and outcome, possibility and principle.My advice to designers: Learn something new every day and stay adaptable in this ever-changing landscape of technology and experience design. Be an early adopter. Embrace experimentation. And work toward becoming a Super IC, a title coined by Garron Engstrom, Director of Product Design at Meta — someone who bridges business goals, leadership, emerging tech, product and human-centered thinking in the age of agentic design.The question for all of us isn’t just “what can we design?” but rather:“What world are we designing toward?”",
  "image": "https://miro.medium.com/v2/resize:fit:1196/1*K4ubDs5GNhdPHZzF1W5aNQ.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"c51b\"\u003eA practical and personal look at how technology is reshaping the role — and responsibility — of designers.\u003c/h2\u003e\u003cdiv tabindex=\"-1\" aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/@j.montalvo21?source=post_page---byline--97f0a11e8319---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Jonathan Montalvo\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*nha7hKj5bvx92wf5F59jcg.jpeg\" width=\"32\" height=\"32\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eImage source: Lummi.ai — by Ricardo Matos\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"ea0b\"\u003eIt started with a late-night scroll — headlines about AI, layoffs, and a new kind of design future.\u003c/p\u003e\u003cp id=\"8bab\"\u003eI sat there, somewhere between awe and anxiety, reflecting on my journey as a designer — where I started, what I’ve learned, and what still feels unknown. I’ve watched the confidence of seasoned peers collide with the hesitation of those just starting out. And somewhere in that tension, I felt the need to write.\u003c/p\u003e\u003cp id=\"b10c\"\u003eThis isn’t just an article — it’s a map of my thoughts. A journal entry from the edge of the change I feel will impact us all.\u003c/p\u003e\u003cp id=\"ff21\"\u003eAs the world grows more digital by the second, screens continue to shape nearly every interaction we have — whether we’re working, socializing, learning, or relaxing. In this shifting landscape, User Experience Design (UX) has become more than just a function of product development. It’s the connective tissue between humans, technology, and business.\u003c/p\u003e\u003cp id=\"1124\"\u003eBut as the boundaries of design blur with the rise of AI, autonomous agents, and emerging modalities like voice and ambient computing — I find myself asking:\u003cstrong\u003e\u003cem\u003e What does the future hold for the UX profession?\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"54dc\"\u003eToday: the current state of UX\u003c/h2\u003e\u003cp id=\"946d\"\u003eAccording to the \u003ca href=\"https://trends.uxdesign.cc/\" rel=\"noopener\" target=\"_blank\"\u003eUX Trends 2025 report\u003c/a\u003e, the state of UX today reflects a maturing discipline that is increasingly foundational to how digital products are built, scaled, and evolved. UX has evolved beyond screens and interfaces — it now, or should guide strategy, inform systems thinking, and shape how teams build for people in complex environments.\u003c/p\u003e\u003ch2 id=\"fba2\"\u003ePersonalized at Scale\u003c/h2\u003e\u003cp id=\"ea53\"\u003ePersonalization today goes far beyond saving preferences or tailoring content. With machine learning models embedded in design systems, we’re delivering experiences that adapt in real time — based on behavior, context, and predictive signals.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eMy personal Spotify\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"3fe9\"\u003eFrom adaptive onboarding flows to intelligent feature delivery, UX is becoming deeply anticipatory. These systems aren’t just responding to users — they’re learning from them, identifying patterns, and proactively shaping interactions to feel more seamless and relevant. Take \u003ca href=\"https://newsroom.spotify.com/2023-10-18/how-spotify-uses-design-to-make-personalization-features-delightful/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSpotify\u003c/a\u003e, for example. Its recommendation engine continuously adapts to listening habits, surfacing new music and curating playlists that evolve with the user. This is personalization at scale — powered by AI, refined through behavior, and executed in milliseconds.\u003c/p\u003e\u003cp id=\"eb03\"\u003eDesigning for these experiences means moving beyond static personas to dynamic behaviors. It’s UX informed by insight, powered by data, and driven by systems that evolve alongside the user.\u003c/p\u003e\u003ch2 id=\"7c85\"\u003eDeep Collaboration\u003c/h2\u003e\u003cp id=\"6f31\"\u003eGone are the days of siloed design handoffs. UX today thrives in environments where designers, PMs, and engineers work together from the outset — not just to ship, but to discover. The \u003ca href=\"https://trends.uxdesign.cc/\" rel=\"noopener\" target=\"_blank\"\u003eUX Trends 2025 report\u003c/a\u003e identifies this as a sign of UX maturity: integrated, problem-solving teams aligned on user value and business impact.\u003c/p\u003e\u003cp id=\"a91a\"\u003eThis vision aligns with Marty Cagan’s idea of \u003ca href=\"https://www.svpg.com/empowered-product-teams/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eempowered product teams\u003c/em\u003e\u003c/a\u003e — cross-functional groups that own outcomes, not just deliverables. In his view, design isn’t just a support role — it’s central to shaping and solving meaningful product challenges.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eImage source: Maze, Marty Cagan — diagram of the product trio\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"4fc1\"\u003eBut while the trend report highlights the presence of collaboration, Cagan pushes us to consider the quality of it. He cautions that collaboration isn’t consensus. Empowered teams are trusted to make decisions, take risks, and challenge each other with alignment, not uniformity.\u003c/p\u003e\u003cp id=\"5056\"\u003eIn his book \u003ca href=\"https://svpg.com/inspired-how-to-create-tech-products-customers-love/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eInspired\u003c/a\u003e, he states that great products emerge when teams operate with clarity, context, and autonomy. UX isn’t just at the table, it’s helping to define the table’s purpose.\u003c/p\u003e\u003ch2 id=\"8a5d\"\u003eEthical \u0026amp; Inclusive by Default\u003c/h2\u003e\u003cp id=\"b87f\"\u003eWith greater influence comes greater responsibility — and yes, I’m paraphrasing Spider-Man’s Uncle Ben. Ethical design is now a core expectation, not just in theory but in practice. Teams are being asked to address everything from algorithmic bias and exclusionary defaults to manipulative patterns and misinformation. Accessibility, privacy, and psychological safety are no longer side considerations — they’re product requirements.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eConcept \u0026amp; illustration by Trina Moore Pervall\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"c49f\"\u003eInclusive design goes beyond compliance. It’s about designing for edge cases and historically underserved users — understanding how race, gender, ability, language, and socioeconomic status intersect with technology. The best teams prioritize co-creation, inviting diverse perspectives into research and testing processes to build more equitable experiences from the ground up.\u003c/p\u003e\u003cp id=\"ef0e\"\u003eAs systems become more automated and autonomous, the ethical stakes are higher. Designers must now think in terms of unintended consequences, system-level harms, and long-term trust. Ethical frameworks are evolving into operational practices: checklists, red-team reviews, bias audits, and accountability rituals.\u003c/p\u003e\u003cp id=\"3752\"\u003eAs highlighted in the \u003ca href=\"https://trends.uxdesign.cc/\" rel=\"noopener\" target=\"_blank\"\u003eUX Trends 2025 report\u003c/a\u003e, ethical and inclusive design isn’t a layer to add, it’s the lens through which the entire product experience must be shaped. It’s the benchmark for quality in today’s experience-driven world.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eImage source: McKinsey \u0026amp; Company — What is an AI agent?\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"bce8\"\u003eThe shift happening now: toolchains, layoffs \u0026amp; the AI reset\u003c/h2\u003e\u003cp id=\"75aa\"\u003eWhile the future often feels theoretical, today’s designers are already navigating dramatic shifts in how work is done.\u003c/p\u003e\u003ch2 id=\"05a3\"\u003eAI agents in the wild\u003c/h2\u003e\u003cp id=\"77c1\"\u003eEarly AI agents are already reshaping workflows. AI agents are a system that can reason, plan, and take actions on its own based on information it’s given to manage workflows, use external tools, and adapt as things change. Tools like \u003ca href=\"https://n8n.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003en8n.io\u003c/a\u003e let teams automate multi-step processes with ease — from generating content and summarizing research to integrating APIs and triggering reminders. These aren’t science fiction sidekicks. They’re real, available, and starting to augment creative work.\u003c/p\u003e\u003cp id=\"86d0\"\u003eLet’s not confuse AI agents with automation. I recently watched a breakdown by \u003ca href=\"https://www.linkedin.com/in/kevin-hutson-a60698a7/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKevin Hutson from Futurepedia\u003c/a\u003e about AI agents and he mentioned, \u003cstrong\u003e\u003cem\u003e“\u003c/em\u003eAutomation is more like predefined fixed steps whereas AI agents are a more dynamic and flexible system capable of reasoning.”\u003c/strong\u003e\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"a379\"\u003e\u003cstrong\u003eIn his video he noted that agents rely on three key components:\u003c/strong\u003e\u003c/p\u003e\u003col\u003e\u003cli id=\"069f\"\u003e\u003cstrong\u003eThe Brain (LLM) \u003c/strong\u003e— A Large Language Model like ChatGPT, Claude, Gemini, etc., handles the reasoning, planning, and language generation.\u003c/li\u003e\u003cli id=\"a92c\"\u003e\u003cstrong\u003eMemory\u003c/strong\u003e — This gives the agent the ability to remember past interactions and use that context to make better decisions. It might remember previous steps in a conversation or pull from stored memory sources like documents or a vector database.\u003c/li\u003e\u003cli id=\"cdf8\"\u003e\u003cstrong\u003eTools\u003c/strong\u003e — How the agent interacts with the world — searching the web or pulling info from a document, taking action by sending emails, updating databases or creating calendar events, and the orchestration of calling other agents, triggering workflow or fusing actions together.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"c9e1\"\u003eDesigners today are experimenting with agents that run user tests, market research, flag accessibility issues, or build prototypes from written prompts. The question is no longer \u003cem\u003eif\u003c/em\u003e agents will assist us, but \u003cem\u003ehow\u003c/em\u003e we design around them.\u003c/p\u003e\u003ch2 id=\"c163\"\u003eThe continued rise of No-Code \u0026amp; Vibe Coding\u003c/h2\u003e\u003cp id=\"477e\"\u003eNo-code platforms like \u003ca href=\"https://webflow.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eWebflow\u003c/a\u003e, \u003ca href=\"https://www.framer.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFramer\u003c/a\u003e, and now \u003ca href=\"https://www.figma.com/make/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFigma Make\u003c/a\u003e are enabling designers to bypass engineers and bring their visions to life faster. Combined with AI-assisted creation, “vibe coding” is emerging — a practice where intuition meets prompt engineering to generate layouts, content, and flows.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eVibe coding toolkit\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"a829\"\u003eVibe coding is more than just a new buzzword — it’s a shift in mindset. Other tools like \u003ca href=\"https://bolt.new/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBolt\u003c/a\u003e and \u003ca href=\"https://lovable.dev/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLovable\u003c/a\u003e, which are more dedicated vibe coding tools, allow creators to work through feeling, tone, and energy. Designers can describe the \u003cem\u003evibe\u003c/em\u003e of an experience — playful, serious, minimal, bold — and use AI tools to manifest that intention instantly in visual form. It’s rapid, intuitive, and deeply aligned with how many creatives actually think.\u003c/p\u003e\u003cp id=\"f6ef\"\u003eThis isn’t just about speed. It’s about shifting power. Designers can now experiment more freely, iterate faster, and own more of the production process. And that freedom changes how we think — focusing us more on expression and good taste, where designers must be able to clearly articulate the language of design in a tasteful way that expresses to a machine how a product needs to feel, flow, and function.\u003c/p\u003e\u003ch2 id=\"17b2\"\u003eThe AI reset \u0026amp; the new team structure\u003c/h2\u003e\u003cp id=\"7056\"\u003eAccording to \u003ca href=\"https://www.forbes.com/sites/jasonsnyder/2025/02/12/ai-reset-layoffs-rto-and-the-new-realities-of-work/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eForbes\u003c/a\u003e, the “AI reset” is prompting mass layoffs across tech — not simply for cost savings, but to recalibrate for a leaner, more AI-native workforce. Organizations are reducing headcount, rethinking team structures, and placing more emphasis on toolchains than headcount.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eImage source: Getty via Forbes article\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"5478\"\u003eDesign teams are being asked to do more with less. This shift demands designers who are not only visually skilled but operationally savvy — comfortable with automation, systems thinking, and rapid prototyping.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"a659\"\u003e\u003cem\u003e“We’re not just designing products anymore. We’re designing the way work itself happens.”\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"201f\"\u003eTomorrow: the near future of UX\u003c/h2\u003e\u003cp id=\"aad3\"\u003e\u003ca href=\"https://blog.google/technology/ai/google-io-2025-all-our-announcements/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGoogle I/O 2025\u003c/a\u003e revealed numerous innovations that serve as early indicators of upcoming UX trends. The new product releases signal a major evolution in human interaction methods with AI systems which work across different platforms and devices. Designers must understand these signals and use them to develop experiences that are usable and inclusive to all humanity.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"9895\"\u003eAI-powered personalization\u003c/h2\u003e\u003cp id=\"180a\"\u003ePicture user interfaces that understand your behaviors so well that they can predict what you’ll do next. The merging of AI into everyday tools enables unprecedented levels of personalization. For instance, Google’s Gemini AI assistant delivers \u003ca href=\"https://blog.google/products/workspace/google-workspace-gemini-may-2025-updates/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e“Personalized Smart Replies”\u003c/a\u003e within Gmail that replicate users’ writing styles to improve communication efficiency. Customization at this level improves user engagement by producing interactions that appear instinctive and straightforward.\u003c/p\u003e\u003cp id=\"6040\"\u003eFrom a UX standpoint, this raises deeper questions: At what point does personalization become manipulation? How can we build user experiences that evolve based on individual needs while ensuring they maintain both transparency and user control?\u003c/p\u003e\u003ch2 id=\"f644\"\u003eVoice, gesture \u0026amp; multimodal UX\u003c/h2\u003e\u003cp id=\"f07f\"\u003eThe traditional screen-based interface is evolving. Devices now possess enhanced functionality to process and react to voice commands along with gestures and visual signals. The launch of\u003ca href=\"https://blog.google/products/gemini/gemini-live-extensions-language-expansion/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e “Gemini Live” \u003c/a\u003edemonstrates this shift. Through this feature users can immediately engage their surroundings by using their camera to have back-and-forth conversations with AI about visible objects.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eImage Source: Google\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"0a97\"\u003eThis ushers in new UX challenges: We must create conversational and gestural feedback loops that match the clarity and assurance provided by a click. What strategies can we implement to guarantee accessibility for every form of user interaction?\u003c/p\u003e\u003ch2 id=\"6abf\"\u003eSystems over screens\u003c/h2\u003e\u003cp id=\"078a\"\u003eSeamless integration across platforms and devices holds the key to the future of UX. Through its \u003ca href=\"https://blog.google/products/search/ai-mode-search/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e“AI Mode”\u003c/a\u003e search function, Google replaces conventional search result formats with interactive chatbot experiences that enable users to gather information more intuitively and faster.\u003c/p\u003e\u003cp id=\"7d90\"\u003eGoogle launched\u003ca href=\"https://developers.googleblog.com/en/stitch-a-new-way-to-design-uis/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e “Stitch”\u003c/a\u003e, their vibe coding tool that allows application development through natural language inputs and image prompts to connect design and development processes. I’m curious about how engineering teams will be affected by these developments.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eMain fuctions of Stitch\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"30a2\"\u003eThe development of these tools shows a continuing shift in the industry toward AI-enhanced design methods that distribute UX design beyond individual UIs to multiple surfaces and contextual moments. With AI serving as the foundational layer for daily interactions, designers now need to focus on orchestrating systems rather than building them as they create invisible architectures which deliver tangible results.\u003c/p\u003e\u003cp id=\"78e4\"\u003eGoogle’s new advancements reveal how the future will unfold. These new product announcements suggest that future UX design will extend beyond traditional clicks to create immersive experiences throughout users’ environments.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"f220\"\u003eBeyond: UX in the next 5–10 years\u003c/h2\u003e\u003cp id=\"f659\"\u003eDesigners who thrive in the next decade will be those who lean into ambiguity, shape emerging tech with empathy, and hold the line on what makes technology human.\u003c/p\u003e\u003ch2 id=\"f071\"\u003eAI agents as experience partners\u003c/h2\u003e\u003cp id=\"be6b\"\u003eThe rise of intelligent agents will mean users don’t just interact with interfaces — they partner with them. From personal schedulers to shopping assistants and creative collaborators, \u003ca rel=\"noopener\" href=\"https://uxdesign.cc/the-agentic-era-of-ux-4b58634e410b\" target=\"_blank\" data-discover=\"true\"\u003eagents will carry out tasks with increasing autonomy\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"779f\"\u003eIn the next 5–10 years, we can expect a shift from isolated, task-specific agents to fully integrated, multi-agent ecosystems. These systems will allow AI agents to collaborate with one another — negotiating schedules, delegating tasks, and even learning from shared user behavior across platforms. Imagine a project management AI coordinating with your personal calendar agent, your team’s research agent, and a marketing analytics agent — all working together to keep you focused and productive. Wow! My brain hurts just thinking about this.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eImage source: n8n.io | n8n research agent workflow\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"0386\"\u003eMass adoption of AI agents will likely become a workforce norm. According to \u003ca href=\"https://www.salesforce.com/news/stories/agentic-ai-impact-on-workforce-research/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSalesforce\u003c/a\u003e, AI agent usage is expected to rise dramatically by 327%, with projected productivity gains of up to 30%. For professionals, this won’t just be a competitive advantage — it will be a basic expectation. Much like we’re expected to know how to use email or spreadsheets today, fluency in managing and co-working with AI agents will become standard.\u003c/p\u003e\u003cp id=\"909b\"\u003eFor UX designers, this means creating not just user flows — but agent frameworks. We’ll need to shape how agents communicate with each other, how they surface decisions to users, and how trust, context, and boundaries are preserved across every interaction.\u003c/p\u003e\u003cp id=\"293a\"\u003eThis challenges us to design experiences that are relational, not transactional. How do we build trust with a digital entity? What does a good “agent personality” look like? How do we give users control without overwhelming them?\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"5c62\"\u003e\u003cem\u003eAs economist \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/richard-baldwin-imd?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAAqazYkBiOUuklMvsa_nrIHQwmkXanU2ii0\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eRichard Baldwin\u003c/em\u003e\u003c/a\u003e\u003cem\u003e said, “AI won’t take your job — but someone who knows how to use AI will.” This is where designers must thrive: crafting agents that build, collaborate, and accelerate workflows. The human advantage is you.\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003ch2 id=\"8634\"\u003eThe non-interface future\u003c/h2\u003e\u003cp id=\"cbde\"\u003eScreens may disappear, but design will persist. As highlighted in \u003ca href=\"https://www.linkedin.com/pulse/future-uiux-how-ai-accessibility-shape-next-decade-asghar-8epkf/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ethis LinkedIn piece by\u003c/a\u003e \u003ca href=\"https://www.linkedin.com/in/m-zeeshan-asghar-designer?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAC-RrLsBq6YvA8MstWrrAIv--M9_cbCfHbM\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMuhammad Zeeshan Asghar\u003c/a\u003e , the next decade of UX may be shaped more by how we feel and perceive than by what we tap and scroll. From neural interfaces to ambient computing, interactions will increasingly be mediated through space, sensation, and subtle context-aware cues.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eImage source: Toptal | multimodal reality\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"55ce\"\u003eThis transition challenges designers to think beyond visible surfaces. We’ll need to become choreographers of the invisible — designing feedback loops that feel intuitive without relying on screens, buttons, or clicks. The future UX layer may live in our environments, wearables, or neural signals — but it will still require intention, clarity, and a human touch.\u003c/p\u003e\u003ch2 id=\"8929\"\u003eDesigners as ethical orchestrators\u003c/h2\u003e\u003cp id=\"7574\"\u003eAs automation takes on the labor, the designer’s role will become increasingly strategic. We’ll be called to shape not just the user journey, but the moral architecture behind it. Our work will determine what AI does, who it serves, and where the boundaries are drawn.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eMade with ChatGPT by author\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"137c\"\u003eAs outlined in \u003ca href=\"https://medium.com/design-bootcamp/navigating-the-ethical-landscape-of-ux-design-in-the-age-of-ai-25c12ad3ed6d\" rel=\"noopener\"\u003ethis article by Design Bootcamp\u003c/a\u003e, ethical UX design in the AI era isn’t just about preventing harm — it’s about anticipating unintended consequences, designing for edge cases, and embedding human values into algorithms that scale. It’s no longer enough to ask, “can we build this?” The more important question is: “should we?”\u003c/p\u003e\u003cp id=\"8be7\"\u003eThis requires deeper collaboration with data scientists, ethicists, and product leaders. It demands transparency in how AI decisions are made and fairness in who they impact. As AI grows more autonomous, designers must champion user trust, agency, and dignity in every experience.\u003c/p\u003e\u003cp id=\"5599\"\u003eEthical design won’t be a slide in the deck — it will be the foundation.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"854a\"\u003e\u003cem\u003eThe future won’t be led by those who simply adapt to AI, but by those who guide it — with intention, responsibility, and humanity.\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003ch2 id=\"9255\"\u003eOpenAI’s acquisition of io: a paradigm shift in UX\u003c/h2\u003e\u003cp id=\"6d50\"\u003eIn a landmark move, \u003ca href=\"https://openai.com/sam-and-jony/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOpenAI has acquired io\u003c/a\u003e, the AI hardware startup founded by former Apple design chief Jony Ive for $6.5 billion. This acquisition signifies OpenAI’s strategic entry into the consumer hardware market, aiming to develop AI-native devices that transcend traditional screens and interfaces.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eImage source: OpenAI—Sam \u0026amp; Jony introduce io\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"253f\"\u003eThe collaboration between OpenAI and Ive’s design firm, LoveFrom, is set to redefine user interactions with technology. By integrating advanced AI capabilities with iconic industrial design, the partnership aspires to create devices that are contextually aware, ambient, and seamlessly integrated into users’ lives.\u003c/p\u003e\u003cp id=\"2146\"\u003eThis move underscores a shift toward experiences that are not just user-friendly but profoundly user-centric — emphasizing empathy, ethical design, and utility that feels almost invisible. As \u003ca href=\"https://nypost.com/2025/05/21/business/ex-apple-exec-jony-ive-joins-openai-in-6-5-billion-deal-for-ai-devices-startup/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ereported by the New York Post\u003c/a\u003e, the deal hints at an entirely new category of interaction design that is screen-optional, multimodal, and shaped by intent rather than input.\u003c/p\u003e\u003cp id=\"3e78\"\u003eFor UX professionals, this development signals a broader transformation: we are not just designing software; we are designing intelligent, embodied experiences. This calls for a hybrid skill set that blends behavioral science, interaction design, and systems thinking in ways never before required.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"607a\"\u003eUX is entering a new era — not defined by tools, but by \u003cem\u003etensions\u003c/em\u003e. Between speed and depth. Automation and intention. Power and empathy. The next frontier of design isn’t about control. It’s about conversation. Between human and machine, intent and outcome, possibility and principle.\u003c/p\u003e\u003cp id=\"f579\"\u003eMy advice to designers: Learn something new every day and stay adaptable in this ever-changing landscape of technology and experience design. Be an early adopter. Embrace experimentation. And work toward becoming a Super IC, a title coined by \u003ca href=\"https://www.linkedin.com/in/garronengstrom/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGarron Engstrom\u003c/a\u003e, Director of Product Design at Meta — someone who bridges business goals, leadership, emerging tech, product and human-centered thinking in the age of agentic design.\u003c/p\u003e\u003cp id=\"cc30\"\u003eThe question for all of us isn’t just “what can we design?” but rather:\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"f081\"\u003e\u003cem\u003e“What world are we designing toward?”\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "20 min read",
  "publishedTime": "2025-06-03T11:18:58.592Z",
  "modifiedTime": null
}
