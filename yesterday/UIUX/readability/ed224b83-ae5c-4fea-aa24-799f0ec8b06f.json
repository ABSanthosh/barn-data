{
  "id": "ed224b83-ae5c-4fea-aa24-799f0ec8b06f",
  "title": "6 Dimensions for Assessing Usability Data in Analysis",
  "link": "https://www.nngroup.com/articles/usability-data-in-analysis/?utm_source=rss\u0026utm_medium=feed\u0026utm_campaign=rss-syndication",
  "description": "Analyze usability findings for authenticity, consistency, repetition, spontaneity, appropriateness, and confounding factors to separate surface impressions from real insights.",
  "author": "Maria Rosala, Feifei Liu",
  "published": "Fri, 11 Apr 2025 17:00:00 +0000",
  "source": "https://www.nngroup.com/feed/rss/",
  "categories": [
    "Article"
  ],
  "byline": "Feifei Liu, Maria Rosala",
  "length": 6839,
  "excerpt": "Analyze usability findings for authenticity, consistency, repetition, spontaneity, appropriateness, and confounding factors to separate surface impressions from real insights.",
  "siteName": "Nielsen Norman Group",
  "favicon": "",
  "text": "Summary:  Analyze usability findings for authenticity, consistency, repetition, spontaneity, appropriateness, and confounding factors to separate surface impressions from real insights. Qualitative usability tests yield two types of data: behavioral data (or performance data) and attitudinal data (or subjective data). During analysis, we must consolidate both types of data while considering additional factors, such as information about the study design or recruitment. Why You Can’t Take a Data Point at Face Value 6 Dimensions for Assessing Qualitative-Date Relevancy Why You Can’t Take a Data Point at Face Value Consider the following example. You’re moderating a usability test of a prototype, aiming to evaluate the usefulness of a comparison feature. After completing a task on the prototype, 4 out of 5 users said that they liked the feature and would use it in the future. From this account you’d be forgiven for thinking that the feature tested well and no changes to the design are required. But, what about if: Only one participant actually used the feature — and they struggled with it. The positive feedback was provided only after the facilitator asked whether the participants liked the comparison feature. The comparison feature wasn’t mentioned during the end-of-test followup questions on what was easy or difficult. When asked to find a suitable product, all participants quickly chose one and didn’t explore other available options. Would this change your assessment of the utility of the comparison feature? Probably! Sometimes, what users say contradicts what they do. And, sometimes, what users say at one point in a session contradicts what they say later in the session! This is why, when analyzing usability test data, each data point must be assessed in the context of other data points and paired with information about the recruitment strategy, study design, and facilitation events. This is partly why you can’t trust AI to analyze usability tests. Currently, AI tools cannot process or “watch” the recordings, so they miss context. They also aren’t intelligent enough to know you asked a leading question or that a participant was not an appropriate recruit. 6 Dimensions for Assessing Qualitative-Date Relevancy Each data point we consider in the qualitative-data analysis should be examined on 6 key dimensions or lenses to assess its accuracy and relevance. Authenticity Consistency Repetition Spontaneity Appropriateness Confounds 6 dimensions can be used to evaluate data points collected in a qualitative usability test study: authenticity, consistency, repetition, spontaneity, appropriateness, and confounds. We’ve also created the following mnemonic to help you remember them. All Curious Researchers Stop and Analyze Carefully. Authenticity This dimension refers to how natural the comment or behavior was. When a participant commented that they liked something, did they mean it? While we can’t read minds, we do make judgments about things our participants say by paying attention to how something is said or done. Was the participant trying to please the session facilitator? Did the participant feel compelled to comment on a feature or design? There are various reasons why a comment or behavior may not be authentic. The participant might have been influenced by how we framed the research, may have been a misrecruit or professional participant, or may have altered their behavior because they were conscious of being “watched.” Consistency This dimension refers to how much a data point (whether verbal or behavioral) is consistent with others. For example: Did a participant say something that contradicts another piece of feedback they gave at a different point in time? Did a participant’s comment align with their behavior? For example, a participant might say that a task was easy, but their behavior tells a different story — they struggled, made errors, or restarted multiple times. This mismatch can happen when participants want to be polite to the researcher or are reluctant to admit difficulty. When inconsistencies between behavioral and verbal data occur in usability testing, pay more attention to what people do than to what they say. Repetition This dimension considers how often a comment or behavior occurs across a session or participants. Repeated behaviors can reveal underlying patterns, tendencies, or mental models. Repeated comments often reflect strong, genuine emotions or attitudes. For example: Repetitive comments: If a participant mentions liking a feature multiple times, we may be more confident that the sentiment is authentic. Repeated errors: When several participants make the same error, or we see the same error repeated in the same session, it's a strong indicator of a real usability issue. Spontaneity This dimension refers to whether the participant’s comment or behavior were cued in some way by the facilitator. When actions or comments are introduced spontaneously, without any priming, we can be more confident that the feedback or behavior is genuine. Watch out for priming, as it can skew participants’ spontaneous behavior. For example: Revealing the study purpose too early, such as during the session introduction or directly in the usability-testing tasks Mentioning UI elements or features in questions or conversations Appropriateness This dimension relates to whether the participant and task were well-suited to the research goals. Key questions to consider include: Was the participant representative of the target user? Did the participant sample adequately reflect the characteristics of the target user population? Was the task realistic and fair, or did it ask participants to do something they wouldn’t typically do? Confounds This final dimension looks at aspects of the study design that may unintendedly influence participant behavior and skew results. Common issues include: Order effects: Did completing one task impact how participants approached subsequent tasks? For example, if the test included many repetitive tasks, participants might have become bored or fatigued towards the end, which could affect performance. Complex task instructions: When task wording isn’t clear — too long, overly technical, or not in plain language — participants may struggle to complete the activity, even if the design itself works well. Conclusion In usability testing, no single data point tells the whole story. Each data point must be examined critically and contextually: what users say, what they do, and how the study was run all shape the insights we draw. Apply the 6 dimensions — authenticity, consistency, repetition, spontaneity, appropriateness, and confounds — to avoid being misled by surface-level signals and to draw insights you can trust.",
  "image": "https://media.nngroup.com/media/articles/opengraph_images/Social-Card-6-Dimensions-Assessing-Usability-1.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp\u003e\u003cspan\u003e\n                  Summary: \n                \u003c/span\u003eAnalyze usability findings for authenticity, consistency, repetition, spontaneity, appropriateness, and confounding factors to separate surface impressions from real insights.\n              \u003c/p\u003e\u003cdiv\u003e\n              \u003cp\u003eQualitative usability tests yield two types of data: \u003ca href=\"https://www.nngroup.com/articles/satisfaction-vs-performance-metrics/\"\u003ebehavioral data\u003c/a\u003e (or performance data) and \u003ca href=\"https://www.nngroup.com/articles/attitudinal-behavioral/\"\u003eattitudinal data\u003c/a\u003e (or subjective data). During analysis, we must consolidate both types of data while considering additional factors, such as information about the study design or recruitment.\u003c/p\u003e\n\u003cdiv\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-why-you-cant-take-a-data-point-at-face-value-1\"\u003eWhy You Can’t Take a Data Point at Face Value\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-6-dimensions-for-assessing-qualitative-date-relevancy-2\"\u003e6 Dimensions for Assessing Qualitative-Date Relevancy\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\u003ch2 id=\"toc-why-you-cant-take-a-data-point-at-face-value-1\"\u003eWhy You Can’t Take a Data Point at Face Value\u003c/h2\u003e\n\u003cp\u003eConsider the following example.\u003c/p\u003e\n\u003cp\u003eYou’re moderating a usability test of a prototype, aiming to evaluate the usefulness of a comparison feature. After completing a task on the prototype, 4 out of 5 users said that they liked the feature and would use it in the future.\u003c/p\u003e\n\u003cp\u003eFrom this account you’d be forgiven for thinking that the feature tested well and no changes to the design are required. But, what about if:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOnly one participant actually used the feature — and they struggled with it.\u003c/li\u003e\n\u003cli\u003eThe positive feedback was provided only after the facilitator asked whether the participants liked the comparison feature.\u003c/li\u003e\n\u003cli\u003eThe comparison feature wasn’t mentioned during the end-of-test followup questions on what was easy or difficult.\u003c/li\u003e\n\u003cli\u003eWhen asked to find a suitable product, all participants quickly chose one and didn’t explore other available options.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWould this change your assessment of the utility of the comparison feature? Probably!\u003c/p\u003e\n\u003cp\u003eSometimes, \u003ca href=\"https://www.nngroup.com/articles/first-rule-of-usability-dont-listen-to-users/\"\u003ewhat users say contradicts what they do\u003c/a\u003e. And, sometimes, what users say at one point in a session contradicts what they say later in the session!\u003c/p\u003e\n\u003cp\u003eThis is why, when analyzing usability test data, \u003cstrong\u003eeach data point must be assessed in the context of other data points\u003c/strong\u003e and paired with information about the recruitment strategy, study design, and facilitation events.\u003c/p\u003e\n\u003cp\u003eThis is partly why \u003cstrong\u003eyou can’t trust AI to analyze usability tests\u003c/strong\u003e.\u003cstrong\u003e \u003c/strong\u003eCurrently, AI tools cannot process or “watch” the recordings, so they miss context. They also aren’t intelligent enough to know you asked a \u003ca href=\"https://www.nngroup.com/articles/leading-questions/\"\u003eleading question\u003c/a\u003e or that a participant was not an appropriate recruit.\u003c/p\u003e\n\u003ch2 id=\"toc-6-dimensions-for-assessing-qualitative-date-relevancy-2\"\u003e6 Dimensions for Assessing Qualitative-Date Relevancy\u003c/h2\u003e\n\u003cp\u003eEach data point we consider in the qualitative-data analysis should be examined on 6 key dimensions or lenses to assess its accuracy and relevance.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAuthenticity\u003c/li\u003e\n\u003cli\u003eConsistency\u003c/li\u003e\n\u003cli\u003eRepetition\u003c/li\u003e\n\u003cli\u003eSpontaneity\u003c/li\u003e\n\u003cli\u003eAppropriateness\u003c/li\u003e\n\u003cli\u003eConfounds\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure\u003e\u003cimg alt=\"Diagram listing six dimensions for assessing usability test data: confounds, appropriateness, spontaneity, authenticity, consistency, and repetition.\" height=\"2700\" loading=\"lazy\" src=\"https://media.nngroup.com/media/editor/2025/04/11/6-dimensions-graphic-08-1.png\" width=\"3600\"/\u003e\n\u003cfigcaption\u003e\u003cem\u003e6 dimensions can be used to evaluate data points collected in a qualitative usability test study: authenticity, consistency, repetition, spontaneity, appropriateness, and confounds.\u003c/em\u003e\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eWe’ve also created the following mnemonic to help you remember them.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eA\u003c/em\u003e\u003c/strong\u003e\u003cem\u003ell \u003cstrong\u003eC\u003c/strong\u003eurious \u003cstrong\u003eR\u003c/strong\u003eesearchers \u003cstrong\u003eS\u003c/strong\u003etop and \u003cstrong\u003eA\u003c/strong\u003enalyze \u003cstrong\u003eC\u003c/strong\u003earefully.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003eAuthenticity\u003c/h3\u003e\n\u003cp\u003eThis dimension refers to \u003cstrong\u003ehow natural the comment or behavior was\u003c/strong\u003e. When a participant commented that they liked something, did they mean it? While we can’t read minds, we do make judgments about things our participants say by paying attention to how something is said or done.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWas the participant trying to please the session facilitator?\u003c/li\u003e\n\u003cli\u003eDid the participant feel compelled to comment on a feature or design?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere are various reasons why a comment or behavior may not be authentic. The participant might have been influenced by how we framed the research, may have been a misrecruit or \u003ca href=\"https://www.nngroup.com/articles/problem-participants-remote-unmoderated/\"\u003eprofessional participant\u003c/a\u003e, or may have altered their behavior because they were \u003ca href=\"https://www.nngroup.com/articles/hawthorne-effect-observer-bias-user-research/\"\u003econscious of being “watched.”\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003eConsistency\u003c/h3\u003e\n\u003cp\u003eThis dimension refers to \u003cstrong\u003ehow much a data point (whether verbal or behavioral) is consistent with others\u003c/strong\u003e. For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDid a participant say something that contradicts another piece of feedback they gave at a different point in time?\u003c/li\u003e\n\u003cli\u003eDid a participant’s comment align with their behavior?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example, a participant might say that a task was easy, but their behavior tells a different story — they struggled, made errors, or restarted multiple times. This mismatch can happen when participants want to be polite to the researcher or are reluctant to admit difficulty. When inconsistencies between behavioral and verbal data occur in usability testing, pay more attention to \u003ca href=\"https://www.nngroup.com/videos/we-like-to-watch/\"\u003ewhat people do\u003c/a\u003e than to what they say.\u003c/p\u003e\n\u003ch3\u003eRepetition\u003c/h3\u003e\n\u003cp\u003eThis dimension considers \u003cstrong\u003ehow often a comment or behavior occurs\u003c/strong\u003e across a session or participants. Repeated behaviors can reveal underlying patterns, tendencies, or \u003ca href=\"https://www.nngroup.com/articles/mental-models/\"\u003emental models\u003c/a\u003e. Repeated comments often reflect strong, genuine emotions or attitudes.\u003c/p\u003e\n\u003cp\u003eFor example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRepetitive comments\u003c/strong\u003e: If a participant mentions liking a feature multiple times, we may be more confident that the sentiment is authentic.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRepeated errors\u003c/strong\u003e: When several participants make the same error, or we see the same error repeated in the same session, it\u0026#39;s a strong indicator of a real usability issue.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eSpontaneity\u003c/h3\u003e\n\u003cp\u003eThis dimension refers to whether the participant’s comment or behavior were cued in some way by the facilitator. When actions or comments are introduced spontaneously, without any \u003ca href=\"https://www.nngroup.com/articles/priming/\"\u003epriming\u003c/a\u003e, we can be more confident that the feedback or behavior is genuine.\u003c/p\u003e\n\u003cp\u003eWatch out for priming, as it can skew participants’ spontaneous behavior. For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRevealing the study purpose too early\u003c/strong\u003e, such as during the session introduction or directly in the usability-testing tasks\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMentioning UI elements or features\u003c/strong\u003e in questions or conversations\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAppropriateness\u003c/h3\u003e\n\u003cp\u003eThis dimension relates to whether the participant and task were well-suited to the research goals. Key questions to consider include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWas the participant representative of the target user?\u003c/li\u003e\n\u003cli\u003eDid the participant sample adequately reflect the characteristics of the target user population?\u003c/li\u003e\n\u003cli\u003eWas the task realistic and fair, or did it ask participants to do something they wouldn’t typically do?\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eConfounds\u003c/h3\u003e\n\u003cp\u003eThis final dimension looks at aspects of the study design that may unintendedly influence participant behavior and skew results. Common issues include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eOrder effects\u003c/strong\u003e: Did completing one task impact how participants approached subsequent tasks? For example, if the test included many repetitive tasks, participants might have become bored or fatigued towards the end, which could affect performance.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eComplex task instructions\u003c/strong\u003e: When task wording isn’t clear — too long, overly technical, or not in plain language — participants may struggle to complete the activity, even if the design itself works well.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eIn usability testing, no single data point tells the whole story. Each data point must be examined critically and contextually: what users say, what they do, and how the study was run all shape the insights we draw. Apply the 6 dimensions — authenticity, consistency, repetition, spontaneity, appropriateness, and confounds — to avoid being misled by surface-level signals and to draw insights you can trust.\u003c/p\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-04-11T17:00:00Z",
  "modifiedTime": null
}
