{
  "id": "07ec6e52-fa5a-4d8b-9357-8f3c53c7f4fd",
  "title": "NotebookLlama: An open source version of NotebookLM",
  "link": "https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama",
  "description": "Article URL: https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama Comments URL: https://news.ycombinator.com/item?id=41964980 Points: 76 # Comments: 21",
  "author": "bibinmohan",
  "published": "Sun, 27 Oct 2024 19:31:02 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "",
  "length": 5920,
  "excerpt": "Scripts for fine-tuning Meta Llama with composable FSDP \u0026 PEFT methods to cover single/multi-node GPUs. Supports default \u0026 custom datasets for applications such as summarization and Q\u0026A...",
  "siteName": "GitHub",
  "favicon": "https://github.githubassets.com/assets/apple-touch-icon-180x180-a80b8e11abe2.png",
  "text": "NotebookLlama: An Open Source version of NotebookLM Listen to audio from the example here This is a guided series of tutorials/notebooks that can be taken as a reference or course to build a PDF to Podcast workflow. You will also learn from the experiments of using Text to Speech Models. It assumes zero knowledge of LLMs, prompting and audio models, everything is covered in their respective notebooks. Outline: Here is step by step thought (pun intended) for the task: Step 1: Pre-process PDF: Use Llama-3.2-1B-Instruct to pre-process the PDF and save it in a .txt file. Step 2: Transcript Writer: Use Llama-3.1-70B-Instruct model to write a podcast transcript from the text Step 3: Dramatic Re-Writer: Use Llama-3.1-8B-Instruct model to make the transcript more dramatic Step 4: Text-To-Speech Workflow: Use parler-tts/parler-tts-mini-v1 and bark/suno to generate a conversational podcast Note 1: In Step 1, we prompt the 1B model to not modify the text or summarize it, strictly clean up extra characters or garbage characters that might get picked due to encoding from PDF. Please see the prompt in Notebook 1 for more details. Note 2: For Step 2, you can also use Llama-3.1-8B-Instruct model, we recommend experimenting and trying if you see any differences. The 70B model was used here because it gave slightly more creative podcast transcripts for the tested examples. Note 3: For Step 4, please try to extend the approach with other models. These models were chosen based on a sample prompt and worked best, newer models might sound better. Please see Notes for some of the sample tests. Detailed steps on running the notebook: Requirements: GPU server or an API provider for using 70B, 8B and 1B Llama models. For running the 70B model, you will need a GPU with aggregated memory around 140GB to infer in bfloat-16 precision. Note: For our GPU Poor friends, you can also use the 8B and lower models for the entire pipeline. There is no strong recommendation. The pipeline below is what worked best on first few tests. You should try and see what works best for you! Before getting started, please make sure to login using the huggingface cli and then launch your jupyter notebook server to make sure you are able to download the Llama models. You'll need your Hugging Face access token, which you can get at your Settings page here. Then run huggingface-cli login and copy and paste your Hugging Face access token to complete the login to make sure the scripts can download Hugging Face models if needed. First, please Install the requirements from here by running inside the folder: git clone https://github.com/meta-llama/llama-recipes cd llama-recipes/recipes/quickstart/NotebookLlama/ pip install -r requirements.txt Notebook 1: This notebook is used for processing the PDF and processing it using the new Feather light model into a .txt file. Update the first cell with a PDF link that you would like to use. Please decide on a PDF to use for Notebook 1, it can be any link but please remember to update the first cell of the notebook with the right link. Please try changing the prompts for the Llama-3.2-1B-Instruct model and see if you can improve results. Notebook 2: This notebook will take in the processed output from Notebook 1 and creatively convert it into a podcast transcript using the Llama-3.1-70B-Instruct model. If you are GPU rich, please feel free to test with the 405B model! Please try experimenting with the System prompts for the model and see if you can improve the results and try the 8B model as well here to see if there is a huge difference! Notebook 3: This notebook takes the transcript from earlier and prompts Llama-3.1-8B-Instruct to add more dramatization and interruptions in the conversations. There is also a key factor here: we return a tuple of conversation which makes our lives easier later. Yes, studying Data Structures 101 was actually useful for once! For our TTS logic, we use two different models that behave differently with certain prompts. So we prompt the model to add specifics for each speaker accordingly. Please again try changing the system prompt and see if you can improve the results. We encourage testing the feather light 3B and 1B models as well at this stage Notebook 4: Finally, we take the results from last notebook and convert them into a podcast. We use the parler-tts/parler-tts-mini-v1 and bark/suno models for a conversation. The speakers and the prompt for parler model were decided based on experimentation and suggestions from the model authors. Please try experimenting, you can find more details in the resources section. Note: Right now there is one issue: Parler needs transformers 4.43.3 or earlier and for steps 1 to 3 of the pipeline you need latest, so we just switch versions in the last notebook. Next-Improvements/Further ideas: Speech Model experimentation: The TTS model is the limitation of how natural this will sound. This probably be improved with a better pipeline and with the help of someone more knowledgable-PRs are welcome! :) LLM vs LLM Debate: Another approach of writing the podcast would be having two agents debate the topic of interest and write the podcast outline. Right now we use a single LLM (70B) to write the podcast outline Testing 405B for writing the transcripts Better prompting Support for ingesting a website, audio file, YouTube links and more. Again, we welcome community PRs! Resources for further learning: https://betterprogramming.pub/text-to-audio-generation-with-bark-clearly-explained-4ee300a3713a https://colab.research.google.com/drive/1dWWkZzvu7L9Bunq9zvD-W02RFUXoW-Pd?usp=sharing https://colab.research.google.com/drive/1eJfA2XUa-mXwdMy7DoYKVYHI1iTd9Vkt?usp=sharing#scrollTo=NyYQ--3YksJY https://replicate.com/suno-ai/bark?prediction=zh8j6yddxxrge0cjp9asgzd534 https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c",
  "image": "https://opengraph.githubassets.com/6c5cfe95e259d1adea82255aa38bb4973230616cf553822649a5440f7fbb5559/meta-llama/llama-recipes",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-hpc=\"true\"\u003e\u003carticle itemprop=\"text\"\u003e\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eNotebookLlama: An Open Source version of NotebookLM\u003c/h2\u003e\u003ca id=\"user-content-notebookllama-an-open-source-version-of-notebooklm\" aria-label=\"Permalink: NotebookLlama: An Open Source version of NotebookLM\" href=\"#notebookllama-an-open-source-version-of-notebooklm\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/NotebookLlama/resources/Outline.jpg\"\u003e\u003cimg src=\"https://github.com/meta-llama/llama-recipes/raw/main/recipes/quickstart/NotebookLlama/resources/Outline.jpg\" alt=\"NotebookLlama\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/NotebookLlama/resources/_podcast.mp3\"\u003eListen to audio from the example here\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThis is a guided series of tutorials/notebooks that can be taken as a reference or course to build a PDF to Podcast workflow.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eYou will also learn from the experiments of using  Text to Speech Models.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eIt assumes zero knowledge of LLMs, prompting and audio models, everything is covered in their respective notebooks.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eOutline:\u003c/h3\u003e\u003ca id=\"user-content-outline\" aria-label=\"Permalink: Outline:\" href=\"#outline\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eHere is step by step thought (pun intended) for the task:\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eStep 1: Pre-process PDF: Use \u003ccode\u003eLlama-3.2-1B-Instruct\u003c/code\u003e to pre-process the PDF and save it in a \u003ccode\u003e.txt\u003c/code\u003e file.\u003c/li\u003e\n\u003cli\u003eStep 2: Transcript Writer: Use \u003ccode\u003eLlama-3.1-70B-Instruct\u003c/code\u003e model to write a podcast transcript from the text\u003c/li\u003e\n\u003cli\u003eStep 3: Dramatic Re-Writer: Use \u003ccode\u003eLlama-3.1-8B-Instruct\u003c/code\u003e model to make the transcript more dramatic\u003c/li\u003e\n\u003cli\u003eStep 4: Text-To-Speech Workflow: Use \u003ccode\u003eparler-tts/parler-tts-mini-v1\u003c/code\u003e and \u003ccode\u003ebark/suno\u003c/code\u003e to generate a conversational podcast\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003eNote 1: In Step 1, we prompt the 1B model to not modify the text or summarize it, strictly clean up extra characters or garbage characters that might get picked due to encoding from PDF. Please see the prompt in Notebook 1 for more details.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eNote 2: For Step 2, you can also use \u003ccode\u003eLlama-3.1-8B-Instruct\u003c/code\u003e model, we recommend experimenting and trying if you see any differences. The 70B model was used here because it gave slightly more creative podcast transcripts for the tested examples.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eNote 3: For Step 4, please try to extend the approach with other models. These models were chosen based on a sample prompt and worked best, newer models might sound better. Please see \u003ca href=\"https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/NotebookLlama/TTS_Notes.md\"\u003eNotes\u003c/a\u003e for some of the sample tests.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eDetailed steps on running the notebook:\u003c/h3\u003e\u003ca id=\"user-content-detailed-steps-on-running-the-notebook\" aria-label=\"Permalink: Detailed steps on running the notebook:\" href=\"#detailed-steps-on-running-the-notebook\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eRequirements: GPU server or an API provider for using 70B, 8B and 1B Llama models.\nFor running the 70B model, you will need a GPU with aggregated memory around 140GB to infer in bfloat-16 precision.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eNote: For our GPU Poor friends, you can also use the 8B and lower models for the entire pipeline. There is no strong recommendation. The pipeline below is what worked best on first few tests. You should try and see what works best for you!\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eBefore getting started, please make sure to login using the \u003ccode\u003ehuggingface cli\u003c/code\u003e and then launch your jupyter notebook server to make sure you are able to download the Llama models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003eYou\u0026#39;ll need your Hugging Face access token, which you can get at your Settings page \u003ca href=\"https://huggingface.co/settings/tokens\" rel=\"nofollow\"\u003ehere\u003c/a\u003e. Then run \u003ccode\u003ehuggingface-cli login\u003c/code\u003e and copy and paste your Hugging Face access token to complete the login to make sure the scripts can download Hugging Face models if needed.\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eFirst, please Install the requirements from \u003ca href=\"https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/NotebookLlama\"\u003ehere\u003c/a\u003e by running inside the folder:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv data-snippet-clipboard-copy-content=\"git clone https://github.com/meta-llama/llama-recipes\ncd llama-recipes/recipes/quickstart/NotebookLlama/\npip install -r requirements.txt\"\u003e\u003cpre\u003e\u003ccode\u003egit clone https://github.com/meta-llama/llama-recipes\ncd llama-recipes/recipes/quickstart/NotebookLlama/\npip install -r requirements.txt\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eNotebook 1:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003eThis notebook is used for processing the PDF and processing it using the new Feather light model into a \u003ccode\u003e.txt\u003c/code\u003e file.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eUpdate the first cell with a PDF link that you would like to use. Please decide on a PDF to use for Notebook 1, it can be any link but please remember to update the first cell of the notebook with the right link.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003ePlease try changing the prompts for the \u003ccode\u003eLlama-3.2-1B-Instruct\u003c/code\u003e model and see if you can improve results.\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eNotebook 2:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003eThis notebook will take in the processed output from Notebook 1 and creatively convert it into a podcast transcript using the \u003ccode\u003eLlama-3.1-70B-Instruct\u003c/code\u003e model. If you are GPU rich, please feel free to test with the 405B model!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003ePlease try experimenting with the System prompts for the model and see if you can improve the results and try the 8B model as well here to see if there is a huge difference!\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eNotebook 3:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003eThis notebook takes the transcript from earlier and prompts \u003ccode\u003eLlama-3.1-8B-Instruct\u003c/code\u003e to add more dramatization and interruptions in the conversations.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThere is also a key factor here: we return a tuple of conversation which makes our lives easier later. Yes, studying Data Structures 101 was actually useful for once!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eFor our TTS logic, we use two different models that behave differently with certain prompts. So we prompt the model to add specifics for each speaker accordingly.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003ePlease again try changing the system prompt and see if you can improve the results. We encourage testing the feather light 3B and 1B models as well at this stage\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eNotebook 4:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003eFinally, we take the results from last notebook and convert them into a podcast. We use the \u003ccode\u003eparler-tts/parler-tts-mini-v1\u003c/code\u003e and \u003ccode\u003ebark/suno\u003c/code\u003e models for a conversation.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe speakers and the prompt for parler model were decided based on experimentation and suggestions from the model authors. Please try experimenting, you can find more details in the resources section.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" dir=\"auto\"\u003eNote: Right now there is one issue: Parler needs transformers 4.43.3 or earlier and for steps 1 to 3 of the pipeline you need latest, so we just switch versions in the last notebook.\u003c/h4\u003e\u003ca id=\"user-content-note-right-now-there-is-one-issue-parler-needs-transformers-4433-or-earlier-and-for-steps-1-to-3-of-the-pipeline-you-need-latest-so-we-just-switch-versions-in-the-last-notebook\" aria-label=\"Permalink: Note: Right now there is one issue: Parler needs transformers 4.43.3 or earlier and for steps 1 to 3 of the pipeline you need latest, so we just switch versions in the last notebook.\" href=\"#note-right-now-there-is-one-issue-parler-needs-transformers-4433-or-earlier-and-for-steps-1-to-3-of-the-pipeline-you-need-latest-so-we-just-switch-versions-in-the-last-notebook\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eNext-Improvements/Further ideas:\u003c/h3\u003e\u003ca id=\"user-content-next-improvementsfurther-ideas\" aria-label=\"Permalink: Next-Improvements/Further ideas:\" href=\"#next-improvementsfurther-ideas\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eSpeech Model experimentation: The TTS model is the limitation of how natural this will sound. This probably be improved with a better pipeline and with the help of someone more knowledgable-PRs are welcome! :)\u003c/li\u003e\n\u003cli\u003eLLM vs LLM Debate: Another approach of writing the podcast would be having two agents debate the topic of interest and write the podcast outline. Right now we use a single LLM (70B) to write the podcast outline\u003c/li\u003e\n\u003cli\u003eTesting 405B for writing the transcripts\u003c/li\u003e\n\u003cli\u003eBetter prompting\u003c/li\u003e\n\u003cli\u003eSupport for ingesting a website, audio file, YouTube links and more. Again, we welcome community PRs!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eResources for further learning:\u003c/h3\u003e\u003ca id=\"user-content-resources-for-further-learning\" aria-label=\"Permalink: Resources for further learning:\" href=\"#resources-for-further-learning\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://betterprogramming.pub/text-to-audio-generation-with-bark-clearly-explained-4ee300a3713a\" rel=\"nofollow\"\u003ehttps://betterprogramming.pub/text-to-audio-generation-with-bark-clearly-explained-4ee300a3713a\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1dWWkZzvu7L9Bunq9zvD-W02RFUXoW-Pd?usp=sharing\" rel=\"nofollow\"\u003ehttps://colab.research.google.com/drive/1dWWkZzvu7L9Bunq9zvD-W02RFUXoW-Pd?usp=sharing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://colab.research.google.com/drive/1eJfA2XUa-mXwdMy7DoYKVYHI1iTd9Vkt?usp=sharing#scrollTo=NyYQ--3YksJY\" rel=\"nofollow\"\u003ehttps://colab.research.google.com/drive/1eJfA2XUa-mXwdMy7DoYKVYHI1iTd9Vkt?usp=sharing#scrollTo=NyYQ--3YksJY\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://replicate.com/suno-ai/bark?prediction=zh8j6yddxxrge0cjp9asgzd534\" rel=\"nofollow\"\u003ehttps://replicate.com/suno-ai/bark?prediction=zh8j6yddxxrge0cjp9asgzd534\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c\" rel=\"nofollow\"\u003ehttps://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": null,
  "modifiedTime": null
}
