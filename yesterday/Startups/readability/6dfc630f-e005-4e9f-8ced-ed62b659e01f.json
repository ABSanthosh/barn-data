{
  "id": "6dfc630f-e005-4e9f-8ced-ed62b659e01f",
  "title": "The CAP theorem of Clustering: Why Every Algorithm Must Sacrifice Something",
  "link": "https://blog.codingconfessions.com/p/the-cap-theorem-of-clustering",
  "description": "Article URL: https://blog.codingconfessions.com/p/the-cap-theorem-of-clustering Comments URL: https://news.ycombinator.com/item?id=42518562 Points: 34 # Comments: 3",
  "author": "fagnerbrack",
  "published": "Thu, 26 Dec 2024 23:04:17 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "Abhinav Upadhyay",
  "length": 11155,
  "excerpt": "No clustering algorithm is perfect and you must make a trade-off.",
  "siteName": "Confessions of a Code Addict",
  "favicon": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc25171f-3ae0-4738-a844-3b694f377b19%2Fapple-touch-icon-1024x1024.png",
  "text": "As software engineers, we use clustering algorithms all the time. Whether it's grouping similar users, categorizing content, or detecting patterns in data, clustering seems deceptively simple: just group similar things together, right? You might have used k-means, DBSCAN, or agglomerative clustering, thinking you just need to pick the right algorithm for your use case.But here's what most tutorials won't tell you: every clustering algorithm you choose is fundamentally flawed. Not because of poor implementation or wrong parameters, but because of the math itself. In 2002, Jon Kleinberg (in a paper published at NIPS 2002) proved something that should make every developer pause: it's impossible for any clustering algorithm to have all three properties we'd naturally want it to have.Think of it as the CAP theorem of clustering. Just as distributed systems force you to choose between consistency, availability, and partition tolerance, Kleinberg showed that clustering algorithms force you to pick between scale invariance, richness, and consistency. You can't have all three ‚Äì ever, it‚Äôs a mathematical impossibility. Before you deploy your next clustering solution in production, you need to understand what you're giving up. Let's dive into these three properties and see why you'll always have to choose what to sacrifice.Before we talk about the theorem, we need a precise definition of clustering. The paper defines it in terms of the set of data points and the distance function as defined below:We refer to the data being clustered as the set S of n points.In order to perform the clustering, the clustering model needs to compute pairwise distance between each data point and for that it needs a distance function.The distance function is a mathematical function which takes two data points i and j as parameters, and computes the distance between them. If the parameters i and j are the same data points then the distance between them as computed by this function should be 0. Finally, the paper defines clustering as a function of the distance function d, and the set of data points S such that it partitions S into smaller subsets where each subset represent a cluster. Mathematically speaking:In terms of the distance function d, the clustering function can be defined as a function ∆í that takes a distance function d on S and returns a partition Œì of S.For instance, the k-means algorithm takes the number of clusters k, a distance function (such as the Euclidean distance function), and the set of data points as input, and results in k clusters as its output. These k clusters are essentially a partition of the original dataset S.We want our clustering algorithm to exhibit three desirable properties, which are termed as scale-invariance, richness, and consistency. Let‚Äôs understand what these properties mean.Scale invariance is a property that sounds almost too obvious: if you take your data points and scale all distances between them by the same factor, your clusters shouldn't change.For instance, if for any two points i,j in the dataset the distance between them is d(i,j). Then, if we scale this distance by a factor ùõº such that it becomes ùõº.d(i,j) then the clustering result should remain unchanged. This means that the clustering algorithm is invariant to the scale.In the real world, this matters more than you might think. Have you ever:Changed your measurement units (like feet to meters)Normalized your data differentlyApplied a different scaling to your features only to find your clustering results completely changed? That's what happens when your algorithm isn't scale invariant.Richness is about possibilities ‚Äî a clustering algorithm should be capable of producing any grouping that might make sense for your data.Imagine you're sorting your wardrobe. Sometimes you might want to group clothes by color (creating many clusters), other times by season (four clusters), or simply into 'wear now' and 'store away' (two clusters). A truly rich clustering algorithm should be able to handle all these possibilities, not force you into a predetermined number of groups.But many popular algorithms fail this requirement. Take k-means, for instance. The moment you specify k=3, you've already ruled out any possibility of finding two clusters or four clusters, even if that's what your data naturally suggests. It's like forcing your wardrobe into exactly three groups, even if that doesn't make sense for your clothes.Mathematically speaking: if f is our clustering function and S is our dataset, richness means that f should be able to produce any possible partitioning of S. In other words, range(f) should equal the set of all possible ways to partition S.While this flexibility sounds great in theory, you can probably see why many practical algorithms sacrifice it. When you're analyzing real data, you often want to control the number of clusters to make the results interpretableThe third property, consistency, means that if your existing clusters are good, making similar points more similar and different points more different shouldn't change these clusters.Let's break this down with a simple example. Imagine you've clustered movies into genres based on their characteristics. Now:Two action movies add even more explosive scenes, making them more similar to each otherMeanwhile, a romance movie adds more romantic scenes, making it even more different from the action moviesConsistency means that if these changes only reinforce the existing grouping, your clustering algorithm shouldn't suddenly decide to reorganize the groups.Mathematically speaking: if Œì is a clustering of points using distance function d, and we create a new distance function d' where:For any two points i,j in the same cluster: d'(i,j) \u003c d(i,j) (similar things become more similar)For any two points i,j in different clusters: d'(i,j) \u003e d(i,j) (different things become more different)Then a consistent clustering algorithm should produce the same clustering Œì with d' as it did with d. This new distance function d' is called a Œì transformation of d.There is no clustering function which satisfies all three properties: scale-invariance, richness and consistency.While Kleinberg proved this mathematically (check out his paper for the full proof), let's see how this 'pick two out of three' limitation shows up in algorithms you might be using today.Single linkage is a form of hierarchical clustering. It starts simple: every point is its own cluster, and we gradually merge the closest clusters. The interesting part is deciding how to stop the algorithm. One of the three common criterion are used to stop the algorithm and each one has its trade off rooted in the impossibility theorem.What we do: Stop clustering after we have k clustersWhat we sacrifice: RichnessWhy? We've locked ourselves into exactly k groups. Our algorithm will never discover any other groupings of size smaller or larger than k.What we do: We keep merging clusters as long as their distance \u003c= some distance r. When all clusters are at a distance larger than r, the algorithm automatically stops.What we sacrifice: Scale-invarianceWhy? If we scale up our data by 2x (or some other factor), then clusters which were previously mergeable are suddenly too far apart and will not be merged. This changes the clustering output.What we do: We calculate the maximum pairwise distance œÅ within our dataset using some distance function d1. After that we only merge two clusters if their distance is \u003c= Œ±œÅ, where Œ± \u003c 1.What we sacrifice: ConsistencyWhy? Let‚Äôs say we change our distance function from d1 to d2, such that d2 makes similar points more similar, and dissimilar points more dissimilar. More formally, d2 is a Œì transformation of d1. Then by definition, the maximum pairwise distance obtained using d2 will be larger than œÅ, and as a result the clustering output obtained using d2 will also be very different than the original clustering.Centroid based clustering refers to the commonly used k-means and k-median algorithms. Where we start with a predefined k number of clusters by selecting k points in the data as centroids and then assigning each point to their nearest cluster. The algorithm iteratively optimizes the centroids of the k clusters by computing the mean (or the median) of each cluster, and then redistributing the points based on their nearest cluster centroid. The algorithm normally stops when the clusters become stable.These algorithms suffer with the problem of not satisfying the richness property because as soon as we fix the number of clusters k, automatically it eliminates the possibility of achieving all the possible clusterings as the output.The paper proves that these algorithms don't satisfy the consistency property as well. For instance, for k=2, k-means may come up with clusters X and Y. However, if we decrease the distance between the points within X, and Y, while increasing the distance between the points in X and Y, then the algorithm might come up with two completely different clusters as its output. The paper has a formal proof for this specific example which generalizes to k \u003e 2 as well.Now you know why clustering algorithms force you to make sacrifices. It's not a flaw in implementation or a limitation we'll eventually overcome ‚Äì it's mathematically impossible to have it all. Every clustering algorithm must give up either scale-invariance, richness, or consistency. There's no escape from this fundamental trade-off.But once you understand what you're giving up, you can make this limitation work for you. Just like engineers choose between consistency and availability in distributed systems, you can strategically choose which clustering property to sacrifice:Need your algorithm to handle data regardless of scale? You might have to give up richness.Want the flexibility to discover any possible grouping? Be prepared to lose scale-invariance.Need results to stay stable when cluster patterns become more pronounced? You'll probably sacrifice richness.Instead of fighting these limitations, use them as a guide. Ask yourself:What property matters most for your specific use case?Which trade-off can your application tolerate?How can you design your system knowing these inherent limitations?Understanding Kleinberg's theorem doesn't just make you a better theorist ‚Äì it makes you a more effective engineer. Because in the real world, success isn't about finding the perfect clustering algorithm (it doesn't exist). It's about choosing the right sacrifices for your specific needs.If you find my work interesting and valuable, you can support me by opting for a paid subscription (it‚Äôs $6 monthly/$60 annual). As a bonus you get access to monthly live sessions, and all the past recordings. Many people report failed payments, or don‚Äôt want a recurring subscription. For that I also have a buymeacoffee page. Where you can buy me coffees or become a member. I will upgrade you to a paid subscription for the equivalent duration here.Buy me a coffeeI also have a GitHub Sponsor page. You will get a sponsorship badge, and also a complementary paid subscription here.Sponsor me on GitHub",
  "image": "https://images.unsplash.com/photo-1536390069759-2db770b49d77?crop=entropy\u0026cs=tinysrgb\u0026fit=max\u0026fm=jpg\u0026ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxjbHVzdGVyaW5nfGVufDB8fHx8MTcyODgxNjg0MHww\u0026ixlib=rb-4.0.3\u0026q=80\u0026w=1080",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv dir=\"auto\"\u003e\u003cp\u003eAs software engineers, we use clustering algorithms all the time. Whether it\u0026#39;s grouping similar users, categorizing content, or detecting patterns in data, clustering seems deceptively simple: just group similar things together, right? You might have used k-means, DBSCAN, or agglomerative clustering, thinking you just need to pick the right algorithm for your use case.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eBut here\u0026#39;s what most tutorials won\u0026#39;t tell you: every clustering algorithm you choose is fundamentally flawed. Not because of poor implementation or wrong parameters, but because of the math itself. In 2002, \u003c/span\u003e\u003ca href=\"https://www.cs.cornell.edu/home/kleinber/\" rel=\"\"\u003eJon Kleinberg\u003c/a\u003e\u003cspan\u003e (in a \u003c/span\u003e\u003ca href=\"https://www.cs.cornell.edu/home/kleinber/nips15.pdf\" rel=\"\"\u003epaper\u003c/a\u003e\u003cspan\u003e published at NIPS 2002) proved something that should make every developer pause: it\u0026#39;s impossible for any clustering algorithm to have all three properties we\u0026#39;d naturally want it to have.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eThink of it as the \u003c/span\u003e\u003ca href=\"https://en.wikipedia.org/wiki/CAP_theorem\" rel=\"\"\u003eCAP theorem\u003c/a\u003e\u003cspan\u003e of clustering. Just as distributed systems force you to choose between consistency, availability, and partition tolerance, Kleinberg showed that clustering algorithms force you to pick between scale invariance, richness, and consistency. You can\u0026#39;t have all three ‚Äì ever, it‚Äôs a mathematical impossibility. \u003c/span\u003e\u003c/p\u003e\u003cp\u003eBefore you deploy your next clustering solution in production, you need to understand what you\u0026#39;re giving up. Let\u0026#39;s dive into these three properties and see why you\u0026#39;ll always have to choose what to sacrifice.\u003c/p\u003e\u003cp\u003eBefore we talk about the theorem, we need a precise definition of clustering. The paper defines it in terms of the set of data points and the distance function as defined below:\u003c/p\u003e\u003cp\u003e\u003cspan\u003eWe refer to the data being clustered as the set \u003c/span\u003e\u003ccode\u003eS\u003c/code\u003e\u003cspan\u003e of n points.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eIn order to perform the clustering, the clustering model needs to compute pairwise distance between each data point and for that it needs a distance function.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eThe distance function is a mathematical function which takes two data points \u003c/span\u003e\u003ccode\u003ei\u003c/code\u003e\u003cspan\u003e and \u003c/span\u003e\u003ccode\u003ej\u003c/code\u003e\u003cspan\u003e as parameters, and computes the distance between them. If the parameters \u003c/span\u003e\u003ccode\u003ei\u003c/code\u003e\u003cspan\u003e and \u003c/span\u003e\u003ccode\u003ej\u003c/code\u003e\u003cspan\u003e are the same data points then the distance between them as computed by this function should be 0. \u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eFinally, the paper defines clustering as a function of the distance function \u003c/span\u003e\u003ccode\u003ed\u003c/code\u003e\u003cspan\u003e, and the set of data points \u003c/span\u003e\u003ccode\u003eS\u003c/code\u003e\u003cspan\u003e such that it partitions \u003c/span\u003e\u003ccode\u003eS\u003c/code\u003e\u003cspan\u003e into smaller subsets where each subset represent a cluster. Mathematically speaking:\u003c/span\u003e\u003c/p\u003e\u003cblockquote\u003e\u003cp\u003e\u003cem\u003eIn terms of the distance function d, the clustering function can be defined as a function ∆í that takes a distance function d on S and returns a partition Œì of S.\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003e\u003cspan\u003eFor instance, the k-means algorithm takes the number of clusters k, a distance function (such as the \u003c/span\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Euclidean_distance\" rel=\"\"\u003eEuclidean distance function\u003c/a\u003e\u003cspan\u003e), and the set of data points as input, and results in k clusters as its output. These k clusters are essentially a partition of the original dataset \u003c/span\u003e\u003ccode\u003eS\u003c/code\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eWe want our clustering algorithm to exhibit three desirable properties, which are termed as scale-invariance, richness, and consistency. Let‚Äôs understand what these properties mean.\u003c/p\u003e\u003cp\u003eScale invariance is a property that sounds almost too obvious: if you take your data points and scale all distances between them by the same factor, your clusters shouldn\u0026#39;t change.\u003c/p\u003e\u003cp\u003eFor instance, if for any two points i,j in the dataset the distance between them is d(i,j). Then, if we scale this distance by a factor ùõº such that it becomes ùõº.d(i,j) then the clustering result should remain unchanged. This means that the clustering algorithm is invariant to the scale.\u003c/p\u003e\u003cp\u003eIn the real world, this matters more than you might think. Have you ever:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eChanged your measurement units (like feet to meters)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eNormalized your data differently\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eApplied a different scaling to your features only to find your clustering results completely changed? That\u0026#39;s what happens when your algorithm isn\u0026#39;t scale invariant.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eRichness is about possibilities ‚Äî a clustering algorithm should be capable of producing any grouping that might make sense for your data.\u003c/p\u003e\u003cp\u003eImagine you\u0026#39;re sorting your wardrobe. Sometimes you might want to group clothes by color (creating many clusters), other times by season (four clusters), or simply into \u0026#39;wear now\u0026#39; and \u0026#39;store away\u0026#39; (two clusters). A truly rich clustering algorithm should be able to handle all these possibilities, not force you into a predetermined number of groups.\u003c/p\u003e\u003cp\u003eBut many popular algorithms fail this requirement. Take k-means, for instance. The moment you specify k=3, you\u0026#39;ve already ruled out any possibility of finding two clusters or four clusters, even if that\u0026#39;s what your data naturally suggests. It\u0026#39;s like forcing your wardrobe into exactly three groups, even if that doesn\u0026#39;t make sense for your clothes.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMathematically speaking: if \u003c/span\u003e\u003ccode\u003ef\u003c/code\u003e\u003cspan\u003e is our clustering function and \u003c/span\u003e\u003ccode\u003eS\u003c/code\u003e\u003cspan\u003e is our dataset, richness means that \u003c/span\u003e\u003ccode\u003ef\u003c/code\u003e\u003cspan\u003e should be able to produce any possible partitioning of \u003c/span\u003e\u003ccode\u003eS\u003c/code\u003e\u003cspan\u003e. In other words, \u003c/span\u003e\u003ccode\u003erange(f)\u003c/code\u003e\u003cspan\u003e should equal the set of all possible ways to partition \u003c/span\u003e\u003ccode\u003eS\u003c/code\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eWhile this flexibility sounds great in theory, you can probably see why many practical algorithms sacrifice it. When you\u0026#39;re analyzing real data, you often want to control the number of clusters to make the results interpretable\u003c/p\u003e\u003cp\u003eThe third property, consistency, means that if your existing clusters are good, making similar points more similar and different points more different shouldn\u0026#39;t change these clusters.\u003c/p\u003e\u003cp\u003eLet\u0026#39;s break this down with a simple example. Imagine you\u0026#39;ve clustered movies into genres based on their characteristics. Now:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eTwo action movies add even more explosive scenes, making them more similar to each other\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eMeanwhile, a romance movie adds more romantic scenes, making it even more different from the action movies\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eConsistency means that if these changes only reinforce the existing grouping, your clustering algorithm shouldn\u0026#39;t suddenly decide to reorganize the groups.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMathematically speaking: if \u003c/span\u003e\u003ccode\u003eŒì\u003c/code\u003e\u003cspan\u003e is a clustering of points using distance function \u003c/span\u003e\u003ccode\u003ed\u003c/code\u003e\u003cspan\u003e, and we create a new distance function \u003c/span\u003e\u003ccode\u003ed\u0026#39;\u003c/code\u003e\u003cspan\u003e where:\u003c/span\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eFor any two points \u003c/span\u003e\u003ccode\u003ei\u003c/code\u003e\u003cspan\u003e,\u003c/span\u003e\u003ccode\u003ej\u003c/code\u003e\u003cspan\u003e in the same cluster: \u003c/span\u003e\u003ccode\u003ed\u0026#39;(i,j) \u0026lt; d(i,j)\u003c/code\u003e\u003cspan\u003e (similar things become more similar)\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eFor any two points \u003c/span\u003e\u003ccode\u003ei\u003c/code\u003e\u003cspan\u003e,\u003c/span\u003e\u003ccode\u003ej\u003c/code\u003e\u003cspan\u003e in different clusters: \u003c/span\u003e\u003ccode\u003ed\u0026#39;(i,j) \u0026gt; d(i,j)\u003c/code\u003e\u003cspan\u003e (different things become more different)\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cspan\u003eThen a consistent clustering algorithm should produce the same clustering \u003c/span\u003e\u003ccode\u003eŒì\u003c/code\u003e\u003cspan\u003e with \u003c/span\u003e\u003ccode\u003ed\u0026#39;\u003c/code\u003e\u003cspan\u003e as it did with \u003c/span\u003e\u003ccode\u003ed\u003c/code\u003e\u003cspan\u003e. This new distance function \u003c/span\u003e\u003ccode\u003ed\u0026#39;\u003c/code\u003e\u003cspan\u003e is called a \u003c/span\u003e\u003ccode\u003eŒì\u003c/code\u003e\u003cspan\u003e transformation of \u003c/span\u003e\u003ccode\u003ed\u003c/code\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eThere is no clustering function which satisfies all three properties: \u003c/strong\u003e\u003cem\u003e\u003cstrong\u003escale-invariance\u003c/strong\u003e\u003c/em\u003e\u003cstrong\u003e, \u003c/strong\u003e\u003cem\u003e\u003cstrong\u003erichness\u003c/strong\u003e\u003c/em\u003e\u003cstrong\u003e and \u003c/strong\u003e\u003cem\u003e\u003cstrong\u003econsistency\u003c/strong\u003e\u003c/em\u003e\u003cstrong\u003e.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhile Kleinberg proved this mathematically (check out his paper for the full proof), let\u0026#39;s see how this \u0026#39;pick two out of three\u0026#39; limitation shows up in algorithms you might be using today.\u003c/p\u003e\u003cp\u003eSingle linkage is a form of hierarchical clustering. It starts simple: every point is its own cluster, and we gradually merge the closest clusters. The interesting part is deciding how to stop the algorithm. One of the three common criterion are used to stop the algorithm and each one has its trade off rooted in the impossibility theorem.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eWhat we do: Stop clustering after we have k clusters\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhat we sacrifice: Richness\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhy? We\u0026#39;ve locked ourselves into exactly k groups. Our algorithm will never discover any other groupings of size smaller or larger than k.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eWhat we do: We keep merging clusters as long as their distance \u0026lt;= some distance r. When all clusters are at a distance larger than r, the algorithm automatically stops.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhat we sacrifice: Scale-invariance\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhy? If we scale up our data by 2x (or some other factor), then clusters which were previously mergeable are suddenly too far apart and will not be merged. This changes the clustering output.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eWhat we do: We calculate the maximum pairwise distance \u003c/span\u003e\u003ccode\u003eœÅ\u003c/code\u003e\u003cspan\u003e within our dataset using some distance function \u003c/span\u003e\u003ccode\u003ed1\u003c/code\u003e\u003cspan\u003e. After that we only merge two clusters if their distance is \u0026lt;= \u003c/span\u003e\u003ccode\u003eŒ±œÅ\u003c/code\u003e\u003cspan\u003e, where \u003c/span\u003e\u003ccode\u003eŒ±\u003c/code\u003e\u003cspan\u003e \u0026lt; 1.\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhat we sacrifice: Consistency\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eWhy?  Let‚Äôs say we change our distance function from \u003c/span\u003e\u003ccode\u003ed1\u003c/code\u003e\u003cspan\u003e to \u003c/span\u003e\u003ccode\u003ed2\u003c/code\u003e\u003cspan\u003e, such that d2 makes similar points more similar, and dissimilar points more dissimilar. More formally, \u003c/span\u003e\u003ccode\u003ed2\u003c/code\u003e\u003cspan\u003e is a \u003c/span\u003e\u003ccode\u003eŒì\u003c/code\u003e\u003cspan\u003e transformation of \u003c/span\u003e\u003ccode\u003ed1\u003c/code\u003e\u003cspan\u003e. Then by definition, the maximum pairwise distance obtained using \u003c/span\u003e\u003ccode\u003ed2\u003c/code\u003e\u003cspan\u003e will be larger than \u003c/span\u003e\u003ccode\u003eœÅ\u003c/code\u003e\u003cspan\u003e, and as a result the clustering output obtained using \u003c/span\u003e\u003ccode\u003ed2\u003c/code\u003e\u003cspan\u003e will also be very different than the original clustering.\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cspan\u003eCentroid based clustering refers to the commonly used \u003c/span\u003e\u003cem\u003ek-means\u003c/em\u003e\u003cspan\u003e and \u003c/span\u003e\u003cem\u003ek-median\u003c/em\u003e\u003cspan\u003e algorithms. Where we start with a predefined \u003c/span\u003e\u003cem\u003ek\u003c/em\u003e\u003cspan\u003e number of clusters by selecting \u003c/span\u003e\u003cem\u003ek\u003c/em\u003e\u003cspan\u003e points in the data as centroids and then assigning each point to their nearest cluster. The algorithm iteratively optimizes the centroids of the k clusters by computing the mean (or the median) of each cluster, and then redistributing the points based on their nearest cluster centroid. The algorithm normally stops when the clusters become stable.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eThese algorithms suffer with the problem of not satisfying the \u003c/span\u003e\u003cem\u003erichness\u003c/em\u003e\u003cspan\u003e property because as soon as we fix the number of clusters k, automatically it eliminates the possibility of achieving all the possible clusterings as the output.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eThe paper proves that these algorithms don\u0026#39;t satisfy the \u003c/span\u003e\u003cem\u003econsistency\u003c/em\u003e\u003cspan\u003e property as well. For instance, for k=2, k-means may come up with clusters X and Y. However, if we decrease the distance between the points within X, and Y, while increasing the distance between the points in X and Y, then the algorithm might come up with two completely different clusters as its output. The paper has a formal proof for this specific example which generalizes to k \u0026gt; 2 as well.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eNow you know why clustering algorithms force you to make sacrifices. It\u0026#39;s not a flaw in implementation or a limitation we\u0026#39;ll eventually overcome ‚Äì it\u0026#39;s mathematically impossible to have it all. Every clustering algorithm must give up either scale-invariance, richness, or consistency. There\u0026#39;s no escape from this fundamental trade-off.\u003c/p\u003e\u003cp\u003eBut once you understand what you\u0026#39;re giving up, you can make this limitation work for you. Just like engineers choose between consistency and availability in distributed systems, you can strategically choose which clustering property to sacrifice:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eNeed your algorithm to handle data regardless of scale? You might have to give up richness.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWant the flexibility to discover any possible grouping? Be prepared to lose scale-invariance.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eNeed results to stay stable when cluster patterns become more pronounced? You\u0026#39;ll probably sacrifice richness.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eInstead of fighting these limitations, use them as a guide. Ask yourself:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eWhat property matters most for your specific use case?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhich trade-off can your application tolerate?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eHow can you design your system knowing these inherent limitations?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eUnderstanding Kleinberg\u0026#39;s theorem doesn\u0026#39;t just make you a better theorist ‚Äì it makes you a more effective engineer. Because in the real world, success isn\u0026#39;t about finding the perfect clustering algorithm (it doesn\u0026#39;t exist). It\u0026#39;s about choosing the right sacrifices for your specific needs.\u003c/p\u003e\u003cp\u003eIf you find my work interesting and valuable, you can support me by opting for a paid subscription (it‚Äôs $6 monthly/$60 annual). As a bonus you get access to monthly live sessions, and all the past recordings. \u003c/p\u003e\u003cp\u003e\u003cspan\u003eMany people report failed payments, or don‚Äôt want a recurring subscription. For that I also have a \u003c/span\u003e\u003ca href=\"https://buymeacoffee.com/codeconfessions\" rel=\"\"\u003ebuymeacoffee page\u003c/a\u003e\u003cspan\u003e. Where you can buy me coffees or become a member. I will upgrade you to a paid subscription for the equivalent duration here.\u003c/span\u003e\u003c/p\u003e\u003cp data-attrs=\"{\u0026#34;url\u0026#34;:\u0026#34;https://buymeacoffee.com/codeconfessions\u0026#34;,\u0026#34;text\u0026#34;:\u0026#34;Buy me a coffee\u0026#34;,\u0026#34;action\u0026#34;:null,\u0026#34;class\u0026#34;:\u0026#34;button-wrapper\u0026#34;}\" data-component-name=\"ButtonCreateButton\"\u003e\u003ca href=\"https://buymeacoffee.com/codeconfessions\" rel=\"\"\u003e\u003cspan\u003eBuy me a coffee\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003eI also have a GitHub Sponsor page. You will get a sponsorship badge, and also a complementary paid subscription here.\u003c/p\u003e\u003cp data-attrs=\"{\u0026#34;url\u0026#34;:\u0026#34;https://github.com/sponsors/abhinav-upadhyay\u0026#34;,\u0026#34;text\u0026#34;:\u0026#34;Sponsor me on GitHub\u0026#34;,\u0026#34;action\u0026#34;:null,\u0026#34;class\u0026#34;:\u0026#34;button-wrapper\u0026#34;}\" data-component-name=\"ButtonCreateButton\"\u003e\u003ca href=\"https://github.com/sponsors/abhinav-upadhyay\" rel=\"\"\u003e\u003cspan\u003eSponsor me on GitHub\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2024-10-29T09:00:44Z",
  "modifiedTime": null
}
