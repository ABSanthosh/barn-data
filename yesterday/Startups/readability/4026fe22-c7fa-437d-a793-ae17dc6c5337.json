{
  "id": "4026fe22-c7fa-437d-a793-ae17dc6c5337",
  "title": "DeepSeek R1’s bold bet on reinforcement learning: How it outpaced OpenAI at 3% of the cost",
  "link": "https://venturebeat.com/ai/deepseek-r1s-bold-bet-on-reinforcement-learning-how-it-outpaced-openai-at-3-of-the-cost/",
  "description": "DeepSeek R1’s Monday release has sent shockwaves through the AI community, disrupting assumptions about what’s required to achieve cutting-edge AI performance. This story focuses on exactly how DeepSeek managed this feat, and what it means for the vast number of users of AI models. For enterprises developing AI-driven solutions, DeepSeek’s breakthrough challenges assumptions of OpenAI's dominance -- and offers a blueprint for cost-efficient innovation.",
  "author": "Matt Marshall",
  "published": "Sun, 26 Jan 2025 02:57:24 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Automation",
    "Data Infrastructure",
    "Programming \u0026 Development",
    "category-/Science/Computer Science",
    "DeepSeek-R1"
  ],
  "byline": "Matt Marshall",
  "length": 12973,
  "excerpt": "DeepSeek R1’s Monday release has sent shockwaves through the AI community, disrupting assumptions about what’s required to achieve cutting-edge AI performance. This story focuses on exactly how DeepSeek managed this feat, and what it means for the vast number of users of AI models. For enterprises developing AI-driven solutions, DeepSeek’s breakthrough challenges assumptions of OpenAI's dominance -- and offers a blueprint for cost-efficient innovation.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "January 25, 2025 6:57 PM Image Credit: VentureBeat via ChatGPT Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More DeepSeek R1’s Monday release has sent shockwaves through the AI community, disrupting assumptions about what’s required to achieve cutting-edge AI performance. Matching OpenAI’s o1 at just 3%-5% of the cost, this open-source model has not only captivated developers but also challenges enterprises to rethink their AI strategies. The model has rocketed to the top-trending model being downloaded on HuggingFace (109,000 times, as of this writing) – as developers rush to try it out and seek to understand what it means for their AI development. Users are commenting that DeepSeek’s accompanying search feature (which you can find at DeepSeek’s site) is now superior to competitors like OpenAI and Perplexity, and is only rivaled by Google’s Gemini Deep Research. The implications for enterprise AI strategies are profound: With reduced costs and open access, enterprises now have an alternative to costly proprietary models like OpenAI’s. DeepSeek’s release could democratize access to cutting-edge AI capabilities, enabling smaller organizations to compete effectively in the AI arms race. This story focuses on exactly how DeepSeek managed this feat, and what it means for the vast number of users of AI models. For enterprises developing AI-driven solutions, DeepSeek’s breakthrough challenges assumptions of OpenAI’s dominance — and offers a blueprint for cost-efficient innovation. It’s the “how” DeepSeek did what it did that should be the most educational here. DeepSeek’s breakthrough: Moving to pure reinforcement learning In November, DeepSeek made headlines with its announcement that it had achieved performance surpassing OpenAI’s o1, but at the time it only offered a limited R1-lite-preview model. With Monday’s full release of R1 and the accompanying technical paper, the company revealed a surprising innovation: a deliberate departure from the conventional supervised fine-tuning (SFT) process widely used in training large language models (LLMs). SFT, a standard step in AI development, involves training models on curated datasets to teach step-by-step reasoning, often referred to as chain-of-thought (CoT). It is considered essential for improving reasoning capabilities. However, DeepSeek challenged this assumption by skipping SFT entirely, opting instead to rely on reinforcement learning (RL) to train the model. This bold move forced DeepSeek-R1 to develop independent reasoning abilities, avoiding the brittleness often introduced by prescriptive datasets. While some flaws emerge – leading the team to reintroduce a limited amount of SFT during the final stages of building the model – the results confirmed the fundamental breakthrough: reinforcement learning alone could drive substantial performance gains. The company got much of the way using open source – a conventional and unsurprising way First, some background on how DeepSeek got to where it did. DeepSeek, a 2023 spin-off from Chinese hedge-fund High-Flyer Quant, began by developing AI models for its proprietary chatbot before releasing them for public use.  Little is known about the company’s exact approach, but it quickly open sourced its models, and it’s extremely likely that the company built upon the open projects produced by Meta, for example the Llama model, and ML library Pytorch.  To train its models, High-Flyer Quant secured over 10,000 Nvidia GPUs before U.S. export restrictions, and reportedly expanded to 50,000 GPUs through alternative supply routes, despite trade barriers. This pales compared to leading AI labs like OpenAI, Google, and Anthropic, which operate with more than 500,000 GPUs each.   DeepSeek’s ability to achieve competitive results with limited resources highlights how ingenuity and resourcefulness can challenge the high-cost paradigm of training state-of-the-art LLMs. Despite speculation, DeepSeek’s full budget is unknown DeepSeek reportedly trained its base model — called V3 — on a $5.58 million budget over two months, according to Nvidia engineer Jim Fan. While the company hasn’t divulged the exact training data it used (side note: critics say this means DeepSeek isn’t truly open-source), modern techniques make training on web and open datasets increasingly accessible. Estimating the total cost of training DeepSeek-R1 is challenging. While running 50,000 GPUs suggests significant expenditures (potentially hundreds of millions of dollars), precise figures remain speculative. What’s clear, though, is that DeepSeek has been very innovative from the get-go. Last year, reports emerged about some initial innovations it was making, around things like Mixture of Experts and Multi-Head Latent Attention. How DeepSeek-R1 got to the “aha moment” The journey to DeepSeek-R1’s final iteration began with an intermediate model, DeepSeek-R1-Zero, which was trained using pure reinforcement learning. By relying solely on RL, DeepSeek incentivized this model to think independently, rewarding both correct answers and the logical processes used to arrive at them. This approach led to an unexpected phenomenon: The model began allocating additional processing time to more complex problems, demonstrating an ability to prioritize tasks based on their difficulty. DeepSeek’s researchers described this as an “aha moment,” where the model itself identified and articulated novel solutions to challenging problems (see screenshot below). This milestone underscored the power of reinforcement learning to unlock advanced reasoning capabilities without relying on traditional training methods like SFT. Source: DeepSeek-R1 paper. Don’t let this graphic intimidate you. The key takeaway is the red line, where the model literally used the phrase “aha moment.” Researchers latched onto this as a striking example of the model’s ability to rethink problems in an anthropomorphic tone. For the researchers, they said it was their own “aha moment.” The researchers conclude: “It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.” More than RL However, it’s true that the model needed more than just RL. The paper goes on to talk about how despite the RL creating unexpected and powerful reasoning behaviors, this intermediate model DeepSeek-R1-Zero did face some challenges, including poor readability, and language mixing (starting in Chinese and switching over to English, for example). So only then did the team decide to create a new model, which would become the final DeepSeek-R1 model. This model, again based on the V3 base model, was first injected with limited SFT – focused on a “small amount of long CoT data” or what was called cold-start data, to fix some of the challenges. After that, it was put through the same reinforcement learning process of R1-Zero. The paper then talks about how R1 went through some final rounds of fine-tuning. The ramifications One question is why there has been so much surprise by the release. It’s not like open source models are new. Open Source models have a huge logic and momentum behind them. Their free cost and malleability is why we reported recently that these models are going to win in the enterprise. Meta’s open-weights model Llama 3, for example, exploded in popularity last year, as it was fine-tuned by developers wanting their own custom models. Similarly, now DeepSeek-R1 is already being used to distill its reasoning into an array of other, much smaller models – the difference being that DeepSeek offers industry-leading performance. This includes running tiny versions of the model on mobile phones, for example. DeepSeek-R1 not only performs better than the leading open source alternative, Llama 3. It shows its entire chain of thought of its answers transparently. Meta’s Llama hasn’t been instructed to do this as a default; it takes aggressive prompting of Llama to do this. The transparency has also provided a PR black-eye to OpenAI, which has so far hidden its chains of thought from users, citing competitive reasons and not to confuse users when a model gets something wrong. Transparency allows developers to pinpoint and address errors in a model’s reasoning, streamlining customizations to meet enterprise requirements more effectively. For enterprise decision-makers, DeepSeek’s success underscores a broader shift in the AI landscape: leaner, more efficient development practices are increasingly viable. Organizations may need to reevaluate their partnerships with proprietary AI providers, considering whether the high costs associated with these services are justified when open-source alternatives can deliver comparable, if not superior, results. To be sure, no massive lead While DeepSeek’s innovation is groundbreaking, by no means has it established a commanding market lead. Because it published its research, other model companies will learn from it, and adapt. Meta and Mistral, the French open source model company, may be a beat behind, but it will probably only be a few months before they catch up. As Meta’s lead researcher Yann Lecun put it: “The idea is that everyone profits from everyone else’s ideas. No one ‘outpaces’ anyone and no country ‘loses’ to another. No one has a monopoly on good ideas. Everyone’s learning from everyone else.” So it’s execution that matters. Ultimately, it’s the consumers, startups and other users who will win the most, because DeepSeek’s offerings will continue to drive the price of using these models near zero (again aside from cost of running models at inference). This rapid commoditization could pose challenges – indeed, massive pain – for leading AI providers that have invested heavily in proprietary infrastructure. As many commentators have put it, including Chamath Palihapitiya, an investor and former executive at Meta, this could mean that years of OpEx and CapEx by OpenAI and others will be wasted. There is substantial commentary about whether it is ethical to use the DeepSeek-R1 model because of the biases instilled in it by Chinese laws, for example that it shouldn’t answer questions about the Chinese government’s brutal crackdown at Tiananmen Square. Despite ethical concerns around biases, many developers view these biases as infrequent edge cases in real-world applications – and they can be mitigated through fine-tuning. Moreover, they point to different, but analogous biases that are held by models from OpenAI and other companies. Meta’s Llama has emerged as a popular open model despite its data sets not being made public, and despite hidden biases, and lawsuits being filed against it as a result. Questions abound around the ROI of big investments by OpenAI This all raises big questions about the investment plans pursued by OpenAI, Microsoft and others. OpenAI’s $500 billion Stargate project reflects its commitment to building massive data centers to power its advanced models. Backed by partners like Oracle and Softbank, this strategy is premised on the belief that achieving artificial general intelligence (AGI) requires unprecedented compute resources. However, DeepSeek’s demonstration of a high-performing model at a fraction of the cost challenges the sustainability of this approach, raising doubts about OpenAI’s ability to deliver returns on such a monumental investment. Entrepreneur and commentator Arnaud Bertrand captured this dynamic, contrasting China’s frugal, decentralized innovation with the U.S. reliance on centralized, resource-intensive infrastructure: “It’s about the world realizing that China has caught up — and in some areas overtaken — the U.S. in tech and innovation, despite efforts to prevent just that.” Indeed, yesterday another Chinese company, ByteDance announced Doubao-1.5-pro, which Includes a “Deep Thinking” mode that surpasses OpenAI’s o1 on the AIME benchmark. Want to dive deeper into how DeepSeek-R1 is reshaping AI development? Check out our in-depth discussion on YouTube, where I explore this breakthrough with ML developer Sam Witteveen. Together, we break down the technical details, implications for enterprises, and what this means for the future of AI: Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/01/DALL·E-2025-01-25-08.38.46-A-minimalistic-vector-style-illustration-symbolizing-Deepseek-R1s-AI-innovation.-The-design-includes-a-single-glowing-node-connected-by-a-few-thin-.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-01-26T02:57:24+00:00\" datetime=\"2025-01-26T02:57:24+00:00\"\u003eJanuary 25, 2025 6:57 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"400\" height=\"229\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/DALL·E-2025-01-25-08.38.46-A-minimalistic-vector-style-illustration-symbolizing-Deepseek-R1s-AI-innovation.-The-design-includes-a-single-glowing-node-connected-by-a-few-thin-.webp?w=400\" alt=\"\"/\u003e\u003c/p\u003e\u003cdiv\u003e\u003cp\u003e\u003cem\u003eImage Credit: VentureBeat via ChatGPT\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eDeepSeek R1’s Monday release has sent \u003ca href=\"https://venturebeat.com/ai/why-everyone-in-ai-is-freaking-out-about-deepseek/\"\u003eshockwaves through the AI community\u003c/a\u003e, disrupting assumptions about what’s required to achieve cutting-edge AI performance. Matching OpenAI’s o1 at just \u003ca href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/\"\u003e3%-5% of the cost\u003c/a\u003e, this open-source model has not only captivated developers but also challenges enterprises to rethink their AI strategies.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe model has rocketed to the top-trending model being downloaded on HuggingFace (\u003ca href=\"https://huggingface.co/models\"\u003e109,000 times, as of this writing\u003c/a\u003e) – as developers rush to try it out and seek to understand what it means for their AI development. Users are commenting that DeepSeek’s accompanying search feature (which you can find at \u003ca href=\"https://chat.deepseek.com/\"\u003eDeepSeek’s site\u003c/a\u003e) is now \u003ca href=\"https://x.com/casper_hansen_/status/1882304126622851168?s=46\"\u003esuperior to competitors like OpenAI and Perplexity\u003c/a\u003e, and is only rivaled by Google’s Gemini Deep Research.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe implications for enterprise AI strategies are profound: With reduced costs and open access, enterprises now have an alternative to costly proprietary models like OpenAI’s. DeepSeek’s release could democratize access to cutting-edge AI capabilities, enabling smaller organizations to compete effectively in the AI arms race.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis story focuses on exactly how DeepSeek managed this feat, and what it means for the vast number of users of AI models. For enterprises developing AI-driven solutions, DeepSeek’s breakthrough challenges assumptions of OpenAI’s dominance — and offers a blueprint for cost-efficient innovation. It’s the “how” DeepSeek did what it did that should be the most educational here.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-deepseek-s-breakthrough-moving-to-pure-reinforcement-learning\"\u003eDeepSeek’s breakthrough: Moving to pure reinforcement learning\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn November, DeepSeek made headlines with its announcement that it had achieved performance surpassing OpenAI’s o1, but at the time it only offered \u003ca href=\"https://venturebeat.com/ai/deepseeks-first-reasoning-model-r1-lite-preview-turns-heads-beating-openai-o1-performance/\"\u003ea limited R1-lite-preview model\u003c/a\u003e. With Monday’s full release of R1 and the accompanying \u003ca href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"\u003etechnical paper\u003c/a\u003e, the company revealed a surprising innovation: a deliberate departure from the conventional supervised fine-tuning (SFT) process widely used in training large language models (LLMs).\u003c/p\u003e\n\n\n\n\u003cp\u003eSFT, a standard step in AI development, involves training models on curated datasets to teach step-by-step reasoning, often referred to as chain-of-thought (CoT). It is considered essential for improving reasoning capabilities. However, DeepSeek challenged this assumption by skipping SFT entirely, opting instead to rely on reinforcement learning (RL) to train the model.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis bold move forced DeepSeek-R1 to develop independent reasoning abilities, avoiding the brittleness often introduced by prescriptive datasets. While some flaws emerge – leading the team to reintroduce a limited amount of SFT during the final stages of building the model – the results confirmed the fundamental breakthrough: reinforcement learning alone could drive substantial performance gains.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-company-got-much-of-the-way-using-open-source-a-conventional-and-unsurprising-way\"\u003eThe company got much of the way using open source – a conventional and unsurprising way\u003c/h2\u003e\n\n\n\n\u003cp\u003eFirst, some background on how DeepSeek got to where it did. DeepSeek, a 2023 spin-off from Chinese hedge-fund High-Flyer Quant, began by developing AI models for its proprietary chatbot before releasing them for public use.  Little is known about the company’s exact approach, but it quickly open sourced its models, and it’s extremely likely that the company built upon the open projects produced by Meta, for example the Llama model, and ML library Pytorch. \u003c/p\u003e\n\n\n\n\u003cp\u003eTo train its models, High-Flyer Quant secured over 10,000 Nvidia GPUs before U.S. export restrictions, and \u003ca href=\"https://x.com/kimmonismus/status/1882824571281436713?s=46\"\u003ereportedly expanded to 50,000 GPUs\u003c/a\u003e through alternative supply routes, despite trade barriers.  This pales compared to leading AI labs like OpenAI, Google, and Anthropic, which \u003ca href=\"https://x.com/mmarshall/status/1804509409680167023?lang=en\"\u003eoperate with more than 500,000 GPUs each\u003c/a\u003e.  \u003c/p\u003e\n\n\n\n\u003cp\u003eDeepSeek’s ability to achieve competitive results with limited resources highlights \u003ca href=\"https://www.linkedin.com/posts/waseemalshikh_lots-of-hype-about-deepseek-and-its-r1-llm-activity-7288684074699816961-1UEy/?utm_source=share\u0026amp;utm_medium=member_ios\"\u003ehow ingenuity and resourcefulness can challenge the high-cost paradigm of training state-of-the-art LLMs\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-despite-speculation-deepseek-s-full-budget-is-unknown\"\u003eDespite speculation, DeepSeek’s full budget is unknown\u003c/h2\u003e\n\n\n\n\u003cp\u003eDeepSeek reportedly trained its base model — called V3 — on a $5.58 million budget over two months, \u003ca href=\"https://www.scmp.com/tech/policy/article/3295662/beijing-meeting-puts-spotlight-chinas-new-face-ai-deepseek-founder-liang-wenfeng\"\u003eaccording to Nvidia engineer Jim Fan\u003c/a\u003e. While the company hasn’t divulged the exact training data it used (side note: critics say this means DeepSeek isn’t truly open-source), modern techniques make training on web and open datasets increasingly accessible. Estimating the total cost of training DeepSeek-R1 is challenging. While running 50,000 GPUs suggests significant expenditures (potentially hundreds of millions of dollars), precise figures remain speculative.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhat’s clear, though, is that DeepSeek has been very innovative from the get-go. Last year, reports emerged about some initial innovations it was making, \u003ca href=\"https://medium.com/@zaiinn440/coding-deepseek-v2-from-scratch-in-pytorch-06dd89917067\"\u003earound things like Mixture of Experts and Multi-Head Latent Attention\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-deepseek-r1-got-to-the-aha-moment\"\u003eHow DeepSeek-R1 got to the “aha moment”\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe journey to DeepSeek-R1’s final iteration began with an intermediate model, DeepSeek-R1-Zero, which was trained using pure reinforcement learning. By relying solely on RL, DeepSeek incentivized this  model to think independently, rewarding both correct answers and the logical processes used to arrive at them.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis approach led to an unexpected phenomenon: The model began allocating additional processing time to more complex problems, demonstrating an ability to prioritize tasks based on their difficulty. DeepSeek’s researchers described this as an “aha moment,” where the model itself identified and articulated novel solutions to challenging problems (see screenshot below). This milestone underscored the power of reinforcement learning to unlock advanced reasoning capabilities without relying on traditional training methods like SFT.\u003c/p\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1317\" height=\"727\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-2025-01-25-at-6.06.56%E2%80%AFPM.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-2025-01-25-at-6.06.56 PM.png 1317w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-2025-01-25-at-6.06.56 PM.png?resize=300,166 300w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-2025-01-25-at-6.06.56 PM.png?resize=768,424 768w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-2025-01-25-at-6.06.56 PM.png?resize=800,442 800w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-2025-01-25-at-6.06.56 PM.png?resize=400,221 400w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-2025-01-25-at-6.06.56 PM.png?resize=750,414 750w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-2025-01-25-at-6.06.56 PM.png?resize=578,319 578w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-2025-01-25-at-6.06.56 PM.png?resize=930,513 930w\" sizes=\"(max-width: 1317px) 100vw, 1317px\"/\u003e\u003cfigcaption\u003eSource: DeepSeek-R1 paper. Don’t let this graphic intimidate you. The key takeaway is the red line, where the model literally used the phrase “aha moment.” Researchers latched onto this as a striking example of the model’s ability to rethink problems in an anthropomorphic tone. For the researchers, they said it was their own “aha moment.” \u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eThe researchers conclude: “It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-more-than-rl\"\u003eMore than RL\u003c/h2\u003e\n\n\n\n\u003cp\u003eHowever, it’s true that the model needed more than just RL. The paper goes on to talk about how despite the RL creating unexpected and powerful reasoning behaviors, this intermediate model DeepSeek-R1-Zero did face some challenges, including poor readability, and language mixing (starting in Chinese and switching over to English, for example). So only then did the team decide to create a new model, which would become the final DeepSeek-R1 model. This model, again based on the V3 base model, was first injected with limited SFT – focused on a “small amount of long CoT data” or what was called cold-start data, to fix some of the challenges. After that, it was put through the same reinforcement learning process of R1-Zero. The paper then talks about how R1 went through some final rounds of fine-tuning.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-ramifications\"\u003eThe ramifications\u003c/h2\u003e\n\n\n\n\u003cp\u003eOne question is why there has been so much surprise by the release. It’s not like open source models are new. Open Source models have a huge logic and momentum behind them. Their free cost and malleability is why \u003ca href=\"https://venturebeat.com/ai/the-enterprise-verdict-on-ai-models-why-open-source-will-win/\"\u003ewe reported recently that these models are going to win in the enterprise\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eMeta’s open-weights model Llama 3, for example, exploded in popularity last year, as it was fine-tuned by developers wanting their own custom models. Similarly, now DeepSeek-R1 is already being used to distill its reasoning into an array of other, much smaller models – the difference being that DeepSeek offers industry-leading performance. This includes running tiny versions of the model on mobile phones, for example.\u003c/p\u003e\n\n\n\n\u003cp\u003eDeepSeek-R1 not only performs better than the leading open source alternative, Llama 3. It shows its entire chain of thought of its answers transparently. Meta’s Llama hasn’t been instructed to do this as a default; it \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1ftvcve/chain_of_thought_reasoning_local_llama/\"\u003etakes aggressive prompting of Llama to do this\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe transparency has also provided a PR black-eye to OpenAI, which has so far hidden its chains of thought from users, citing competitive reasons and not to confuse users when a model gets something wrong. Transparency allows developers to pinpoint and address errors in a model’s reasoning, streamlining customizations to meet enterprise requirements more effectively.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor enterprise decision-makers, DeepSeek’s success underscores a broader shift in the AI landscape: leaner, more efficient development practices are increasingly viable. Organizations may need to reevaluate their partnerships with proprietary AI providers, considering whether the high costs associated with these services are justified when open-source alternatives can deliver comparable, if not superior, results.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-to-be-sure-no-massive-lead\"\u003eTo be sure, no massive lead\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile DeepSeek’s innovation is groundbreaking, by no means has it established a commanding market lead. Because it published its research, other model companies will learn from it, and adapt. Meta and Mistral, the French open source model company, may be a beat behind, but it will probably only be a few months before they catch up. As Meta’s lead researcher Yann Lecun \u003ca href=\"https://x.com/ylecun/status/1882946965761347885?t=ErilaoVvmBLk2r5kmKcOBw\u0026amp;s=19\"\u003eput it\u003c/a\u003e: “The idea is that everyone profits from everyone else’s ideas. No one ‘outpaces’ anyone and no country ‘loses’ to another. No one has a monopoly on good ideas. Everyone’s learning from everyone else.” So it’s execution that matters.\u003c/p\u003e\n\n\n\n\u003cp\u003eUltimately, it’s the consumers, startups and other users who will win the most, because DeepSeek’s offerings will continue to drive the price of using these models near zero (again aside from cost of running models at inference). This rapid commoditization could pose challenges – indeed, massive pain – for leading AI providers that have invested heavily in proprietary infrastructure. As many commentators have put it, including Chamath Palihapitiya, an investor and former executive at Meta, this could mean that \u003ca href=\"https://x.com/tphuang/status/1882228995095437699?s=4\"\u003eyears of OpEx and CapEx by OpenAI and others will be wasted\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eThere is substantial commentary about whether it is ethical to use the DeepSeek-R1 model because of the biases instilled in it by Chinese laws, for example that it shouldn’t answer questions about the Chinese government’s brutal crackdown at Tiananmen Square. Despite ethical concerns around biases, many developers view these biases as infrequent edge cases in real-world applications – and they can be mitigated through fine-tuning. Moreover, they point to different, but analogous biases that are held by models from OpenAI and other companies. Meta’s Llama has emerged as a popular open model despite its data sets not being made public, and despite hidden biases, and \u003ca href=\"https://www.wired.com/story/new-documents-unredacted-meta-copyright-ai-lawsuit/\"\u003elawsuits being filed against it as a result\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-questions-abound-around-the-roi-of-big-investments-by-openai\"\u003eQuestions abound around the ROI of big investments by OpenAI\u003c/h2\u003e\n\n\n\n\u003cp\u003eThis all raises big questions about the investment plans pursued by OpenAI, Microsoft and others. OpenAI’s \u003ca href=\"https://venturebeat.com/ai/openai-stargate-is-a-500b-bet-americas-ai-manhattan-project-or-costly-dead-end/\"\u003e$500 billion Stargate project\u003c/a\u003e reflects its commitment to building massive data centers to power its advanced models. Backed by partners like Oracle and Softbank, this strategy is premised on the belief that achieving artificial general intelligence (AGI) requires unprecedented compute resources. However, DeepSeek’s demonstration of a high-performing model at a fraction of the cost challenges the sustainability of this approach, raising doubts about OpenAI’s ability to deliver returns on such a monumental investment.\u003c/p\u003e\n\n\n\n\u003cp\u003eEntrepreneur and commentator Arnaud Bertrand \u003ca href=\"https://x.com/RnaudBertrand/status/1882732479288942781\"\u003ecaptured this dynamic\u003c/a\u003e, contrasting China’s frugal, decentralized innovation with the U.S. reliance on centralized, resource-intensive infrastructure: “It’s about the world realizing that China has caught up — and in some areas overtaken — the U.S. in tech and innovation, despite efforts to prevent just that.” Indeed, yesterday another Chinese company, ByteDance \u003ca href=\"https://x.com/_akhaliq/status/1882904208757666086\"\u003eannounced Doubao-1.5-pro\u003c/a\u003e, which Includes a “Deep Thinking” mode that surpasses OpenAI’s o1 on the AIME benchmark.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eWant to dive deeper into how DeepSeek-R1 is reshaping AI development?\u003c/strong\u003e Check out our in-depth discussion on YouTube, where I explore this breakthrough with ML developer Sam Witteveen. Together, we break down the technical details, implications for enterprises, and what this means for the future of AI:\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\n\u003ciframe title=\"Deepseek R1: How China’s open source AI model beats OpenAI at 3% of the cost\" width=\"500\" height=\"281\" src=\"https://www.youtube.com/embed/bJzj5lTiqe0?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003c/p\u003e\u003c/figure\u003e\n\n\n\n\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": "2025-01-26T02:57:24Z",
  "modifiedTime": "2025-01-26T03:57:33Z"
}
