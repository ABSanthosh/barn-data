{
  "id": "8a701d6b-1bff-4263-8040-d9cd58f037df",
  "title": "The Cost of Being Crawled: LLM Bots and Vercel Image API Pricing",
  "link": "https://metacast.app/blog/engineering/postmortem-llm-bots-image-optimization",
  "description": "Article URL: https://metacast.app/blog/engineering/postmortem-llm-bots-image-optimization Comments URL: https://news.ycombinator.com/item?id=43687431 Points: 45 # Comments: 37",
  "author": "navs",
  "published": "Mon, 14 Apr 2025 23:33:38 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "Ilya Bezdelev",
  "length": 12084,
  "excerpt": "A misconfiguration that might have cost us $7,000",
  "siteName": "Metacast: podcast app with transcripts",
  "favicon": "",
  "text": "A misconfiguration that might have cost us $7,000Ilya BezdelevPublished on February 10, 2025Table of ContentsTL;DRContextHow we discovered the problemStep 1: A cost spikeStep 2: Image Optimization API usage spikeStep 3: Tens of thousands of requests from LLM botsMitigationStep 1: Stop the bleedingStep 2: Disable Image OptimizationStep 3: robots.txtUser agents of LLM botsUser agents of search engine crawlersUser agents of SEO botsHow do we prevent this in the future?Continue with a sensitive spend limitMindset for scaleReady for defenseSocial media responseParting thoughtsUPD: Vercel changed their image optimization pricingTL;DR On Friday, Feb 7, 2025 we had an incident with our Next.js web app hosted on Vercel that could've cost us $7,000 if we didn't notice it in time. We had a spike in LLM bot traffic coming from Amazonbot, Claudebot, Meta and an unknown bot. Together they sent 66.5k requests to our site within a single day. Bots scraped thousands of images that used Vercel's Image Optimization API, which cost us $5 per 1k images. The misconfiguration on our side combined with the aggressive bot traffic created an economically risky situation for our tiny bootstrapped startup. Context Metacast is a podcast tech startup. Our main product is a podcast app for iOS and Android. For every podcast episode on the platform, our web app has a web page. Our platform has ~1.4M episodes, which means we have 1.4M web pages that are discoverable by crawlers. These pages are generated server-side at request time, then cached. How we discovered the problem Step 1: A cost spike First, we received a cost alert from Vercel saying that we've hit 50% of the budget for resources metered by usage. Step 2: Image Optimization API usage spike We looked into it and saw that it's driven by the Image Optimization API, which peaked on Feb 7. Every page in the podcast directory has an image of a podcast cover (source image dimensions are 3000x3000px). With Image Optimization, podcast covers were reduced to 1/10th of the size, then cached. Image Optimization made the web app really snappy. It worked like a charm, except it turned out to be very expensive. Vercel charges $5 for every 1,000 images optimized. With thousands of requests coming our way, we were accumulating cost at the rate of $5 per each 1k image requests. In the worst case scenario, if all 1.4M images were crawled we'd hypothetically be looking at a $7k bill from Vercel. Step 3: Tens of thousands of requests from LLM bots We looked at the user agents of requests in the Firewall in Vercel and saw Amazonbot, ClaudeBot, meta_externalagent and an unknown bot disguising itself as a browser. We can't say definitively which bots were downloading images, because we are on the Pro plan on Vercel and no longer have access to logs from Friday. We only know that it was bot traffic. Mitigation Step 1: Stop the bleeding Both of us used to work at AWS where we internalized the golden rule of incident recovery - stop the bleeding first, do a long-term fix later. We configured firewall rules in Vercel to block bots from Amazon, Anthropic, OpenAI and Meta. To be fair, OpenAI didn't crawl our site, but we blocked it as a preventative measure. Step 2: Disable Image Optimization First, we disabled image optimization by adding an unoptimized property to podcast images in Next.js. Our reasoning was that users accessing the pages will get the latest version of the page with unoptimized images. We didn't consider that: Bots had already crawled thousands of pages and would crawl the optimized images using the URLs they extracted from the \"old\" HTML. Our site enabled image optimization for all external hosts. The latter is the most embarrassing part of the story. We missed an obvious exploit in the web app. const nextConfig = { images: { remotePatterns: [ { protocol: 'https', hostname: '**', }, { protocol: 'http', hostname: '**', }, ], }, ... To explain why we did this in the first place, we need to add some important context about podcasting. We do not own the podcast content displayed on our site. Similar to other podcast apps like Apple and Spotify, we ingest podcast information from RSS feeds and display it in our directory. The cover images are hosted on specialized podcast hosting platforms like Transistor, Buzzsprout, and others. But podcasts could be hosted anywhere from a WordPress website to an S3 bucket. It is impractical to allowlist all possible hosts. Optimizing an image meant that Next.js downloaded the image from one of those hosts to Vercel first, optimized it, then served to the users. If we wanted to make our site snappy, we had to either build and maintain an image optimization pipeline ourselves or use the built-in capability. As a scrappy startup for whom a web app was at best secondary, we chose the faster route without thinking much about it. In retrospect, we should've researched how it works. We're lucky no one started using our site as an image optimization API. To mitigate the problem entirely, we disabled image optimization for any external URLs. Now, image optimization is only enabled for images hosted on our own domain. Podcast covers load noticeably slower. We'll need to do something about it eventually. But this is not all. Step 3: robots.txt Of course, we knew about robots.txt, a file that tells crawlers whether they're allowed to crawl the site or not. Since both of us were new to managing a large-scale content site (our background is in backends, APIs, and web apps behind auth), we didn't even think about LLM bots. It's just not something that was on our radar. So, our robots.txt was a simple allow-all except for a few paths that we disallowed. Our first reaction was to disable all bot traffic except Google. But when we understood that the root cause of the problem lied in the misconfigured image optimization, we decided to keep our site open to all LLM and search engine bots. Serving the text content doesn't cost us much, but we may benefit from being shown as a source of data in LLMs, which would be similar to being shown on a search engine results page (SERP). We generate robots.txt programmatically using robots.ts in Next.js. We researched the bots and added their user agents to our code. If we ever need to disable any of the bots, we can do so very quickly now. While we were at it, we disabled some paths for SEO bots like Semrush and MJ12Bot. Note that robots.txt only works if bots respect it. It's honor-based system and there are still bad bots out there that ignore it and/or attempt to disguise themselves as users. User agents of LLM bots User Agent Link Amazonbot Amazon CCBot Common Crawl ClaudeBot Anthropic GPTBot OpenAI Meta-ExternalAgent Meta PerplexityBot Perplexity User agents of search engine crawlers User Agent Link Applebot Apple Baiduspider Baidu Bingbot Bing ChatGPT-User \u0026 OAI-SearchBot OpenAI DuckDuckBot DuckDuckGo Googlebot Google ImageSift ImageSift by Hive Perplexity‑User Perplexity YandexBot Yandex User agents of SEO bots User Agent Link AhrefsBot Ahrefs DataForSeoBot DataForSeoBot DotBot DotBot MJ12bot MS12Bot SemrushBot Semrush How do we prevent this in the future? We will start with the one thing we've done well. Continue with a sensitive spend limit We had a very sensitive spend limit alert. We knew we should not be spending much on Vercel, so we set it very low. When it triggered, we knew something was off. This may be the most important lesson to all startups and big enterprises alike - always set spend limits for your infrastructure, or the bill may ruin you. You can probably negotiate with Vercel, AWS, GCP, etc. and they'll reduce or forgive your bill. But it's best to not put yourself in a situation where you have to ask for a favor. Mindset for scale We've learned a ton and have (hopefully) attuned ourselves to: The scale we're operating at – we're serving millions of pages and need to be prepared for user traffic at that scale. The bots gave us a taste for what it would've been like had our app gone viral. The scale of web crawlers, both good and bad – we need to be prepared to be \"anthropized\", \"openAIed\", \"amazoned\", or \"semrushed.\" It's the new slasdot effect but without the benefit of immediate gratification. Ready for defense We've now better understood the options we have for firewalling ourselves from bots if we have to do so in the future. We can use Vercel firewall as the first line of defense or add a more advanced WAF from Cloudflare if things get dire. See this post from Cloudflare: Declare your AIndependence: block AI bots, scrapers and crawlers with a single click When we discovered the rate at which bots were crawling our site, we posted about it on LinkedIn. We were just sharing what's going on in real time, but boy did it hit the nerve. Almost 400k impressions, 2.4k likes, 270+ comments, 120+ reposts. We've gone through all comments on the post and responded to most of them. Lots of folks offered solutions like CloudFlare, using middleware, rate limiting, etc. Some offered to feed junk back to LLM bots. We learned about tarpit tools like iocaine and Nepenthes. You could lure them into a honeypot? Like nepenthes or locaine. If you feel like poisoning the ai well People rightfully pointed out that you can get ruined by infinite scalability of cloud resources. that's my biggest concern about cloud providers. You make a small mistake (everyone does) and the costs can skyrocket overnight. We learned that some people aren't aware of the LLM bot crawling activity or the scale of it. They thanked us for raising awareness. WOW - thanks for alerting us. Some people had been surprised by bots just like we were. Same here. At first I was super excited to get so many new subscriptions. We did reCaptcha and Cloudflare. Things have quieted down. Thanks for posting. I thought we were the only ones Some aren't surprised at all and see it as a problem. Very recognizable (unfortunately). These (predominantly AI) bots started noticeably hitting our platform back in May/June 2024. Lots of time \u0026 efforts wasted to keep our bills in check. We also found out that not all of them respect Robots.txt, so indeed a WAF is needed as well. I can(not) imagine how painful this must/will be for smaller businesses... Some people blamed us for not being prepared and called us out on calling out AI companies. Others defended us. Virality is a double-edged sword. A large portion of the comments were claiming that data scraping is unethical, illegal, etc. People were outraged. It wasn't our intention, but our post brought the issue to the zeitgeist of that day. Parting thoughts There's a part of me that is glad that this happened. We got a taste of operating a web app at scale before reaching scale. It was easy to block bots, but had it been caused by user traffic, we'd have to swallow the cost or downgrade the experience. Bots were the canaries in a coalmine. Any technology has negative externalities. Some are obvious, some aren't. Of all the things that were happening, I was worried that we'd get penalized by podcast hosters whose endpoints we were hitting at the same rate as bots requested images from our site. Operating at scale on the internet is a game of defense We can rant about bots as much as we want, but that is the reality we operate in. So we better acknowledge it and deal with it. P.S. We'll be discussing this topic on the next episode of the Metacast: Behind the scenes podcast. Follow us wherever you listen to podcasts to hear the story with more nuance. UPD: Vercel changed their image optimization pricing On Feb 18, 2025, just a few days after we published this blog post, Vercel changed their image optimization pricing. With the new pricing we'd not have faced a huge bill. However, this wouldn't address the problem that we need to optimize images hosted outside of our domain. We ended up implementing our own image optimization.",
  "image": "https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/og-postmortem-llm-bots-image-optimization.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cp\u003eA misconfiguration that might have cost us $7,000\u003c/p\u003e\u003cp\u003e\u003cimg alt=\"LLM bots + Next.js image optimization = recipe for bankruptcy (post-mortem)\" loading=\"lazy\" width=\"1200\" height=\"630\" decoding=\"async\" data-nimg=\"1\" sizes=\"100vw\" srcset=\"https://metacast.app/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png\u0026amp;w=640\u0026amp;q=75 640w, https://metacast.app/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png\u0026amp;w=750\u0026amp;q=75 750w, https://metacast.app/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png\u0026amp;w=828\u0026amp;q=75 828w, https://metacast.app/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png\u0026amp;w=1080\u0026amp;q=75 1080w, https://metacast.app/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png\u0026amp;w=1200\u0026amp;q=75 1200w, https://metacast.app/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png\u0026amp;w=1920\u0026amp;q=75 1920w, https://metacast.app/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png\u0026amp;w=2048\u0026amp;q=75 2048w, https://metacast.app/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png\u0026amp;w=3840\u0026amp;q=75 3840w\" src=\"https://metacast.app/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png\u0026amp;w=3840\u0026amp;q=75\"/\u003e\u003c/p\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Author\u0026#39;s picture\" loading=\"lazy\" width=\"100\" height=\"100\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://metacast.app/_next/image?url=%2Fimages%2Fpeople%2Filya-avatar.jpg\u0026amp;w=128\u0026amp;q=75 1x, https://metacast.app/_next/image?url=%2Fimages%2Fpeople%2Filya-avatar.jpg\u0026amp;w=256\u0026amp;q=75 2x\" src=\"https://metacast.app/_next/image?url=%2Fimages%2Fpeople%2Filya-avatar.jpg\u0026amp;w=256\u0026amp;q=75\"/\u003e\u003c/p\u003e\u003cp\u003eIlya Bezdelev\u003c/p\u003e\u003c/div\u003e\u003cp\u003ePublished on \u003ctime datetime=\"2025-02-10T13:33:07.000Z\"\u003eFebruary\t10, 2025\u003c/time\u003e\u003c/p\u003e\u003c/div\u003e\u003chr/\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003eTable of Contents\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003ca href=\"#tldr\"\u003eTL;DR\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#context\"\u003eContext\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#how-we-discovered-the-problem\"\u003eHow we discovered the problem\u003c/a\u003e\u003col\u003e\u003cli\u003e\u003ca href=\"#step-1-a-cost-spike\"\u003eStep 1: A cost spike\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#step-2-image-optimization-api-usage-spike\"\u003eStep 2: Image Optimization API usage spike\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#step-3-tens-of-thousands-of-requests-from-llm-bots\"\u003eStep 3: Tens of thousands of requests from LLM bots\u003c/a\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#mitigation\"\u003eMitigation\u003c/a\u003e\u003col\u003e\u003cli\u003e\u003ca href=\"#step-1-stop-the-bleeding\"\u003eStep 1: Stop the bleeding\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#step-2-disable-image-optimization\"\u003eStep 2: Disable Image Optimization\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#step-3-robotstxt\"\u003eStep 3: robots.txt\u003c/a\u003e\u003col\u003e\u003cli\u003e\u003ca href=\"#user-agents-of-llm-bots\"\u003eUser agents of LLM bots\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#user-agents-of-search-engine-crawlers\"\u003eUser agents of search engine crawlers\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#user-agents-of-seo-bots\"\u003eUser agents of SEO bots\u003c/a\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#how-do-we-prevent-this-in-the-future\"\u003eHow do we prevent this in the future?\u003c/a\u003e\u003col\u003e\u003cli\u003e\u003ca href=\"#continue-with-a-sensitive-spend-limit\"\u003eContinue with a sensitive spend limit\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#mindset-for-scale\"\u003eMindset for scale\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#ready-for-defense\"\u003eReady for defense\u003c/a\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#social-media-response\"\u003eSocial media response\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#parting-thoughts\"\u003eParting thoughts\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#upd-vercel-changed-their-image-optimization-pricing\"\u003eUPD: Vercel changed their image optimization pricing\u003c/a\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003ch2 id=\"tldr\"\u003e\u003ca href=\"#tldr\"\u003eTL;DR\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eOn Friday, Feb 7, 2025 we had an incident with our Next.js web app hosted on Vercel that could\u0026#39;ve cost us $7,000 if we didn\u0026#39;t notice it in time.\u003c/p\u003e\n\u003cp\u003eWe had a spike in LLM bot traffic coming from Amazonbot, Claudebot, Meta and an unknown bot. Together they sent 66.5k requests to our site within a single day. Bots scraped thousands of images that used Vercel\u0026#39;s Image Optimization API, which cost us $5 per 1k images.\u003c/p\u003e\n\u003cp\u003eThe misconfiguration on our side combined with the aggressive bot traffic created an economically risky situation for our tiny bootstrapped startup.\u003c/p\u003e\n\u003ch2 id=\"context\"\u003e\u003ca href=\"#context\"\u003eContext\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eMetacast is a podcast tech startup. Our main product is a \u003ca href=\"https://metacast.app\"\u003epodcast app\u003c/a\u003e for iOS and Android.\u003c/p\u003e\n\u003cp\u003eFor every podcast episode on the platform, our web app has a web page. Our platform has ~1.4M episodes, which means we have 1.4M web pages that are discoverable by crawlers. These pages are generated server-side at request time, then cached.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/lex-podcast-page-metacast.png\" alt=\"Lex Fridman Podcast\"/\u003e\u003c/p\u003e\n\u003ch2 id=\"how-we-discovered-the-problem\"\u003e\u003ca href=\"#how-we-discovered-the-problem\"\u003eHow we discovered the problem\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"step-1-a-cost-spike\"\u003e\u003ca href=\"#step-1-a-cost-spike\"\u003eStep 1: A cost spike\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFirst, we received a cost alert from Vercel saying that we\u0026#39;ve hit 50% of the budget for resources metered by usage.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-cost-alert.png\" alt=\"Vercel cost alert\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"step-2-image-optimization-api-usage-spike\"\u003e\u003ca href=\"#step-2-image-optimization-api-usage-spike\"\u003eStep 2: Image Optimization API usage spike\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eWe looked into it and saw that it\u0026#39;s driven by the Image Optimization API, which peaked on Feb 7.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-image-optimization-usage-summary.jpg\" alt=\"Image Optimization Usage\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-image-optimization-chart.jpg\" alt=\"Image Optimization Usage Chart\"/\u003e\u003c/p\u003e\n\u003cp\u003eEvery page in the podcast directory has an image of a podcast cover (source image dimensions are 3000x3000px). With Image Optimization, podcast covers were reduced to 1/10th of the size, then cached. Image Optimization made the web app really snappy. It worked like a charm, except it turned out to be very expensive.\u003c/p\u003e\n\u003cp\u003eVercel \u003ca href=\"https://vercel.com/docs/image-optimization/limits-and-pricing\"\u003echarges\u003c/a\u003e $5 for every 1,000 images optimized. With thousands of requests coming our way, we were accumulating cost at the rate of $5 per each 1k image requests. In the worst case scenario, if all 1.4M images were crawled we\u0026#39;d hypothetically be looking at a $7k bill from Vercel.\u003c/p\u003e\n\u003ch3 id=\"step-3-tens-of-thousands-of-requests-from-llm-bots\"\u003e\u003ca href=\"#step-3-tens-of-thousands-of-requests-from-llm-bots\"\u003eStep 3: Tens of thousands of requests from LLM bots\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eWe looked at the user agents of requests in the Firewall in Vercel and saw Amazonbot, ClaudeBot, meta_externalagent and an unknown bot disguising itself as a browser.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-user-agent-stats.png\" alt=\"User Agents\"/\u003e\u003c/p\u003e\n\u003cp\u003eWe can\u0026#39;t say definitively which bots were downloading images, because we are on the Pro plan on Vercel and no longer have access to logs from Friday. We only know that it was bot traffic.\u003c/p\u003e\n\u003ch2 id=\"mitigation\"\u003e\u003ca href=\"#mitigation\"\u003eMitigation\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"step-1-stop-the-bleeding\"\u003e\u003ca href=\"#step-1-stop-the-bleeding\"\u003eStep 1: Stop the bleeding\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eBoth of us used to work at AWS where we internalized the golden rule of incident recovery - \u003cstrong\u003estop the bleeding first, do a long-term fix later\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eWe configured firewall rules in Vercel to block bots from Amazon, Anthropic, OpenAI and Meta. To be fair, OpenAI didn\u0026#39;t crawl our site, but we blocked it as a preventative measure.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-firewall-rules.jpg\" alt=\"Firewall rules\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"step-2-disable-image-optimization\"\u003e\u003ca href=\"#step-2-disable-image-optimization\"\u003eStep 2: Disable Image Optimization\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFirst, we disabled image optimization by adding an \u003ccode\u003eunoptimized\u003c/code\u003e property to podcast images in Next.js. Our reasoning was that users accessing the pages will get the latest version of the page with unoptimized images.\u003c/p\u003e\n\u003cp\u003eWe didn\u0026#39;t consider that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBots had already crawled thousands of pages and would crawl the \u003cem\u003eoptimized\u003c/em\u003e images using the URLs they extracted from the \u0026#34;old\u0026#34; HTML.\u003c/li\u003e\n\u003cli\u003eOur site enabled image optimization for all external hosts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe latter is the most embarrassing part of the story. We missed an obvious exploit in the web app.\u003c/p\u003e\n\u003cfigure data-rehype-pretty-code-figure=\"\"\u003e\u003cpre tabindex=\"0\" data-language=\"javascript\" data-theme=\"github-dark-dimmed github-light\"\u003e\u003ccode data-language=\"javascript\" data-theme=\"github-dark-dimmed github-light\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e nextConfig\u003c/span\u003e\u003cspan\u003e =\u003c/span\u003e\u003cspan\u003e {\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e  images: {\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e    remotePatterns: [\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e      {\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e        protocol: \u003c/span\u003e\u003cspan\u003e\u0026#39;https\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e        hostname: \u003c/span\u003e\u003cspan\u003e\u0026#39;**\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e      },\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e      {\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e        protocol: \u003c/span\u003e\u003cspan\u003e\u0026#39;http\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e        hostname: \u003c/span\u003e\u003cspan\u003e\u0026#39;**\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e      },\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e    ],\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e  },\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan\u003e  ...\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/figure\u003e\n\u003cp\u003eTo explain why we did this in the first place, we need to add some important context about podcasting.\u003c/p\u003e\n\u003cp\u003eWe do not own the podcast content displayed on our site. Similar to other podcast apps like Apple and Spotify, we ingest podcast information from RSS feeds and display it in our directory. The cover images are hosted on specialized podcast hosting platforms like \u003ca href=\"https://transistor.fm/\"\u003eTransistor\u003c/a\u003e, \u003ca href=\"https://www.buzzsprout.com/\"\u003eBuzzsprout\u003c/a\u003e, and others. But podcasts could be hosted anywhere from a WordPress website to an S3 bucket. It is impractical to allowlist all possible hosts.\u003c/p\u003e\n\u003cp\u003eOptimizing an image meant that Next.js downloaded the image from one of those hosts to Vercel first, optimized it, then served to the users. If we wanted to make our site snappy, we had to either build and maintain an image optimization pipeline ourselves or use the built-in capability. As a scrappy startup for whom a web app was at best secondary, we chose the faster route without thinking much about it.\u003c/p\u003e\n\u003cp\u003eIn retrospect, we should\u0026#39;ve researched how it works. We\u0026#39;re lucky no one started using our site as an image optimization API.\u003c/p\u003e\n\u003cp\u003eTo mitigate the problem entirely, we disabled image optimization for any external URLs. Now, image optimization is only enabled for images hosted on our own domain. Podcast covers load noticeably slower. We\u0026#39;ll need to do something about it eventually.\u003c/p\u003e\n\u003cp\u003eBut this is not all.\u003c/p\u003e\n\u003ch3 id=\"step-3-robotstxt\"\u003e\u003ca href=\"#step-3-robotstxt\"\u003eStep 3: robots.txt\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eOf course, we knew about \u003ccode\u003erobots.txt\u003c/code\u003e, a file that tells crawlers whether they\u0026#39;re allowed to crawl the site or not.\u003c/p\u003e\n\u003cp\u003eSince both of us were new to managing a large-scale content site (our background is in backends, APIs, and web apps behind auth), we didn\u0026#39;t even think about LLM bots. It\u0026#39;s just not something that was on our radar. So, our \u003ccode\u003erobots.txt\u003c/code\u003e was a simple allow-all except for a few paths that we disallowed.\u003c/p\u003e\n\u003cp\u003eOur first reaction was to disable all bot traffic except Google. But when we understood that the root cause of the problem lied in the misconfigured image optimization, we decided to keep our site open to all LLM and search engine bots. Serving the text content doesn\u0026#39;t cost us much, but we may benefit from being shown as a source of data in LLMs, which would be similar to being shown on a search engine results page (SERP).\u003c/p\u003e\n\u003cp\u003eWe generate \u003ccode\u003erobots.txt\u003c/code\u003e programmatically using \u003ca href=\"https://nextjs.org/docs/app/api-reference/file-conventions/metadata/robots\"\u003erobots.ts\u003c/a\u003e in Next.js. We researched the bots and added their user agents to our code. If we ever need to disable any of the bots, we can do so very quickly now. While we were at it, we disabled some paths for SEO bots like Semrush and MJ12Bot.\u003c/p\u003e\n\u003cp\u003eNote that \u003ccode\u003erobots.txt\u003c/code\u003e only works if bots respect it. It\u0026#39;s honor-based system and there are still bad bots out there that ignore it and/or attempt to disguise themselves as users.\u003c/p\u003e\n\u003ch4 id=\"user-agents-of-llm-bots\"\u003e\u003ca href=\"#user-agents-of-llm-bots\"\u003eUser agents of LLM bots\u003c/a\u003e\u003c/h4\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eUser Agent\u003c/th\u003e\n\u003cth\u003eLink\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eAmazonbot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://developer.amazon.com/amazonbot\"\u003eAmazon\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eCCBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://commoncrawl.org/faq\"\u003eCommon Crawl\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eClaudeBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://privacy.anthropic.com/en/articles/10023637-does-anthropic-crawl-data-from-the-web-and-how-can-site-owners-block-the-crawler\"\u003eAnthropic\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eGPTBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://platform.openai.com/docs/bots/\"\u003eOpenAI\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eMeta-ExternalAgent\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://developers.facebook.com/docs/sharing/webmasters/web-crawlers/\"\u003eMeta\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ePerplexityBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://docs.perplexity.ai/guides/bots\"\u003ePerplexity\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch4 id=\"user-agents-of-search-engine-crawlers\"\u003e\u003ca href=\"#user-agents-of-search-engine-crawlers\"\u003eUser agents of search engine crawlers\u003c/a\u003e\u003c/h4\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eUser Agent\u003c/th\u003e\n\u003cth\u003eLink\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eApplebot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://support.apple.com/en-us/119829\"\u003eApple\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eBaiduspider\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://www.baidu.com/search/robots_english.html\"\u003eBaidu\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eBingbot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://www.bing.com/webmasters/help/which-crawlers-does-bing-use-8c184ec0\"\u003eBing\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eChatGPT-User\u003c/code\u003e \u0026amp; \u003ccode\u003eOAI-SearchBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://platform.openai.com/docs/bots/\"\u003eOpenAI\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eDuckDuckBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://duckduckgo.com/duckduckgo-help-pages/results/duckduckbot/\"\u003eDuckDuckGo\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eGooglebot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://developers.google.com/search/docs/crawling-indexing/googlebot\"\u003eGoogle\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eImageSift\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://imagesift.com/about\"\u003eImageSift by Hive\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ePerplexity‑User\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://docs.perplexity.ai/guides/bots\"\u003ePerplexity\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eYandexBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://www.yandex.com/support/webmaster/robot-workings/user-agent.html\"\u003eYandex\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch4 id=\"user-agents-of-seo-bots\"\u003e\u003ca href=\"#user-agents-of-seo-bots\"\u003eUser agents of SEO bots\u003c/a\u003e\u003c/h4\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eUser Agent\u003c/th\u003e\n\u003cth\u003eLink\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eAhrefsBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://ahrefs.com/robot\"\u003eAhrefs\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eDataForSeoBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://dataforseo.com/dataforseo-bot\"\u003eDataForSeoBot\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eDotBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://moz.com/help/moz-procedures/crawlers/dotbot\"\u003eDotBot\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eMJ12bot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://mj12bot.com/\"\u003eMS12Bot\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eSemrushBot\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://www.semrush.com/bot/\"\u003eSemrush\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"how-do-we-prevent-this-in-the-future\"\u003e\u003ca href=\"#how-do-we-prevent-this-in-the-future\"\u003eHow do we prevent this in the future?\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe will start with the one thing we\u0026#39;ve done well.\u003c/p\u003e\n\u003ch3 id=\"continue-with-a-sensitive-spend-limit\"\u003e\u003ca href=\"#continue-with-a-sensitive-spend-limit\"\u003eContinue with a sensitive spend limit\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eWe had a very sensitive spend limit alert. We knew we should not be spending much on Vercel, so we set it very low. When it triggered, we knew something was off.\u003c/p\u003e\n\u003cp\u003eThis may be the most important lesson to all startups and big enterprises alike - always set spend limits for your infrastructure, or the bill may ruin you. You can probably negotiate with Vercel, AWS, GCP, etc. and they\u0026#39;ll reduce or forgive your bill. But it\u0026#39;s best to not put yourself in a situation where you have to ask for a favor.\u003c/p\u003e\n\u003ch3 id=\"mindset-for-scale\"\u003e\u003ca href=\"#mindset-for-scale\"\u003eMindset for scale\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eWe\u0026#39;ve learned a ton and have (hopefully) attuned ourselves to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eThe scale we\u0026#39;re operating at\u003c/strong\u003e – we\u0026#39;re serving millions of pages and need to be prepared for user traffic at that scale. The bots gave us a taste for what it would\u0026#39;ve been like had our app gone viral.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe scale of web crawlers, both good and bad\u003c/strong\u003e – we need to be prepared to be \u0026#34;anthropized\u0026#34;, \u0026#34;openAIed\u0026#34;, \u0026#34;amazoned\u0026#34;, or \u0026#34;semrushed.\u0026#34; It\u0026#39;s the new \u003ca href=\"https://en.wikipedia.org/wiki/Slashdot_effect\"\u003eslasdot effect\u003c/a\u003e but without the benefit of immediate gratification.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"ready-for-defense\"\u003e\u003ca href=\"#ready-for-defense\"\u003eReady for defense\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eWe\u0026#39;ve now better understood the options we have for firewalling ourselves from bots if we have to do so in the future. We can use Vercel firewall as the first line of defense or add a more advanced WAF from Cloudflare if things get dire.\u003c/p\u003e\n\u003cp\u003eSee this post from Cloudflare: \u003ca href=\"https://blog.cloudflare.com/declaring-your-aindependence-block-ai-bots-scrapers-and-crawlers-with-a-single-click/\"\u003eDeclare your AIndependence: block AI bots, scrapers and crawlers with a single click\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen we discovered the rate at which bots were crawling our site, we \u003ca href=\"https://www.linkedin.com/posts/ilyabezdelev_our-site-is-getting-totally-slammed-by-activity-7293596095778074624-MD_I\"\u003eposted about it\u003c/a\u003e on LinkedIn. We were just sharing what\u0026#39;s going on in real time, but boy did it hit the nerve. Almost 400k impressions, 2.4k likes, 270+ comments, 120+ reposts.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/linkedin-post.jpg\" alt=\"LinkedIn post stats\"/\u003e\u003c/p\u003e\n\u003cp\u003eWe\u0026#39;ve gone through all comments on the post and responded to most of them.\u003c/p\u003e\n\u003cp\u003eLots of folks offered solutions like \u003ca href=\"https://blog.cloudflare.com/declaring-your-aindependence-block-ai-bots-scrapers-and-crawlers-with-a-single-click/\"\u003eCloudFlare\u003c/a\u003e, using middleware, rate limiting, etc. Some offered to feed junk back to LLM bots.\u003c/p\u003e\n\u003cp\u003eWe learned about \u003ca href=\"https://arstechnica.com/tech-policy/2025/01/ai-haters-build-tarpits-to-trap-and-trick-ai-scrapers-that-ignore-robots-txt/\"\u003etarpit\u003c/a\u003e tools like \u003ca href=\"https://iocaine.madhouse-project.org/\"\u003eiocaine\u003c/a\u003e and \u003ca href=\"https://zadzmo.org/code/nepenthes/\"\u003eNepenthes\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eYou could lure them into a honeypot?\u003cbr/\u003e\nLike nepenthes or locaine.\u003cbr/\u003e\nIf you feel like poisoning the ai well\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003ePeople rightfully pointed out that you can get ruined by infinite scalability of cloud resources.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003ethat\u0026#39;s my biggest concern about cloud providers. You make a small mistake (everyone does) and the costs can skyrocket overnight.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe learned that some people aren\u0026#39;t aware of the LLM bot crawling activity or the scale of it. They thanked us for raising awareness.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eWOW - thanks for alerting us.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSome people had been surprised by bots just like we were.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eSame here. At first I was super excited to get so many new subscriptions. We did reCaptcha and Cloudflare. Things have quieted down. Thanks for posting. I thought we were the only ones\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSome aren\u0026#39;t surprised at all and see it as a problem.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eVery recognizable (unfortunately). These (predominantly AI) bots started noticeably hitting our platform back in May/June 2024. Lots of time \u0026amp; efforts wasted to keep our bills in check. We also found out that not all of them respect Robots.txt, so indeed a WAF is needed as well. I can(not) imagine how painful this must/will be for smaller businesses...\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSome people blamed us for not being prepared and called us out on calling out AI companies. Others defended us. Virality is a double-edged sword.\u003c/p\u003e\n\u003cp\u003eA large portion of the comments were claiming that data scraping is unethical, illegal, etc. People were outraged. It wasn\u0026#39;t our intention, but our post brought the issue to the zeitgeist of that day.\u003c/p\u003e\n\u003ch2 id=\"parting-thoughts\"\u003e\u003ca href=\"#parting-thoughts\"\u003eParting thoughts\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eThere\u0026#39;s a part of me that is glad that this happened.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe got a taste of operating a web app at scale before reaching scale. It was easy to block bots, but had it been caused by user traffic, we\u0026#39;d have to swallow the cost or downgrade the experience. Bots were the canaries in a coalmine.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAny technology has negative externalities.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSome are obvious, some aren\u0026#39;t. Of all the things that were happening, I was worried that we\u0026#39;d get penalized by podcast hosters whose endpoints we were hitting at the same rate as bots requested images from our site.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOperating at scale on the internet is a game of defense\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe can rant about bots as much as we want, but that \u003cem\u003eis\u003c/em\u003e the reality we operate in. So we better acknowledge it and deal with it.\u003c/p\u003e\n\u003cp\u003eP.S. We\u0026#39;ll be discussing this topic on the next episode of the \u003ca href=\"https://metacast.app/podcasts/7d7e381e-907c-5b22-aefc-1fc8311d2a71\"\u003eMetacast: Behind the scenes\u003c/a\u003e podcast. Follow us wherever you listen to podcasts to hear the story with more nuance.\u003c/p\u003e\n\u003ch2 id=\"upd-vercel-changed-their-image-optimization-pricing\"\u003e\u003ca href=\"#upd-vercel-changed-their-image-optimization-pricing\"\u003eUPD: Vercel changed their image optimization pricing\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eOn Feb 18, 2025, just a few days after we published this blog post, Vercel \u003ca href=\"https://vercel.com/changelog/faster-transformations-and-reduced-pricing-for-image-optimization\"\u003echanged\u003c/a\u003e their image optimization pricing. With the new pricing we\u0026#39;d not have faced a huge bill.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-new-pricing.webp\" alt=\"New Image Optimization pricing on Vercel\"/\u003e\u003c/p\u003e\n\u003cp\u003eHowever, this wouldn\u0026#39;t address the problem that we need to optimize images hosted outside of our domain. We ended up implementing our own image optimization.\u003c/p\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2025-02-10T13:33:07Z",
  "modifiedTime": null
}
