{
  "id": "1a2da286-db81-41a4-873e-73310f9a5921",
  "title": "From MIPS to exaflops in mere decades: Compute power is exploding, and it will transform AI",
  "link": "https://venturebeat.com/ai/mips-to-exaflops-in-just-40-years-compute-power-is-exploding-and-it-will-transform-ai/",
  "description": "For the first time, we may have the computing power and the intelligence to tackle problems with AI that were once beyond human reach.",
  "author": "Gary Grossman, Edelman",
  "published": "Sun, 06 Apr 2025 21:35:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "DataDecisionMakers",
    "AI, ML and Deep Learning",
    "category-/Computers \u0026 Electronics/Computer Hardware",
    "Conversational AI",
    "Generative AI",
    "large language models",
    "Nvidia"
  ],
  "byline": "Gary Grossman, Edelman",
  "length": 9834,
  "excerpt": "For the first time, we may have the computing power and the intelligence to tackle problems with AI that were once beyond human reach.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More At the recent Nvidia GTC conference, the company unveiled what it described as the first single-rack system of servers capable of one exaflop — one billion billion, or a quintillion, floating-point operations (FLOPS) per second. This breakthrough is based on the latest GB200 NVL72 system, which incorporates Nvidia’s latest Blackwell graphics processing units (GPUs). A standard computer rack is about 6 feet tall, a little more than 3 feet deep and less than 2 feet wide. Shrinking an exaflop: From Frontier to Blackwell A couple of things about the announcement struck me. First, the world’s first exaflop-capable computer was installed only a few years ago, in 2022, at Oak Ridge National Laboratory. For comparison, the “Frontier” supercomputer built by HPE and powered by AMD GPUs and CPUs, originally consisted of 74 racks of servers. The new Nvidia system has achieved roughly 73X greater performance density in just three years, equivalent to a tripling of performance every year. This advancement reflects remarkable progress in computing density, energy efficiency and architectural design. Secondly, it needs to be said that while both systems hit the exascale milestone, they are built for different challenges, one optimized for speed, the other for precision. Nvidia’s exaflop specification is based on lower-precision math — specifically 4-bit and 8-bit floating-point operations — considered optimal for AI workloads including tasks like training and running large language models (LLMs). These calculations prioritize speed over precision. By contrast, the exaflop rating for Frontier was achieved using 64-bit double-precision math, the gold standard for scientific simulations where accuracy is critical. We’ve come a long way (very quickly) This level of progress seems almost unbelievable, especially as I recall the state-of-the-art when I began my career in the computing industry. My first professional job was as a programmer on the DEC KL 1090. This machine, part of DEC’s PDP-10 series of timeshare mainframes, offered 1.8 million instructions per second (MIPS). Aside from its CPU performance, the machine connected to cathode ray tube (CRT) displays via hardwired cables. There were no graphics capabilities, just light text on a dark background. And of course, no Internet. Remote users connected over phone lines using modems running at speeds up to 1,200 bits per second. DEC System 10; Source: By Joe Mabel, CC BY-SA 3.0. 500 billion times more compute While comparing MIPS to FLOPS gives a general sense of progress, it is important to remember that these metrics measure different computing workloads. MIPS reflects integer processing speed, which is useful for general-purpose computing, particularly in business applications. FLOPS measures floating-point performance that is crucial for scientific workloads and the heavy number-crunching behind modern AI, such as the matrix math and linear algebra used to train and run machine learning (ML) models. While not a direct comparison, the sheer scale of the difference between MIPS then and FLOPS now provides a powerful illustration of the rapid growth in computing performance. Using these as a rough heuristic to measure work performed, the new Nvidia system is approximately 500 billion times more powerful than the DEC machine. That kind of leap exemplifies the exponential growth of computing power over a single professional career and raises the question: If this much progress is possible in 40 years, what might the next 5 bring? Nvidia, for its part, has offered some clues. At GTC, the company shared a roadmap predicting that its next-generation full-rack system based on the “Vera Rubin” Ultra architecture will deliver 14X the performance of the Blackwell Ultra rack shipping this year, reaching somewhere between 14 and 15 exaflops in AI-optimized work in the next year or two. Just as notable is the efficiency. Achieving this level of performance in a single rack means less physical space per unit of work, fewer materials and potentially lower energy use per operation, although the absolute power demands of these systems remain immense. Does AI really need all that compute power? While such performance gains are indeed impressive, the AI industry is now grappling with a fundamental question: How much computing power is truly necessary and at what cost? The race to build massive new AI data centers is being driven by the growing demands of exascale computing and ever-more capable AI models. The most ambitious effort is the $500 billion Project Stargate, which envisions 20 data centers across the U.S., each spanning half a million square feet. A wave of other hyperscale projects is either underway or in planning stages around the world, as companies and countries scramble to ensure they have the infrastructure to support the AI workloads of tomorrow. Some analysts now worry that we may be overbuilding AI data center capacity. Concern intensified after the release of R1, a reasoning model from China’s DeepSeek that requires significantly less compute than many of its peers. Microsoft later canceled leases with multiple data center providers, sparking speculation that it might be recalibrating its expectations for future AI infrastructure demand. However, The Register suggested that this pullback may have more to do with some of the planned AI data centers not having sufficiently robust ability to support the power and cooling needs of next-gen AI systems. Already, AI models are pushing the limits of what present infrastructure can support. MIT Technology Review reported that this may be the reason many data centers in China are struggling and failing, having been built to specifications that are not optimal for the present need, let alone those of the next few years. AI inference demands more FLOPs Reasoning models perform most of their work at runtime through a process known as inference. These models power some of the most advanced and resource-intensive applications today, including deep research assistants and the emerging wave of agentic AI systems. While DeepSeek-R1 initially spooked the industry into thinking that future AI might require less computing power, Nvidia CEO Jensen Huang pushed back hard. Speaking to CNBC, he countered this perception: “It was the exact opposite conclusion that everybody had.” He added that reasoning AI consumes 100X more computing than non-reasoning AI. As AI continues to evolve from reasoning models to autonomous agents and beyond, demand for computing is likely to surge once again. The next breakthroughs may come not just in language or vision, but in AI agent coordination, fusion simulations or even large-scale digital twins, each made possible by the kind of computing ability leap we have just witnessed. Seemingly right on cue, OpenAI just announced $40 billion in new funding, the largest private tech funding round on record. The company said in a blog post that the funding “enables us to push the frontiers of AI research even further, scale our compute infrastructure and deliver increasingly powerful tools for the 500 million people who use ChatGPT every week.” Why is so much capital flowing into AI? The reasons range from competitiveness to national security. Although one particular factor stands out, as exemplified by a McKinsey headline: “AI could increase corporate profits by $4.4 trillion a year.” What comes next? It’s anybody’s guess At their core, information systems are about abstracting complexity, whether through an emergency vehicle routing system I once wrote in Fortran, a student achievement reporting tool built in COBOL, or modern AI systems accelerating drug discovery. The goal has always been the same: To make greater sense of the world. Now, with powerful AI beginning to appear, we are crossing a threshold. For the first time, we may have the computing power and the intelligence to tackle problems that were once beyond human reach. New York Times columnist Kevin Roose recently captured this moment well: “Every week, I meet engineers and entrepreneurs working on AI who tell me that change — big change, world-shaking change, the kind of transformation we’ve never seen before — is just around the corner.” And that does not even count the breakthroughs that arrive each week. Just in the past few days, we’ve seen OpenAI’s GPT-4o generate nearly perfect images from text, Google release what may be the most advanced reasoning model yet in Gemini 2.5 Pro and Runway unveil a video model with shot-to-shot character and scene consistency, something VentureBeat notes has eluded most AI video generators until now. What comes next is truly a guess. We do not know whether powerful AI will be a breakthrough or breakdown, whether it will help solve fusion energy or unleash new biological risks. But with ever more FLOPS coming online over the next five years, one thing seems certain: Innovation will come fast — and with force. It is clear, too, that as FLOPS scale, so must our conversations about responsibility, regulation and restraint. Gary Grossman is EVP of technology practice at Edelman and global lead of the Edelman AI Center of Excellence. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/04/GG1.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eAt the recent Nvidia GTC conference, the company unveiled what it described as the first single-rack system of servers capable of one exaflop — one billion billion, or a quintillion, floating-point operations (FLOPS) per second. This breakthrough is based on the latest GB200 NVL72 system, which incorporates Nvidia’s latest Blackwell graphics processing units (GPUs). A standard computer rack is about 6 feet tall, a little more than 3 feet deep and less than 2 feet wide.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-shrinking-an-exaflop-from-frontier-to-blackwell\"\u003eShrinking an exaflop: From Frontier to Blackwell\u003c/h2\u003e\n\n\n\n\u003cp\u003eA couple of things about the announcement struck me. First, the world’s first exaflop-capable computer was installed only a few years ago, in 2022, at Oak Ridge National Laboratory. For comparison, the “Frontier” supercomputer built by HPE and powered by AMD GPUs and CPUs, originally consisted of 74 racks of servers. The new Nvidia system has achieved roughly 73X greater performance density in just three years, equivalent to a tripling of performance every year. This advancement reflects remarkable progress in computing density, energy efficiency and architectural design.\u003c/p\u003e\n\n\n\n\u003cp\u003eSecondly, it needs to be said that while both systems hit the exascale milestone, they are built for different challenges, one optimized for speed, the other for precision. Nvidia’s exaflop specification is based on lower-precision math — specifically 4-bit and 8-bit floating-point operations — considered optimal for AI workloads including tasks like training and running large language models (LLMs). These calculations prioritize speed over precision. By contrast, the exaflop rating for Frontier was achieved using 64-bit double-precision math, the gold standard for scientific simulations where accuracy is critical.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-we-ve-come-a-long-way-very-quickly\"\u003eWe’ve come a long way (very quickly)\u003c/h2\u003e\n\n\n\n\u003cp\u003eThis level of progress seems almost unbelievable, especially as I recall the state-of-the-art when I began my career in the computing industry. My first professional job was as a programmer on the DEC KL 1090. This machine, part of DEC’s PDP-10 series of timeshare mainframes, offered 1.8 million instructions per second (MIPS). Aside from its CPU performance, the machine connected to cathode ray tube (CRT) displays via hardwired cables. There were no graphics capabilities, just light text on a dark background. And of course, no Internet. Remote users connected over phone lines using modems running at speeds up to 1,200 bits per second.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"936\" height=\"622\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image2.jpg?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image2.jpg 936w, https://venturebeat.com/wp-content/uploads/2025/04/image2.jpg?resize=300,200 300w, https://venturebeat.com/wp-content/uploads/2025/04/image2.jpg?resize=768,510 768w, https://venturebeat.com/wp-content/uploads/2025/04/image2.jpg?resize=800,532 800w, https://venturebeat.com/wp-content/uploads/2025/04/image2.jpg?resize=400,266 400w, https://venturebeat.com/wp-content/uploads/2025/04/image2.jpg?resize=750,498 750w, https://venturebeat.com/wp-content/uploads/2025/04/image2.jpg?resize=578,384 578w, https://venturebeat.com/wp-content/uploads/2025/04/image2.jpg?resize=930,618 930w\" sizes=\"(max-width: 936px) 100vw, 936px\"/\u003e\u003cfigcaption\u003e\u003cem\u003e\u003ca href=\"https://commons.wikimedia.org/w/index.php?curid=31997651\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDEC System 10\u003c/a\u003e; Source: By Joe Mabel, CC BY-SA 3.0. \u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-500-billion-times-more-compute\"\u003e500 billion times more compute\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile comparing MIPS to FLOPS gives a general sense of progress, it is important to remember that these metrics measure different computing workloads. MIPS reflects integer processing speed, which is useful for general-purpose computing, particularly in business applications. FLOPS measures floating-point performance that is crucial for scientific workloads and the heavy number-crunching behind modern AI, such as the matrix math and linear algebra used to train and run machine learning (ML) models.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile not a direct comparison, the sheer scale of the difference between MIPS then and FLOPS now provides a powerful illustration of the rapid growth in computing performance. Using these as a rough heuristic to measure work performed, the new Nvidia system is approximately 500 billion times more powerful than the DEC machine. That kind of leap exemplifies the exponential growth of computing power over a single professional career and raises the question: If this much progress is possible in 40 years, what might the next 5 bring?\u003c/p\u003e\n\n\n\n\u003cp\u003eNvidia, for its part, has offered some clues. At GTC, the company shared a roadmap predicting that its next-generation full-rack system based on the “Vera Rubin” Ultra architecture will deliver 14X the performance of the Blackwell Ultra rack shipping this year, reaching somewhere between 14 and 15 exaflops in AI-optimized work in the next year or two.\u003c/p\u003e\n\n\n\n\u003cp\u003eJust as notable is the efficiency. Achieving this level of performance in a single rack means less physical space per unit of work, fewer materials and potentially lower energy use per operation, although the absolute power demands of these systems remain immense.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-does-ai-really-need-all-that-compute-power\"\u003eDoes AI really need all that compute power?\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile such performance gains are indeed impressive, the AI industry is now grappling with a fundamental question: How much computing power is truly necessary and at what cost? The race to build massive new AI data centers is being driven by the growing demands of exascale computing and ever-more capable AI models.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe most ambitious effort is the $500 billion Project Stargate, which envisions 20 data centers across the U.S., each spanning half a million square feet. A wave of other hyperscale projects is either underway or in planning stages around the world, as companies and countries scramble to ensure they have the infrastructure to support the AI workloads of tomorrow.\u003c/p\u003e\n\n\n\n\u003cp\u003eSome analysts now worry that we may be overbuilding AI data center capacity. Concern intensified after the release of R1, a reasoning model from China’s DeepSeek that requires significantly less compute than many of its peers. Microsoft later canceled leases with multiple data center providers, sparking speculation that it might be recalibrating its expectations for future AI infrastructure demand.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, \u003cem\u003eThe Register\u003c/em\u003e \u003ca href=\"https://www.theregister.com/2025/03/26/microsoft_ai_apocalypse/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esuggested\u003c/a\u003e that this pullback may have more to do with some of the planned AI data centers not having sufficiently robust ability to support the power and cooling needs of next-gen AI systems. Already, AI models are pushing the limits of what present infrastructure can support. MIT Technology Review \u003ca href=\"https://www.technologyreview.com/2025/03/26/1113802/china-ai-data-centers-unused/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ereported\u003c/a\u003e that this may be the reason many data centers in China are struggling and failing, having been built to specifications that are not optimal for the present need, let alone those of the next few years.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-ai-inference-demands-more-flops\"\u003eAI inference demands more FLOPs\u003c/h2\u003e\n\n\n\n\u003cp\u003eReasoning models perform most of their work at runtime through a process known as inference. These models power some of the most advanced and resource-intensive applications today, including deep research assistants and the emerging wave of agentic AI systems.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile DeepSeek-R1 initially spooked the industry into thinking that future AI might require \u003cem\u003eless\u003c/em\u003e computing power, Nvidia CEO Jensen Huang pushed back hard. \u003ca href=\"https://www.cnbc.com/2025/03/19/nvidia-ceo-jensen-huang-why-deepseek-model-needs-100-times-more-computing.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpeaking\u003c/a\u003e to CNBC, he countered this perception: “It was the exact opposite conclusion that everybody had.” He added that reasoning AI consumes 100X more computing than non-reasoning AI.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs AI continues to evolve from reasoning models to autonomous agents and beyond, demand for computing is likely to surge once again. The next breakthroughs may come not just in language or vision, but in AI agent coordination, fusion simulations or even large-scale digital twins, each made possible by the kind of computing ability leap we have just witnessed.\u003c/p\u003e\n\n\n\n\u003cp\u003eSeemingly right on cue, OpenAI just announced \u003ca href=\"https://venturebeat.com/ai/40b-into-the-furnace-as-openai-adds-a-million-users-an-hour-the-race-for-enterprise-ai-dominance-hits-a-new-gear/\"\u003e$40 billion in new funding\u003c/a\u003e, the largest private tech funding round on record. The company said in a \u003ca href=\"https://openai.com/index/march-funding-updates/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eblog post\u003c/a\u003e that the funding “enables us to push the frontiers of AI research even further, scale our compute infrastructure and deliver increasingly powerful tools for the 500 million people who use ChatGPT every week.”\u003c/p\u003e\n\n\n\n\u003cp\u003eWhy is so much capital flowing into AI? The reasons range from competitiveness to national security. Although one particular factor stands out, as exemplified by a McKinsey \u003ca href=\"https://www.mckinsey.com/mgi/overview/in-the-news/ai-could-increase-corporate-profits-by-4-trillion-a-year-according-to-new-research\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eheadline\u003c/a\u003e: “AI could increase corporate profits by $4.4 trillion a year.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-comes-next-it-s-anybody-s-guess\"\u003eWhat comes next? It’s anybody’s guess\u003c/h2\u003e\n\n\n\n\u003cp\u003eAt their core, information systems are about abstracting complexity, whether through an emergency vehicle routing system I once wrote in Fortran, a student achievement reporting tool built in COBOL, or modern AI systems accelerating drug discovery. The goal has always been the same: To make greater sense of the world.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, with powerful AI beginning to appear, we are crossing a threshold. For the first time, we may have the computing power and the intelligence to tackle problems that were once beyond human reach.\u003c/p\u003e\n\n\n\n\u003cp\u003eNew York Times columnist Kevin Roose \u003ca href=\"https://www.nytimes.com/2025/03/14/technology/why-im-feeling-the-agi.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003erecently captured this moment well\u003c/a\u003e: “Every week, I meet engineers and entrepreneurs working on AI who tell me that change — big change, world-shaking change, the kind of transformation we’ve never seen before — is just around the corner.” And that does not even count the breakthroughs that arrive each week.\u003c/p\u003e\n\n\n\n\u003cp\u003eJust in the past few days, we’ve seen OpenAI’s GPT-4o generate \u003ca href=\"https://openai.com/index/introducing-4o-image-generation/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enearly perfect images\u003c/a\u003e from text, Google release what may be the most advanced \u003ca href=\"https://venturebeat.com/ai/googles-gemini-2-5-pro-is-the-smartest-model-youre-not-using-and-4-reasons-it-matters-for-enterprise-ai/\"\u003ereasoning model\u003c/a\u003e yet in Gemini 2.5 Pro and Runway unveil a video model with shot-to-shot character and scene consistency, something VentureBeat \u003ca href=\"https://venturebeat.com/ai/runways-gen-4-ai-solves-the-character-consistency-challenge-making-ai-filmmaking-actually-useful/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enotes\u003c/a\u003e has eluded most AI video generators until now.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhat comes next is truly a guess. We do not know whether powerful AI will be a breakthrough or breakdown, whether it will help solve fusion energy or unleash new biological risks. But with ever more FLOPS coming online over the next five years, one thing seems certain: Innovation will come fast — and with force. It is clear, too, that as FLOPS scale, so must our conversations about responsibility, regulation and restraint.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eGary Grossman is EVP of technology practice at \u003ca href=\"https://www.edelman.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eEdelman\u003c/a\u003e and global lead of the Edelman AI Center of Excellence. \u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-04-06T21:35:00Z",
  "modifiedTime": "2025-04-04T23:48:51Z"
}
