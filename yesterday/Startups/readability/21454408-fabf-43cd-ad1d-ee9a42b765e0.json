{
  "id": "21454408-fabf-43cd-ad1d-ee9a42b765e0",
  "title": "AI on your smartphone? Hugging Face’s SmolLM2 brings powerful models to the palm of your hand",
  "link": "https://venturebeat.com/ai/ai-on-your-smartphone-hugging-faces-smollm2-brings-powerful-models-to-the-palm-of-your-hand/",
  "description": "Hugging Face launches SmolLM2, a new family of compact AI language models that deliver impressive performance on mobile and edge devices, outperforming larger models like Meta’s LLaMA in key benchmarks.",
  "author": "Michael Nuñez",
  "published": "Fri, 01 Nov 2024 23:42:25 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Data Infrastructure",
    "AI benchmarks",
    "AI efficiency",
    "AI on smartphones",
    "AI, ML and Deep Learning",
    "Business Intelligence",
    "category-/Computers \u0026 Electronics/Programming",
    "category-/Science/Computer Science",
    "Compact AI models",
    "Conversational AI",
    "Data Management",
    "Data Science",
    "edge AI",
    "Edge Device AI",
    "Hugging Face",
    "Hugging Face AI models",
    "Hugging Face SmolLM",
    "Hugging Face SmolLM2",
    "Lightweight Language Models",
    "mobile AI",
    "Mobile AI performance",
    "NLP",
    "Small AI models",
    "SmolLM v2",
    "SmolLM2",
    "SmolLM2 vs Llama"
  ],
  "byline": "Michael Nuñez",
  "length": 4783,
  "excerpt": "Hugging Face launches SmolLM2, a new family of compact AI language models that deliver impressive performance on mobile and edge devices, outperforming larger models like Meta’s LLaMA in key benchmarks.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Hugging Face today has released SmolLM2, a new family of compact language models that achieve impressive performance while requiring far fewer computational resources than their larger counterparts. The new models, released under the Apache 2.0 license, come in three sizes — 135M, 360M and 1.7B parameters — making them suitable for deployment on smartphones and other edge devices where processing power and memory are limited. Most notably, the 1.7B parameter version outperforms Meta’s Llama 1B model on several key benchmarks. Performance comparison shows SmolLM2-1B outperforming larger rival models on most cognitive benchmarks, with particularly strong results in science reasoning and commonsense tasks. Credit: Hugging Face Small models pack a powerful punch in AI performance tests “SmolLM2 demonstrates significant advances over its predecessor, particularly in instruction following, knowledge, reasoning and mathematics,” according to Hugging Face’s model documentation. The largest variant was trained on 11 trillion tokens using a diverse dataset combination including FineWeb-Edu and specialized mathematics and coding datasets. This development comes at a crucial time when the AI industry is grappling with the computational demands of running large language models (LLMs). While companies like OpenAI and Anthropic push the boundaries with increasingly massive models, there’s growing recognition of the need for efficient, lightweight AI that can run locally on devices. The push for bigger AI models has left many potential users behind. Running these models requires expensive cloud computing services, which come with their own problems: slow response times, data privacy risks and high costs that small companies and independent developers simply can’t afford. SmolLM2 offers a different approach by bringing powerful AI capabilities directly to personal devices, pointing toward a future where advanced AI tools are within reach of more users and companies, not just tech giants with massive data centers. A comparison of AI language models shows SmolLM2’s superior efficiency, achieving higher performance scores with fewer parameters than larger rivals like Llama3.2 and Gemma, where the horizontal axis represents the model size and the vertical axis shows accuracy on benchmark tests. Credit: Hugging Face Edge computing gets a boost as AI moves to mobile devices SmolLM2’s performance is particularly noteworthy given its size. On the MT-Bench evaluation, which measures chat capabilities, the 1.7B model achieves a score of 6.13, competitive with much larger models. It also shows strong performance on mathematical reasoning tasks, scoring 48.2 on the GSM8K benchmark. These results challenge the conventional wisdom that bigger models are always better, suggesting that careful architecture design and training data curation may be more important than raw parameter count. The models support a range of applications including text rewriting, summarization and function calling. Their compact size enables deployment in scenarios where privacy, latency or connectivity constraints make cloud-based AI solutions impractical. This could prove particularly valuable in healthcare, financial services and other industries where data privacy is non-negotiable. Industry experts see this as part of a broader trend toward more efficient AI models. The ability to run sophisticated language models locally on devices could enable new applications in areas like mobile app development, IoT devices, and enterprise solutions where data privacy is paramount. The race for efficient AI: Smaller models challenge industry giants However, these smaller models still have limitations. According to Hugging Face’s documentation, they “primarily understand and generate content in English” and may not always produce factually accurate or logically consistent output. The release of SmolLM2 suggests that the future of AI may not solely belong to increasingly large models, but rather to more efficient architectures that can deliver strong performance with fewer resources. This could have significant implications for democratizing AI access and reducing the environmental impact of AI deployment. The models are available immediately through Hugging Face’s model hub, with both base and instruction-tuned versions offered for each size variant. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/11/2616a6ce-5645-48c3-bd5d-9a00f1dd0c9e.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://huggingface.co/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e today has released \u003ca href=\"https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSmolLM2\u003c/a\u003e, a new family of compact language models that achieve impressive performance while requiring far fewer computational resources than their larger counterparts.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe new models, released under the Apache 2.0 license, come in three sizes — \u003ca href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-135M\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e135M\u003c/a\u003e, \u003ca href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-360M\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e360M\u003c/a\u003e and \u003ca href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e1.7B\u003c/a\u003e parameters — making them suitable for deployment on smartphones and other edge devices where processing power and memory are limited. Most notably, the 1.7B parameter version outperforms Meta’s \u003ca href=\"https://huggingface.co/meta-llama/Llama-3.2-1B\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLlama 1B model\u003c/a\u003e on several key benchmarks.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1584\" height=\"1226\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/SmolLM2-Comparison-on-SOTA-Benchmarks.png?w=775\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/11/SmolLM2-Comparison-on-SOTA-Benchmarks.png 1584w, https://venturebeat.com/wp-content/uploads/2024/11/SmolLM2-Comparison-on-SOTA-Benchmarks.png?resize=300,232 300w, https://venturebeat.com/wp-content/uploads/2024/11/SmolLM2-Comparison-on-SOTA-Benchmarks.png?resize=768,594 768w, https://venturebeat.com/wp-content/uploads/2024/11/SmolLM2-Comparison-on-SOTA-Benchmarks.png?resize=775,600 775w, https://venturebeat.com/wp-content/uploads/2024/11/SmolLM2-Comparison-on-SOTA-Benchmarks.png?resize=1536,1189 1536w, https://venturebeat.com/wp-content/uploads/2024/11/SmolLM2-Comparison-on-SOTA-Benchmarks.png?resize=400,310 400w, https://venturebeat.com/wp-content/uploads/2024/11/SmolLM2-Comparison-on-SOTA-Benchmarks.png?resize=750,580 750w, https://venturebeat.com/wp-content/uploads/2024/11/SmolLM2-Comparison-on-SOTA-Benchmarks.png?resize=578,447 578w, https://venturebeat.com/wp-content/uploads/2024/11/SmolLM2-Comparison-on-SOTA-Benchmarks.png?resize=930,720 930w\" sizes=\"(max-width: 1584px) 100vw, 1584px\"/\u003e\u003cfigcaption\u003e\u003cem\u003ePerformance comparison shows SmolLM2-1B outperforming larger rival models on most cognitive benchmarks, with particularly strong results in science reasoning and commonsense tasks. Credit: Hugging Face\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-small-models-pack-a-powerful-punch-in-ai-performance-tests\"\u003eSmall models pack a powerful punch in AI performance tests\u003c/h2\u003e\n\n\n\n\u003cp\u003e“SmolLM2 demonstrates significant advances over its predecessor, particularly in instruction following, knowledge, reasoning and mathematics,” according to Hugging Face’s \u003ca href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emodel documentation\u003c/a\u003e. The largest variant was trained on 11 trillion tokens using a diverse dataset combination including \u003ca href=\"https://arxiv.org/abs/2406.17557\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eFineWeb-Edu\u003c/a\u003e and specialized mathematics and coding datasets.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis development comes at a crucial time when the \u003ca href=\"https://venturebeat.com/category/ai/\"\u003eAI\u003c/a\u003e industry is grappling with the computational demands of running large language models (LLMs). While companies like OpenAI and Anthropic push the boundaries with increasingly massive models, there’s growing recognition of the need for efficient, \u003ca href=\"https://venturebeat.com/ai/cloud-edge-or-on-prem-navigating-the-new-ai-infrastructure-paradigm/\"\u003elightweight AI\u003c/a\u003e that can run locally on devices.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe push for bigger AI models has left many potential users behind. Running these models requires \u003ca href=\"https://www.supplychaindive.com/news/generative-ai-drives-unmanageable-cloud-cost-finops/729246/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eexpensive cloud computing services\u003c/a\u003e, which come with their own problems: slow response times, data privacy risks and high costs that small companies and independent developers simply can’t afford. SmolLM2 offers a different approach by bringing powerful AI capabilities directly to personal devices, pointing toward a future where advanced AI tools are within reach of more users and companies, not just tech giants with massive data centers.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1916\" height=\"1488\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/Small-model-ecosystem.png?w=773\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/11/Small-model-ecosystem.png 1916w, https://venturebeat.com/wp-content/uploads/2024/11/Small-model-ecosystem.png?resize=300,233 300w, https://venturebeat.com/wp-content/uploads/2024/11/Small-model-ecosystem.png?resize=768,596 768w, https://venturebeat.com/wp-content/uploads/2024/11/Small-model-ecosystem.png?resize=773,600 773w, https://venturebeat.com/wp-content/uploads/2024/11/Small-model-ecosystem.png?resize=1536,1193 1536w, https://venturebeat.com/wp-content/uploads/2024/11/Small-model-ecosystem.png?resize=400,311 400w, https://venturebeat.com/wp-content/uploads/2024/11/Small-model-ecosystem.png?resize=750,582 750w, https://venturebeat.com/wp-content/uploads/2024/11/Small-model-ecosystem.png?resize=578,449 578w, https://venturebeat.com/wp-content/uploads/2024/11/Small-model-ecosystem.png?resize=930,722 930w\" sizes=\"(max-width: 1916px) 100vw, 1916px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eA comparison of AI language models shows SmolLM2’s superior efficiency, achieving higher performance scores with fewer parameters than larger rivals like Llama3.2 and Gemma, where the horizontal axis represents the model size and the vertical axis shows accuracy on benchmark tests. Credit: Hugging Face\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-edge-computing-gets-a-boost-as-ai-moves-to-mobile-devices\"\u003eEdge computing gets a boost as AI moves to mobile devices\u003c/h2\u003e\n\n\n\n\u003cp\u003eSmolLM2’s performance is particularly noteworthy given its size. On the \u003ca href=\"https://klu.ai/glossary/mt-bench-eval\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMT-Bench evaluation\u003c/a\u003e, which measures chat capabilities, the 1.7B model achieves a score of 6.13, competitive with much larger models. It also shows strong performance on mathematical reasoning tasks, scoring 48.2 on the \u003ca href=\"https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGSM8K benchmark\u003c/a\u003e. These results challenge the conventional wisdom that bigger models are always better, suggesting that careful architecture design and training data curation may be more important than raw parameter count.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe models support a range of applications including text rewriting, summarization and function calling. Their compact size enables deployment in scenarios where privacy, latency or connectivity constraints make cloud-based AI solutions impractical. This could prove particularly valuable in healthcare, financial services and other industries where data privacy is non-negotiable.\u003c/p\u003e\n\n\n\n\u003cp\u003eIndustry experts see this as part of a broader trend toward \u003ca href=\"https://futuretech.mit.edu/news/what-drives-progress-in-ai-trends-in-data#:~:text=We%20are%20learning%20to%20use%20data%20more%20efficiently\u0026amp;text=AI%20models%20are%20also%20being,adding%20noise%20to%20audio%20files.\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emore efficient AI models\u003c/a\u003e. The ability to run sophisticated language models locally on devices could enable new applications in areas like mobile app development, IoT devices, and enterprise solutions where data privacy is paramount.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-race-for-efficient-ai-smaller-models-challenge-industry-giants\"\u003eThe race for efficient AI: Smaller models challenge industry giants\u003c/h2\u003e\n\n\n\n\u003cp\u003eHowever, these smaller models still have limitations. According to Hugging Face’s documentation, they “\u003ca href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-135M\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eprimarily understand and generate content in English\u003c/a\u003e” and may not always produce factually accurate or logically consistent output.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe release of SmolLM2 suggests that the future of AI may not solely belong to increasingly large models, but rather to more efficient architectures that can deliver strong performance with fewer resources. This could have significant implications for democratizing AI access and reducing the environmental impact of AI deployment.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe models are available immediately through \u003ca href=\"https://huggingface.co/HuggingFaceTB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face’s model hub\u003c/a\u003e, with both base and instruction-tuned versions offered for each size variant.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-11-01T23:42:25Z",
  "modifiedTime": "2024-11-02T00:10:00Z"
}
