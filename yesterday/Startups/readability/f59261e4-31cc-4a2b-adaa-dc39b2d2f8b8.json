{
  "id": "f59261e4-31cc-4a2b-adaa-dc39b2d2f8b8",
  "title": "Arch-Function LLMs promise lightning-fast agentic AI for complex enterprise workflows",
  "link": "https://venturebeat.com/ai/arch-function-llms-promise-lightning-fast-agentic-ai-for-complex-enterprise-workflows/",
  "description": "Katanemo's new Arch-Function LLMs promise 12x faster function-calling capabilities, empowering enterprises to build ultra-fast, cost-effective agentic AI applications.",
  "author": "Shubham Sharma",
  "published": "Tue, 15 Oct 2024 23:22:47 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "agentic applications",
    "agentic workflows",
    "ai agent applications",
    "AI agents",
    "AI, ML and Deep Learning",
    "Arch-Function",
    "category-/Business \u0026 Industrial",
    "category-/Computers \u0026 Electronics/Enterprise Technology",
    "category-/Computers \u0026 Electronics/Software/Business \u0026 Productivity Software",
    "claude 3.5 sonnet",
    "function calling",
    "Function calling AI",
    "Generative AI",
    "GPT-4",
    "gpt-4o",
    "Katanemo",
    "large language models",
    "llm apps",
    "LLMs",
    "machine learning",
    "NLP",
    "Salman Paracha"
  ],
  "byline": "Shubham Sharma",
  "length": 5240,
  "excerpt": "Katanemo's new Arch-Function LLMs promise 12x faster function-calling capabilities, empowering enterprises to build ultra-fast, cost-effective agentic AI applications.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "October 15, 2024 4:22 PM Credit: Image generated by VentureBeat using FLUX-pro-1.1 Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Enterprises are bullish on agentic applications that can understand user instructions and intent to perform different tasks in digital environments. It’s the next wave in the age of generative AI, but many organizations still struggle with low throughputs with their models. Today, Katanemo, a startup building intelligent infrastructure for AI-native applications, took a step to solve this problem by open-sourcing Arch-Function. This is a collection of state-of-the-art large language models (LLMs) promising ultra-fast speeds at function-calling tasks critical to agentic workflows. But, just how fast are we talking about here? According to Salman Paracha, the founder and CEO of Katanemo, the new open models are nearly 12 times faster than OpenAI’s GPT-4. It even outperforms offerings from Anthropic all while delivering significant cost savings at the same time.  The move can easily pave the way for super-responsive agents that could handle domain-specific use cases without burning a hole in the businesses’ pockets. According to Gartner, by 2028, 33% of enterprise software tools will use agentic AI, up from less than 1% at present, enabling 15% of day-to-day work decisions to be made autonomously. What exactly does Arch-Function bring to the table? A week ago, Katanemo open-sourced Arch, an intelligent prompt gateway that uses specialized (sub-billion) LLMs to handle all critical tasks related to the handling and processing of prompts. This includes detecting and rejecting jailbreak attempts, intelligently calling “backend” APIs to fulfill the user’s request and managing the observability of prompts and LLM interactions in a centralized way.  The offering allows developers to build fast, secure and personalized gen AI apps at any scale. Now, as the next step in this work, the company has open-sourced some of the “intelligence” behind the gateway in the form of Arch-Function LLMs. As the founder puts it, these new LLMs – built on top of Qwen 2.5 with 3B and 7B parameters – are designed to handle function calls, which essentially allows them to interact with external tools and systems for performing digital tasks and accessing up-to-date information.  Using a given set of natural language prompts, the Arch-Function models can understand complex function signatures, identify required parameters and produce accurate function call outputs. This allows it to execute any required task, be it an API interaction or an automated backend workflow. This, in turn, can enable enterprises to develop agentic applications.  “In simple terms, Arch-Function helps you personalize your LLM apps by calling application-specific operations triggered via user prompts. With Arch-Function, you can build fast ‘agentic’ workflows tailored to domain-specific use cases – from updating insurance claims to creating ad campaigns via prompts. Arch-Function analyzes prompts, extracts critical information from them, engages in lightweight conversations to gather missing parameters from the user, and makes API calls so that you can focus on writing business logic,” Paracha explained. Speed and cost are the biggest highlights While function calling is not a new capability (many models support it), how effectively Arch-Function LLMs handle is the highlight. According to details shared by Paracha on X, the models beat or match frontier models, including those from OpenAI and Anthropic, in terms of quality but deliver significant benefits in terms of speed and cost savings.  For instance, compared to GPT-4, Arch-Function-3B delivers approximately 12x throughput improvement and massive 44x cost savings. Similar results were also seen against GPT-4o and Claude 3.5 Sonnet. The company has yet to share full benchmarks, but Paracha did note that the throughput and cost savings were seen when an L40S Nvidia GPU was used to host the 3B parameter model. “The standard is using the V100 or A100 to run/benchmark LLMS, and the L40S is a cheaper instance than both. Of course, this is our quantized version, with similar quality performance,” he noted. https://twitter.com/salman_paracha/status/1846180933206266082 With this work, enterprises can have a faster and more affordable family of function-calling LLMs to power their agentic applications. The company has yet to share case studies of how these models are being utilized, but high-throughput performance with low costs makes an ideal combo for real-time, production use cases such as processing incoming data for campaign optimization or sending emails to clients. According to Markets and Markets, globally, the market for AI agents is expected to grow with a CAGR of nearly 45% to become a $47 billion opportunity by 2030. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/10/agentic-rag-smk.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-10-15T23:22:47+00:00\" datetime=\"2024-10-15T23:22:47+00:00\"\u003eOctober 15, 2024 4:22 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"422\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/agentic-rag-smk.jpg?w=750\" alt=\"Credit: Image generated by VentureBeat using FLUX-pro-1.1\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: Image generated by VentureBeat using FLUX-pro-1.1\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eEnterprises are bullish on \u003ca href=\"https://venturebeat.com/ai/how-ai-agents-are-changing-software-development/\"\u003eagentic applications\u003c/a\u003e that can understand user instructions and intent to perform different tasks in digital environments. It’s the next wave in the age of generative AI, but many organizations still struggle with low throughputs with their models. Today, \u003ca href=\"https://www.katanemo.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eKatanemo\u003c/a\u003e, a startup building intelligent infrastructure for AI-native applications, took a step to solve this problem by \u003ca href=\"https://huggingface.co/katanemo/Arch-Function-3B\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eopen-sourcing\u003c/a\u003e Arch-Function. This is a collection of state-of-the-art large language models (LLMs) promising ultra-fast speeds at function-calling tasks critical to agentic workflows.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut, just how fast are we talking about here? According to \u003ca href=\"https://x.com/salman_paracha/status/1846180933206266082\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSalman Paracha\u003c/a\u003e, the founder and CEO of Katanemo, the new open models are nearly 12 times faster than OpenAI’s GPT-4. It even outperforms offerings from Anthropic all while delivering significant cost savings at the same time. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe move can easily pave the way for super-responsive agents that could handle domain-specific use cases without burning a hole in the businesses’ pockets. According to \u003ca href=\"https://www.gartner.com/en/articles/intelligent-agent-in-ai\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGartner\u003c/a\u003e, by 2028, 33% of enterprise software tools will use agentic AI, up from less than 1% at present, enabling 15% of day-to-day work decisions to be made autonomously.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-exactly-does-arch-function-bring-to-the-table\"\u003eWhat exactly does Arch-Function bring to the table?\u003c/h2\u003e\n\n\n\n\u003cp\u003eA week ago, Katanemo open-sourced \u003ca href=\"https://archgw.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eArch\u003c/a\u003e, an intelligent prompt gateway that uses specialized (sub-billion) LLMs to handle all critical tasks related to the handling and processing of prompts. This includes detecting and rejecting jailbreak attempts, intelligently calling “backend” APIs to fulfill the user’s request and managing the observability of prompts and LLM interactions in a centralized way. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe offering allows developers to build fast, secure and personalized gen AI apps at any scale. Now, as the next step in this work, the company has open-sourced some of the “intelligence” behind the gateway in the form of Arch-Function LLMs.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs the founder puts it, these new LLMs – built on top of Qwen 2.5 with 3B and 7B parameters – are designed to handle function calls, which essentially allows them to interact with external tools and systems for performing digital tasks and accessing up-to-date information. \u003c/p\u003e\n\n\n\n\u003cp\u003eUsing a given set of natural language prompts, the Arch-Function models can understand complex function signatures, identify required parameters and produce accurate function call outputs. This allows it to execute any required task, be it an API interaction or an automated backend workflow. This, in turn, can enable enterprises to develop agentic applications. \u003c/p\u003e\n\n\n\n\u003cp\u003e“In simple terms, Arch-Function helps you personalize your LLM apps by calling application-specific operations triggered via user prompts. With Arch-Function, you can build fast ‘agentic’ workflows tailored to domain-specific use cases – from updating insurance claims to creating ad campaigns via prompts. Arch-Function analyzes prompts, extracts critical information from them, engages in lightweight conversations to gather missing parameters from the user, and makes API calls so that you can focus on writing business logic,” Paracha explained.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-speed-and-cost-are-the-biggest-highlights\"\u003eSpeed and cost are the biggest highlights\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile function calling is not a new capability (many models support it), how effectively Arch-Function LLMs handle is the highlight. According to details shared by Paracha on X, the models beat or match frontier models, including those from OpenAI and Anthropic, in terms of quality but deliver significant benefits in terms of speed and cost savings. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor instance, compared to GPT-4, Arch-Function-3B delivers approximately 12x throughput improvement and massive 44x cost savings. Similar results were also seen against \u003ca href=\"https://venturebeat.com/ai/openai-announces-new-free-model-gpt-4o-and-chatgpt-for-desktop/\"\u003eGPT-4o\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/anthropic-unveils-claude-3-5-sonnet-pushing-the-boundaries-of-ai-capabilities-and-affordability/\"\u003eClaude 3.5 Sonnet\u003c/a\u003e. The company has yet to share full benchmarks, but Paracha did note that the throughput and cost savings were seen when an L40S Nvidia GPU was used to host the 3B parameter model.\u003c/p\u003e\n\n\n\n\u003cp\u003e“The standard is using the V100 or A100 to run/benchmark LLMS, and the L40S is a cheaper instance than both. Of course, this is our quantized version, with similar quality performance,” he noted.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\nhttps://twitter.com/salman_paracha/status/1846180933206266082\n\u003c/p\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWith this work, enterprises can have a faster and more affordable family of function-calling LLMs to power their agentic applications. The company has yet to share case studies of how these models are being utilized, but high-throughput performance with low costs makes an ideal combo for real-time, production use cases such as processing incoming data for campaign optimization or sending emails to clients.\u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to \u003ca href=\"https://www.marketsandmarkets.com/Market-Reports/ai-agents-market-15761548.html#:~:text=Restraint:%20Data%20privacy%20norms%20in,variety%20of%20contradictory%20statutory%20provisions.\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMarkets and Markets\u003c/a\u003e, globally, the market for AI agents is expected to grow with a CAGR of nearly 45% to become a $47 billion opportunity by 2030.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-10-15T23:22:47Z",
  "modifiedTime": "2024-10-15T23:22:53Z"
}
