{
  "id": "90c31a5f-a7bf-44b2-894a-f27c16b6a483",
  "title": "DeepMind’s Michelangelo benchmark reveals limitations of long-context LLMs",
  "link": "https://venturebeat.com/ai/deepminds-michelangelo-benchmark-reveals-limitations-of-long-context-llms/",
  "description": "LLMs can retrieve disparate facts from their context windows, but when it comes to reasoning over their context, they struggle badly.",
  "author": "Ben Dickson",
  "published": "Thu, 10 Oct 2024 21:47:39 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI research",
    "AI, ML and Deep Learning",
    "claude 3.5 sonnet",
    "DeepMind",
    "Gemini",
    "Google Deepmind",
    "GPT-4",
    "gpt-4o",
    "large language models",
    "LLMs",
    "research"
  ],
  "byline": "Ben Dickson",
  "length": 8190,
  "excerpt": "LLMs can retrieve disparate facts from their context windows, but when it comes to reasoning over their context, they struggle badly.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "October 10, 2024 2:47 PM Image credit: VentureBeat with DALL-E 3 Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Large language models (LLMs) with very long context windows have been making headlines lately. The ability to cram hundreds of thousands or even millions of tokens into a single prompt unlocks many possibilities for developers.  But how well do these long-context LLMs really understand and utilize the vast amounts of information they receive? Researchers at Google DeepMind have introduced Michelangelo, a new benchmark designed to evaluate the long-context reasoning capabilities of LLMs. Their findings, published in a new research paper, show that while current frontier models have progressed in retrieving information from large in-context data, they still struggle with tasks that require reasoning over the data structure. The need for better long-context benchmarks The emergence of LLMs with extremely long context windows, ranging from 128,000 to over 1 million tokens, has prompted researchers to develop new benchmarks to evaluate their capabilities. However, most of the focus has been on retrieval tasks, such as the popular “needle-in-a-haystack” evaluation, where the model is tasked with finding a specific piece of information within a large context. “Over time, models have grown considerably more capable in long context performance,” Kiran Vodrahalli, research scientist at Google DeepMind, told VentureBeat. “For instance, the popular needle-in-a-haystack evaluation for retrieval has now been well saturated up to extremely long context lengths. Thus, it has become important to determine whether the harder tasks models are capable of solving in short context regimes are also solvable at long ranges.” Retrieval tasks don’t necessarily reflect a model’s capacity for reasoning over the entire context. A model might be able to find a specific fact without understanding the relationships between different parts of the text. Meanwhile, existing benchmarks that evaluate a model’s ability to reason over long contexts have limitations. “It is easy to develop long reasoning evaluations which are solvable with a combination of only using retrieval and information stored in model weights, thus ‘short-circuiting’ the test of the model’s ability to use the long-context,” Vodrahalli said. Michelangelo To address the limitations of current benchmarks, the researchers introduced Michelangelo, a “minimal, synthetic, and unleaked long-context reasoning evaluation for large language models.”  Michelangelo is based on the analogy of a sculptor chiseling away irrelevant pieces of marble to reveal the underlying structure. The benchmark focuses on evaluating the model’s ability to understand the relationships and structure of the information within its context window, rather than simply retrieving isolated facts. The benchmark consists of three core tasks: Latent list: The model must process a long sequence of operations performed on a Python list, filter out irrelevant or redundant statements, and determine the final state of the list. “Latent List measures the ability of a model to track a latent data structure’s properties over the course of a stream of code instructions,” the researchers write. Multi-round co-reference resolution (MRCR): The model must produce parts of a long conversation between a user and an LLM. This requires the model to understand the structure of the conversation and resolve references to previous turns, even when the conversation contains confusing or distracting elements. “MRCR measures the model’s ability to understanding ordering in natural text, to distinguish between similar drafts of writing, and to reproduce a specified piece of previous context subject to adversarially difficult queries,” the researchers write. “I don’t know” (IDK): The model is given a long story and asked to answer multiple-choice questions about it. For some questions, the context does not contain the answer, and the model must be able to recognize the limits of its knowledge and respond with “I don’t know.” “IDK measures the model’s ability to understand whether it knows what it doesn’t know based on the presented context,” the researchers write. Latent Structure Queries The tasks in Michelangelo are based on a novel framework called Latent Structure Queries (LSQ). LSQ provides a general approach for designing long-context reasoning evaluations that can be extended to arbitrary lengths. It can also test the model’s understanding of implicit information as opposed to retrieving simple facts. LSQ relies on synthesizing test data to avoid the pitfalls of test data leaking into the training corpus. “By requiring the model to extract information from structures rather than values from keys (sculptures from marble rather than needles from haystacks), we can more deeply test language model context understanding beyond retrieval,” the researchers write. LSQ has three key differences from other approaches to evaluating long-context LLMs. First, it has been explicitly designed to avoid short-circuiting flaws in evaluations that go beyond retrieval tasks. Second, it specifies a methodology for increasing task complexity and context length independently. And finally, it is general enough to capture a large range of reasoning tasks. The three tests used in Michelangelo cover code interpretation and reasoning over loosely written text. “The goal is that long-context beyond-reasoning evaluations implemented by following LSQ will lead to fewer scenarios where a proposed evaluation reduces to solving a retrieval task,” Vodrahalli said. Evaluating frontier models on Michelangelo The researchers evaluated ten frontier LLMs on Michelangelo, including different variants of Gemini, GPT-4 and 4o, and Claude. They tested the models on contexts up to 1 million tokens. Gemini models performed best on MRCR, GPT models excelled on Latent List, and Claude 3.5 Sonnet achieved the highest scores on IDK. However, all models exhibited a significant drop in performance as the complexity of the reasoning tasks increased, suggesting that even with very long context windows, current LLMs still have room to improve in their ability to reason over large amounts of information. Frontier LLMs struggle with reasoning on long-context windows (source: arxiv) “Frontier models have room to improve on all of the beyond-retrieval reasoning primitives (Latent List, MRCR, IDK) that we investigate in Michelangelo,” Vodrahalli said. “Different frontier models have different strengths and weaknesses – each class performs well on different context ranges and on different tasks. What does seem to be universal across models is the initial drop in performance on long reasoning tasks.” The Michelangelo evaluations capture basic primitives necessary for long-context reasoning and the findings can have important implications for enterprise applications. For example, in real-world applications where the model can’t rely on its pretraining knowledge and must perform multi-hop reasoning over many disparate locations in very long contexts, Vodrahalli expects performance to drop as the context length grows. “This is particularly true if the documents have a lot of information that is irrelevant to the task at hand, making it hard for a model to easily immediately distinguish which information is relevant or not,” Vodrahalli said. “It is also likely that models will continue to perform well on tasks where all of the relevant information to answer a question is located in one general spot in the document.” The researchers will continue to add more evaluations to Michelangelo and hope to make them directly available so that other researchers can test their models on them. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/10/Robot-sculpture.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-10-10T21:47:39+00:00\" datetime=\"2024-10-10T21:47:39+00:00\"\u003eOctober 10, 2024 2:47 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"422\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/Robot-sculpture.jpg?w=750\" alt=\"Robot sculpture\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with DALL-E 3\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eLarge language models (LLMs) with \u003ca href=\"https://venturebeat.com/ai/deepmind-researchers-discover-impressive-learning-capabilities-in-long-context-llms/\"\u003every long context windows\u003c/a\u003e have been making headlines lately. The ability to cram hundreds of thousands or even millions of tokens into a single prompt unlocks many possibilities for developers. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut how well do these long-context LLMs really understand and utilize the vast amounts of information they receive?\u003c/p\u003e\n\n\n\n\u003cp\u003eResearchers at \u003ca href=\"https://deepmind.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle DeepMind\u003c/a\u003e have introduced \u003ca href=\"https://arxiv.org/abs/2409.12640\"\u003eMichelangelo\u003c/a\u003e, a new benchmark designed to evaluate the long-context reasoning capabilities of LLMs. Their findings, published in a new research paper, show that while current frontier models have progressed in retrieving information from large in-context data, they still struggle with tasks that require reasoning over the data structure.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-need-for-better-long-context-benchmarks\"\u003eThe need for better long-context benchmarks\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe emergence of LLMs with extremely long context windows, ranging from 128,000 to \u003ca href=\"https://venturebeat.com/ai/google-unveils-gemini-1-5-a-next-gen-ai-model-with-million-token-context-window/\"\u003eover 1 million tokens\u003c/a\u003e, has prompted researchers to develop new benchmarks to evaluate their capabilities. However, most of the focus has been on retrieval tasks, such as the popular “needle-in-a-haystack” evaluation, where the model is tasked with finding a specific piece of information within a large context.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Over time, models have grown considerably more capable in long context performance,” Kiran Vodrahalli, research scientist at Google DeepMind, told VentureBeat. “For instance, the popular needle-in-a-haystack evaluation for retrieval has now been well saturated up to extremely long context lengths. Thus, it has become important to determine whether the harder tasks models are capable of solving in short context regimes are also solvable at long ranges.”\u003c/p\u003e\n\n\n\n\u003cp\u003eRetrieval tasks don’t necessarily reflect a model’s capacity for reasoning over the entire context. A model might be able to find a specific fact without understanding the relationships between different parts of the text. Meanwhile, existing benchmarks that evaluate a model’s ability to reason over long contexts have limitations.\u003c/p\u003e\n\n\n\n\u003cp\u003e“It is easy to develop long reasoning evaluations which are solvable with a combination of only using retrieval and information stored in model weights, thus ‘short-circuiting’ the test of the model’s ability to use the long-context,” Vodrahalli said.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-michelangelo\"\u003eMichelangelo\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo address the limitations of current benchmarks, the researchers introduced Michelangelo, a “minimal, synthetic, and unleaked long-context reasoning evaluation for large language models.” \u003c/p\u003e\n\n\n\n\u003cp\u003eMichelangelo is based on the analogy of a sculptor chiseling away irrelevant pieces of marble to reveal the underlying structure. The benchmark focuses on evaluating the model’s ability to understand the relationships and structure of the information within its context window, rather than simply retrieving isolated facts.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe benchmark consists of three core tasks:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eLatent list:\u003c/strong\u003e The model must process a long sequence of operations performed on a Python list, filter out irrelevant or redundant statements, and determine the final state of the list. “Latent List measures the ability of a model to track a latent data structure’s properties over the course of a stream of code instructions,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eMulti-round co-reference resolution (MRCR):\u003c/strong\u003e The model must produce parts of a long conversation between a user and an LLM. This requires the model to understand the structure of the conversation and resolve references to previous turns, even when the conversation contains confusing or distracting elements. “MRCR measures the model’s ability to understanding ordering in natural text, to distinguish between similar drafts of writing, and to reproduce a specified piece of previous context subject to adversarially difficult queries,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e“I don’t know” (IDK):\u003c/strong\u003e The model is given a long story and asked to answer multiple-choice questions about it. For some questions, the context does not contain the answer, and the model must be able to recognize the limits of its knowledge and respond with “I don’t know.” “IDK measures the model’s ability to understand whether it knows what it doesn’t know based on the presented context,” the researchers write.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-latent-structure-queries\"\u003eLatent Structure Queries\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe tasks in Michelangelo are based on a novel framework called Latent Structure Queries (LSQ). LSQ provides a general approach for designing long-context reasoning evaluations that can be extended to arbitrary lengths. It can also test the model’s understanding of implicit information as opposed to retrieving simple facts. LSQ relies on synthesizing test data to avoid the pitfalls of test data leaking into the training corpus.\u003c/p\u003e\n\n\n\n\u003cp\u003e“By requiring the model to extract information from structures rather than values from keys (sculptures from marble rather than needles from haystacks), we can more deeply test language model context understanding beyond retrieval,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cp\u003eLSQ has three key differences from other approaches to evaluating long-context LLMs. First, it has been explicitly designed to avoid short-circuiting flaws in evaluations that go beyond retrieval tasks. Second, it specifies a methodology for increasing task complexity and context length independently. And finally, it is general enough to capture a large range of reasoning tasks. The three tests used in Michelangelo cover code interpretation and reasoning over loosely written text.\u003c/p\u003e\n\n\n\n\u003cp\u003e“The goal is that long-context beyond-reasoning evaluations implemented by following LSQ will lead to fewer scenarios where a proposed evaluation reduces to solving a retrieval task,” Vodrahalli said.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-evaluating-frontier-models-on-michelangelo\"\u003eEvaluating frontier models on Michelangelo\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers evaluated ten frontier LLMs on Michelangelo, including different variants of Gemini, \u003ca href=\"https://venturebeat.com/ai/openai-launches-experimental-gpt-4o-long-output-model-with-16x-token-capacity/\"\u003eGPT-4 and 4o\u003c/a\u003e, and Claude. They tested the models on contexts up to 1 million tokens. Gemini models performed best on MRCR, GPT models excelled on Latent List, and \u003ca href=\"https://venturebeat.com/ai/anthropic-unveils-claude-3-5-sonnet-pushing-the-boundaries-of-ai-capabilities-and-affordability/\"\u003eClaude 3.5 Sonnet\u003c/a\u003e achieved the highest scores on IDK.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, all models exhibited a significant drop in performance as the complexity of the reasoning tasks increased, suggesting that even with very long context windows, current LLMs still have room to improve in their ability to reason over large amounts of information.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1400\" height=\"782\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/Long-context-reasoning.jpg?w=800\" alt=\"long-context reasoning\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/10/Long-context-reasoning.jpg 1400w, https://venturebeat.com/wp-content/uploads/2024/10/Long-context-reasoning.jpg?resize=300,168 300w, https://venturebeat.com/wp-content/uploads/2024/10/Long-context-reasoning.jpg?resize=768,429 768w, https://venturebeat.com/wp-content/uploads/2024/10/Long-context-reasoning.jpg?resize=800,447 800w, https://venturebeat.com/wp-content/uploads/2024/10/Long-context-reasoning.jpg?resize=400,223 400w, https://venturebeat.com/wp-content/uploads/2024/10/Long-context-reasoning.jpg?resize=750,419 750w, https://venturebeat.com/wp-content/uploads/2024/10/Long-context-reasoning.jpg?resize=578,323 578w, https://venturebeat.com/wp-content/uploads/2024/10/Long-context-reasoning.jpg?resize=930,519 930w\" sizes=\"(max-width: 1400px) 100vw, 1400px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eFrontier LLMs struggle with reasoning on long-context windows (source: arxiv)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e“Frontier models have room to improve on all of the beyond-retrieval reasoning primitives (Latent List, MRCR, IDK) that we investigate in Michelangelo,” Vodrahalli said. “Different frontier models have different strengths and weaknesses – each class performs well on different context ranges and on different tasks. What does seem to be universal across models is the initial drop in performance on long reasoning tasks.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe Michelangelo evaluations capture basic primitives necessary for long-context reasoning and the findings can have important implications for enterprise applications. For example, in real-world applications where the model can’t rely on its pretraining knowledge and must perform multi-hop reasoning over many disparate locations in very long contexts, Vodrahalli expects performance to drop as the context length grows.\u003c/p\u003e\n\n\n\n\u003cp\u003e“This is particularly true if the documents have a lot of information that is irrelevant to the task at hand, making it hard for a model to easily immediately distinguish which information is relevant or not,” Vodrahalli said. “It is also likely that models will continue to perform well on tasks where all of the relevant information to answer a question is located in one general spot in the document.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers will continue to add more evaluations to Michelangelo and hope to make them directly available so that other researchers can test their models on them.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2024-10-10T21:47:39Z",
  "modifiedTime": "2024-10-10T21:47:47Z"
}
