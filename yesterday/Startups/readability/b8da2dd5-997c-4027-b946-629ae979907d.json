{
  "id": "b8da2dd5-997c-4027-b946-629ae979907d",
  "title": "Google study shows LLMs abandon correct answers under pressure, threatening multi-turn AI systems",
  "link": "https://venturebeat.com/ai/google-study-shows-llms-abandon-correct-answers-under-pressure-threatening-multi-turn-ai-systems/",
  "description": "A DeepMind study finds LLMs are both stubborn and easily swayed. This confidence paradox has key implications for building AI applications.",
  "author": "Ben Dickson",
  "published": "Wed, 16 Jul 2025 00:28:03 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI research",
    "AI, ML and Deep Learning",
    "choice-supportive bias",
    "DeepMind",
    "Google Deepmind",
    "large language models",
    "large language models (LLMs)",
    "LLMs",
    "reinforcement learning from human feedback (RLHF)",
    "research",
    "sycophancy",
    "University College London"
  ],
  "byline": "Ben Dickson",
  "length": 7289,
  "excerpt": "A DeepMind study finds LLMs are both stubborn and easily swayed. This confidence paradox has key implications for building AI applications.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "July 15, 2025 5:28 PM Image credit: VentureBeat with ChatGPT Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now A new study by researchers at Google DeepMind and University College London reveals how large language models (LLMs) form, maintain and lose confidence in their answers. The findings reveal striking similarities between the cognitive biases of LLMs and humans, while also highlighting stark differences. The research reveals that LLMs can be overconfident in their own answers yet quickly lose that confidence and change their minds when presented with a counterargument, even if the counterargument is incorrect. Understanding the nuances of this behavior can have direct consequences on how you build LLM applications, especially conversational interfaces that span several turns. Testing confidence in LLMs A critical factor in the safe deployment of LLMs is that their answers are accompanied by a reliable sense of confidence (the probability that the model assigns to the answer token). While we know LLMs can produce these confidence scores, the extent to which they can use them to guide adaptive behavior is poorly characterized. There is also empirical evidence that LLMs can be overconfident in their initial answer but also be highly sensitive to criticism and quickly become underconfident in that same choice. To investigate this, the researchers developed a controlled experiment to test how LLMs update their confidence and decide whether to change their answers when presented with external advice. In the experiment, an “answering LLM” was first given a binary-choice question, such as identifying the correct latitude for a city from two options. After making its initial choice, the LLM was given advice from a fictitious “advice LLM.” This advice came with an explicit accuracy rating (e.g., “This advice LLM is 70% accurate”) and would either agree with, oppose, or stay neutral on the answering LLM’s initial choice. Finally, the answering LLM was asked to make its final choice. The AI Impact Series Returns to San Francisco â August 5 The next phase of AI is here â are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows â from real-time decision-making to end-to-end automation. Secure your spot now â space is limited: https://bit.ly/3GuuPLF Example test of confidence in LLMs Source: arXiv A key part of the experiment was controlling whether the LLM’s own initial answer was visible to it during the second, final decision. In some cases, it was shown, and in others, it was hidden. This unique setup, impossible to replicate with human participants who can’t simply forget their prior choices, allowed the researchers to isolate how memory of a past decision influences current confidence.  A baseline condition, where the initial answer was hidden and the advice was neutral, established how much an LLM’s answer might change simply due to random variance in the model’s processing. The analysis focused on how the LLM’s confidence in its original choice changed between the first and second turn, providing a clear picture of how initial belief, or prior, affects a “change of mind” in the model. Overconfidence and underconfidence The researchers first examined how the visibility of the LLM’s own answer affected its tendency to change its answer. They observed that when the model could see its initial answer, it showed a reduced tendency to switch, compared to when the answer was hidden. This finding points to a specific cognitive bias. As the paper notes, “This effect – the tendency to stick with one’s initial choice to a greater extent when that choice was visible (as opposed to hidden) during the contemplation of final choice – is closely related to a phenomenon described in the study of human decision making, a choice-supportive bias.” The study also confirmed that the models do integrate external advice. When faced with opposing advice, the LLM showed an increased tendency to change its mind, and a reduced tendency when the advice was supportive. “This finding demonstrates that the answering LLM appropriately integrates the direction of advice to modulate its change of mind rate,” the researchers write. However, they also discovered that the model is overly sensitive to contrary information and performs too large of a confidence update as a result. Sensitivity of LLMs to different settings in confidence testing Source: arXiv Interestingly, this behavior is contrary to the confirmation bias often seen in humans, where people favor information that confirms their existing beliefs. The researchers found that LLMs “overweight opposing rather than supportive advice, both when the initial answer of the model was visible and hidden from the model.” One possible explanation is that training techniques like reinforcement learning from human feedback (RLHF) may encourage models to be overly deferential to user input, a phenomenon known as sycophancy (which remains a challenge for AI labs). Implications for enterprise applications This study confirms that AI systems are not the purely logical agents they are often perceived to be. They exhibit their own set of biases, some resembling human cognitive errors and others unique to themselves, which can make their behavior unpredictable in human terms. For enterprise applications, this means that in an extended conversation between a human and an AI agent, the most recent information could have a disproportionate impact on the LLM’s reasoning (especially if it is contradictory to the model’s initial answer), potentially causing it to discard an initially correct answer. Fortunately, as the study also shows, we can manipulate an LLM’s memory to mitigate these unwanted biases in ways that are not possible with humans. Developers building multi-turn conversational agents can implement strategies to manage the AI’s context. For example, a long conversation can be periodically summarized, with key facts and decisions presented neutrally and stripped of which agent made which choice. This summary can then be used to initiate a new, condensed conversation, providing the model with a clean slate to reason from and helping to avoid the biases that can creep in during extended dialogues. As LLMs become more integrated into enterprise workflows, understanding the nuances of their decision-making processes is no longer optional. Following foundational research like this enables developers to anticipate and correct for these inherent biases, leading to applications that are not just more capable, but also more robust and reliable. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/07/AI-overconfidence-and-underconfidence.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-07-16T00:28:03+00:00\" datetime=\"2025-07-16T00:28:03+00:00\"\u003eJuly 15, 2025 5:28 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"500\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/AI-overconfidence-and-underconfidence.png?w=750\" alt=\"Image credit: VentureBeat with ChatGPT\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with ChatGPT\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\u003c/em\u003e \u003cem\u003e\u003ca href=\"https://venturebeat.com/newsletters/\"\u003eSubscribe Now\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eA \u003ca href=\"https://www.arxiv.org/abs/2507.03120\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew study\u003c/a\u003e by researchers at \u003ca href=\"https://deepmind.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle DeepMind\u003c/a\u003e and \u003ca href=\"https://www.ucl.ac.uk/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eUniversity College London\u003c/a\u003e reveals how large language models (LLMs) form, maintain and lose confidence in their answers. The findings reveal striking similarities between the cognitive biases of LLMs and humans, while also highlighting stark differences.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe research reveals that LLMs can be overconfident in their own answers yet quickly lose that confidence and change their minds when presented with a counterargument, even if the counterargument is incorrect. Understanding the nuances of this behavior can have direct consequences on how you build LLM applications, especially conversational interfaces that span several turns.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-testing-confidence-in-llms\"\u003eTesting confidence in LLMs\u003c/h2\u003e\n\n\n\n\u003cp\u003eA critical factor in the safe deployment of LLMs is that their answers are accompanied by a reliable sense of confidence (the probability that the model assigns to the answer token). While we know LLMs can produce these confidence scores, the extent to which they can use them to guide adaptive behavior is poorly characterized. There is also empirical evidence that LLMs can be overconfident in their initial answer but also be highly sensitive to criticism and quickly become underconfident in that same choice.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo investigate this, the researchers developed a controlled experiment to test how LLMs update their confidence and decide whether to change their answers when presented with external advice. In the experiment, an “answering LLM” was first given a binary-choice question, such as identifying the correct latitude for a city from two options. After making its initial choice, the LLM was given advice from a fictitious “advice LLM.” This advice came with an explicit accuracy rating (e.g., “This advice LLM is 70% accurate”) and would either agree with, oppose, or stay neutral on the answering LLM’s initial choice. Finally, the answering LLM was asked to make its final choice.\u003c/p\u003e\n\n\n\n\u003cdiv id=\"boilerplate_2803147\"\u003e\n\u003chr/\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e\u003c/strong\u003e\u003cstrong\u003eThe AI Impact Series Returns to San Francisco â August 5\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe next phase of AI is here â are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows â from real-time decision-making to end-to-end automation. \u003c/p\u003e\n\n\n\n\u003cp\u003eSecure your spot now â space is limited: \u003ca href=\"https://bit.ly/3GuuPLF\"\u003ehttps://bit.ly/3GuuPLF\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" height=\"460\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image_7d1064.png?w=800\" alt=\"Example test of confidence in LLMs (source: arXiv)\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image_7d1064.png 1286w, https://venturebeat.com/wp-content/uploads/2025/07/image_7d1064.png?resize=300,173 300w, https://venturebeat.com/wp-content/uploads/2025/07/image_7d1064.png?resize=768,442 768w, https://venturebeat.com/wp-content/uploads/2025/07/image_7d1064.png?resize=800,460 800w, https://venturebeat.com/wp-content/uploads/2025/07/image_7d1064.png?resize=400,230 400w, https://venturebeat.com/wp-content/uploads/2025/07/image_7d1064.png?resize=750,432 750w, https://venturebeat.com/wp-content/uploads/2025/07/image_7d1064.png?resize=578,333 578w, https://venturebeat.com/wp-content/uploads/2025/07/image_7d1064.png?resize=930,535 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eExample test of confidence in LLMs Source: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eA key part of the experiment was controlling whether the LLM’s own initial answer was visible to it during the second, final decision. In some cases, it was shown, and in others, it was hidden. This unique setup, impossible to replicate with human participants who can’t simply forget their prior choices, allowed the researchers to isolate how memory of a past decision influences current confidence. \u003c/p\u003e\n\n\n\n\u003cp\u003eA baseline condition, where the initial answer was hidden and the advice was neutral, established how much an LLM’s answer might change simply due to random variance in the model’s processing. The analysis focused on how the LLM’s confidence in its original choice changed between the first and second turn, providing a clear picture of how initial belief, or prior, affects a “change of mind” in the model.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-overconfidence-and-underconfidence\"\u003eOverconfidence and underconfidence\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers first examined how the visibility of the LLM’s own answer affected its tendency to change its answer. They observed that when the model could see its initial answer, it showed a reduced tendency to switch, compared to when the answer was hidden. This finding points to a specific cognitive bias. As the paper notes, “This effect – the tendency to stick with one’s initial choice to a greater extent when that choice was visible (as opposed to hidden) during the contemplation of final choice – is closely related to a phenomenon described in the study of human decision making, a \u003ca href=\"https://en.wikipedia.org/wiki/Choice-supportive_bias\" target=\"_blank\" rel=\"noreferrer noopener\"\u003echoice-supportive bias\u003c/a\u003e.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe study also confirmed that the models do integrate external advice. When faced with opposing advice, the LLM showed an increased tendency to change its mind, and a reduced tendency when the advice was supportive. “This finding demonstrates that the answering LLM appropriately integrates the direction of advice to modulate its change of mind rate,” the researchers write. However, they also discovered that the model is overly sensitive to contrary information and performs too large of a confidence update as a result.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" height=\"212\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image_6e0710.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image_6e0710.png 1454w, https://venturebeat.com/wp-content/uploads/2025/07/image_6e0710.png?resize=300,80 300w, https://venturebeat.com/wp-content/uploads/2025/07/image_6e0710.png?resize=768,204 768w, https://venturebeat.com/wp-content/uploads/2025/07/image_6e0710.png?resize=800,212 800w, https://venturebeat.com/wp-content/uploads/2025/07/image_6e0710.png?resize=400,106 400w, https://venturebeat.com/wp-content/uploads/2025/07/image_6e0710.png?resize=750,199 750w, https://venturebeat.com/wp-content/uploads/2025/07/image_6e0710.png?resize=578,153 578w, https://venturebeat.com/wp-content/uploads/2025/07/image_6e0710.png?resize=930,247 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eSensitivity of LLMs to different settings in confidence testing Source: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eInterestingly, this behavior is contrary to the \u003ca href=\"https://en.wikipedia.org/wiki/Confirmation_bias\" target=\"_blank\" rel=\"noreferrer noopener\"\u003econfirmation bias\u003c/a\u003e often seen in humans, where people favor information that confirms their existing beliefs. The researchers found that LLMs “overweight opposing rather than supportive advice, both when the initial answer of the model was visible and hidden from the model.” One possible explanation is that training techniques like \u003ca href=\"https://venturebeat.com/ai/new-reinforcement-learning-method-uses-human-cues-to-correct-its-mistakes/\"\u003ereinforcement learning from human feedback\u003c/a\u003e (RLHF) may encourage models to be overly deferential to user input, a phenomenon known as sycophancy (which \u003ca href=\"https://venturebeat.com/ai/openai-rolls-back-chatgpts-sycophancy-and-explains-what-went-wrong/\"\u003eremains a challenge for AI labs\u003c/a\u003e).\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-implications-for-enterprise-applications\"\u003eImplications for enterprise applications\u003c/h2\u003e\n\n\n\n\u003cp\u003eThis study confirms that AI systems are not the purely logical agents they are often perceived to be. They exhibit their own set of biases, some resembling human cognitive errors and others unique to themselves, which can make their behavior unpredictable in human terms. For enterprise applications, this means that in an extended conversation between a human and an AI agent, the most recent information could have a disproportionate impact on the LLM’s reasoning (especially if it is contradictory to the model’s initial answer), potentially causing it to discard an initially correct answer.\u003c/p\u003e\n\n\n\n\u003cp\u003eFortunately, as the study also shows, we can manipulate an LLM’s memory to mitigate these unwanted biases in ways that are not possible with humans. Developers building multi-turn conversational agents can implement strategies to manage the AI’s context. For example, a long conversation can be periodically summarized, with key facts and decisions presented neutrally and stripped of which agent made which choice. This summary can then be used to initiate a new, condensed conversation, providing the model with a clean slate to reason from and helping to avoid the biases that can creep in during extended dialogues.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs LLMs become more integrated into enterprise workflows, understanding the nuances of their decision-making processes is no longer optional. Following foundational research like this enables developers to anticipate and correct for these inherent biases, leading to applications that are not just more capable, but also more robust and reliable.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-07-16T00:28:03Z",
  "modifiedTime": "2025-07-16T00:28:10Z"
}
