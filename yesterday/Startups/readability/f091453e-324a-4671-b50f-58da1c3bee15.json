{
  "id": "f091453e-324a-4671-b50f-58da1c3bee15",
  "title": "Alibaba releases Qwen with Questions, an open reasoning model that beats o1-preview",
  "link": "https://venturebeat.com/ai/alibaba-releases-qwen-with-questions-an-open-reasoning-model-that-beats-o1-preview/",
  "description": "QwQ uses inference-time scaling to solve complex reasoning and planning questions, besting OpenAI's o1 in several benchmarks.",
  "author": "Ben Dickson",
  "published": "Fri, 29 Nov 2024 14:37:38 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI research",
    "AI, ML and Deep Learning",
    "alibaba",
    "category-/Science/Computer Science",
    "large language models",
    "LLM reasoning",
    "LLMs",
    "o1",
    "o1-mini",
    "research"
  ],
  "byline": "Ben Dickson",
  "length": 4903,
  "excerpt": "QwQ uses inference-time scaling to solve complex reasoning and planning questions, besting OpenAI's o1 in several benchmarks.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "November 29, 2024 6:37 AM Ideogram prompt by VentureBeat Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Chinese e-commerce giant Alibaba has released the latest model in its ever-expanding Qwen family. This one is known as Qwen with Questions (QwQ), and serves as the latest open source competitor to OpenAI’s o1 reasoning model. Like other large reasoning models (LRMs), QwQ uses extra compute cycles during inference to review its answers and correct its mistakes, making it more suitable for tasks that require logical reasoning and planning like math and coding. What is Qwen with Questions (OwQ?) and can it be used for commercial purposes? Alibaba has released a 32-billion-parameter version of QwQ with a 32,000-token context. The model is currently in preview, which means a higher-performing version is likely to follow. According to Alibaba’s tests, QwQ beats o1-preview on the AIME and MATH benchmarks, which evaluate mathematical problem-solving abilities. It also outperforms o1-mini on GPQA, a benchmark for scientific reasoning. QwQ is inferior to o1 on the LiveCodeBench coding benchmarks but still outperforms other frontier models such as GPT-4o and Claude 3.5 Sonnet. Example output of Qwen with Questions QwQ does not come with an accompanying paper that describes the data or the process used to train the model, which makes it difficult to reproduce the model’s results. However, since the model is open, unlike OpenAI o1, its “thinking process” is not hidden and can be used to make sense of how the model reasons when solving problems. Alibaba has also released the model under an Apache 2.0 license, which means it can be used for commercial purposes. ‘We discovered something profound’ According to a blog post that was published along with the model’s release, “Through deep exploration and countless trials, we discovered something profound: when given time to ponder, to question, and to reflect, the model’s understanding of mathematics and programming blossoms like a flower opening to the sun… This process of careful reflection and self-questioning leads to remarkable breakthroughs in solving complex problems.” This is very similar to what we know about how reasoning models work. By generating more tokens and reviewing their previous responses, the models are more likely to correct potential mistakes. Marco-o1, another reasoning model recently released by Alibaba might also contain hints of how QwQ might be working. Marco-o1 uses Monte Carlo Tree Search (MCTS) and self-reflection at inference time to create different branches of reasoning and choose the best answers. The model was trained on a mixture of chain-of-thought (CoT) examples and synthetic data generated with MCTS algorithms. Alibaba points out that QwQ still has limitations such as mixing languages or getting stuck in circular reasoning loops. The model is available for download on Hugging Face and an online demo can be found on Hugging Face Spaces. The LLM age gives way to LRMs: Large Reasoning Models The release of o1 has triggered growing interest in creating LRMs, even though not much is known about how the model works under the hood aside from using inference-time scale to improve the model’s responses.  There are now several Chinese competitors to o1. Chinese AI lab DeepSeek recently released R1-Lite-Preview, its o1 competitor, which is currently only available through the company’s online chat interface. R1-Lite-Preview reportedly beats o1 on several key benchmarks. Another recently released model is LLaVA-o1, developed by researchers from multiple universities in China, which brings the inference-time reasoning paradigm to open-source vision language models (VLMs).  The focus on LRMs comes at a time of uncertainty about the future of model scaling laws. Reports indicate that AI labs such as OpenAI, Google DeepMind, and Anthropic are getting diminishing returns on training larger models. And creating larger volumes of quality training data is becoming increasingly difficult as models are already being trained on trillions of tokens gathered from the internet.  Meanwhile, inference-time scale offers an alternative that might provide the next breakthrough in improving the abilities of the next generation of AI models. There are reports that OpenAI is using o1 to generate synthetic reasoning data to train the next generation of its LLMs. The release of open reasoning models is likely to stimulate progress and make the space more competitive. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/01/a_brain_with_gears_and_question_marks_in_steampu-transformed.jpeg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-11-29T14:37:38+00:00\" datetime=\"2024-11-29T14:37:38+00:00\"\u003eNovember 29, 2024 6:37 AM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"469\" src=\"https://venturebeat.com/wp-content/uploads/2024/01/a_brain_with_gears_and_question_marks_in_steampu-transformed.jpeg?w=750\" alt=\"Ideogram prompt by VentureBeat\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eIdeogram prompt by VentureBeat\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eChinese e-commerce giant Alibaba has released the latest model in its ever-expanding Qwen family. This one is known as Qwen with Questions (QwQ), and serves as the latest open source competitor to \u003ca href=\"https://venturebeat.com/ai/forget-gpt-5-openai-launches-new-ai-model-family-o1-claiming-phd-level-performance/\"\u003eOpenAI’s o1\u003c/a\u003e reasoning model. \u003c/p\u003e\n\n\n\n\u003cp\u003eLike other large reasoning models (LRMs), QwQ uses extra compute cycles during inference to review its answers and correct its mistakes, making it more suitable for tasks that require logical reasoning and planning like math and coding.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-is-qwen-with-questions-owq-and-can-it-be-used-for-commercial-purposes\"\u003eWhat is Qwen with Questions (OwQ?) and can it be used for commercial purposes?\u003c/h2\u003e\n\n\n\n\u003cp\u003eAlibaba has released a 32-billion-parameter version of QwQ with a 32,000-token context. The model is currently in preview, which means a higher-performing version is likely to follow. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Alibaba’s tests, QwQ beats o1-preview on the AIME and MATH benchmarks, which evaluate mathematical problem-solving abilities. It also outperforms o1-mini on GPQA, a benchmark for scientific reasoning. QwQ is inferior to o1 on the LiveCodeBench coding benchmarks but still outperforms other frontier models such as \u003ca href=\"https://venturebeat.com/ai/openai-launches-experimental-gpt-4o-long-output-model-with-16x-token-capacity/\"\u003eGPT-4o\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/anthropic-unveils-claude-3-5-sonnet-pushing-the-boundaries-of-ai-capabilities-and-affordability/\"\u003eClaude 3.5 Sonnet\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1000\" height=\"615\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/Qwen-with-Questions.jpg?w=800\" alt=\"Qwen with Questions\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/11/Qwen-with-Questions.jpg 1000w, https://venturebeat.com/wp-content/uploads/2024/11/Qwen-with-Questions.jpg?resize=300,185 300w, https://venturebeat.com/wp-content/uploads/2024/11/Qwen-with-Questions.jpg?resize=768,472 768w, https://venturebeat.com/wp-content/uploads/2024/11/Qwen-with-Questions.jpg?resize=800,492 800w, https://venturebeat.com/wp-content/uploads/2024/11/Qwen-with-Questions.jpg?resize=400,246 400w, https://venturebeat.com/wp-content/uploads/2024/11/Qwen-with-Questions.jpg?resize=750,461 750w, https://venturebeat.com/wp-content/uploads/2024/11/Qwen-with-Questions.jpg?resize=578,355 578w, https://venturebeat.com/wp-content/uploads/2024/11/Qwen-with-Questions.jpg?resize=930,572 930w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003eExample output of Qwen with Questions\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eQwQ does not come with an accompanying paper that describes the data or the process used to train the model, which makes it difficult to reproduce the model’s results. However, since the model is open, unlike OpenAI o1, its “thinking process” is not hidden and can be used to make sense of how the model reasons when solving problems. \u003c/p\u003e\n\n\n\n\u003cp\u003eAlibaba has also released the model under an Apache 2.0 license, which means it can be used for commercial purposes.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-we-discovered-something-profound\"\u003e‘We discovered something profound’\u003c/h2\u003e\n\n\n\n\u003cp\u003eAccording to a \u003ca href=\"https://qwenlm.github.io/blog/qwq-32b-preview/\"\u003eblog post\u003c/a\u003e that was published along with the model’s release, “Through deep exploration and countless trials, we discovered something profound: when given time to ponder, to question, and to reflect, the model’s understanding of mathematics and programming blossoms like a flower opening to the sun… This process of careful reflection and self-questioning leads to remarkable breakthroughs in solving complex problems.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThis is very similar to what we know about how reasoning models work. By generating more tokens and reviewing their previous responses, the models are more likely to correct potential mistakes. \u003ca href=\"https://venturebeat.com/ai/alibaba-researchers-unveil-marco-o1-an-llm-with-advanced-reasoning-capabilities/\"\u003eMarco-o1\u003c/a\u003e, another reasoning model recently released by Alibaba might also contain hints of how QwQ might be working. Marco-o1 uses \u003ca href=\"https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\"\u003eMonte Carlo Tree Search\u003c/a\u003e (MCTS) and self-reflection at inference time to create different branches of reasoning and choose the best answers. The model was trained on a mixture of chain-of-thought (CoT) examples and synthetic data generated with MCTS algorithms.\u003c/p\u003e\n\n\n\n\u003cp\u003eAlibaba points out that QwQ still has limitations such as mixing languages or getting stuck in circular reasoning loops. The model is available for download on \u003ca href=\"https://huggingface.co/Qwen\"\u003eHugging Face\u003c/a\u003e and an online demo can be found on \u003ca href=\"https://huggingface.co/spaces/Qwen/QwQ-32B-preview\"\u003eHugging Face Spaces\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-llm-age-gives-way-to-lrms-large-reasoning-models\"\u003eThe LLM age gives way to LRMs: Large Reasoning Models\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe release of o1 has triggered growing interest in creating LRMs, even though not much is known about how the model works under the hood aside from using inference-time scale to improve the model’s responses. \u003c/p\u003e\n\n\n\n\u003cp\u003eThere are now \u003ca href=\"https://venturebeat.com/ai/openai-faces-critical-test-as-chinese-models-close-the-gap-in-ai-leadership/\"\u003eseveral Chinese competitors to o1\u003c/a\u003e. Chinese AI lab DeepSeek recently released \u003ca href=\"https://venturebeat.com/ai/deepseeks-first-reasoning-model-r1-lite-preview-turns-heads-beating-openai-o1-performance/\"\u003eR1-Lite-Preview\u003c/a\u003e, its o1 competitor, which is currently only available through the company’s online chat interface. R1-Lite-Preview reportedly beats o1 on several key benchmarks.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnother recently released model is \u003ca href=\"https://venturebeat.com/ai/chinese-researchers-unveil-llava-o1-to-challenge-openais-o1-model/\"\u003eLLaVA-o1\u003c/a\u003e, developed by researchers from multiple universities in China, which brings the inference-time reasoning paradigm to open-source vision language models (VLMs). \u003c/p\u003e\n\n\n\n\u003cp\u003eThe focus on LRMs comes at a time of uncertainty about the future of model scaling laws. \u003ca href=\"https://www.bloomberg.com/news/articles/2024-11-13/openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai\"\u003eReports\u003c/a\u003e indicate that AI labs such as OpenAI, Google DeepMind, and Anthropic are getting diminishing returns on training larger models. And creating larger volumes of quality training data is becoming increasingly difficult as models are already being trained on trillions of tokens gathered from the internet. \u003c/p\u003e\n\n\n\n\u003cp\u003eMeanwhile, inference-time scale offers an alternative that might provide the next breakthrough in improving the abilities of the next generation of AI models. There are reports that OpenAI is \u003ca href=\"https://www.theverge.com/2024/10/24/24278999/openai-plans-orion-ai-model-release-december\"\u003eusing o1 to generate synthetic reasoning data\u003c/a\u003e to train the next generation of its LLMs. The release of open reasoning models is likely to stimulate progress and make the space more competitive.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-11-29T14:37:38Z",
  "modifiedTime": "2024-11-29T14:37:45Z"
}
