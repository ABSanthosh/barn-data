{
  "id": "4392fb82-77d3-4c6b-951b-182f7bff99de",
  "title": "AI can fix bugs—but can’t find them: OpenAI’s study highlights limits of LLMs in software engineering",
  "link": "https://venturebeat.com/ai/ai-can-fix-bugs-but-cant-find-them-openais-study-highlights-limits-of-llms-in-software-engineering/",
  "description": "A new test from OpenAI researchers found that LLMs were unable to resolve some freelance coding tests, failing to earn full value.",
  "author": "Emilia David",
  "published": "Tue, 18 Feb 2025 23:16:55 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI Coding",
    "AI Coding Assistant",
    "AI, ML and Deep Learning",
    "category-/Computers \u0026 Electronics/Programming",
    "category-/Science/Computer Science",
    "claude 3.5 sonnet",
    "coding",
    "coding assistant",
    "gpt-4o",
    "OpenAI",
    "openai o1"
  ],
  "byline": "Emilia David",
  "length": 5157,
  "excerpt": "A new test from OpenAI researchers found that LLMs were unable to resolve some freelance coding tests, failing to earn full value.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "February 18, 2025 3:16 PM Credit: VentureBeat made with Midjourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Large language models (LLMs) may have changed software development, but enterprises will need to think twice about entirely replacing human software engineers with LLMs, despite OpenAI CEO Sam Altman’s claim that models can replace “low-level” engineers. In a new paper, OpenAI researchers detail how they developed an LLM benchmark called SWE-Lancer to test how much foundation models can earn from real-life freelance software engineering tasks. The test found that, while the models can solve bugs, they can’t see why the bug exists and continue to make more mistakes.  The researchers tasked three LLMs — OpenAI’s GPT-4o and o1 and Anthropic’s Claude-3.5 Sonnet — with 1,488 freelance software engineer tasks from the freelance platform Upwork amounting to $1 million in payouts. They divided the tasks into two categories: individual contributor tasks (resolving bugs or implementing features), and management tasks (where the model roleplays as a manager who will choose the best proposal to resolve issues).  “Results indicate that the real-world freelance work in our benchmark remains challenging for frontier language models,” the researchers write.  The test shows that foundation models cannot fully replace human engineers. While they can help solve bugs, they’re not quite at the level where they can start earning freelancing cash by themselves.  Benchmarking freelancing models The researchers and 100 other professional software engineers identified potential tasks on Upwork and, without changing any words, fed these to a Docker container to create the SWE-Lancer dataset. The container does not have internet access and cannot access GitHub “to avoid the possible of models scraping code diffs or pull request details,” they explained. The team identified 764 individual contributor tasks, totaling about $414,775, ranging from 15-minute bug fixes to weeklong feature requests. These tasks, which included reviewing freelancer proposals and job postings, would pay out $585,225. The tasks were added to the expensing platform Expensify.  The researchers generated prompts based on the task title and description and a snapshot of the codebase. If there were additional proposals to resolve the issue, “we also generated a management task using the issue description and list of proposals,” they explained. From here, the researchers moved to end-to-end test development. They wrote Playwright tests for each task that applies these generated patches which were then “triple-verified” by professional software engineers. “Tests simulate real-world user flows, such as logging into the application, performing complex actions (making financial transactions) and verifying that the model’s solution works as expected,” the paper explains.  Test results After running the test, the researchers found that none of the models earned the full $1 million value of the tasks. Claude 3.5 Sonnet, the best-performing model, earned only $208,050 and resolved 26.2% of the individual contributor issues. However, the researchers point out, “the majority of its solutions are incorrect, and higher reliability is needed for trustworthy deployment.” The models performed well across most individual contributor tasks, with Claude 3.5-Sonnet performing best, followed by o1 and GPT-4o.  “Agents excel at localizing, but fail to root cause, resulting in partial or flawed solutions,” the report explains. “Agents pinpoint the source of an issue remarkably quickly, using keyword searches across the whole repository to quickly locate the relevant file and functions — often far faster than a human would. However, they often exhibit a limited understanding of how the issue spans multiple components or files, and fail to address the root cause, leading to solutions that are incorrect or insufficiently comprehensive. We rarely find cases where the agent aims to reproduce the issue or fails due to not finding the right file or location to edit.” Interestingly, the models all performed better on manager tasks that required reasoning to evaluate technical understanding. These benchmark tests showed that AI models can solve some “low-level” coding problems and can’t replace “low-level” software engineers yet. The models still took time, often made mistakes, and couldn’t chase a bug around to find the root cause of coding problems. Many “low-level” engineers work better, but the researchers said this may not be the case for very long.  Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2023/11/cfr0z3n_software_developer_geek_coding_on_a_terminal_window_on__7529178a-36d0-47b9-905e-f2a828e2fc04-e1699632740189-transformed.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-02-18T23:16:55+00:00\" datetime=\"2025-02-18T23:16:55+00:00\"\u003eFebruary 18, 2025 3:16 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"375\" src=\"https://venturebeat.com/wp-content/uploads/2023/11/cfr0z3n_software_developer_geek_coding_on_a_terminal_window_on__7529178a-36d0-47b9-905e-f2a828e2fc04-e1699632740189-transformed.webp?w=750\" alt=\"Glitchwave software developer typing on a computer lit in neon green against a neon yellow, orange and pink backdrop.\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eLarge language models (LLMs) \u003ca href=\"https://venturebeat.com/ai/how-ai-agents-are-changing-software-development/\"\u003emay have changed software development\u003c/a\u003e, but enterprises will need to think twice about entirely replacing human software engineers with LLMs, despite OpenAI CEO Sam Altman’s claim that \u003ca href=\"https://www.youtube.com/live/8JZh3zLrmRE\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emodels can replace\u003c/a\u003e “low-level” engineers.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a \u003ca href=\"https://arxiv.org/pdf/2502.12115\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew paper\u003c/a\u003e, \u003ca href=\"https://openai.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI\u003c/a\u003e researchers detail how they developed an LLM benchmark called SWE-Lancer to test how much foundation models can earn from real-life freelance software engineering tasks. The test found that, while the models can solve bugs, they can’t see why the bug exists and continue to make more mistakes. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers tasked three LLMs — OpenAI’s GPT-4o and o1 and \u003ca href=\"https://venturebeat.com/ai/the-code-whisperer-how-anthropics-claude-is-changing-the-game-for-software-developers/\"\u003eAnthropic’s Claude-3.5 Sonnet\u003c/a\u003e — with 1,488 freelance software engineer tasks \u003ca href=\"https://venturebeat.com/business/upwork-shares-leap-40-as-ceo-calls-ipo-beginning-of-a-new-chapter/\"\u003efrom the freelance platform\u003c/a\u003e Upwork amounting to $1 million in payouts. They divided the tasks into two categories: individual contributor tasks (resolving bugs or implementing features), and management tasks (where the model roleplays as a manager who will choose the best proposal to resolve issues). \u003c/p\u003e\n\n\n\n\u003cp\u003e“Results indicate that the real-world freelance work in our benchmark remains challenging for frontier language models,” the researchers write. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe test shows that foundation models cannot fully replace human engineers. While they can help solve bugs, they’re not quite at the level where they can start earning freelancing cash by themselves. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-benchmarking-freelancing-models\"\u003eBenchmarking freelancing models\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers and 100 other professional software engineers identified potential tasks on Upwork and, without changing any words, fed these to a Docker container to create the SWE-Lancer dataset. The container does not have internet access and cannot access GitHub “to avoid the possible of models scraping code diffs or pull request details,” they explained. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe team identified 764 individual contributor tasks, totaling about $414,775, ranging from 15-minute bug fixes to weeklong feature requests. These tasks, which included reviewing freelancer proposals and job postings, would pay out $585,225.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe tasks were added to the expensing platform Expensify. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers generated prompts based on the task title and description and a snapshot of the codebase. If there were additional proposals to resolve the issue, “we also generated a management task using the issue description and list of proposals,” they explained. \u003c/p\u003e\n\n\n\n\u003cp\u003eFrom here, the researchers moved to end-to-end test development. They wrote Playwright tests for each task that applies these generated patches which were then “triple-verified” by professional software engineers.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Tests simulate real-world user flows, such as logging into the application, performing complex actions (making financial transactions) and verifying that the model’s solution works as expected,” the paper explains. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-test-results\"\u003eTest results\u003c/h2\u003e\n\n\n\n\u003cp\u003eAfter running the test, the researchers found that none of the models earned the full $1 million value of the tasks. Claude 3.5 Sonnet, the best-performing model, earned only $208,050 and resolved 26.2% of the individual contributor issues. However, the researchers point out, “the majority of its solutions are incorrect, and higher reliability is needed for trustworthy deployment.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe models performed well across most individual contributor tasks, with Claude 3.5-Sonnet performing best, followed by o1 and GPT-4o. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Agents excel at localizing, but fail to root cause, resulting in partial or flawed solutions,” the report explains. “Agents pinpoint the source of an issue remarkably quickly, using keyword searches across the whole repository to quickly locate the relevant file and functions — often far faster than a human would. However, they often exhibit a limited understanding of how the issue spans multiple components or files, and fail to address the root cause, leading to solutions that are incorrect or insufficiently comprehensive. We rarely find cases where the agent aims to reproduce the issue or fails due to not finding the right file or location to edit.”\u003c/p\u003e\n\n\n\n\u003cp\u003eInterestingly, the models all performed better on manager tasks that required reasoning to evaluate technical understanding.\u003c/p\u003e\n\n\n\n\u003cp\u003eThese benchmark tests showed that AI models can solve some “low-level” coding problems and can’t replace “low-level” software engineers yet. The models still took time, often made mistakes, and couldn’t chase a bug around to find the root cause of coding problems. Many “low-level” engineers work better, but the researchers said this may not be the case for very long. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-02-18T23:16:55Z",
  "modifiedTime": "2025-02-18T23:17:01Z"
}
