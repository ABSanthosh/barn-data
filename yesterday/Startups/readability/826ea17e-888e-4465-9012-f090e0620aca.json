{
  "id": "826ea17e-888e-4465-9012-f090e0620aca",
  "title": "Meta’s answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way!",
  "link": "https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/",
  "description": "While DeepSeek R1 and OpenAI o1 edge out Behemoth on a couple metrics, Llama 4 Behemoth remains highly competitive.",
  "author": "Carl Franzen",
  "published": "Sat, 05 Apr 2025 21:46:57 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI, ML and Deep Learning",
    "Conversational AI",
    "Deepseek",
    "Deepseek R1",
    "DeepSeek V3",
    "LLaMA",
    "llama 4",
    "llama 4 behemoth",
    "llama 4 maverick",
    "llama 4 scout",
    "Mark Zuckerberg",
    "Meta",
    "Meta Platforms",
    "Mistral AI",
    "mixture of experts",
    "Mixture-of-Experts model",
    "NLP",
    "OpenAI",
    "politics",
    "research",
    "Science",
    "Trump",
    "Yann LeCun"
  ],
  "byline": "Carl Franzen",
  "length": 11202,
  "excerpt": "While DeepSeek R1 and OpenAI o1 edge out Behemoth on a couple metrics, Llama 4 Behemoth remains highly competitive.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "April 5, 2025 2:46 PM VentureBeat made with Midjourney v7 Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More The entire AI landscape shifted back in January 2025 after a then little-known Chinese AI startup DeepSeek (a subsidiary of the Hong Kong-based quantitative analysis firm High-Flyer Capital Management) launched its powerful open source language reasoning model DeepSeek R1 publicly to the world, besting U.S. giants such as Meta. As DeepSeek usage spread rapidly among researchers and enterprises, Meta was reportedly sent into panic mode upon learning that this new R1 model had been trained for a fraction of the cost of many other leading models yet outclassed them for as little as several million dollars — what it pays some of its own AI team leaders. Meta’s whole generative AI strategy had until that point been predicated on releasing best-in-class open source models under its brand name “Llama” for researchers and companies to build upon freely (at least, if they had fewer than 700 million monthly users, at which point they are supposed to contact Meta for special paid licensing terms). Yet DeepSeek R1’s astonishingly good performance on a far smaller budget had allegedly shaken the company leadership and forced some kind of reckoning, with the last version of Llama, 3.3, having been released just a month prior in December 2024 yet already looking outdated. Now we know the fruits of that reckoning: today, Meta founder and CEO Mark Zuckerberg took to his Instagram account to announced a new Llama 4 series of models, with two of them — the 400-billion parameter Llama 4 Maverick and 109-billion parameter Llama 4 Scout — available today for developers to download and begin using or fine-tuning now on llama.com and AI code sharing community Hugging Face. A massive 2-trillion parameter Llama 4 Behemoth is also being previewed today, though Meta’s blog post on the releases said it was still being trained, and gave no indication of when it might be released. (Recall parameters refer to the settings that govern the model’s behavior and that generally more mean a more powerful and complex all around model.) One headline feature of these models is that they are all multimodal — trained on, and therefore, capable of receiving and generating text, video, and imagery (hough audio was not mentioned). Another is that they have incredibly long context windows — 1 million tokens for Llama 4 Maverick and 10 million for Llama 4 Scout — which is equivalent to about 1,500 and 15,000 pages of text, respectively, all of which the model can handle in a single input/output interaction. That means a user could theoretically upload or paste up to 7,500 pages-worth-of text and receive that much in return from Llama 4 Scout, which would be handy for information-dense fields such as medicine, science, engineering, mathematics, literature etc. Here’s what else we’ve learned about this release so far: All-in on mixture-of-experts All three models use the “mixture-of-experts (MoE)” architecture approach popularized in earlier model releases from OpenAI and Mistral, which essentially combines multiple smaller models specialized (“experts”) in different tasks, subjects and media formats into a unified whole, larger model. Each Llama 4 release is said to be therefore a mixture of 128 different experts, and more efficient to run because only the expert needed for a particular task, plus a “shared” expert, handles each token, instead of the entire model having to run for each one. As the Llama 4 blog post notes: As a result, while all parameters are stored in memory, only a subset of the total parameters are activated while serving these models. This improves inference efficiency by lowering model serving costs and latency—Llama 4 Maverick can be run on a single [Nvidia] H100 DGX host for easy deployment, or with distributed inference for maximum efficiency. Both Scout and Maverick are available to the public for self-hosting, while no hosted API or pricing tiers have been announced for official Meta infrastructure. Instead, Meta focuses on distribution through open download and integration with Meta AI in WhatsApp, Messenger, Instagram, and web. Meta estimates the inference cost for Llama 4 Maverick at $0.19 to $0.49 per 1 million tokens (using a 3:1 blend of input and output). This makes it substantially cheaper than proprietary models like GPT-4o, which is estimated to cost $4.38 per million tokens, based on community benchmarks. All three Llama 4 models—especially Maverick and Behemoth—are explicitly designed for reasoning, coding, and step-by-step problem solving — though they don’t appear to exhibit the chains-of-thought of dedicated reasoning models such as the OpenAI “o” series, nor DeepSeek R1. Instead, they seem designed to compete more directly with “classical,” non-reasoning LLMs and multimodal models such as OpenAI’s GPT-4o and DeepSeek’s V3 — with the exception of Llama 4 Behemoth, which does appear to threaten DeepSeek R1 (more on this below!) In addition, for Llama 4, Meta built custom post-training pipelines focused on enhancing reasoning, such as: Removing over 50% of “easy” prompts during supervised fine-tuning. Adopting a continuous reinforcement learning loop with progressively harder prompts. Using pass@k evaluation and curriculum sampling to strengthen performance in math, logic, and coding. Implementing MetaP, a new technique that lets engineers tune hyperparameters (like per-layer learning rates) on models and apply them to other model sizes and types of tokens while preserving the intended model behavior. MetaP is of particular interest as it could be used going forward to set hyperparameters on on model and then get many other types of models out of it, increasing training efficiency. As my VentureBeat colleague and LLM expert Ben Dickson opined ont the new MetaP technique: “This can save a lot of time and money. It means that they run experiments on the smaller models instead of doing them on the large-scale ones.” This is especially critical when training models as large as Behemoth, which uses 32K GPUs and FP8 precision, achieving 390 TFLOPs/GPU over more than 30 trillion tokens—more than double the Llama 3 training data. In other words: the researchers can tell the model broadly how they want it to act, and apply this to larger and smaller version of the model, and across different forms of media. A powerful – but not yet the most powerful — model family In his announcement video on Instagram (a Meta subsidiary, naturally), Meta CEO Mark Zuckerberg said that the company’s “goal is to build the world’s leading AI, open source it, and make it universally accessible so that everyone in the world benefits…I’ve said for a while that I think open source AI is going to become the leading models, and with Llama 4, that is starting to happen.” It’s a clearly carefully worded statement, as is Meta’s blog post calling Llama 4 Scout, “the best multimodal model in the world in its class and is more powerful than all previous generation Llama models,” (emphasis added by me). In other words, these are very powerful models, near the top of the heap compared to others in their parameter-size class, but not necessarily setting new performance records. Nonetheless, Meta was keen to trumpet the models its new Llama 4 family beats, among them: Llama 4 Behemoth Outperforms GPT-4.5, Gemini 2.0 Pro, and Claude Sonnet 3.7 on: MATH-500 (95.0) GPQA Diamond (73.7) MMLU Pro (82.2) Llama 4 Maverick Beats GPT-4o and Gemini 2.0 Flash on most multimodal reasoning benchmarks: ChartQA, DocVQA, MathVista, MMMU Competitive with DeepSeek v3.1 (45.8B params) while using less than half the active parameters (17B) Benchmark scores: ChartQA: 90.0 (vs. GPT-4o’s 85.7) DocVQA: 94.4 (vs. 92.8) MMLU Pro: 80.5 Cost-effective: $0.19–$0.49 per 1M tokens Llama 4 Scout Matches or outperforms models like Mistral 3.1, Gemini 2.0 Flash-Lite, and Gemma 3 on: DocVQA: 94.4 MMLU Pro: 74.3 MathVista: 70.7 Unmatched 10M token context length—ideal for long documents, codebases, or multi-turn analysis Designed for efficient deployment on a single H100 GPU But after all that, how does Llama 4 stack up to DeepSeek? But of course, there are a whole other class of reasoning-heavy models such as DeepSeek R1, OpenAI’s “o” series (like GPT-4o), Gemini 2.0, and Claude Sonnet. Using the highest-parameter model benchmarked—Llama 4 Behemoth—and comparing it to the intial DeepSeek R1 release chart for R1-32B and OpenAI o1 models, here’s how Llama 4 Behemoth stacks up: BenchmarkLlama 4 BehemothDeepSeek R1OpenAI o1-1217MATH-50095.097.396.4GPQA Diamond73.771.575.7MMLU82.290.891.8 What can we conclude? MATH-500: Llama 4 Behemoth is slightly behind DeepSeek R1 and OpenAI o1. GPQA Diamond: Behemoth is ahead of DeepSeek R1, but behind OpenAI o1. MMLU: Behemoth trails both, but still outperforms Gemini 2.0 Pro and GPT-4.5. Takeaway: While DeepSeek R1 and OpenAI o1 edge out Behemoth on a couple metrics, Llama 4 Behemoth remains highly competitive and performs at or near the top of the reasoning leaderboard in its class. Safety and less political ‘bias’ Meta also emphasized model alignment and safety by introducing tools like Llama Guard, Prompt Guard, and CyberSecEval to help developers detect unsafe input/output or adversarial prompts, and implementing Generative Offensive Agent Testing (GOAT) for automated red-teaming. The company also claims Llama 4 shows substantial improvement on “political bias” and says “specifically, [leading LLMs] historically have leaned left when it comes to debated political and social topics,” that that Llama 4 does better at courting the right wing…in keeping with Zuckerberg’s embrace of Republican U.S. president Donald J. Trump and his party following the 2024 election. Where Llama 4 stands so far Meta’s Llama 4 models bring together efficiency, openness, and high-end performance across multimodal and reasoning tasks. With Scout and Maverick now publicly available and Behemoth previewed as a state-of-the-art teacher model, the Llama ecosystem is positioned to offer a competitive open alternative to top-tier proprietary models from OpenAI, Anthropic, DeepSeek, and Google. Whether you’re building enterprise-scale assistants, AI research pipelines, or long-context analytical tools, Llama 4 offers flexible, high-performance options with a clear orientation toward reasoning-first design. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/04/cfr0z3n_vector_art_minimalist_acid_wash_flat_illustration_2D_sc_3a905d5f-566a-4c8b-b16b-8ac8836f837a.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-04-05T21:46:57+00:00\" datetime=\"2025-04-05T21:46:57+00:00\"\u003eApril 5, 2025 2:46 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/cfr0z3n_vector_art_minimalist_acid_wash_flat_illustration_2D_sc_3a905d5f-566a-4c8b-b16b-8ac8836f837a.png?w=750\" alt=\"Four neon llamas in close up standing side-by-side wearing sunglasses in front of a neon pink hued wall\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eVentureBeat made with Midjourney v7\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eThe entire AI landscape shifted back in January 2025 after a then little-known Chinese AI startup DeepSeek (a subsidiary of the Hong Kong-based quantitative analysis firm High-Flyer Capital Management)\u003ca href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e launched its powerful open source language reasoning model DeepSeek R1\u003c/a\u003e publicly to the world, besting U.S. giants such as Meta. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs DeepSeek usage spread rapidly among researchers and enterprises, \u003ca href=\"https://fortune.com/2025/01/27/mark-zuckerberg-meta-llama-assembling-war-rooms-engineers-deepseek-ai-china/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMeta was reportedly sent into panic mode\u003c/a\u003e upon learning that this new R1 model had been trained for a fraction of the cost of many other leading models yet outclassed them for \u003ca href=\"https://venturebeat.com/ai/deepseek-r1s-bold-bet-on-reinforcement-learning-how-it-outpaced-openai-at-3-of-the-cost/\"\u003eas little as several million dollars\u003c/a\u003e — what it pays some of its own AI team leaders.\u003c/p\u003e\n\n\n\n\u003cp\u003eMeta’s whole generative AI strategy had until that point been predicated on releasing best-in-class open source models under its brand name “\u003ca href=\"https://venturebeat.com/ai/meta-unleashes-its-most-powerful-ai-model-llama-3-1-with-405b-parameters/\"\u003eLlama\u003c/a\u003e” for researchers and companies to build upon freely (at least, if they had fewer than 700 million monthly users, at which point they are supposed to contact Meta for special paid licensing terms). \u003c/p\u003e\n\n\n\n\u003cp\u003eYet DeepSeek R1’s astonishingly good performance on a far smaller budget had allegedly shaken the company leadership and forced some kind of reckoning, with the last version of Llama, \u003ca href=\"https://ai.meta.com/blog/future-of-ai-built-with-llama/\"\u003e3.3\u003c/a\u003e, having been released just a month prior in December 2024 yet already looking outdated.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow we know the fruits of that reckoning: today, \u003ca href=\"https://www.instagram.com/p/DIE0TmPyORV/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMeta founder and CEO Mark Zuckerberg took to his Instagram account\u003c/a\u003e to announced a \u003ca href=\"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew Llama 4 series of models\u003c/a\u003e, with two of them — the 400-billion parameter Llama 4 Maverick and 109-billion parameter Llama 4 Scout — available today for developers to download and begin using or fine-tuning now on \u003ca href=\"https://llama.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ellama.com\u003c/a\u003e and AI code sharing community \u003ca href=\"https://huggingface.co/collections/meta-llama/llama-4-67f0c30d9fe03840bc9d0164\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eA massive 2-trillion parameter Llama 4 Behemoth is also being previewed today, \u003ca href=\"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethough Meta’s blog post on the releases\u003c/a\u003e said it was still being trained, and gave no indication of when it might be released. (Recall parameters refer to the settings that govern the model’s behavior and that generally more mean a more powerful and complex all around model.)\u003c/p\u003e\n\n\n\n\u003cp\u003eOne headline feature of these models is that they are all multimodal — trained on, and therefore, capable of receiving and generating text, video, and imagery (hough audio was not mentioned).\u003c/p\u003e\n\n\n\n\u003cp\u003eAnother is that they have incredibly long context windows — 1 million tokens for Llama 4 Maverick and 10 million for Llama 4 Scout — which is equivalent to about 1,500 and 15,000 pages of text, respectively, all of which the model can handle in a single input/output interaction. That means a user could theoretically upload or paste up to 7,500 pages-worth-of text and receive that much in return from Llama 4 Scout, which would be handy for information-dense fields such as medicine, science, engineering, mathematics, literature etc.\u003c/p\u003e\n\n\n\n\u003cp\u003eHere’s what else we’ve learned about this release so far:\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-all-in-on-mixture-of-experts\"\u003eAll-in on mixture-of-experts\u003c/h2\u003e\n\n\n\n\u003cp\u003eAll three models use the “mixture-of-experts (MoE)” architecture approach \u003ca href=\"https://medium.com/@seanbetts/peering-inside-gpt-4-understanding-its-mixture-of-experts-moe-architecture-2a42eb8bdcb3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epopularized in earlier model releases from OpenAI\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/mistral-ai-drops-new-mixture-of-experts-model-with-a-torrent-link/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMistral\u003c/a\u003e, which essentially combines multiple smaller models specialized (“experts”) in different tasks, subjects and media formats into a unified whole, larger model. Each Llama 4 release is said to be therefore a mixture of 128 different experts, and more efficient to run because only the expert needed for a particular task, plus a “shared” expert, handles each token, instead of the entire model having to run for each one. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs the Llama 4 blog post notes:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eAs a result, while all parameters are stored in memory, only a subset of the total parameters are activated while serving these models. This improves inference efficiency by lowering model serving costs and latency—Llama 4 Maverick can be run on a single [Nvidia] H100 DGX host for easy deployment, or with distributed inference for maximum efficiency.\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eBoth Scout and Maverick are available to the public for self-hosting, while no hosted API or pricing tiers have been announced for official Meta infrastructure. Instead, Meta focuses on distribution through open download and integration with Meta AI in WhatsApp, Messenger, Instagram, and web.\u003c/p\u003e\n\n\n\n\u003cp\u003eMeta estimates the inference cost for Llama 4 Maverick at $0.19 to $0.49 per 1 million tokens (using a 3:1 blend of input and output). This makes it substantially cheaper than proprietary models like GPT-4o, which is estimated to cost $4.38 per million tokens, based on community benchmarks.\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cp\u003eAll three Llama 4 models—especially Maverick and Behemoth—are explicitly designed for reasoning, coding, and step-by-step problem solving — though they don’t appear to exhibit the chains-of-thought of dedicated reasoning models such as the OpenAI “o” series, nor DeepSeek R1. \u003c/p\u003e\n\n\n\n\u003cp\u003eInstead, they seem designed to compete more directly with “classical,” non-reasoning LLMs and multimodal models such as OpenAI’s GPT-4o and DeepSeek’s V3 — with the exception of Llama 4 Behemoth, which \u003cem\u003edoes\u003c/em\u003e appear to threaten DeepSeek R1 (more on this below!)\u003c/p\u003e\n\n\n\n\u003cp\u003eIn addition, for Llama 4, Meta built custom post-training pipelines focused on enhancing reasoning, such as:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eRemoving over 50% of “easy” prompts during supervised fine-tuning.\u003c/li\u003e\n\n\n\n\u003cli\u003eAdopting a continuous reinforcement learning loop with progressively harder prompts.\u003c/li\u003e\n\n\n\n\u003cli\u003eUsing pass@k evaluation and curriculum sampling to strengthen performance in math, logic, and coding.\u003c/li\u003e\n\n\n\n\u003cli\u003eImplementing MetaP, a new technique that lets engineers tune hyperparameters (like per-layer learning rates) on models and apply them to other model sizes and types of tokens while preserving the intended model behavior.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eMetaP is of particular interest as it could be used going forward to set hyperparameters on on model and then get many other types of models out of it, increasing training efficiency. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs my VentureBeat colleague and LLM expert Ben Dickson opined ont the new MetaP technique: “This can save a lot of time and money. It means that they run experiments on the smaller models instead of doing them on the large-scale ones.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThis is especially critical when training models as large as Behemoth, which uses 32K GPUs and FP8 precision, achieving 390 TFLOPs/GPU over more than 30 trillion tokens—more than double the Llama 3 training data.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn other words: the researchers can tell the model broadly how they want it to act, and apply this to larger and smaller version  of the model, and across different forms of media.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-a-powerful-but-not-yet-the-most-powerful-model-family\"\u003eA powerful – but not yet \u003cem\u003ethe\u003c/em\u003e \u003cem\u003emost\u003c/em\u003e powerful — model family\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn his \u003ca href=\"https://www.instagram.com/p/DIE0TmPyORV/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eannouncement video on Instagram\u003c/a\u003e (a Meta subsidiary, naturally), Meta CEO Mark Zuckerberg said that the company’s “goal is to build the world’s leading AI, open source it, and make it universally accessible so that everyone in the world benefits…I’ve said for a while that I think open source AI is going to become the leading models, and with Llama 4, that is starting to happen.”\u003c/p\u003e\n\n\n\n\u003cp\u003eIt’s a clearly carefully worded statement, as is Meta’s blog post calling Llama 4 Scout, “the best multimodal model in the world \u003cem\u003ein its class\u003c/em\u003e and is more powerful than all previous generation Llama models,” (emphasis added by me). \u003c/p\u003e\n\n\n\n\u003cp\u003eIn other words, these are very powerful models, near the top of the heap compared to others in their parameter-size class, but not necessarily setting new performance records. Nonetheless, Meta was keen to trumpet the models its new Llama 4 family beats, among them:\u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-llama-4-behemoth\"\u003eLlama 4 Behemoth\u003c/h3\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eOutperforms GPT-4.5, Gemini 2.0 Pro, and Claude Sonnet 3.7 on:\n\u003cul\u003e\n\u003cli\u003eMATH-500 (95.0)\u003c/li\u003e\n\n\n\n\u003cli\u003eGPQA Diamond (73.7)\u003c/li\u003e\n\n\n\n\u003cli\u003eMMLU Pro (82.2)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1920\" height=\"1016\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/489511937_1627813884508038_4209289296588372348_n.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/489511937_1627813884508038_4209289296588372348_n.png 1920w, https://venturebeat.com/wp-content/uploads/2025/04/489511937_1627813884508038_4209289296588372348_n.png?resize=300,159 300w, https://venturebeat.com/wp-content/uploads/2025/04/489511937_1627813884508038_4209289296588372348_n.png?resize=768,406 768w, https://venturebeat.com/wp-content/uploads/2025/04/489511937_1627813884508038_4209289296588372348_n.png?resize=800,423 800w, https://venturebeat.com/wp-content/uploads/2025/04/489511937_1627813884508038_4209289296588372348_n.png?resize=1536,813 1536w, https://venturebeat.com/wp-content/uploads/2025/04/489511937_1627813884508038_4209289296588372348_n.png?resize=400,212 400w, https://venturebeat.com/wp-content/uploads/2025/04/489511937_1627813884508038_4209289296588372348_n.png?resize=750,397 750w, https://venturebeat.com/wp-content/uploads/2025/04/489511937_1627813884508038_4209289296588372348_n.png?resize=578,306 578w, https://venturebeat.com/wp-content/uploads/2025/04/489511937_1627813884508038_4209289296588372348_n.png?resize=930,492 930w\" sizes=\"(max-width: 1920px) 100vw, 1920px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch3 id=\"h-llama-4-maverick\"\u003eLlama 4 Maverick\u003c/h3\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eBeats GPT-4o and Gemini 2.0 Flash on most multimodal reasoning benchmarks:\n\u003cul\u003e\n\u003cli\u003eChartQA, DocVQA, MathVista, MMMU\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\n\n\n\u003cli\u003eCompetitive with DeepSeek v3.1 (45.8B params) while using less than half the active parameters (17B)\u003c/li\u003e\n\n\n\n\u003cli\u003eBenchmark scores:\n\u003cul\u003e\n\u003cli\u003eChartQA: 90.0 (vs. GPT-4o’s 85.7)\u003c/li\u003e\n\n\n\n\u003cli\u003eDocVQA: 94.4 (vs. 92.8)\u003c/li\u003e\n\n\n\n\u003cli\u003eMMLU Pro: 80.5\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\n\n\n\u003cli\u003eCost-effective: $0.19–$0.49 per 1M tokens\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1920\" height=\"1638\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/489031501_1656960988514372_2535138154557835854_n.png?w=703\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/489031501_1656960988514372_2535138154557835854_n.png 1920w, https://venturebeat.com/wp-content/uploads/2025/04/489031501_1656960988514372_2535138154557835854_n.png?resize=300,256 300w, https://venturebeat.com/wp-content/uploads/2025/04/489031501_1656960988514372_2535138154557835854_n.png?resize=768,655 768w, https://venturebeat.com/wp-content/uploads/2025/04/489031501_1656960988514372_2535138154557835854_n.png?resize=703,600 703w, https://venturebeat.com/wp-content/uploads/2025/04/489031501_1656960988514372_2535138154557835854_n.png?resize=1536,1310 1536w, https://venturebeat.com/wp-content/uploads/2025/04/489031501_1656960988514372_2535138154557835854_n.png?resize=400,341 400w, https://venturebeat.com/wp-content/uploads/2025/04/489031501_1656960988514372_2535138154557835854_n.png?resize=750,640 750w, https://venturebeat.com/wp-content/uploads/2025/04/489031501_1656960988514372_2535138154557835854_n.png?resize=578,493 578w, https://venturebeat.com/wp-content/uploads/2025/04/489031501_1656960988514372_2535138154557835854_n.png?resize=930,793 930w\" sizes=\"(max-width: 1920px) 100vw, 1920px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch3 id=\"h-llama-4-scout\"\u003eLlama 4 Scout\u003c/h3\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eMatches or outperforms models like Mistral 3.1, Gemini 2.0 Flash-Lite, and Gemma 3 on:\n\u003cul\u003e\n\u003cli\u003eDocVQA: 94.4\u003c/li\u003e\n\n\n\n\u003cli\u003eMMLU Pro: 74.3\u003c/li\u003e\n\n\n\n\u003cli\u003eMathVista: 70.7\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\n\n\n\u003cli\u003eUnmatched 10M token context length—ideal for long documents, codebases, or multi-turn analysis\u003c/li\u003e\n\n\n\n\u003cli\u003eDesigned for efficient deployment on a single H100 GPU\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1920\" height=\"1359\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/488658055_1347378876402143_3412007366291908454_n.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/488658055_1347378876402143_3412007366291908454_n.png 1920w, https://venturebeat.com/wp-content/uploads/2025/04/488658055_1347378876402143_3412007366291908454_n.png?resize=300,212 300w, https://venturebeat.com/wp-content/uploads/2025/04/488658055_1347378876402143_3412007366291908454_n.png?resize=768,544 768w, https://venturebeat.com/wp-content/uploads/2025/04/488658055_1347378876402143_3412007366291908454_n.png?resize=800,566 800w, https://venturebeat.com/wp-content/uploads/2025/04/488658055_1347378876402143_3412007366291908454_n.png?resize=1536,1087 1536w, https://venturebeat.com/wp-content/uploads/2025/04/488658055_1347378876402143_3412007366291908454_n.png?resize=400,283 400w, https://venturebeat.com/wp-content/uploads/2025/04/488658055_1347378876402143_3412007366291908454_n.png?resize=750,531 750w, https://venturebeat.com/wp-content/uploads/2025/04/488658055_1347378876402143_3412007366291908454_n.png?resize=578,409 578w, https://venturebeat.com/wp-content/uploads/2025/04/488658055_1347378876402143_3412007366291908454_n.png?resize=930,658 930w\" sizes=\"(max-width: 1920px) 100vw, 1920px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-but-after-all-that-how-does-llama-4-stack-up-to-deepseek\"\u003eBut after all that, how does Llama 4 stack up to DeepSeek?\u003c/h2\u003e\n\n\n\n\u003cp\u003eBut of course, there are a whole other class of reasoning-heavy models such as DeepSeek R1, OpenAI’s “o” series (like GPT-4o), Gemini 2.0, and Claude Sonnet. \u003c/p\u003e\n\n\n\n\u003cp\u003eUsing the highest-parameter model benchmarked—Llama 4 Behemoth—and comparing it to the intial \u003ca href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDeepSeek R1 release chart\u003c/a\u003e for R1-32B and OpenAI o1 models, here’s how Llama 4 Behemoth stacks up:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eBenchmark\u003c/th\u003e\u003cth\u003eLlama 4 Behemoth\u003c/th\u003e\u003cth\u003eDeepSeek R1\u003c/th\u003e\u003cth\u003eOpenAI o1-1217\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eMATH-500\u003c/td\u003e\u003ctd\u003e95.0\u003c/td\u003e\u003ctd\u003e97.3\u003c/td\u003e\u003ctd\u003e96.4\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eGPQA Diamond\u003c/td\u003e\u003ctd\u003e73.7\u003c/td\u003e\u003ctd\u003e71.5\u003c/td\u003e\u003ctd\u003e75.7\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eMMLU\u003c/td\u003e\u003ctd\u003e82.2\u003c/td\u003e\u003ctd\u003e90.8\u003c/td\u003e\u003ctd\u003e91.8\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhat can we conclude?\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eMATH-500: Llama 4 Behemoth is slightly \u003cem\u003ebehind\u003c/em\u003e DeepSeek R1 and OpenAI o1.\u003c/li\u003e\n\n\n\n\u003cli\u003eGPQA Diamond: Behemoth is \u003cem\u003eahead of DeepSeek R\u003c/em\u003e1, but behind OpenAI o1.\u003c/li\u003e\n\n\n\n\u003cli\u003eMMLU: Behemoth trails both, but still outperforms Gemini 2.0 Pro and GPT-4.5.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eTakeaway: While DeepSeek R1 and OpenAI o1 edge out Behemoth on a couple metrics, Llama 4 Behemoth remains highly competitive and performs at or near the top of the reasoning leaderboard in its class.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-safety-and-less-political-bias\"\u003eSafety and less political ‘bias’\u003c/h2\u003e\n\n\n\n\u003cp\u003eMeta also emphasized model alignment and safety by introducing tools like Llama Guard, Prompt Guard, and CyberSecEval to help developers detect unsafe input/output or adversarial prompts, and implementing Generative Offensive Agent Testing (GOAT) for automated red-teaming.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe company also claims Llama 4 shows substantial improvement on “political bias” and says “specifically, [leading LLMs] historically have leaned left when it comes to debated political and social topics,” that that Llama 4 does better at courting the right wing…in keeping with\u003ca href=\"https://www.newsweek.com/facebook-embraces-donald-trump-mark-zuckerberg-2011026\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e Zuckerberg’s embrace of Republican U.S. president Donald J. Trump\u003c/a\u003e and his party following the 2024 election.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-where-llama-4-stands-so-far\"\u003eWhere Llama 4 stands so far\u003c/h2\u003e\n\n\n\n\u003cp\u003eMeta’s Llama 4 models bring together efficiency, openness, and high-end performance across multimodal and reasoning tasks. \u003c/p\u003e\n\n\n\n\u003cp\u003eWith Scout and Maverick now publicly available and Behemoth previewed as a state-of-the-art teacher model, the Llama ecosystem is positioned to offer a competitive open alternative to top-tier proprietary models from OpenAI, Anthropic, DeepSeek, and Google.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhether you’re building enterprise-scale assistants, AI research pipelines, or long-context analytical tools, Llama 4 offers flexible, high-performance options with a clear orientation toward reasoning-first design.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2025-04-05T21:46:57Z",
  "modifiedTime": "2025-04-05T22:27:24Z"
}
