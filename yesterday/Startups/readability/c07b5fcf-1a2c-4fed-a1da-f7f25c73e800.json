{
  "id": "c07b5fcf-1a2c-4fed-a1da-f7f25c73e800",
  "title": "OpenAI’s o3 shows remarkable progress on ARC-AGI, sparking debate on AI reasoning",
  "link": "https://venturebeat.com/ai/openais-o3-shows-remarkable-progress-on-arc-agi-sparking-debate-on-ai-reasoning/",
  "description": "o3 solved one of the most difficult AI challenges, scoring 75.7% on the ARC-AGI benchmark. But does it really mean we're closer to AGI?",
  "author": "Ben Dickson",
  "published": "Tue, 24 Dec 2024 19:40:51 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI research",
    "AI, ML and Deep Learning",
    "artificial general intelligence",
    "category-/Science/Mathematics",
    "large language models",
    "large language models (LLMs)",
    "LLM reasoning",
    "LLMs",
    "OpenAI",
    "openai o1",
    "OpenAI o3"
  ],
  "byline": "Ben Dickson",
  "length": 7939,
  "excerpt": "o3 solved one of the most difficult AI challenges, scoring 75.7% on the ARC-AGI benchmark. But does it really mean we're closer to AGI?",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 24, 2024 11:40 AM Image created with DALL-E 3 for VentureBeat Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More OpenAI’s latest o3 model has achieved a breakthrough that has surprised the AI research community. o3 scored an unprecedented 75.7% on the super-difficult ARC-AGI benchmark under standard compute conditions, with a high-compute version reaching 87.5%.  While the achievement in ARC-AGI is impressive, it does not yet prove that the code to artificial general intelligence (AGI) has been cracked. Abstract Reasoning Corpus The ARC-AGI benchmark is based on the Abstract Reasoning Corpus, which tests an AI system’s ability to adapt to novel tasks and demonstrate fluid intelligence. ARC is composed of a set of visual puzzles that require understanding of basic concepts such as objects, boundaries and spatial relationships. While humans can easily solve ARC puzzles with very few demonstrations, current AI systems struggle with them. ARC has long been considered one of the most challenging measures of AI.  Example of ARC puzzle (source: arcprize.org) ARC has been designed in a way that it can’t be cheated by training models on millions of examples in hopes of covering all possible combinations of puzzles.  The benchmark is composed of a public training set that contains 400 simple examples. The training set is complemented by a public evaluation set that contains 400 puzzles that are more challenging as a means to evaluate the generalizability of AI systems. The ARC-AGI Challenge contains private and semi-private test sets of 100 puzzles each, which are not shared with the public. They are used to evaluate candidate AI systems without running the risk of leaking the data to the public and contaminating future systems with prior knowledge. Furthermore, the competition sets limits on the amount of computation participants can use to ensure that the puzzles are not solved through brute-force methods. A breakthrough in solving novel tasks o1-preview and o1 scored a maximum of 32% on ARC-AGI. Another method developed by researcher Jeremy Berman used a hybrid approach, combining Claude 3.5 Sonnet with genetic algorithms and a code interpreter to achieve 53%, the highest score before o3. In a blog post, François Chollet, the creator of ARC, described o3’s performance as “a surprising and important step-function increase in AI capabilities, showing novel task adaptation ability never seen before in the GPT-family models.” It is important to note that using more compute on previous generations of models could not reach these results. For context, it took 4 years for models to progress from 0% with GPT-3 in 2020 to just 5% with GPT-4o in early 2024. While we don’t know much about o3’s architecture, we can be confident that it is not orders of magnitude larger than its predecessors. Performance of different models on ARC-AGI (source: arcprize.org) “This is not merely incremental improvement, but a genuine breakthrough, marking a qualitative shift in AI capabilities compared to the prior limitations of LLMs,” Chollet wrote. “o3 is a system capable of adapting to tasks it has never encountered before, arguably approaching human-level performance in the ARC-AGI domain.” It is worth noting that o3’s performance on ARC-AGI comes at a steep cost. On the low-compute configuration, it costs the model $17 to $20 and 33 million tokens to solve each puzzle, while on the high-compute budget, the model uses around 172X more compute and billions of tokens per problem. However, as the costs of inference continue to decrease, we can expect these figures to become more reasonable. A new paradigm in LLM reasoning? The key to solving novel problems is what Chollet and other scientists refer to as “program synthesis.” A thinking system should be able to develop small programs for solving very specific problems, then combine these programs to tackle more complex problems. Classic language models have absorbed a lot of knowledge and contain a rich set of internal programs. But they lack compositionality, which prevents them from figuring out puzzles that are beyond their training distribution. Unfortunately, there is very little information about how o3 works under the hood, and here, the opinions of scientists diverge. Chollet speculates that o3 uses a type of program synthesis that uses chain-of-thought (CoT) reasoning and a search mechanism combined with a reward model that evaluates and refines solutions as the model generates tokens. This is similar to what open source reasoning models have been exploring in the past few months.  Other scientists such as Nathan Lambert from the Allen Institute for AI suggest that “o1 and o3 can actually be just the forward passes from one language model.” On the day o3 was announced, Nat McAleese, a researcher at OpenAI, posted on X that o1 was “just an LLM trained with RL. o3 is powered by further scaling up RL beyond o1.” On the same day, Denny Zhou from Google DeepMind’s reasoning team called the combination of search and current reinforcement learning approaches a “dead end.”  “The most beautiful thing on LLM reasoning is that the thought process is generated in an autoregressive way, rather than relying on search (e.g. mcts) over the generation space, whether by a well-finetuned model or a carefully designed prompt,” he posted on X. While the details of how o3 reasons might seem trivial in comparison to the breakthrough on ARC-AGI, it can very well define the next paradigm shift in training LLMs. There is currently a debate on whether the laws of scaling LLMs through training data and compute have hit a wall. Whether test-time scaling depends on better training data or different inference architectures can determine the next path forward. Not AGI The name ARC-AGI is misleading and some have equated it to solving AGI. However, Chollet stresses that “ARC-AGI is not an acid test for AGI.”  “Passing ARC-AGI does not equate to achieving AGI, and, as a matter of fact, I don’t think o3 is AGI yet,” he writes. “o3 still fails on some very easy tasks, indicating fundamental differences with human intelligence.” Moreover, he notes that o3 cannot autonomously learn these skills and it relies on external verifiers during inference and human-labeled reasoning chains during training.  Other scientists have pointed to the flaws of OpenAI’s reported results. For example, the model was fine-tuned on the ARC training set to achieve state-of-the-art results. “The solver should not need much specific ‘training’, either on the domain itself or on each specific task,” writes scientist Melanie Mitchell. To verify whether these models possess the kind of abstraction and reasoning the ARC benchmark was created to measure, Mitchell proposes “seeing if these systems can adapt to variants on specific tasks or to reasoning tasks using the same concepts, but in other domains than ARC.” Chollet and his team are currently working on a new benchmark that is challenging for o3, potentially reducing its score to under 30% even at a high-compute budget. Meanwhile, humans would be able to solve 95% of the puzzles without any training. “You’ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible,” Chollet writes. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2023/11/DALL·E-2023-11-12-18.17.05-Create-an-abstract-depiction-of-artificial-general-intelligence-AGI-in-a-16_9-format.-The-image-should-feature-a-dynamic-and-complex-array-of-interc-1.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-24T19:40:51+00:00\" datetime=\"2024-12-24T19:40:51+00:00\"\u003eDecember 24, 2024 11:40 AM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"429\" src=\"https://venturebeat.com/wp-content/uploads/2023/11/DALL·E-2023-11-12-18.17.05-Create-an-abstract-depiction-of-artificial-general-intelligence-AGI-in-a-16_9-format.-The-image-should-feature-a-dynamic-and-complex-array-of-interc-1.png?w=750\" alt=\"Image created with DALL-E 3 for VentureBeat\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage created with DALL-E 3 for VentureBeat\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eOpenAI’s latest \u003ca href=\"https://venturebeat.com/ai/openai-confirms-new-frontier-models-o3-and-o3-mini/\"\u003eo3 model\u003c/a\u003e has achieved a breakthrough that has surprised the AI research community. o3 scored an unprecedented 75.7% on the super-difficult ARC-AGI benchmark under standard compute conditions, with a high-compute version reaching 87.5%. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile the achievement in ARC-AGI is impressive, it does not yet prove that the code to \u003ca href=\"https://venturebeat.com/ai/here-is-how-far-we-are-to-achieving-agi-according-to-deepmind/\"\u003eartificial general intelligence\u003c/a\u003e (AGI) has been cracked.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-abstract-reasoning-corpus\"\u003eAbstract Reasoning Corpus\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe ARC-AGI benchmark is based on the \u003ca href=\"https://aiguide.substack.com/p/why-the-abstraction-and-reasoning\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAbstract Reasoning Corpus\u003c/a\u003e, which tests an AI system’s ability to adapt to novel tasks and demonstrate fluid intelligence. ARC is composed of a set of visual puzzles that require understanding of basic concepts such as objects, boundaries and spatial relationships. While humans can easily solve ARC puzzles with very few demonstrations, current AI systems struggle with them. ARC has long been considered one of the most challenging measures of AI. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1600\" height=\"915\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/arc-agi-task-c6e1b8da.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/arc-agi-task-c6e1b8da.png 1600w, https://venturebeat.com/wp-content/uploads/2024/12/arc-agi-task-c6e1b8da.png?resize=300,172 300w, https://venturebeat.com/wp-content/uploads/2024/12/arc-agi-task-c6e1b8da.png?resize=768,439 768w, https://venturebeat.com/wp-content/uploads/2024/12/arc-agi-task-c6e1b8da.png?resize=800,458 800w, https://venturebeat.com/wp-content/uploads/2024/12/arc-agi-task-c6e1b8da.png?resize=1536,878 1536w, https://venturebeat.com/wp-content/uploads/2024/12/arc-agi-task-c6e1b8da.png?resize=400,229 400w, https://venturebeat.com/wp-content/uploads/2024/12/arc-agi-task-c6e1b8da.png?resize=750,429 750w, https://venturebeat.com/wp-content/uploads/2024/12/arc-agi-task-c6e1b8da.png?resize=578,331 578w, https://venturebeat.com/wp-content/uploads/2024/12/arc-agi-task-c6e1b8da.png?resize=930,532 930w\" sizes=\"(max-width: 1600px) 100vw, 1600px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eExample of ARC puzzle (source: arcprize.org)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eARC has been designed in a way that it can’t be cheated by training models on millions of examples in hopes of covering all possible combinations of puzzles. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe benchmark is composed of a public training set that contains 400 simple examples. The training set is complemented by a public evaluation set that contains 400 puzzles that are more challenging as a means to evaluate the generalizability of \u003ca href=\"https://venturebeat.com/ai/the-4-biggest-ai-stories-from-2024-and-one-key-prediction-for-2025/\"\u003eAI systems\u003c/a\u003e. The ARC-AGI Challenge contains private and semi-private test sets of 100 puzzles each, which are not shared with the public. They are used to evaluate candidate AI systems without running the risk of leaking the data to the public and contaminating future systems with prior knowledge. Furthermore, the competition sets limits on the amount of computation participants can use to ensure that the puzzles are not solved through brute-force methods.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-a-breakthrough-in-solving-novel-tasks\"\u003eA breakthrough in solving novel tasks\u003c/h2\u003e\n\n\n\n\u003cp\u003eo1-preview and o1 scored a maximum of 32% on ARC-AGI. Another method developed by researcher \u003ca href=\"https://jeremyberman.substack.com/p/how-i-got-a-record-536-on-arc-agi\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eJeremy Berman\u003c/a\u003e used a hybrid approach, combining Claude 3.5 Sonnet with genetic algorithms and a code interpreter to achieve 53%, the highest score before o3.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a \u003ca href=\"https://arcprize.org/blog/oai-o3-pub-breakthrough\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eblog post\u003c/a\u003e, François Chollet, the creator of ARC, described o3’s performance as “a surprising and important step-function increase in AI capabilities, showing novel task adaptation ability never seen before in the GPT-family models.”\u003c/p\u003e\n\n\n\n\u003cp\u003eIt is important to note that using more compute on previous generations of models could not reach these results. For context, it took 4 years for models to progress from 0% with GPT-3 in 2020 to just 5% with GPT-4o in early 2024. While we don’t know much about o3’s architecture, we can be confident that it is not orders of magnitude larger than its predecessors.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1200\" height=\"675\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/o-series-performance.jpg?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/o-series-performance.jpg 1200w, https://venturebeat.com/wp-content/uploads/2024/12/o-series-performance.jpg?resize=300,169 300w, https://venturebeat.com/wp-content/uploads/2024/12/o-series-performance.jpg?resize=768,432 768w, https://venturebeat.com/wp-content/uploads/2024/12/o-series-performance.jpg?resize=800,450 800w, https://venturebeat.com/wp-content/uploads/2024/12/o-series-performance.jpg?resize=400,225 400w, https://venturebeat.com/wp-content/uploads/2024/12/o-series-performance.jpg?resize=750,422 750w, https://venturebeat.com/wp-content/uploads/2024/12/o-series-performance.jpg?resize=578,325 578w, https://venturebeat.com/wp-content/uploads/2024/12/o-series-performance.jpg?resize=930,523 930w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e\u003cfigcaption\u003e\u003cem\u003ePerformance of different models on ARC-AGI (source: arcprize.org)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e“This is not merely incremental improvement, but a genuine breakthrough, marking a qualitative shift in AI capabilities compared to the prior limitations of LLMs,” Chollet wrote. “o3 is a system capable of adapting to tasks it has never encountered before, arguably approaching human-level performance in the ARC-AGI domain.”\u003c/p\u003e\n\n\n\n\u003cp\u003eIt is worth noting that o3’s performance on ARC-AGI comes at a steep cost. On the low-compute configuration, it costs the model $17 to $20 and 33 million tokens to solve each puzzle, while on the high-compute budget, the model uses around 172X more compute and billions of tokens per problem. However, as the costs of inference continue to decrease, we can expect these figures to become more reasonable.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-a-new-paradigm-in-llm-reasoning\"\u003eA new paradigm in LLM reasoning?\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe key to solving novel problems is what Chollet and other scientists refer to as “program synthesis.” A thinking system should be able to develop small programs for solving very specific problems, then combine these programs to tackle more complex problems. Classic language models have absorbed a lot of knowledge and contain a rich set of internal programs. But they lack compositionality, which prevents them from figuring out puzzles that are beyond their training distribution.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnfortunately, there is very little information about how o3 works under the hood, and here, the opinions of scientists diverge. Chollet speculates that o3 uses a type of program synthesis that uses \u003ca href=\"https://venturebeat.com/ai/whats-next-in-large-language-model-llm-research-heres-whats-coming-down-the-ml-pike/\"\u003echain-of-thought\u003c/a\u003e (CoT) reasoning and a search mechanism combined with a reward model that evaluates and refines solutions as the model generates tokens. This is similar to what \u003ca href=\"https://venturebeat.com/ai/hugging-face-shows-how-test-time-scaling-helps-small-language-models-punch-above-their-weight/\"\u003eopen source reasoning models\u003c/a\u003e have been exploring in the past few months. \u003c/p\u003e\n\n\n\n\u003cp\u003eOther scientists such as \u003ca href=\"https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eNathan Lambert\u003c/a\u003e from the Allen Institute for AI suggest that “o1 and o3 can actually be just the forward passes from one language model.” On the day o3 was announced, Nat McAleese, a researcher at OpenAI, \u003ca href=\"https://x.com/__nmca__/status/1870170101091008860\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eposted on X\u003c/a\u003e that o1 was “just an LLM trained with RL. o3 is powered by further scaling up RL beyond o1.”\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1188\" height=\"1170\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image_8d8f86.png?w=609\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image_8d8f86.png 1188w, https://venturebeat.com/wp-content/uploads/2024/12/image_8d8f86.png?resize=300,295 300w, https://venturebeat.com/wp-content/uploads/2024/12/image_8d8f86.png?resize=768,756 768w, https://venturebeat.com/wp-content/uploads/2024/12/image_8d8f86.png?resize=609,600 609w, https://venturebeat.com/wp-content/uploads/2024/12/image_8d8f86.png?resize=52,52 52w, https://venturebeat.com/wp-content/uploads/2024/12/image_8d8f86.png?resize=400,394 400w, https://venturebeat.com/wp-content/uploads/2024/12/image_8d8f86.png?resize=750,739 750w, https://venturebeat.com/wp-content/uploads/2024/12/image_8d8f86.png?resize=578,569 578w, https://venturebeat.com/wp-content/uploads/2024/12/image_8d8f86.png?resize=930,916 930w\" sizes=\"(max-width: 1188px) 100vw, 1188px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eOn the same day, Denny Zhou from Google DeepMind’s reasoning team called the combination of search and current reinforcement learning approaches a “dead end.” \u003c/p\u003e\n\n\n\n\u003cp\u003e“The most beautiful thing on LLM reasoning is that the thought process is generated in an autoregressive way, rather than relying on search (e.g. mcts) over the generation space, whether by a well-finetuned model or a carefully designed prompt,” he \u003ca href=\"https://x.com/denny_zhou/status/1870551510741811644\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eposted on X\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1188\" height=\"728\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image_313a6c.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image_313a6c.png 1188w, https://venturebeat.com/wp-content/uploads/2024/12/image_313a6c.png?resize=300,184 300w, https://venturebeat.com/wp-content/uploads/2024/12/image_313a6c.png?resize=768,471 768w, https://venturebeat.com/wp-content/uploads/2024/12/image_313a6c.png?resize=800,490 800w, https://venturebeat.com/wp-content/uploads/2024/12/image_313a6c.png?resize=400,245 400w, https://venturebeat.com/wp-content/uploads/2024/12/image_313a6c.png?resize=750,460 750w, https://venturebeat.com/wp-content/uploads/2024/12/image_313a6c.png?resize=578,354 578w, https://venturebeat.com/wp-content/uploads/2024/12/image_313a6c.png?resize=930,570 930w\" sizes=\"(max-width: 1188px) 100vw, 1188px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhile the details of how o3 reasons might seem trivial in comparison to the breakthrough on ARC-AGI, it can very well define the next paradigm shift in training LLMs. There is currently a debate on whether the laws of scaling LLMs through training data and compute have hit a wall. Whether test-time scaling depends on better training data or different inference architectures can determine the next path forward.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-not-agi\"\u003eNot AGI\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe name ARC-AGI is misleading and some have equated it to solving AGI. However, Chollet stresses that “ARC-AGI is not an acid test for AGI.” \u003c/p\u003e\n\n\n\n\u003cp\u003e“Passing ARC-AGI does not equate to achieving AGI, and, as a matter of fact, I don’t think o3 is AGI yet,” he writes. “o3 still fails on some very easy tasks, indicating fundamental differences with human intelligence.”\u003c/p\u003e\n\n\n\n\u003cp\u003eMoreover, he notes that o3 cannot autonomously learn these skills and it relies on external verifiers during inference and human-labeled reasoning chains during training. \u003c/p\u003e\n\n\n\n\u003cp\u003eOther scientists have pointed to the flaws of OpenAI’s reported results. For example, the model was fine-tuned on the ARC training set to achieve state-of-the-art results. “The solver should not need much specific ‘training’, either on the domain itself or on each specific task,” writes scientist \u003ca href=\"https://aiguide.substack.com/p/did-openai-just-solve-abstract-reasoning\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMelanie Mitchell\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo verify whether these models possess the kind of abstraction and reasoning the ARC benchmark was created to measure, Mitchell proposes “seeing if these systems can adapt to variants on specific tasks or to reasoning tasks using the same concepts, but in other domains than ARC.”\u003c/p\u003e\n\n\n\n\u003cp\u003eChollet and his team are currently working on a new benchmark that is challenging for o3, potentially reducing its score to under 30% even at a high-compute budget. Meanwhile, humans would be able to solve 95% of the puzzles without any training.\u003c/p\u003e\n\n\n\n\u003cp\u003e“You’ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible,” Chollet writes.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2024-12-24T19:40:51Z",
  "modifiedTime": "2024-12-24T19:41:33Z"
}
