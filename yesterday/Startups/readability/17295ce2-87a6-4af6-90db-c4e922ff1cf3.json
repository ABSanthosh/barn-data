{
  "id": "17295ce2-87a6-4af6-90db-c4e922ff1cf3",
  "title": "ChatGPT gets screensharing and real-time video analysis, rivaling Gemini 2",
  "link": "https://venturebeat.com/ai/chatgpt-gets-screensharing-and-real-time-video-analysis-rivaling-gemini-2/",
  "description": "OpenAI now lets users video chat with ChatGPT in advanced voice mode, and the chatbot will respond to real-time images.",
  "author": "Emilia David",
  "published": "Thu, 12 Dec 2024 21:55:49 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "advanced voice mode",
    "AI, ML and Deep Learning",
    "category-/Computers \u0026 Electronics",
    "category-/Internet \u0026 Telecom/Mobile \u0026 Wireless/Mobile Apps \u0026 Add-Ons",
    "ChatGPT",
    "ChatGPT Edu",
    "ChatGPT Enterprise",
    "ChatGPT Plus",
    "ChatGPT Teams",
    "Conversational AI",
    "Gemini",
    "Google",
    "Microsoft",
    "OpenAI",
    "Text-to-Speech"
  ],
  "byline": "Emilia David",
  "length": 4285,
  "excerpt": "OpenAI now lets users video chat with ChatGPT in advanced voice mode, and the chatbot will respond to real-time images.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 12, 2024 1:55 PM Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More OpenAI finally added long-awaited video and screen sharing to its advanced voice mode, allowing users to interact with the chatbot in different modalities. Both capabilities are now available on iOS and Android mobile apps for ChatGPT Teams, Plus and Pro users, and will be rolled out to ChatGPT Enterprise and Edu subscribers in January. However, users in the EU, Switzerland, Iceland, Norway and Liechtenstein won’t be able to access advanced voice mode. OpenAI first teased the feature in May, when the company unveiled GPT-4o and discussed ChatGPT learning to “watch” a game and explain what’s happening. Advanced voice mode was rolled out to users in September. Credit: OpenAI Users can access video via new buttons on the advanced voice mode screen to start a video.  OpenAI’s video mode feels like a video call like Facetime, because ChatGPT responds in real-time to what users show in the video. It can see what is around the user, identify objects and even remember people who introduce themselves. In an OpenAI demo as part of the company’s “12 Days of Shipmas” event, ChatGPT used the video feature to help brew coffee. ChatGPT saw the coffee paraphernalia, instructed when to put in a filter and critiqued the result.  It is also very similar to Google’s recently announced Project Astra, in which users can open a video chat, and Gemini 2.0 will respond to questions about what it sees, like identifying a sculpture found in a London street. In many ways, these features are more advanced versions of what AI devices like the Humane Pin and the Rabbit r1 were marketed to do: Have an AI voice assistant respond to questions about what it’s seeing in a video.  Sharing a screen  The new screen-sharing feature brings ChatGPT out of the app and into the realm of the browser.  For screen share, a three-dot menu allows users to navigate out of the ChatGPT app. They can open apps on their phones and ask ChatGPT questions about what it’s seeing. In the demo, OpenAI researchers triggered screen share, then opened the messages app to ask ChatGPT for help responding to a photo sent via text message.  However, the screen-sharing feature on advanced voice mode bears similarities to recently released features from Microsoft and Google.  Last week, Microsoft released a preview version of Copilot Vision, which lets Pro subscribers open a Copilot chat while browsing a webpage. Copilot Vision can look at photos on a store’s website or even help play the map guessing game Geoguessr. Google’s Project Astra can also read browsers in the same way.  Both Google and OpenAI released screen-sharing AI chat features on phones to target the consumer base who may be using ChatGPT or Gemini more on the go. But these types of features could signal a way for enterprises to collaborate more with AI agents, as the agent can see what a person is looking at onscreen. It can be a precursor to models that use computers, like Anthropic’s Computer Use, where the AI model is not only looking at a screen but is actively opening tabs and programs for the user.  Ho ho ho, ask Santa a question  In a bid for levity, OpenAI also rolled out “Santa Mode” in advanced voice mode. The new preset voice sounds much like the jolly old man in a red suit. Unlike the new features restricted to specific users, “Santa Mode” is now available to users with access to advanced voice mode on the mobile app, the web version of ChatGPT and the Windows and MacOS apps until early January.  Chats with Santa, though, will not be saved in chat history and will not affect ChatGPT’s memory.  Even OpenAI is feeling the Christmas spirit. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/SantaVoice_Still.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-12T21:55:49+00:00\" datetime=\"2024-12-12T21:55:49+00:00\"\u003eDecember 12, 2024 1:55 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"422\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/SantaVoice_Still.png?w=750\" alt=\"ChatGPT Advanced Voice Mode Santa\"/\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://www.openai.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI\u003c/a\u003e finally added long-awaited video and screen sharing to its advanced voice mode, allowing users to interact with the chatbot in different modalities.\u003c/p\u003e\n\n\n\n\u003cp\u003eBoth capabilities are now available on iOS and Android mobile apps for ChatGPT Teams, Plus and Pro users, and will be rolled out to ChatGPT Enterprise and Edu subscribers in January. However, users in the EU, Switzerland, Iceland, Norway and Liechtenstein won’t be able to access advanced voice mode. \u003c/p\u003e\n\n\n\n\u003cp\u003eOpenAI first teased the feature in May, when the company unveiled GPT-4o and discussed ChatGPT learning to “watch” a game and explain what’s happening. \u003ca href=\"https://venturebeat.com/ai/openai-finally-brings-humanlike-chatgpt-advanced-voice-mode-to-u-s-plus-team-users/\"\u003eAdvanced voice mode was rolled out\u003c/a\u003e to users in September.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\n\u003ciframe title=\"Santa Mode \u0026amp; Video in Advanced Voice—12 Days of OpenAI: Day 6\" width=\"500\" height=\"281\" src=\"https://www.youtube.com/embed/NIQDnWlwYyQ?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003c/p\u003e\u003cfigcaption\u003e\u003cem\u003eCredit: OpenAI\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eUsers can access video via new buttons on the advanced voice mode screen to start a video. \u003c/p\u003e\n\n\n\n\u003cp\u003eOpenAI’s video mode feels like a video call like Facetime, because ChatGPT responds in real-time to what users show in the video. It can see what is around the user, identify objects and even remember people who introduce themselves. In an OpenAI demo as part of the company’s “12 Days of Shipmas” event, ChatGPT used the video feature to help brew coffee. ChatGPT saw the coffee paraphernalia, instructed when to put in a filter and critiqued the result. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt is also very similar to \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle’s\u003c/a\u003e \u003ca href=\"https://venturebeat.com/ai/google-gemini-2-0-could-this-be-the-beginning-of-truly-autonomous-ai/\"\u003erecently announced Project Astra\u003c/a\u003e, in which users can open a video chat, and Gemini 2.0 will respond to questions about what it sees, like identifying a sculpture found in a London street. In many ways, these features are more advanced versions of what AI devices like the Humane Pin and the \u003ca href=\"https://venturebeat.com/ai/rabbit-unveils-r1-ai-pocket-companion-to-handle-tasks-for-you/\"\u003eRabbit r1\u003c/a\u003e were marketed to do: Have an AI voice assistant respond to questions about what it’s seeing in a video. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-sharing-a-screen-nbsp\"\u003eSharing a screen \u003c/h2\u003e\n\n\n\n\u003cp\u003eThe new screen-sharing feature brings ChatGPT out of the app and into the realm of the browser. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor screen share, a three-dot menu allows users to navigate out of the ChatGPT app. They can open apps on their phones and ask ChatGPT questions about what it’s seeing. In the demo, OpenAI researchers triggered screen share, then opened the messages app to ask ChatGPT for help responding to a photo sent via text message. \u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, the screen-sharing feature on advanced voice mode bears similarities to recently released features from \u003ca href=\"https://microsoft.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMicrosoft\u003c/a\u003e and Google. \u003c/p\u003e\n\n\n\n\u003cp\u003eLast week, Microsoft released a \u003ca href=\"https://venturebeat.com/ai/microsoft-copilot-vision-is-here-letting-ai-see-what-you-do-online/\"\u003epreview version of Copilot Vision\u003c/a\u003e, which lets Pro subscribers open a Copilot chat while browsing a webpage. Copilot Vision can look at photos on a store’s website or even help play the map guessing game Geoguessr. Google’s Project Astra can also read browsers in the same way. \u003c/p\u003e\n\n\n\n\u003cp\u003eBoth Google and OpenAI released screen-sharing AI chat features on phones to target the consumer base who may be using ChatGPT or Gemini more on the go. But these types of features could signal a way for enterprises to collaborate more with AI agents, as the agent can see what a person is looking at onscreen. It can be a precursor to models that use computers, like \u003ca href=\"https://www.anthropic.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAnthropic’s\u003c/a\u003e \u003ca href=\"https://venturebeat.com/ai/anthropic-new-ai-can-use-computers-like-a-human-redefining-automation-for-enterprises/#:~:text=The%20new%20%E2%80%9CComputer%20Use%E2%80%9D%20feature,interfaces%2C%20and%20filling%20out%20forms.\"\u003eComputer Use\u003c/a\u003e, where the AI model is not only looking at a screen but is actively opening tabs and programs for the user. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-ho-ho-ho-ask-santa-a-question-nbsp\"\u003eHo ho ho, ask Santa a question \u003c/h2\u003e\n\n\n\n\u003cp\u003eIn a bid for levity, OpenAI also rolled out “Santa Mode” in advanced voice mode. The new preset voice sounds much like the jolly old man in a red suit.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnlike the new features restricted to specific users, “Santa Mode” is now available to users with access to advanced voice mode on the mobile app, the web version of ChatGPT and the Windows and MacOS apps until early January. \u003c/p\u003e\n\n\n\n\u003cp\u003eChats with Santa, though, will not be saved in chat history and will not affect ChatGPT’s memory. \u003c/p\u003e\n\n\n\n\u003cp\u003eEven OpenAI is feeling the Christmas spirit.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-12-12T21:55:49Z",
  "modifiedTime": "2024-12-12T22:00:23Z"
}
