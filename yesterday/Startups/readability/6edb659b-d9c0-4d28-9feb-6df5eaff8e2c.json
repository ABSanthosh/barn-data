{
  "id": "6edb659b-d9c0-4d28-9feb-6df5eaff8e2c",
  "title": "How S\u0026P is using deep web scraping, ensemble learning and Snowflake architecture to collect 5X more data on SMEs",
  "link": "https://venturebeat.com/data-infrastructure/how-sp-is-using-deep-web-scraping-ensemble-learning-and-snowflake-architecture-to-collect-5x-more-data-on-smes/",
  "description": "Previously, S\u0026P only had data on about 2 million SMEs, but its AI-powered RiskGauge platform expanded that to 10 million.",
  "author": "Taryn Plumb",
  "published": "Mon, 02 Jun 2025 21:45:06 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Data Infrastructure",
    "AI, ML and Deep Learning",
    "Big Data and Analytics",
    "data",
    "Data Management",
    "Generative AI",
    "large language models",
    "RiskGauge",
    "S\u0026P",
    "SMEs",
    "Snowflake",
    "Snowpark"
  ],
  "byline": "Taryn Plumb",
  "length": 7575,
  "excerpt": "Previously, S\u0026P only had data on about 2 million SMEs, but its AI-powered RiskGauge platform expanded that to 10 million.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More The investing world has a significant problem when it comes to data about small and medium-sized enterprises (SMEs). This has nothing to do with data quality or accuracy — it’s the lack of any data at all.  Assessing SME creditworthiness has been notoriously challenging because small enterprise financial data is not public, and therefore very difficult to access. S\u0026P Global Market Intelligence, a division of S\u0026P Global and a foremost provider of credit ratings and benchmarks, claims to have solved this longstanding problem. The company’s technical team built RiskGauge, an AI-powered platform that crawls otherwise elusive data from over 200 million websites, processes it through numerous algorithms and generates risk scores.  Built on Snowflake architecture, the platform has increased S\u0026P’s coverage of SMEs by 5X.  “Our objective was expansion and efficiency,” explained Moody Hadi, S\u0026P Global’s head of risk solutions’ new product development. “The project has improved the accuracy and coverage of the data, benefiting clients.”  RiskGauge’s underlying architecture Counterparty credit management essentially assesses a company’s creditworthiness and risk based on several factors, including financials, probability of default and risk appetite. S\u0026P Global Market Intelligence provides these insights to institutional investors, banks, insurance companies, wealth managers and others.  “Large and financial corporate entities lend to suppliers, but they need to know how much to lend, how frequently to monitor them, what the duration of the loan would be,” Hadi explained. “They rely on third parties to come up with a trustworthy credit score.”  But there has long been a gap in SME coverage. Hadi pointed out that, while large public companies like IBM, Microsoft, Amazon, Google and the rest are required to disclose their quarterly financials, SMEs don’t have that obligation, thus limiting financial transparency. From an investor perspective, consider that there are about 10 million SMEs in the U.S., compared to roughly 60,000 public companies.  S\u0026P Global Market Intelligence claims it now has all of those covered: Previously, the firm only had data on about 2 million, but RiskGauge expanded that to 10 million.   The platform, which went into production in January, is based on a system built by Hadi’s team that pulls firmographic data from unstructured web content, combines it with anonymized third-party datasets, and applies machine learning (ML) and advanced algorithms to generate credit scores.  The company uses Snowflake to mine company pages and process them into firmographics drivers (market segmenters) that are then fed into RiskGauge.  The platform’s data pipeline consists of: Crawlers/web scrapers A pre-processing layer Miners Curators RiskGauge scoring Specifically, Hadi’s team uses Snowflake’s data warehouse and Snowpark Container Services in the middle of the pre-processing, mining and curation steps.  At the end of this process, SMEs are scored based on a combination of financial, business and market risk; 1 being the highest, 100 the lowest. Investors also receive reports on RiskGauge detailing financials, firmographics, business credit reports, historical performance and key developments. They can also compare companies to their peers.  How S\u0026P is collecting valuable company data Hadi explained that RiskGauge employs a multi-layer scraping process that pulls various details from a company’s web domain, such as basic ‘contact us’ and landing pages and news-related information. The miners go down several URL layers to scrape relevant data.  “As you can imagine, a person can’t do this,” said Hadi. “It is going to be very time-consuming for a human, especially when you’re dealing with 200 million web pages.” Which, he noted, results in several terabytes of website information.  After data is collected, the next step is to run algorithms that remove anything that isn’t text; Hadi noted that the system is not interested in JavaScript or even HTML tags. Data is cleaned so it becomes human-readable, not code. Then, it’s loaded into Snowflake and several data miners are run against the pages. Ensemble algorithms are critical to the prediction process; these types of algorithms combine predictions from several individual models (base models or ‘weak learners’ that are essentially a little better than random guessing) to validate company information such as name, business description, sector, location, and operational activity. The system also factors in any polarity in sentiment around announcements disclosed on the site.  “After we crawl a site, the algorithms hit different components of the pages pulled, and they vote and come back with a recommendation,” Hadi explained. “There is no human in the loop in this process, the algorithms are basically competing with each other. That helps with the efficiency to increase our coverage.”  Following that initial load, the system monitors site activity, automatically running weekly scans. It doesn’t update information weekly; only when it detects a change, Hadi added. When performing subsequent scans, a hash key tracks the landing page from the previous crawl, and the system generates another key; if they are identical, no changes were made, and no action is required. However, if the hash keys don’t match, the system will be triggered to update company information.  This continuous scraping is important to ensure the system remains as up-to-date as possible. “If they’re updating the site often, that tells us they’re alive, right?,” Hadi noted.  Challenges with processing speed, giant datasets, unclean websites There were challenges to overcome when building out the system, of course, particularly due to the sheer size of datasets and the need for quick processing. Hadi’s team had to make trade-offs to balance accuracy and speed.  “We kept optimizing different algorithms to run faster,” he explained. “And tweaking; some algorithms we had were really good, had high accuracy, high precision, high recall, but they were computationally too costly.”  Websites do not always conform to standard formats, requiring flexible scraping methods. “You hear a lot about designing websites with an exercise like this, because when we originally started, we thought, ‘Hey, every website should conform to a sitemap or XML,’” said Hadi. “And guess what? Nobody follows that.” They didn’t want to hard code or incorporate robotic process automation (RPA) into the system because sites vary so widely, Hadi said, and they knew the most important information they needed was in the text. This led to the creation of a system that only pulls necessary components of a site, then cleanses it for the actual text and discards code and any JavaScript or TypeScript. As Hadi noted, “the biggest challenges were around performance and tuning and the fact that websites by design are not clean.”  Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/05/SP.jpeg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eThe investing world has a significant problem when it comes to data about small and medium-sized enterprises (SMEs). This has nothing to do with data quality or accuracy — it’s the lack of any data at all. \u003c/p\u003e\n\n\n\n\u003cp\u003eAssessing SME creditworthiness has been notoriously challenging because small enterprise financial data is not public, and therefore very difficult to access.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://www.spglobal.com/market-intelligence/en\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eS\u0026amp;P Global Market Intelligence\u003c/a\u003e, a division of S\u0026amp;P Global and a foremost provider of credit ratings and benchmarks, claims to have solved this longstanding problem. The company’s technical team built \u003ca href=\"https://www.spglobal.com/market-intelligence/en/solutions/products/riskgauge-desktop\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRiskGauge\u003c/a\u003e, an AI-powered platform that crawls otherwise elusive data from over 200 million websites, processes it through numerous algorithms and generates risk scores. \u003c/p\u003e\n\n\n\n\u003cp\u003eBuilt on Snowflake architecture, the platform has increased S\u0026amp;P’s coverage of SMEs by 5X. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Our objective was expansion and efficiency,” explained Moody Hadi, S\u0026amp;P Global’s head of risk solutions’ new product development. “The project has improved the accuracy and coverage of the data, benefiting clients.” \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-riskgauge-s-underlying-architecture\"\u003eRiskGauge’s underlying architecture\u003c/h2\u003e\n\n\n\n\u003cp\u003eCounterparty credit management essentially assesses a company’s creditworthiness and risk based on several factors, including financials, probability of default and risk appetite. S\u0026amp;P Global Market Intelligence provides these insights to institutional investors, banks, insurance companies, wealth managers and others. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Large and financial corporate entities lend to suppliers, but they need to know how much to lend, how frequently to monitor them, what the duration of the loan would be,” Hadi explained. “They rely on third parties to come up with a trustworthy credit score.” \u003c/p\u003e\n\n\n\n\u003cp\u003eBut there has long been a gap in SME coverage. Hadi pointed out that, while large public companies like IBM, Microsoft, Amazon, Google and the rest are required to disclose their quarterly financials, SMEs don’t have that obligation, thus limiting financial transparency. From an investor perspective, consider that there are about 10 million SMEs in the U.S., compared to roughly 60,000 public companies. \u003c/p\u003e\n\n\n\n\u003cp\u003eS\u0026amp;P Global Market Intelligence claims it now has all of those covered: Previously, the firm only had data on about 2 million, but RiskGauge expanded that to 10 million.  \u003c/p\u003e\n\n\n\n\u003cp\u003eThe platform, which went into production in January, is based on a system built by Hadi’s team that pulls firmographic data from unstructured web content, combines it with anonymized third-party datasets, and applies machine learning (ML) and \u003ca href=\"https://venturebeat.com/ai/everyones-looking-to-get-in-on-vibe-coding-and-google-is-no-different-with-stitch-its-follow-up-to-jules/\"\u003eadvanced algorithms\u003c/a\u003e to generate credit scores. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe company uses \u003ca href=\"https://venturebeat.com/ai/how-snowflakes-open-source-text-to-sql-and-arctic-inference-models-solve-enterprise-ais-two-biggest-deployment-headaches/\"\u003eSnowflake\u003c/a\u003e to mine company pages and process them into firmographics drivers (market segmenters) that are then fed into RiskGauge. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe platform’s data pipeline consists of:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eCrawlers/web scrapers\u003c/li\u003e\n\n\n\n\u003cli\u003eA pre-processing layer\u003c/li\u003e\n\n\n\n\u003cli\u003eMiners\u003c/li\u003e\n\n\n\n\u003cli\u003eCurators\u003c/li\u003e\n\n\n\n\u003cli\u003eRiskGauge scoring\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eSpecifically, Hadi’s team uses Snowflake’s data warehouse and Snowpark Container Services in the middle of the pre-processing, mining and curation steps. \u003c/p\u003e\n\n\n\n\u003cp\u003eAt the end of this process, SMEs are scored based on a combination of financial, business and market risk; 1 being the highest, 100 the lowest. Investors also receive reports on RiskGauge detailing financials, firmographics, business credit reports, historical performance and key developments. They can also compare companies to their peers. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-s-amp-p-is-collecting-valuable-company-data\"\u003eHow S\u0026amp;P is collecting valuable company data\u003c/h2\u003e\n\n\n\n\u003cp\u003eHadi explained that RiskGauge employs a multi-layer scraping process that pulls various details from a company’s web domain, such as basic ‘contact us’ and landing pages and news-related information. The miners go down several URL layers to scrape relevant data. \u003c/p\u003e\n\n\n\n\u003cp\u003e“As you can imagine, a person can’t do this,” said Hadi. “It is going to be very time-consuming for a human, especially when you’re dealing with 200 million web pages.” Which, he noted, results in several terabytes of website information. \u003c/p\u003e\n\n\n\n\u003cp\u003eAfter data is collected, the next step is to run algorithms that remove anything that isn’t text; Hadi noted that the system is not interested in JavaScript or even HTML tags. Data is cleaned so it becomes human-readable, not code. Then, it’s loaded into \u003ca href=\"https://venturebeat.com/ai/snowflake-expands-ai-tools-with-anthropic-partnership-what-it-means-for-businesses/\"\u003eSnowflake\u003c/a\u003e and several data miners are run against the pages.\u003c/p\u003e\n\n\n\n\u003cp\u003eEnsemble algorithms are critical to the prediction process; these types of algorithms combine predictions from several individual models (base models or ‘weak learners’ that are essentially a little better than random guessing) to validate company information such as name, business description, sector, location, and operational activity. The system also factors in any polarity in sentiment around announcements disclosed on the site. \u003c/p\u003e\n\n\n\n\u003cp\u003e“After we crawl a site, the algorithms hit different components of the pages pulled, and they vote and come back with a recommendation,” Hadi explained. “There is no human in the loop in this process, the algorithms are basically competing with each other. That helps with the efficiency to increase our coverage.” \u003c/p\u003e\n\n\n\n\u003cp\u003eFollowing that initial load, the system monitors site activity, automatically running weekly scans. It doesn’t update information weekly; only when it detects a change, Hadi added. When performing subsequent scans, a hash key tracks the landing page from the previous crawl, and the system generates another key; if they are identical, no changes were made, and no action is required. However, if the hash keys don’t match, the system will be triggered to update company information. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis continuous scraping is important to ensure the system remains as up-to-date as possible. “If they’re updating the site often, that tells us they’re alive, right?,” Hadi noted. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-challenges-with-processing-speed-giant-datasets-unclean-websites\"\u003eChallenges with processing speed, giant datasets, unclean websites\u003c/h2\u003e\n\n\n\n\u003cp\u003eThere were challenges to overcome when building out the system, of course, particularly due to the sheer size of datasets and the need for quick processing. Hadi’s team had to make trade-offs to balance accuracy and speed. \u003c/p\u003e\n\n\n\n\u003cp\u003e“We kept optimizing different algorithms to run faster,” he explained. “And tweaking; some algorithms we had were really good, had high accuracy, high precision, high recall, but they were computationally too costly.” \u003c/p\u003e\n\n\n\n\u003cp\u003eWebsites do not always conform to standard formats, requiring flexible scraping methods.\u003c/p\u003e\n\n\n\n\u003cp\u003e“You hear a lot about designing websites with an exercise like this, because when we originally started, we thought, ‘Hey, every website should conform to a sitemap or XML,’” said Hadi. “And guess what? Nobody follows that.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThey didn’t want to hard code or incorporate robotic process automation (RPA) into the system because sites vary so widely, Hadi said, and they knew the most important information they needed was in the text. This led to the creation of a system that only pulls necessary components of a site, then cleanses it for the actual text and discards code and any JavaScript or TypeScript.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs Hadi noted, “the biggest challenges were around performance and tuning and the fact that websites by design are not clean.” \u003cbr/\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-06-02T21:45:06Z",
  "modifiedTime": "2025-06-02T21:45:19Z"
}
