{
  "id": "b281a58d-44d1-4e50-97ba-a83fe8a79063",
  "title": "Meta launches open source Llama 3.3, shrinking powerful bigger model into smaller size",
  "link": "https://venturebeat.com/ai/meta-launches-open-source-llama-3-3-shrinking-powerful-bigger-model-into-smaller-size/",
  "description": "The 70B-Llama 3.3 is specifically optimized for cost-effective inference, with token generation costs as low as $0.01 per million tokens.",
  "author": "Carl Franzen",
  "published": "Fri, 06 Dec 2024 18:24:35 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI, ML and Deep Learning",
    "Conversational AI",
    "LLaMA",
    "Llama 3.1",
    "llama 3.1-405b",
    "llama 3.3",
    "llama 405b",
    "Meta",
    "NLP",
    "Open source"
  ],
  "byline": "Carl Franzen",
  "length": 5713,
  "excerpt": "The 70B-Llama 3.3 is specifically optimized for cost-effective inference, with token generation costs as low as $0.01 per million tokens.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 6, 2024 10:24 AM Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Meta’s VP of generative AI, Ahmad Al-Dahle took to rival social network X today to announce the release of Llama 3.3, the latest open-source multilingual large language model (LLM) from the parent company of Facebook, Instagram, WhatsApp and Quest VR. As he wrote: “Llama 3.3 improves core performance at a significantly lower cost, making it even more accessible to the entire open-source community.” With 70 billion parameters — or settings governing the model’s behavior — Llama 3.3 delivers results on par with Meta’s 405B parameter model from the Llama 3.1 from the summer, but at a fraction of the cost and computational overhead — e.g., the GPU capacity needed to run the model in an inference. It’s designed to offer top-tier performance and accessibility yet in a smaller package than prior foundation models. Meta’s Llama 3.3 is offered under the Llama 3.3 Community License Agreement, which grants a non-exclusive, royalty-free license for use, reproduction, distribution, and modification of the model and its outputs. Developers integrating Llama 3.3 into products or services must include appropriate attribution, such as “Built with Llama,” and adhere to an Acceptable Use Policy that prohibits activities like generating harmful content, violating laws, or enabling cyberattacks. While the license is generally free, organizations with over 700 million monthly active users must obtain a commercial license directly from Meta. A statement from the AI at Meta team underscores this vision: “Llama 3.3 delivers leading performance and quality across text-based use cases at a fraction of the inference cost.” How much savings are we talkin’ about, really? Some back-of-the-envelope math: Llama 3.1-405B requires between 243 GB and 1944 GB of GPU memory, according to the Substratus blog (for the open source cross cloud substrate). Meanwhile, the older Llama 2-70B requires between 42-168 GB of GPU memory, according to the same blog, though same have claimed as low as 4 GB, or as Exo Labs has shown, a few Mac computers with M4 chips and no discrete GPUs. Therefore, if the GPU savings for lower-parameter models holds up in this case, those looking to deploy Meta’s most powerful open source Llama models can expect to save up to nearly 1940 GB worth of GPU memory, or potentially, 24 times reduced GPU load for a standard 80 GB Nvidia H100 GPU. At an estimated $25,000 per H100 GPU, that’s up to $600,000 in up-front GPU cost savings, potentially — not to mention the continuous power costs. A highly performant model in a small form factor According to Meta AI on X, the Llama 3.3 model handedly outperforms the identically sized Llama 3.1-70B as well as Amazon’s new Nova Pro model in several benchmarks such as multilingual dialogue, reasoning, and other advanced natural language processing (NLP) tasks (Nova outperforms it in HumanEval coding tasks). Llama 3.3 has been pretrained on 15 trillion tokens from “publicly available” data and fine-tuned on over 25 million synthetically generated examples, according to the information Meta provided in the “model card” posted on its website. Leveraging 39.3 million GPU hours on H100-80GB hardware, the model’s development underscores Meta’s commitment to energy efficiency and sustainability. Llama 3.3 leads in multilingual reasoning tasks with a 91.1% accuracy rate on MGSM, demonstrating its effectiveness in supporting languages such as German, French, Italian, Hindi, Portuguese, Spanish, and Thai, in addition to English. Cost-effective and environmentally conscious Llama 3.3 is specifically optimized for cost-effective inference, with token generation costs as low as $0.01 per million tokens. This makes the model highly competitive against industry counterparts like GPT-4 and Claude 3.5, with greater affordability for developers seeking to deploy sophisticated AI solutions. Meta has also emphasized the environmental responsibility of this release. Despite its intensive training process, the company leveraged renewable energy to offset greenhouse gas emissions, resulting in net-zero emissions for the training phase. Location-based emissions totaled 11,390 tons of CO2-equivalent, but Meta’s renewable energy initiatives ensured sustainability. Advanced features and deployment options The model introduces several enhancements, including a longer context window of 128k tokens (comparable to GPT-4o, about 400 pages of book text), making it suitable for long-form content generation and other advanced use cases. Its architecture incorporates Grouped Query Attention (GQA), improving scalability and performance during inference. Designed to align with user preferences for safety and helpfulness, Llama 3.3 uses reinforcement learning with human feedback (RLHF) and supervised fine-tuning (SFT). This alignment ensures robust refusals to inappropriate prompts and an assistant-like behavior optimized for real-world applications. Llama 3.3 is already available for download through Meta, Hugging Face, GitHub, and other platforms, with integration options for researchers and developers. Meta is also offering resources like Llama Guard 3 and Prompt Guard to help users deploy the model safely and responsibly. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/cfr0z3n_pop_art_collage_screen_printed_style_image_of_three_ide_11ef319a-2a12-402e-9ae7-358d2443fdda.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-06T18:24:35+00:00\" datetime=\"2024-12-06T18:24:35+00:00\"\u003eDecember 6, 2024 10:24 AM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/cfr0z3n_pop_art_collage_screen_printed_style_image_of_three_ide_11ef319a-2a12-402e-9ae7-358d2443fdda.png?w=750\" alt=\"\"/\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eMeta’s VP of generative AI, Ahmad Al-Dahle took to rival social network X today to \u003ca href=\"https://x.com/Ahmad_Al_Dahle/status/1865071436630778109\"\u003eannounce the release\u003c/a\u003e of \u003ca href=\"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/#introduction\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLlama 3.3,\u003c/a\u003e the latest open-source multilingual large language model (LLM) from the parent company of Facebook, Instagram, WhatsApp and Quest VR.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs he wrote: “Llama 3.3 improves core performance at a significantly lower cost, making it even more accessible to the entire open-source community.”\u003c/p\u003e\n\n\n\n\u003cp\u003eWith 70 billion parameters — or settings governing the model’s behavior — Llama 3.3 delivers results on par with \u003ca href=\"https://venturebeat.com/ai/meta-unleashes-its-most-powerful-ai-model-llama-3-1-with-405b-parameters/\"\u003eMeta’s 405B parameter model from the Llama 3.1\u003c/a\u003e from the summer, but at a fraction of the cost and computational overhead — e.g., the GPU capacity needed to run the model in an inference.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt’s designed to offer top-tier performance and accessibility yet in a smaller package than prior foundation models.\u003c/p\u003e\n\n\n\n\u003cp\u003eMeta’s Llama 3.3 is offered under the \u003ca href=\"https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLlama 3.3 Community License Agreement\u003c/a\u003e, which grants a non-exclusive, royalty-free license for use, reproduction, distribution, and modification of the model and its outputs. Developers integrating Llama 3.3 into products or services must include appropriate attribution, such as “Built with Llama,” and adhere to an Acceptable Use Policy that prohibits activities like generating harmful content, violating laws, or enabling cyberattacks. While the license is generally free, organizations with over 700 million monthly active users must obtain a commercial license directly from Meta.\u003c/p\u003e\n\n\n\n\u003cp\u003eA statement from the AI at Meta team underscores this vision: “Llama 3.3 delivers leading performance and quality across text-based use cases at a fraction of the inference cost.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-much-savings-are-we-talkin-about-really-some-back-of-the-envelope-math\"\u003eHow much savings are we talkin’ about, really? Some back-of-the-envelope math:\u003c/h2\u003e\n\n\n\n\u003cp\u003eLlama 3.1-405B requires between 243 GB and 1944 GB of GPU memory, according to the\u003ca href=\"https://www.substratus.ai/blog/llama-3-1-405b-gpu-requirements\"\u003e Substratus blog\u003c/a\u003e (for \u003ca href=\"https://www.linkedin.com/posts/samstoelinga_introducing-substratus-substratus-activity-7093353427119849474-P5nx/?trk=public_profile_like_view\"\u003ethe open source cross cloud substrate)\u003c/a\u003e. Meanwhile, the older Llama 2-70B requires between 42-168 GB of GPU memory, according to the \u003ca href=\"https://www.substratus.ai/blog/calculating-gpu-memory-for-llm\"\u003esame blog\u003c/a\u003e, though same have \u003ca href=\"https://huggingface.co/blog/lyogavin/airllm\"\u003eclaimed as low as 4 GB\u003c/a\u003e, or as \u003ca href=\"https://venturebeat.com/ai/you-can-now-run-the-most-powerful-open-source-ai-models-locally-on-mac-m4-computers-thanks-to-exo-labs/\"\u003eExo Labs has shown, a few Mac computers with M4 chips\u003c/a\u003e and no discrete GPUs.\u003c/p\u003e\n\n\n\n\u003cp\u003eTherefore, if the GPU savings for lower-parameter models holds up in this case, those looking to deploy Meta’s most powerful open source Llama models can expect to save up to nearly 1940 GB worth of GPU memory, or potentially, 24 times reduced GPU load for a standard \u003ca href=\"https://www.techpowerup.com/gpu-specs/h100-pcie-80-gb.c3899\"\u003e80 GB Nvidia H100 GPU\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eAt an estimated \u003ca href=\"https://modal.com/blog/nvidia-h100-price-article\"\u003e$25,000 per H100 GPU\u003c/a\u003e, that’s up to $600,000 in up-front GPU cost savings, potentially — not to mention the continuous power costs. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-a-highly-performant-model-in-a-small-form-factor\"\u003eA highly performant model in a small form factor\u003c/h2\u003e\n\n\n\n\u003cp\u003eAccording to \u003ca href=\"https://x.com/AIatMeta/status/1865079067390956006\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMeta AI on X\u003c/a\u003e, the Llama 3.3 model handedly outperforms the identically sized Llama 3.1-70B as well as \u003ca href=\"https://venturebeat.com/ai/amazon-launches-nova-ai-model-family-for-generating-text-images-and-videos/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAmazon’s new Nova Pro model in several benchmarks\u003c/a\u003e such as multilingual dialogue, reasoning, and other advanced natural language processing (NLP) tasks (Nova outperforms it in HumanEval coding tasks).\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"680\" height=\"530\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/GeIXhxyakAErybT.jpg\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/GeIXhxyakAErybT.jpg 680w, https://venturebeat.com/wp-content/uploads/2024/12/GeIXhxyakAErybT.jpg?resize=300,234 300w, https://venturebeat.com/wp-content/uploads/2024/12/GeIXhxyakAErybT.jpg?resize=400,312 400w, https://venturebeat.com/wp-content/uploads/2024/12/GeIXhxyakAErybT.jpg?resize=578,451 578w\" sizes=\"(max-width: 680px) 100vw, 680px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eLlama 3.3 has been pretrained on 15 trillion tokens from “publicly available” data and fine-tuned on over 25 million synthetically generated examples, according to the information Meta provided in the “model card” posted on its website.\u003c/p\u003e\n\n\n\n\u003cp\u003eLeveraging 39.3 million GPU hours on H100-80GB hardware, the model’s development underscores Meta’s commitment to energy efficiency and sustainability.\u003c/p\u003e\n\n\n\n\u003cp\u003eLlama 3.3 leads in multilingual reasoning tasks with a 91.1% accuracy rate on MGSM, demonstrating its effectiveness in supporting languages such as German, French, Italian, Hindi, Portuguese, Spanish, and Thai, in addition to English.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-cost-effective-and-environmentally-conscious\"\u003eCost-effective and environmentally conscious\u003c/h2\u003e\n\n\n\n\u003cp\u003eLlama 3.3 is specifically optimized for cost-effective inference, with token generation costs as low as $0.01 per million tokens. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis makes the model highly competitive against industry counterparts like GPT-4 and Claude 3.5, with greater affordability for developers seeking to deploy sophisticated AI solutions.\u003c/p\u003e\n\n\n\n\u003cp\u003eMeta has also emphasized the environmental responsibility of this release. Despite its intensive training process, the company leveraged renewable energy to offset greenhouse gas emissions, resulting in net-zero emissions for the training phase. Location-based emissions totaled 11,390 tons of CO2-equivalent, but Meta’s renewable energy initiatives ensured sustainability.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-advanced-features-and-deployment-options\"\u003eAdvanced features and deployment options\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe model introduces several enhancements, including a longer context window of 128k tokens (comparable to GPT-4o, about 400 pages of book text), making it suitable for long-form content generation and other advanced use cases. \u003c/p\u003e\n\n\n\n\u003cp\u003eIts architecture incorporates Grouped Query Attention (GQA), improving scalability and performance during inference.\u003c/p\u003e\n\n\n\n\u003cp\u003eDesigned to align with user preferences for safety and helpfulness, Llama 3.3 uses reinforcement learning with human feedback (RLHF) and supervised fine-tuning (SFT). This alignment ensures robust refusals to inappropriate prompts and an assistant-like behavior optimized for real-world applications.\u003c/p\u003e\n\n\n\n\u003cp\u003eLlama 3.3 is already available for download through \u003ca href=\"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/#introduction\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMeta\u003c/a\u003e, \u003ca href=\"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/README.md#model-information\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e, \u003ca href=\"https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGitHub\u003c/a\u003e, and other platforms, with integration options for researchers and developers. Meta is also offering resources like Llama Guard 3 and Prompt Guard to help users deploy the model safely and responsibly.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-12-06T18:24:35Z",
  "modifiedTime": "2024-12-06T18:29:25Z"
}
