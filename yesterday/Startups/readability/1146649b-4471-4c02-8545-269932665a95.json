{
  "id": "1146649b-4471-4c02-8545-269932665a95",
  "title": "ARC-AGI without pretraining",
  "link": "https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html",
  "description": "Article URL: https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html Comments URL: https://news.ycombinator.com/item?id=43259182 Points: 181 # Comments: 37",
  "author": "georgehill",
  "published": "Tue, 04 Mar 2025 19:52:38 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "Isaac Liao, Albert Gu",
  "length": 62392,
  "excerpt": "By Isaac Liao and Albert Gu",
  "siteName": "iliao2345",
  "favicon": "",
  "text": "By Isaac Liao and Albert Gu In this blog post, we aim to answer a simple yet fundamental question: Can lossless information compression by itself produce intelligent behavior? The idea that efficient compression by itself lies at the heart of intelligence is not new (see, e.g., Hernández-Orallo \u0026 Minaya-Collado, 1998; Mahoney, 1999; Hutter, 2005; Legg \u0026 Hutter, 2007). Rather than revisiting those theoretical discussions, we make a practical demonstration instead. In this work, we give evidence that lossless compression during inference time is sufficient to produce intelligent behavior, by developing a method purely based on compression that performs well on the ARC-AGI challenge, a dataset of IQ-test-like puzzles about inferring a procedure/rule from limited demonstrations. Crucially, our solution, which we name CompressARC, obeys the following three restrictions: No pretraining; models are randomly initialized and trained during inference time. No dataset; one model trains on just the target ARC-AGI puzzle and outputs one answer. No search, in most senses of the word—just gradient descent. Despite these constraints, CompressARC achieves 34.75% on the training set and 20% on the evaluation set—processing each puzzle in roughly 20 minutes on an RTX 4070. To our knowledge, this is the first neural method for solving ARC-AGI where the training data is limited to just the target puzzle. CompressARC’s intelligence emerges not from pretraining, vast datasets, exhaustive search, or massive compute—but from compression. We challenge the conventional reliance on extensive pretraining and data, and propose a future where tailored compressive objectives and efficient inference-time computation work together to extract deep intelligence from minimal input. What is ARC-AGI? ARC-AGI, introduced in 2019, is an artificial intelligence benchmark designed to test a system’s ability to infer and generalize abstract rules from minimal examples. The dataset consists of IQ-test-like puzzles, where each puzzle provides several example images that demonstrate an underlying rule, along with a test image that requires completing or applying that rule. While some have suggested that solving ARC-AGI might signal the advent of artificial general intelligence (AGI), its true purpose is to spotlight the current challenges hindering progress toward AGI. Below are three of the 1000 puzzles: Hidden rule: Shift every object to the right by one pixel, except the bottom/right edges of the object. Hidden rule: Shrink the big object and set its color to the scattered dots’ color. Hidden rule: Extend the green line to meet the red line by turning when hitting a wall. For every puzzle, there is a hidden rule that maps each input grid to each output grid. You are given some number of examples of input-to-output mappings, and you get two attempts to guess the output grid for a given input grid, without being told the hidden rule. If either guess is correct, then you score 1 for that puzzle, else you score 0. You are allowed to change the size of the output grid and pick the color of every pixel. The puzzles are designed so that humans can reasonably find the answer, but machines should have more difficulty. The average human can solve 76.2% of the training set, and a human expert can solve 98.5%. The 400 training puzzles are easier than the rest, and are meant to help you learn the following patterns: Objectness: Objects persist and cannot appear or disappear without reason. Objects can interact or not depending on the circumstances. Goal-directedness: Objects can be animate or inanimate. Some objects are “agents” - they have intentions and they pursue goals. Numbers \u0026 counting: Objects can be counted or sorted by their shape, appearance, or movement using basic mathematics like addition, subtraction, and comparison. Basic geometry \u0026 topology: Objects can be shapes like rectangles, triangles, and circles which can be mirrored, rotated, translated, deformed, combined, repeated, etc. Differences in distances can be detected. The ARC Prize team has repeatedly launched competitions for solving ARC-AGI, with monetary rewards. The most recent competition involved potential prizes and awards of upwards of $1,000,000, with the main prize reserved for methods which could achieve 85% on a private test set of 100 puzzles, using 12 hours of compute in a constrained environment. Our Solution Method We propose that lossless information compression can serve as an effective framework for solving ARC-AGI puzzles. A more efficient (i.e., lower-bit) compression of a puzzle correlates with a more accurate solution. To solve ARC-AGI puzzles, we design a system that transforms an incomplete puzzle into a completed one—filling in the answers—by finding a compact representation that, when decompressed, reproduces the puzzle with any solution. The key challenge is to obtain this compact representation without needing the answers as inputs. CompressARC uses a neural network as the decoder. However, the encoding algorithm is not another network—instead, encoding is realized by the gradient descent algorithm that performs inference-time training on the decoder while maintaining correct decoded output. In other words, running the encoder means optimizing the decoder’s parameters and input distribution to achieve the most compressed puzzle representation. The resulting optimized parameters (e.g., weights and input distribution settings) themselves serve as the compressed bit representation that encodes the puzzle along with its answer. In standard machine learning lingo: (without compression terminology, and with some simplifications) We start at inference time, and we are given an ARC-AGI puzzle to solve. (e.g., puzzle in the diagram below.) We construct a neural network $f$ (see architecture) designed for the puzzle’s specifics (e.g., number of examples, observed colors). The network takes random normal input $z \\sim N(\\mu, \\Sigma)$, and per-pixel color logit predictions across all the grids, including an answer grid (3 input-output examples, for a total of 6 grids). Importantly, $f_\\theta$ is equivariant to common augmentations—such as reordering input-output pairs (including the answer’s pair), color permutations, and spatial rotations/reflections. We initialize the network weights $\\theta$ and set the parameters $\\mu$ and $\\Sigma$ for the $z$ distribution. We jointly optimize $\\theta$, $\\mu$, and $\\Sigma$ to minimize the sum of cross-entropies over the known grids (5 of them,) ignoring the answer grid. A KL divergence penalty keeps $N(\\mu, \\Sigma)$ close to $N(0,1)$, as in a VAE. Since the generated answer grid is stochastic due to the randomness in $z$, we save the answer grids throughout training and choose the most frequently occuring one as our final prediction. It isn’t obvious why such a method is performing compression. You’ll see later how we derived it from trying to compress ARC-AGI. First, let’s see it try to solve the puzzle above. Watching the Network Learn: Color the Boxes Human Solution: We first realize that the input is divided into boxes, and the boxes are still there in the output, but now they’re colored. We then try to figure out which colors go in which boxes. First, we notice that the corners are always black. Then, we notice that the middle is always magenta. And after that, we notice that the color of the side boxes depends on which direction they are in: red for up, blue for down, green for right, and yellow for left. At this point, we copy the input over to the answer grid, then we color the middle box magenta, and then color the rest of the boxes according to their direction. CompressARC Solution: 50 steps of learning: CompressARC's network outputs an answer grid (sample) with light blue rows/columns wherever the input has the same. It has noticed that all the other input-output pairs in the puzzle exhibit this correspondence. It doesn't know how the other output pixels are assigned colors; an exponential moving average of the network output (sample average) shows the network assigning mostly the same average color to non-light-blue pixels. 150 steps of learning: The network outputs a grid where nearby pixels have similar colors. It has likely noticed that this is common among all the outputs, and is guessing that it applies to the answer too. 200 steps of learning: The network output now shows larger blobs of colors that are cut off by the light blue borders. It has noticed the common usage of borders to demarcate blobs of colors in other outputs, and applies the same idea here. It has also noticed black corner blobs in other given outputs, which the network imitates. 350 steps of learning: The network output now shows the correct colors assigned to boxes of the correct direction from the center. It has realized that a single color-to-direction mapping is used to pick the blob colors in the other given outputs, so it imitates this mapping. It is still not the best at coloring within the lines, and it's also confused about the center blob, probably because the middle does not correspond to a direction. Nevertheless, the averate network output does show a tinge of the correct magenta color in the middle, meaning the network is catching on. 1500 steps of learning: The network is as refined as it will ever be. Sometimes it will still make a mistake in the sample it outputs, but this uncommon and filtered out. After training, we can deconstruct the learned z distribution to find that it codes for a color-direction correspondence table and row/column divider positions! How to Derive Our Solution Method Again, it isn’t obvious how we get from trying to perform compression to the method we ended up using. The derivation of our algorithm takes us on a detour through information theory, algorithmic information theory, and coding theory, with machine learning only making an appearance near the end. A Primer on Lossless Information Compression In information theory, lossless information compression is about trying to represent some information in as few bits as possible, while still being able to reconstruct that information from the bit representation. This type of problem is abstracted as follows: A source produces some symbol $x$ from some process that generates symbols from a probability distribution $p(x)$. A compressor/encoder $E$ must map the symbol $x$ to a string of bits $s$. A decompressor/decoder $D$ must exactly map $s$ back to the original symbol $x$. The goal is to use $p$ to construct functions $(E, D)$ which are bit-efficient, (ie. that minimize the expected length of $s$,) without getting any symbols wrong. In our case, the symbol $x$ is the ARC-AGI dataset (many puzzle + answer pairs), and we want to figure out what answers the best compression system might decompress the answers to be. Except, we don’t have the answers (only the puzzles) to give as input to $E$, and we don’t know $p$, since it’s hard to model the intelligent process of puzzle ideation in humans. One-Size-Fits-All Compression To build our compression scheme, you might think we need to know what $p$ is, but we argue that it doesn’t really matter since we can make a one-size-fits-all compressor. It all hinges on the following assumption: There exists some practically implementable, bit efficient compression system $(E, D)$ for ARC-AGI datasets $x$ sampled from $p$. If this were false, our whole idea of solving ARC-AGI with compression is doomed even if we knew $p$ anyways, so we might as well make this assumption. Our one-size-fits-all compressor $(E’, D’)$ is built without knowing $p$, and it is almost just as bit-efficient as the original $(E, D)$: $E’$ observes symbol $x$, picks a program $f$ and input $s$ to minimize $\\text{len}(f)+\\text{len}(s)$ under the constraint that running the program makes $f(s)=x$, and then sends the pair $(f, s)$. $D’$ is just a program executor that executes $f$ on $s$, correctly producing $x$. It is possible to prove with algorithmic information theory that $(E’, D’)$ achieves a bit efficiency at most $\\text{len}(f)$ bits worse than the bit efficiency of $(E, D)$, where $f$ is the code for implementing D. But since compression is practically implementable, the code for $D$ should be simple enough for a human engineer to write, so $\\text{len}(f)$ must be short, meaning our one-size-fits-all compressor will be close to the best possible bit efficiency. Ironically, the only problem with using this to solve ARC-AGI is that implementing $E’$ is not practical, since $E’$ needs to minimize the length of a program-input pair $(f, s)$ under partial fixed output constraint $f(s)_{answers}=x_{answers}$. Neural Networks to the Rescue To avoid searching through program space, we just pick a program $f$ for a small sacrifice in bit efficiency. We hope the diversity of program space can be delegated to diversity in input $s$ space instead. Specifically, we write a program $f$ that runs the forward pass of a neural network, where $s=(\\theta, z, \\epsilon)$ are the weights, inputs, and corrections to the outputs of the neural network. Then, we can use gradient descent to “search” over $s$. This restricted compression scheme uses Relative Entropy Coding (REC)1 to encode noisy weights $\\theta$ and neural network inputs $z$ into bits $s_\\theta$ and $s_z$, and arithmetic coding to encode output error corrections $\\epsilon$ into bits $s_\\epsilon$, to make a bit string $s$ consisting of three blocks $(s_\\theta, s_z, s_\\epsilon)$. The compression scheme runs as follows: The decoder runs $\\theta = \\text{REC-decode}(s_\\theta)$, $z = \\text{REC-decode}(s_z)$, $\\text{logits} = \\text{Neural-Net}(\\theta, z)$, and $x=\\text{Arithmetic-decode}(s_\\epsilon, \\text{logits})$. The encoder trains $\\theta$ and $z$ to minimize the total code length $\\mathbb{E}[\\text{len}(s)]$. $s_\\epsilon$ is fixed by arithmetic coding to guarantee correct decoding. To calculate the three components of the loss $\\mathbb{E}[\\text{len}(s)]$ in a differentiable way, we refer to the properties of REC and arithmetic coding: It turns out that the $\\epsilon$ code length $\\mathbb{E}[\\text{len}(s_\\epsilon)]$ is equal to the total crossentropy error on all the given grids in the puzzle. REC requires us to fix some reference distribution $q_\\theta$, and also add noise to $\\theta$, turning it into a distribution $p_\\theta$. Then, REC allows you to store noisy $\\theta$ using a code length of $\\mathbb{E}[\\text{len}(s_\\theta)] = KL(p_\\theta|| q_\\theta) = \\mathbb{E}_{\\theta \\sim p_\\theta} [\\log (p_\\theta(\\theta) / q_\\theta(\\theta))]$ bits. We will choose to fix $q_\\theta = N(0, I/2\\lambda)$ for large $\\lambda$, such that the loss component $\\mathbb{E}[\\text{len}(s_\\theta)] \\approx \\lambda | \\theta|^2 + \\text{const}$ is equivalent to regularizing the decoder. We must also do for $z$ what we do for $\\theta$, since it’s also represented using REC. We will choose to fix $q_z = N(0,I)$, so the code length of $z$ is $\\mathbb{E}[\\text{len}(s_z)] = KL(p_z|| q_z) = \\mathbb{E}_{z \\sim p_z} [\\log (p_z(z) / q_z(z))]$. We can compute gradients of these code lengths via the reparameterization trick. At this point, we observe that the total code length for $s$ that we described is actually the VAE loss with decoder regularization (= KL for $z$ + reconstruction error + regularization). Likewise, if we port the rest of what we described above (plus modifications regarding equivariances and inter-puzzle independence, and ignoring regularization) into typical machine learning lingo, we get the above description of CompressARC. Architecture We designed our own neural network architecture for decoding the latents $z$ into ARC-AGI puzzles. The most important feature of our architecture is it’s equivariances, which are symmetry rules dictating that whenever the input $z$ undergoes a transformation, the output ARC-AGI puzzle must also transform the same way. Some examples: reordering of input/output pairs shuffling colors flips, rotations, and reflections of grids There are too many equivariances for us to think about at once, so we decided to make a base architecture that’s fully symmetric, and break unwanted symmetries one by one by adding asymmetric layers to give it specific non-equivariant abilities. To illustrate what we mean, suppose that both $z$ and an ARC-AGI puzzle take the form of a tensor of shape $[n\\_examples, n\\_colors, height, width, 2 \\text{ for input/output}]$ (This is not actually the format of the data (see multitensors) but it gets the idea across best.) Then, our network starts out as equivariant to permutations of indices in the $example$, $color$, $height$, and $width$ dimensions. Some extra care must be taken with weight sharing, to force the network to also be equivariant to swapping the $width$ and $height$ dimensions. We may then add a layer involving a roll by one in the $width$ and $height$ dimensions, to let the network distinguish short range spatial interactions but not long-range ones. The actual data ($z$, hidden activations, and puzzles) passing through our layers comes in a format that we call a “multitensor”, which is just a bucket of tensors of various shapes. All the equivariances can be described in terms of how they change a multitensor. In order to understand any of the layers we list, you must first read the below section on multitensors. Multitensors Most common classes of machine learning architectures operate on a single type of tensor with constant rank. LLMs operate on rank 3 tensors of shape $[n\\_batch, n\\_tokens, n\\_channels]$, and CNNs operate on a rank 4 tensors of shape $[n\\_batch, n\\_channels, height, width]$. Our multitensors are a set of varying-rank tensors of unique type, whose dimensions are a subset of a rank 6 tensor of shape $[n\\_examples$, $n\\_colors$, $n\\_directions$, $height$, $width$, $n\\_channels]$. We always keep the $channel$ dimension, so there are at most 32 tensors in every multitensor. We also maintain several rules that determine whether a tensor shape is “legal” or not, which reduces the number of tensors in a multitensor to 18. Dimension Size Example Number of examples in the ARC-AGI puzzle, including the one with held-out answer Color Number of unique colors in the ARC-AGI puzzle, not including black Direction 8 Height Determined when preprocessing the puzzle Width Determined when preprocessing the puzzle Channel In the residual connections, the size is 8 if the $direction$ dimension is included, else 16. Within layers it is layer-dependent. To give an idea of how a multitensor stores data, an ARC-AGI puzzle can be represented by using the $[examples, colors, height, width, channel]$ tensor, by using the $channel$ dimension to select either the input or output grid, and the $width$/$height$ dimensions for pixel location, a one hot vector in the $color$ dimension, specifying what color that pixel is. The $[examples, width, channel]$ and $[examples, height, channel]$ tensors can similarly be used to store masks representing grid shapes for every example for every input/output grid. All those tensors are included in a single multitensor that is computed by the network just before the final linear heads layer. When we apply an operation on a multitensor, we by default assume that all non-$channel$ dimensions are treated identically as batch dimensions by default. The operation is copied across the indices of dimensions unless specified. This ensures that we keep all our symmetries intact until we use a specific layer meant to break a specific symmetry. A final note on the $channel$ dimension: usually when talking about a tensor’s shape, we will not even mention the $channel$ dimension as it is included by default. The full architecture consists of the following layers, which are each described in the Appendix: Begin with parameters of the $z$ distribution, Decoding Layer 4x Multitensor Communication Layer, Upwards Softmax Layer Directional Cummax Layer Directional Shift Layer Directional Communication Layer Nonlinear Layer Multitensor Communication Layer, Downwards Normalization Layer Linear Heads Results Training set: 34.75% Training Iteration Time Pass@1 Pass@2 Pass@5 Pass@10 Pass@100 Pass@1000 100 6 h 1% 2.25% 3.5% 4.75% 6.75% 6.75% 200 13 h 11.5% 14.25% 16.5% 18.25% 23.25% 23.5% 300 19 h 18.5% 21.25% 23.5% 26.75% 31.5% 32.5% 400 26 h 21% 25% 28.75% 31% 36% 37.5% 500 32 h 23% 27.5% 31.5% 33.5% 39.25% 40.75% 750 49 h 28% 30.5% 34% 36.25% 42.75% 44.5% 1000 65 h 28% 31.75% 35.5% 37.75% 43.75% 46.5% 1250 81 h 29% 32.25% 37% 39.25% 45.5% 49.25% 1500 97 h 29.5% 33% 38.25% 40.75% 46.75% 51.75% 2000 130 h 30.25% 34.75% 38.25% 41.5% 48.5% 52.75% Evaluation set: 20% Training Iteration Time Pass@1 Pass@2 Pass@5 Pass@10 Pass@100 Pass@1000 100 7 h 0.75% 1.25% 2.25% 2.5% 3% 3% 200 14 h 5% 6% 7% 7.75% 12% 12.25% 300 21 h 10% 10.75% 12.25% 13.25% 15.5% 16.25% 400 28 h 11.75% 13.75% 16% 17% 19.75% 20% 500 34 h 13.5% 15% 17.75% 19.25% 20.5% 21.5% 750 52 h 15.5% 17.75% 19.75% 21.5% 22.75% 25.5% 1000 69 h 16.75% 19.25% 21.75% 23% 26% 28.75% 1250 86 h 17% 20.75% 23% 24.5% 28.25% 30.75% 1500 103 h 18.25% 21.5% 24.25% 25.5% 29.5% 31.75% 2000 138 h 18.5% 20% 24.25% 26% 31.25% 33.75% What Puzzles Can and Can’t We Solve? CompressARC tries to use its abilities to figure out as much as it can, until it gets bottlenecked by one of it’s inabilities. For example, puzzle 28e73c20 in the training set requires extension of a pattern from the edge towards the middle: Given the layers in it’s network, CompressARC is generally able to extend patterns for short ranges but not long ranges. So, it does the best that it can, and correctly extends the pattern a short distance before guessing at what happens near the center: A short list of abilities that can be performed by CompressARC includes: Assigning individual colors to individual procedures (see puzzle 0ca9ddb6) Infilling (see puzzle 0dfd9992) Cropping (see puzzle 1c786137) Connecting dots with lines, including 45 degree diagonal lines (see puzzle 1f876c06) Same color detection (see puzzle 1f876c06) Identifying pixel adjacencies (see puzzle 42a50994) Assigning individual colors to individual examples (see puzzle 3bd67248) Identifying parts of a shape (see puzzle 025d127b) Translation by short distances (see puzzle 025d127b) A short list of abilities that cannot be performed by CompressARC includes: Assigning two colors to each other (see puzzle 0d3d703e) Repeating an operation in series many times (see puzzle 0a938d79) Counting/numbers (see puzzle ce9e57f2) Translation, rotation, reflections, rescaling, image duplication (see puzzles 0e206a2e, 5ad4f10b, and 2bcee788) Detecting topological properties such as connectivity (see puzzle 7b6016b9) Planning, simulating the behavior of an agent (see puzzle 2dd70a9a) Long range extensions of patterns (see puzzle 28e73c20 above) Case Study: Color the Boxes (Additional case studies can be found in the Appendix.) We show the puzzle again for convenience. During training, the reconstruction error fell extremely quickly. It remained low on average, but would spike up every once in a while, causing the KL from $z$ to bump upwards at these moments. Solution Analysis: Color the Boxes So how does CompressARC learn to solve the puzzle? Let’s look at the representations stored in $z$ to find out. Since $z$ is a multitensor, each of the tensors it contains produces an additive contribution to the total KL for $z$. By looking at the per-tensor contributions, we can determine which tensors in $z$ code for information that is used to represent the puzzle. Below is a plot showing the quantity of information stored in each tensor of $z$, ie. the KL contribution used by the decoding layer. All the tensors fall to zero information content during training, except for four tensors. In some replications of this experiment, we saw one of these four necessary tensors fall to zero information content, and CompressARC typically does not recover the correct answer after that. Here we are showing a lucky run where the $(color, direction, channel)$ tensor almost falls but gets picked up 200 steps in, which is right around when the samples from the model begin to show the correct colors in the correct boxes. We can look at the average output of the decoding layer corresponding to individual tensors of $z$, to see what information is stored there. Each tensor contains a vector of dimension $n\\_channels$ for various indices of the tensor. Taking the PCA of these vectors reveals some number of activated components, telling us how many pieces of information are coded by the tensor. (Examples, height, channel) tensor: For every example and row, there is a vector of dimension $n\\_channels$. This forms a dataset of vectors. Taking the PCA of these vectors, the top principal component vector reformatted back into an $(examples, height)$ matrix (shown on right) can tell us which examples/row combinations are uniquely identified by the stored information. The top principal component (shown on right) is 1485 times stronger than the second principal component, which indicates to us that basically all of the information is in the above tensor. For every example, the two brightest pixels give the rows where the light blue rows in the grids are. (Examples, width, channel) tensor: A very similar story here: in the top principal component of this tensor, the two darkest pixels for every example give the columns where the light blue columns in the grids are. The top principal component is 1253 times stronger than the next principal component. (Direction, color, channel) tensor: In this tensor, we see that the four brightest pixels identify blue with up, green with left, red with down, and yellow with right. This tensor seems to tell each direction which color to use for the opposite direction's corresponding box. The top principal component is 829 times stronger than the next principal component. (Color, channel) tensor: Here, we look at the top three principal components, since the first and second principal components are 134 and 87 times stronger than the third component, indicating that they play a role while the third component does not. The magenta and light blue colors are uniquely identified, indicating their special usage amongst the rest of the colors as the center color and the color of the row/column divisions, respectively. How to Improve Our Work At the time of release of CompressARC, there were several ideas which we thought of trying or attempted at some point, but didn’t manage to get working for one reason or another. Some ideas we still believe in, but didn’t use, are listed below. Joint Compression via Weight Sharing Between Puzzles CompressARC tries to solve each puzzle serially by compressing each puzzle on its own. We believe that joint compression of all the entire ARC-AGI dataset at once should yield better learned inductive biases per-puzzle, since computations learned for one puzzle can be transferred to other puzzles. We do not account for the complexity of $f$ in our method, allowing for $f$ to be used for memorization/overfitting. By jointly compressing the whole dataset, we only need to have one $f$, whereas when compressing each puzzle individually, we need to have an $f$ for every puzzle, allowing for more memorization/overfitting. To implement this, we would most likely explore strategies like: Using the same network weights for all puzzles, and training for puzzles in parallel. Each puzzle gets assigned some perturbation to the weights, that is constrained in some way, e.g., LORA. Learning a “puzzle embedding” for every puzzle that is a high dimensional vector (more than 16 dim, less than 256 dim), and learning a linear mapping from puzzle embeddings to weights for our network. This mapping serves as a basic hypernetwork, ie. a neural network that outputs weights for another neural network. In a successful case, we might want to also try adding in some form of positional encodings, with the hope that $f$ is now small/simple enough to be incapable of memorization/overfitting using positional encodings. The reason we didn’t try this is because it would slow down the research iteration process. Convolution-like Layers for Shape Copying Tasks This improvement is more ARC-AGI-specific and may have less to do with AGI in our view. Many ARC-AGI puzzles can be seen to involve copying shapes from one place to another, and our network has no inductive biases for such an operation. An operation which is capable of copying shapes onto multiple locations is the convolution. With one grid storing the shape and another with pixels activated at locations to copy to, convolving the two grids will produce another grid with the shape copied to the designated locations. There are several issues with introducing a convolutional operation for the network to use. Ideally, we would read two grids via projection from the residual stream, convolve them, and write it back in via another projection, with norms in the right places and such. Ignoring the fact that the grid size changes during convolution (can be solved with two parallel networks using different grid sizes), the bigger problem is that convolutions tend to amplify noise in the grids much more than the sparse signals, so their inductive bias is not good for shape copying. We can try to apply a softmax to one or both of the grids to reduce the noise (and to draw an interesting connection to attention), but we didn’t find any success. The last idea that we were tried before discarding the idea was to modify the functional form of the convolution: \\[(f * g)(x) = \\sum_y f(x-y)g(y)\\] to a tropical convolution, which we found to work well on toy puzzles, but not well enough for ARC-AGI training puzzles (which is why we discarded this idea): \\[(f*g)(x) = \\max_y f(x-y) + g(y)\\] Convolutions, when repeated with some grids flipped by 180 degrees, tend to create high activations at the center pixel, so sometimes it is important to zero out the center pixel to preserve the signal. KL Floor for Posterior Collapse We noticed during testing that crucial posterior tensors whose KL fell to zero during learning would never make a recovery and play their role in the encoding. We believe that the KL divergence may upper bound the information content of the gradient training signal for parts of the network that process the encoded information. Thus, when a tensor falls to zero KL, the network stops learning to use its information, so the KL is no longer given encouragement to recover. If we can hold the KL above zero for a while, the network may then learn to use the information, giving the KL a reason to stay above zero when released again. We implemented a mechanism to keep the KL above a minimum threshold so that the network always learns to use that information, but we do not believe it learns fast enough for this to be useful, as we have never seen a tensor recover before. Therefore, it might be useful to explore different ways to schedule this KL floor to start high and decay to zero, to allow learning when the KL is forced to be high, and to leave the KL unaffected later on in learning. This might cause training results to be more consistent across runs. Regularization We don’t use it. Maybe it matters, but we don’t know. Regularization measures the complexity of $f$ in our problem formulation, and is native to our derivation of CompressARC. It is somewhat reckless for us to exclude it in our implementation. Equivalence of Compression and Intelligence The original inspiration of this work came from the Hutter Prize, which awards a prize for those who can compress a file of Wikipedia text the most, as a motivation for researchers to build intelligent systems. It is premised upon the idea that the ability to compress information is equivalent to intelligence. This equivalence between inteeligence and compression has a long history. For example, when talking about intelligent solutions to prediction problems, the ideal predictor implements Solomonoff Induction, a theoretically best possible but uncomputable prediction algorithm that works universally for all prediction tasks. This prediction algorithm is then equivalent to a best possible compression algorithm whose compressed code length is the Kolmogorov Complexity of the data. In our work, we try to approximate this best possible compression algorithm with a neural network. A related measure of complexity is known as the Minimum Description Length. Information Theory and Coding Theory Since we build an information compression system, we make use of many results in information theory and coding theory. The main result required to motivate our model architecture is the existence of Relative Entropy Coding (REC). The fact that REC exists means that as long as a KL divergence can be bounded, the construction of a compression algorithm is always possible and the issue of realizing the algorithm can be abstracted away. Thus, problems about coding theory and translating information from Gaussians into binary and back can be ignored, since we can figure out the binary code length directly from the Gaussians instead. In other words, we only need to do enough information theory using the Gaussians to get the job done, with no coding theory at all. While the existence of arithmetic coding would suffice to abstract the problem away when distributions are discrete, neural networks operate in a continuous space so we need REC instead. Our architecture sends $z$ information through an additive white Gaussian noise (AWGN) channel, so the AWGN channel capacity formula (Gaussian input Gaussian noise) plays a heavy role in the design of our decoding layer. Variational Autoencoders The decoder side of the variational autoencoder serves as our decompression algorithm. While we would use something that has more general capabilities like a neural Turing machine instead, neural Turing machines are not very amenable to gradient descent-based optimization so we stuck with the VAE. VAEs have a long history of developments that are relevant to our work. At one point, we tried using multiple decoding layers to make a hierarchical VAE decoder instead. This does not affect Relative Entropy Coding with the AWGN channel because channel capacity with feedback is equal to channel capacity without feedback. But, we found empirically that the first decoding layer would absorb all of the KL contribution, making the later decoding layers useless. Thus, we only used one decoding layer at the beginning. The beta-VAE introduces a reweighting of the reconstruction loss to be stronger than the KL loss, and we found that to work well in our case. The NVAE applies a non-constant weighting to loss components. A rudimentary form of scheduled loss recombination is used in CompressARC. ARC-AGI Methods Current methods for solving ARC-AGI focus primarily on using large language models (LLMs). ARC-AGI puzzles are converted into textual representations which are fed into LLMs as input. The LLM may directly output a textual representation of an answer, or some code which tries to convert input grids into output grids. Top methods rely heavily on data augmentation and larger alternative datasets, and sometimes perform autoregressive training on the target puzzle during inference time. Top solutions (example) in the 2024 Kaggle prize competition frequently used test-time training. Reasoning models have managed to get up to 87.5% on the semi-private evaluation set, albeit with astronomical amounts of compute. An older class of methods consists of hard-coded searches through program spaces in hand-written domain-specific languages designed specifically for ARC. Another example here. Bonnet and Macfarlane introduced a VAE-based method for searching through a latent space of programs. We believe CompressARC is the only method so far that uses deep learning without external pretraining nor any large-scale search. Deep Learning Architectures We designed our own neural network architecture from scratch, but not without borrowing crucial design principles from many others. Our architecture is fundamentally structured like a transformer, consisting of a residual stream where representations are stored and operated upon, followed by a linear head. Pre-and post-norms with linear up- and down-projections allow layers to read and write to the residual stream. The SiLU-based nonlinear layer is especially similar to a transformer’s. Our equivariance structures are inspired by permutation-invariant neural networks, which are a type of equivariant neural network. Equivariance transformations are taken from common augmentations to ARC-AGI puzzles. Appendix Layers in the Architecture Decoding Layer This layer’s job is to sample a multitensor $z$ and bound its information content, before it is passed to the next layer. This layer and outputs the KL divergence between the learned $z$ distribution and $N(0,I)$. Penalizing the KL prevents CompressARC from learning a distribution for $z$ that memorizes the ARC-AGI puzzle in an uncompressed fashion, and forces it to represent the puzzle more succinctly. Specifically, it forces CompressARC to spend more bits on the KL whenever it uses $z$ to break a symmetry, and the larger the symmetry group broken, the more bits it spends. This layer takes as input: A learned target multiscalar, called the “target capacity”.2 The decoding layer will output $z$ whose information content per tensor is close to the target capacity,3 learned per-element means for $z$,4 learned per-element capacity adjustments for $z$. We begin by normalizing the learned per-element means for $z$.5 Then, we figure out how much Gaussian noise we must add into every tensor to make the AWGN channel capacity equal to the target capacity for every tensor (including per-element capacity adjustments). We apply the noise to sample $z$, keeping unit variance of $z$ by rescaling.6 We compute the information content of $z$ as the KL divergence between the distribution of this sample and $N(0,1)$. Finally, we postprocess the noisy $z$ by scaling it by the sigmoid of the signal-to-noise ratio.7 This ensures that $z$ is kept as-is when its variance consists mostly of useful information and it is nearly zero when its variance consists mostly of noise. All this is done 4 times to make a $channel$ dimension of 4. Then we apply a projection (with different weights per tensor in the multitensor, ie. per-tensor projections) mapping the $channel$ dimension up to the dimension of the residual stream. Multitensor Communication Layer This layer allows different tensors in a multitensor to interact with each other. First, the input from the residual stream passes through per-tensor projections to a fixed size (8 for downwards communication and 16 for upwards communication). Then a message is sent to every other tensor that has at least the same dimensions for upwards communication, or at most the same dimensions for downwards communication. This message is created by either taking means along dimensions to remove them, or unsqueezing+broadcasting dimensions to add them. All the messages received by every tensor are summed together and normalization is applied. This result gets up-projected back and then added to the residual stream. Softmax Layer This layer allows the network to work with internal one-hot representations, by giving it the tools to denoise and sharpen noisy one-hot vectors. For every tensor in the input multitensor, this layer lists out all the possible subsets of dimensions of the tensor to take a softmax over8, takes the softmax over these subsets of dimensions, and concatenates all the softmaxxed results together in the $channel$ dimension. The output dimension varies across different tensors in the multitensor, depending on their tensor rank. A pre-norm is applied, and per-tensor projections map to and from the residual stream. The layer has input $channel$ dimension of 2. Directional Cummax/Shift Layer The directional cummax and shift layers allow the network to perform the non-equivariant cummax and shift operations in an equivariant way, namely by applying the operations once per direction, and only letting the output be influenced by the results once the directions are aggregated back together (by the multitensor communication layer). These layers are the sole reason we included the $direction$ dimension when defining a multitensor: to store the results of directional layers and operate on each individually. Of course, this means when we apply a spatial equivariance transformation, we must also permute the indices of the $direction$ dimension accordingly, which can get complicated sometimes. The directional cummax layer takes the eight indices of the $direction$ dimension, treats each slice as corresponding to one direction (4 cardinal, 4 diagonal), performs a cumulative max in the respective direction for each slice, does it in the opposite direction for half the channels, and stacks the slices back together in the $direction$ dimension. The slices are rescaled to have min $-1$ and max $1$ before applying the cumulative max. The directional shift layer does the same thing, but for shifting the grid by one pixel instead of applying the cumulative max, and without the rescaling. Some details: Per-tensor projections map to and from the residual stream, with pre-norm. Input $channel$ dimension is 4 These layers are only applied to the $[example, color, direction, height, width, channel]$ and $[example, direction, height, width, channel]$ tensors in the input multitensor. Directional Communication Layer By default, the network is equivariant to permutations of the eight directions, but we only want symmetry up to rotations and flips. So, this layer provides a way to send information between two slices in the $direction$ dimension, depending on the angular difference in the two directions. This layer defines a separate linear map to be used for each of the 64 possible combinations of angles, but the weights of the linear maps are minimally tied such that the directional communication layer is equivariant to reflections and rotations. This gets complicated really fast, since the $direction$ dimension’s indices also permute when equivariance transformations are applied. Every direction slice in a tensor accumulates it’s 8 messages, and adds the results together.9 For this layer, there are per-tensor projections to and from the residual stream with pre-norm. The input $channel$ dimension is 2. Nonlinear Layer We use a SiLU nonlinearity with $channel$ dimension 16, surrounded by per-tensor projections with pre-norm. Normalization Layer We normalize all the tensors in the multitensor, using means and variances computed across all dimensions except the $channel$ dimension. Normalization as used within other layers also generally operates this way. Linear Heads We must take the final multitensor, and convert it to the format of an ARC-AGI puzzle. More specifically, we must convert the multitensor into a distribution over ARC-AGI puzzles, so that we can compute the log-likelihood of the observed grids in the puzzle. The colors of every pixel for every example for both input and output, have logits defined by the $[examples, colors, height, width, channel]$ tensor, with the $channel$ dimension linearly mapped down to a size of 2, representing the input and output grids.10 The log-likelihood is given by the crossentropy, with sum reduction across all the grids. For grids of non-constant shape, the $[examples, width, channel]$ and $[examples, height, channel]$ tensors are used to create distributions over possible contiguous rectangular slices of each grid of colors. Again, the $channel$ dimension is mapped down to a size of 2 for input and output grids. For every grid, we have a vector of size $[width]$ and a vector of size $[height]$. The log likelihood of every slice of the vector is taken to be the sum of the values within the slice, minus the values outside the slice. The log likelihoods for all the possible slices are then normalized to have total probability one, and the colors for every slice are given by the color logits defined in the previous paragraph. With the puzzle distribution now defined, we can now evaluate the log-likelihood of the observed target puzzle, to use as the reconstruction error.11 Other Architectural Details Rules for legal multitensors At least one non-$example$ dimension must be included. Examples are not special for any reason not having to do with colors, directions, rows, and columns. If the $width$ or $height$ dimension is included, the $example$ dimension should also be included. Positions are intrinsic to grids, which are indexed by the $example$ dimension. Without a grid it doesn’t make as much sense to talk about positions. Weight Tying for Reflection/Rotation Symmetry When applying a different linear layer to every tensor in a multitensor, we have a linear layer for tensors having a $width$ but not $height$ dimension, and another linear layer for tensors having a $height$ but not $width$ dimension. Whenever this is the case, we tie the weights together in order to preserve the whole network’s equivariance to diagonal reflections and 90 degree rotations, which swap the $width$ and $height$ dimensions. The softmax layer is not completely symmetrized because different indices of the output correspond to different combinations of dimension to softmax over. Tying the weights properly would be a bit complicated and time consuming for the performance improvement we expect, so we did not do this. Training We train for 2000 iterations using Adam, with learning rate 0.01, $\\beta_1$ of 0.5, and $\\beta_2$ of 0.9. Preprocessing Output Shape Determination The raw data consists of grids of various shapes, while the neural network operates on grids of constant shape. Most of the preprocessing that we do is aimed towards this shape inconsistency problem. Before doing any training, we determine whether the given ARC-AGI puzzle follows three possible shape consistency rules: The outputs in a given ARC-AGI puzzle are always the same shape as corresponding inputs. All the inputs in the given ARC-AGI puzzle are the same shape. All the outputs in the given ARC-AGI puzzle are the same shape. Based on rules 1 and 3, we try to predict the shape of held-out outputs, prioritizing rule 1 over rule 3. If either rule holds, we force the postprocessing step to only consider the predicted shape by overwriting the masks produced by the linear heads. If neither rule holds, we make a temporary prediction of the largest width and height out of the grids in the given ARC-AGI puzzle, and we allow the masks to predict shapes that are smaller than that. The largest width and height that is given or predicted, are used as the size of the multitensor’s $width$ and $height$ dimensions. The predicted shapes are also used as masks when performing the multitensor communication, directional communication and directional cummax/shift layers12. We did not apply masks for the other layers because of time constraints and because we do not believe it will provide for much of a performance improvement. Number of Colors We notice that in almost all ARC-AGI puzzles, colors that are not present in the puzzle are not present in the true answers. Hence, any colors that do not appear in the puzzle are not given an index in the $color$ dimension of the multitensor. In addition, black is treated as a special color that is never included in the multitensor, since it normally represents the background in many puzzles. When performing color classification, a tensor of zeros is appended to the $color$ dimension after applying the linear head, to represent logits for the black color. Postprocessing Postprocessing primarily deals with denoising the answers sampled from the network. There are also some operations performed to convert the constant-shape grids outputted by the network to the variable shape grids present in some puzzles. Generally, when we sample answers from the network by taking the logits of the $[examples, colors, height, width, channels]$ tensor and argmaxxing over the $color$ dimension, we find that the grids are noisy and will often have the wrong colors for several random pixels. We developed several methods for removing this noise: Find the most commonly sampled answer. Construct an exponential moving average of the output color logits before taking the softmax to produce probabilities. Also construct an exponential moving average of the masks. Construct an exponential moving average of the output color probabilities after taking the softmax. Also construct an exponential moving average of the masks. When applying these techniques, we always take the slice of highest probability given the mask, and then we take the colors of highest probability afterwards. We explored several different rules for when to select which method, and arrived at a combination of 1 and 2 with a few modifications: At every iteration, count up the sampled answer, as well as the exponential moving average answer (decay $=0.97$). If before 150 iterations of training, then downweight the answer by a factor of $e^{-10}$. (Effectively, don’t count the answer.) If the answer is from the exponential moving average as opposed to the sample, then downweight the answer by a factor of $e^{-4}$. Downweight the answer by a factor of $e^{-10*uncertainty}$, where $uncertainty$ is the average (across pixels) negative log probability assigned to the top color of every pixel. What Happens to the Representations during Learning During training, the gradient descent tries to find representations of the puzzle that require less and less information to encode. This information is measured by the KL term for $z$, plus the a heavily penalized reconstruction error. Due to the 10x penalization on reconstruction error, and the initial high capacity for $z$, the $z$ distribution (which we call the “posterior”) quickly learns the information that is required to perfectly reconstruct the given input/output pairs in the puzzle, within the first 20 or so steps. The remainder of the training steps are about compressing $z$ information under the constraint of perfect reconstruction, by tuning the representations to be more concise. Our mental model of how gradient descent compresses the $z$ information consists of several steps which we list below: Suppose the posterior $p$ originally codes for some number $n$ pieces of information $z_1, \\dots, z_n$ using thin Gaussians. The posterior widens and becomes more noisy to try to get closer to the wide Gaussian “prior” $q=N(0,1)$, but since all $n$ pieces of information are needed to ensure good reconstruction, the noise is limited by the reconstruction loss incurred. The ever-widening posteriors push the neurons to become more and more resilient to noise, until some limit is reached. Learning remains stagnant for a while, as a stalemate between compression and reconstruction. If it turns out that $z_1$ is not reconstructible using $z_2, \\dots, z_n$, then stop. Else, proceed to step 6. The neurons, pushed by the widening posterior of $z_1$, figure out a procedure to denoise $z_1$ using information from $z_2, \\dots, z_n$, in the event that the noise sample for $z_1$ is too extreme. The posterior for the last piece keeps pushing wider, producing more extreme values for $z_1$, and the denoising procedure is improved, until the $z_1$ representation consists completely of noise, and its usage in the network is replaced by the output of the denoising procedure. The posterior for $z_1$ is now identical to the prior, so nothing is coded in $z_1$ and it no longer contributes to the KL loss. The posterior now codes for $n-1$ pieces of information $z_2, \\dots, z_n$, and compression has occurred. These steps happen repeatedly for different unnecessarily coded pieces of information, until there are no more. More than one piece of information can be compressed away at once, and there is no need for the steps to proceed serially. The process stops when all information coded by the posterior is unique, and no piece is reconstructable using the others. Additional Case Studies Below, we show two additional puzzles and a dissection of CompressARC’s solution to them. Case Study: Bounding Box Puzzle 6d75e8bb is part of the training split. Watching the Network Learn: Bounding Box Human Solution: We first realize that the input is red and black, and the output is also red and black, but some of the black pixels are replaced by light blue pixels. We see that the red shape remains unaffected. We notice that the light blue box surrounds the red shape, and finally that it is the smallest possible surrounding box that contains the red shape. At this point, we copy the input over to the answer grid, then we figure out the horizontal and vertical extent of the red shape, and color all of the non-red pixels within that extent as light blue. CompressARC Solution: 50 steps of learning: The average of sampled outputs shows that light blue pixels in the input are generally preserved in the output. However, black pixels in the input are haphazardly and randomly colored light blue and red. CompressARC does not seem to know that the colored input/output pixels lie within some kind of bounding box, or that the bounding box is the same for the input and output grids. 100 steps of learning: The average of sampled outputs shows red pixels confined to an imaginary rectangle surrounding the light blue pixels. CompressARC seems to have perceived that other examples use a common bounding box for the input and output pixels, but is not completely sure about where the boundary lies and what colors go inside the box in the output. Nevertheless, guess 2 (the second most frequently sampled output) shows that the correct answer is already being sampled quite often now. 150 steps of learning: The average of sampled outputs shows almost all of the pixels in the imaginary bounding box to be colored red. CompressARC has figured out the answer, and further training only refines the answer. Solution Analysis: Bounding Box Below is a plot of the amount of contained information in every tensor composing the latent $z$: All the tensors in $z$ fall to zero information content during training, except for three tensors. From 600-1000 steps, we see the $(example, height, width, channel)$ tensor suffer a massive drop in information content, with no change in the outputted answer. We believe it was being used to identify the light blue pixels in the input, but this information then got memorized by the nonlinear portions of the network, using the $(example, height, channel)$ and $(example, width, channel)$ as positional encodings. We can look at the average output of the decoding layer for these tensors to see what information is stored there. (Examples, height, channel) tensor: The first principal component is 771 times stronger than the second principal component. A brighter pixel indicates a row with more light blue pixels. It is unclear how CompressARC knows where the borders of the bounding box are. (Examples, width, channel) tensor: The first principal component is 550 times stronger than the second principal component. A darker pixel indicates a column with more light blue pixels. It is unclear how CompressARC knows where the borders of the bounding box are. (Color, channel) tensor: This tensor serves to distinguish the roles of the two colors apart. Case Study: Center Cross Puzzle 41e4d17e from training split Our Network’s Answer Human Solution: We first notice that the input consists of blue “bubble” shapes (really they are just squares, but the fact that they’re blue reminds us of bubbles) on a light blue background and the output has the same. But in the output, there are now magenta rays emanating from the center of each bubble. We copy the input over to the answer grid, and then draw magenta rays starting from the center of each bubble out to the edge in every cardinal direction. At this point, we submit our answer and find that it is wrong, and we notice that in the given demonstrations, the blue bubble color is drawn on top of the magenta rays, and we have drawn the rays on top of the bubbles instead. So, we pick up the blue color and correct each point where a ray pierces a bubble, back to blue. CompressARC Solution: We don’t show CompressARC’s solution evolving over time because we think it is uninteresting; instead will describe. We don’t see much change in CompressARC’s answer over time during learning. It starts by copying over the input grid, and at some point, magenta rows and columns start to appear, and they slowly settle on the correct positions. At no point does CompressARC mistakenly draw the rays on top of the bubbles; it has always had the order correct. Solution Analysis: Center Cross Another plot of the amount of information in every tensor in $z$: The only surviving tensors are the $(color, channel)$ and $(example, height, width, channel)$ tensors. (Examples, height, width, channel) tensor: The top principal component is 2496 times stronger than the second principal component. The (examples, height, width, channel) tensor codes for the centers of the bubbles. In the KL contribution plot, we can see that the information content of this tensor is decreasing over time. Likely, CompressARC is in the process of eliminating the plus shaped representation, and replacing it with a pixel instead, which takes fewer bits. (Color, channel) tensor: The $(color, channel)$ tensor just serves to distinguish the individual roles of the colors in the puzzle. List of Mentioned ARC-AGI Puzzles All the puzzles we mentioned are part of the training split. Name Puzzle Name Puzzle 025d127b 0a938d79 0ca9ddb6 0d3d703e 0dfd9992 0e206a2e 1c786137 1f876c06 28e73c20 272f95fa 2bcee788 2dd70a9a 3bd67248 41e4d17e 42a50994 5ad4f10b 6d75e8bb 7b6016b9 ce9e57f2     Code Code for this project is available here. If you’d like to cite this blog post, use the following entry: @online{liao2025arcagiwithoutpretraining, author = {Isaac Liao and Albert Gu}, title = {ARC-AGI Without Pretraining}, year = {2025}, url = {https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html}, }",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"content\"\u003e\n  \u003cp\u003e\u003ca name=\"topofpage\"\u003e\u003c/a\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/teaser_figure_w_title.png\" alt=\"image\"/\u003e\nBy \u003ca href=\"https://iliao2345.github.io/\"\u003eIsaac Liao\u003c/a\u003e and \u003ca href=\"https://goombalab.github.io/\"\u003eAlbert Gu\u003c/a\u003e\u003c/p\u003e\n\n  \n  \u003cp\u003eIn this blog post, we aim to answer a simple yet fundamental question:\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eCan lossless information compression by itself produce intelligent behavior?\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eThe idea that efficient compression by itself lies at the heart of intelligence is not new (see, e.g., \u003ca href=\"https://www.researchgate.net/publication/2472570_A_Formal_Definition_of_Intelligence_Based_on_an_Intensional_Variant_of_Algorithmic_Complexity\"\u003eHernández-Orallo \u0026amp; Minaya-Collado, 1998\u003c/a\u003e; \u003ca href=\"https://gwern.net/doc/cs/algorithm/information/compression/1999-mahoney.pdf\"\u003eMahoney, 1999\u003c/a\u003e; \u003ca href=\"https://link.springer.com/book/10.1007/b138233\"\u003eHutter, 2005\u003c/a\u003e; \u003ca href=\"https://arxiv.org/abs/0712.3329\"\u003eLegg \u0026amp; Hutter, 2007\u003c/a\u003e). Rather than revisiting those theoretical discussions, we make a practical demonstration instead.\u003c/p\u003e\n\n  \u003cp\u003eIn this work, we give evidence that lossless compression during inference time is sufficient to produce intelligent behavior, by developing a method \u003cstrong\u003epurely based on compression\u003c/strong\u003e that performs well on the \u003ca href=\"https://arcprize.org/\"\u003eARC-AGI challenge\u003c/a\u003e, a dataset of IQ-test-like puzzles about inferring a procedure/rule from limited demonstrations. Crucially, our solution, which we name \u003cem\u003eCompressARC\u003c/em\u003e, obeys the following three restrictions:\u003c/p\u003e\n\n  \u003cul\u003e\n    \u003cli\u003e\u003cstrong\u003eNo pretraining\u003c/strong\u003e; models are randomly initialized and trained during inference time.\u003c/li\u003e\n    \u003cli\u003e\u003cstrong\u003eNo dataset\u003c/strong\u003e; one model trains on just the target ARC-AGI puzzle and outputs one answer.\u003c/li\u003e\n    \u003cli\u003e\u003cstrong\u003eNo search\u003c/strong\u003e, in most senses of the word—just gradient descent.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eDespite these constraints, CompressARC achieves 34.75% on the training set and 20% on the evaluation set—processing each puzzle in roughly 20 minutes on an RTX 4070. To our knowledge, this is the first neural method for solving ARC-AGI where the training data is limited to just the target puzzle. CompressARC’s intelligence emerges not from pretraining, vast datasets, exhaustive search, or massive compute—but from compression. We challenge the conventional reliance on extensive pretraining and data, and propose a future where tailored compressive objectives and efficient inference-time computation work together to extract deep intelligence from minimal input.\u003c/p\u003e\n\n  \n\n  \u003ch2 id=\"what-is-arc-agi\"\u003eWhat is ARC-AGI?\u003c/h2\u003e\n\n  \u003cp\u003e\u003ca href=\"https://arcprize.org/\"\u003eARC-AGI\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/1911.01547\"\u003eintroduced in 2019\u003c/a\u003e, is an artificial intelligence benchmark designed to test a system’s ability to infer and generalize abstract rules from minimal examples. The dataset consists of IQ-test-like puzzles, where each puzzle provides several example images that demonstrate an underlying rule, along with a test image that requires completing or applying that rule. While some have suggested that solving ARC-AGI might signal the advent of \u003ca href=\"https://arxiv.org/abs/1911.01547\"\u003eartificial general intelligence\u003c/a\u003e (AGI), its true purpose is to spotlight the current challenges hindering progress toward AGI. Below are three of the 1000 puzzles:\u003c/p\u003e\n\n  \u003ctable\u003e\n    \u003cthead\u003e\n      \u003ctr\u003e\n        \u003cth\u003eHidden rule: Shift every object to the right by one pixel, except the bottom/right edges of the object.\u003c/th\u003e\n        \u003cth\u003eHidden rule: Shrink the big object and set its color to the scattered dots’ color.\u003c/th\u003e\n        \u003cth\u003eHidden rule: Extend the green line to meet the red line by turning when hitting a wall.\u003c/th\u003e\n      \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/025d127b_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/5ad4f10b_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2dd70a9a_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n    \u003c/tbody\u003e\n  \u003c/table\u003e\n\n  \u003cp\u003eFor every puzzle, there is a hidden rule that maps each input grid to each output grid. You are given some number of examples of input-to-output mappings, and you get \u003cstrong\u003etwo attempts\u003c/strong\u003e to guess the output grid for a given input grid, without being told the hidden rule. If either guess is correct, then you score 1 for that puzzle, else you score 0. You are allowed to change the size of the output grid and pick the color of every pixel. The puzzles are designed so that \u003cstrong\u003ehumans can reasonably find the answer, but machines should have more difficulty\u003c/strong\u003e. \u003ca href=\"https://arcprize.org/guide\"\u003eThe average human can solve 76.2% of the training set\u003c/a\u003e, and \u003ca href=\"https://arxiv.org/abs/2409.01374\"\u003ea human expert can solve 98.5%.\u003c/a\u003e\u003c/p\u003e\n\n  \u003cp\u003eThe 400 training puzzles are easier than the rest, and are meant to help you learn the following patterns:\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003cstrong\u003eObjectness:\u003c/strong\u003e Objects persist and cannot appear or disappear without reason. Objects can interact or not depending on the circumstances.\u003c/li\u003e\n    \u003cli\u003e\u003cstrong\u003eGoal-directedness:\u003c/strong\u003e Objects can be animate or inanimate. Some objects are “agents” - they have intentions and they pursue goals.\u003c/li\u003e\n    \u003cli\u003e\u003cstrong\u003eNumbers \u0026amp; counting:\u003c/strong\u003e Objects can be counted or sorted by their shape, appearance, or movement using basic mathematics like addition, subtraction, and comparison.\u003c/li\u003e\n    \u003cli\u003e\u003cstrong\u003eBasic geometry \u0026amp; topology:\u003c/strong\u003e Objects can be shapes like rectangles, triangles, and circles which can be mirrored, rotated, translated, deformed, combined, repeated, etc. Differences in distances can be detected.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eThe ARC Prize team has repeatedly launched competitions for solving ARC-AGI, with monetary rewards. \u003ca href=\"https://www.kaggle.com/competitions/arc-prize-2024\"\u003eThe most recent competition\u003c/a\u003e involved potential prizes and awards of upwards of \u003cstrong\u003e$1,000,000\u003c/strong\u003e, with the main prize reserved for methods which could achieve 85% on a private test set of 100 puzzles, using 12 hours of compute in a constrained environment.\u003c/p\u003e\n\n  \n\n  \u003ch2 id=\"our-solution-method\"\u003eOur Solution Method\u003c/h2\u003e\n\n  \n\n  \u003cp\u003e\u003cstrong\u003eWe propose that lossless information compression can serve as an effective framework for solving ARC-AGI puzzles. A more efficient (i.e., lower-bit) compression of a puzzle correlates with a more accurate solution.\u003c/strong\u003e To solve ARC-AGI puzzles, we design a system that transforms an incomplete puzzle into a completed one—filling in the answers—by finding a compact representation that, when decompressed, reproduces the puzzle with any solution. The key challenge is to obtain this compact representation without needing the answers as inputs.\u003c/p\u003e\n\n  \u003cp\u003eCompressARC uses a neural network as the decoder. However, the encoding algorithm is not another network—instead, encoding is realized by the gradient descent algorithm that performs inference-time training on the decoder while maintaining correct decoded output. In other words, running the encoder means optimizing the decoder’s parameters and input distribution to achieve the most compressed puzzle representation. The resulting optimized parameters (e.g., weights and input distribution settings) themselves serve as the compressed bit representation that encodes the puzzle along with its answer.\u003c/p\u003e\n\n  \u003cp\u003eIn standard machine learning lingo: (without compression terminology, and with some simplifications)\u003c/p\u003e\n\n  \u003col\u003e\n    \u003cli\u003eWe start at inference time, and we are given an ARC-AGI puzzle to solve. (e.g., puzzle in the diagram below.)\u003c/li\u003e\n    \u003cli\u003eWe construct a neural network $f$ (see \u003ca href=\"#architecture\"\u003earchitecture\u003c/a\u003e) designed for the puzzle’s specifics (e.g., number of examples, observed colors). The network takes random normal input $z \\sim N(\\mu, \\Sigma)$, and per-pixel color logit predictions across all the grids, including an answer grid (3 input-output examples, for a total of 6 grids). Importantly, $f_\\theta$ is equivariant to common augmentations—such as reordering input-output pairs (including the answer’s pair), color permutations, and spatial rotations/reflections.\u003c/li\u003e\n    \u003cli\u003eWe initialize the network weights $\\theta$ and set the parameters $\\mu$ and $\\Sigma$ for the $z$ distribution.\u003c/li\u003e\n    \u003cli\u003eWe jointly optimize $\\theta$, $\\mu$, and $\\Sigma$ to minimize the sum of cross-entropies over the known grids (5 of them,) ignoring the answer grid. A KL divergence penalty keeps $N(\\mu, \\Sigma)$ close to $N(0,1)$, as in a VAE.\u003c/li\u003e\n    \u003cli\u003eSince the generated answer grid is stochastic due to the randomness in $z$, we save the answer grids throughout training and choose the most frequently occuring one as our final prediction.\u003c/li\u003e\n  \u003c/ol\u003e\n\n  \u003cp\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Method_Overview.png\"/\u003e\u003c/p\u003e\n\n  \u003cp\u003eIt isn’t obvious why such a method is performing compression. You’ll see later \u003ca href=\"#how-to-derive-our-solution-method\"\u003ehow we derived it\u003c/a\u003e from trying to compress ARC-AGI. First, let’s see it try to solve the puzzle above.\u003c/p\u003e\n\n  \u003ch3 id=\"watching-the-network-learn-color-the-boxes\"\u003eWatching the Network Learn: Color the Boxes\u003c/h3\u003e\n\n  \u003ch4 id=\"human-solution\"\u003eHuman Solution:\u003c/h4\u003e\n  \u003cp\u003eWe first realize that the input is divided into boxes, and the boxes are still there in the output, but now they’re colored. We then try to figure out which colors go in which boxes. First, we notice that the corners are always black. Then, we notice that the middle is always magenta. And after that, we notice that the color of the side boxes depends on which direction they are in: red for up, blue for down, green for right, and yellow for left. At this point, we copy the input over to the answer grid, then we color the middle box magenta, and then color the rest of the boxes according to their direction.\u003c/p\u003e\n\n  \u003ch4 id=\"compressarc-solution\"\u003eCompressARC Solution:\u003c/h4\u003e\n  \u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e 50 steps of learning:\u003c/strong\u003e\n  \u003cp\u003e\n  CompressARC\u0026#39;s network outputs an answer grid (sample) with light blue rows/columns wherever the input has the same. It has noticed that all the other input-output pairs in the puzzle exhibit this correspondence. It doesn\u0026#39;t know how the other output pixels are assigned colors; an exponential moving average of the network output (sample average) shows the network assigning mostly the same average color to non-light-blue pixels.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_50_steps.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e 150 steps of learning:\u003c/strong\u003e\n  \u003cp\u003e\n  The network outputs a grid where nearby pixels have similar colors. It has likely noticed that this is common among all the outputs, and is guessing that it applies to the answer too.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_150_steps.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e 200 steps of learning:\u003c/strong\u003e\n  \u003cp\u003e\n  The network output now shows larger blobs of colors that are cut off by the light blue borders. It has noticed the common usage of borders to demarcate blobs of colors in other outputs, and applies the same idea here. It has also noticed black corner blobs in other given outputs, which the network imitates.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_200_steps.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e 350 steps of learning:\u003c/strong\u003e\n  \u003cp\u003e\n  The network output now shows the correct colors assigned to boxes of the correct direction from the center. It has realized that a single color-to-direction mapping is used to pick the blob colors in the other given outputs, so it imitates this mapping. It is still not the best at coloring within the lines, and it\u0026#39;s also confused about the center blob, probably because the middle does not correspond to a direction. Nevertheless, the averate network output does show a tinge of the correct magenta color in the middle, meaning the network is catching on.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_350_steps.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e 1500 steps of learning:\u003c/strong\u003e\n  \u003cp\u003e\n  The network is as refined as it will ever be. Sometimes it will still make a mistake in the sample it outputs, but this uncommon and filtered out.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_1500_steps.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n  \u003cp\u003eAfter training, \u003ca href=\"#solution-analysis-color-the-boxes\"\u003ewe can deconstruct the learned z distribution\u003c/a\u003e to find that it codes for a color-direction correspondence table and row/column divider positions!\u003c/p\u003e\n\n  \n\n  \u003ch2 id=\"how-to-derive-our-solution-method\"\u003eHow to Derive Our Solution Method\u003c/h2\u003e\n\n  \u003cp\u003eAgain, it isn’t obvious how we get from trying to perform compression to the method we ended up using. The derivation of our algorithm takes us on a detour through \u003ca href=\"https://en.wikipedia.org/wiki/Information_theory\"\u003einformation theory\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/0809.2754\"\u003ealgorithmic information theory\u003c/a\u003e, and \u003ca href=\"https://en.wikipedia.org/wiki/Coding_theory\"\u003ecoding theory\u003c/a\u003e, with machine learning only making an appearance near the end.\u003c/p\u003e\n\n  \u003ch3 id=\"a-primer-on-lossless-information-compression\"\u003eA Primer on Lossless Information Compression\u003c/h3\u003e\n\n  \u003cp\u003eIn \u003ca href=\"https://en.wikipedia.org/wiki/Information_theory\"\u003einformation theory\u003c/a\u003e, lossless information compression is about trying to represent some information in as few bits as possible, while still being able to reconstruct that information from the bit representation. This type of problem is abstracted as follows:\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003eA source produces some symbol $x$ from some process that generates symbols from a probability distribution $p(x)$.\u003c/li\u003e\n    \u003cli\u003eA compressor/encoder $E$ must map the symbol $x$ to a string of bits $s$.\u003c/li\u003e\n    \u003cli\u003eA decompressor/decoder $D$ must exactly map $s$ back to the original symbol $x$.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eThe goal is to use $p$ to construct functions $(E, D)$ which are bit-efficient, (ie. that minimize the expected length of $s$,) without getting any symbols wrong. In our case, the symbol $x$ is the ARC-AGI dataset (many puzzle + answer pairs), and we want to figure out what answers the best compression system might decompress the answers to be. Except, we don’t have the answers (only the puzzles) to give as input to $E$, and we don’t know $p$, since it’s hard to model the intelligent process of puzzle ideation in humans.\u003c/p\u003e\n\n  \u003ch3 id=\"one-size-fits-all-compression\"\u003eOne-Size-Fits-All Compression\u003c/h3\u003e\n\n  \u003cp\u003eTo build our compression scheme, you might think we need to know what $p$ is, but we argue that it doesn’t really matter since we can make a one-size-fits-all compressor. It all hinges on the following assumption:\u003c/p\u003e\n  \u003cblockquote\u003e\n    \u003cp\u003eThere exists some practically implementable, bit efficient compression system $(E, D)$ for ARC-AGI datasets $x$ sampled from $p$.\u003c/p\u003e\n  \u003c/blockquote\u003e\n\n  \u003cp\u003eIf this were false, our whole idea of solving ARC-AGI with compression is doomed even if we knew $p$ anyways, so we might as well make this assumption.\u003c/p\u003e\n\n  \u003cp\u003eOur one-size-fits-all compressor $(E’, D’)$ is built without knowing $p$, and it is almost just as bit-efficient as the original $(E, D)$:\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003e$E’$ observes symbol $x$, picks a program $f$ and input $s$ to minimize $\\text{len}(f)+\\text{len}(s)$ under the constraint that running the program makes $f(s)=x$, and then sends the pair $(f, s)$.\u003c/li\u003e\n    \u003cli\u003e$D’$ is just a program executor that executes $f$ on $s$, correctly producing $x$.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eIt is possible to prove with \u003ca href=\"https://arxiv.org/abs/0809.2754\"\u003ealgorithmic information theory\u003c/a\u003e that $(E’, D’)$ achieves a bit efficiency at most $\\text{len}(f)$ bits worse than the bit efficiency of $(E, D)$, where $f$ is the \u003cem\u003ecode for implementing D\u003c/em\u003e. But since compression is practically implementable, the code for $D$ should be simple enough for a human engineer to write, so $\\text{len}(f)$ must be short, meaning our one-size-fits-all compressor will be close to the best possible bit efficiency.\u003c/p\u003e\n\n  \u003cp\u003eIronically, the only problem with using this to solve ARC-AGI is that implementing $E’$ is not practical, since $E’$ needs to minimize the length of a program-input pair $(f, s)$ under partial fixed output constraint $f(s)_{answers}=x_{answers}$.\u003c/p\u003e\n\n  \u003ch3 id=\"neural-networks-to-the-rescue\"\u003eNeural Networks to the Rescue\u003c/h3\u003e\n\n  \u003cp\u003eTo avoid searching through program space, we just pick a program $f$ for a small sacrifice in bit efficiency. We hope the diversity of program space can be delegated to diversity in input $s$ space instead. Specifically, we write a program $f$ that runs the forward pass of a neural network, where $s=(\\theta, z, \\epsilon)$ are the weights, inputs, and corrections to the outputs of the neural network. Then, we can use gradient descent to “search” over $s$.\u003c/p\u003e\n\n  \u003cp\u003eThis restricted compression scheme uses \u003ca href=\"https://arxiv.org/abs/2010.01185\"\u003eRelative Entropy Coding\u003c/a\u003e (REC)\u003csup id=\"fnref:3\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:3\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e to encode noisy weights $\\theta$ and neural network inputs $z$ into bits $s_\\theta$ and $s_z$, and \u003ca href=\"https://en.wikipedia.org/wiki/Arithmetic_coding\"\u003earithmetic coding\u003c/a\u003e to encode output error corrections $\\epsilon$ into bits $s_\\epsilon$, to make a bit string $s$ consisting of three blocks $(s_\\theta, s_z, s_\\epsilon)$. The compression scheme runs as follows:\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003eThe decoder runs $\\theta = \\text{REC-decode}(s_\\theta)$, $z = \\text{REC-decode}(s_z)$, $\\text{logits} = \\text{Neural-Net}(\\theta, z)$, and $x=\\text{Arithmetic-decode}(s_\\epsilon, \\text{logits})$.\u003c/li\u003e\n    \u003cli\u003eThe encoder trains $\\theta$ and $z$ to minimize the total code length $\\mathbb{E}[\\text{len}(s)]$. $s_\\epsilon$ is fixed by arithmetic coding to guarantee correct decoding. To calculate the three components of the loss $\\mathbb{E}[\\text{len}(s)]$ in a differentiable way, we refer to the properties of REC and arithmetic coding:\n      \u003cul\u003e\n        \u003cli\u003eIt turns out that the $\\epsilon$ code length $\\mathbb{E}[\\text{len}(s_\\epsilon)]$ is equal to the total crossentropy error on all the given grids in the puzzle.\u003c/li\u003e\n        \u003cli\u003eREC requires us to fix some reference distribution $q_\\theta$, and also add noise to $\\theta$, turning it into a distribution $p_\\theta$. Then, REC allows you to store noisy $\\theta$ using a code length of $\\mathbb{E}[\\text{len}(s_\\theta)] = KL(p_\\theta|| q_\\theta) = \\mathbb{E}_{\\theta \\sim p_\\theta} [\\log (p_\\theta(\\theta) / q_\\theta(\\theta))]$ bits. We will choose to fix $q_\\theta = N(0, I/2\\lambda)$ for large $\\lambda$, such that the loss component $\\mathbb{E}[\\text{len}(s_\\theta)] \\approx \\lambda | \\theta|^2 + \\text{const}$ is equivalent to regularizing the decoder.\u003c/li\u003e\n        \u003cli\u003eWe must also do for $z$ what we do for $\\theta$, since it’s also represented using REC. We will choose to fix $q_z = N(0,I)$, so the code length of $z$ is $\\mathbb{E}[\\text{len}(s_z)] = KL(p_z|| q_z) = \\mathbb{E}_{z \\sim p_z} [\\log (p_z(z) / q_z(z))]$.\u003c/li\u003e\n      \u003c/ul\u003e\n\n      \u003cp\u003eWe can compute gradients of these code lengths via the \u003ca href=\"https://arxiv.org/abs/1312.6114\"\u003ereparameterization trick\u003c/a\u003e.\u003c/p\u003e\n    \u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eAt this point, we observe that the total code length for $s$ that we described is actually the VAE loss with decoder regularization (= KL for $z$ + reconstruction error + regularization). Likewise, if we port the rest of what we described above (plus modifications regarding equivariances and inter-puzzle independence, and ignoring regularization) into typical machine learning lingo, we get the \u003ca href=\"#our-solution-method\"\u003eabove description of CompressARC\u003c/a\u003e.\u003c/p\u003e\n\n  \n\n  \u003ch2 id=\"architecture\"\u003eArchitecture\u003c/h2\u003e\n\n  \u003cp\u003eWe designed our own neural network architecture for decoding the latents $z$ into ARC-AGI puzzles. The most important feature of our architecture is it’s equivariances, which are symmetry rules dictating that whenever the input $z$ undergoes a transformation, the output ARC-AGI puzzle must also transform the same way. Some examples:\u003c/p\u003e\n\n  \u003cul\u003e\n    \u003cli\u003ereordering of input/output pairs\u003c/li\u003e\n    \u003cli\u003eshuffling colors\u003c/li\u003e\n    \u003cli\u003eflips, rotations, and reflections of grids\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eThere are too many equivariances for us to think about at once, so we decided to make a \u003cstrong\u003ebase architecture that’s fully symmetric\u003c/strong\u003e, and break unwanted symmetries one by one by \u003cstrong\u003eadding asymmetric layers\u003c/strong\u003e to give it \u003ca href=\"#what-puzzles-can-and-cant-we-solve\"\u003especific non-equivariant abilities\u003c/a\u003e.\u003c/p\u003e\n\n  \u003cp\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Equivariant_Architecture.png\"/\u003e\u003c/p\u003e\n\n  \u003cp\u003eTo illustrate what we mean, suppose that both $z$ and an ARC-AGI puzzle take the form of a tensor of shape $[n\\_examples, n\\_colors, height, width, 2 \\text{ for input/output}]$ (This is not actually the format of the data (see \u003ca href=\"#multitensors\"\u003emultitensors\u003c/a\u003e) but it gets the idea across best.) Then, our network starts out as equivariant to permutations of indices in the $example$, $color$, $height$, and $width$ dimensions. Some extra care must be taken with weight sharing, to force the network to also be equivariant to swapping the $width$ and $height$ dimensions. We may then add a layer involving a roll by one in the $width$ and $height$ dimensions, to let the network distinguish short range spatial interactions but not long-range ones.\u003c/p\u003e\n\n  \n\n  \u003cp\u003eThe actual data ($z$, hidden activations, and puzzles) passing through our layers comes in a format that we call a “\u003cstrong\u003emultitensor\u003c/strong\u003e”, which is just a bucket of tensors of various shapes. All the equivariances can be described in terms of how they change a multitensor. \u003cstrong\u003eIn order to understand any of the layers we list, you must first read the below section on multitensors.\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Multitensor.png\" width=\"50%\"/\u003e\u003c/p\u003e\n\n  \u003ch3 id=\"multitensors\"\u003eMultitensors\u003c/h3\u003e\n\n  \u003cp\u003eMost common classes of machine learning architectures operate on a single type of tensor with constant rank. LLMs operate on rank 3 tensors of shape $[n\\_batch, n\\_tokens, n\\_channels]$, and CNNs operate on a rank 4 tensors of shape $[n\\_batch, n\\_channels, height, width]$. Our multitensors are a set of varying-rank tensors of unique type, whose dimensions are a subset of a rank 6 tensor of shape $[n\\_examples$, $n\\_colors$, $n\\_directions$, $height$, $width$, $n\\_channels]$. We always keep the $channel$ dimension, so there are at most 32 tensors in every multitensor. We also maintain \u003ca href=\"#rules-for-legal-multitensors\"\u003eseveral rules\u003c/a\u003e that determine whether a tensor shape is “legal” or not, which reduces the number of tensors in a multitensor to 18.\u003c/p\u003e\n\n  \u003ctable\u003e\n    \u003cthead\u003e\n      \u003ctr\u003e\n        \u003cth\u003eDimension\u003c/th\u003e\n        \u003cth\u003eSize\u003c/th\u003e\n      \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eExample\u003c/td\u003e\n        \u003ctd\u003eNumber of examples in the ARC-AGI puzzle, including the one with held-out answer\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eColor\u003c/td\u003e\n        \u003ctd\u003eNumber of unique colors in the ARC-AGI puzzle, \u003ca href=\"#number-of-colors\"\u003enot including black\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eDirection\u003c/td\u003e\n        \u003ctd\u003e8\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eHeight\u003c/td\u003e\n        \u003ctd\u003e\u003ca href=\"#output-shape-determination\"\u003eDetermined when preprocessing the puzzle\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eWidth\u003c/td\u003e\n        \u003ctd\u003e\u003ca href=\"#output-shape-determination\"\u003eDetermined when preprocessing the puzzle\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003eChannel\u003c/td\u003e\n        \u003ctd\u003eIn the residual connections, the size is 8 if the $direction$ dimension is included, else 16. Within layers it is layer-dependent.\u003c/td\u003e\n      \u003c/tr\u003e\n    \u003c/tbody\u003e\n  \u003c/table\u003e\n\n  \u003cp\u003eTo give an idea of how a multitensor stores data, an ARC-AGI puzzle can be represented by using the $[examples, colors, height, width, channel]$ tensor, by using the $channel$ dimension to select either the input or output grid, and the $width$/$height$ dimensions for pixel location, a one hot vector in the $color$ dimension, specifying what color that pixel is. The $[examples, width, channel]$ and $[examples, height, channel]$ tensors can similarly be used to store masks representing grid shapes for every example for every input/output grid. All those tensors are included in a single multitensor that is computed by the network just before the final \u003ca href=\"#linear-heads\"\u003elinear heads\u003c/a\u003e layer.\u003c/p\u003e\n\n  \u003cp\u003eWhen we apply an operation on a multitensor, we by default assume that all non-$channel$ dimensions are treated identically as batch dimensions by default. The operation is copied across the indices of dimensions unless specified. This ensures that we keep all our symmetries intact until we use a specific layer meant to break a specific symmetry.\u003c/p\u003e\n\n  \u003cp\u003eA final note on the $channel$ dimension: usually when talking about a tensor’s shape, we will not even mention the $channel$ dimension as it is included by default.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eThe full architecture consists of the following layers, which are each described in the Appendix:\u003c/strong\u003e\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003eBegin with parameters of the $z$ distribution,\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#decoding-layer\"\u003eDecoding Layer\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e4x\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#multitensor-communication-layer\"\u003eMultitensor Communication Layer, Upwards\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#softmax-layer\"\u003eSoftmax Layer\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#directional-cummaxshift-layer\"\u003eDirectional Cummax Layer\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#directional-cummaxshift-layer\"\u003eDirectional Shift Layer\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#directional-communication-layer\"\u003eDirectional Communication Layer\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#nonlinear-layer\"\u003eNonlinear Layer\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#multitensor-communication-layer\"\u003eMultitensor Communication Layer, Downwards\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#normalization-layer\"\u003eNormalization Layer\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#linear-heads\"\u003eLinear Heads\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \n\n  \u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\n  \u003ch3 id=\"training-set-3475\"\u003eTraining set: 34.75%\u003c/h3\u003e\n\n  \u003cp\u003e\n  \u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/accuracy_curve_at_n_training.png\"/\u003e\n\u003c/p\u003e\n\n  \u003cdiv\u003e\n  \u003ctable\u003e\n    \u003ctbody\u003e\u003ctr\u003e\n      \u003cth\u003eTraining Iteration\u003c/th\u003e\n      \u003cth\u003eTime\u003c/th\u003e\n      \u003cth\u003ePass@1\u003c/th\u003e\n      \u003cth\u003ePass@2\u003c/th\u003e\n      \u003cth\u003ePass@5\u003c/th\u003e\n      \u003cth\u003ePass@10\u003c/th\u003e\n      \u003cth\u003ePass@100\u003c/th\u003e\n      \u003cth\u003ePass@1000\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e100\u003c/td\u003e\n      \u003ctd\u003e6 h\u003c/td\u003e\n      \u003ctd\u003e1%\u003c/td\u003e\n      \u003ctd\u003e2.25%\u003c/td\u003e\n      \u003ctd\u003e3.5%\u003c/td\u003e\n      \u003ctd\u003e4.75%\u003c/td\u003e\n      \u003ctd\u003e6.75%\u003c/td\u003e\n      \u003ctd\u003e6.75%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e200\u003c/td\u003e\n      \u003ctd\u003e13 h\u003c/td\u003e\n      \u003ctd\u003e11.5%\u003c/td\u003e\n      \u003ctd\u003e14.25%\u003c/td\u003e\n      \u003ctd\u003e16.5%\u003c/td\u003e\n      \u003ctd\u003e18.25%\u003c/td\u003e\n      \u003ctd\u003e23.25%\u003c/td\u003e\n      \u003ctd\u003e23.5%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e300\u003c/td\u003e\n      \u003ctd\u003e19 h\u003c/td\u003e\n      \u003ctd\u003e18.5%\u003c/td\u003e\n      \u003ctd\u003e21.25%\u003c/td\u003e\n      \u003ctd\u003e23.5%\u003c/td\u003e\n      \u003ctd\u003e26.75%\u003c/td\u003e\n      \u003ctd\u003e31.5%\u003c/td\u003e\n      \u003ctd\u003e32.5%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e400\u003c/td\u003e\n      \u003ctd\u003e26 h\u003c/td\u003e\n      \u003ctd\u003e21%\u003c/td\u003e\n      \u003ctd\u003e25%\u003c/td\u003e\n      \u003ctd\u003e28.75%\u003c/td\u003e\n      \u003ctd\u003e31%\u003c/td\u003e\n      \u003ctd\u003e36%\u003c/td\u003e\n      \u003ctd\u003e37.5%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e500\u003c/td\u003e\n      \u003ctd\u003e32 h\u003c/td\u003e\n      \u003ctd\u003e23%\u003c/td\u003e\n      \u003ctd\u003e27.5%\u003c/td\u003e\n      \u003ctd\u003e31.5%\u003c/td\u003e\n      \u003ctd\u003e33.5%\u003c/td\u003e\n      \u003ctd\u003e39.25%\u003c/td\u003e\n      \u003ctd\u003e40.75%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e750\u003c/td\u003e\n      \u003ctd\u003e49 h\u003c/td\u003e\n      \u003ctd\u003e28%\u003c/td\u003e\n      \u003ctd\u003e30.5%\u003c/td\u003e\n      \u003ctd\u003e34%\u003c/td\u003e\n      \u003ctd\u003e36.25%\u003c/td\u003e\n      \u003ctd\u003e42.75%\u003c/td\u003e\n      \u003ctd\u003e44.5%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e1000\u003c/td\u003e\n      \u003ctd\u003e65 h\u003c/td\u003e\n      \u003ctd\u003e28%\u003c/td\u003e\n      \u003ctd\u003e31.75%\u003c/td\u003e\n      \u003ctd\u003e35.5%\u003c/td\u003e\n      \u003ctd\u003e37.75%\u003c/td\u003e\n      \u003ctd\u003e43.75%\u003c/td\u003e\n      \u003ctd\u003e46.5%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e1250\u003c/td\u003e\n      \u003ctd\u003e81 h\u003c/td\u003e\n      \u003ctd\u003e29%\u003c/td\u003e\n      \u003ctd\u003e32.25%\u003c/td\u003e\n      \u003ctd\u003e37%\u003c/td\u003e\n      \u003ctd\u003e39.25%\u003c/td\u003e\n      \u003ctd\u003e45.5%\u003c/td\u003e\n      \u003ctd\u003e49.25%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e1500\u003c/td\u003e\n      \u003ctd\u003e97 h\u003c/td\u003e\n      \u003ctd\u003e29.5%\u003c/td\u003e\n      \u003ctd\u003e33%\u003c/td\u003e\n      \u003ctd\u003e38.25%\u003c/td\u003e\n      \u003ctd\u003e40.75%\u003c/td\u003e\n      \u003ctd\u003e46.75%\u003c/td\u003e\n      \u003ctd\u003e51.75%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e2000\u003c/td\u003e\n      \u003ctd\u003e130 h\u003c/td\u003e\n      \u003ctd\u003e30.25%\u003c/td\u003e\n      \u003ctd\u003e34.75%\u003c/td\u003e\n      \u003ctd\u003e38.25%\u003c/td\u003e\n      \u003ctd\u003e41.5%\u003c/td\u003e\n      \u003ctd\u003e48.5%\u003c/td\u003e\n      \u003ctd\u003e52.75%\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\u003c/table\u003e\n\u003c/div\u003e\n\n  \u003ch3 id=\"evaluation-set-20\"\u003eEvaluation set: 20%\u003c/h3\u003e\n\n  \u003cp\u003e\n  \u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/accuracy_curve_at_n_evaluation.png\"/\u003e\n\u003c/p\u003e\n\n  \u003cdiv\u003e\n  \u003ctable\u003e\n    \u003ctbody\u003e\u003ctr\u003e\n      \u003cth\u003eTraining Iteration\u003c/th\u003e\n      \u003cth\u003eTime\u003c/th\u003e\n      \u003cth\u003ePass@1\u003c/th\u003e\n      \u003cth\u003ePass@2\u003c/th\u003e\n      \u003cth\u003ePass@5\u003c/th\u003e\n      \u003cth\u003ePass@10\u003c/th\u003e\n      \u003cth\u003ePass@100\u003c/th\u003e\n      \u003cth\u003ePass@1000\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e100\u003c/td\u003e\n      \u003ctd\u003e7 h\u003c/td\u003e\n      \u003ctd\u003e0.75%\u003c/td\u003e\n      \u003ctd\u003e1.25%\u003c/td\u003e\n      \u003ctd\u003e2.25%\u003c/td\u003e\n      \u003ctd\u003e2.5%\u003c/td\u003e\n      \u003ctd\u003e3%\u003c/td\u003e\n      \u003ctd\u003e3%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e200\u003c/td\u003e\n      \u003ctd\u003e14 h\u003c/td\u003e\n      \u003ctd\u003e5%\u003c/td\u003e\n      \u003ctd\u003e6%\u003c/td\u003e\n      \u003ctd\u003e7%\u003c/td\u003e\n      \u003ctd\u003e7.75%\u003c/td\u003e\n      \u003ctd\u003e12%\u003c/td\u003e\n      \u003ctd\u003e12.25%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e300\u003c/td\u003e\n      \u003ctd\u003e21 h\u003c/td\u003e\n      \u003ctd\u003e10%\u003c/td\u003e\n      \u003ctd\u003e10.75%\u003c/td\u003e\n      \u003ctd\u003e12.25%\u003c/td\u003e\n      \u003ctd\u003e13.25%\u003c/td\u003e\n      \u003ctd\u003e15.5%\u003c/td\u003e\n      \u003ctd\u003e16.25%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e400\u003c/td\u003e\n      \u003ctd\u003e28 h\u003c/td\u003e\n      \u003ctd\u003e11.75%\u003c/td\u003e\n      \u003ctd\u003e13.75%\u003c/td\u003e\n      \u003ctd\u003e16%\u003c/td\u003e\n      \u003ctd\u003e17%\u003c/td\u003e\n      \u003ctd\u003e19.75%\u003c/td\u003e\n      \u003ctd\u003e20%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e500\u003c/td\u003e\n      \u003ctd\u003e34 h\u003c/td\u003e\n      \u003ctd\u003e13.5%\u003c/td\u003e\n      \u003ctd\u003e15%\u003c/td\u003e\n      \u003ctd\u003e17.75%\u003c/td\u003e\n      \u003ctd\u003e19.25%\u003c/td\u003e\n      \u003ctd\u003e20.5%\u003c/td\u003e\n      \u003ctd\u003e21.5%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e750\u003c/td\u003e\n      \u003ctd\u003e52 h\u003c/td\u003e\n      \u003ctd\u003e15.5%\u003c/td\u003e\n      \u003ctd\u003e17.75%\u003c/td\u003e\n      \u003ctd\u003e19.75%\u003c/td\u003e\n      \u003ctd\u003e21.5%\u003c/td\u003e\n      \u003ctd\u003e22.75%\u003c/td\u003e\n      \u003ctd\u003e25.5%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e1000\u003c/td\u003e\n      \u003ctd\u003e69 h\u003c/td\u003e\n      \u003ctd\u003e16.75%\u003c/td\u003e\n      \u003ctd\u003e19.25%\u003c/td\u003e\n      \u003ctd\u003e21.75%\u003c/td\u003e\n      \u003ctd\u003e23%\u003c/td\u003e\n      \u003ctd\u003e26%\u003c/td\u003e\n      \u003ctd\u003e28.75%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e1250\u003c/td\u003e\n      \u003ctd\u003e86 h\u003c/td\u003e\n      \u003ctd\u003e17%\u003c/td\u003e\n      \u003ctd\u003e20.75%\u003c/td\u003e\n      \u003ctd\u003e23%\u003c/td\u003e\n      \u003ctd\u003e24.5%\u003c/td\u003e\n      \u003ctd\u003e28.25%\u003c/td\u003e\n      \u003ctd\u003e30.75%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e1500\u003c/td\u003e\n      \u003ctd\u003e103 h\u003c/td\u003e\n      \u003ctd\u003e18.25%\u003c/td\u003e\n      \u003ctd\u003e21.5%\u003c/td\u003e\n      \u003ctd\u003e24.25%\u003c/td\u003e\n      \u003ctd\u003e25.5%\u003c/td\u003e\n      \u003ctd\u003e29.5%\u003c/td\u003e\n      \u003ctd\u003e31.75%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e2000\u003c/td\u003e\n      \u003ctd\u003e138 h\u003c/td\u003e\n      \u003ctd\u003e18.5%\u003c/td\u003e\n      \u003ctd\u003e20%\u003c/td\u003e\n      \u003ctd\u003e24.25%\u003c/td\u003e\n      \u003ctd\u003e26%\u003c/td\u003e\n      \u003ctd\u003e31.25%\u003c/td\u003e\n      \u003ctd\u003e33.75%\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\u003c/table\u003e\n\u003c/div\u003e\n\n  \u003ch3 id=\"what-puzzles-can-and-cant-we-solve\"\u003eWhat Puzzles Can and Can’t We Solve?\u003c/h3\u003e\n\n  \u003cp\u003e\u003cstrong\u003eCompressARC tries to use its abilities to figure out as much as it can, until it gets bottlenecked by one of it’s inabilities.\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eFor example, puzzle 28e73c20 in the training set requires extension of a pattern from the edge towards the middle:\u003c/p\u003e\n\n  \u003cp\u003e\n  \u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_problem.png\"/\u003e\n\u003c/p\u003e\n\n  \u003cp\u003eGiven the layers in it’s network, CompressARC is generally able to extend patterns for short ranges but not long ranges. So, it does the best that it can, and correctly extends the pattern a short distance before guessing at what happens near the center:\u003c/p\u003e\n\n  \u003cp\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_solutions.png\" alt=\"image\"/\u003e\u003c/p\u003e\n\n  \u003cp\u003eA short list of abilities that \u003cstrong\u003ecan\u003c/strong\u003e be performed by CompressARC includes:\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003eAssigning individual colors to individual procedures (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e0ca9ddb6\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eInfilling (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e0dfd9992\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eCropping (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e1c786137\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eConnecting dots with lines, including 45 degree diagonal lines (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e1f876c06\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eSame color detection (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e1f876c06\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eIdentifying pixel adjacencies (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e42a50994\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eAssigning individual colors to individual examples (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e3bd67248\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eIdentifying parts of a shape (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e025d127b\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eTranslation by short distances (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e025d127b\u003c/a\u003e)\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eA short list of abilities that \u003cstrong\u003ecannot\u003c/strong\u003e be performed by CompressARC includes:\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003eAssigning two colors to each other (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e0d3d703e\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eRepeating an operation in series many times (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e0a938d79\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eCounting/numbers (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003ece9e57f2\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eTranslation, rotation, reflections, rescaling, image duplication (see puzzles \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e0e206a2e\u003c/a\u003e, \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e5ad4f10b\u003c/a\u003e, and \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e2bcee788\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eDetecting topological properties such as connectivity (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e7b6016b9\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003ePlanning, simulating the behavior of an agent (see puzzle \u003ca href=\"#list-of-mentioned-arc-agi-puzzles\"\u003e2dd70a9a\u003c/a\u003e)\u003c/li\u003e\n    \u003cli\u003eLong range extensions of patterns (see puzzle 28e73c20 above)\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \n\n  \u003ch2 id=\"case-study-color-the-boxes\"\u003eCase Study: Color the Boxes\u003c/h2\u003e\n\n  \u003cp\u003e(Additional case studies can be found in the \u003ca href=\"#additional-case-studies\"\u003eAppendix\u003c/a\u003e.)\u003c/p\u003e\n\n  \u003cp\u003eWe show the puzzle again for convenience.\u003c/p\u003e\n  \u003cp\u003e\n  \u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_problem.png\"/\u003e\n\u003c/p\u003e\n\n  \u003cp\u003eDuring training, the reconstruction error fell extremely quickly. It remained low on average, but would spike up every once in a while, causing the KL from $z$ to bump upwards at these moments.\u003c/p\u003e\n\n  \u003cp\u003e\n  \u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_KL_vs_reconstruction.png\"/\u003e\n\u003c/p\u003e\n\n  \u003ch3 id=\"solution-analysis-color-the-boxes\"\u003eSolution Analysis: Color the Boxes\u003c/h3\u003e\n\n  \u003cp\u003eSo how does CompressARC learn to solve the puzzle? Let’s look at the representations stored in $z$ to find out.\u003c/p\u003e\n\n  \u003cp\u003eSince $z$ is a \u003ca href=\"#multitensors\"\u003emultitensor\u003c/a\u003e, each of the tensors it contains produces an additive contribution to the total KL for $z$. By looking at the per-tensor contributions, we can determine which tensors in $z$ code for information that is used to represent the puzzle. Below is a plot showing the quantity of information stored in each tensor of $z$, ie. the KL contribution used by the \u003ca href=\"#decoding-layer\"\u003edecoding layer\u003c/a\u003e.\u003c/p\u003e\n\n  \u003cp\u003e\n  \u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_KL_components.png\"/\u003e\n\u003c/p\u003e\n\n  \u003cp\u003eAll the tensors fall to zero information content during training, except for four tensors. In some replications of this experiment, we saw one of these four necessary tensors fall to zero information content, and CompressARC typically does not recover the correct answer after that. Here we are showing a lucky run where the $(color, direction, channel)$ tensor almost falls but gets picked up 200 steps in, which is right around when the samples from the model begin to show the correct colors in the correct boxes.\u003c/p\u003e\n\n  \u003cp\u003eWe can look at the average output of the \u003ca href=\"#decoding-layer\"\u003edecoding layer\u003c/a\u003e corresponding to individual tensors of $z$, to see what information is stored there. Each tensor contains a vector of dimension $n\\_channels$ for various indices of the tensor. Taking the PCA of these vectors reveals some number of activated components, telling us how many pieces of information are coded by the tensor.\u003c/p\u003e\n\n  \u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e (Examples, height, channel) tensor:\u003c/strong\u003e\n  \u003cp\u003e\n  For every example and row, there is a vector of dimension $n\\_channels$. This forms a dataset of vectors. Taking the PCA of these vectors, the top principal component vector reformatted back into an $(examples, height)$ matrix (shown on right) can tell us which examples/row combinations are uniquely identified by the stored information. The top principal component (shown on right) is 1485 times stronger than the second principal component, which indicates to us that basically all of the information is in the above tensor. \u003cstrong\u003eFor every example, the two brightest pixels give the rows where the light blue rows in the grids are.\u003c/strong\u003e\u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_example_height_component_0.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e (Examples, width, channel) tensor:\u003c/strong\u003e\n  \u003cp\u003e\n  A very similar story here: in the top principal component of this tensor, \u003cstrong\u003ethe two darkest pixels for every example give the columns where the light blue columns in the grids are.\u003c/strong\u003e The top principal component is 1253 times stronger than the next principal component.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_example_width_component_0.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e (Direction, color, channel) tensor:\u003c/strong\u003e\n  \u003cp\u003e\n  In this tensor, we see that the four brightest pixels identify blue with up, green with left, red with down, and yellow with right. \u003cstrong\u003eThis tensor seems to tell each direction which color to use for the opposite direction\u0026#39;s corresponding box.\u003c/strong\u003e The top principal component is 829 times stronger than the next principal component.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_direction_component_0.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e (Color, channel) tensor:\u003c/strong\u003e\n  \u003cp\u003e\n  Here, we look at the top three principal components, since the first and second principal components are 134 and 87 times stronger than the third component, indicating that they play a role while the third component does not. The \u003cstrong\u003emagenta and light blue colors\u003c/strong\u003e are uniquely identified, indicating their special usage amongst the rest of the colors as \u003cstrong\u003ethe center color and the color of the row/column divisions\u003c/strong\u003e, respectively.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_component_0.png\"/\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_component_1.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n  \n\n  \u003ch2 id=\"how-to-improve-our-work\"\u003eHow to Improve Our Work\u003c/h2\u003e\n\n  \u003cp\u003eAt the time of release of CompressARC, there were several ideas which we thought of trying or attempted at some point, but didn’t manage to get working for one reason or another. Some ideas we still believe in, but didn’t use, are listed below.\u003c/p\u003e\n\n  \u003ch4 id=\"joint-compression-via-weight-sharing-between-puzzles\"\u003eJoint Compression via Weight Sharing Between Puzzles\u003c/h4\u003e\n\n  \u003cp\u003eCompressARC tries to solve each puzzle serially by compressing each puzzle on its own. We believe that joint compression of all the entire ARC-AGI dataset at once should yield better learned inductive biases per-puzzle, since computations learned for one puzzle can be transferred to other puzzles. We do not account for the complexity of $f$ in our \u003ca href=\"#how-to-derive-our-solution-method\"\u003emethod\u003c/a\u003e, allowing for $f$ to be used for memorization/overfitting. By jointly compressing the whole dataset, we only need to have one $f$, whereas when compressing each puzzle individually, we need to have an $f$ for every puzzle, allowing for more memorization/overfitting.\u003c/p\u003e\n\n  \u003cp\u003eTo implement this, we would most likely explore strategies like:\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003eUsing the same network weights for all puzzles, and training for puzzles in parallel. Each puzzle gets assigned some perturbation to the weights, that is constrained in some way, e.g., \u003ca href=\"https://arxiv.org/abs/2106.09685\"\u003eLORA\u003c/a\u003e.\u003c/li\u003e\n    \u003cli\u003eLearning a “puzzle embedding” for every puzzle that is a high dimensional vector (more than 16 dim, less than 256 dim), and learning a linear mapping from puzzle embeddings to weights for our network. This mapping serves as a basic \u003ca href=\"https://arxiv.org/abs/2306.06955\"\u003ehypernetwork\u003c/a\u003e, ie. a neural network that outputs weights for another neural network.\nIn a successful case, we might want to also try adding in some form of positional encodings, with the hope that $f$ is now small/simple enough to be incapable of memorization/overfitting using positional encodings.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eThe reason we didn’t try this is because it would slow down the research iteration process.\u003c/p\u003e\n\n  \u003ch4 id=\"convolution-like-layers-for-shape-copying-tasks\"\u003eConvolution-like Layers for Shape Copying Tasks\u003c/h4\u003e\n\n  \u003cp\u003eThis improvement is more ARC-AGI-specific and may have less to do with AGI in our view. Many ARC-AGI puzzles can be seen to involve copying shapes from one place to another, and our network has no inductive biases for such an operation. An operation which is capable of copying shapes onto multiple locations is the \u003ca href=\"https://en.wikipedia.org/wiki/Convolution\"\u003econvolution\u003c/a\u003e. With one grid storing the shape and another with pixels activated at locations to copy to, convolving the two grids will produce another grid with the shape copied to the designated locations.\u003c/p\u003e\n\n  \u003cp\u003eThere are several issues with introducing a convolutional operation for the network to use. Ideally, we would read two grids via projection from the residual stream, convolve them, and write it back in via another projection, with norms in the right places and such. Ignoring the fact that the grid size changes during convolution (can be solved with two parallel networks using different grid sizes), the bigger problem is that convolutions tend to amplify noise in the grids much more than the sparse signals, so their inductive bias is not good for shape copying. We can try to apply a softmax to one or both of the grids to reduce the noise (and to draw an interesting connection to attention), but we didn’t find any success.\u003c/p\u003e\n\n  \u003cp\u003eThe last idea that we were tried before discarding the idea was to modify the functional form of the convolution:\u003c/p\u003e\u003cp\u003e\n\n\\[(f * g)(x) = \\sum_y f(x-y)g(y)\\]\n\n  \u003c/p\u003e\u003cp\u003eto \u003ca href=\"https://arxiv.org/abs/2103.02096\"\u003ea tropical convolution\u003c/a\u003e, which we found to work well on toy puzzles, but not well enough for ARC-AGI training puzzles (which is why we discarded this idea):\u003c/p\u003e\u003cp\u003e\n\n\\[(f*g)(x) = \\max_y f(x-y) + g(y)\\]\n\n  \u003c/p\u003e\u003cp\u003eConvolutions, when repeated with some grids flipped by 180 degrees, tend to create high activations at the center pixel, so sometimes it is important to zero out the center pixel to preserve the signal.\u003c/p\u003e\n\n  \u003ch4 id=\"kl-floor-for-posterior-collapse\"\u003eKL Floor for Posterior Collapse\u003c/h4\u003e\n\n  \u003cp\u003eWe noticed during testing that crucial posterior tensors whose \u003ca href=\"https://arxiv.org/abs/1711.00937\"\u003eKL fell to zero during learning\u003c/a\u003e would never make a recovery and play their role in the encoding. We believe that the KL divergence may upper bound the information content of the gradient training signal for parts of the network that process the encoded information. Thus, when a tensor falls to zero KL, the network stops learning to use its information, so the KL is no longer given encouragement to recover. If we can hold the KL above zero for a while, the network may then learn to use the information, giving the KL a reason to stay above zero when released again.\u003c/p\u003e\n\n  \u003cp\u003eWe implemented a mechanism to keep the KL above a minimum threshold so that the network always learns to use that information, but we do not believe it learns fast enough for this to be useful, as we have never seen a tensor recover before. Therefore, it might be useful to explore different ways to schedule this KL floor to start high and decay to zero, to allow learning when the KL is forced to be high, and to leave the KL unaffected later on in learning. This might cause training results to be more consistent across runs.\u003c/p\u003e\n\n  \u003ch4 id=\"regularization\"\u003eRegularization\u003c/h4\u003e\n\n  \u003cp\u003eWe don’t use it. Maybe it matters, but we don’t know. Regularization measures the complexity of $f$ in our \u003ca href=\"#how-to-derive-our-solution-method\"\u003eproblem formulation\u003c/a\u003e, and is native to our derivation of CompressARC. It is somewhat reckless for us to exclude it in our implementation.\u003c/p\u003e\n\n  \n\n  \n\n  \u003ch4 id=\"equivalence-of-compression-and-intelligence\"\u003eEquivalence of Compression and Intelligence\u003c/h4\u003e\n\n  \u003cp\u003eThe original inspiration of this work came from the \u003ca href=\"http://prize.hutter1.net/\"\u003eHutter Prize\u003c/a\u003e, which awards a prize for those who can compress a file of Wikipedia text the most, as a motivation for researchers to build intelligent systems. It is premised upon the idea that the ability to compress information is equivalent to intelligence.\u003c/p\u003e\n\n  \u003cp\u003eThis equivalence between inteeligence and compression has a long history. For example, when talking about intelligent solutions to prediction problems, the ideal predictor implements \u003ca href=\"https://www.sciencedirect.com/science/article/pii/S0019995864902232\"\u003eSolomonoff Induction\u003c/a\u003e, a theoretically best possible but uncomputable prediction algorithm that works universally for all prediction tasks. This prediction algorithm is then equivalent to a best possible compression algorithm whose compressed code length is the \u003ca href=\"https://www.sciencedirect.com/science/article/pii/S0304397598000759?via%3Dihub\"\u003eKolmogorov Complexity\u003c/a\u003e of the data. In our work, we try to approximate this best possible compression algorithm with a neural network. A related measure of complexity is known as the \u003ca href=\"https://www.sciencedirect.com/science/article/abs/pii/0005109878900055?via%3Dihub\"\u003eMinimum Description Length\u003c/a\u003e.\u003c/p\u003e\n\n  \u003ch4 id=\"information-theory-and-coding-theory\"\u003eInformation Theory and Coding Theory\u003c/h4\u003e\n\n  \u003cp\u003eSince we build an information compression system, we make use of many results in information theory and coding theory. The main result required to motivate our model architecture is the existence of \u003ca href=\"https://arxiv.org/abs/2010.01185\"\u003eRelative Entropy Coding\u003c/a\u003e (REC). The fact that REC exists means that as long as a KL divergence can be bounded, the construction of a compression algorithm is always possible and the issue of realizing the algorithm can be abstracted away. Thus, problems about coding theory and translating information from Gaussians into binary and back can be ignored, since we can figure out the binary code length directly from the Gaussians instead. In other words, we only need to do enough information theory using the Gaussians to get the job done, with no coding theory at all. While the existence of \u003ca href=\"https://en.wikipedia.org/wiki/Arithmetic_coding\"\u003earithmetic coding\u003c/a\u003e would suffice to abstract the problem away when distributions are discrete, neural networks operate in a continuous space so we need REC instead.\u003c/p\u003e\n\n  \u003cp\u003eOur architecture sends $z$ information through an additive white Gaussian noise (AWGN) channel, so the \u003ca href=\"https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem\"\u003eAWGN channel capacity formula\u003c/a\u003e (Gaussian input Gaussian noise) plays a heavy role in the design of our \u003ca href=\"#decoding-layer\"\u003edecoding layer\u003c/a\u003e.\u003c/p\u003e\n\n  \u003ch4 id=\"variational-autoencoders\"\u003eVariational Autoencoders\u003c/h4\u003e\n\n  \u003cp\u003eThe decoder side of the \u003ca href=\"https://arxiv.org/abs/1312.6114\"\u003evariational autoencoder\u003c/a\u003e serves as our decompression algorithm. While we would use something that has more general capabilities like a \u003ca href=\"https://arxiv.org/abs/1410.5401\"\u003eneural Turing machine instead\u003c/a\u003e, neural Turing machines are not very amenable to gradient descent-based optimization so we stuck with the VAE.\u003c/p\u003e\n\n  \u003cp\u003eVAEs have a long history of developments that are relevant to our work. At one point, we tried using multiple \u003ca href=\"#decoding-layer\"\u003edecoding layers\u003c/a\u003e to make a \u003ca href=\"https://arxiv.org/abs/1602.02282\"\u003ehierarchical VAE\u003c/a\u003e decoder instead. This does not affect Relative Entropy Coding with the AWGN channel because \u003ca href=\"https://ieeexplore.ieee.org/document/1056798\"\u003echannel capacity with feedback is equal to channel capacity without feedback\u003c/a\u003e. But, we found empirically that the first decoding layer would absorb all of the KL contribution, making the later decoding layers useless. Thus, we only used one decoding layer at the beginning.\u003c/p\u003e\n\n  \u003cp\u003eThe \u003ca href=\"https://openreview.net/forum?id=Sy2fzU9gl\"\u003ebeta-VAE\u003c/a\u003e introduces a reweighting of the reconstruction loss to be stronger than the KL loss, and we found that to work well in our case. The \u003ca href=\"https://arxiv.org/abs/2007.03898\"\u003eNVAE\u003c/a\u003e applies a non-constant weighting to loss components. A rudimentary form of scheduled loss recombination is used in CompressARC.\u003c/p\u003e\n\n  \u003ch4 id=\"arc-agi-methods\"\u003eARC-AGI Methods\u003c/h4\u003e\n\n  \u003cp\u003eCurrent methods for solving ARC-AGI focus primarily on using large language models (LLMs). ARC-AGI puzzles are converted into textual representations which are fed into LLMs as input. The LLM may directly output a textual representation of an answer, or some \u003ca href=\"https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt\"\u003ecode which tries to convert input grids into output grids\u003c/a\u003e. Top methods rely heavily on data augmentation and larger \u003ca href=\"https://arxiv.org/abs/2411.02272\"\u003ealternative datasets\u003c/a\u003e, and sometimes perform autoregressive training on the target puzzle during inference time. Top solutions (\u003ca href=\"https://ironbar.github.io/arc24/05_Solution_Summary/\"\u003eexample\u003c/a\u003e) in the 2024 Kaggle prize competition frequently used \u003ca href=\"https://arxiv.org/abs/1909.13231\"\u003etest-time training\u003c/a\u003e. \u003ca href=\"https://arcprize.org/blog/oai-o3-pub-breakthrough\"\u003eReasoning models\u003c/a\u003e have managed to get up to 87.5% on the semi-private evaluation set, albeit with astronomical amounts of compute.\u003c/p\u003e\n\n  \u003cp\u003eAn older class of methods consists of hard-coded searches through program spaces in \u003ca href=\"https://github.com/michaelhodel/arc-dsl\"\u003ehand-written domain-specific languages designed specifically for ARC\u003c/a\u003e. Another example \u003ca href=\"https://github.com/victorvikram/ARC-icecuber\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n  \u003cp\u003e\u003ca href=\"https://arxiv.org/html/2411.08706v1\"\u003eBonnet and Macfarlane introduced a VAE-based method\u003c/a\u003e for searching through a latent space of programs.\u003c/p\u003e\n\n  \u003cp\u003eWe believe CompressARC is the only method so far that uses deep learning without external pretraining nor any large-scale search.\u003c/p\u003e\n\n  \u003ch4 id=\"deep-learning-architectures\"\u003eDeep Learning Architectures\u003c/h4\u003e\n\n  \u003cp\u003eWe designed our own neural network architecture from scratch, but not without borrowing crucial design principles from many others.\u003c/p\u003e\n\n  \u003cp\u003eOur architecture is fundamentally structured like a \u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003etransformer\u003c/a\u003e, consisting of a \u003ca href=\"https://arxiv.org/abs/1512.03385\"\u003eresidual stream\u003c/a\u003e where representations are stored and operated upon, followed by a linear head. \u003ca href=\"https://arxiv.org/abs/2002.04745\"\u003ePre-and post-norms\u003c/a\u003e with linear up- and down-projections allow layers to read and write to the residual stream. The \u003ca href=\"https://arxiv.org/abs/1606.08415\"\u003eSiLU\u003c/a\u003e-based \u003ca href=\"#nonlinear-layer\"\u003enonlinear layer\u003c/a\u003e is especially similar to a transformer’s.\u003c/p\u003e\n\n  \u003cp\u003eOur equivariance structures are inspired by \u003ca href=\"https://arxiv.org/abs/1703.06114\"\u003epermutation-invariant neural networks\u003c/a\u003e, which are a type of \u003ca href=\"https://arxiv.org/abs/1602.07576\"\u003eequivariant neural network\u003c/a\u003e. Equivariance transformations are taken from common augmentations to ARC-AGI puzzles.\u003c/p\u003e\n\n  \n\n  \u003chr/\u003e\n  \n\n  \u003ch2 id=\"appendix\"\u003eAppendix\u003c/h2\u003e\n\n  \u003ch3 id=\"layers-in-the-architecture\"\u003eLayers in the Architecture\u003c/h3\u003e\n\n  \u003ch4 id=\"decoding-layer\"\u003eDecoding Layer\u003c/h4\u003e\n\n  \u003cp\u003eThis layer’s job is to sample a multitensor $z$ and bound its information content, before it is passed to the next layer. This layer and outputs the KL divergence between the learned $z$ distribution and $N(0,I)$. Penalizing the KL prevents CompressARC from learning a distribution for $z$ that memorizes the ARC-AGI puzzle in an uncompressed fashion, and forces it to represent the puzzle more succinctly. Specifically, it forces CompressARC to spend more bits on the KL whenever it uses $z$ to break a symmetry, and the larger the symmetry group broken, the more bits it spends.\u003c/p\u003e\n\n  \u003cp\u003eThis layer takes as input:\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003eA learned target multiscalar, called the “target capacity”.\u003csup id=\"fnref:9\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:9\" rel=\"footnote\"\u003e2\u003c/a\u003e\u003c/sup\u003e The decoding layer will output $z$ whose information content per tensor is close to the target capacity,\u003csup id=\"fnref:10\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:10\" rel=\"footnote\"\u003e3\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n    \u003cli\u003elearned per-element means for $z$,\u003csup id=\"fnref:11\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:11\" rel=\"footnote\"\u003e4\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n    \u003cli\u003elearned per-element capacity adjustments for $z$.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eWe begin by normalizing the learned per-element means for $z$.\u003csup id=\"fnref:12\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:12\" rel=\"footnote\"\u003e5\u003c/a\u003e\u003c/sup\u003e Then, we figure out how much Gaussian noise we must add into every tensor to make the \u003ca href=\"https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem\"\u003eAWGN channel capacity\u003c/a\u003e equal to the target capacity for every tensor (including per-element capacity adjustments). We apply the noise to sample $z$, keeping unit variance of $z$ by rescaling.\u003csup id=\"fnref:13\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:13\" rel=\"footnote\"\u003e6\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003eWe compute the information content of $z$ as the KL divergence between the distribution of this sample and $N(0,1)$.\u003c/p\u003e\n\n  \u003cp\u003eFinally, we postprocess the noisy $z$ by scaling it by the sigmoid of the signal-to-noise ratio.\u003csup id=\"fnref:14\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:14\" rel=\"footnote\"\u003e7\u003c/a\u003e\u003c/sup\u003e This ensures that $z$ is kept as-is when its variance consists mostly of useful information and it is nearly zero when its variance consists mostly of noise. All this is done 4 times to make a $channel$ dimension of 4. Then we apply a projection (with different weights per tensor in the multitensor, ie. per-tensor projections) mapping the $channel$ dimension up to the dimension of the residual stream.\u003c/p\u003e\n\n  \u003cp\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Multitensor_Sharing.png\" width=\"50%\"/\u003e\u003c/p\u003e\n\n  \u003ch4 id=\"multitensor-communication-layer\"\u003eMultitensor Communication Layer\u003c/h4\u003e\n\n  \u003cp\u003eThis layer allows different tensors in a multitensor to interact with each other.\u003c/p\u003e\n\n  \u003cp\u003eFirst, the input from the residual stream passes through per-tensor projections to a fixed size (8 for downwards communication and 16 for upwards communication). Then a message is sent to every other tensor that has at least the same dimensions for upwards communication, or at most the same dimensions for downwards communication. This message is created by either taking means along dimensions to remove them, or unsqueezing+broadcasting dimensions to add them. All the messages received by every tensor are summed together and normalization is applied. This result gets up-projected back and then added to the residual stream.\u003c/p\u003e\n\n  \u003ch4 id=\"softmax-layer\"\u003eSoftmax Layer\u003c/h4\u003e\n\n  \u003cp\u003eThis layer allows the network to work with internal one-hot representations, by giving it the tools to denoise and sharpen noisy one-hot vectors. For every tensor in the input multitensor, this layer lists out all the possible subsets of dimensions of the tensor to take a softmax over\u003csup id=\"fnref:15\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:15\" rel=\"footnote\"\u003e8\u003c/a\u003e\u003c/sup\u003e, takes the softmax over these subsets of dimensions, and concatenates all the softmaxxed results together in the $channel$ dimension. The output dimension varies across different tensors in the multitensor, depending on their tensor rank. A pre-norm is applied, and per-tensor projections map to and from the residual stream. The layer has input $channel$ dimension of 2.\u003c/p\u003e\n\n  \u003ch4 id=\"directional-cummaxshift-layer\"\u003eDirectional Cummax/Shift Layer\u003c/h4\u003e\n\n  \u003cp\u003eThe directional cummax and shift layers allow the network to perform the non-equivariant cummax and shift operations in an equivariant way, namely by applying the operations once per direction, and only letting the output be influenced by the results once the directions are aggregated back together (by the \u003ca href=\"#multitensor-communication-layer\"\u003emultitensor communication layer\u003c/a\u003e). These layers are the sole reason we included the $direction$ dimension when defining a multitensor: to store the results of directional layers and operate on each individually. Of course, this means when we apply a spatial equivariance transformation, we must also permute the indices of the $direction$ dimension accordingly, which can get complicated sometimes.\u003c/p\u003e\n\n  \u003cp\u003eThe directional cummax layer takes the eight indices of the $direction$ dimension, treats each slice as corresponding to one direction (4 cardinal, 4 diagonal), performs a cumulative max in the respective direction for each slice, does it in the opposite direction for half the channels, and stacks the slices back together in the $direction$ dimension. The slices are rescaled to have min $-1$ and max $1$ before applying the cumulative max.\u003c/p\u003e\n\n  \u003cp\u003eThe directional shift layer does the same thing, but for shifting the grid by one pixel instead of applying the cumulative max, and without the rescaling.\u003c/p\u003e\n\n  \u003cp\u003eSome details:\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003ePer-tensor projections map to and from the residual stream, with pre-norm.\u003c/li\u003e\n    \u003cli\u003eInput $channel$ dimension is 4\u003c/li\u003e\n    \u003cli\u003eThese layers are only applied to the $[example, color, direction, height, width, channel]$ and $[example, direction, height, width, channel]$ tensors in the input multitensor.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Directional_Shift_Cummax.png\" alt=\"image\"/\u003e\u003c/p\u003e\n\n  \u003ch4 id=\"directional-communication-layer\"\u003eDirectional Communication Layer\u003c/h4\u003e\n\n  \u003cp\u003eBy default, the network is equivariant to permutations of the eight directions, but we only want symmetry up to rotations and flips. So, this layer provides a way to send information between two slices in the $direction$ dimension, depending on the angular difference in the two directions. This layer defines a separate linear map to be used for each of the 64 possible combinations of angles, but the weights of the linear maps are minimally tied such that the directional communication layer is equivariant to reflections and rotations. This gets complicated really fast, since the $direction$ dimension’s indices also permute when equivariance transformations are applied. Every direction slice in a tensor accumulates it’s 8 messages, and adds the results together.\u003csup id=\"fnref:16\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:16\" rel=\"footnote\"\u003e9\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003eFor this layer, there are per-tensor projections to and from the residual stream with pre-norm. The input $channel$ dimension is 2.\u003c/p\u003e\n\n  \u003ch4 id=\"nonlinear-layer\"\u003eNonlinear Layer\u003c/h4\u003e\n\n  \u003cp\u003eWe use a SiLU nonlinearity with $channel$ dimension 16, surrounded by per-tensor projections with pre-norm.\u003c/p\u003e\n\n  \u003ch4 id=\"normalization-layer\"\u003eNormalization Layer\u003c/h4\u003e\n\n  \u003cp\u003eWe normalize all the tensors in the multitensor, using means and variances computed across all dimensions except the $channel$ dimension. Normalization as used within other layers also generally operates this way.\u003c/p\u003e\n\n  \u003ch4 id=\"linear-heads\"\u003eLinear Heads\u003c/h4\u003e\n\n  \u003cp\u003eWe must take the final multitensor, and convert it to the format of an ARC-AGI puzzle. More specifically, we must convert the multitensor into a distribution over ARC-AGI puzzles, so that we can compute the log-likelihood of the observed grids in the puzzle.\u003c/p\u003e\n\n  \u003cp\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Linear_heads.png\" width=\"50%\"/\u003e\u003c/p\u003e\n\n  \u003cp\u003eThe colors of every pixel for every example for both input and output, have logits defined by the $[examples, colors, height, width, channel]$ tensor, with the $channel$ dimension linearly mapped down to a size of 2, representing the input and output grids.\u003csup id=\"fnref:17\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:17\" rel=\"footnote\"\u003e10\u003c/a\u003e\u003c/sup\u003e The log-likelihood is given by the crossentropy, with sum reduction across all the grids.\u003c/p\u003e\n\n  \u003cp\u003eFor grids of non-constant shape, the $[examples, width, channel]$ and $[examples, height, channel]$ tensors are used to create distributions over possible contiguous rectangular slices of each grid of colors. Again, the $channel$ dimension is mapped down to a size of 2 for input and output grids. For every grid, we have a vector of size $[width]$ and a vector of size $[height]$. The log likelihood of every slice of the vector is taken to be the sum of the values within the slice, minus the values outside the slice. The log likelihoods for all the possible slices are then normalized to have total probability one, and the colors for every slice are given by the color logits defined in the previous paragraph.\u003c/p\u003e\n\n  \u003cp\u003eWith the puzzle distribution now defined, we can now evaluate the log-likelihood of the observed target puzzle, to use as the reconstruction error.\u003csup id=\"fnref:18\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:18\" rel=\"footnote\"\u003e11\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003ch3 id=\"other-architectural-details\"\u003eOther Architectural Details\u003c/h3\u003e\n\n  \u003ch4 id=\"rules-for-legal-multitensors\"\u003eRules for legal multitensors\u003c/h4\u003e\n\n  \u003col\u003e\n    \u003cli\u003eAt least one non-$example$ dimension must be included. Examples are not special for any reason not having to do with colors, directions, rows, and columns.\u003c/li\u003e\n    \u003cli\u003eIf the $width$ or $height$ dimension is included, the $example$ dimension should also be included. Positions are intrinsic to grids, which are indexed by the $example$ dimension. Without a grid it doesn’t make as much sense to talk about positions.\u003c/li\u003e\n  \u003c/ol\u003e\n\n  \u003ch4 id=\"weight-tying-for-reflectionrotation-symmetry\"\u003eWeight Tying for Reflection/Rotation Symmetry\u003c/h4\u003e\n\n  \u003cp\u003eWhen applying a different linear layer to every tensor in a multitensor, we have a linear layer for tensors having a $width$ but not $height$ dimension, and another linear layer for tensors having a $height$ but not $width$ dimension. Whenever this is the case, we tie the weights together in order to preserve the whole network’s equivariance to diagonal reflections and 90 degree rotations, which swap the $width$ and $height$ dimensions.\u003c/p\u003e\n\n  \u003cp\u003eThe softmax layer is not completely symmetrized because different indices of the output correspond to different combinations of dimension to softmax over. Tying the weights properly would be a bit complicated and time consuming for the performance improvement we expect, so we did not do this.\u003c/p\u003e\n\n  \u003ch4 id=\"training\"\u003eTraining\u003c/h4\u003e\n\n  \u003cp\u003eWe train for 2000 iterations using Adam, with learning rate 0.01, $\\beta_1$ of 0.5, and $\\beta_2$ of 0.9.\u003c/p\u003e\n\n  \u003ch3 id=\"preprocessing\"\u003ePreprocessing\u003c/h3\u003e\n\n  \u003ch4 id=\"output-shape-determination\"\u003eOutput Shape Determination\u003c/h4\u003e\n\n  \u003cp\u003eThe raw data consists of grids of various shapes, while the neural network operates on grids of constant shape. Most of the preprocessing that we do is aimed towards this shape inconsistency problem.\u003c/p\u003e\n\n  \u003cp\u003eBefore doing any training, we determine whether the given ARC-AGI puzzle follows three possible shape consistency rules:\u003c/p\u003e\n  \u003col\u003e\n    \u003cli\u003eThe outputs in a given ARC-AGI puzzle are always the same shape as corresponding inputs.\u003c/li\u003e\n    \u003cli\u003eAll the inputs in the given ARC-AGI puzzle are the same shape.\u003c/li\u003e\n    \u003cli\u003eAll the outputs in the given ARC-AGI puzzle are the same shape.\u003c/li\u003e\n  \u003c/ol\u003e\n\n  \u003cp\u003eBased on rules 1 and 3, we try to predict the shape of held-out outputs, prioritizing rule 1 over rule 3. If either rule holds, we force the postprocessing step to only consider the predicted shape by overwriting the masks produced by the \u003ca href=\"#linear-heads\"\u003elinear heads\u003c/a\u003e. If neither rule holds, we make a temporary prediction of the largest width and height out of the grids in the given ARC-AGI puzzle, and we allow the masks to predict shapes that are smaller than that.\u003c/p\u003e\n\n  \u003cp\u003eThe largest width and height that is given or predicted, are used as the size of the \u003ca href=\"#multitensors\"\u003emultitensor\u003c/a\u003e’s $width$ and $height$ dimensions.\u003c/p\u003e\n\n  \u003cp\u003eThe predicted shapes are also used as masks when performing the \u003ca href=\"#multitensor-communication-layer\"\u003emultitensor communication\u003c/a\u003e, \u003ca href=\"#directional-communication-layer\"\u003edirectional communication\u003c/a\u003e and \u003ca href=\"#directional-cummaxshift-layer\"\u003edirectional cummax/shift\u003c/a\u003e layers\u003csup id=\"fnref:19\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:19\" rel=\"footnote\"\u003e12\u003c/a\u003e\u003c/sup\u003e. We did not apply masks for the other layers because of time constraints and because we do not believe it will provide for much of a performance improvement.\u003c/p\u003e\n\n  \u003ch4 id=\"number-of-colors\"\u003eNumber of Colors\u003c/h4\u003e\n\n  \u003cp\u003eWe notice that in almost all ARC-AGI puzzles, colors that are not present in the puzzle are not present in the true answers. Hence, any colors that do not appear in the puzzle are not given an index in the $color$ dimension of the \u003ca href=\"#multitensors\"\u003emultitensor\u003c/a\u003e.\u003c/p\u003e\n\n  \u003cp\u003eIn addition, black is treated as a special color that is never included in the multitensor, since it normally represents the background in many puzzles. When performing color classification, a tensor of zeros is appended to the $color$ dimension after applying the \u003ca href=\"#linear-heads\"\u003elinear head\u003c/a\u003e, to represent logits for the black color.\u003c/p\u003e\n\n  \u003ch3 id=\"postprocessing\"\u003ePostprocessing\u003c/h3\u003e\n\n  \u003cp\u003ePostprocessing primarily deals with denoising the answers sampled from the network. There are also \u003ca href=\"#linear-heads\"\u003esome operations\u003c/a\u003e performed to convert the constant-shape grids outputted by the network to the variable shape grids present in some puzzles.\u003c/p\u003e\n\n  \u003cp\u003eGenerally, when we sample answers from the network by taking the logits of the $[examples, colors, height, width, channels]$ tensor and argmaxxing over the $color$ dimension, we find that the grids are noisy and will often have the wrong colors for several random pixels. We developed several methods for removing this noise:\u003c/p\u003e\n  \u003col\u003e\n    \u003cli\u003eFind the most commonly sampled answer.\u003c/li\u003e\n    \u003cli\u003eConstruct an exponential moving average of the output color logits before taking the softmax to produce probabilities. Also construct an exponential moving average of the masks.\u003c/li\u003e\n    \u003cli\u003eConstruct an exponential moving average of the output color probabilities after taking the softmax. Also construct an exponential moving average of the masks.\u003c/li\u003e\n  \u003c/ol\u003e\n\n  \u003cp\u003eWhen applying these techniques, we always take the slice of highest probability given the mask, and then we take the colors of highest probability afterwards.\u003c/p\u003e\n\n  \u003cp\u003eWe explored several different rules for when to select which method, and arrived at a combination of 1 and 2 with a few modifications:\u003c/p\u003e\n  \u003cul\u003e\n    \u003cli\u003eAt every iteration, count up the sampled answer, as well as the exponential moving average answer (decay $=0.97$).\u003c/li\u003e\n    \u003cli\u003eIf before 150 iterations of training, then downweight the answer by a factor of $e^{-10}$. (Effectively, don’t count the answer.)\u003c/li\u003e\n    \u003cli\u003eIf the answer is from the exponential moving average as opposed to the sample, then downweight the answer by a factor of $e^{-4}$.\u003c/li\u003e\n    \u003cli\u003eDownweight the answer by a factor of $e^{-10*uncertainty}$, where $uncertainty$ is the average (across pixels) negative log probability assigned to the top color of every pixel.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003ch3 id=\"what-happens-to-the-representations-during-learning\"\u003eWhat Happens to the Representations during Learning\u003c/h3\u003e\n\n  \u003cp\u003eDuring training, the gradient descent tries to find representations of the puzzle that require less and less information to encode. This information is measured by the KL term for $z$, plus the a heavily penalized reconstruction error.\u003c/p\u003e\n\n  \u003cp\u003eDue to the 10x penalization on reconstruction error, and the initial high capacity for $z$, the $z$ distribution (which we call the “posterior”) quickly learns the information that is required to perfectly reconstruct the given input/output pairs in the puzzle, within the first 20 or so steps. The remainder of the training steps are about compressing $z$ information under the constraint of perfect reconstruction, by tuning the representations to be more concise.\u003c/p\u003e\n\n  \u003cp\u003eOur mental model of how gradient descent compresses the $z$ information consists of several steps which we list below:\u003c/p\u003e\n  \u003col\u003e\n    \u003cli\u003eSuppose the posterior $p$ originally codes for some number $n$ pieces of information $z_1, \\dots, z_n$ using thin Gaussians.\u003c/li\u003e\n    \u003cli\u003eThe posterior widens and becomes more noisy to try to get closer to the wide Gaussian “prior” $q=N(0,1)$, but since all $n$ pieces of information are needed to ensure good reconstruction, the noise is limited by the reconstruction loss incurred.\u003c/li\u003e\n    \u003cli\u003eThe ever-widening posteriors push the neurons to become more and more resilient to noise, until some limit is reached.\u003c/li\u003e\n    \u003cli\u003eLearning remains stagnant for a while, as a stalemate between compression and reconstruction.\u003c/li\u003e\n    \u003cli\u003eIf it turns out that $z_1$ is not reconstructible using $z_2, \\dots, z_n$, then stop. Else, proceed to step 6.\u003c/li\u003e\n    \u003cli\u003eThe neurons, pushed by the widening posterior of $z_1$, figure out a procedure to denoise $z_1$ using information from $z_2, \\dots, z_n$, in the event that the noise sample for $z_1$ is too extreme.\u003c/li\u003e\n    \u003cli\u003eThe posterior for the last piece keeps pushing wider, producing more extreme values for $z_1$, and the denoising procedure is improved, until the $z_1$ representation consists completely of noise, and its usage in the network is replaced by the output of the denoising procedure.\u003c/li\u003e\n    \u003cli\u003eThe posterior for $z_1$ is now identical to the prior, so nothing is coded in $z_1$ and it no longer contributes to the KL loss.\u003c/li\u003e\n    \u003cli\u003eThe posterior now codes for $n-1$ pieces of information $z_2, \\dots, z_n$, and compression has occurred.\u003c/li\u003e\n  \u003c/ol\u003e\n\n  \u003cp\u003eThese steps happen repeatedly for different unnecessarily coded pieces of information, until there are no more. More than one piece of information can be compressed away at once, and there is no need for the steps to proceed serially. The process stops when all information coded by the posterior is unique, and no piece is reconstructable using the others.\u003c/p\u003e\n\n  \n\n  \u003ch2 id=\"additional-case-studies\"\u003eAdditional Case Studies\u003c/h2\u003e\n\n  \u003cp\u003eBelow, we show two additional puzzles and a dissection of CompressARC’s solution to them.\u003c/p\u003e\n\n  \u003ch3 id=\"case-study-bounding-box\"\u003eCase Study: Bounding Box\u003c/h3\u003e\n\n  \u003cp\u003ePuzzle 6d75e8bb is part of the training split.\u003c/p\u003e\n\n  \u003cp\u003e\n  \u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_problem.png\" width=\"20%\"/\u003e\n\u003c/p\u003e\n\n  \u003ch4 id=\"watching-the-network-learn-bounding-box\"\u003eWatching the Network Learn: Bounding Box\u003c/h4\u003e\n\n  \u003ch5 id=\"human-solution-1\"\u003eHuman Solution:\u003c/h5\u003e\n  \u003cp\u003eWe first realize that the input is red and black, and the output is also red and black, but some of the black pixels are replaced by light blue pixels. We see that the red shape remains unaffected. We notice that the light blue box surrounds the red shape, and finally that it is the smallest possible surrounding box that contains the red shape. At this point, we copy the input over to the answer grid, then we figure out the horizontal and vertical extent of the red shape, and color all of the non-red pixels within that extent as light blue.\u003c/p\u003e\n\n  \u003ch5 id=\"compressarc-solution-1\"\u003eCompressARC Solution:\u003c/h5\u003e\n  \u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e 50 steps of learning:\u003c/strong\u003e\n  \u003cp\u003e\n  The average of sampled outputs shows that light blue pixels in the input are generally preserved in the output. However, black pixels in the input are haphazardly and randomly colored light blue and red. CompressARC does not seem to know that the colored input/output pixels lie within some kind of bounding box, or that the bounding box is the same for the input and output grids.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_50_steps.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e 100 steps of learning:\u003c/strong\u003e\n  \u003cp\u003e\n  The average of sampled outputs shows red pixels confined to an imaginary rectangle surrounding the light blue pixels. CompressARC seems to have perceived that other examples use a common bounding box for the input and output pixels, but is not completely sure about where the boundary lies and what colors go inside the box in the output. Nevertheless, guess 2 (the second most frequently sampled output) shows that the correct answer is already being sampled quite often now.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_100_steps.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e 150 steps of learning:\u003c/strong\u003e\n  \u003cp\u003e\n  The average of sampled outputs shows almost all of the pixels in the imaginary bounding box to be colored red. CompressARC has figured out the answer, and further training only refines the answer.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_150_steps.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n  \u003ch4 id=\"solution-analysis-bounding-box\"\u003eSolution Analysis: Bounding Box\u003c/h4\u003e\n\n  \u003cp\u003eBelow is a plot of the amount of contained information in every tensor composing the latent $z$:\u003c/p\u003e\n\n  \u003cp\u003e\n  \u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_KL_components.png\"/\u003e\n\u003c/p\u003e\n\n  \u003cp\u003eAll the tensors in $z$ fall to zero information content during training, except for three tensors. From 600-1000 steps, we see the $(example, height, width, channel)$ tensor suffer a massive drop in information content, with no change in the outputted answer. We believe it was being used to identify the light blue pixels in the input, but this information then got memorized by the nonlinear portions of the network, using the $(example, height, channel)$ and $(example, width, channel)$ as positional encodings.\u003c/p\u003e\n\n  \u003cp\u003eWe can look at the average output of the \u003ca href=\"#decoding-layer\"\u003edecoding layer\u003c/a\u003e for these tensors to see what information is stored there.\u003c/p\u003e\n\n  \u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e (Examples, height, channel) tensor:\u003c/strong\u003e\n  \u003cp\u003e\n  The first principal component is 771 times stronger than the second principal component. \u003cstrong\u003eA brighter pixel indicates a row with more light blue pixels.\u003c/strong\u003e It is unclear how CompressARC knows where the borders of the bounding box are.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_example_height_component_0.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e (Examples, width, channel) tensor:\u003c/strong\u003e\n  \u003cp\u003e\n  The first principal component is 550 times stronger than the second principal component. \u003cstrong\u003eA darker pixel indicates a column with more light blue pixels.\u003c/strong\u003e It is unclear how CompressARC knows where the borders of the bounding box are.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_example_width_component_0.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e (Color, channel) tensor:\u003c/strong\u003e\n  \u003cp\u003e\n  This tensor serves to distinguish the roles of the two colors apart.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_color_component_0.png\" width=\"50%\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n  \n\n  \u003ch3 id=\"case-study-center-cross\"\u003eCase Study: Center Cross\u003c/h3\u003e\n\n  \u003ctable\u003e\n    \u003cthead\u003e\n      \u003ctr\u003e\n        \u003cth\u003ePuzzle 41e4d17e from training split\u003c/th\u003e\n        \u003cth\u003eOur Network’s Answer\u003c/th\u003e\n      \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_at_1500_steps.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n    \u003c/tbody\u003e\n  \u003c/table\u003e\n\n  \u003ch5 id=\"human-solution-2\"\u003eHuman Solution:\u003c/h5\u003e\n  \u003cp\u003eWe first notice that the input consists of blue “bubble” shapes (really they are just squares, but the fact that they’re blue reminds us of bubbles) on a light blue background and the output has the same. But in the output, there are now magenta rays emanating from the center of each bubble. We copy the input over to the answer grid, and then draw magenta rays starting from the center of each bubble out to the edge in every cardinal direction. At this point, we submit our answer and find that it is wrong, and we notice that in the given demonstrations, the blue bubble color is drawn on top of the magenta rays, and we have drawn the rays on top of the bubbles instead. So, we pick up the blue color and correct each point where a ray pierces a bubble, back to blue.\u003c/p\u003e\n\n  \u003ch5 id=\"compressarc-solution-2\"\u003eCompressARC Solution:\u003c/h5\u003e\n  \u003cp\u003eWe don’t show CompressARC’s solution evolving over time because we think it is uninteresting; instead will describe. We don’t see much change in CompressARC’s answer over time during learning. It starts by copying over the input grid, and at some point, magenta rows and columns start to appear, and they slowly settle on the correct positions. At no point does CompressARC mistakenly draw the rays on top of the bubbles; it has always had the order correct.\u003c/p\u003e\n\n  \u003ch4 id=\"solution-analysis-center-cross\"\u003eSolution Analysis: Center Cross\u003c/h4\u003e\n\n  \u003cp\u003eAnother plot of the amount of information in every tensor in $z$:\u003c/p\u003e\n\n  \u003cp\u003e\n  \u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_KL_components.png\"/\u003e\n\u003c/p\u003e\n\n  \u003cp\u003eThe only surviving tensors are the $(color, channel)$ and $(example, height, width, channel)$ tensors.\u003c/p\u003e\n\n  \u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e (Examples, height, width, channel) tensor:\u003c/strong\u003e\n  \u003cp\u003e\n  The top principal component is 2496 times stronger than the second principal component. \u003cstrong\u003eThe (examples, height, width, channel) tensor codes for the centers of the bubbles.\u003c/strong\u003e In the KL contribution plot, we can see that the information content of this tensor is decreasing over time. Likely, CompressARC is in the process of eliminating the plus shaped representation, and replacing it with a pixel instead, which takes fewer bits.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_example_height_width_component_0.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e\n  \u003cstrong\u003e (Color, channel) tensor:\u003c/strong\u003e\n  \u003cp\u003e\n  The $(color, channel)$ tensor just serves to distinguish the individual roles of the colors in the puzzle.\n  \u003c/p\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_color_component_0.png\"/\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_color_component_1.png\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n  \n\n  \u003ch2 id=\"list-of-mentioned-arc-agi-puzzles\"\u003eList of Mentioned ARC-AGI Puzzles\u003c/h2\u003e\n\n  \u003cp\u003eAll the puzzles we mentioned are part of the training split.\u003c/p\u003e\n\n  \u003ctable\u003e\n    \u003cthead\u003e\n      \u003ctr\u003e\n        \u003cth\u003eName\u003c/th\u003e\n        \u003cth\u003ePuzzle\u003c/th\u003e\n        \u003cth\u003eName\u003c/th\u003e\n        \u003cth\u003ePuzzle\u003c/th\u003e\n      \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e025d127b\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/025d127b_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e0a938d79\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0a938d79_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e0ca9ddb6\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0ca9ddb6_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e0d3d703e\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0d3d703e_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e0dfd9992\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0dfd9992_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e0e206a2e\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0e206a2e_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e1c786137\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/1c786137_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e1f876c06\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/1f876c06_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e28e73c20\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e272f95fa\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e2bcee788\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2bcee788_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e2dd70a9a\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2dd70a9a_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e3bd67248\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/3bd67248_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e41e4d17e\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e42a50994\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/42a50994_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e5ad4f10b\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/5ad4f10b_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003e6d75e8bb\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e7b6016b9\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/7b6016b9_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n        \u003ctd\u003ece9e57f2\u003c/td\u003e\n        \u003ctd\u003e\u003cimg src=\"https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/ce9e57f2_problem.png\" alt=\"image\"/\u003e\u003c/td\u003e\n        \u003ctd\u003e \u003c/td\u003e\n        \u003ctd\u003e \u003c/td\u003e\n      \u003c/tr\u003e\n    \u003c/tbody\u003e\n  \u003c/table\u003e\n\n  \n\n  \u003ch2 id=\"code\"\u003eCode\u003c/h2\u003e\n\n  \u003cp\u003eCode for this project is available \u003ca href=\"https://github.com/iliao2345/CompressARC\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n  \u003cp\u003eIf you’d like to cite this blog post, use the following entry:\u003c/p\u003e\n  \u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e@online{liao2025arcagiwithoutpretraining,\n\tauthor = {Isaac Liao and Albert Gu},\n\ttitle = {ARC-AGI Without Pretraining},\n\tyear = {2025},\n\turl = {https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html},\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\n  \n\n  \n\n  \n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "65 min read",
  "publishedTime": "2025-03-04T00:00:00Z",
  "modifiedTime": null
}
