{
  "id": "39af31a5-07e3-4e38-8a05-3afd39452796",
  "title": "The ITTAGE indirect branch predictor",
  "link": "https://blog.nelhage.com/post/ittage-branch-predictor/",
  "description": "Article URL: https://blog.nelhage.com/post/ittage-branch-predictor/ Comments URL: https://news.ycombinator.com/item?id=44468999 Points: 10 # Comments: 2",
  "author": "Bogdanp",
  "published": "Fri, 04 Jul 2025 23:57:21 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "Nelson Elhage",
  "length": 22899,
  "excerpt": "Modern CPUs are actually pretty good at predicting the indirect branch inside an interpreter loop, _contra_ the conventional wisdom. We take a deep dive into the ITTAGE indirect branch prediction algorithm, which is capable of making those predictions, and draw some connections to some other interests of mine in the areas of fuzzing and reinforcement learning.",
  "siteName": "Made of Bugs",
  "favicon": "https://blog.nelhage.com/favicon.png",
  "text": "While investigating the performance of the new Python 3.14 tail-calling interpreter, I learned (via this very informative comment from Sam Gross) new (to me) piece of performance trivia: Modern CPUs mostly no longer struggle to predict the bytecode-dispatch indirect jump inside a “conventional” bytecode interpreter loop. In steady-state, assuming the bytecode itself is reasonable stable, modern CPUs achieve very high accuracy predicting the dispatch, even for “vanilla” while / switch-style interpreter loops1! Intrigued, I spent a bit of time reading about just how branch predictors achieve this feat. I found the answer pretty fascinating, so I’m going to try to share the salient high-level features – as I understand them – as well as some interesting connections and ideas that came up in response. A quick caveat: I am not a hardware engineer or CPU designer, and I’m mostly going to be focused on some high-level ideas I find interesting. I’ll probably get some things wrong. If you want an introduction to CPU branch prediction by someone who actually knows what they’re talking about, I’ll refer you to Dan Luu’s piece on the topic. The TAGE and ITTAGE branch predictors  🔗︎ In general, modern state-of-the-art CPUs don’t seem to document too much about their branch predictors, so we don’t know – or at least, I couldn’t easily discover – what branch prediction looks like in real cutting-edge CPUs. However, there is (at least) one published algorithm that is both practical and can predict bytecode interpreter loops – the ITTAGE indirect branch predictor – and that’s what I’m talking about. The author of that predictor wrote a paper exploring prediction on bytecode interpreters and found that his ITTAGE exhibited similar performance to Intel Haswell CPUs and suggested they use a variation of it, but I don’t think we know for sure. ITTAGE is a variant of the TAGE predictor; TAGE predicts taken/not-taken behavior for conditional branches, while ITTAGE predicts the destination of indirect jumps. Both predictors have a very similar structure, so I will mostly lump them together for most of this post. A brief preview  🔗︎ Before I dive in, I’ll give you a brief summary of where we’ll be going. Both TAGE and ITTAGE: Predict branch behavior by mapping from (PC, PC history) -\u003e past behavior, and hoping that the future resembles the past. Stores many such tables, using a geometrically-increasing series of history lengths Attempt to dynamically choose the correct table (history length) for each branch. Do so by adaptively moving to a longer history on error, and using a careful replacement policy to preferentially keep useful entries. Now, onto the longer version! Or, if that’s enough for you, skip ahead to some reflections and some of the connections that make this topic particularly interesting to me. Dynamic branch prediction 101  🔗︎ Many dynamic branch prediction algorithms work off of a simple premise: Keep track of a table of historical data in some form, and, when asked to predict a branch, look up what happened “last time” and assume that history will repeat itself. Thinking in C++-flavored pseudocode, I tend to mentally model this approach as something like: struct BranchDetails { // The information we use to identify a branch }; struct BranchHistory { // The information we store about eaach branch // Predict the outcome of a branch based on past state bool predict() const { /* ... */ }; // Update our state based on a resolved branch void update(bool taken) { /* ... */ }; }; // We store a mapping from one to the other. This is a fixed-size chunk of // hardware, so it stores a fixed number of entries. We'll talk a bit about // replacement strategy and some details later on. using PredictorState = FixedSizeMap\u003cBranchDetails, BranchHistory\u003e; void on_resolve_branch(PredictorState \u0026pred, BranchDetails \u0026branch, bool taken) { pred[branch].update(taken); } bool predict_branch(PredictorState \u0026pred, BranchDetails \u0026branch) { return pred[branch].predict(); } So, what do we use for the BranchDetails and BranchHistory? Perhaps the simplest option – used by some early CPUs! – is to just use the branch address to identify the branch – essentially, track state for each branch instruction in the program text – and to just use one bit of information for the history: struct BranchDetails { uintptr_t addr; }; struct BranchHistory { bool taken_; bool predict() { return taken_; } void update(bool taken) { taken_ = taken; } }; The next-simplest strategy replaces our bool per-branch state with a small counter (as few as 2 bits!), to give us a measure of hysteresis. We increment or decrement the counter on branch resolution, and use the sign for our prediction. It turns out that most branches are heavily “biased” one way or another – e.g. a “branch taken” rate of 10% or 90% is more common than 50% – and a small amount of hysteresis can let us absorb occasional outlier behaviors without forgetting everything we know: struct BranchHistory { int2_t counter_; bool predict() { return counter_ \u003e= 0; } void update(bool taken) { saturating_increment(\u0026counter_, taken ? 1 : -1); } }; Moving beyond just PC  🔗︎ Indexing branches by PC is simple and efficient, but limiting. Many branches exhibit dynamic data-dependent behavior and we need to somehow distinguish with more granularity, if we want better accuracy. Because branch predictors live in the CPU frontend, and have to make predictions well before an instruction is actually executed – or potentially even fully-decoded – and so they don’t have much access to other CPU state to use in their predictions. However, there is one type of context they can access practically “for free”: the history of the program counter and recent branches, since the predictor had to help generate those, to start with! Thus, a branch predictor can maintain a circular buffer of some fixed size, store a rolling “branch history” or “PC history” in some form, and use that state to distinguish different branches. In the simplest case, we might store one bit per previous branch, writing a “1” for branch-taken, or “0” for not-taken. In more sophisticated predictors, we might include unconditional, and/or write a few bits of the PC value into the history: constexpr int history_length = ...; struct BranchDetails { uintptr_t pc; bitarray\u003chistory_length\u003e history; } Sizing our branch history  🔗︎ How large of a history should we use? Is larger always better? Longer histories allow us to learn more total patterns, and more complex patterns. For a program with stable behavior, in steady state, a larger history will potentially allow us to learn more of the program’s behavior, and distinguish between different situations more finely. However, longer history means we need more space in our table to learn simple patterns, and potentially more time, since we have more possible states, and we need to encounter each one separately to learn it. Imagine a simple function that looks like this: bool logging_active; void log(const char *msg) { if (logging_active) { printf(\"%s\\n\", msg); } } Suppose this function isn’t inlined, and is called from many different places in an executable. Assuming logging_active is static or mostly-static at runtime, this branch is highly predictable. A simple PC-only predictor should achieve near-perfect accuracy. However, if we also consider branch history, the predictor no longer “sees” this branch as a single entity; instead it needs to track every path that arrives at this branch instruction, separately. In the worst case, if we store k bits of history, we may need to use 2^k different table entries for this one branch! Worse, we need to encounter each state individually, and we don’t “learn anything” about different paths, from each other. The TAGE algorithm: big ideas  🔗︎ We now have the necessary background to sketch a description of the TAGE predictor. TAGE keeps track of branch history, as I’ve described, but unlike simpler predictors, it tracks hundreds or even thousands of bits of history, allowing it to potentially learn patterns over very long distances. In order to make use of all this history without unwanted state explosion, TAGE stores multiple history tables (perhaps on the order of 10-20), indexed by a geometric series of history lengths (i.e. table N uses history length \\( L_n ≈ L_0\\cdot{}r^n \\) for some ratio \\(r\\)). TAGE then attempts to adaptively choose, for each branch, the shortest history length (and corresponding table) that suffices to make good predictions. How does it do that? Here are the core ideas (as I understand them). I’ll link to some papers and code later on if you want to get into the details! Tag bits in each table  🔗︎ So far, I’ve entirely glossed over the question of how these lookup tables are actually implemented, and in particular concretely how we implement “lookup the entry with a given key.” In many simple branch predictors, history tables “don’t know” anything about which keys they are storing, and just directly index based on some of the bits of the key. For instance, for a predictor indexed by PC, we might just have an array of \\(2^k\\) counters, and use the low \\(k\\) bits of PC to select an entry. If two branches have the same address modulo \\(2^k\\), they will collide and use the same table entry, and we will make no effort to detect the collision or behave differently. This choice makes tables extremely cheap and efficient, and turns out to be a good tradeoff in many cases. Intuitively, we already have to handle the branch predictor being wrong, and such collisions are just another reason they might be wrong; detecting and reacting to collisions would take more hardware and more storage, and it turns out we’re better off using that to lower the error rate in other ways, instead. TAGE, however, stores multiple tables, and needs to use different tables for different branches, which in turn necessitates knowing which tables are actually storing information for a given key, vs a colliding key. Thus, in addition to the other payload, each table entry stores a tag, containing additional bits of metadata describing which key is stored in the entry. Given a (PC, branch history) tuple T, TAGE uses two different hash functions for each table, H_index and H_tag. The branch state T will be stored in the table at index H_index(T), with a tag value of H_tag(T). On lookup, we check the value at H_index(T) and compare the tag to H_tag(T). If the tags disagree, this entry is currently storing information about some other branch, and we won’t use it (but may decide to overwrite it) If the tag agrees, we assume this state matches our branch, and we use and/or update it. Note that we still only check the hashes, and so it’s possible we collided with a different branch in both hashes, but we will design them and choose their sizes so this condition is sufficiently rare in practice. These tag bits give TAGE the “TA” in its name; the “GE” comes from the geometric series of history lengths. Basic prediction algorithm  🔗︎ Given this setup, the basic prediction algorithm of TAGE is fairly simple. Each table entry stores a counter (called ctr in the papers), as described above – it is incremented on “branch taken,” and decremented on “not taken.” To make a prediction, TAGE checks every table, using the appropriate history length. It considers all the table entries which have matching tags, and uses the prediction from the matching entry corresponding to the longest history length. The base predictor – in the simplest case, a table indexed only by PC – does not use tag bits, and thus will always match, and be used as a fallback if no longer history matches. Move to a longer history on prediction error  🔗︎ Once a branch resolves and we know the correct answer, we need to update the predictor. TAGE always updates the ctr field on the entry which was used to make the prediction. However, if the prediction was wrong, it also attempts to allocate a new entry, into one of the tables using a longer history length. The goal, thus, is to dynamically try longer and longer histories until we find a length that works. Track the usefulness of table entries  🔗︎ Since table entries are tagged, we need a way to decide when we will replace a table entry and reuse it for a new branch. For the predictor to work well, we aim to keep entries which are likely to make useful predictions in the future, and discard ones which won’t. In order to approximate that goal, TAGE tracks which entries have been useful recently. In addition to the tag and the counter, each table entry also has a u (“useful”) counter (typically just 1 or 2 bits), which tracks whether the table has recently produced useful predictions. When we allocate new table entries – as described above – we will only ever overwrite a slot with u=0, and we will initialize new slots with u=0; thus, new entries must prove their worth or risk being replaced. The u counter is incremented any time a given table entry: Is used for a prediction, and That prediction turns out to be correct, and The prediction from that entry is different from the prediction given by the matching entry with the next-longest history. Thus, it’s not enough for an entry to yield correct predictions; it also needs to yield a correct prediction that counterfactually would have been wrong. In addition, the u counters are periodically decayed (or just set to zero) in some form, to prevent entries from lingering forever. The precise algorithm here varies a lot between published versions. From TAGE to ITTAGE  🔗︎ I’ve been describing the behavior of TAGE, which predicts one bit of information (taken/not-taken) for conditional branches. ITTAGE, which predicts the target of indirect branches (the whole reason I’m writing about this system!) is virtually identical; the primary changes are only: Each table entry also stores a predicted target address The ctr counter is retained, but becomes a “confidence” counter. It is incremented on “correct prediction” and decremented on “incorrect.” On incorrect prediction, we will update the predicted target to the new value iff ctr is at the minimum value. Thus, ctr tracks our confidence in this particular target address, while u tracks the usefulness of this entire entry in the context of the entire predictor. In fact, the same tables may be combined into a joint predictor, called “COTTAGE” in the paper. References  🔗︎ I haven’t found an enormous amount written about TAGE and ITTAGE, but I will include here the best links I have found, in case you’re curious and want to dig into more details! Reading these papers, it really jumped out at me the extent to which it’s not enough to have the right high-level ideas; a high-performing implementation of TAGE or ITTAGE (or any branch predictor) is the result of both a good design, and a ton of careful tuning and balancing of tradeoffs. Here’s the links: A case for (partially) tagged geometric history length branch prediction As best I can tell, this is the paper that proposed TAGE and ITTAGE The L-TAGE Branch Predictor An implementation of TAGE for a branch prediction competition in 2007 (“CBP-2”). A 64 Kbytes ISL-TAGE branch predictor The description of an updated version for the successor competition, in 2011 (“CBP-3”). A 64-Kbytes ITTAGE indirect branch predictor The description of an ITTAGE predictor for the indirect branch track of the same competition. The program for JWAC2, which hosted the CBP-3 competition In particular, containing links to source code for the TAGE and ITTAGE implementations submitted to that competition (in a microarchitectural simulator). BOOM (Berkeley Out Of Order Machine)’s documentation on their TAGE implementation BOOM is an open-source RISC-V core, intended for microarchitecture research. Why I find ITTAGE interesting  🔗︎ On one hand, I find ITTAGE interesting because I occasionally have cause to think about the performance of interpreter loops or similar software, and it represents an important update to how I need to reason about those situations. Very concretely, it informed my CPython benchmarking from last post. However, I also find it fascinating for some broader reasons, and in connection to some other areas of interest. I’ve written in the past about a class of software tools (including both coverage-guided fuzzers and tracing JITs), which attempt to understand some program’s behavior in large part by looking at the behavior of the program counter over time, and about how those tools struggle – in related ways – on interpreters and similar software for which the program state is “hidden in the data” and where the control flow alone is a poor proxy for the “interesting” state. I didn’t write about this connection in that post, but I’ve always considered branch predictors to be another member of this class of tool. As mentioned above, they also understand program execution mostly through the lens of “a series of program counter values,” and they, too – at least historically – have struggled to behave well on interpreter loops. Thus, learning about ITTAGE and its success predicting interpreter behavior naturally raises the question, for me: Is there anything to be learned from the ITTAGE algorithm for those other tools? In particular, I wonder about… ITTAGE for coverage-guided fuzzing and program state exploration?  🔗︎ As I sketched in that older post, coverage-guided fuzzing is a technique which attempts to automatically explore the behavior of a target program, by generating candidate inputs, and then observing which inputs generate “new” behavior in the program. In order for this loop to work, we need some way of characterizing or bucketing program behavior, so we can decide what counts as “new” or “interesting” behavior, versus behavior we have already observed. I will admit that I am not up-to-date on the current frontiers in this field, but historically this has been done using “coverage”-like metrics, which count the occurrences of either PC values or of branches (which essentially means (PC, PC’) pairs). These counts are potentially bucketed, and we “fingerprint” execution by the list of [(PC, bucketed_count)] values generated during execution. This approach is, in practice, fabulously effective. However, it can struggle on certain shapes of programs – including often interpreters – where the “interesting” state does not map well onto sets of program counters or branches. One of my favorite illustrations of this problem is the IJON paper, which demonstrates some concrete problems, and attacks them using human-added annotations. My questions, then, is thus: Could something like the TAGE/ITTAGE approach help coverage-guided fuzzers to better explore the state space for interpreters, and interpreter-like programs? Could we, for instance, train a TAGE-like predictor on existing corpus entries, and then prioritize candidate mutants based on their rate of prediction errors? Might this allow a fuzzer to (e.g.) effectively explore the state space of code in an interpreted language, only by annotating the interpreter? There are a large number of practical challenges, but in principle this seems like it might allow more-nuanced exploration of state spaces, and discovering “novel behavior” which can only be identified by means of long-range correlations and patterns. I will note that TAGE/ITTAGE specifically are designed and tuned around hardware performance characteristics and tradeoffs; the performance landscape in software is very different, and so if such an idea does work, I suspect the details look fairly different and are optimized for an efficient software implementation, it seems plausible to me there’s a place for borrowing the core idea of “dynamically picking a history length on a per-branch basis.” An even whackier idea might be to use the actual hardware branch predictor. Modern CPUs allow you to observe branch prediction accuracy via hardware performance counters, and we could imagine executing the corpus of existing examples to train the branch predictor, and then observing the actual hardware misprediction count as a novelty signal. This approach also has a ton of challenges, in part because of the opacity of the hardware branch predictor and the inability to explicitly control it; however, it has the virtue of potentially being much, much cheaper than a software predictor. It does make me wonder whether there are any CPUs which expose the branch predictor state explicitly – even in the form of “save or restore predictor state” operations – which seems like it would make such an approach far more viable. If anyone is aware of a project that’s tried something like this – or is inspired to experiment – please do let me know. Curiosity and Reinforcement Learning  🔗︎ As outlined in the section above, my best guess for how you might apply a TAGE/ITTAGE-like algorithm to fuzzing is by treating “prediction error” as a reward signal, and spending time on inputs which have high prediction error. As I was thinking through that idea, I realized that it sounded familiar because, at some level of abstraction, that’s a classic idea from the domain of reinforcement learning! Perhaps most notably, in 2018, OpenAI published two papers about “curiosity-driven learning,” exploring techniques to enhance reinforcement learning by adding reward terms that encourage exploration, even absent reward signal from the environment. The two papers differ in the details of their approach, but share the same basic idea: Along with the policy network – which is the one that determines the actions to take – you train a predictor network, which attempts to predict some features of the environment, or the outcomes of your actions. You then reward the policy model for discovering actions or states with a high prediction error, which – if all goes right – encourages exploration of novel parts of the environment. As best I can tell, this technique worked fairly well; the second paper achieved state-of-the-art performance on Montezuma’s Revenge, an Atari game which was famously hard for reinforcement learning algorithms, since it requires extensive exploration and manipulation of keys and equipment prior to receiving any score. However, I don’t know, off the top of my head, what the future trajectory of that work and that approach has been. I was aware of those papers, and followed the work at the time, but hadn’t been consciously aware of them when I started trying to fit the “ITTAGE” and “coverage-guided fuzzing” pieces together in my head. The confluence does suggest to me that there may be something here; although, at the same time, in 2025 it might end up being easier to just throw a neural net at the problem, instead of a custom-designed-and-tuned prediction algorithm!",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n  \u003cp\u003eWhile \u003ca href=\"https://blog.nelhage.com/post/cpython-tail-call/\"\u003einvestigating the performance\u003c/a\u003e of the new \u003ca href=\"https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-tail-call\"\u003ePython 3.14 tail-calling interpreter\u003c/a\u003e, I learned (\u003ca href=\"https://github.com/python/cpython/issues/129987#issuecomment-2654837868\"\u003evia this very informative comment from Sam Gross\u003c/a\u003e) new (to me) piece of performance trivia: Modern CPUs mostly no longer struggle to predict the bytecode-dispatch indirect jump inside a “conventional” bytecode interpreter loop. In steady-state, assuming the bytecode itself is reasonable stable, modern CPUs \u003ca href=\"https://inria.hal.science/hal-01100647/document\"\u003eachieve very high accuracy\u003c/a\u003e predicting the dispatch, even for “vanilla” \u003ccode\u003ewhile / switch\u003c/code\u003e-style interpreter loops\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e!\u003c/p\u003e\n\u003cp\u003eIntrigued, I spent a bit of time reading about just \u003cstrong\u003ehow\u003c/strong\u003e branch predictors achieve this feat. I found the answer pretty fascinating, so I’m going to try to share the salient high-level features – as I understand them – as well as some interesting connections and ideas that came up in response.\u003c/p\u003e\n\u003cp\u003eA quick caveat: I am not a hardware engineer or CPU designer, and I’m mostly going to be focused on some high-level ideas I find interesting. I’ll probably get some things wrong. If you want an introduction to CPU branch prediction by someone who actually knows what they’re talking about, I’ll refer you to \u003ca href=\"https://danluu.com/branch-prediction/\"\u003eDan Luu’s piece on the topic\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"the-tage-and-ittage-branch-predictors\"\u003eThe TAGE and ITTAGE branch predictors \u003ca href=\"#the-tage-and-ittage-branch-predictors\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h2\u003e\n\u003cp\u003eIn general, modern state-of-the-art CPUs don’t seem to document too much about their branch predictors, so we don’t know – or at least, I couldn’t easily discover – what branch prediction looks like in real cutting-edge CPUs. However, there \u003cstrong\u003eis\u003c/strong\u003e (at least) one published algorithm that is both practical and can predict bytecode interpreter loops – the ITTAGE indirect branch predictor – and that’s what I’m talking about. The author of that predictor \u003ca href=\"https://inria.hal.science/hal-01100647/document\"\u003ewrote a paper\u003c/a\u003e exploring prediction on bytecode interpreters and found that his ITTAGE exhibited similar performance to Intel Haswell CPUs and suggested they use a variation of it, but I don’t think we know for sure.\u003c/p\u003e\n\u003cp\u003eITTAGE is a variant of the TAGE predictor; TAGE predicts taken/not-taken behavior for conditional branches, while ITTAGE predicts the destination of indirect jumps. Both predictors have a very similar structure, so I will mostly lump them together for most of this post.\u003c/p\u003e\n\u003ch3 id=\"a-brief-preview\"\u003eA brief preview \u003ca href=\"#a-brief-preview\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eBefore I dive in, I’ll give you a brief summary of where we’ll be going. Both TAGE and ITTAGE:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePredict branch behavior by mapping from \u003cem\u003e(PC, PC history) -\u0026gt; past behavior\u003c/em\u003e, and hoping that the future resembles the past.\u003c/li\u003e\n\u003cli\u003eStores many such tables, using a geometrically-increasing series of history lengths\u003c/li\u003e\n\u003cli\u003eAttempt to dynamically choose the correct table (history length) for each branch.\u003c/li\u003e\n\u003cli\u003eDo so by adaptively moving to a longer history on error, and using a careful replacement policy to preferentially keep useful entries.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNow, onto the longer version! Or, if that’s enough for you, \u003ca href=\"#why-i-find-ittage-interesting\"\u003eskip ahead\u003c/a\u003e to some reflections and some of the connections that make this topic particularly interesting to me.\u003c/p\u003e\n\u003ch3 id=\"dynamic-branch-prediction-101\"\u003eDynamic branch prediction 101 \u003ca href=\"#dynamic-branch-prediction-101\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eMany dynamic branch prediction algorithms work off of a simple premise: Keep track of a table of historical data in some form, and, when asked to predict a branch, look up what happened “last time” and assume that history will repeat itself. Thinking in C++-flavored pseudocode, I tend to mentally model this approach as something like:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"c++\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003estruct\u003c/span\u003e \u003cspan\u003eBranchDetails\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003e// The information we use to identify a branch\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e};\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003estruct\u003c/span\u003e \u003cspan\u003eBranchHistory\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003e// The information we store about eaach branch\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003e// Predict the outcome of a branch based on past state\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e  \u003cspan\u003ebool\u003c/span\u003e \u003cspan\u003epredict\u003c/span\u003e() \u003cspan\u003econst\u003c/span\u003e { \u003cspan\u003e/* ... */\u003c/span\u003e };\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003e// Update our state based on a resolved branch\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e  \u003cspan\u003evoid\u003c/span\u003e \u003cspan\u003eupdate\u003c/span\u003e(\u003cspan\u003ebool\u003c/span\u003e taken) { \u003cspan\u003e/* ... */\u003c/span\u003e };\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e};\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e// We store a mapping from one to the other. This is a fixed-size chunk of\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e// hardware, so it stores a fixed number of entries. We\u0026#39;ll talk a bit about\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e// replacement strategy and some details later on.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e \u003cspan\u003eusing\u003c/span\u003e PredictorState \u003cspan\u003e=\u003c/span\u003e FixedSizeMap\u003cspan\u003e\u0026lt;\u003c/span\u003eBranchDetails, BranchHistory\u003cspan\u003e\u0026gt;\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evoid\u003c/span\u003e \u003cspan\u003eon_resolve_branch\u003c/span\u003e(PredictorState \u003cspan\u003e\u0026amp;\u003c/span\u003epred, BranchDetails \u003cspan\u003e\u0026amp;\u003c/span\u003ebranch, \u003cspan\u003ebool\u003c/span\u003e taken) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  pred[branch].update(taken);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003ebool\u003c/span\u003e \u003cspan\u003epredict_branch\u003c/span\u003e(PredictorState \u003cspan\u003e\u0026amp;\u003c/span\u003epred, BranchDetails \u003cspan\u003e\u0026amp;\u003c/span\u003ebranch) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003ereturn\u003c/span\u003e pred[branch].predict();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSo, what do we use for the \u003ccode\u003eBranchDetails\u003c/code\u003e and \u003ccode\u003eBranchHistory\u003c/code\u003e? Perhaps the simplest option – used by some early CPUs! – is to just use the branch address to identify the branch – essentially, track state for each branch instruction in the program text – and to just use one bit of information for the history:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"c++\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003estruct\u003c/span\u003e \u003cspan\u003eBranchDetails\u003c/span\u003e { uintptr_t addr; };\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003estruct\u003c/span\u003e \u003cspan\u003eBranchHistory\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003ebool\u003c/span\u003e taken_;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003ebool\u003c/span\u003e \u003cspan\u003epredict\u003c/span\u003e() { \u003cspan\u003ereturn\u003c/span\u003e taken_; }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003evoid\u003c/span\u003e \u003cspan\u003eupdate\u003c/span\u003e(\u003cspan\u003ebool\u003c/span\u003e taken) { taken_ \u003cspan\u003e=\u003c/span\u003e taken; }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e};\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe next-simplest strategy replaces our \u003ccode\u003ebool\u003c/code\u003e per-branch state with a small counter (as few as 2 bits!), to give us a measure of hysteresis. We increment or decrement the counter on branch resolution, and use the sign for our prediction. It turns out that most branches are heavily “biased” one way or another – e.g. a “branch taken” rate of 10% or 90% is more common than 50% – and a small amount of hysteresis can let us absorb occasional outlier behaviors without forgetting everything we know:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"c++\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003estruct\u003c/span\u003e \u003cspan\u003eBranchHistory\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  int2_t counter_;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003ebool\u003c/span\u003e \u003cspan\u003epredict\u003c/span\u003e() { \u003cspan\u003ereturn\u003c/span\u003e counter_ \u003cspan\u003e\u0026gt;=\u003c/span\u003e \u003cspan\u003e0\u003c/span\u003e; }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003evoid\u003c/span\u003e \u003cspan\u003eupdate\u003c/span\u003e(\u003cspan\u003ebool\u003c/span\u003e taken) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e   saturating_increment(\u003cspan\u003e\u0026amp;\u003c/span\u003ecounter_, taken \u003cspan\u003e?\u003c/span\u003e \u003cspan\u003e1\u003c/span\u003e \u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e};\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"moving-beyond-just-pc\"\u003eMoving beyond just \u003ccode\u003ePC\u003c/code\u003e \u003ca href=\"#moving-beyond-just-pc\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eIndexing branches by \u003ccode\u003ePC\u003c/code\u003e is simple and efficient, but limiting. Many branches exhibit dynamic data-dependent behavior and we need to somehow distinguish with more granularity, if we want better accuracy.\u003c/p\u003e\n\u003cp\u003eBecause branch predictors live in the CPU frontend, and have to make predictions well before an instruction is actually executed – or potentially even fully-decoded – and so they don’t have much access to other CPU state to use in their predictions. However, there is one type of context they can access practically “for free”: the \u003cstrong\u003ehistory\u003c/strong\u003e of the program counter and recent branches, since the predictor had to help generate those, to start with!\u003c/p\u003e\n\u003cp\u003eThus, a branch predictor can maintain a circular buffer of some fixed size, store a rolling “branch history” or “PC history” in some form, and use that state to distinguish different branches. In the simplest case, we might store one bit per previous branch, writing a “1” for branch-taken, or “0” for not-taken. In more sophisticated predictors, we might include unconditional, and/or write a few bits of the PC value into the history:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"c++\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003econstexpr\u003c/span\u003e \u003cspan\u003eint\u003c/span\u003e history_length \u003cspan\u003e=\u003c/span\u003e ...;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003estruct\u003c/span\u003e \u003cspan\u003eBranchDetails\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  uintptr_t pc;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  bitarray\u003cspan\u003e\u0026lt;\u003c/span\u003ehistory_length\u003cspan\u003e\u0026gt;\u003c/span\u003e history;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"sizing-our-branch-history\"\u003eSizing our branch history \u003ca href=\"#sizing-our-branch-history\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eHow large of a history should we use? Is larger always better?\u003c/p\u003e\n\u003cp\u003eLonger histories allow us to learn more total patterns, and more complex patterns. For a program with stable behavior, in steady state, a larger history will potentially allow us to learn more of the program’s behavior, and distinguish between different situations more finely.\u003c/p\u003e\n\u003cp\u003eHowever, longer history means we \u003cstrong\u003eneed\u003c/strong\u003e more space in our table to learn simple patterns, and potentially more time, since we have more possible states, and we need to encounter each one separately to learn it. Imagine a simple function that looks like this:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"cpp\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003ebool\u003c/span\u003e logging_active;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evoid\u003c/span\u003e \u003cspan\u003elog\u003c/span\u003e(\u003cspan\u003econst\u003c/span\u003e \u003cspan\u003echar\u003c/span\u003e \u003cspan\u003e*\u003c/span\u003emsg) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003eif\u003c/span\u003e (logging_active) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    printf(\u003cspan\u003e\u0026#34;%s\u003c/span\u003e\u003cspan\u003e\\n\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e, msg);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSuppose this function isn’t inlined, and is called from many different places in an executable.\u003c/p\u003e\n\u003cp\u003eAssuming \u003ccode\u003elogging_active\u003c/code\u003e is static or mostly-static at runtime, this branch is highly predictable. A simple PC-only predictor should achieve near-perfect accuracy. However, if we also consider branch history, the predictor no longer “sees” this branch as a single entity; instead it needs to track every path that arrives at this branch instruction, separately. In the worst case, if we store \u003ccode\u003ek\u003c/code\u003e bits of history, we may need to use 2^k different table entries for this one branch! Worse, we need to encounter each state individually, and we don’t “learn anything” about different paths, from each other.\u003c/p\u003e\n\u003ch2 id=\"the-tage-algorithm-big-ideas\"\u003eThe TAGE algorithm: big ideas \u003ca href=\"#the-tage-algorithm-big-ideas\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h2\u003e\n\u003cp\u003eWe now have the necessary background to sketch a description of the TAGE predictor.\u003c/p\u003e\n\u003cp\u003eTAGE keeps track of branch history, as I’ve described, but unlike simpler predictors, it tracks hundreds or even thousands of bits of history, allowing it to potentially learn patterns over very long distances.\u003c/p\u003e\n\u003cp\u003eIn order to make use of all this history without unwanted state explosion, TAGE stores \u003cstrong\u003emultiple\u003c/strong\u003e history tables (perhaps on the order of 10-20), indexed by a geometric series of history lengths (i.e. table N uses history length \\( L_n ≈ L_0\\cdot{}r^n \\) for some ratio \\(r\\)). TAGE then attempts to adaptively choose, for each branch, the shortest history length (and corresponding table) that \u003cstrong\u003esuffices to make good predictions.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eHow does it do that? Here are the core ideas (as I understand them). I’ll link to some papers and code later on if you want to get into the details!\u003c/p\u003e\n\u003ch3 id=\"tag-bits-in-each-table\"\u003eTag bits in each table \u003ca href=\"#tag-bits-in-each-table\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eSo far, I’ve entirely glossed over the question of \u003cstrong\u003ehow\u003c/strong\u003e these lookup tables are actually implemented, and in particular concretely how we implement “lookup the entry with a given key.”\u003c/p\u003e\n\u003cp\u003eIn many simple branch predictors, history tables “don’t know” anything about which keys they are storing, and just directly index based on some of the bits of the key.\u003c/p\u003e\n\u003cp\u003eFor instance, for a predictor indexed by PC, we might just have an array of \\(2^k\\) counters, and use the low \\(k\\) bits of PC to select an entry. If two branches have the same address modulo \\(2^k\\), they will collide and use the same table entry, and we will make no effort to detect the collision or behave differently. This choice makes tables extremely cheap and efficient, and turns out to be a good tradeoff in many cases. Intuitively, we already have to handle the branch predictor being wrong, and such collisions are just another reason they might be wrong; detecting and reacting to collisions would take more hardware and more storage, and it turns out we’re better off using that to lower the error rate in other ways, instead.\u003c/p\u003e\n\u003cp\u003eTAGE, however, stores multiple tables, and needs to use different tables for different branches, which in turn necessitates \u003cstrong\u003eknowing\u003c/strong\u003e which tables are actually storing information for a given key, vs a colliding key. Thus, in addition to the other payload, each table entry stores a \u003cstrong\u003etag\u003c/strong\u003e, containing additional bits of metadata describing which key is stored in the entry.\u003c/p\u003e\n\u003cp\u003eGiven a (PC, branch history) tuple \u003cem\u003eT\u003c/em\u003e, TAGE uses two different hash functions for each table, \u003ccode\u003eH_index\u003c/code\u003e and \u003ccode\u003eH_tag\u003c/code\u003e. The branch state \u003ccode\u003eT\u003c/code\u003e will be stored in the table at index \u003ccode\u003eH_index(T)\u003c/code\u003e, with a tag value of \u003ccode\u003eH_tag(T)\u003c/code\u003e. On lookup, we check the value at \u003ccode\u003eH_index(T)\u003c/code\u003e and compare the tag to \u003ccode\u003eH_tag(T)\u003c/code\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf the tags disagree, this entry is currently storing information about some \u003cstrong\u003eother\u003c/strong\u003e branch, and we won’t use it (but may decide to overwrite it)\u003c/li\u003e\n\u003cli\u003eIf the tag agrees, we assume this state matches our branch, and we use and/or update it. Note that we still only check the hashes, and so it’s possible we collided with a different branch in \u003cstrong\u003eboth\u003c/strong\u003e hashes, but we will design them and choose their sizes so this condition is sufficiently rare in practice.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese \u003cstrong\u003eta\u003c/strong\u003eg bits give TAGE the “TA” in its name; the “GE” comes from the \u003cstrong\u003ege\u003c/strong\u003eometric series of history lengths.\u003c/p\u003e\n\u003ch3 id=\"basic-prediction-algorithm\"\u003eBasic prediction algorithm \u003ca href=\"#basic-prediction-algorithm\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eGiven this setup, the basic prediction algorithm of TAGE is fairly simple. Each table entry stores a counter (called \u003cem\u003ectr\u003c/em\u003e in the papers), as described above – it is incremented on “branch taken,” and decremented on “not taken.”\u003c/p\u003e\n\u003cp\u003eTo make a prediction, TAGE checks every table, using the appropriate history length. It considers all the table entries which have matching tags, and uses the prediction from the matching entry corresponding to the \u003cstrong\u003elongest\u003c/strong\u003e history length.\u003c/p\u003e\n\u003cp\u003eThe base predictor – in the simplest case, a table indexed only by PC – does not use tag bits, and thus will always match, and be used as a fallback if no longer history matches.\u003c/p\u003e\n\u003ch3 id=\"move-to-a-longer-history-on-prediction-error\"\u003eMove to a longer history on prediction error \u003ca href=\"#move-to-a-longer-history-on-prediction-error\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eOnce a branch resolves and we know the correct answer, we need to update the predictor.\u003c/p\u003e\n\u003cp\u003eTAGE always updates the \u003ccode\u003ectr\u003c/code\u003e field on the entry which was used to make the prediction. However, if the prediction was \u003cstrong\u003ewrong\u003c/strong\u003e, it also attempts to allocate a new entry, into one of the tables using a \u003cstrong\u003elonger\u003c/strong\u003e history length. The goal, thus, is to dynamically try longer and longer histories until we find a length that works.\u003c/p\u003e\n\u003ch3 id=\"track-the-usefulness-of-table-entries\"\u003eTrack the usefulness of table entries \u003ca href=\"#track-the-usefulness-of-table-entries\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eSince table entries are tagged, we need a way to decide when we will replace a table entry and reuse it for a new branch. For the predictor to work well, we aim to keep entries which are likely to make useful predictions in the future, and discard ones which won’t.\u003c/p\u003e\n\u003cp\u003eIn order to approximate that goal, TAGE tracks which entries have been useful \u003cstrong\u003erecently\u003c/strong\u003e. In addition to the tag and the counter, each table entry also has a \u003cem\u003eu\u003c/em\u003e (“useful”) counter (typically just 1 or 2 bits), which tracks whether the table has recently produced useful predictions.\u003c/p\u003e\n\u003cp\u003eWhen we allocate new table entries – as described above – we will only ever overwrite a slot with \u003cem\u003eu\u003c/em\u003e=0, and we will initialize new slots with \u003cem\u003eu\u003c/em\u003e=0; thus, new entries must prove their worth or risk being replaced.\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003eu\u003c/em\u003e counter is incremented any time a given table entry:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIs used for a prediction, and\u003c/li\u003e\n\u003cli\u003eThat prediction turns out to be correct, and\u003c/li\u003e\n\u003cli\u003eThe prediction from that entry is \u003cstrong\u003edifferent\u003c/strong\u003e from the prediction given by the matching entry with the next-longest history.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThus, it’s not enough for an entry to yield correct predictions; it also needs to yield a correct prediction that counterfactually would have been wrong.\u003c/p\u003e\n\u003cp\u003eIn addition, the \u003cem\u003eu\u003c/em\u003e counters are periodically decayed (or just set to zero) in some form, to prevent entries from lingering forever. The precise algorithm here varies a lot between published versions.\u003c/p\u003e\n\u003ch3 id=\"from-tage-to-ittage\"\u003eFrom TAGE to ITTAGE \u003ca href=\"#from-tage-to-ittage\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eI’ve been describing the behavior of TAGE, which predicts one bit of information (taken/not-taken) for conditional branches. ITTAGE, which predicts the target of indirect branches (the whole reason I’m writing about this system!) is virtually identical; the primary changes are only:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEach table entry also stores a predicted target address\u003c/li\u003e\n\u003cli\u003eThe \u003cem\u003ectr\u003c/em\u003e counter is retained, but becomes a “confidence” counter. It is incremented on “correct prediction” and decremented on “incorrect.” On incorrect prediction, we will update the predicted target to the new value iff \u003cem\u003ectr\u003c/em\u003e is at the minimum value. Thus, \u003cem\u003ectr\u003c/em\u003e tracks our confidence in this particular target address, while \u003cem\u003eu\u003c/em\u003e tracks the usefulness of this entire entry in the context of the entire predictor.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn fact, the same tables may be combined into a joint predictor, called “COTTAGE” \u003ca href=\"https://inria.hal.science/hal-03408381/document\"\u003ein the paper\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"references\"\u003eReferences \u003ca href=\"#references\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eI haven’t found an enormous amount written about TAGE and ITTAGE, but I will include here the best links I have found, in case you’re curious and want to dig into more details! Reading these papers, it really jumped out at me the extent to which it’s \u003cstrong\u003enot\u003c/strong\u003e enough to have the right high-level ideas; a high-performing implementation of TAGE or ITTAGE (or any branch predictor) is the result of both a good design, and a \u003cstrong\u003eton\u003c/strong\u003e of careful tuning and balancing of tradeoffs. Here’s the links:\u003c/p\u003e\n\u003cdl\u003e\n\u003cdt\u003e\u003ca href=\"https://inria.hal.science/hal-03408381/document\"\u003eA case for (partially) tagged geometric history length branch prediction\u003c/a\u003e\u003c/dt\u003e\n\u003cdd\u003eAs best I can tell, this is the paper that proposed TAGE and ITTAGE\u003c/dd\u003e\n\u003cdt\u003e\u003ca href=\"https://jilp.org/vol9/v9paper6.pdf\"\u003eThe L-TAGE Branch Predictor\u003c/a\u003e\u003c/dt\u003e\n\u003cdd\u003eAn implementation of TAGE for a branch prediction competition in 2007 (“CBP-2”).\u003c/dd\u003e\n\u003cdt\u003e\u003ca href=\"https://jilp.org/jwac-2/program/cbp3_03_seznec.pdf\"\u003eA 64 Kbytes ISL-TAGE branch predictor\u003c/a\u003e\u003c/dt\u003e\n\u003cdd\u003eThe description of an updated version for the successor competition, in 2011 (“CBP-3”).\u003c/dd\u003e\n\u003cdt\u003e\u003ca href=\"https://inria.hal.science/hal-00639041v1/document\"\u003eA 64-Kbytes ITTAGE indirect branch predictor\u003c/a\u003e\u003c/dt\u003e\n\u003cdd\u003eThe description of an ITTAGE predictor for the indirect branch track of the same competition.\u003c/dd\u003e\n\u003cdt\u003e\u003ca href=\"https://jilp.org/jwac-2/program/JWAC-2-program.htm\"\u003eThe program for JWAC2, which hosted the CBP-3 competition\u003c/a\u003e\u003c/dt\u003e\n\u003cdd\u003eIn particular, containing links to source code for the TAGE and ITTAGE implementations submitted to that competition (in a microarchitectural simulator).\u003c/dd\u003e\n\u003cdt\u003e\u003ca href=\"https://docs.boom-core.org/en/latest/sections/branch-prediction/backing-predictor.html#the-tage-predictor\"\u003eBOOM (Berkeley Out Of Order Machine)’s documentation on their TAGE implementation\u003c/a\u003e\u003c/dt\u003e\n\u003cdd\u003eBOOM is an open-source RISC-V core, intended for microarchitecture research.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003ch2 id=\"why-i-find-ittage-interesting\"\u003eWhy I find ITTAGE interesting \u003ca href=\"#why-i-find-ittage-interesting\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h2\u003e\n\u003cp\u003eOn one hand, I find ITTAGE interesting because I occasionally have cause to think about the performance of interpreter loops or similar software, and it represents an important update to how I need to reason about those situations. Very concretely, it informed my \u003ca href=\"https://blog.nelhage.com/post/cpython-tail-call/\"\u003eCPython benchmarking\u003c/a\u003e from last post.\u003c/p\u003e\n\u003cp\u003eHowever, I also find it fascinating for some broader reasons, and in connection to some other areas of interest.\u003c/p\u003e\n\u003cp\u003eI’ve \u003ca href=\"https://buttondown.com/nelhage/archive/tracing-jits-and-coverage-guided-fuzzers/\"\u003ewritten in the past\u003c/a\u003e about a class of software tools (including both coverage-guided fuzzers and tracing JITs), which attempt to understand some program’s behavior in large part by looking at the behavior of the program counter over time, and about how those tools struggle – in related ways – on interpreters and similar software for which the program state is “hidden in the data” and where the control flow alone is a poor proxy for the “interesting” state.\u003c/p\u003e\n\u003cp\u003eI didn’t write about this connection in that post, but I’ve always considered branch predictors to be another member of this class of tool. As \u003ca href=\"#moving-beyond-just-pc\"\u003ementioned above\u003c/a\u003e, they also understand program execution mostly through the lens of “a series of program counter values,” and they, too – at least historically – have struggled to behave well on interpreter loops.\u003c/p\u003e\n\u003cp\u003eThus, learning about ITTAGE and its success predicting interpreter behavior naturally raises the question, for me: Is there anything to be learned from the ITTAGE algorithm for those other tools?\u003c/p\u003e\n\u003cp\u003eIn particular, I wonder about…\u003c/p\u003e\n\u003ch3 id=\"ittage-for-coverage-guided-fuzzing-and-program-state-exploration\"\u003eITTAGE for coverage-guided fuzzing and program state exploration? \u003ca href=\"#ittage-for-coverage-guided-fuzzing-and-program-state-exploration\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eAs I sketched in \u003ca href=\"https://buttondown.com/nelhage/archive/tracing-jits-and-coverage-guided-fuzzers/\"\u003ethat older post\u003c/a\u003e, coverage-guided fuzzing is a technique which attempts to automatically explore the behavior of a target program, by generating candidate inputs, and then observing which inputs generate “new” behavior in the program.\u003c/p\u003e\n\u003cp\u003eIn order for this loop to work, we need some way of characterizing or bucketing program behavior, so we can decide what counts as “new” or “interesting” behavior, versus behavior we have already observed. I will admit that I am not up-to-date on the current frontiers in this field, but \u003ca href=\"https://lcamtuf.coredump.cx/afl/technical_details.txt\"\u003ehistorically this has been done\u003c/a\u003e using “coverage”-like metrics, which count the occurrences of either PC values or of branches (which essentially means (PC, PC’) pairs). These counts are potentially bucketed, and we “fingerprint” execution by the list of \u003ccode\u003e[(PC, bucketed_count)]\u003c/code\u003e values generated during execution.\u003c/p\u003e\n\u003cp\u003eThis approach is, in practice, fabulously effective. However, it can struggle on certain shapes of programs – including often interpreters – where the “interesting” state does not map well onto sets of program counters or branches. One of my favorite illustrations of this problem is the \u003ca href=\"https://ieeexplore.ieee.org/document/9152719\"\u003eIJON paper\u003c/a\u003e, which demonstrates some concrete problems, and attacks them using human-added annotations.\u003c/p\u003e\n\u003cp\u003eMy questions, then, is thus: Could something like the TAGE/ITTAGE approach help coverage-guided fuzzers to better explore the state space for interpreters, and interpreter-like programs? Could we, for instance, train a TAGE-like predictor on existing corpus entries, and then prioritize candidate mutants based on their rate of prediction errors? Might this allow a fuzzer to (e.g.) effectively explore the state space of code in an interpreted language, only by annotating the interpreter?\u003c/p\u003e\n\u003cp\u003eThere are a large number of practical challenges, but in principle this seems like it might allow more-nuanced exploration of state spaces, and discovering “novel behavior” which can only be identified by means of long-range correlations and patterns.\u003c/p\u003e\n\u003cp\u003eI will note that TAGE/ITTAGE \u003cstrong\u003especifically\u003c/strong\u003e are designed and tuned around hardware performance characteristics and tradeoffs; the performance landscape in software is very different, and so if such an idea \u003cstrong\u003edoes\u003c/strong\u003e work, I suspect the details look fairly different and are optimized for an efficient software implementation, it seems plausible to me there’s a place for borrowing the core idea of “dynamically picking a history length on a per-branch basis.”\u003c/p\u003e\n\u003cp\u003eAn even whackier idea might be to use the \u003cstrong\u003eactual hardware branch predictor\u003c/strong\u003e. Modern CPUs allow you to observe branch prediction accuracy via hardware performance counters, and we could imagine executing the corpus of existing examples to train the branch predictor, and then observing the actual hardware misprediction count as a novelty signal. This approach also has a ton of challenges, in part because of the opacity of the hardware branch predictor and the inability to explicitly control it; however, it has the virtue of potentially being much, much cheaper than a software predictor. It does make me wonder whether there are any CPUs which expose the branch predictor state explicitly – even in the form of “save or restore predictor state” operations – which seems like it would make such an approach far more viable.\u003c/p\u003e\n\u003cp\u003eIf anyone is aware of a project that’s tried something like this – or is inspired to experiment – please do let me know.\u003c/p\u003e\n\u003ch3 id=\"curiosity-and-reinforcement-learning\"\u003eCuriosity and Reinforcement Learning \u003ca href=\"#curiosity-and-reinforcement-learning\"\u003e\u003ci\u003e\t🔗︎\u003c/i\u003e\u003c/a\u003e \u003c/h3\u003e\n\u003cp\u003eAs outlined in the section above, my best guess for how you might apply a TAGE/ITTAGE-like algorithm to fuzzing is by treating “prediction error” as a reward signal, and spending time on inputs which have high prediction error.\u003c/p\u003e\n\u003cp\u003eAs I was thinking through that idea, I realized that it sounded familiar because, at some level of abstraction, that’s a classic idea from the domain of reinforcement learning!\u003c/p\u003e\n\u003cp\u003ePerhaps most notably, in 2018, OpenAI published \u003ca href=\"https://arxiv.org/pdf/1808.04355\"\u003etwo\u003c/a\u003e \u003ca href=\"https://openai.com/index/reinforcement-learning-with-prediction-based-rewards/\"\u003epapers\u003c/a\u003e about “curiosity-driven learning,” exploring techniques to enhance reinforcement learning by adding reward terms that encourage exploration, even absent reward signal from the environment. The two papers differ in the details of their approach, but share the same basic idea: Along with the policy network – which is the one that determines the actions to take – you train a predictor network, which attempts to predict some features of the environment, or the outcomes of your actions. You then reward the policy model for discovering actions or states with a high prediction error, which – if all goes right – encourages exploration of novel parts of the environment.\u003c/p\u003e\n\u003cp\u003eAs best I can tell, this technique worked fairly well; the \u003ca href=\"https://openai.com/index/reinforcement-learning-with-prediction-based-rewards/\"\u003esecond paper\u003c/a\u003e achieved state-of-the-art performance on \u003ca href=\"https://en.wikipedia.org/wiki/Montezuma%27s_Revenge_(video_game)\"\u003eMontezuma’s Revenge\u003c/a\u003e, an Atari game which was famously hard for reinforcement learning algorithms, since it requires extensive exploration and manipulation of keys and equipment prior to receiving any score. However, I don’t know, off the top of my head, what the future trajectory of that work and that approach has been.\u003c/p\u003e\n\u003cp\u003eI was aware of those papers, and followed the work at the time, but hadn’t been consciously aware of them when I started trying to fit the “ITTAGE” and “coverage-guided fuzzing” pieces together in my head. The confluence does suggest to me that there may be something here; although, at the same time, in 2025 it might end up being easier to just throw a neural net at the problem, instead of a custom-designed-and-tuned prediction algorithm!\u003c/p\u003e\n\n\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "24 min read",
  "publishedTime": "2025-07-04T14:30:00-07:00",
  "modifiedTime": "2025-07-04T14:30:00-07:00"
}
