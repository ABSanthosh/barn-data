{
  "id": "4bd6fa00-0371-497c-b3e4-3c76798a50f6",
  "title": "Former DeepSeeker and collaborators release new method for training reliable AI agents: RAGEN",
  "link": "https://venturebeat.com/ai/former-deepseeker-and-collaborators-release-new-method-for-training-reliable-ai-agents-ragen/",
  "description": "RAGEN stands out not just as a technical contribution but as a conceptual step toward more autonomous, reasoning-capable AI agents.",
  "author": "Carl Franzen",
  "published": "Wed, 23 Apr 2025 20:04:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "Agentic AI",
    "AI agents",
    "AI, ML and Deep Learning",
    "alibaba",
    "Alibaba cloud",
    "alibaba qwen",
    "Conversational AI",
    "Deepseek",
    "Github",
    "LLMs",
    "NLP",
    "Qwen",
    "Qwen 2.5",
    "RAGEN",
    "StarPO",
    "Zihan WAng"
  ],
  "byline": "Carl Franzen",
  "length": 8999,
  "excerpt": "RAGEN stands out not just as a technical contribution but as a conceptual step toward more autonomous, reasoning-capable AI agents.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "April 23, 2025 1:04 PM Credit: VentureBeat made with Midjourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More 2025 was, by many expert accounts, supposed to be the year of AI agents — task-specific AI implementations powered by leading large language and multimodal models (LLMs) like the kinds offered by OpenAI, Anthropic, Google, and DeepSeek. But so far, most AI agents remain stuck as experimental pilots in a kind of corporate purgatory, according to a recent poll conducted by VentureBeat on the social network X. Help may be on the way: a collaborative team from Northwestern University, Microsoft, Stanford, and the University of Washington — including a former DeepSeek researcher named Zihan Wang, currently completing a computer science PhD at Northwestern — has introduced RAGEN, a new system for training and evaluating AI agents that they hope makes them more reliable and less brittle for real-world, enterprise-grade usage. Unlike static tasks like math solving or code generation, RAGEN focuses on multi-turn, interactive settings where agents must adapt, remember, and reason in the face of uncertainty. Built on a custom RL framework called StarPO (State-Thinking-Actions-Reward Policy Optimization), the system explores how LLMs can learn through experience rather than memorization. The focus is on entire decision-making trajectories, not just one-step responses. StarPO operates in two interleaved phases: a rollout stage where the LLM generates complete interaction sequences guided by reasoning, and an update stage where the model is optimized using normalized cumulative rewards. This structure supports a more stable and interpretable learning loop compared to standard policy optimization approaches. The authors implemented and tested the framework using fine-tuned variants of Alibaba’s Qwen models, including Qwen 1.5 and Qwen 2.5. These models served as the base LLMs for all experiments and were chosen for their open weights and robust instruction-following capabilities. This decision enabled reproducibility and consistent baseline comparisons across symbolic tasks. Here’s how they did it and what they found: The Echo trap: how reinforcement learning rewards lead to LLM reasoning loss Wang summarized the core challenge in a widely shared X thread: Why does your RL training always collapse? According to the team, LLM agents initially generate symbolic, well-reasoned responses. But over time, RL systems tend to reward shortcuts, leading to repetitive behaviors that degrade overall performance—a pattern they call the “Echo Trap.” This regression is driven by feedback loops where certain phrases or strategies earn high rewards early on, encouraging overuse and stifling exploration. Wang notes that the symptoms are measurable: reward variance cliffs, gradient spikes, and disappearing reasoning traces. RAGEN test environments aren’t exactly enterprise-grade To study these behaviors in a controlled setting, RAGEN evaluates agents across three symbolic environments: Bandit: A single-turn, stochastic task that tests symbolic risk-reward reasoning. Sokoban: A multi-turn, deterministic puzzle involving irreversible decisions. Frozen Lake: A stochastic, multi-turn task requiring adaptive planning. Each environment is designed to minimize real-world priors and focus solely on decision-making strategies developed during training. In the Bandit environment, for instance, agents are told that Dragon and Phoenix arms represent different reward distributions. Rather than being told the probabilities directly, they must reason symbolically—e.g., interpreting Dragon as “strength” and Phoenix as “hope”—to predict outcomes. This kind of setup pressures the model to generate explainable, analogical reasoning. Stabilizing reinforcement learning with StarPO-S To address training collapse, the researchers introduced StarPO-S, a stabilized version of the original framework. StarPO-S incorporates three key interventions: Uncertainty-based rollout filtering: Prioritizing rollouts where the agent shows outcome uncertainty. KL penalty removal: Allowing the model to deviate more freely from its original policy and explore new behaviors. Asymmetric PPO clipping: Amplifying high-reward trajectories more than low-reward ones to boost learning. These changes delay or eliminate training collapse and improve performance across all three tasks. As Wang put it: “StarPO-S… works across all 3 tasks. Relieves collapse. Better reward.” What makes for a good agentic AI model? The success of RL training hinges not just on architecture, but on the quality of the data generated by the agents themselves. The team identified three dimensions that significantly impact training: Task diversity: Exposing the model to a wide range of initial scenarios improves generalization. Interaction granularity: Allowing multiple actions per turn enables more meaningful planning. Rollout freshness: Keeping training data aligned with the current model policy avoids outdated learning signals. Together, these factors make the training process more stable and effective. An interactive demo site published by the researchers on Github makes this explicit, visualizing agent rollouts as full dialogue turns—including not just actions, but the step-by-step thought process that preceded them. For example, in solving a math problem, an agent may first ‘think’ about isolating a variable, then submit an answer like ‘x = 5’. These intermediate thoughts are visible and traceable, which adds transparency into how agents arrive at decisions. When reasoning runs out While explicit reasoning improves performance in simple, single-turn tasks like Bandit, it tends to decay during multi-turn training. Despite the use of structured prompts and  tokens, reasoning traces often shrink or vanish unless directly rewarded. This points to a limitation in how rewards are typically designed: focusing on task completion may neglect the quality of the process behind it. The team experimented with format-based penalties to encourage better-structured reasoning, but acknowledges that more refined reward shaping is likely needed. RAGEN, along with its StarPO and StarPO-S frameworks, is now available as an open-source project at https://github.com/RAGEN-AI/RAGEN. However, no explicit license is listed in the GitHub repository at the time of writing, which may limit use or redistribution by others. The system provides a valuable foundation for those interested in developing AI agents that do more than complete tasks—they think, plan, and evolve. As AI continues to move toward autonomy, projects like RAGEN help illuminate what it takes to train models that learn not just from data, but from the consequences of their own actions. Outstanding Questions for Real-World Adoption While the RAGEN paper offers a detailed technical roadmap, several practical questions remain for those looking to apply these methods in enterprise settings. For example, how transferable is RAGEN’s approach beyond stylized, symbolic tasks? Would businesses need to design entirely new environments and reward functions to use this system in workflows like invoice processing or customer support? Another critical area is scalability. Even with the enhancements provided by StarPO-S, the paper acknowledges that training still eventually collapses over longer horizons. This raises the question: is there a theoretical or practical path to sustaining reasoning over open-ended or continuously evolving task sequences? At the time of writing, no explicit license is listed in the RAGEN GitHub repository or documentation, leaving open questions about usage rights. To explore these and other questions—including how non-technical decision-makers should interpret RAGEN’s implications—I reached out to co-author Wang for further insight. At the time of writing, a response is pending. Should any comments arrive, they will be included in a follow-up to this article or integrated as an update. RAGEN stands out not just as a technical contribution but as a conceptual step toward more autonomous, reasoning-capable AI agents. Whether it becomes part of the enterprise AI stack remains to be seen, but its insights into agent learning dynamics are already helping redefine the frontier of LLM training. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/04/nuneybits_Vector_art_of_college_classroom_with_robots_in_it_a5945ebd-0047-40a5-aeb7-79daba2024a0.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-04-23T20:04:00+00:00\" datetime=\"2025-04-23T20:04:00+00:00\"\u003eApril 23, 2025 1:04 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/nuneybits_Vector_art_of_college_classroom_with_robots_in_it_a5945ebd-0047-40a5-aeb7-79daba2024a0.webp?w=750\" alt=\"Small white robots with displays for faces stand in a children\u0026#39;s classroom between desks\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e2025 was, by many expert accounts, supposed to be the year of AI agents — task-specific AI implementations powered by leading large language and multimodal models (LLMs) like the kinds offered by OpenAI, Anthropic, Google, and DeepSeek.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut so far, most AI agents remain stuck as experimental pilots in a kind of corporate purgatory, according to a recent poll conducted by \u003ca href=\"https://x.com/VentureBeat/status/1913348068973441400\"\u003eVentureBeat on the social network X\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eHelp may be on the way: a collaborative team from Northwestern University, Microsoft, Stanford, and the University of Washington — including a former \u003ca href=\"https://zihanwang314.github.io/\"\u003eDeepSeek researcher named Zihan Wang\u003c/a\u003e, currently completing a computer science PhD at Northwestern — has\u003ca href=\"https://x.com/wzihanw/status/1915052871474712858\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e introduced RAGEN\u003c/a\u003e, a new system for training and evaluating AI agents that they hope makes them more reliable and less brittle for real-world, enterprise-grade usage.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnlike static tasks like math solving or code generation, RAGEN focuses on multi-turn, interactive settings where agents must adapt, remember, and reason in the face of uncertainty.\u003c/p\u003e\n\n\n\n\u003cp\u003eBuilt on a custom RL framework called StarPO (State-Thinking-Actions-Reward Policy Optimization), the system explores how LLMs can learn through experience rather than memorization. The focus is on entire decision-making trajectories, not just one-step responses.\u003c/p\u003e\n\n\n\n\u003cp\u003eStarPO operates in two interleaved phases: a rollout stage where the LLM generates complete interaction sequences guided by reasoning, and an update stage where the model is optimized using normalized cumulative rewards. This structure supports a more stable and interpretable learning loop compared to standard policy optimization approaches.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe authors implemented and tested the framework using fine-tuned variants of Alibaba’s Qwen models, including Qwen 1.5 and Qwen 2.5. These models served as the base LLMs for all experiments and were chosen for their open weights and robust instruction-following capabilities. This decision enabled reproducibility and consistent baseline comparisons across symbolic tasks.\u003c/p\u003e\n\n\n\n\u003cp\u003eHere’s how they did it and what they found:\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-echo-trap-how-reinforcement-learning-rewards-lead-to-llm-reasoning-loss\"\u003eThe Echo trap: how reinforcement learning rewards lead to LLM reasoning loss\u003c/h2\u003e\n\n\n\n\u003cp\u003eWang summarized the core challenge in a \u003ca href=\"https://x.com/wzihanw/status/1915052871474712858\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ewidely shared X thread\u003c/a\u003e: \u003cem\u003eWhy does your RL training always collapse?\u003c/em\u003e \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to the team, LLM agents initially generate symbolic, well-reasoned responses. But over time, RL systems tend to reward shortcuts, leading to repetitive behaviors that degrade overall performance—a pattern they call the “Echo Trap.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThis regression is driven by feedback loops where certain phrases or strategies earn high rewards early on, encouraging overuse and stifling exploration. \u003c/p\u003e\n\n\n\n\u003cp\u003eWang notes that the symptoms are measurable: reward variance cliffs, gradient spikes, and disappearing reasoning traces.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-ragen-test-environments-aren-t-exactly-enterprise-grade\"\u003eRAGEN test environments aren’t exactly enterprise-grade\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo study these behaviors in a controlled setting, RAGEN evaluates agents across three symbolic environments:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBandit\u003c/strong\u003e: A single-turn, stochastic task that tests symbolic risk-reward reasoning.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eSokoban\u003c/strong\u003e: A multi-turn, deterministic puzzle involving irreversible decisions.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eFrozen Lake\u003c/strong\u003e: A stochastic, multi-turn task requiring adaptive planning.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eEach environment is designed to minimize real-world priors and focus solely on decision-making strategies developed during training.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the Bandit environment, for instance, agents are told that Dragon and Phoenix arms represent different reward distributions. \u003c/p\u003e\n\n\n\n\u003cp\u003eRather than being told the probabilities directly, they must reason symbolically—e.g., interpreting Dragon as “strength” and Phoenix as “hope”—to predict outcomes. This kind of setup pressures the model to generate explainable, analogical reasoning.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-stabilizing-reinforcement-learning-with-starpo-s\"\u003eStabilizing reinforcement learning with StarPO-S\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo address training collapse, the researchers introduced StarPO-S, a stabilized version of the original framework. StarPO-S incorporates three key interventions:\u003c/p\u003e\n\n\n\n\u003col start=\"1\"\u003e\n\u003cli\u003e\u003cstrong\u003eUncertainty-based rollout filtering\u003c/strong\u003e: Prioritizing rollouts where the agent shows outcome uncertainty.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eKL penalty removal\u003c/strong\u003e: Allowing the model to deviate more freely from its original policy and explore new behaviors.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eAsymmetric PPO clipping\u003c/strong\u003e: Amplifying high-reward trajectories more than low-reward ones to boost learning.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eThese changes delay or eliminate training collapse and improve performance across all three tasks. As Wang put it: “StarPO-S… works across all 3 tasks. Relieves collapse. Better reward.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-makes-for-a-good-agentic-ai-model\"\u003eWhat makes for a good agentic AI model?\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe success of RL training hinges not just on architecture, but on the quality of the data generated by the agents themselves. The team identified three dimensions that significantly impact training:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTask diversity\u003c/strong\u003e: Exposing the model to a wide range of initial scenarios improves generalization.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eInteraction granularity\u003c/strong\u003e: Allowing multiple actions per turn enables more meaningful planning.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eRollout freshness\u003c/strong\u003e: Keeping training data aligned with the current model policy avoids outdated learning signals.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eTogether, these factors make the training process more stable and effective.\u003c/p\u003e\n\n\n\n\u003cp\u003eAn interactive \u003ca href=\"https://ragen-ai.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edemo site published by the researchers on Github\u003c/a\u003e makes this explicit, visualizing agent rollouts as full dialogue turns—including not just actions, but the step-by-step thought process that preceded them. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, in solving a math problem, an agent may first ‘think’ about isolating a variable, then submit an answer like ‘x = 5’. These intermediate thoughts are visible and traceable, which adds transparency into how agents arrive at decisions.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-when-reasoning-runs-out\"\u003eWhen reasoning runs out\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile explicit reasoning improves performance in simple, single-turn tasks like Bandit, it tends to decay during multi-turn training. Despite the use of structured prompts and  tokens, reasoning traces often shrink or vanish unless directly rewarded.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis points to a limitation in how rewards are typically designed: focusing on task completion may neglect the quality of the process behind it. The team experimented with format-based penalties to encourage better-structured reasoning, but acknowledges that more refined reward shaping is likely needed.\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cp\u003eRAGEN, along with its StarPO and StarPO-S frameworks, is now available as an open-source project at \u003ca href=\"https://github.com/RAGEN-AI/RAGEN\"\u003ehttps://github.com/RAGEN-AI/RAGEN\u003c/a\u003e. However, no explicit license is listed in the GitHub repository at the time of writing, which may limit use or redistribution by others.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe system provides a valuable foundation for those interested in developing AI agents that do more than complete tasks—they think, plan, and evolve.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs AI continues to move toward autonomy, projects like RAGEN help illuminate what it takes to train models that learn not just from data, but from the consequences of their own actions.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-outstanding-questions-for-real-world-adoption\"\u003eOutstanding Questions for Real-World Adoption\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile the RAGEN paper offers a detailed technical roadmap, several practical questions remain for those looking to apply these methods in enterprise settings. For example, how transferable is RAGEN’s approach beyond stylized, symbolic tasks? Would businesses need to design entirely new environments and reward functions to use this system in workflows like invoice processing or customer support?\u003c/p\u003e\n\n\n\n\u003cp\u003eAnother critical area is scalability. Even with the enhancements provided by StarPO-S, the paper acknowledges that training still eventually collapses over longer horizons. This raises the question: is there a theoretical or practical path to sustaining reasoning over open-ended or continuously evolving task sequences?\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003eAt the time of writing, no explicit license is listed in the RAGEN GitHub repository or documentation, leaving open questions about usage rights. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eTo explore these and other questions—including how non-technical decision-makers should interpret RAGEN’s implications—I reached out to co-author Wang for further insight. At the time of writing, a response is pending. Should any comments arrive, they will be included in a follow-up to this article or integrated as an update.\u003c/p\u003e\n\n\n\n\u003cp\u003eRAGEN stands out not just as a technical contribution but as a conceptual step toward more autonomous, reasoning-capable AI agents. Whether it becomes part of the enterprise AI stack remains to be seen, but its insights into agent learning dynamics are already helping redefine the frontier of LLM training.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-04-23T20:04:00Z",
  "modifiedTime": "2025-04-23T20:04:07Z"
}
