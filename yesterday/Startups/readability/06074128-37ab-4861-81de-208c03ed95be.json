{
  "id": "06074128-37ab-4861-81de-208c03ed95be",
  "title": "Cisco: Fine-tuned LLMs are now threat multipliers—22x more likely to go rogue",
  "link": "https://venturebeat.com/ai/cisco-warns-fine-tuning-turns-llms-into-threat-vectorsstructure/",
  "description": "Cisco warns LLMs fine-tuned for business are now being weaponized. Guardrails aren't failing. They're being engineered around.",
  "author": "Louis Columbus",
  "published": "Fri, 04 Apr 2025 22:12:32 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "Security",
    "$csco",
    "$goog",
    "$MSFT",
    "$NVDA",
    "AdversarialAI",
    "AgenticAI",
    "AI attacks",
    "ai orchestration",
    "AIInfrastructure",
    "AISecurity",
    "APIs",
    "category-/Computers \u0026 Electronics/Computer Security",
    "Cisco",
    "cybercrime",
    "DarkGPT",
    "DarkWeb",
    "decomposition prompting",
    "FineTuningLLMs",
    "FraudGPT",
    "GhostGPT",
    "jailbreak",
    "large language models (LLMs)",
    "LLMs",
    "MaliciousLLMs",
    "model inversion",
    "prompt injection",
    "saas",
    "SupplyChainAttacks",
    "WeaponizedLLMs",
    "ZeroDay"
  ],
  "byline": "Louis Columbus",
  "length": 8776,
  "excerpt": "Cisco warns LLMs fine-tuned for business are now being weaponized. Guardrails aren't failing. They're being engineered around.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Weaponized large language models (LLMs) fine-tuned with offensive tradecraft are reshaping cyberattacks, forcing CISOs to rewrite their playbooks. They’ve proven capable of automating reconnaissance, impersonating identities and evading real-time detection, accelerating large-scale social engineering attacks. Models, including FraudGPT, GhostGPT and DarkGPT, retail for as little as $75 a month and are purpose-built for attack strategies such as phishing, exploit generation, code obfuscation, vulnerability scanning and credit card validation. Cybercrime gangs, syndicates and nation-states see revenue opportunities in providing platforms, kits and leasing access to weaponized LLMs today. These LLMs are being packaged much like legitimate businesses package and sell SaaS apps. Leasing a weaponized LLM often includes access to dashboards, APIs, regular updates and, for some, customer support. VentureBeat continues to track the progression of weaponized LLMs closely. It’s becoming evident that the lines are blurring between developer platforms and cybercrime kits as weaponized LLMs’ sophistication continues to accelerate. With lease or rental prices plummeting, more attackers are experimenting with platforms and kits, leading to a new era of AI-driven threats. Legitimate LLMs in the cross-hairs The spread of weaponized LLMs has progressed so quickly that legitimate LLMs are at risk of being compromised and integrated into cybercriminal tool chains. The bottom line is that legitimate LLMs and models are now in the blast radius of any attack. The more fine-tuned a given LLM is, the greater the probability it can be directed to produce harmful outputs. Cisco’s The State of AI Security Report reports that fine-tuned LLMs are 22 times more likely to produce harmful outputs than base models. Fine-tuning models is essential for ensuring their contextual relevance. The trouble is that fine-tuning also weakens guardrails and opens the door to jailbreaks, prompt injections and model inversion. Cisco’s study proves that the more production-ready a model becomes, the more exposed it is to vulnerabilities that must be considered in an attack’s blast radius. The core tasks teams rely on to fine-tune LLMs, including continuous fine-tuning, third-party integration, coding and testing, and agentic orchestration, create new opportunities for attackers to compromise LLMs. Once inside an LLM, attackers work fast to poison data, attempt to hijack infrastructure, modify and misdirect agent behavior and extract training data at scale. Cisco’s study infers that without independent security layers, the models teams work so diligently on to fine-tune aren’t just at risk; they’re quickly becoming liabilities. From an attacker’s perspective, they’re assets ready to be infiltrated and turned. Fine-Tuning LLMs dismantles safety controls at scale A key part of Cisco’s security team’s research centered on testing multiple fine-tuned models, including Llama-2-7B and domain-specialized Microsoft Adapt LLMs. These models were tested across a wide variety of domains including healthcare, finance and law. One of the most valuable takeaways from Cisco’s study of AI security is that fine-tuning destabilizes alignment, even when trained on clean datasets. Alignment breakdown was the most severe in biomedical and legal domains, two industries known for being among the most stringent regarding compliance, legal transparency and patient safety.  While the intent behind fine-tuning is improved task performance, the side effect is systemic degradation of built-in safety controls. Jailbreak attempts that routinely failed against foundation models succeeded at dramatically higher rates against fine-tuned variants, especially in sensitive domains governed by strict compliance frameworks. The results are sobering. Jailbreak success rates tripled and malicious output generation soared by 2,200% compared to foundation models. Figure 1 shows just how stark that shift is. Fine-tuning boosts a model’s utility but comes at a cost, which is a substantially broader attack surface. TAP achieves up to 98% jailbreak success, outperforming other methods across open- and closed-source LLMs. Source: Cisco State of AI Security 2025, p. 16. Malicious LLMs are a $75 commodity Cisco Talos is actively tracking the rise of black-market LLMs and provides insights into their research in the report. Talos found that GhostGPT, DarkGPT and FraudGPT are sold on Telegram and the dark web for as little as $75/month. These tools are plug-and-play for phishing, exploit development, credit card validation and obfuscation. DarkGPT underground dashboard offers “uncensored intelligence” and subscription-based access for as little as 0.0098 BTC—framing malicious LLMs as consumer-grade SaaS.Source: Cisco State of AI Security 2025, p. 9. Unlike mainstream models with built-in safety features, these LLMs are pre-configured for offensive operations and offer APIs, updates, and dashboards that are indistinguishable from commercial SaaS products. $60 dataset poisoning threatens AI supply chains “For just $60, attackers can poison the foundation of AI models—no zero-day required,” write Cisco researchers. That’s the takeaway from Cisco’s joint research with Google, ETH Zurich and Nvidia, which shows how easily adversaries can inject malicious data into the world’s most widely used open-source training sets. By exploiting expired domains or timing Wikipedia edits during dataset archiving, attackers can poison as little as 0.01% of datasets like LAION-400M or COYO-700M and still influence downstream LLMs in meaningful ways. The two methods mentioned in the study, split-view poisoning and frontrunning attacks, are designed to leverage the fragile trust model of web-crawled data. With most enterprise LLMs built on open data, these attacks scale quietly and persist deep into inference pipelines. Decomposition attacks quietly extract copyrighted and regulated content One of the most startling discoveries Cisco researchers demonstrated is that LLMs can be manipulated to leak sensitive training data without ever triggering guardrails. Cisco researchers used a method called decomposition prompting to reconstruct over 20% of select New York Times and Wall Street Journal articles. Their attack strategy broke down prompts into sub-queries that guardrails classified as safe, then reassembled the outputs to recreate paywalled or copyrighted content. Successfully evading guardrails to access proprietary datasets or licensed content is an attack vector every enterprise is grappling to protect today. For those that have LLMs trained on proprietary datasets or licensed content, decomposition attacks can be particularly devastating. Cisco explains that the breach isn’t happening at the input level, it’s emerging from the models’ outputs. That makes it far more challenging to detect, audit or contain. If you’re deploying LLMs in regulated sectors like healthcare, finance or legal, you’re not just staring down GDPR, HIPAA or CCPA violations. You’re dealing with an entirely new class of compliance risk, where even legally sourced data can get exposed through inference, and the penalties are just the beginning. Final Word: LLMs aren’t just a tool, they’re the latest attack surface Cisco’s ongoing research, including Talos’ dark web monitoring, confirms what many security leaders already suspect: weaponized LLMs are growing in sophistication while a price and packaging war is breaking out on the dark web. Cisco’s findings also prove LLMs aren’t on the edge of the enterprise; they are the enterprise. From fine-tuning risks to dataset poisoning and model output leaks, attackers treat LLMs like infrastructure, not apps. One of the most valuable key takeaways from Cisco’s report is that static guardrails will no longer cut it. CISOs and security leaders need real-time visibility across the entire IT estate, stronger adversarial testing, and a more streamlined tech stack to keep up – and a new recognition that LLMs and models are an attack surface that becomes more vulnerable with greater fine-tuning. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/03/Cisco-Reveals-Fine-Tuned-LLMs-Evade-Controls-Mimic-Insider-Threats-With-22x-More-Success-1.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://venturebeat.com/security/the-age-of-weaponized-llms-is-here/\"\u003eWeaponized large language models\u003c/a\u003e (LLMs) fine-tuned with offensive tradecraft are reshaping cyberattacks, forcing CISOs to rewrite their playbooks. They’ve proven capable of automating reconnaissance, impersonating identities and evading real-time detection, accelerating large-scale social engineering attacks.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cspan\u003eModels, including \u003ca href=\"https://venturebeat.com/security/how-fraudgpt-presages-the-future-of-weaponized-ai/\"\u003eFraudGPT\u003c/a\u003e, \u003ca href=\"https://www.mimecast.com/blog/what-is-ghostgpt-implications-for-insider-risk-management/\" target=\"_blank\"\u003eGhostGPT\u003c/a\u003e and \u003ca href=\"https://thereviewhive.blog/darkgpt-ai-features-risks-ethical-concerns/\" target=\"_blank\"\u003eDarkGPT, retail for as little as $75 a month and\u003c/a\u003e are purpose-built for attack strategies such as\u003c/span\u003e phishing, exploit generation, code obfuscation, vulnerability scanning and credit card validation.\u003c/p\u003e\n\n\n\n\u003cp\u003eCybercrime gangs, syndicates and nation-states see revenue opportunities in providing platforms, kits and leasing access to weaponized LLMs today. These LLMs are being packaged much like legitimate businesses package and sell SaaS apps. Leasing a weaponized LLM often includes access to dashboards, APIs, regular updates and, for some, customer support.\u003c/p\u003e\n\n\n\n\u003cp\u003eVentureBeat continues to track the progression of weaponized LLMs closely. It’s becoming evident that the lines are blurring between developer platforms and cybercrime kits as weaponized LLMs’ sophistication continues to accelerate. With lease or rental prices plummeting, more attackers are experimenting with platforms and kits, leading to a new era of AI-driven threats.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-legitimate-llms-in-the-cross-hairs\"\u003e\u003cstrong\u003eLegitimate LLMs in the cross-hairs\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe spread of weaponized LLMs has progressed so quickly that legitimate LLMs are at risk of being compromised and integrated into cybercriminal tool chains. The bottom line is that legitimate LLMs and models are now in the blast radius of any attack.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe more fine-tuned a given LLM is, the greater the probability it can be directed to produce harmful outputs. \u003ca href=\"https://www.cisco.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCisco’s\u003c/a\u003e \u003ca href=\"https://www.cisco.com/c/en/us/products/security/state-of-ai-security.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eThe State of AI Security Report\u003c/a\u003e reports that fine-tuned LLMs are 22 times more likely to produce harmful outputs than base models. Fine-tuning models is essential for ensuring their contextual relevance. The trouble is that fine-tuning also weakens guardrails and opens the door to jailbreaks, prompt injections and model inversion.\u003c/p\u003e\n\n\n\n\u003cp\u003eCisco’s study proves that the more production-ready a model becomes, the more exposed it is to vulnerabilities that must be considered in an attack’s blast radius. The core tasks teams rely on to fine-tune LLMs, including continuous fine-tuning, third-party integration, coding and testing, and agentic orchestration, create new opportunities for attackers to compromise LLMs.\u003c/p\u003e\n\n\n\n\u003cp\u003eOnce inside an LLM, attackers work fast to poison data, attempt to hijack infrastructure, modify and misdirect agent behavior and extract training data at scale. Cisco’s study infers that without independent security layers, the models teams work so diligently on to fine-tune aren’t just at risk; they’re quickly becoming liabilities. From an attacker’s perspective, they’re assets ready to be infiltrated and turned.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-fine-tuning-llms-dismantles-safety-controls-at-scale\"\u003e\u003cstrong\u003eFine-Tuning LLMs dismantles safety controls at scale\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eA key part of Cisco’s security team’s research centered on testing multiple fine-tuned models, including Llama-2-7B and domain-specialized Microsoft Adapt LLMs. These models were tested across a wide variety of domains including healthcare, finance and law.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne of the most valuable takeaways from Cisco’s study of AI security is that fine-tuning destabilizes alignment, even when trained on clean datasets. Alignment breakdown was the most severe in biomedical and legal domains, two industries known for being among the most stringent regarding compliance, legal transparency and patient safety. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile the intent behind fine-tuning is improved task performance, the side effect is systemic degradation of built-in safety controls. Jailbreak attempts that routinely failed against foundation models succeeded at dramatically higher rates against fine-tuned variants, especially in sensitive domains governed by strict compliance frameworks.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe results are sobering. Jailbreak success rates tripled and malicious output generation soared by 2,200% compared to foundation models. Figure 1 shows just how stark that shift is. Fine-tuning boosts a model’s utility but comes at a cost, which is a substantially broader attack surface.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"571\" height=\"470\" src=\"https://venturebeat.com/wp-content/uploads/2025/03/figure-1.jpg\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/03/figure-1.jpg 571w, https://venturebeat.com/wp-content/uploads/2025/03/figure-1.jpg?resize=300,247 300w, https://venturebeat.com/wp-content/uploads/2025/03/figure-1.jpg?resize=400,329 400w\" sizes=\"(max-width: 571px) 100vw, 571px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eTAP achieves up to 98% jailbreak success, outperforming other methods across open- and closed-source LLMs. Source: Cisco State of AI Security 2025, p. 16.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-malicious-llms-are-a-75-commodity\"\u003e\u003cstrong\u003eMalicious LLMs are a $75 commodity\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eCisco Talos is actively tracking the rise of black-market LLMs and provides insights into their research in the report. Talos found that GhostGPT, DarkGPT and FraudGPT are sold on Telegram and the dark web for as little as $75/month. These tools are plug-and-play for phishing, exploit development, credit card validation and obfuscation.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"536\" height=\"314\" data-id=\"3002499\" src=\"https://venturebeat.com/wp-content/uploads/2025/03/gpt-threat-interface.jpg\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/03/gpt-threat-interface.jpg 536w, https://venturebeat.com/wp-content/uploads/2025/03/gpt-threat-interface.jpg?resize=300,176 300w, https://venturebeat.com/wp-content/uploads/2025/03/gpt-threat-interface.jpg?resize=400,234 400w\" sizes=\"(max-width: 536px) 100vw, 536px\"/\u003e\u003cfigcaption\u003eDarkGPT underground dashboard offers “uncensored intelligence” and subscription-based access for as little as 0.0098 BTC—framing malicious LLMs as consumer-grade SaaS.\u003cbr/\u003e\u003cstrong\u003eSource:\u003c/strong\u003e Cisco \u003cem\u003eState of AI Security 2025\u003c/em\u003e, p. 9.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003c/figure\u003e\n\n\n\n\u003cp\u003eUnlike mainstream models with built-in safety features, these LLMs are pre-configured for offensive operations and offer APIs, updates, and dashboards that are indistinguishable from commercial SaaS products.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-60-dataset-poisoning-threatens-ai-supply-chains\"\u003e\u003cstrong\u003e$60 dataset poisoning threatens AI supply chains\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003e“For just $60, attackers can poison the foundation of AI models—no zero-day required,” write Cisco researchers. That’s the takeaway from Cisco’s joint research with Google, ETH Zurich and Nvidia, which shows how easily adversaries can inject malicious data into the world’s most widely used open-source training sets.\u003c/p\u003e\n\n\n\n\u003cp\u003eBy exploiting expired domains or timing Wikipedia edits during dataset archiving, attackers can poison as little as 0.01% of datasets like LAION-400M or COYO-700M and still influence downstream LLMs in meaningful ways.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe two methods mentioned in the study, split-view poisoning and frontrunning attacks, are designed to leverage the fragile trust model of web-crawled data. With most enterprise LLMs built on open data, these attacks scale quietly and persist deep into inference pipelines.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-decomposition-attacks-quietly-extract-copyrighted-and-regulated-content\"\u003eDecomposition attacks quietly extract copyrighted and regulated content\u003c/h2\u003e\n\n\n\n\u003cp\u003eOne of the most startling discoveries Cisco researchers demonstrated is that LLMs can be manipulated to leak sensitive training data without ever triggering guardrails. Cisco researchers used a method called \u003ca href=\"https://learnprompting.org/docs/advanced/decomposition/introduction?srsltid=AfmBOorNSpu-sKbZSPm5GIlkfneez1_7FuB-BUCJV6fOorB9inDQWkKi\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edecomposition prompting\u003c/a\u003e to reconstruct over 20% of select \u003cem\u003eNew York Times\u003c/em\u003e and \u003cem\u003eWall Street Journal\u003c/em\u003e articles. Their attack strategy broke down prompts into sub-queries that guardrails classified as safe, then reassembled the outputs to recreate paywalled or copyrighted content.\u003c/p\u003e\n\n\n\n\u003cp\u003eSuccessfully evading guardrails to access proprietary datasets or licensed content is an attack vector every enterprise is grappling to protect today. For those that have LLMs trained on proprietary datasets or licensed content, decomposition attacks can be particularly devastating. Cisco explains that the breach isn’t happening at the input level, it’s emerging from the models’ outputs. That makes it far more challenging to detect, audit or contain.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you’re deploying LLMs in regulated sectors like healthcare, finance or legal, you’re not just staring down GDPR, HIPAA or CCPA violations. You’re dealing with an entirely new class of compliance risk, where even legally sourced data can get exposed through inference, and the penalties are just the beginning.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-final-word-llms-aren-t-just-a-tool-they-re-the-latest-attack-surface\"\u003eFinal Word: LLMs aren’t just a tool, they’re the latest attack surface\u003c/h3\u003e\n\n\n\n\u003cp\u003eCisco’s ongoing research, including Talos’ dark web monitoring, confirms what many security leaders already suspect: weaponized LLMs are growing in sophistication while a price and packaging war is breaking out on the dark web. Cisco’s findings also prove LLMs aren’t on the edge of the enterprise; they are the enterprise. From fine-tuning risks to dataset poisoning and model output leaks, attackers treat LLMs like infrastructure, not apps.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne of the most valuable key takeaways from Cisco’s report is that static guardrails will no longer cut it. CISOs and security leaders need real-time visibility across the entire IT estate, stronger adversarial testing, and a more streamlined tech stack to keep up – and a new recognition that LLMs and models are an attack surface that becomes more vulnerable with greater fine-tuning.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-04-04T22:12:32Z",
  "modifiedTime": "2025-04-04T22:12:39Z"
}
