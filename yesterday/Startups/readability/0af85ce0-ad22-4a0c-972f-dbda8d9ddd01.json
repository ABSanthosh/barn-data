{
  "id": "0af85ce0-ad22-4a0c-972f-dbda8d9ddd01",
  "title": "Why multi-agent AI tackles complexities LLMs can’t",
  "link": "https://venturebeat.com/ai/why-multi-agent-ai-conquers-complexities-llms-cant/",
  "description": "While AGI and fully autonomous systems are still on the horizon, multi-agents will bridge the current gap between LLMs and AGI.",
  "author": "Abhishek Gupta, Talentica Software",
  "published": "Sat, 02 Nov 2024 19:05:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "AI agents",
    "AI, ML and Deep Learning",
    "category-/Business \u0026 Industrial",
    "category-/Computers \u0026 Electronics/Software",
    "Conversational AI",
    "Generative AI",
    "large language models",
    "LLM-based AI agents",
    "NLP"
  ],
  "byline": "Abhishek Gupta, Talentica Software",
  "length": 8351,
  "excerpt": "While AGI and fully autonomous systems are still on the horizon, multi-agents will bridge the current gap between LLMs and AGI.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "November 2, 2024 12:05 PM Image Credit: Image credit: Venturebeat with DALL-E 3 Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More The introduction of ChatGPT has brought large language models (LLMs) into widespread use across both tech and non-tech industries. This popularity is primarily due to two factors: LLMs as a knowledge storehouse: LLMs are trained on a vast amount of internet data and are updated at regular intervals (that is, GPT-3, GPT-3.5, GPT-4, GPT-4o, and others);  Emergent abilities: As LLMs grow, they display abilities not found in smaller models. Does this mean we have already reached human-level intelligence, which we call artificial general intelligence (AGI)? Gartner defines AGI as a form of AI that possesses the ability to understand, learn and apply knowledge across a wide range of tasks and domains. The road to AGI is long, with one key hurdle being the auto-regressive nature of LLM training that predicts words based on past sequences. As one of the pioneers in AI research, Yann LeCun points out that LLMs can drift away from accurate responses due to their auto-regressive nature. Consequently, LLMs have several limitations: Limited knowledge: While trained on vast data, LLMs lack up-to-date world knowledge. Limited reasoning: LLMs have limited reasoning capability. As Subbarao Kambhampati points out LLMs are good knowledge retrievers but not good reasoners. No Dynamicity: LLMs are static and unable to access real-time information. To overcome LLM’s challenges, a more advanced approach is required. This is where agents become crucial. Agents to the rescue The concept of intelligent agent in AI has evolved over two decades, with implementations changing over time. Today, agents are discussed in the context of LLMs. Simply put, an agent is like a Swiss Army knife for LLM challenges: It can help us in reasoning, provide means to get up-to-date information from the Internet (solving dynamicity issues with LLM) and can achieve a task autonomously. With LLM as its backbone, an agent formally comprises tools, memory, reasoning (or planning) and action components. Components of an agent (Image Credit: Lilian Weng) Components of AI agents Tools enable agents to access external information — whether from the internet, databases, or APIs — allowing them to gather necessary data. Memory can be short or long-term. Agents use scratchpad memory to temporarily hold results from various sources, while chat history is an example of long-term memory. The Reasoner allows agents to think methodically, breaking complex tasks into manageable subtasks for effective processing. Actions: Agents perform actions based on their environment and reasoning, adapting and solving tasks iteratively through feedback. ReAct is one of the common methods for iteratively performing reasoning and action. What are agents good at? Agents excel at complex tasks, especially when in a role-playing mode, leveraging the enhanced performance of LLMs. For instance, when writing a blog, one agent may focus on research while another handles writing — each tackling a specific sub-goal. This multi-agent approach applies to numerous real-life problems. Role-playing helps agents stay focused on specific tasks to achieve larger objectives, reducing hallucinations by clearly defining parts of a prompt — such as role, instruction and context. Since LLM performance depends on well-structured prompts, various frameworks formalize this process. One such framework, CrewAI, provides a structured approach to defining role-playing, as we’ll discuss next. Multi agents vs single agent Take the example of retrieval augmented generation (RAG) using a single agent. It’s an effective way to empower LLMs to handle domain-specific queries by leveraging information from indexed documents. However, single-agent RAG comes with its own limitations, such as retrieval performance or document ranking. Multi-agent RAG overcomes these limitations by employing specialized agents for document understanding, retrieval and ranking. In a multi-agent scenario, agents collaborate in different ways, similar to distributed computing patterns: sequential, centralized, decentralized or shared message pools. Frameworks like CrewAI, Autogen, and langGraph+langChain enable complex problem-solving with multi-agent approaches. In this article, I have used CrewAI as the reference framework to explore autonomous workflow management. Workflow management: A use case for multi-agent systems Most industrial processes are about managing workflows, be it loan processing, marketing campaign management or even DevOps. Steps, either sequential or cyclic, are required to achieve a particular goal. In a traditional approach, each step (say, loan application verification) requires a human to perform the tedious and mundane task of manually processing each application and verifying them before moving to the next step. Each step requires input from an expert in that area. In a multi-agent setup using CrewAI, each step is handled by a crew consisting of multiple agents. For instance, in loan application verification, one agent may verify the user’s identity through background checks on documents like a driving license, while another agent verifies the user’s financial details. This raises the question: Can a single crew (with multiple agents in sequence or hierarchy) handle all loan processing steps? While possible, it complicates the crew, requiring extensive temporary memory and increasing the risk of goal deviation and hallucination. A more effective approach is to treat each loan processing step as a separate crew, viewing the entire workflow as a graph of crew nodes (using tools like langGraph) operating sequentially or cyclically. Since LLMs are still in their early stages of intelligence, full workflow management cannot be entirely autonomous. Human-in-the-loop is needed at key stages for end-user verification. For instance, after the crew completes the loan application verification step, human oversight is necessary to validate the results. Over time, as confidence in AI grows, some steps may become fully autonomous. Currently, AI-based workflow management functions in an assistive role, streamlining tedious tasks and reducing overall processing time. Production challenges Bringing multi-agent solutions into production can present several challenges. Scale: As the number of agents grows, collaboration and management become challenging. Various frameworks offer scalable solutions — for example, Llamaindex takes event-driven workflow to manage multi-agents at scale. Latency: Agent performance often incurs latency as tasks are executed iteratively, requiring multiple LLM calls. Managed LLMs (like GPT-4o) are slow because of implicit guardrails and network delays. Self-hosted LLMs (with GPU control) come in handy in solving latency issues. Performance and hallucination issues: Due to the probabilistic nature of LLM, agent performance can vary with each execution. Techniques like output templating (for instance, JSON format) and providing ample examples in prompts can help reduce response variability. The problem of hallucination can be further reduced by training agents. Final thoughts As Andrew Ng points out, agents are the future of AI and will continue to evolve alongside LLMs. Multi-agent systems will advance in processing multi-modal data (text, images, video, audio) and tackling increasingly complex tasks. While AGI and fully autonomous systems are still on the horizon, multi-agents will bridge the current gap between LLMs and AGI. Abhishek Gupta is a principal data scientist at Talentica Software. DataDecisionMakers Welcome to the VentureBeat community! DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation. If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers. You might even consider contributing an article of your own! Read More From DataDecisionMakers",
  "image": "https://venturebeat.com/wp-content/uploads/2024/07/AI-agents.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-11-02T19:05:00+00:00\" datetime=\"2024-11-02T19:05:00+00:00\"\u003eNovember 2, 2024 12:05 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"422\" src=\"https://venturebeat.com/wp-content/uploads/2024/07/AI-agents.jpg?w=750\" alt=\"AI agents\"/\u003e\u003c/p\u003e\u003cdiv\u003e\u003cp\u003e\u003cem\u003eImage Credit: Image credit: Venturebeat with DALL-E 3\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eThe introduction of ChatGPT has brought \u003ca href=\"https://venturebeat.com/ai/the-great-ai-masquerade-when-automation-wears-an-agent-costume/\"\u003elarge language models\u003c/a\u003e (LLMs) into widespread use across both tech and non-tech industries. This popularity is primarily due to two factors:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eLLMs as a knowledge storehouse: LLMs are trained on a vast amount of internet data and are updated at regular intervals (that is, GPT-3, GPT-3.5, GPT-4, GPT-4o, and others);\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\n\u003cli\u003e Emergent abilities: As LLMs grow, they display \u003ca href=\"https://arxiv.org/pdf/2206.07682\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eabilities\u003c/a\u003e not found in smaller models.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eDoes this mean we have already reached human-level intelligence, which we call \u003ca href=\"https://venturebeat.com/ai/have-we-reached-peak-human/\"\u003eartificial general intelligence\u003c/a\u003e (AGI)? \u003ca href=\"https://www.gartner.com/en/information-technology/glossary/artificial-general-intelligence-agi\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGartner defines\u003c/a\u003e AGI as a form of AI that possesses the ability to understand, learn and apply knowledge across a wide range of tasks and domains. The road to AGI is long, with one key hurdle being the auto-regressive nature of LLM training that predicts words based on past sequences. As one of the pioneers in AI research, Yann LeCun \u003ca href=\"https://www.youtube.com/watch?v=5t1vTLU7s40\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epoints out that LLMs\u003c/a\u003e can drift away from accurate responses due to their auto-regressive nature. Consequently, LLMs have several limitations:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eLimited knowledge: While trained on vast data, LLMs lack up-to-date world knowledge.\u003c/li\u003e\n\n\n\n\u003cli\u003eLimited reasoning: LLMs have limited reasoning capability. As Subbarao Kambhampati points out\u003ca href=\"https://www.youtube.com/watch?v=y1WnHpedi2A\"\u003e \u003c/a\u003eLLMs are good knowledge retrievers but \u003ca href=\"https://www.youtube.com/watch?v=y1WnHpedi2A\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enot good reasoners\u003c/a\u003e.\u003c/li\u003e\n\n\n\n\u003cli\u003eNo Dynamicity: LLMs are static and unable to access real-time information.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eTo overcome LLM’s challenges, a more advanced approach is required. This is where agents become crucial.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-agents-to-the-rescue\"\u003eAgents to the rescue\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe concept of \u003ca href=\"https://en.wikipedia.org/wiki/Intelligent_agent\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eintelligent agent in AI\u003c/a\u003e has evolved over two decades, with implementations changing over time. Today, agents are discussed in the context of LLMs. Simply put, an agent is like a Swiss Army knife for LLM challenges: It can help us in reasoning, provide means to get up-to-date information from the Internet (solving dynamicity issues with LLM) and can achieve a task autonomously. With LLM as its backbone, an agent formally comprises tools, memory, reasoning (or planning) and action components.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1286\" height=\"513\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/image1.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/11/image1.png 1286w, https://venturebeat.com/wp-content/uploads/2024/11/image1.png?resize=300,120 300w, https://venturebeat.com/wp-content/uploads/2024/11/image1.png?resize=768,306 768w, https://venturebeat.com/wp-content/uploads/2024/11/image1.png?resize=800,319 800w, https://venturebeat.com/wp-content/uploads/2024/11/image1.png?resize=400,160 400w, https://venturebeat.com/wp-content/uploads/2024/11/image1.png?resize=750,299 750w, https://venturebeat.com/wp-content/uploads/2024/11/image1.png?resize=578,231 578w, https://venturebeat.com/wp-content/uploads/2024/11/image1.png?resize=930,371 930w\" sizes=\"(max-width: 1286px) 100vw, 1286px\"/\u003e\u003cfigcaption\u003e\u003ca href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eComponents of an\u003c/em\u003e a\u003cem\u003egent\u003c/em\u003e\u003c/a\u003e\u003cem\u003e (Image Credit: Lilian Weng)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-components-of-ai-agents\"\u003eComponents of AI agents\u003c/h2\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eTools enable agents to access external information — whether from the internet, databases, or APIs — allowing them to gather necessary data.\u003c/li\u003e\n\n\n\n\u003cli\u003eMemory can be short or long-term. Agents use scratchpad memory to temporarily hold results from various sources, while chat history is an example of long-term memory.\u003c/li\u003e\n\n\n\n\u003cli\u003eThe Reasoner allows agents to think methodically, breaking complex tasks into manageable subtasks for effective processing.\u003c/li\u003e\n\n\n\n\u003cli\u003eActions: Agents perform actions based on their environment and reasoning, adapting and solving tasks iteratively through feedback. ReAct is one of the common methods for iteratively performing reasoning and action.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch2 id=\"h-what-are-agents-good-at\"\u003eWhat are agents good at?\u003c/h2\u003e\n\n\n\n\u003cp\u003eAgents excel at complex tasks, especially when in a \u003ca href=\"https://arxiv.org/pdf/2305.16367\" target=\"_blank\" rel=\"noreferrer noopener\"\u003erole-playing\u003c/a\u003e mode, leveraging the enhanced performance of LLMs. For instance, when writing a blog, one agent may focus on research while another handles writing — each tackling a \u003ca href=\"https://github.com/crewAIInc/crewAI\" target=\"_blank\" rel=\"noreferrer noopener\"\u003especific sub-goal\u003c/a\u003e. This multi-agent approach applies to numerous real-life problems.\u003c/p\u003e\n\n\n\n\u003cp\u003eRole-playing helps agents stay focused on specific tasks to achieve larger objectives, reducing hallucinations by clearly \u003ca href=\"https://learnprompting.org/docs/basics/formalizing\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edefining parts\u003c/a\u003e of a prompt — such as role, instruction and context. Since LLM performance depends on well-structured prompts, various frameworks formalize this process. One such framework, \u003ca href=\"https://venturebeat.com/ai/crewai-launches-its-first-multi-agent-builder-speeding-the-way-to-agentic-ai/\"\u003eCrewAI\u003c/a\u003e, provides a structured approach to defining role-playing, as we’ll discuss next.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-multi-agents-vs-single-agent\"\u003eMulti agents vs single agent\u003c/h2\u003e\n\n\n\n\u003cp\u003eTake the example of retrieval augmented generation (RAG) using a single agent. It’s an effective way to empower LLMs to handle domain-specific queries by leveraging information from indexed documents. However, single-agent \u003ca href=\"https://superlinked.com/vectorhub/articles/enhancing-rag-multi-agent-system\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRAG comes with its own limitations\u003c/a\u003e, such as retrieval performance or document ranking. Multi-agent RAG overcomes these limitations by employing specialized agents for document understanding, retrieval and ranking.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a multi-agent scenario, agents collaborate in different ways, similar to distributed computing patterns: sequential, centralized, decentralized or shared message pools. Frameworks like CrewAI, Autogen, and langGraph+langChain enable complex problem-solving with multi-agent approaches. In this article, I have used CrewAI as the reference framework to explore autonomous workflow management.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-workflow-management-a-use-case-for-multi-agent-systems\"\u003eWorkflow management: A use case for multi-agent systems\u003c/h2\u003e\n\n\n\n\u003cp\u003eMost industrial processes are about managing workflows, be it loan processing, marketing campaign management or even DevOps. Steps, either sequential or cyclic, are required to achieve a particular goal. In a traditional approach, each step (say, loan application verification) requires a human to perform the tedious and mundane task of manually processing each application and verifying them before moving to the next step. \u003c/p\u003e\n\n\n\n\u003cp\u003eEach step requires input from an expert in that area. In a multi-agent setup using CrewAI, each step is handled by a crew consisting of multiple agents. For instance, in loan application verification, one agent may verify the user’s identity through background checks on documents like a driving license, while another agent verifies the user’s financial details.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis raises the question: Can a single crew (with multiple agents in sequence or hierarchy) handle all loan processing steps? While possible, it complicates the crew, requiring extensive temporary memory and increasing the risk of goal deviation and hallucination. A more effective approach is to treat each loan processing step as a separate crew, viewing the entire workflow as a graph of crew nodes (using tools like langGraph) operating sequentially or cyclically.\u003c/p\u003e\n\n\n\n\u003cp\u003eSince LLMs are still in their early stages of intelligence, full workflow management cannot be entirely autonomous. Human-in-the-loop is needed at key stages for end-user verification. For instance, after the crew completes the loan application verification step, human oversight is necessary to validate the results. Over time, as confidence in AI grows, some steps may become fully autonomous. Currently, AI-based workflow management functions in an assistive role, streamlining tedious tasks and reducing overall processing time.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-production-challenges\"\u003eProduction challenges\u003c/h2\u003e\n\n\n\n\u003cp\u003eBringing multi-agent solutions into production can present several challenges.\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eScale: As the number of agents grows, collaboration and management become challenging. Various frameworks offer scalable solutions — for example, \u003ca href=\"https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLlamaindex takes event-driven workflow\u003c/a\u003e to manage multi-agents at scale.\u003c/li\u003e\n\n\n\n\u003cli\u003eLatency: Agent performance often incurs latency as tasks are executed iteratively, requiring multiple LLM calls. Managed LLMs (like GPT-4o) are slow because of implicit guardrails and network delays. Self-hosted LLMs (with GPU control) come in handy in solving latency issues.\u003c/li\u003e\n\n\n\n\u003cli\u003ePerformance and hallucination issues: Due to the probabilistic nature of LLM, agent performance can vary with each execution. Techniques like output templating (for instance, JSON format) and providing ample examples in prompts can help reduce response variability. The problem of hallucination can be further reduced \u003ca href=\"https://docs.crewai.com/core-concepts/Training-Crew/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eby training agents\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch2 id=\"h-final-thoughts\"\u003eFinal thoughts\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs \u003ca href=\"https://www.youtube.com/watch?v=q1XFm21I-VQ\u0026amp;t=720s\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAndrew Ng points out\u003c/a\u003e, agents are the future of AI and will continue to evolve alongside LLMs. Multi-agent systems will advance in processing multi-modal data (text, images, video, audio) and tackling increasingly complex tasks. While AGI and fully autonomous systems are still on the horizon, multi-agents will bridge the current gap between LLMs and AGI.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://www.linkedin.com/in/abhishek-gupta-93a6808/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eAbhishek Gupta\u003c/em\u003e\u003c/a\u003e\u003cem\u003e is a principal data scientist at \u003c/em\u003e\u003ca href=\"https://www.talentica.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eTalentica Software\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2736392\"\u003e\n\u003cp\u003e\u003cstrong\u003eDataDecisionMakers\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWelcome to the VentureBeat community!\u003c/p\u003e\n\n\n\n\u003cp\u003eDataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might even consider \u003ca rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://venturebeat.com/guest-posts/\"\u003econtributing an article\u003c/a\u003e of your own!\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca rel=\"noreferrer noopener\" href=\"https://venturebeat.com/category/DataDecisionMakers/\" target=\"_blank\"\u003eRead More From DataDecisionMakers\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2024-11-02T19:05:00Z",
  "modifiedTime": "2024-11-02T18:46:36Z"
}
