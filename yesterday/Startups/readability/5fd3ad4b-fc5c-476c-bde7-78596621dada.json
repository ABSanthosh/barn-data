{
  "id": "5fd3ad4b-fc5c-476c-bde7-78596621dada",
  "title": "Beyond RAG: How cache-augmented generation reduces latency, complexity for smaller workloads",
  "link": "https://venturebeat.com/ai/beyond-rag-how-cache-augmented-generation-reduces-latency-complexity-for-smaller-workloads/",
  "description": "As LLMs become more capable, many RAG applications can be replaced with cache-augmented generation that include documents in the prompt.",
  "author": "Ben Dickson",
  "published": "Fri, 17 Jan 2025 21:25:12 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI research",
    "AI, ML and Deep Learning",
    "category-/Computers \u0026 Electronics/Programming",
    "category-/Science/Computer Science",
    "large language models",
    "large language models (LLMs)",
    "LLMs",
    "research",
    "retrieval augmented generation",
    "Retrieval-augmented generation (RAG)"
  ],
  "byline": "Ben Dickson",
  "length": 7452,
  "excerpt": "As LLMs become more capable, many RAG applications can be replaced with cache-augmented generation that include documents in the prompt.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "January 17, 2025 1:25 PM Image credit: VentureBeat with Ideogram Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Retrieval-augmented generation (RAG) has become the de-facto way of customizing large language models (LLMs) for bespoke information. However, RAG comes with upfront technical costs and can be slow. Now, thanks to advances in long-context LLMs, enterprises can bypass RAG by inserting all the proprietary information in the prompt. A new study by the National Chengchi University in Taiwan shows that by using long-context LLMs and caching techniques, you can create customized applications that outperform RAG pipelines. Called cache-augmented generation (CAG), this approach can be a simple and efficient replacement for RAG in enterprise settings where the knowledge corpus can fit in the model’s context window. Limitations of RAG RAG is an effective method for handling open-domain questions and specialized tasks. It uses retrieval algorithms to gather documents that are relevant to the request and adds context to enable the LLM to craft more accurate responses. However, RAG introduces several limitations to LLM applications. The added retrieval step introduces latency that can degrade the user experience. The result also depends on the quality of the document selection and ranking step. In many cases, the limitations of the models used for retrieval require documents to be broken down into smaller chunks, which can harm the retrieval process.  And in general, RAG adds complexity to the LLM application, requiring the development, integration and maintenance of additional components. The added overhead slows the development process. Cache-augmented retrieval RAG (top) vs CAG (bottom) (source: arXiv) The alternative to developing a RAG pipeline is to insert the entire document corpus into the prompt and have the model choose which bits are relevant to the request. This approach removes the complexity of the RAG pipeline and the problems caused by retrieval errors. However, there are three key challenges with front-loading all documents into the prompt. First, long prompts will slow down the model and increase the costs of inference. Second, the length of the LLM’s context window sets limits to the number of documents that fit in the prompt. And finally, adding irrelevant information to the prompt can confuse the model and reduce the quality of its answers. So, just stuffing all your documents into the prompt instead of choosing the most relevant ones can end up hurting the model’s performance. The CAG approach proposed leverages three key trends to overcome these challenges. First, advanced caching techniques are making it faster and cheaper to process prompt templates. The premise of CAG is that the knowledge documents will be included in every prompt sent to the model. Therefore, you can compute the attention values of their tokens in advance instead of doing so when receiving requests. This upfront computation reduces the time it takes to process user requests. Leading LLM providers such as OpenAI, Anthropic and Google provide prompt caching features for the repetitive parts of your prompt, which can include the knowledge documents and instructions that you insert at the beginning of your prompt. With Anthropic, you can reduce costs by up to 90% and latency by 85% on the cached parts of your prompt. Equivalent caching features have been developed for open-source LLM-hosting platforms. Second, long-context LLMs are making it easier to fit more documents and knowledge into prompts. Claude 3.5 Sonnet supports up to 200,000 tokens, while GPT-4o supports 128,000 tokens and Gemini up to 2 million tokens. This makes it possible to include multiple documents or entire books in the prompt. And finally, advanced training methods are enabling models to do better retrieval, reasoning and question-answering on very long sequences. In the past year, researchers have developed several LLM benchmarks for long-sequence tasks, including BABILong, LongICLBench, and RULER. These benchmarks test LLMs on hard problems such as multiple retrieval and multi-hop question-answering. There is still room for improvement in this area, but AI labs continue to make progress. As newer generations of models continue to expand their context windows, they will be able to process larger knowledge collections. Moreover, we can expect models to continue improving in their abilities to extract and use relevant information from long contexts. “These two trends will significantly extend the usability of our approach, enabling it to handle more complex and diverse applications,” the researchers write. “Consequently, our methodology is well-positioned to become a robust and versatile solution for knowledge-intensive tasks, leveraging the growing capabilities of next-generation LLMs.” RAG vs CAG To compare RAG and CAG, the researchers ran experiments on two widely recognized question-answering benchmarks: SQuAD, which focuses on context-aware Q\u0026A from single documents, and HotPotQA, which requires multi-hop reasoning across multiple documents. They used a Llama-3.1-8B model with a 128,000-token context window. For RAG, they combined the LLM with two retrieval systems to obtain passages relevant to the question: the basic BM25 algorithm and OpenAI embeddings. For CAG, they inserted multiple documents from the benchmark into the prompt and let the model itself determine which passages to use to answer the question. Their experiments show that CAG outperformed both RAG systems in most situations.  CAG outperforms both sparse RAG (BM25 retrieval) and dense RAG (OpenAI embeddings) (source: arXiv) “By preloading the entire context from the test set, our system eliminates retrieval errors and ensures holistic reasoning over all relevant information,” the researchers write. “This advantage is particularly evident in scenarios where RAG systems might retrieve incomplete or irrelevant passages, leading to suboptimal answer generation.” CAG also significantly reduces the time to generate the answer, particularly as the reference text length increases.  Generation time for CAG is much smaller than RAG (source: arXiv) That said, CAG is not a silver bullet and should be used with caution. It is well suited for settings where the knowledge base does not change often and is small enough to fit within the context window of the model. Enterprises should also be careful of cases where their documents contain conflicting facts based on the context of the documents, which might confound the model during inference.  The best way to determine whether CAG is good for your use case is to run a few experiments. Fortunately, the implementation of CAG is very easy and should always be considered as a first step before investing in more development-intensive RAG solutions. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/01/TJVRewV0S_GqJz_1gX2vEw.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-01-17T21:25:12+00:00\" datetime=\"2025-01-17T21:25:12+00:00\"\u003eJanuary 17, 2025 1:25 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"421\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/TJVRewV0S_GqJz_1gX2vEw.webp?w=750\" alt=\"Image credit: VentureBeat with Ideogram\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with Ideogram\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eRetrieval-augmented generation (RAG) has become the de-facto way of customizing large language models (LLMs) for bespoke information. However, RAG comes with upfront technical costs and can be slow. Now, thanks to advances in long-context LLMs, enterprises can bypass RAG by inserting all the proprietary information in the prompt.\u003c/p\u003e\n\n\n\n\u003cp\u003eA \u003ca href=\"https://arxiv.org/abs/2412.15605\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew study\u003c/a\u003e by the National Chengchi University in Taiwan shows that by using long-context LLMs and caching techniques, you can create customized applications that outperform RAG pipelines. Called cache-augmented generation (CAG), this approach can be a simple and efficient replacement for RAG in enterprise settings where the knowledge corpus can fit in the model’s context window.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-limitations-of-rag\"\u003eLimitations of RAG\u003c/h2\u003e\n\n\n\n\u003cp\u003eRAG is an \u003ca href=\"https://venturebeat.com/ai/how-llamaindex-is-ushering-in-the-future-of-rag-for-enterprises/\"\u003eeffective method\u003c/a\u003e for handling open-domain questions and specialized tasks. It uses retrieval algorithms to gather documents that are relevant to the request and adds context to enable the LLM to craft more accurate responses.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, RAG introduces several limitations to LLM applications. The added retrieval step introduces latency that can degrade the user experience. The result also depends on the \u003ca href=\"https://venturebeat.com/ai/new-technique-makes-rag-systems-much-better-at-retrieving-the-right-documents/\"\u003equality of the document selection\u003c/a\u003e and ranking step. In many cases, the limitations of the models used for retrieval require documents to be broken down into smaller chunks, which can harm the retrieval process. \u003c/p\u003e\n\n\n\n\u003cp\u003eAnd in general, RAG adds complexity to the LLM application, requiring the development, integration and maintenance of additional components. The added overhead slows the development process.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-cache-augmented-retrieval\"\u003eCache-augmented retrieval\u003c/h2\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1284\" height=\"878\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/image_be5e5b.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/image_be5e5b.png 1284w, https://venturebeat.com/wp-content/uploads/2025/01/image_be5e5b.png?resize=300,205 300w, https://venturebeat.com/wp-content/uploads/2025/01/image_be5e5b.png?resize=768,525 768w, https://venturebeat.com/wp-content/uploads/2025/01/image_be5e5b.png?resize=800,547 800w, https://venturebeat.com/wp-content/uploads/2025/01/image_be5e5b.png?resize=400,274 400w, https://venturebeat.com/wp-content/uploads/2025/01/image_be5e5b.png?resize=750,513 750w, https://venturebeat.com/wp-content/uploads/2025/01/image_be5e5b.png?resize=578,395 578w, https://venturebeat.com/wp-content/uploads/2025/01/image_be5e5b.png?resize=930,636 930w\" sizes=\"(max-width: 1284px) 100vw, 1284px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eRAG (top) vs CAG (bottom) (source: arXiv)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe alternative to developing a RAG pipeline is to insert the entire document corpus into the prompt and have the model choose which bits are relevant to the request. This approach removes the complexity of the RAG pipeline and the problems caused by retrieval errors.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, there are three key challenges with front-loading all documents into the prompt. First, long prompts will slow down the model and increase the costs of inference. Second, the \u003ca href=\"https://venturebeat.com/ai/how-gradient-created-an-open-llm-with-a-million-token-context-window/\"\u003elength of the LLM’s context window\u003c/a\u003e sets limits to the number of documents that fit in the prompt. And finally, adding irrelevant information to the prompt can confuse the model and reduce the quality of its answers. So, just stuffing all your documents into the prompt instead of choosing the most relevant ones can end up hurting the model’s performance.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe CAG approach proposed leverages three key trends to overcome these challenges.\u003c/p\u003e\n\n\n\n\u003cp\u003eFirst, advanced caching techniques are making it faster and cheaper to process prompt templates. The premise of CAG is that the knowledge documents will be included in every prompt sent to the model. Therefore, you can compute the attention values of their tokens in advance instead of doing so when receiving requests. This upfront computation reduces the time it takes to process user requests.\u003c/p\u003e\n\n\n\n\u003cp\u003eLeading LLM providers such as OpenAI, Anthropic and Google provide prompt caching features for the repetitive parts of your prompt, which can include the knowledge documents and instructions that you insert at the beginning of your prompt. With Anthropic, you can \u003ca href=\"https://venturebeat.com/ai/anthropics-new-claude-prompt-caching-will-save-developers-a-fortune/\"\u003ereduce costs by up to 90%\u003c/a\u003e and latency by 85% on the cached parts of your prompt. Equivalent caching features have been developed for open-source LLM-hosting platforms.\u003c/p\u003e\n\n\n\n\u003cp\u003eSecond, \u003ca href=\"https://venturebeat.com/ai/googles-new-technique-gives-llms-infinite-context/\"\u003elong-context LLMs\u003c/a\u003e are making it easier to fit more documents and knowledge into prompts. Claude 3.5 Sonnet supports up to 200,000 tokens, while GPT-4o supports 128,000 tokens and Gemini up to 2 million tokens. This makes it possible to include multiple documents or entire books in the prompt.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnd finally, advanced training methods are enabling models to do better retrieval, reasoning and question-answering on very long sequences. In the past year, researchers have developed several LLM benchmarks for long-sequence tasks, including \u003ca href=\"https://arxiv.org/abs/2406.10149\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBABILong\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2404.02060\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLongICLBench\u003c/a\u003e, and \u003ca href=\"https://arxiv.org/abs/2404.06654\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRULER\u003c/a\u003e. These benchmarks test LLMs on hard problems such as multiple retrieval and multi-hop question-answering. There is still room for improvement in this area, but AI labs continue to make progress.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs newer generations of models continue to expand their context windows, they will be able to process larger knowledge collections. Moreover, we can expect models to continue improving in their abilities to extract and use relevant information from long contexts.\u003c/p\u003e\n\n\n\n\u003cp\u003e“These two trends will significantly extend the usability of our approach, enabling it to handle more complex and diverse applications,” the researchers write. “Consequently, our methodology is well-positioned to become a robust and versatile solution for knowledge-intensive tasks, leveraging the growing capabilities of next-generation LLMs.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-rag-vs-cag\"\u003eRAG vs CAG\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo compare RAG and CAG, the researchers ran experiments on two widely recognized question-answering benchmarks: \u003ca href=\"https://rajpurkar.github.io/SQuAD-explorer/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSQuAD\u003c/a\u003e, which focuses on context-aware Q\u0026amp;A from single documents, and \u003ca href=\"https://hotpotqa.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHotPotQA\u003c/a\u003e, which requires multi-hop reasoning across multiple documents.\u003c/p\u003e\n\n\n\n\u003cp\u003eThey used a \u003ca href=\"https://venturebeat.com/ai/meta-unleashes-its-most-powerful-ai-model-llama-3-1-with-405b-parameters/\"\u003eLlama-3.1-8B\u003c/a\u003e model with a 128,000-token context window. For RAG, they combined the LLM with two retrieval systems to obtain passages relevant to the question: the basic \u003ca href=\"https://en.wikipedia.org/wiki/Okapi_BM25\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBM25 algorithm\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/openai-launches-new-generation-of-embedding-models-and-other-api-updates/\"\u003eOpenAI embeddings\u003c/a\u003e. For CAG, they inserted multiple documents from the benchmark into the prompt and let the model itself determine which passages to use to answer the question. Their experiments show that CAG outperformed both RAG systems in most situations. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1284\" height=\"744\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/image_e6f50f.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/image_e6f50f.png 1284w, https://venturebeat.com/wp-content/uploads/2025/01/image_e6f50f.png?resize=300,174 300w, https://venturebeat.com/wp-content/uploads/2025/01/image_e6f50f.png?resize=768,445 768w, https://venturebeat.com/wp-content/uploads/2025/01/image_e6f50f.png?resize=800,464 800w, https://venturebeat.com/wp-content/uploads/2025/01/image_e6f50f.png?resize=400,232 400w, https://venturebeat.com/wp-content/uploads/2025/01/image_e6f50f.png?resize=750,435 750w, https://venturebeat.com/wp-content/uploads/2025/01/image_e6f50f.png?resize=578,335 578w, https://venturebeat.com/wp-content/uploads/2025/01/image_e6f50f.png?resize=930,539 930w\" sizes=\"(max-width: 1284px) 100vw, 1284px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eCAG outperforms both sparse RAG (BM25 retrieval) and dense RAG (OpenAI embeddings) (source: arXiv)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e“By preloading the entire context from the test set, our system eliminates retrieval errors and ensures holistic reasoning over all relevant information,” the researchers write. “This advantage is particularly evident in scenarios where RAG systems might retrieve incomplete or irrelevant passages, leading to suboptimal answer generation.”\u003c/p\u003e\n\n\n\n\u003cp\u003eCAG also significantly reduces the time to generate the answer, particularly as the reference text length increases. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1188\" height=\"878\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/image_8e162a.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/image_8e162a.png 1188w, https://venturebeat.com/wp-content/uploads/2025/01/image_8e162a.png?resize=300,222 300w, https://venturebeat.com/wp-content/uploads/2025/01/image_8e162a.png?resize=768,568 768w, https://venturebeat.com/wp-content/uploads/2025/01/image_8e162a.png?resize=800,591 800w, https://venturebeat.com/wp-content/uploads/2025/01/image_8e162a.png?resize=400,296 400w, https://venturebeat.com/wp-content/uploads/2025/01/image_8e162a.png?resize=750,554 750w, https://venturebeat.com/wp-content/uploads/2025/01/image_8e162a.png?resize=578,427 578w, https://venturebeat.com/wp-content/uploads/2025/01/image_8e162a.png?resize=930,687 930w\" sizes=\"(max-width: 1188px) 100vw, 1188px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eGeneration time for CAG is much smaller than RAG (source: arXiv)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThat said, CAG is not a silver bullet and should be used with caution. It is well suited for settings where the knowledge base does not change often and is small enough to fit within the context window of the model. Enterprises should also be careful of cases where their documents contain conflicting facts based on the context of the documents, which might confound the model during inference. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe best way to determine whether CAG is good for your use case is to run a few experiments. Fortunately, the implementation of CAG is very easy and should always be considered as a first step before investing in more development-intensive RAG solutions.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-01-17T21:25:12Z",
  "modifiedTime": "2025-01-17T21:25:21Z"
}
