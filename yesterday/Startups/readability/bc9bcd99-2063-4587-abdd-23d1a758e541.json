{
  "id": "bc9bcd99-2063-4587-abdd-23d1a758e541",
  "title": "Anthropomorphizing AI: Dire consequences of mistaking human-like for human have already emerged",
  "link": "https://venturebeat.com/ai/anthropomorphizing-ai-dire-consequences-of-mistaking-human-like-for-human-have-already-emerged/",
  "description": "Anthropomorphizing AI creates dangerous blind spots in business decision-making beyond simple operational inefficiencies.",
  "author": "Roanie Levy, CCC",
  "published": "Sat, 18 Jan 2025 20:25:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "AI, ML and Deep Learning",
    "category-/Science/Computer Science",
    "Conversational AI",
    "Generative AI",
    "large language models",
    "NLP"
  ],
  "byline": "Roanie Levy, CCC",
  "length": 11059,
  "excerpt": "Anthropomorphizing AI creates dangerous blind spots in business decision-making beyond simple operational inefficiencies.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "January 18, 2025 12:25 PM VentureBeat/Ideogram Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More In our rush to understand and relate to AI, we have fallen into a seductive trap: Attributing human characteristics to these robust but fundamentally non-human systems. This anthropomorphizing of AI is not just a harmless quirk of human nature — it is becoming an increasingly dangerous tendency that might cloud our judgment in critical ways. Business leaders are comparing AI learning to human education to justify training practices to lawmakers crafting policies based on flawed human-AI analogies. This tendency to humanize AI might inappropriately shape crucial decisions across industries and regulatory frameworks. Viewing AI through a human lens in business has led companies to overestimate AI capabilities or underestimate the need for human oversight, sometimes with costly consequences. The stakes are particularly high in copyright law, where anthropomorphic thinking has led to problematic comparisons between human learning and AI training. The language trap Listen to how we talk about AI: We say it “learns,” “thinks,” “understands” and even “creates.” These human terms feel natural, but they are misleading. When we say an AI model “learns,” it is not gaining understanding like a human student. Instead, it performs complex statistical analyses on vast amounts of data, adjusting weights and parameters in its neural networks based on mathematical principles. There is no comprehension, eureka moment, spark of creativity or actual understanding — just increasingly sophisticated pattern matching. This linguistic sleight of hand is more than merely semantic. As noted in the paper, Generative AI’s Illusory Case for Fair Use: “The use of anthropomorphic language to describe the development and functioning of AI models is distorting because it suggests that once trained, the model operates independently of the content of the works on which it has trained.” This confusion has real consequences, mainly when it influences legal and policy decisions. The cognitive disconnect Perhaps the most dangerous aspect of anthropomorphizing AI is how it masks the fundamental differences between human and machine intelligence. While some AI systems excel at specific types of reasoning and analytical tasks, the large language models (LLMs) that dominate today’s AI discourse — and that we focus on here — operate through sophisticated pattern recognition. These systems process vast amounts of data, identifying and learning statistical relationships between words, phrases, images and other inputs to predict what should come next in a sequence. When we say they “learn,” we’re describing a process of mathematical optimization that helps them make increasingly accurate predictions based on their training data. Consider this striking example from research by Berglund and his colleagues: A model trained on materials stating “A is equal to B” often cannot reason, as a human would, to conclude that “B is equal to A.” If an AI learns that Valentina Tereshkova was the first woman in space, it might correctly answer “Who was Valentina Tereshkova?” but struggle with “Who was the first woman in space?” This limitation reveals the fundamental difference between pattern recognition and true reasoning — between predicting likely sequences of words and understanding their meaning. The copyright conundrum This anthropomorphic bias has particularly troubling implications in the ongoing debate about AI and copyright. Microsoft CEO Satya Nadella recently compared AI training to human learning, suggesting that AI should be able to do the same if humans can learn from books without copyright implications. This comparison perfectly illustrates the danger of anthropomorphic thinking in discussions about ethical and responsible AI. Some argue that this analogy needs to be revised to understand human learning and AI training. When humans read books, we do not make copies of them — we understand and internalize concepts. AI systems, on the other hand, must make actual copies of works — often obtained without permission or payment — encode them into their architecture and maintain these encoded versions to function. The works don’t disappear after “learning,” as AI companies often claim; they remain embedded in the system’s neural networks. The business blind spot Anthropomorphizing AI creates dangerous blind spots in business decision-making beyond simple operational inefficiencies. When executives and decision-makers think of AI as “creative” or “intelligent” in human terms, it can lead to a cascade of risky assumptions and potential legal liabilities. Overestimating AI capabilities One critical area where anthropomorphizing creates risk is content generation and copyright compliance. When businesses view AI as capable of “learning” like humans, they might incorrectly assume that AI-generated content is automatically free from copyright concerns. This misunderstanding can lead companies to: Deploy AI systems that inadvertently reproduce copyrighted material, exposing the business to infringement claims Fail to implement proper content filtering and oversight mechanisms Assume incorrectly that AI can reliably distinguish between public domain and copyrighted material Underestimate the need for human review in content generation processes The cross-border compliance blind spot The anthropomorphic bias in AI creates dangers when we consider cross-border compliance. As explained by Daniel Gervais, Haralambos Marmanis, Noam Shemtov, and Catherine Zaller Rowland in “The Heart of the Matter: Copyright, AI Training, and LLMs,” copyright law operates on strict territorial principles, with each jurisdiction maintaining its own rules about what constitutes infringement and what exceptions apply. This territorial nature of copyright law creates a complex web of potential liability. Companies might mistakenly assume their AI systems can freely “learn” from copyrighted materials across jurisdictions, failing to recognize that training activities that are legal in one country may constitute infringement in another. The EU has recognized this risk in its AI Act, particularly through Recital 106, which requires any general-purpose AI model offered in the EU to comply with EU copyright law regarding training data, regardless of where that training occurred. This matters because anthropomorphizing AI’s capabilities can lead companies to underestimate or misunderstand their legal obligations across borders. The comfortable fiction of AI “learning” like humans obscures the reality that AI training involves complex copying and storage operations that trigger different legal obligations in other jurisdictions. This fundamental misunderstanding of AI’s actual functioning, combined with the territorial nature of copyright law, creates significant risks for businesses operating globally. The human cost One of the most concerning costs is the emotional toll of anthropomorphizing AI. We see increasing instances of people forming emotional attachments to AI chatbots, treating them as friends or confidants. This can be particularly dangerous for vulnerable individuals who might share personal information or rely on AI for emotional support it cannot provide. The AI’s responses, while seemingly empathetic, are sophisticated pattern matching based on training data — there is no genuine understanding or emotional connection. This emotional vulnerability could also manifest in professional settings. As AI tools become more integrated into daily work, employees might develop inappropriate levels of trust in these systems, treating them as actual colleagues rather than tools. They might share confidential work information too freely or hesitate to report errors out of a misplaced sense of loyalty. While these scenarios remain isolated right now, they highlight how anthropomorphizing AI in the workplace could cloud judgment and create unhealthy dependencies on systems that, despite their sophisticated responses, are incapable of genuine understanding or care. Breaking free from the anthropomorphic trap So how do we move forward? First, we need to be more precise in our language about AI. Instead of saying an AI “learns” or “understands,” we might say it “processes data” or “generates outputs based on patterns in its training data.” This is not just pedantic — it helps clarify what these systems do. Second, we must evaluate AI systems based on what they are rather than what we imagine them to be. This means acknowledging both their impressive capabilities and their fundamental limitations. AI can process vast amounts of data and identify patterns humans might miss, but it cannot understand, reason or create in the way humans do. Finally, we must develop frameworks and policies that address AI’s actual characteristics rather than imagined human-like qualities. This is particularly crucial in copyright law, where anthropomorphic thinking can lead to flawed analogies and inappropriate legal conclusions. The path forward As AI systems become more sophisticated at mimicking human outputs, the temptation to anthropomorphize them will grow stronger. This anthropomorphic bias affects everything from how we evaluate AI’s capabilities to how we assess its risks. As we have seen, it extends into significant practical challenges around copyright law and business compliance. When we attribute human learning capabilities to AI systems, we must understand their fundamental nature and the technical reality of how they process and store information. Understanding AI for what it truly is — sophisticated information processing systems, not human-like learners — is crucial for all aspects of AI governance and deployment. By moving past anthropomorphic thinking, we can better address the challenges of AI systems, from ethical considerations and safety risks to cross-border copyright compliance and training data governance. This more precise understanding will help businesses make more informed decisions while supporting better policy development and public discourse around AI. The sooner we embrace AI’s true nature, the better equipped we will be to navigate its profound societal implications and practical challenges in our global economy. Roanie Levy is licensing and legal advisor at CCC. DataDecisionMakers Welcome to the VentureBeat community! DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation. If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers. You might even consider contributing an article of your own! Read More From DataDecisionMakers",
  "image": "https://venturebeat.com/wp-content/uploads/2025/01/a-vector-art-image-of-an-ai-model-sittin_4wKKsh8yQ5ewNsr-M-I2IQ_3tSjphUASQiv-ufJPT_lNA-transformed.jpeg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-01-18T20:25:00+00:00\" datetime=\"2025-01-18T20:25:00+00:00\"\u003eJanuary 18, 2025 12:25 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"421\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/a-vector-art-image-of-an-ai-model-sittin_4wKKsh8yQ5ewNsr-M-I2IQ_3tSjphUASQiv-ufJPT_lNA-transformed.jpeg?w=750\" alt=\"VentureBeat/Ideogram\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eVentureBeat/Ideogram\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eIn our rush to understand and \u003ca href=\"https://venturebeat.com/ai/why-context-aware-ai-agents-will-give-us-superpowers-in-2025/\"\u003erelate to AI\u003c/a\u003e, we have fallen into a seductive trap: Attributing human characteristics to these robust but fundamentally non-human systems. This anthropomorphizing of AI is not just a harmless quirk of human nature — it is becoming an increasingly dangerous tendency that might cloud our judgment in critical ways. Business leaders are comparing AI learning to \u003ca href=\"https://venturebeat.com/ai/ai-is-set-to-transform-education-what-enterprise-leaders-can-learn-from-this-development/\"\u003ehuman education\u003c/a\u003e to justify training practices to lawmakers crafting policies based on flawed human-AI analogies.  This tendency to humanize AI might inappropriately shape crucial decisions across industries and regulatory frameworks.\u003c/p\u003e\n\n\n\n\u003cp\u003eViewing AI through a human lens in business has led companies to overestimate AI capabilities or underestimate the need for human oversight, sometimes with costly consequences. The stakes are particularly high in copyright law, where anthropomorphic thinking has led to problematic comparisons between human learning and AI training.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-language-trap\"\u003eThe language trap\u003c/h2\u003e\n\n\n\n\u003cp\u003eListen to how we talk about AI: We say it “learns,” “thinks,” “understands” and even “creates.” These human terms feel natural, but they are misleading. When we say an AI model “learns,” it is not gaining understanding like a human student. Instead, it performs complex statistical analyses on vast amounts of data, adjusting weights and parameters in its neural networks based on mathematical principles. There is no comprehension, eureka moment, spark of creativity or actual understanding — just increasingly sophisticated pattern matching.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis linguistic sleight of hand is more than merely semantic. As noted in the paper, \u003ca href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4924997\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGenerative AI’s Illusory Case for Fair Use\u003c/a\u003e: “The use of anthropomorphic language to describe the development and functioning of AI models is distorting because it suggests that once trained, the model operates independently of the content of the works on which it has trained.” This confusion has real consequences, mainly when it influences legal and policy decisions.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-cognitive-disconnect\"\u003eThe cognitive disconnect\u003c/h2\u003e\n\n\n\n\u003cp\u003ePerhaps the most dangerous aspect of anthropomorphizing AI is how it masks the fundamental differences between human and machine intelligence. While some AI systems excel at specific types of reasoning and analytical tasks, the \u003ca href=\"https://venturebeat.com/ai/despite-heated-ai-arms-race-were-in-for-a-multi-modal-future/\"\u003elarge language models\u003c/a\u003e (LLMs) that dominate today’s AI discourse — and that we focus on here — operate through sophisticated pattern recognition. \u003c/p\u003e\n\n\n\n\u003cp\u003eThese systems process vast amounts of data, identifying and learning statistical relationships between words, phrases, images and other inputs to predict what should come next in a sequence. When we say they “learn,” we’re describing a process of mathematical optimization that helps them make increasingly accurate predictions based on their training data.\u003c/p\u003e\n\n\n\n\u003cp\u003eConsider this striking example from research by \u003ca href=\"https://arxiv.org/pdf/2309.12288\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBerglund and his colleagues:\u003c/a\u003e A model trained on materials stating “A is equal to B” often cannot reason, as a human would, to conclude that “B is equal to A.” If an AI learns that Valentina Tereshkova was the first woman in space, it might correctly answer “Who was Valentina Tereshkova?” but struggle with “Who was the first woman in space?” This limitation reveals the fundamental difference between pattern recognition and true reasoning — between predicting likely sequences of words and understanding their meaning.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-copyright-conundrum\"\u003eThe copyright conundrum\u003c/h2\u003e\n\n\n\n\u003cp\u003eThis anthropomorphic bias has particularly troubling implications in the ongoing debate about AI and copyright. Microsoft CEO Satya Nadella \u003ca href=\"https://www.thetimes.com/business-money/companies/article/microsoft-ceo-copyright-laws-information-ai-pc9hh2c0p\" target=\"_blank\" rel=\"noreferrer noopener\"\u003erecently compared AI training\u003c/a\u003e to human learning, suggesting that AI should be able to do the same if humans can learn from books without copyright implications. This comparison perfectly illustrates the danger of anthropomorphic thinking in discussions about ethical and responsible AI.\u003c/p\u003e\n\n\n\n\u003cp\u003eSome argue that this analogy needs to be revised to understand human learning and AI training. When humans read books, we do not make copies of them — we understand and internalize concepts. AI systems, on the other hand, must make actual copies of works — often obtained without permission or payment — encode them into their architecture and maintain these encoded versions to function. The works don’t disappear after “learning,” as AI companies often claim; they remain embedded in the \u003ca href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4963711\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esystem’s neural networks\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-business-blind-spot\"\u003eThe business blind spot\u003c/h2\u003e\n\n\n\n\u003cp\u003eAnthropomorphizing AI creates dangerous blind spots in business decision-making beyond simple operational inefficiencies. When executives and decision-makers think of AI as “creative” or “intelligent” in human terms, it can lead to a cascade of risky assumptions and potential legal liabilities.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-overestimating-ai-capabilities\"\u003eOverestimating AI capabilities\u003c/h2\u003e\n\n\n\n\u003cp\u003eOne critical area where anthropomorphizing creates risk is content generation and copyright compliance. When businesses view AI as capable of “learning” like humans, they might incorrectly assume that AI-generated content is automatically free from copyright concerns. This misunderstanding can lead companies to:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eDeploy AI systems that inadvertently reproduce copyrighted material, exposing the business to infringement claims\u003c/li\u003e\n\n\n\n\u003cli\u003eFail to implement proper content filtering and oversight mechanisms\u003c/li\u003e\n\n\n\n\u003cli\u003eAssume incorrectly that AI can reliably distinguish between public domain and copyrighted material\u003c/li\u003e\n\n\n\n\u003cli\u003eUnderestimate the need for human review in content generation processes\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch2 id=\"h-the-cross-border-compliance-blind-spot\"\u003eThe cross-border compliance blind spot\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe anthropomorphic bias in AI creates dangers when we consider cross-border compliance. As explained by Daniel Gervais, Haralambos Marmanis, Noam Shemtov, and Catherine Zaller Rowland in \u003ca href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4963711\"\u003e“The Heart of the Matter: Copyright, AI Training, and LLMs,\u003c/a\u003e” copyright law operates on strict territorial principles, with each jurisdiction maintaining its own rules about what constitutes infringement and what exceptions apply.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis territorial nature of copyright law creates a complex web of potential liability. Companies might mistakenly assume their AI systems can freely “learn” from copyrighted materials across jurisdictions, failing to recognize that training activities that are legal in one country may constitute infringement in another. The EU has recognized this risk in its AI Act, particularly through \u003ca href=\"https://ai-act-law.eu/recital/106/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRecital 106\u003c/a\u003e, which requires any general-purpose AI model offered in the EU to comply with EU copyright law regarding training data, regardless of where that training occurred.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis matters because anthropomorphizing AI’s capabilities can lead companies to underestimate or misunderstand their legal obligations across borders. The comfortable fiction of AI “learning” like humans obscures the reality that AI training involves complex copying and storage operations that trigger different legal obligations in other jurisdictions. This fundamental misunderstanding of AI’s actual functioning, combined with the territorial nature of copyright law, creates significant risks for businesses operating globally.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-human-cost\"\u003eThe human cost\u003c/h2\u003e\n\n\n\n\u003cp\u003eOne of the most concerning costs is the emotional toll of anthropomorphizing AI. We see increasing instances of people forming emotional attachments to AI chatbots, treating them as friends or confidants. This can be particularly \u003ca href=\"https://www.reuters.com/legal/mother-sues-ai-chatbot-company-characterai-google-sued-over-sons-suicide-2024-10-23/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edangerous for vulnerable individuals\u003c/a\u003e who might share personal information or rely on AI for emotional support it cannot provide. The AI’s responses, while seemingly empathetic, are sophisticated pattern matching based on training data — there is no genuine understanding or emotional connection.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis emotional vulnerability could also manifest in professional settings. As AI tools become more integrated into daily work, employees might develop inappropriate levels of trust in these systems, treating them as actual colleagues rather than tools. They might share confidential work information too freely or hesitate to report errors out of a misplaced sense of loyalty. While these scenarios remain isolated right now, they highlight how anthropomorphizing AI in the workplace could cloud judgment and create unhealthy dependencies on systems that, despite their sophisticated responses, are incapable of genuine understanding or care.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-breaking-free-from-the-anthropomorphic-trap\"\u003eBreaking free from the anthropomorphic trap\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo how do we move forward? First, we need to be more precise in our language about AI. Instead of saying an AI “learns” or “understands,” we might say it “processes data” or “generates outputs based on patterns in its training data.” This is not just pedantic — it helps clarify what these systems do.\u003c/p\u003e\n\n\n\n\u003cp\u003eSecond, we must evaluate AI systems based on what they are rather than what we imagine them to be. This means acknowledging both their impressive capabilities and their fundamental limitations. AI can process vast amounts of data and identify patterns humans might miss, but it cannot understand, reason or create in the way humans do.\u003c/p\u003e\n\n\n\n\u003cp\u003eFinally, we must develop frameworks and policies that address AI’s actual characteristics rather than imagined human-like qualities. This is particularly crucial in copyright law, where anthropomorphic thinking can lead to flawed analogies and inappropriate legal conclusions.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-path-forward\"\u003eThe path forward\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs AI systems become more sophisticated at mimicking human outputs, the temptation to anthropomorphize them will grow stronger. This anthropomorphic bias affects everything from how we evaluate AI’s capabilities to how we assess its risks. As we have seen, it extends into significant practical challenges around copyright law and business compliance. When we attribute human learning capabilities to AI systems, we must understand their fundamental nature and the technical reality of how they process and store information.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnderstanding AI for what it truly is — sophisticated information processing systems, not human-like learners — is crucial for all aspects of AI governance and deployment. By moving past anthropomorphic thinking, we can better address the challenges of AI systems, from ethical considerations and safety risks to cross-border copyright compliance and training data governance. This more precise understanding will help businesses make more informed decisions while supporting better policy development and public discourse around AI.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe sooner we embrace AI’s true nature, the better equipped we will be to navigate its profound societal implications and practical challenges in our global economy.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eRoanie Levy is licensing and legal advisor at \u003ca href=\"https://www.copyright.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCCC\u003c/a\u003e. \u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2736392\"\u003e\n\u003cp\u003e\u003cstrong\u003eDataDecisionMakers\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWelcome to the VentureBeat community!\u003c/p\u003e\n\n\n\n\u003cp\u003eDataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might even consider \u003ca rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://venturebeat.com/guest-posts/\"\u003econtributing an article\u003c/a\u003e of your own!\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca rel=\"noreferrer noopener\" href=\"https://venturebeat.com/category/DataDecisionMakers/\" target=\"_blank\"\u003eRead More From DataDecisionMakers\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2025-01-18T20:25:00Z",
  "modifiedTime": "2025-01-18T20:30:44Z"
}
