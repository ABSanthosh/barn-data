{
  "id": "0a5e8284-d1a2-46f6-bcc0-a5a6f426308c",
  "title": "Less is more: UC Berkeley and Google unlock LLM potential through simple sampling",
  "link": "https://venturebeat.com/ai/less-is-more-uc-berkeley-and-google-unlock-llm-potential-through-simple-sampling/",
  "description": "With multiple sampling and self-verification, Gemini 1.5 Pro can outperform o1-preview in reasoning tasks.",
  "author": "Ben Dickson",
  "published": "Fri, 21 Mar 2025 22:39:14 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI research",
    "AI, ML and Deep Learning",
    "chain of thought reasoning",
    "Google Research",
    "large language models",
    "large language models (LLMs)",
    "LLM reasoning",
    "LLMs",
    "reasoning models",
    "research",
    "sampling",
    "UC Berkeley"
  ],
  "byline": "Ben Dickson",
  "length": 7959,
  "excerpt": "With multiple sampling and self-verification, Gemini 1.5 Pro can outperform o1-preview in reasoning tasks.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "March 21, 2025 3:39 PM Image credit: VentureBeat with Imagen 3 Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More A new paper by researchers from Google Research and the University of California, Berkeley, demonstrates that a surprisingly simple test-time scaling approach can boost the reasoning abilities of large language models (LLMs). The key? Scaling up sampling-based search, a technique that relies on generating multiple responses and using the model itself to verify them.  The core finding is that even a minimalist implementation of sampling-based search, using random sampling and self-verification, can elevate the reasoning performance of models like Gemini 1.5 Pro beyond that of o1-Preview on popular benchmarks. The findings can have important implications for enterprise applications and challenge the assumption that highly specialized training or complex architectures are always necessary for achieving top-tier performance. The limits of current test-time compute scaling The current popular method for test-time scaling in LLMs is to train the model through reinforcement learning to generate longer responses with chain-of-thought (CoT) traces. This approach is used in models such as OpenAI o1 and DeepSeek-R1. While beneficial, these methods usually require substantial investment in the training phase. Another test-time scaling method is “self-consistency,” where the model generates multiple responses to the query and chooses the answer that appears more often. Self-consistency reaches its limits when handling complex problems, as in these cases, the most repeated answer is not necessarily the correct one. Sampling-based search offers a simpler and highly scalable alternative to test-time scaling: Let the model generate multiple responses and select the best one through a verification mechanism. Sampling-based search can complement other test-time compute scaling strategies and, as the researchers write in their paper, “it also has the unique advantage of being embarrassingly parallel and allowing for arbitrarily scaling: simply sample more responses.” More importantly, sampling-based search can be applied to any LLM, including those that have not been explicitly trained for reasoning. How sampling-based search works The researchers focus on a minimalist implementation of sampling-based search, using a language model to both generate candidate responses and verify them. This is a “self-verification” process, where the model assesses its own outputs without relying on external ground-truth answers or symbolic verification systems. Search-based sampling Credit: VentureBeat The algorithm works in a few simple steps:  1—The algorithm begins by generating a set of candidate solutions to the given problem using a language model. This is done by giving the model the same prompt multiple times and using a non-zero temperature setting to create a diverse set of responses. 2—Each candidate’s response undergoes a verification process in which the LLM is prompted multiple times to determine whether the response is correct. The verification outcomes are then averaged to create a final verification score for the response. 3— The algorithm selects the highest-scored response as the final answer. If multiple candidates are within close range of each other, the LLM is prompted to compare them pairwise and choose the best one. The response that wins the most pairwise comparisons is chosen as the final answer. The researchers considered two key axes for test-time scaling: Sampling: The number of responses the model generates for each input problem. Verification: The number of verification scores computed for each generated solution How sampling-based search compares to other techniques The study revealed that reasoning performance continues to improve with sampling-based search, even when test-time compute is scaled far beyond the point where self-consistency saturates.  At a sufficient scale, this minimalist implementation significantly boosts reasoning accuracy on reasoning benchmarks like AIME and MATH. For example, Gemini 1.5 Pro’s performance surpassed that of o1-Preview, which has explicitly been trained on reasoning problems, and Gemini 1.5 Flash surpassed Gemini 1.5 Pro. “This not only highlights the importance of sampling-based search for scaling capability, but also suggests the utility of sampling-based search as a simple baseline on which to compare other test-time compute scaling strategies and measure genuine improvements in models’ search capabilities,” the researchers write. It is worth noting that while the results of search-based sampling are impressive, the costs can also become prohibitive. For example, with 200 samples and 50 verification steps per sample, a query from AIME will generate around 130 million tokens, which costs $650 with Gemini 1.5 Pro. However, this is a very minimalistic approach to sampling-based search, and it is compatible with optimization techniques proposed in other studies. With smarter sampling and verification methods, the inference costs can be reduced considerably by using smaller models and generating fewer tokens. For example, by using Gemini 1.5 Flash to perform the verification, the costs drop to $12 per question. Effective self-verification strategies There is an ongoing debate on whether LLMs can verify their own answers. The researchers identified two key strategies for improving self-verification using test-time compute: Directly comparing response candidates: Disagreements between candidate solutions strongly indicate potential errors. By providing the verifier with multiple responses to compare, the model can better identify mistakes and hallucinations, addressing a core weakness of LLMs. The researchers describe this as an instance of “implicit scaling.” Task-specific rewriting: The researchers propose that the optimal output style of an LLM depends on the task. Chain-of-thought is effective for solving reasoning tasks, but responses are easier to verify when written in a more formal, mathematically conventional style. Verifiers can rewrite candidate responses into a more structured format (e.g., theorem-lemma-proof) before evaluation. “We anticipate model self-verification capabilities to rapidly improve in the short term, as models learn to leverage the principles of implicit scaling and output style suitability, and drive improved scaling rates for sampling-based search,” the researchers write. Implications for real-world applications The study demonstrates that a relatively simple technique can achieve impressive results, potentially reducing the need for complex and costly model architectures or training regimes. This is also a scalable technique, enabling enterprises to increase performance by allocating more compute resources to sampling and verification. It also enables developers to push frontier language models beyond their limitations on complex tasks. “Given that it complements other test-time compute scaling strategies, is parallelizable and allows for arbitrarily scaling, and admits simple implementations that are demonstrably effective, we expect sampling-based search to play a crucial role as language models are tasked with solving increasingly complex problems with increasingly large compute budgets,” the researchers write.  Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/03/LLM-self-verification.jpeg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-03-21T22:39:14+00:00\" datetime=\"2025-03-21T22:39:14+00:00\"\u003eMarch 21, 2025 3:39 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"455\" src=\"https://venturebeat.com/wp-content/uploads/2025/03/LLM-self-verification.jpeg?w=750\" alt=\"LLM self-verification\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with Imagen 3\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eA \u003ca href=\"https://arxiv.org/abs/2502.01839\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew paper\u003c/a\u003e by researchers from \u003ca href=\"https://research.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle Research\u003c/a\u003e and \u003ca href=\"https://www.berkeley.edu/\"\u003ethe University of California, Berkeley, \u003c/a\u003edemonstrates that a surprisingly simple test-time scaling approach can boost the reasoning abilities of large language models (LLMs). The key? Scaling up sampling-based search, a technique that relies on generating multiple responses and using the model itself to verify them. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe core finding is that even a minimalist implementation of sampling-based search, using random sampling and self-verification, can elevate the reasoning performance of models like Gemini 1.5 Pro beyond that of o1-Preview on popular benchmarks. The findings can have important implications for enterprise applications and challenge the assumption that highly specialized training or complex architectures are always necessary for achieving top-tier performance.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-limits-of-current-test-time-compute-scaling\"\u003eThe limits of current test-time compute scaling\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe current popular method for test-time scaling in LLMs is to train the model through reinforcement learning to generate longer responses with chain-of-thought (CoT) traces. This approach is used in models such as \u003ca href=\"https://venturebeat.com/programming-development/openai-opens-its-most-powerful-model-o1-up-to-third-party-developers/\"\u003eOpenAI o1\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/\"\u003eDeepSeek-R1\u003c/a\u003e. While beneficial, these methods usually require substantial investment in the training phase.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnother test-time scaling method is “self-consistency,” where the model generates multiple responses to the query and chooses the answer that appears more often. Self-consistency reaches its limits when handling complex problems, as in these cases, the most repeated answer is not necessarily the correct one.\u003c/p\u003e\n\n\n\n\u003cp\u003eSampling-based search offers a simpler and highly scalable alternative to test-time scaling: Let the model generate multiple responses and select the best one through a verification mechanism. Sampling-based search can complement other test-time compute scaling strategies and, as the researchers write in their paper, “it also has the unique advantage of being embarrassingly parallel and allowing for arbitrarily scaling: simply sample more responses.”\u003c/p\u003e\n\n\n\n\u003cp\u003eMore importantly, sampling-based search can be applied to any LLM, including those that have not been explicitly trained for reasoning.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-sampling-based-search-works\"\u003eHow sampling-based search works\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers focus on a minimalist implementation of sampling-based search, using a language model to both generate candidate responses and verify them. This is a “self-verification” process, where the model assesses its own outputs without relying on external ground-truth answers or symbolic verification systems.\u003c/p\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1466\" height=\"1260\" src=\"https://venturebeat.com/wp-content/uploads/2025/03/Screenshot-2025-03-21-at-6.43.49%E2%80%AFPM.png?w=698\" alt=\"Search-based sampling\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/03/Screenshot-2025-03-21-at-6.43.49 PM.png 1466w, https://venturebeat.com/wp-content/uploads/2025/03/Screenshot-2025-03-21-at-6.43.49 PM.png?resize=300,258 300w, https://venturebeat.com/wp-content/uploads/2025/03/Screenshot-2025-03-21-at-6.43.49 PM.png?resize=768,660 768w, https://venturebeat.com/wp-content/uploads/2025/03/Screenshot-2025-03-21-at-6.43.49 PM.png?resize=698,600 698w, https://venturebeat.com/wp-content/uploads/2025/03/Screenshot-2025-03-21-at-6.43.49 PM.png?resize=400,344 400w, https://venturebeat.com/wp-content/uploads/2025/03/Screenshot-2025-03-21-at-6.43.49 PM.png?resize=750,645 750w, https://venturebeat.com/wp-content/uploads/2025/03/Screenshot-2025-03-21-at-6.43.49 PM.png?resize=578,497 578w, https://venturebeat.com/wp-content/uploads/2025/03/Screenshot-2025-03-21-at-6.43.49 PM.png?resize=930,799 930w\" sizes=\"(max-width: 1466px) 100vw, 1466px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eSearch-based sampling Credit: VentureBeat\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eThe algorithm works in a few simple steps: \u003c/p\u003e\n\n\n\n\u003cp\u003e1—The algorithm begins by generating a set of candidate solutions to the given problem using a language model. This is done by giving the model the same prompt multiple times and using a non-zero temperature setting to create a diverse set of responses.\u003c/p\u003e\n\n\n\n\u003cp\u003e2—Each candidate’s response undergoes a verification process in which the LLM is prompted multiple times to determine whether the response is correct. The verification outcomes are then averaged to create a final verification score for the response.\u003c/p\u003e\n\n\n\n\u003cp\u003e3— The algorithm selects the highest-scored response as the final answer. If multiple candidates are within close range of each other, the LLM is prompted to compare them pairwise and choose the best one. The response that wins the most pairwise comparisons is chosen as the final answer.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers considered two key axes for test-time scaling:\u003c/p\u003e\n\n\n\n\u003cp\u003eSampling: The number of responses the model generates for each input problem.\u003c/p\u003e\n\n\n\n\u003cp\u003eVerification: The number of verification scores computed for each generated solution\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-sampling-based-search-compares-to-other-techniques\"\u003eHow sampling-based search compares to other techniques\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe study revealed that reasoning performance continues to improve with sampling-based search, even when test-time compute is scaled far beyond the point where self-consistency saturates. \u003c/p\u003e\n\n\n\n\u003cp\u003eAt a sufficient scale, this minimalist implementation significantly boosts reasoning accuracy on reasoning benchmarks like AIME and MATH. For example, Gemini 1.5 Pro’s performance surpassed that of o1-Preview, which has explicitly been trained on reasoning problems, and Gemini 1.5 Flash surpassed Gemini 1.5 Pro.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"2376\" height=\"552\" src=\"https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png 2376w, https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png?resize=300,70 300w, https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png?resize=768,178 768w, https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png?resize=800,186 800w, https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png?resize=1536,357 1536w, https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png?resize=2048,476 2048w, https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png?resize=400,93 400w, https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png?resize=750,174 750w, https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png?resize=578,134 578w, https://venturebeat.com/wp-content/uploads/2025/03/image_292ff6.png?resize=930,216 930w\" sizes=\"(max-width: 2376px) 100vw, 2376px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e“This not only highlights the importance of sampling-based search for scaling capability, but also suggests the utility of sampling-based search as a simple baseline on which to compare other test-time compute scaling strategies and measure genuine improvements in models’ search capabilities,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt is worth noting that while the results of search-based sampling are impressive, the costs can also become prohibitive. For example, with 200 samples and 50 verification steps per sample, a query from AIME will generate around 130 million tokens, which costs $650 with Gemini 1.5 Pro. However, this is a very minimalistic approach to sampling-based search, and it is compatible with optimization techniques proposed in other studies. With smarter sampling and verification methods, the inference costs can be reduced considerably by \u003ca href=\"https://venturebeat.com/ai/how-test-time-scaling-unlocks-hidden-reasoning-abilities-in-small-language-models-and-allows-them-to-outperform-llms/\"\u003eusing smaller models\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/not-every-ai-prompt-deserves-multiple-seconds-of-thinking-how-meta-is-teaching-models-to-prioritize/\"\u003egenerating fewer tokens\u003c/a\u003e. For example, by using Gemini 1.5 Flash to perform the verification, the costs drop to $12 per question.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-effective-self-verification-strategies\"\u003eEffective self-verification strategies\u003c/h2\u003e\n\n\n\n\u003cp\u003eThere is an ongoing debate on whether LLMs can verify their own answers. The researchers identified two key strategies for improving self-verification using test-time compute:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eDirectly comparing response candidates:\u003c/strong\u003e Disagreements between candidate solutions strongly indicate potential errors. By providing the verifier with multiple responses to compare, the model can better identify mistakes and hallucinations, addressing a core weakness of LLMs. The researchers describe this as an instance of “implicit scaling.”\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eTask-specific rewriting:\u003c/strong\u003e The researchers propose that the optimal output style of an LLM depends on the task. Chain-of-thought is effective for solving reasoning tasks, but responses are easier to verify when written in a more formal, mathematically conventional style. Verifiers can rewrite candidate responses into a more structured format (e.g., theorem-lemma-proof) before evaluation.\u003c/p\u003e\n\n\n\n\u003cp\u003e“We anticipate model self-verification capabilities to rapidly improve in the short term, as models learn to leverage the principles of implicit scaling and output style suitability, and drive improved scaling rates for sampling-based search,” the researchers write.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-implications-for-real-world-applications\"\u003eImplications for real-world applications\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe study demonstrates that a relatively simple technique can achieve impressive results, potentially reducing the need for complex and costly model architectures or training regimes.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis is also a scalable technique, enabling enterprises to increase performance by allocating more compute resources to sampling and verification. It also enables developers to push frontier language models beyond their limitations on complex tasks.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Given that it complements other test-time compute scaling strategies, is parallelizable and allows for arbitrarily scaling, and admits simple implementations that are demonstrably effective, we expect sampling-based search to play a crucial role as language models are tasked with solving increasingly complex problems with increasingly large compute budgets,” the researchers write. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2025-03-21T22:39:14Z",
  "modifiedTime": "2025-03-21T22:39:22Z"
}
