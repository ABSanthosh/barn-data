{
  "id": "265afed4-7da5-45a0-9ebc-6ddf1371d228",
  "title": "AWS doubles down on infrastructure as strategy in the AI race with SageMaker upgrades",
  "link": "https://venturebeat.com/ai/aws-doubles-down-on-infrastructure-as-strategy-in-the-ai-race-with-sagemaker-upgrades/",
  "description": "AWS upgraded its SageMaker platform to offer more observability and streamlined functions to make AI model inference and training easier.",
  "author": "Emilia David",
  "published": "Thu, 10 Jul 2025 21:37:51 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI inference",
    "AI infrastructure",
    "AI, ML and Deep Learning",
    "AWS",
    "AWS SageMaker",
    "cluster",
    "GPU cloud providers",
    "inference",
    "SageMaker"
  ],
  "byline": "Emilia David",
  "length": 5876,
  "excerpt": "AWS upgraded its SageMaker platform to offer more observability and streamlined functions to make AI model inference and training easier.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "July 10, 2025 2:37 PM LAS VEGAS, NEVADA - DECEMBER 3: Attendees walk through an expo hall at AWS re:Invent 2024, a conference hosted by Amazon Web Services, at The Venetian Las Vegas on December 3, 2024 in Las Vegas, Nevada. (Photo by Noah Berger/Getty Images for Amazon Web Services) Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now AWS seeks to extend its market position with updates to SageMaker, its machine learning and AI model training and inference platform, adding new observability capabilities, connected coding environments and GPU cluster performance management.  However, AWS continues to face competition from Google and Microsoft, which also offer many features that help accelerate AI training and inference.   SageMaker, which transformed into a unified hub for integrating data sources and accessing machine learning tools in 2024, will add features that provide insight into why model performance slows and offer AWS customers more control over the amount of compute allocated for model development. Other new features include connecting local integrated development environments (IDEs) to SageMaker, so locally written AI projects can be deployed on the platform.  SageMaker General Manager Ankur Mehrotra told VentureBeat that many of these new updates originated from customers themselves.  “One challenge that we’ve seen our customers face while developing Gen AI models is that when something goes wrong or when something is not working as per the expectation, it’s really hard to find what’s going on in that layer of the stack,” Mehrotra said. SageMaker HyperPod observability enables engineers to examine the various layers of the stack, such as the compute layer or networking layer. If anything goes wrong or models become slower, SageMaker can alert them and publish metrics on a dashboard. Mehrotra pointed to a real issue his own team faced while training new models, where training code began stressing GPUs, causing temperature fluctuations. He said that without the latest tools, developers would have taken weeks to identify the source of the issue and then fix it.  Connected IDEs SageMaker already offered two ways for AI developers to train and run models. It had access to fully managed IDEs, such as Jupyter Lab or Code Editor, to seamlessly run the training code on the models through SageMaker. Understanding that other engineers prefer to use their local IDEs, including all the extensions they have installed, AWS allowed them to run their code on their machines as well.  However, Mehrotra pointed out that it meant locally coded models only ran locally, so if developers wanted to scale, it proved to be a significant challenge.  AWS added new secure remote execution to allow customers to continue working on their preferred IDE — either locally or managed — and connect ot to SageMaker. “So this capability now gives them the best of both worlds where if they want, they can develop locally on a local IDE, but then in terms of actual task execution, they can benefit from the scalability of SageMaker,” he said.  More flexibility in compute AWS launched SageMaker HyperPod in December 2023 as a means to help customers manage clusters of servers for training models. Similar to providers like CoreWeave, HyperPod enables SageMaker customers to direct unused compute power to their preferred location. HyperPod knows when to schedule GPU usage based on demand patterns and allows organizations to balance their resources and costs effectively.  However, AWS said many customers wanted the same service for inference. Many inference tasks occur during the day when people use models and applications, while training is usually scheduled during off-peak hours.  Mehrotra noted that even in the world inference, developers can prioritize the inference tasks that HyperPod should focus on. Laurent Sifre, co-founder and CTO at AI agent company H AI, said in an AWS blog post that the company used SageMaker HyperPod when building out its agentic platform. “This seamless transition from training to inference streamlined our workflow, reduced time to production, and delivered consistent performance in live environments,” Sifre said.  AWS and the competition Amazon may not be offering the splashiest foundation models like its cloud provider rivals, Google and Microsoft. Still, AWS has been more focused on providing the infrastructure backbone for enterprises to build AI models, applications, or agents.  In addition to SageMaker, AWS also offers Bedrock, a platform specifically designed for building applications and agents.  SageMaker has been around for years, initially serving as a means to connect disparate machine learning tools to data lakes. As the generative AI boom began, AI engineers began using SageMaker to help train language models. However, Microsoft is pushing hard for its Fabric ecosystem, with 70% of Fortune 500 companies adopting it, to become a leader in the data and AI acceleration space. Google, through Vertex AI, has quietly made inroads in enterprise AI adoption. AWS, of course, has the advantage of being the most widely used cloud provider. Any updates that would make its many AI infrastructure platforms easier to use will always be a benefit.  Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/776246284_03.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-07-10T21:37:51+00:00\" datetime=\"2025-07-10T21:37:51+00:00\"\u003eJuly 10, 2025 2:37 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"400\" height=\"267\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/776246284_03.jpg?w=400\" alt=\"AWS logo in lower case sans serif white font with Amazon smile below it against a purple glowing backdrop with blurry conference attendees walking in front\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eLAS VEGAS, NEVADA - DECEMBER 3: Attendees walk through an expo hall at AWS re:Invent 2024, a conference hosted by Amazon Web Services, at The Venetian Las Vegas on December 3, 2024 in Las Vegas, Nevada. (Photo by Noah Berger/Getty Images for Amazon Web Services)\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\u003c/em\u003e \u003cem\u003e\u003ca href=\"https://venturebeat.com/newsletters/\"\u003eSubscribe Now\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://aws.amazon.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAWS\u003c/a\u003e seeks to extend its market position with \u003ca href=\"https://aws.amazon.com/blogs/machine-learning/new-capabilities-in-amazon-sagemaker-ai-continue-to-transform-how-organizations-develop-ai-models/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eupdates to SageMaker\u003c/a\u003e, its machine learning and AI model training and inference platform, adding new observability capabilities, connected coding environments and GPU cluster performance management. \u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, AWS continues to face competition from \u003ca href=\"https://www.google.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle\u003c/a\u003e and \u003ca href=\"https://www.microsoft.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMicrosoft\u003c/a\u003e, which also offer many features that help accelerate AI training and inference.  \u003c/p\u003e\n\n\n\n\u003cp\u003eSageMaker, which \u003ca href=\"https://venturebeat.com/data-infrastructure/aws-sagemaker-is-transforming-into-a-combined-data-and-ai-hub/\"\u003etransformed into a unified hub\u003c/a\u003e for integrating data sources and accessing machine learning tools in 2024, will add features that provide insight into why model performance slows and offer AWS customers more control over the amount of compute allocated for model development.\u003c/p\u003e\n\n\n\n\u003cp\u003eOther new features include connecting local integrated development environments (IDEs) to SageMaker, so locally written AI projects can be deployed on the platform. \u003c/p\u003e\n\n\n\n\u003cp\u003eSageMaker General Manager Ankur Mehrotra told VentureBeat that many of these new updates originated from customers themselves. \u003c/p\u003e\n\n\n\n\u003cp\u003e“One challenge that we’ve seen our customers face while developing Gen AI models is that when something goes wrong or when something is not working as per the expectation, it’s really hard to find what’s going on in that layer of the stack,” Mehrotra said.\u003c/p\u003e\n\n\n\n\u003cp\u003eSageMaker HyperPod observability enables engineers to examine the various layers of the stack, such as the compute layer or networking layer. If anything goes wrong or models become slower, SageMaker can alert them and publish metrics on a dashboard.\u003c/p\u003e\n\n\n\n\u003cp\u003eMehrotra pointed to a real issue his own team faced while training new models, where training code began stressing GPUs, causing temperature fluctuations. He said that without the latest tools, developers would have taken weeks to identify the source of the issue and then fix it. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-connected-ides\"\u003eConnected IDEs\u003c/h2\u003e\n\n\n\n\u003cp\u003eSageMaker already offered two ways for AI developers to train and run models. It had access to fully managed IDEs, such as Jupyter Lab or Code Editor, to seamlessly run the training code on the models through SageMaker. Understanding that other engineers prefer to use their local IDEs, including all the extensions they have installed, AWS allowed them to run their code on their machines as well. \u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, Mehrotra pointed out that it meant locally coded models only ran locally, so if developers wanted to scale, it proved to be a significant challenge. \u003c/p\u003e\n\n\n\n\u003cp\u003eAWS added new secure remote execution to allow customers to continue working on their preferred IDE — either locally or managed — and connect ot to SageMaker.\u003c/p\u003e\n\n\n\n\u003cp\u003e“So this capability now gives them the best of both worlds where if they want, they can develop locally on a local IDE, but then in terms of actual task execution, they can benefit from the scalability of SageMaker,” he said. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-more-flexibility-in-compute\"\u003eMore flexibility in compute\u003c/h2\u003e\n\n\n\n\u003cp\u003eAWS launched SageMaker HyperPod in \u003ca href=\"https://venturebeat.com/ai/amazon-awss-barrage-of-gen-ai-announcements-aim-to-outdo-microsoft/\"\u003eDecember 2023\u003c/a\u003e as a means to help customers manage clusters of servers for training models. Similar to providers like \u003ca href=\"https://www.coreweave.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCoreWeave\u003c/a\u003e, HyperPod enables SageMaker customers to direct unused compute power to their preferred location. HyperPod knows when to schedule GPU usage based on demand patterns and allows organizations to balance their resources and costs effectively. \u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, AWS said many customers wanted the same service for inference. Many inference tasks occur during the day when people use models and applications, while training is usually scheduled during off-peak hours. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\n\u003ciframe title=\"Amazon Trains Nova Foundation Models at Scale with SageMaker HyperPod | Amazon Web Services\" width=\"500\" height=\"281\" src=\"https://www.youtube.com/embed/aS1EU_kkGcI?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003c/p\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eMehrotra noted that even in the world inference, developers can prioritize the inference tasks that HyperPod should focus on.\u003c/p\u003e\n\n\n\n\u003cp\u003eLaurent Sifre, co-founder and CTO at AI agent company \u003ca href=\"https://runner.hcompany.ai/mcps\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eH AI\u003c/a\u003e, said in an AWS blog post that the company used SageMaker HyperPod when building out its agentic platform.\u003c/p\u003e\n\n\n\n\u003cp\u003e“This seamless transition from training to inference streamlined our workflow, reduced time to production, and delivered consistent performance in live environments,” Sifre said. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-aws-and-the-competition\"\u003eAWS and the competition\u003c/h2\u003e\n\n\n\n\u003cp\u003eAmazon may not be offering the splashiest foundation models like its cloud provider rivals, Google and Microsoft. Still, AWS has been more focused on providing the infrastructure backbone for enterprises to build \u003ca href=\"https://venturebeat.com/ai/enterprise-giants-atlassian-intuit-and-aws-are-planning-for-a-world-where-agents-call-the-apis/\"\u003eAI models, applications, or agents\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn addition to SageMaker, AWS also \u003ca href=\"https://venturebeat.com/ai/aws-brings-multi-agent-orchestration-to-bedrock/\"\u003eoffers Bedrock\u003c/a\u003e, a platform specifically designed for building applications and agents. \u003c/p\u003e\n\n\n\n\u003cp\u003eSageMaker has been around for years, initially serving as a means to connect disparate machine learning tools to data lakes. As the generative AI boom began, AI engineers began using SageMaker to help train language models. However, Microsoft is pushing hard for its Fabric ecosystem, \u003ca href=\"https://venturebeat.com/ai/70-of-the-fortune-500-already-use-microsoft-fabric-and-its-now-getting-even-more-features-including-cosmosdb-support/\"\u003ewith 70% of Fortune 500 companies adopting it\u003c/a\u003e, to become a leader in the data and AI acceleration space. Google, through Vertex AI, has quietly made \u003ca href=\"https://venturebeat.com/ai/from-catch-up-to-catch-us-how-google-quietly-took-the-lead-in-enterprise-ai/\"\u003einroads in enterprise AI adoption\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eAWS, of course, has the advantage of being the \u003ca href=\"https://venturebeat.com/ai/aws-ai-takeover-5-cloud-winning-plays-theyre-using-to-dominate-the-market/\"\u003emost widely used cloud provider\u003c/a\u003e. Any updates that would make its many AI infrastructure platforms easier to use will always be a benefit. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-07-10T21:37:51Z",
  "modifiedTime": "2025-07-10T21:37:56Z"
}
