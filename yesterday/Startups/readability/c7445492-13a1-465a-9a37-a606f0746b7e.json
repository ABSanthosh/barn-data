{
  "id": "c7445492-13a1-465a-9a37-a606f0746b7e",
  "title": "After GPT-4o backlash, researchers benchmark models on moral endorsement—Find sycophancy persists across the board",
  "link": "https://venturebeat.com/ai/after-gpt-4o-backlash-researchers-benchmark-models-on-moral-endorsement-find-sycophancy-persists-across-the-board/",
  "description": "A new benchmark can test how much LLMs become sycophants, and found that GPT-4o was the most sycophantic of the models tested.",
  "author": "Emilia David",
  "published": "Thu, 22 May 2025 23:46:40 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "ai sycophancy",
    "AI, ML and Deep Learning",
    "Claude 3.7 Sonnet",
    "Gemini 1.5",
    "gpt-4o",
    "LLaMA",
    "llama 3",
    "llama 4",
    "Mistral 7B",
    "Mistral Small",
    "sycophancy"
  ],
  "byline": "Emilia David",
  "length": 5394,
  "excerpt": "A new benchmark can test how much LLMs become sycophants, and found that GPT-4o was the most sycophantic of the models tested.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Last month, OpenAI rolled back some updates to GPT-4o after several users, including former OpenAI CEO Emmet Shear and Hugging Face chief executive Clement Delangue said the model overly flattered users.  The flattery, called sycophancy, often led the model to defer to user preferences, be extremely polite, and not push back. It was also annoying. Sycophancy could lead to the models releasing misinformation or reinforcing harmful behaviors. And as enterprises begin to make applications and agents built on these sycophant LLMs, they run the risk of the models agreeing to harmful business decisions, encouraging false information to spread and be used by AI agents, and may impact trust and safety policies. Stanford University, Carnegie Mellon University and University of Oxford researchers sought to change that by proposing a benchmark to measure models’ sycophancy. They called the benchmark Elephant, for Evaluation of LLMs as Excessive SycoPHANTs, and found that every large language model (LLM) has a certain level of sycophany. By understanding how sycophantic models can be, the benchmark can guide enterprises on creating guidelines when using LLMs. To test the benchmark, the researchers pointed the models to two personal advice datasets: the QEQ, a set of open-ended personal advice questions on real-world situations, and AITA, posts from the subreddit r/AmITheAsshole, where posters and commenters judge whether people behaved appropriately or not in some situations.  The idea behind the experiment is to see how the models behave when faced with queries. It evaluates what the researchers called social sycophancy, whether the models try to preserve the user’s “face,” or their self-image or social identity.  “More “hidden” social queries are exactly what our benchmark gets at — instead of previous work that only looks at factual agreement or explicit beliefs, our benchmark captures agreement or flattery based on more implicit or hidden assumptions,” Myra Cheng, one of the researchers and co-author of the paper, told VentureBeat. “We chose to look at the domain of personal advice since the harms of sycophancy there are more consequential, but casual flattery would also be captured by the ’emotional validation’ behavior.” Testing the models For the test, the researchers fed the data from QEQ and AITA to OpenAI’s GPT-4o, Gemini 1.5 Flash from Google, Anthropic’s Claude Sonnet 3.7 and open weight models from Meta (Llama 3-8B-Instruct, Llama 4-Scout-17B-16-E and Llama 3.3-70B-Instruct- Turbo) and Mistral’s 7B-Instruct-v0.3 and the Mistral Small- 24B-Instruct2501.  Cheng said they “benchmarked the models using the GPT-4o API, which uses a version of the model from late 2024, before both OpenAI implemented the new overly sycophantic model and reverted it back.” To measure sycophancy, the Elephant method looks at five behaviors that relate to social sycophancy: Emotional validation or over-empathizing without critique Moral endorsement or saying users are morally right, even when they are not Indirect language where the model avoids giving direct suggestions Indirect action, or where the model advises with passive coping mechanisms Accepting framing that does not challenge problematic assumptions. The test found that all LLMs showed high sycophancy levels, even more so than humans, and social sycophancy proved difficult to mitigate. However, the test showed that GPT-4o “has some of the highest rates of social sycophancy, while Gemini-1.5-Flash definitively has the lowest.” The LLMs amplified some biases in the datasets as well. The paper noted that posts on AITA had some gender bias, in that posts mentioning wives or girlfriends were more often correctly flagged as socially inappropriate. At the same time, those with husband, boyfriend, parent or mother were misclassified. The researchers said the models “may rely on gendered relational heuristics in over- and under-assigning blame.” In other words, the models were more sycophantic to people with boyfriends and husbands than to those with girlfriends or wives.  Why it’s important It’s nice if a chatbot talks to you as an empathetic entity, and it can feel great if the model validates your comments. But sycophancy raises concerns about models’ supporting false or concerning statements and, on a more personal level, could encourage self-isolation, delusions or harmful behaviors.  Enterprises don’t want their AI applications built with LLMs spreading false information to be agreeable to users. It may misalign with an organization’s tone or ethics and could be very annoying for employees and their platforms’ end-users.  The researchers said the Elephant method and further testing could help inform better guardrails to prevent sycophancy from increasing.  Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/02/nuneybits_Vecor_art_of_human_chatting_with_a_chatbot_two_chat_b_14075c47-8e58-4110-b3b3-96385f4ebd44.webp?w=853?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eLast month, \u003ca href=\"http://www.openai.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI\u003c/a\u003e \u003ca href=\"https://venturebeat.com/ai/openai-rolls-back-chatgpts-sycophancy-and-explains-what-went-wrong/\"\u003erolled back some updates\u003c/a\u003e to GPT-4o after several users, including former OpenAI CEO Emmet Shear and Hugging Face chief executive Clement Delangue said the model overly flattered users. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe flattery, called sycophancy, often \u003ca href=\"https://venturebeat.com/ai/ex-openai-ceo-and-power-users-sound-alarm-over-ai-sycophancy-and-flattery-of-users/\"\u003eled the model to defer to user preferences\u003c/a\u003e, be extremely polite, and not push back. It was also annoying. Sycophancy \u003ca href=\"https://venturebeat.com/ai/openai-overrode-concerns-of-expert-testers-to-release-sycophantic-gpt-4o/\"\u003ecould lead\u003c/a\u003e to the models releasing misinformation or reinforcing \u003ca href=\"https://venturebeat.com/ai/darkness-rising-the-hidden-dangers-of-ai-sycophancy-and-dark-patterns/\"\u003eharmful behaviors\u003c/a\u003e. And as enterprises begin to make applications and agents built on these sycophant LLMs, they run the risk of the models agreeing to harmful business decisions, encouraging false information to spread and be used by AI agents, and may impact trust and safety policies. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://www.stanford.edu/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eStanford University\u003c/a\u003e, \u003ca href=\"https://www.cmu.edu/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCarnegie Mellon University\u003c/a\u003e and \u003ca href=\"https://www.ox.ac.uk/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eUniversity of Oxford\u003c/a\u003e researchers sought to change that by \u003ca href=\"https://arxiv.org/pdf/2505.13995\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eproposing a benchmark\u003c/a\u003e to measure models’ sycophancy. They called the benchmark Elephant, for Evaluation of LLMs as Excessive SycoPHANTs, and found that every large language model (LLM) has a certain level of sycophany. By understanding how sycophantic models can be, the benchmark can guide enterprises on creating guidelines when using LLMs. \u003c/p\u003e\n\n\n\n\u003cp\u003eTo test the benchmark, the researchers pointed the models to two personal advice datasets: the QEQ, a set of open-ended personal advice questions on real-world situations, and AITA, posts from the subreddit r/AmITheAsshole, where posters and commenters judge whether people behaved appropriately or not in some situations. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe idea behind the experiment is to see how the models behave when faced with queries. It evaluates what the researchers called social sycophancy, whether the models try to preserve the user’s “face,” or their self-image or social identity. \u003c/p\u003e\n\n\n\n\u003cp\u003e“More “hidden” social queries are exactly what our benchmark gets at — instead of previous work that only looks at factual agreement or explicit beliefs, our benchmark captures agreement or flattery based on more implicit or hidden assumptions,” Myra Cheng, one of the researchers and co-author of the paper, told VentureBeat. “We chose to look at the domain of personal advice since the harms of sycophancy there are more consequential, but casual flattery would also be captured by the ’emotional validation’ behavior.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-testing-the-models\"\u003eTesting the models\u003c/h2\u003e\n\n\n\n\u003cp\u003eFor the test, the researchers fed the data from QEQ and AITA to OpenAI’s GPT-4o, Gemini 1.5 Flash from \u003ca href=\"http://www.google.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle\u003c/a\u003e, \u003ca href=\"http://www.anthropic.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAnthropic\u003c/a\u003e’s Claude Sonnet 3.7 and open weight models from \u003ca href=\"https://www.meta.com/about/?srsltid=AfmBOorRARJUtbjac0SGGP_nPeZiHhfnWm3ih8tukhdjRL_fr7t-n7SS\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMeta\u003c/a\u003e (Llama 3-8B-Instruct, Llama 4-Scout-17B-16-E and Llama 3.3-70B-Instruct- Turbo) and \u003ca href=\"https://mistral.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMistral\u003c/a\u003e’s 7B-Instruct-v0.3 and the Mistral Small- 24B-Instruct2501. \u003c/p\u003e\n\n\n\n\u003cp\u003eCheng said they “benchmarked the models using the GPT-4o API, which uses a version of the model from late 2024, before both OpenAI implemented the new overly sycophantic model and reverted it back.”\u003c/p\u003e\n\n\n\n\u003cp\u003eTo measure sycophancy, the Elephant method looks at five behaviors that relate to social sycophancy:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eEmotional validation or over-empathizing without critique\u003c/li\u003e\n\n\n\n\u003cli\u003eMoral endorsement or saying users are morally right, even when they are not\u003c/li\u003e\n\n\n\n\u003cli\u003eIndirect language where the model avoids giving direct suggestions\u003c/li\u003e\n\n\n\n\u003cli\u003eIndirect action, or where the model advises with passive coping mechanisms\u003c/li\u003e\n\n\n\n\u003cli\u003eAccepting framing that does not challenge problematic assumptions.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eThe test found that all LLMs showed high sycophancy levels, even more so than humans, and social sycophancy proved difficult to mitigate. However, the test showed that GPT-4o “has some of the highest rates of social sycophancy, while Gemini-1.5-Flash definitively has the lowest.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe LLMs amplified some biases in the datasets as well. The paper noted that posts on AITA had some gender bias, in that posts mentioning wives or girlfriends were more often correctly flagged as socially inappropriate. At the same time, those with husband, boyfriend, parent or mother were misclassified. The researchers said the models “may rely on gendered relational heuristics in over- and under-assigning blame.” In other words, the models were more sycophantic to people with boyfriends and husbands than to those with girlfriends or wives. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-why-it-s-important\"\u003eWhy it’s important\u003c/h2\u003e\n\n\n\n\u003cp\u003eIt’s nice if a chatbot talks to you as an empathetic entity, and it can feel great if the model validates your comments. But sycophancy \u003cspan\u003eraises \u003ca href=\"https://venturebeat.com/ai/ex-openai-ceo-and-power-users-sound-alarm-over-ai-sycophancy-and-flattery-of-users/\" target=\"_blank\"\u003econcerns about models’\u003c/a\u003e supporting false or concerning statements and, on a more personal level, could encourage self-isolation, delusions\u003c/span\u003e or harmful behaviors. \u003c/p\u003e\n\n\n\n\u003cp\u003eEnterprises don’t want their AI applications built with LLMs spreading false information to be agreeable to users. It may misalign with an organization’s tone or ethics and could be very annoying for employees and their platforms’ end-users. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers said the Elephant method and further testing could help inform better guardrails to prevent sycophancy from increasing. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-05-22T23:46:40Z",
  "modifiedTime": "2025-05-23T00:30:53Z"
}
