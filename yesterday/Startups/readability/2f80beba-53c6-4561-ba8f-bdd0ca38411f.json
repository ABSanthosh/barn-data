{
  "id": "2f80beba-53c6-4561-ba8f-bdd0ca38411f",
  "title": "DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch",
  "link": "https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/",
  "description": "The new model shows open-source closing in on closed-source models, suggesting reduced chances of one big AI player ruling the game.",
  "author": "Shubham Sharma",
  "published": "Thu, 26 Dec 2024 18:46:47 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI, ML and Deep Learning",
    "artificial intelligence",
    "category-/News",
    "Chinese AI",
    "claude 3.5 sonnet",
    "Conversational AI",
    "Deepseek",
    "DeepSeek-V2",
    "DeepSeek-V3",
    "Generative AI",
    "gpt-4o",
    "language models",
    "large language models",
    "Llama-3.1",
    "Llama-3.1-405B",
    "Meta",
    "NLP",
    "Open AI",
    "open-source AI",
    "OpenAI"
  ],
  "byline": "Shubham Sharma",
  "length": 6081,
  "excerpt": "The new model shows open-source closing in on closed-source models, suggesting reduced chances of one big AI player ruling the game.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 26, 2024 10:46 AM Credit: VentureBeat made with Midjourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Chinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3. Available via Hugging Face under the company’s license agreement, the new model comes with 671B parameters but uses a mixture-of-experts architecture to activate only select parameters, in order to handle given tasks accurately and efficiently. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including Meta’s Llama 3.1-405B, and closely matching the performance of closed models from Anthropic and OpenAI. The release marks another major development closing the gap between closed and open-source AI. Ultimately, DeepSeek, which started as an offshoot of Chinese quantitative hedge fund High-Flyer Capital Management, hopes these developments will pave the way for artificial general intelligence (AGI), where models will have the ability to understand or learn any intellectual task that a human being can. What does DeepSeek-V3 bring to the table? Just like its predecessor DeepSeek-V2, the new ultra-large model uses the same basic architecture revolving around multi-head latent attention (MLA) and DeepSeekMoE. This approach ensures it maintains efficient training and inference — with specialized and shared “experts” (individual, smaller neural networks within the larger model) activating 37B parameters out of 671B for each token. While the basic architecture ensures robust performance for DeepSeek-V3, the company has also debuted two innovations to further push the bar. The first is an auxiliary loss-free load-balancing strategy. This dynamically monitors and adjusts the load on experts to utilize them in a balanced way without compromising overall model performance. The second is multi-token prediction (MTP), which allows the model to predict multiple future tokens simultaneously. This innovation not only enhances the training efficiency but enables the model to perform three times faster, generating 60 tokens per second. “During pre-training, we trained DeepSeek-V3 on 14.8T high-quality and diverse tokens…Next, we conducted a two-stage context length extension for DeepSeek-V3,” the company wrote in a technical paper detailing the new model. “In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K. Following this, we conducted post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential. During the post-training stage, we distill the reasoning capability from the DeepSeekR1 series of models, and meanwhile carefully maintain the balance between model accuracy and generation length.” Notably, during the training phase, DeepSeek used multiple hardware and algorithmic optimizations, including the FP8 mixed precision training framework and the DualPipe algorithm for pipeline parallelism, to cut down on the costs of the process. Overall, it claims to have completed DeepSeek-V3’s entire training in about 2788K H800 GPU hours, or about $5.57 million, assuming a rental price of $2 per GPU hour. This is much lower than the hundreds of millions of dollars usually spent on pre-training large language models. Llama-3.1, for instance, is estimated to have been trained with an investment of over $500 million.  Strongest open-source model currently available Despite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market. The company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B. It even outperforms closed-source GPT-4o on most benchmarks, except English-focused SimpleQA and FRAMES — where the OpenAI model sat ahead with scores of 38.2 and 80.5 (vs 24.9 and 73.3), respectively. Notably, DeepSeek-V3’s performance particularly stood out on the Chinese and math-centric benchmarks, scoring better than all counterparts. In the Math-500 test, it scored 90.2, with Qwen’s score of 80 the next best.  The only model that managed to challenge DeepSeek-V3 was Anthropic’s Claude 3.5 Sonnet, outperforming it with higher scores in MMLU-Pro, IF-Eval, GPQA-Diamond, SWE Verified and Aider-Edit. https://twitter.com/deepseek_ai/status/1872242657348710721 The work shows that open-source is closing in on closed-source models, promising nearly equivalent performance across different tasks. The development of such systems is extremely good for the industry as it potentially eliminates the chances of one big AI player ruling the game. It also gives enterprises multiple options to choose from and work with while orchestrating their stacks. Currently, the code for DeepSeek-V3 is available via GitHub under an MIT license, while the model is being provided under the company’s model license. Enterprises can also test out the new model via DeepSeek Chat, a ChatGPT-like platform, and access the API for commercial use. DeepSeek is providing the API at the same price as DeepSeek-V2 until February 8. After that, it will charge $0.27/million input tokens ($0.07/million tokens with cache hits) and $1.10/million output tokens. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/11/cfr0z3n_vector_art_line_art_flat_illustration_graphic_novel_spl_5e4ba6f6-8ff9-4899-a927-5e1aba8fb9e0.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-26T18:46:47+00:00\" datetime=\"2024-12-26T18:46:47+00:00\"\u003eDecember 26, 2024 10:46 AM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"400\" height=\"224\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/cfr0z3n_vector_art_line_art_flat_illustration_graphic_novel_spl_5e4ba6f6-8ff9-4899-a927-5e1aba8fb9e0.png?w=400\" alt=\"Black and white AI vector image of robot jumping over the heads of onlookers in a city\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eChinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3.\u003c/p\u003e\n\n\n\n\u003cp\u003eAvailable via \u003ca href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3-Base\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e under the company’s license agreement, the new model comes with 671B parameters but uses a mixture-of-experts architecture to activate only select parameters, in order to handle given tasks accurately and efficiently. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including \u003ca href=\"https://venturebeat.com/ai/meta-unleashes-its-most-powerful-ai-model-llama-3-1-with-405b-parameters/\"\u003eMeta’s Llama 3.1-405B\u003c/a\u003e, and closely matching the performance of closed models from Anthropic and OpenAI.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe release marks another major development closing the gap between closed and open-source AI. Ultimately, DeepSeek, which started as an offshoot of Chinese quantitative hedge fund \u003ca href=\"https://www.highflyercapital.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHigh-Flyer Capital Management\u003c/a\u003e, hopes these developments will pave the way for artificial general intelligence (AGI), where models will have the ability to understand or learn any intellectual task that a human being can.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-does-deepseek-v3-bring-to-the-table\"\u003eWhat does DeepSeek-V3 bring to the table?\u003c/h2\u003e\n\n\n\n\u003cp\u003eJust like its predecessor DeepSeek-V2, the new ultra-large model uses the same basic architecture revolving around \u003ca href=\"https://arxiv.org/html/2405.04434v2\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emulti-head latent attention (MLA)\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/2401.06066\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDeepSeekMoE\u003c/a\u003e. This approach ensures it maintains efficient training and inference — with specialized and shared “experts” (individual, smaller neural networks within the larger model) activating 37B parameters out of 671B for each token.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile the basic architecture ensures robust performance for DeepSeek-V3, the company has also debuted two innovations to further push the bar. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe first is an auxiliary loss-free load-balancing strategy. This dynamically monitors and adjusts the load on experts to utilize them in a balanced way without compromising overall model performance. The second is multi-token prediction (MTP), which allows the model to predict multiple future tokens simultaneously. This innovation not only enhances the training efficiency but enables the model to perform three times faster, generating 60 tokens per second.\u003c/p\u003e\n\n\n\n\u003cp\u003e“During pre-training, we trained DeepSeek-V3 on 14.8T high-quality and diverse tokens…Next, we conducted a two-stage context length extension for DeepSeek-V3,” the company wrote in a \u003ca href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etechnical paper\u003c/a\u003e detailing the new model. “In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K. Following this, we conducted post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential. During the post-training stage, we distill the reasoning capability from the \u003ca href=\"https://venturebeat.com/ai/deepseeks-first-reasoning-model-r1-lite-preview-turns-heads-beating-openai-o1-performance/\"\u003eDeepSeekR1 series of models\u003c/a\u003e, and meanwhile carefully maintain the balance between model accuracy and generation length.”\u003c/p\u003e\n\n\n\n\u003cp\u003eNotably, during the training phase, DeepSeek used multiple hardware and algorithmic optimizations, including the FP8 mixed precision training framework and the DualPipe algorithm for pipeline parallelism, to cut down on the costs of the process.\u003c/p\u003e\n\n\n\n\u003cp\u003eOverall, it claims to have completed DeepSeek-V3’s entire training in about 2788K H800 GPU hours, or about $5.57 million, assuming a rental price of $2 per GPU hour. This is much lower than the hundreds of millions of dollars usually spent on pre-training large language models.\u003c/p\u003e\n\n\n\n\u003cp\u003eLlama-3.1, for instance, is estimated to have been trained with an investment of over $500 million. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-strongest-open-source-model-currently-available\"\u003eStrongest open-source model currently available\u003c/h2\u003e\n\n\n\n\u003cp\u003eDespite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B. It even outperforms closed-source \u003ca href=\"https://venturebeat.com/ai/openai-announces-new-free-model-gpt-4o-and-chatgpt-for-desktop/\"\u003eGPT-4o\u003c/a\u003e on most benchmarks, except English-focused SimpleQA and FRAMES — where the OpenAI model sat ahead with scores of 38.2 and 80.5 (vs 24.9 and 73.3), respectively.\u003c/p\u003e\n\n\n\n\u003cp\u003eNotably, DeepSeek-V3’s performance particularly stood out on the Chinese and math-centric benchmarks, scoring better than all counterparts. In the Math-500 test, it scored 90.2, with Qwen’s score of 80 the next best. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe only model that managed to challenge DeepSeek-V3 was \u003ca href=\"https://venturebeat.com/ai/anthropic-unveils-claude-3-5-sonnet-pushing-the-boundaries-of-ai-capabilities-and-affordability/\"\u003eAnthropic’s Claude 3.5 Sonnet\u003c/a\u003e, outperforming it with higher scores in MMLU-Pro, IF-Eval, GPQA-Diamond, SWE Verified and Aider-Edit.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\nhttps://twitter.com/deepseek_ai/status/1872242657348710721\n\u003c/p\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe work shows that open-source is closing in on closed-source models, promising nearly equivalent performance across different tasks. The development of such systems is extremely good for the industry as it potentially eliminates the chances of one big AI player ruling the game. It also gives enterprises multiple options to choose from and work with while orchestrating their stacks.\u003c/p\u003e\n\n\n\n\u003cp\u003eCurrently, the code for DeepSeek-V3 is available via \u003ca href=\"https://t.co/9iwEF6aLuk\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGitHub\u003c/a\u003e under an MIT license, while the model is being provided under the company’s model license. Enterprises can also test out the new model via \u003ca href=\"http://chat.deepseek.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDeepSeek Chat\u003c/a\u003e, a ChatGPT-like platform, and access the API for commercial use. DeepSeek is providing the API at the \u003ca href=\"https://twitter.com/deepseek_ai/status/1872242663489188088/photo/2\"\u003esame price as DeepSeek-V2\u003c/a\u003e until February 8. After that, it will charge $0.27/million input tokens ($0.07/million tokens with cache hits) and $1.10/million output tokens.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"546\" height=\"618\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/Screen-Shot-2024-12-26-at-1.24.36-PM.png?w=530\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/Screen-Shot-2024-12-26-at-1.24.36-PM.png 546w, https://venturebeat.com/wp-content/uploads/2024/12/Screen-Shot-2024-12-26-at-1.24.36-PM.png?resize=300,340 300w, https://venturebeat.com/wp-content/uploads/2024/12/Screen-Shot-2024-12-26-at-1.24.36-PM.png?resize=530,600 530w, https://venturebeat.com/wp-content/uploads/2024/12/Screen-Shot-2024-12-26-at-1.24.36-PM.png?resize=400,453 400w\" sizes=\"(max-width: 546px) 100vw, 546px\"/\u003e\u003c/figure\u003e\n\n\n\n\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-12-26T18:46:47Z",
  "modifiedTime": "2024-12-26T18:46:55Z"
}
