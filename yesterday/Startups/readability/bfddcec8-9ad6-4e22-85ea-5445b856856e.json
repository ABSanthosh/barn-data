{
  "id": "bfddcec8-9ad6-4e22-85ea-5445b856856e",
  "title": "Cohere launches new AI models to bridge global language divide",
  "link": "https://venturebeat.com/ai/cohere-launches-new-ai-models-to-bridge-global-language-divide/",
  "description": "Cohere released two new open weight AI models from its Aya initiative that looks to expand LLM performance for languages other than English.",
  "author": "Emilia David",
  "published": "Thu, 24 Oct 2024 23:12:19 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Data Infrastructure",
    "AI, ML and Deep Learning",
    "Aya Project",
    "category-/Science",
    "Cohere",
    "Cohere AI",
    "Data Management",
    "large language models",
    "LLMs",
    "non-english languages",
    "Retrieval-augmented generation (RAG)",
    "Synthetic Data"
  ],
  "byline": "Emilia David",
  "length": 4583,
  "excerpt": "Cohere released two new open weight AI models from its Aya initiative that looks to expand LLM performance for languages other than English.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "October 24, 2024 4:12 PM Credit: VentureBeat made with Midjourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Cohere today released two new open-weight models in its Aya project to close the language gap in foundation models.  Aya Expanse 8B and 35B, now available on Hugging Face, expands performance advancements in 23 languages. Cohere said in a blog post the 8B parameter model “makes breakthroughs more accessible to researchers worldwide,” while the 32B parameter model provides state-of-the-art multilingual capabilities.  The Aya project seeks to expand access to foundation models in more global languages than English. Cohere for AI, the company’s research arm, launched the Aya initiative last year. In February, it released the Aya 101 large language model (LLM), a 13-billion-parameter model covering 101 languages. Cohere for AI also released the Aya dataset to help expand access to other languages for model training.  Aya Expanse uses much of the same recipe used to build Aya 101.  “The improvements in Aya Expanse are the result of a sustained focus on expanding how AI serves languages around the world by rethinking the core building blocks of machine learning breakthroughs,” Cohere said. “Our research agenda for the last few years has included a dedicated focus on bridging the language gap, with several breakthroughs that were critical to the current recipe: data arbitrage, preference training for general performance and safety, and finally model merging.” Aya performs well Cohere said the two Aya Expanse models consistently outperformed similar-sized AI models from Google, Mistral and Meta.  Aya Expanse 32B did better in benchmark multilingual tests than Gemma 2 27B, Mistral 8x22B and even the much larger Llama 3.1 70B. The smaller 8B also performed better than Gemma 2 9B, Llama 3.1 8B and Ministral 8B.  Cohere developed the Aya models using a data sampling method called data arbitrage as a means to avoid the generation of gibberish that happens when models rely on synthetic data. Many models use synthetic data created from a “teacher” model for training purposes. However, due to the difficulty in finding good teacher models for other languages, especially for low-resource languages.  It also focused on guiding the models toward “global preferences” and accounting for different cultural and linguistic perspectives. Cohere said it figured out a way to improve performance and safety even while guiding the models’ preferences.  “We think of it as the ‘final sparkle’ in training an AI model,” the company said. “However, preference training and safety measures often overfit to harms prevalent in Western-centric datasets. Problematically, these safety protocols frequently fail to extend to multilingual settings.  Our work is one of the first that extends preference training to a massively multilingual setting, accounting for different cultural and linguistic perspectives.” Models in different languages The Aya initiative focuses on ensuring research around LLMs that perform well in languages other than English.  Many LLMs eventually become available in other languages, especially for widely spoken languages, but there is difficulty in finding data to train models with the different languages. English, after all, tends to be the official language of governments, finance, internet conversations and business, so it’s far easier to find data in English.  It can also be difficult to accurately benchmark the performance of models in different languages because of the quality of translations.  Other developers have released their own language datasets to further research into non-English LLMs. OpenAI, for example, made its Multilingual Massive Multitask Language Understanding Dataset on Hugging Face last month. The dataset aims to help better test LLM performance across 14 languages, including Arabic, German, Swahili and Bengali.  Cohere has been busy these last few weeks. This week, the company added image search capabilities to Embed 3, its enterprise embedding product used in retrieval augmented generation (RAG) systems. It also enhanced fine-tuning for its Command R 08-2024 model this month.  VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/03/nuneybits_Abstract_art_of_a_speech_bubble_conversation_translat_19fd673f-4f7b-4043-8907-caec01c3bdf7-transformed.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-10-24T23:12:19+00:00\" datetime=\"2024-10-24T23:12:19+00:00\"\u003eOctober 24, 2024 4:12 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2024/03/nuneybits_Abstract_art_of_a_speech_bubble_conversation_translat_19fd673f-4f7b-4043-8907-caec01c3bdf7-transformed.webp?w=750\" alt=\"Credit: VentureBeat made with Midjourney\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://cohere.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCohere\u003c/a\u003e today released two new open-weight models in its Aya project to close the language gap in foundation models. \u003c/p\u003e\n\n\n\n\u003cp\u003eAya Expanse 8B and 35B, now available on \u003ca href=\"https://huggingface.co/CohereForAI/aya-expanse-8b\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e, expands performance advancements in 23 languages. Cohere said in \u003ca href=\"https://cohere.com/blog/aya-expanse-connecting-our-world\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ea blog post\u003c/a\u003e the 8B parameter model “makes breakthroughs more accessible to researchers worldwide,” while the 32B parameter model provides state-of-the-art multilingual capabilities. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://cohere.com/research/aya\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eThe Aya project\u003c/a\u003e seeks to expand access to foundation models in more global languages than English. Cohere for AI, the company’s research arm, launched the Aya initiative last year. In February, \u003ca href=\"https://venturebeat.com/ai/cohere-for-ai-launches-open-source-llm-for-101-languages/\"\u003eit released the Aya 101 large language model (LLM\u003c/a\u003e), a 13-billion-parameter model covering 101 languages. Cohere for AI also released the Aya dataset to help expand access to other languages for model training. \u003c/p\u003e\n\n\n\n\u003cp\u003eAya Expanse uses much of the same recipe used to build Aya 101. \u003c/p\u003e\n\n\n\n\u003cp\u003e“The improvements in Aya Expanse are the result of a sustained focus on expanding how AI serves languages around the world by rethinking the core building blocks of machine learning breakthroughs,” Cohere said. “Our research agenda for the last few years has included a dedicated focus on bridging the language gap, with several breakthroughs that were critical to the current recipe: data arbitrage, preference training for general performance and safety, and finally model merging.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-aya-performs-well\"\u003eAya performs well\u003c/h2\u003e\n\n\n\n\u003cp\u003eCohere said the two Aya Expanse models consistently outperformed similar-sized AI models from Google, Mistral and Meta. \u003c/p\u003e\n\n\n\n\u003cp\u003eAya Expanse 32B did better in benchmark multilingual tests than Gemma 2 27B, Mistral 8x22B and even the much larger Llama 3.1 70B. The smaller 8B also performed better than Gemma 2 9B, Llama 3.1 8B and Ministral 8B. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdfAFg6s0Rrqi4DeYsf2owg8DB1Xgov_2GXtlfAePzfGH9UYJYRv-8mLGQR7ktJsAkRdC57dkEqXiOHTk48W_BvZGQFGR45wXHyLDPI7jZv408-VhS-FmfAR2itZ2TwY0zOBx8pMkzZT2u6mY_gyalaJicb?key=OUAbNHSE7EIONbzxlEd9mw\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eCohere developed the Aya models using a data sampling method called data arbitrage as a means to avoid the generation of gibberish that happens when models rely on synthetic data. Many models use synthetic data created from a “teacher” model for training purposes. However, due to the difficulty in finding good teacher models for other languages, especially for low-resource languages. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt also focused on guiding the models toward “global preferences” and accounting for different cultural and linguistic perspectives. Cohere said it figured out a way to improve performance and safety even while guiding the models’ preferences. \u003c/p\u003e\n\n\n\n\u003cp\u003e“We think of it as the ‘final sparkle’ in training an AI model,” the company said. “However, preference training and safety measures often overfit to harms prevalent in Western-centric datasets. Problematically, these safety protocols frequently fail to extend to multilingual settings.  Our work is one of the first that extends preference training to a massively multilingual setting, accounting for different cultural and linguistic perspectives.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-models-in-different-languages\"\u003eModels in different languages\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Aya initiative focuses on ensuring research around LLMs that perform well in languages other than English. \u003c/p\u003e\n\n\n\n\u003cp\u003eMany LLMs eventually become available in other languages, especially for widely spoken languages, but there is difficulty in finding data to train models with the different languages. English, after all, tends to be the official language of governments, finance, internet conversations and business, so it’s far easier to find data in English. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt can also be difficult to accurately benchmark the performance of models in different languages because of the quality of translations. \u003c/p\u003e\n\n\n\n\u003cp\u003eOther developers have released their own language datasets to further research into non-English LLMs. OpenAI, for example, made its \u003ca href=\"https://venturebeat.com/ai/openai-tackles-global-language-divide-with-massive-multilingual-ai-dataset-release/\"\u003eMultilingual Massive Multitask Language Understanding Dataset \u003c/a\u003eon Hugging Face last month. The dataset aims to help better test LLM performance across 14 languages, including Arabic, German, Swahili and Bengali. \u003c/p\u003e\n\n\n\n\u003cp\u003eCohere has been busy these last few weeks. This week, the company added image search \u003ca href=\"https://venturebeat.com/ai/cohere-adds-vision-to-its-rag-search-capabilities/\"\u003ecapabilities to Embed 3\u003c/a\u003e, its enterprise embedding product used in retrieval augmented generation (RAG) systems. It also enhanced fine-tuning for its Command R 08-2024 model this month. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-10-24T23:12:19Z",
  "modifiedTime": "2024-10-24T23:12:27Z"
}
