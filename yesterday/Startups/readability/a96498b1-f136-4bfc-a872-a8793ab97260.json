{
  "id": "a96498b1-f136-4bfc-a872-a8793ab97260",
  "title": "Lotus: Diffusion-Based Visual Foundation Model for High-Quality Dense Prediction",
  "link": "https://lotus3d.github.io/",
  "description": "Article URL: https://lotus3d.github.io/ Comments URL: https://news.ycombinator.com/item?id=41864515 Points: 4 # Comments: 0",
  "author": "jasondavies",
  "published": "Wed, 16 Oct 2024 22:31:58 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "Jing He1✱ ,",
  "length": 5300,
  "excerpt": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Predictionn",
  "siteName": "",
  "favicon": "https://lotus3d.github.io/images/lotus_icon.png",
  "text": "arXiv 2024 1HKUST(GZ) 2University of Adelaide 3Noah's Ark Lab 4HKUST ✱Both authors contributed equally. ✉Corresponding author. We present Lotus, a diffusion-based visual foundation model for dense geometry prediction. With minimal training data, Lotus achieves SoTA performance in two key geometry perception tasks, i.e., zero-shot depth and normal estimation. \"Avg. Rank\" indicates the average ranking across all metrics, where lower values are better. Bar length represents the amount of training data used. Abstract Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus' superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc. Methodology Fine-tuning Protocol After the pre-trained VAE encoder $\\mathcal{E}$ encodes the image $\\textbf{x}$ and annotation $\\textbf{y}$ to the latent space: ①the denoiser U-Net model $f_\\theta$ is fine-tuned using $x_0$-prediction; ②we employ single-step diffusion formulation at time-step $t=T$ for better coverage; ③we propose a novel detail preserver, to switch the model either to reconstruct the image or generate the dense prediction via a switcher $s$, ensuring a more fine-grained prediction. The noise $\\mathbf{z_T^y}$ in bracket is used for our generative Lotus-G and is omitted for the discriminative Lotus-D. Inference Scheme The standard Gaussian noise $\\mathbf{z_T^y}$ and encoded RGB image $\\mathbf{z^x}$ are concatenated to form the input. We set $t=T$ and the switcher to $s_y$. The denoiser U-Net model then predicts the latent dense prediction that is further decoded to get the final output. The noise $\\mathbf{z_T^y}$ in bracket is used for Lotus-G and omitted for Lotus-D. Experiments Zero-shot Affine-invariant Depth Estimation Quantitative comparison of Lotus with SoTA affine-invariant depth estimators on several zero-shot benchmarks. The upper section lists discriminative methods, the lower lists generative ones. The best and second best performances are highlighted. Lotus-G outperforms all others methods while Lotus-D is slightly inferior to DepthAnything. Please note that DepthAnything is trained on $62.6$M images while Lotus is only trained on $0.059$M images. Zero-shot Surface Normal Estimation Quantitative comparison of Lotus with SoTA surface normal estimators on several zero-shot benchmarks. Both Lotus-G and Lotus-D outperform all other methods with significant margins. Please refer to our paper linked above for more technical details :) BibTeX @article{he2024lotus, title={Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction}, author={He, Jing and Li, Haodong and Yin, Wei and Liang, Yixun and Li, Leheng and Zhou, Kaiqiang and Liu, Hongbo and Liu, Bingbing and Chen, Ying-Cong}, journal={arXiv preprint arXiv:2409.18124}, year={2024} }",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\n\n\n\u003cdiv\u003e\n          \n          \u003ch3\u003earXiv 2024\u003c/h3\u003e\n          \n\n          \u003cp\u003e\u003cspan\u003e\u003csup\u003e1\u003c/sup\u003eHKUST(GZ)\u003c/span\u003e\n            \u003cspan\u003e\u003csup\u003e2\u003c/sup\u003eUniversity of Adelaide\u003c/span\u003e\n            \u003cspan\u003e\u003csup\u003e3\u003c/sup\u003eNoah\u0026#39;s Ark Lab\u003c/span\u003e\n            \u003cspan\u003e\u003csup\u003e4\u003c/sup\u003eHKUST\u003c/span\u003e\n            \u003cspan\u003e\n                \u003csup\u003e✱\u003c/sup\u003e\u003cstrong\u003eBoth authors contributed equally.\u003c/strong\u003e\n                \u003csup\u003e✉\u003c/sup\u003eCorresponding author.\n            \u003c/span\u003e\n          \u003c/p\u003e\n\n          \n        \u003c/div\u003e\n\n\u003cdiv\u003e\n      \u003cp\u003e\u003cimg id=\"teaser\" width=\"100%\" src=\"https://lotus3d.github.io/images/teaser_1.jpg\" alt=\"Teaser 1\"/\u003e\n      \u003cimg id=\"teaser\" width=\"100%\" src=\"https://lotus3d.github.io/images/teaser_2.jpg\" alt=\"Teaser 2\"/\u003e\u003c/p\u003e\u003ch2\u003e\n        We present \u003cspan\u003eLotus\u003c/span\u003e, a diffusion-based visual foundation model for dense geometry prediction.\n        With minimal training data, \u003cspan\u003eLotus\u003c/span\u003e achieves SoTA performance in two key geometry perception tasks, \u003ci\u003ei.e.\u003c/i\u003e, zero-shot depth and normal estimation.\n        \u0026#34;Avg. Rank\u0026#34; indicates the average ranking across all metrics, where lower values are better. Bar length represents the amount of training data used.\n      \u003c/h2\u003e\n    \u003c/div\u003e\n\n\n\u003cdiv\u003e\n        \u003ch2\u003eAbstract\u003c/h2\u003e\n        \u003cdiv\u003e\n        \u003cp\u003e\n            Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks.\n            However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation.\n        \u003c/p\u003e\u003cp\u003e\n            In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize.\n        \u003c/p\u003e\u003cp\u003e\n            Based on these insights, we introduce \u003cspan\u003eLotus\u003c/span\u003e, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction.\n            Specifically, \u003cspan\u003eLotus\u003c/span\u003e is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed.\n            Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions.\n        \u003c/p\u003e\u003cp\u003e\n            Without scaling up the training data or model capacity, \u003cspan\u003eLotus\u003c/span\u003e achieves SoTA performance in zero-shot depth and normal estimation across various datasets. \n            It also enhances efficiency, being significantly faster than most existing diffusion-based methods.\n            \u003cspan\u003eLotus\u003c/span\u003e\u0026#39; superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc.\n        \u003c/p\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n\n\u003cdiv\u003e\n        \u003ch2\u003eMethodology\u003c/h2\u003e\n        \u003cdiv\u003e\n          \u003ch3\u003e\n            Fine-tuning Protocol\n          \u003c/h3\u003e\n\n          \u003cp\u003e\n          After the pre-trained VAE encoder $\\mathcal{E}$ encodes the image $\\textbf{x}$ and annotation $\\textbf{y}$ to the latent space:\n          ①the denoiser U-Net model $f_\\theta$ is fine-tuned using $x_0$-prediction;\n          ②we employ single-step diffusion formulation at time-step $t=T$ for better coverage;\n          ③we propose a novel detail preserver, to switch the model either to reconstruct the image or generate the dense prediction via a switcher $s$, ensuring a more fine-grained prediction.\n          The noise $\\mathbf{z_T^y}$ in bracket is used for our generative \u003cspan\u003eLotus-G\u003c/span\u003e and is omitted for the discriminative \u003cspan\u003eLotus-D\u003c/span\u003e.\n        \u003c/p\u003e\n          \n          \u003cp\u003e\u003cimg id=\"method_train\" width=\"100%\" src=\"https://lotus3d.github.io/images/method_training_compressed.jpg\" alt=\"Marigold training scheme\"/\u003e\u003c/p\u003e\u003ch3\u003e\n            Inference Scheme\n          \u003c/h3\u003e\n\n          \u003cp\u003e\n            The standard Gaussian noise $\\mathbf{z_T^y}$ and encoded RGB image $\\mathbf{z^x}$ are concatenated to form the input. We set $t=T$ and\n            the switcher to $s_y$. The denoiser U-Net model then predicts the latent dense prediction that is further decoded to get the final output.\n            The noise $\\mathbf{z_T^y}$ in bracket is used for \u003cspan\u003eLotus-G\u003c/span\u003e and omitted for \u003cspan\u003eLotus-D\u003c/span\u003e.\n          \u003c/p\u003e\n\n          \u003cp\u003e\u003cimg id=\"method_inference\" width=\"50%\" src=\"https://lotus3d.github.io/images/method_inference_compressed.jpg\" alt=\"Marigold inference scheme\"/\u003e\n        \u003c/p\u003e\u003c/div\u003e\n          \u003ch2\u003eExperiments\u003c/h2\u003e\n          \u003cdiv\u003e\n          \u003ch3\u003e\n            Zero-shot Affine-invariant Depth Estimation\n          \u003c/h3\u003e\n\n          \u003cp\u003e\n            Quantitative comparison of \u003cspan\u003eLotus\u003c/span\u003e with SoTA affine-invariant depth estimators\n            on several zero-shot benchmarks. The upper section lists discriminative methods,\n            the lower lists generative ones. The \u003cspan\u003ebest\u003c/span\u003e and \n            \u003cspan\u003esecond best\u003c/span\u003e performances are highlighted. \n            \u003cspan\u003eLotus-G\u003c/span\u003e outperforms all others methods while \u003cspan\u003eLotus-D\u003c/span\u003e is slightly \n            inferior to DepthAnything. Please note that DepthAnything is trained on $62.6$M images while \n            \u003cspan\u003eLotus\u003c/span\u003e is only trained on $0.059$M images.\n          \u003c/p\u003e\n\n          \u003cp\u003e\u003cimg id=\"comparison\" width=\"100%\" src=\"https://lotus3d.github.io/images/exp_depth.png\" alt=\"Comparison with other methods\"/\u003e\u003c/p\u003e\u003ch3\u003e\n            Zero-shot Surface Normal Estimation\n          \u003c/h3\u003e\n\n          \u003cp\u003e\n            Quantitative comparison of \u003cspan\u003eLotus\u003c/span\u003e with SoTA surface normal estimators\n            on several zero-shot benchmarks. Both \u003cspan\u003eLotus-G\u003c/span\u003e and \u003cspan\u003eLotus-D\u003c/span\u003e outperform all other methods with significant margins.\n          \u003c/p\u003e\n\n          \u003cp\u003e\u003cimg id=\"comparison\" width=\"100%\" src=\"https://lotus3d.github.io/images/exp_normal.png\" alt=\"Comparison with other methods\"/\u003e\u003c/p\u003e\u003cp\u003e\n            Please refer to our paper linked above for more technical details :)\n          \u003c/p\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n\n\n\u003cdiv id=\"BibTeX\"\u003e\n    \u003ch2\u003eBibTeX\u003c/h2\u003e\n    \u003cpre\u003e\u003ccode\u003e@article{he2024lotus,\n    title={Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction},\n    author={He, Jing and Li, Haodong and Yin, Wei and Liang, Yixun and Li, Leheng and Zhou, Kaiqiang and Liu, Hongbo and Liu, Bingbing and Chen, Ying-Cong},\n    journal={arXiv preprint arXiv:2409.18124},\n    year={2024}\n}\n\u003c/code\u003e\u003c/pre\u003e\n  \u003c/div\u003e\n\n\n\n\n\n\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
