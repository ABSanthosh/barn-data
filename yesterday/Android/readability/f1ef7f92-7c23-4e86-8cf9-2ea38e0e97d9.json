{
  "id": "f1ef7f92-7c23-4e86-8cf9-2ea38e0e97d9",
  "title": "New AI and accessibility updates across Android, Chrome and more",
  "link": "https://blog.google/outreach-initiatives/accessibility/android-gemini-ai-gaad-2025/",
  "description": "In honor of Global Accessibility Awareness Day, we’re excited to roll out new updates across Android and Chrome, plus new resources for the ecosystem.",
  "author": "Angana GhoshDirector, Product ManagementAndroid",
  "published": "Thu, 15 May 2025 16:00:00 +0000",
  "source": "https://blog.google/products/android/rss",
  "categories": [
    "Accessibility",
    "Android",
    "AI",
    "Chrome"
  ],
  "byline": "Angana Ghosh",
  "length": 5289,
  "excerpt": "In honor of Global Accessibility Awareness Day, we’re excited to roll out new updates across Android and Chrome, plus new resources for the ecosystem.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png?version=pr20250514-1654",
  "text": "Advances in AI continue to make our world more and more accessible. Today, in honor of Global Accessibility Awareness Day, we’re rolling out new updates to our products across Android and Chrome, as well as adding new resources for developers building speech recognition tools.More AI-powered innovation with AndroidWe’re building on our work and integrating the best of Google AI and Gemini into core mobile experiences customized for vision and hearing.Get all the details with Gemini and TalkBackLast year we brought Gemini’s capabilities to TalkBack, Android’s screen reader, providing people who are blind or low-vision with AI-generated descriptions for images, even when there’s no alt text. Today, we’re expanding this Gemini integration so people can ask questions and get responses about their images. That means the next time a friend texts you a photo of their new guitar, you can get a description and ask follow-up questions about the make and color, or even what else is in the image. People can now also get descriptions and ask questions about their whole screen. So if you’re shopping for the latest sales on your favorite shopping app, you can ask Gemini about the material of an item or if a discount is available. Use TalkBack's Gemini powered capabilities to get a description of what’s on your screen. Understand mooooore of the emotion behind captionsWith Expressive Captions, your phone provides real-time captions for anything with sound across most apps on your phone — using AI to not only capture what someone says, but how they say it. We know one of the ways people express themselves is by dragging out the sound of their words, which is why we developed a new duration feature on Expressive Captions, so you can know when a sports announcing is calling out an “amaaazing shot” or when the video message is not “no” but “nooooo.” You’ll also receive even more labels for sounds, so you know when someone is whistling or clearing their throat. This new version is rolling out in English in the U.S., U.K., Canada and Australia for devices running Android 15 and above. With Expressive Captions new duration feature, get even more context of what's being said in the audio and video on your phone. Improving speech recognition around the worldIn 2019, we launched Project Euphonia to find ways to make speech recognition more accessible for people with non-standard speech. Now we’re supporting developers and organizations around the world as they bring that work to even more languages and cultural contexts.New developer resourcesTo improve the ecosystem of tools globally, we’re providing developers with our open-source repositories via Project Euphonia’s GitHub page. They can now develop personalized audio tools for research or train their models for diverse speech patterns.Support for new projects in AfricaEarlier this year we partnered with Google.org to provide support to the University College London in their creation of the Centre for Digital Language Inclusion (CDLI). The CDLI is working to improve speech recognition technology for non-English speakers in Africa by creating open-source datasets in 10 African languages, building new speech recognition models and continuing to support the ecosystem of organizations and developers in this space.Expanding accessibility options for studentsAccessibility tools can be particularly helpful for students with disabilities, from using facial gestures to navigate their Chromebooks with Face Control to customizing their reading experience with Reading Mode.And now when you use your Chromebook with College Board’s Bluebook testing app (which is where students can take the SAT and most Advanced Placement exams) you’ll have access to all of Google’s built-in accessibility features. This includes ChromeVox screen reader and Dictation, along with College Board’s own digital testing tools.Making Chrome more accessibleWith more than 2 billion people using Chrome each day, we’re always striving to make our browser easier to use and more accessible for everyone with features like Live Caption and image descriptions for screen reader users.Access PDFs more easily on ChromePreviously, if you opened a scanned PDF in your desktop Chrome browser, you wouldn’t be able to use your screen reader to interact with it. Now with Optical Character Recognition (OCR), Chrome automatically recognizes these types of PDFs, so you can highlight, copy and search for text like any other page and use your screen reader to read them.Read with ease with Page ZoomPage Zoom now lets you increase the size of the text you see in Chrome on Android without affecting the webpage layout or your browsing experience — just like how it works on Chrome desktop. You can customize how much you want to zoom in and easily apply the preference to all the pages you visit or just specific ones. Page Zoom works with Chrome on Android, letting you customize how you see pages To start using this feature, just tap the three-dot menu in the top right corner on Chrome and set your zoom preferences.",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Blog_header_2096x1182_Option_2.width-1300.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-reading-time=\"true\" data-component=\"uni-article-body\"\u003e\n\n            \n              \n\n\n\n\n\n\u003cuni-article-speakable page-title=\"New AI and accessibility updates across Android, Chrome and more\" listen-to-article=\"Listen to article\" data-date-modified=\"2025-05-19T01:06:27.576113+00:00\" data-tracking-ids=\"G-HGNBTNCHCQ,G-6NKTLKV14N\" data-voice-list=\"en.ioh-pngnat:Cyan,en.usb-pngnat:Lime\" data-script-src=\"https://www.gstatic.com/readaloud/player/web/api/js/api.js\"\u003e\u003c/uni-article-speakable\u003e\n\n            \n\n            \n            \n\n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;New AI and accessibility updates across Android, Chrome and more\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"qe4en\"\u003eAdvances in AI continue to make our world more and more accessible. Today, in honor of Global Accessibility Awareness Day, we’re rolling out new updates to our products across Android and Chrome, as well as adding new resources for developers building speech recognition tools.\u003c/p\u003e\u003ch2 data-block-key=\"asv97\"\u003e\u003cb\u003eMore AI-powered innovation with Android\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"1r6k1\"\u003eWe’re building on our work and integrating the best of Google AI and Gemini into core mobile experiences customized for vision and hearing.\u003c/p\u003e\u003ch3 data-block-key=\"761ol\"\u003eGet all the details with Gemini and TalkBack\u003c/h3\u003e\u003cp data-block-key=\"63di8\"\u003eLast year we brought \u003ca href=\"https://blog.google/products/android/google-ai-android-update-io-2024/#talkback\"\u003eGemini’s capabilities to TalkBack\u003c/a\u003e, Android’s screen reader, providing people who are blind or low-vision with AI-generated descriptions for images, even when there’s no alt text. Today, we’re expanding this Gemini integration so people can ask questions and get responses about their images. That means the next time a friend texts you a photo of their new guitar, you can get a description and ask follow-up questions about the make and color, or even what else is in the image. People can now also get descriptions and ask questions about their whole screen. So if you’re shopping for the latest sales on your favorite shopping app, you can ask Gemini about the material of an item or if a discount is available.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Android phone displaying a shopping app with four dresses, while TalkBack\u0026#39;s Gemini powered feature describes the screen and answers questions.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"New AI and accessibility updates across Android, Chrome and more\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"4tad1\"\u003eUse TalkBack\u0026#39;s Gemini powered capabilities to get a description of what’s on your screen.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Android phone displaying a shopping app with four dresses, while TalkBack\u0026#39;s Gemini powered feature describes the screen and answers questions.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/2469_ANC_Accessibility-Launches_Gemini-Talkback-Option-1_v03-optimize.gif\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;New AI and accessibility updates across Android, Chrome and more\u0026#34;\n         }\"\u003e\u003ch3 data-block-key=\"qe4en\"\u003eUnderstand mooooore of the emotion behind captions\u003c/h3\u003e\u003cp data-block-key=\"19vim\"\u003eWith Expressive Captions, your phone provides real-time captions for anything with sound across most apps on your phone — using AI to not only capture what someone says, but how they say it. We know one of the ways people express themselves is by dragging out the sound of their words, which is why we developed a new duration feature on Expressive Captions, so you can know when a sports announcing is calling out an “amaaazing shot” or when the video message is not “no” but “nooooo.” You’ll also receive even more labels for sounds, so you know when someone is whistling or clearing their throat. This new version is rolling out in English in the U.S., U.K., Canada and Australia for devices running Android 15 and above.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"medium\" alt-text=\"Android phone shows a soccer game with Expressive Captions on, highlighting duration with someone describing a goal as \u0026#34;Amaazing\u0026#34;.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"New AI and accessibility updates across Android, Chrome and more\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"4tad1\"\u003eWith Expressive Captions new duration feature, get even more context of what\u0026#39;s being said in the audio and video on your phone.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Android phone shows a soccer game with Expressive Captions on, highlighting duration with someone describing a goal as \u0026#34;Amaazing\u0026#34;.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Pixel_10_-_DARK_MODE_Soccer_Case_V3.gif\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;New AI and accessibility updates across Android, Chrome and more\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"qe4en\"\u003e\u003cb\u003eImproving speech recognition around the world\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"2jck3\"\u003eIn 2019, we launched \u003ca href=\"http://g.co/euphonia\"\u003eProject Euphonia\u003c/a\u003e to find ways to make speech recognition more accessible for people with non-standard speech. Now we’re supporting developers and organizations around the world as they bring that work to even more languages and cultural contexts.\u003c/p\u003e\u003ch3 data-block-key=\"7ukh8\"\u003eNew developer resources\u003c/h3\u003e\u003cp data-block-key=\"eqvqp\"\u003eTo improve the ecosystem of tools globally, we’re providing developers with our open-source repositories via Project Euphonia’s GitHub page. They can now develop personalized \u003ca href=\"https://github.com/google/euphonia\"\u003eaudio tools\u003c/a\u003e for research or \u003ca href=\"https://github.com/google/project-euphonia-app\"\u003etrain their models\u003c/a\u003e for diverse speech patterns.\u003cbr/\u003e\u003c/p\u003e\u003ch3 data-block-key=\"5kvfh\"\u003eSupport for new projects in Africa\u003c/h3\u003e\u003cp data-block-key=\"191c1\"\u003eEarlier this year we partnered with \u003ca href=\"http://google.org/\"\u003eGoogle.org\u003c/a\u003e to provide support to the University College London in their creation of the \u003ca href=\"https://www.cdl-inclusion.com/\"\u003eCentre for Digital Language Inclusion (CDLI)\u003c/a\u003e. The CDLI is working to improve speech recognition technology for non-English speakers in Africa by creating open-source datasets in 10 African languages, building new speech recognition models and continuing to support the ecosystem of organizations and developers in this space.\u003c/p\u003e\u003ch2 data-block-key=\"el41n\"\u003e\u003cb\u003eExpanding accessibility options for students\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"8s06i\"\u003eAccessibility tools can be \u003ca href=\"https://blog.google/outreach-initiatives/education/global-accessibility-awareness-day-2025\"\u003eparticularly helpful for students with disabilities\u003c/a\u003e, from using facial gestures to navigate their Chromebooks with \u003ca href=\"https://support.google.com/chromebook/answer/15824371?hl=en\"\u003eFace Control\u003c/a\u003e to customizing their reading experience with \u003ca href=\"https://support.google.com/chromebook/answer/15284142?sjid=13213058016893776222-NC\"\u003eReading Mode\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"5bni6\"\u003eAnd now when you use your Chromebook with College Board’s \u003ca href=\"https://bluebook.collegeboard.org/students\"\u003eBluebook\u003c/a\u003e testing app (which is where students can take the SAT and most Advanced Placement exams) you’ll have access to all of Google’s built-in accessibility features. This includes \u003ca href=\"https://support.google.com/chromebook/answer/7031755?hl=en\"\u003eChromeVox screen reader\u003c/a\u003e and \u003ca href=\"https://support.google.com/chromebook/answer/12001244?hl=en\"\u003eDictation\u003c/a\u003e, along with College Board’s own digital testing tools.\u003c/p\u003e\u003ch2 data-block-key=\"2icj5\"\u003e\u003cb\u003eMaking Chrome more accessible\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"232f2\"\u003eWith more than 2 billion people using Chrome each day, we’re always striving to make our browser easier to use and more accessible for everyone with features like \u003ca href=\"https://support.google.com/chrome/answer/10538231?hl=en\u0026amp;ref_topic=7437725\u0026amp;sjid=13213058016893776222-NC\"\u003eLive Caption\u003c/a\u003e and \u003ca href=\"https://support.google.com/chrome/answer/9311597?sjid=15422105452954241411-NA\"\u003eimage descriptions\u003c/a\u003e for screen reader users.\u003c/p\u003e\u003ch3 data-block-key=\"1sf4u\"\u003eAccess PDFs more easily on Chrome\u003c/h3\u003e\u003cp data-block-key=\"18ic0\"\u003ePreviously, if you opened a scanned PDF in your desktop Chrome browser, you wouldn’t be able to use your screen reader to interact with it. Now with Optical Character Recognition (OCR), Chrome automatically recognizes these types of PDFs, so you can highlight, copy and search for text like any other page and use your screen reader to read them.\u003c/p\u003e\u003ch3 data-block-key=\"1rps4\"\u003eRead with ease with Page Zoom\u003c/h3\u003e\u003cp data-block-key=\"bve1h\"\u003e\u003ca href=\"https://support.google.com/accessibility/android/answer/13532420?hl=en\"\u003ePage Zoom\u003c/a\u003e now lets you increase the size of the text you see in Chrome on Android without affecting the webpage layout or your browsing experience — just like how it works on Chrome desktop. You can customize how much you want to zoom in and easily apply the preference to all the pages you visit or just specific ones.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"medium\" alt-text=\"Page Zoom is shown via Chrome on Android with the text on the page being zoomed in and out\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"New AI and accessibility updates across Android, Chrome and more\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"hyog7\"\u003ePage Zoom works with Chrome on Android, letting you customize how you see pages\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Page Zoom is shown via Chrome on Android with the text on the page being zoomed in and out\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/GoogleKeyword_GAAD_InLineAsset_GIF.gif\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;New AI and accessibility updates across Android, Chrome and more\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"q5xky\"\u003eTo start using this feature, just tap the three-dot menu in the top right corner on Chrome and set your zoom preferences.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\n\u003cuni-related-content-tout title=\"5 Chromebook accessibility features\" cta=\"See more\" summary=\"For Global Accessibility Awareness Day, we’re sharing Chromebook accessibility features and use cases.\" hideimage=\"False\" eyebrow=\"Related Article\" image-alt-text=\"\" role=\"none\" fullurl=\"https://blog.google/outreach-initiatives/education/global-accessibility-awareness-day-2025/\" pagetype=\"articlepage\" isarticlepage=\"\" data-ga4-related-article=\"{\n  \u0026#34;event\u0026#34;: \u0026#34;article_lead_click\u0026#34;,\n  \u0026#34;link_text\u0026#34;: \u0026#34;5 Chromebook accessibility features for inclusive classrooms\u0026#34;,\n  \u0026#34;link_type\u0026#34;: \u0026#34;internal\u0026#34;,\n  \u0026#34;full_url\u0026#34;: \u0026#34;https://blog.google/outreach-initiatives/education/global-accessibility-awareness-day-2025/\u0026#34;,\n  \u0026#34;title\u0026#34;: \u0026#34;5 Chromebook accessibility features for inclusive classrooms\u0026#34;,\n  \u0026#34;author\u0026#34; : \u0026#34;Tricia Davis-Muffett\u0026#34;,\n  \u0026#34;slug\u0026#34;: \u0026#34;global-accessibility-awareness-day-2025\u0026#34;,\n  \u0026#34;position\u0026#34;: \u0026#34;1 of 1\u0026#34;,\n  \u0026#34;click_location\u0026#34;: \u0026#34;undefined\u0026#34;,\n  \u0026#34;primary_tag\u0026#34;: \u0026#34;Topics - Learning \u0026amp; Education\u0026#34;,\n  \u0026#34;secondary_tags\u0026#34;: \u0026#34;Chromebooks\u0026#34;,\n  \u0026#34;published_date\u0026#34;: \u0026#34;2025-05-15|16:00\u0026#34;,\n  \u0026#34;hero_media_type\u0026#34;: \u0026#34;image\u0026#34;,\n  \u0026#34;days_since_published\u0026#34;: \u0026#34;5\u0026#34;,\n  \u0026#34;content_category\u0026#34;: \u0026#34;Announcement\u0026#34;,\n  \u0026#34;word_count\u0026#34;: \u0026#34;494\u0026#34;,\n  \u0026#34;has_audio\u0026#34;: \u0026#34;no\u0026#34;,\n  \u0026#34;has_video\u0026#34;: \u0026#34;yes\u0026#34;,\n  \u0026#34;has_image\u0026#34;: \u0026#34;no\u0026#34;,\n  \u0026#34;has_carousel\u0026#34;: \u0026#34;yes\u0026#34;\n}\"\u003e\n  \n    \u003cdiv slot=\"rct-image-slot\"\u003e\n      \n    \u003cfigure\u003e\n        \u003cpicture\u003e\n            \n\n\n    \n\n    \n        \u003csource media=\"(max-resolution: 1.5dppx)\" sizes=\"300px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2442-EDU____GAAD_Blog_Post___blog.width-300.format-webp.webp 300w\"/\u003e\n    \n        \u003csource media=\"(min-resolution: 1.5dppx)\" sizes=\"600px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2442-EDU____GAAD_Blog_Post___blog.width-600.format-webp.webp 600w\"/\u003e\n    \n\n    \u003cimg src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2442-EDU____GAAD_Blog_Post___blog.width-600.format-webp.webp\" alt=\"2442-EDU _  GAAD Blog Post _ blog header\" sizes=\" 300px,  600px\" srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2442-EDU____GAAD_Blog_Post___blog.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2442-EDU____GAAD_Blog_Post___blog.width-600.format-webp.webp 600w\" data-target=\"image\" loading=\"lazy\"/\u003e\n    \n\n\n        \u003c/picture\u003e\n    \u003c/figure\u003e\n\n\n    \u003c/div\u003e\n  \n\u003c/uni-related-content-tout\u003e\n\n  \n\n\n            \n            \n\n            \n              \n\n\n\n\n            \n          \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-05-15T16:00:00Z",
  "modifiedTime": null
}
