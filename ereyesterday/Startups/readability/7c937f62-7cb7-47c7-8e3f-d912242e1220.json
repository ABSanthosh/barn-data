{
  "id": "7c937f62-7cb7-47c7-8e3f-d912242e1220",
  "title": "Chinese researchers unveil LLaVA-o1 to challenge OpenAI’s o1 model",
  "link": "https://venturebeat.com/ai/chinese-researchers-unveil-llava-o1-to-challenge-openais-o1-model/",
  "description": "LLaVA-o1 breaks down the answer into multiple reasoning components and uses inference-time scaling to optimize each stage.",
  "author": "Ben Dickson",
  "published": "Fri, 22 Nov 2024 23:26:42 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI, ML and Deep Learning",
    "chain of thought reasoning",
    "inference time scaling",
    "language model",
    "large language models",
    "Llama 3.2",
    "Llama 3.2 Vision",
    "LLaVA-o1",
    "LLaVA-o1-100k",
    "LLM reasoning",
    "LLMs",
    "multimodal benchmark",
    "o1",
    "O1 model",
    "Open source",
    "OpenAI",
    "vision language model",
    "vision language models",
    "VLMs"
  ],
  "byline": "Ben Dickson",
  "length": 6425,
  "excerpt": "LLaVA-o1 breaks down the answer into multiple reasoning components and uses inference-time scaling to optimize each stage.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "November 22, 2024 3:26 PM Credit: VentureBeat with DALL-E 3 Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More OpenAI‘s o1 model has shown that inference-time scaling—using more compute during inference—can significantly boost a language model’s reasoning abilities. LLaVA-o1, a new model developed by researchers from multiple universities in China, brings this paradigm to open-source vision language models (VLMs). Early open-source VLMs typically use a direct prediction approach, generating answers without reasoning about the prompt and the steps required to solve the prompt. Without a structured reasoning process, they are less effective at tasks that require logical reasoning. Advanced prompting techniques such as chain-of-thought (CoT) prompting, where the model is encouraged to generate intermediate reasoning steps, produce some marginal improvements. But VLMs often produce errors or hallucinate. The researchers observed that a key issue is that the reasoning process in existing VLMs is not sufficiently systematic and structured. The models do not generate reasoning chains and often get stuck in reasoning processes where they don’t know at what stage they are and what specific problem they must solve. “We observe that VLMs often initiate responses without adequately organizing the problem and the available information,” the researchers write. “Moreover, they frequently deviate from a logical reasoning toward conclusions, instead of presenting a conclusion prematurely and subsequently attempting to justify it. Given that language models generate responses token-by-token, once an erroneous conclusion is introduced, the model typically continues along a flawed reasoning path.” Multistage reasoning OpenAI o1 uses inference-time scaling to solve the systematic and structured reasoning problem and allows the model to pause and review its results as it gradually solves the problem. While OpenAI has not released much detail about the underlying mechanism of o1, its results show promising directions for improving the reasoning abilities of foundational models. Inspired by o1, the researchers designed LLaVA-o1 to perform stage-by-stage reasoning. Instead of generating a direct reasoning chain, LLaVA-o1 breaks down the reasoning process into four distinct stages: Summary: The model first provides a high-level summary of the question, outlining the core problem it needs to address. Caption:  If an image is present, the model describes the relevant parts, focusing on elements related to the question. Reasoning:  Building on the summary, the model performs structured, logical reasoning to derive a preliminary answer. Conclusion: Finally, the model presents a concise summary of the answer based on the preceding reasoning. Only the conclusion stage is visible to the user; the other three stages represent the model’s internal reasoning process, similar to the hidden reasoning trace of o1. This structured approach allows LLaVA-o1 to manage its reasoning process independently, leading to improved performance on complex tasks. “This structured approach enables the model to independently manage its reasoning process, improving its adaptability and performance on complex reasoning tasks,” the researchers write. Stage-level beam search (right) vs other inference-time scaling techniques Source: arXiv LLaVA-o1 also introduces a novel inference-time scaling technique called “stage-level beam search.” Stage-level beam search generates multiple candidate outputs at each reasoning stage. It then selects the best candidate at each stage to continue the generation process. This is in contrast to the classic best-of-N approach, in which the model is prompted to generate multiple complete responses before selecting one. “Notably, it is the structured output design of LLaVA-o1 that makes this approach feasible, enabling efficient and accurate verification at each stage,” the researchers write. “This validates the effectiveness of structured output in improving inference time scaling.” Training LLaVA-o1 LLaVA-o1 training data is annotated with GPT-4o Source: arXiv To train LLaVA-o1, the researchers compiled a new dataset of around 100,000 image-question-answer pairs obtained from several widely used VQA datasets. The dataset covers a variety of tasks, from multi-turn question answering to chart interpretation and geometric reasoning. The researchers used GPT-4o to generate the detailed four-stage reasoning processes for each example, including the summary, caption, reasoning and conclusion stages.  The researchers then fine-tuned Llama-3.2-11B-Vision-Instruct on this dataset to obtain the final LLaVA-o1 model. The researchers have not released the model but plan to release the dataset, called the LLaVA-o1-100k. LLaVA-o1 in action The researchers evaluated LLaVA-o1 on several multimodal reasoning benchmarks.  Despite being trained on only 100,000 examples, LLaVA-o1 showed significant performance improvements over the base Llama model, with an average benchmark score increase of 6.9%.   LLaVA-o1 vs other open and closed models Source: arXiv Furthermore, stage-level beam search led to additional performance gains, demonstrating the effectiveness of inference-time scaling. Due to computational resource constraints, the researchers were only able to test the technique with a beam size of 2. They expect even greater improvements with larger beam sizes. Impressively, LLaVA-o1 outperformed not only other open-source models of the same size or larger but also some closed-source models like GPT-4-o-mini and Gemini 1.5 Pro. “LLaVA-o1 establishes a new standard for multimodal reasoning in VLMs, offering robust performance and scalability, especially in inference time,” the researchers write. “Our work paves the way for future research on structured reasoning in VLMs, including potential expansions with external verifiers and the use of reinforcement learning to further enhance complex multimodal reasoning capabilities.” VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/11/robotic-llama.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-11-22T23:26:42+00:00\" datetime=\"2024-11-22T23:26:42+00:00\"\u003eNovember 22, 2024 3:26 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"422\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/robotic-llama.jpg?w=750\" alt=\"robotic llama\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat with DALL-E 3\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://openai.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI\u003c/a\u003e‘s o1 model has shown that inference-time scaling—using more compute during inference—can significantly boost a language model’s reasoning abilities. \u003ca href=\"https://arxiv.org/abs/2411.10440\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLLaVA-o1\u003c/a\u003e, a new model developed by researchers from multiple universities in China, brings this paradigm to open-source vision language models (VLMs).\u003c/p\u003e\n\n\n\n\u003cp\u003eEarly open-source VLMs typically use a direct prediction approach, generating answers without reasoning about the prompt and the steps required to solve the prompt. Without a structured reasoning process, they are less effective at tasks that require logical reasoning. Advanced prompting techniques such as \u003ca href=\"https://venturebeat.com/ai/improving-decision-making-in-llms-two-contemporary-approaches/\"\u003echain-of-thought\u003c/a\u003e (CoT) prompting, where the model is encouraged to generate intermediate reasoning steps, produce some marginal improvements. But VLMs often produce errors or hallucinate.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers observed that a key issue is that the reasoning process in existing VLMs is not sufficiently systematic and structured. The models do not generate reasoning chains and often get stuck in reasoning processes where they don’t know at what stage they are and what specific problem they must solve.\u003c/p\u003e\n\n\n\n\u003cp\u003e“We observe that VLMs often initiate responses without adequately organizing the problem and the available information,” the researchers write. “Moreover, they frequently deviate from a logical reasoning toward conclusions, instead of presenting a conclusion prematurely and subsequently attempting to justify it. Given that language models generate responses token-by-token, once an erroneous conclusion is introduced, the model typically continues along a flawed reasoning path.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-multistage-reasoning\"\u003eMultistage reasoning\u003c/h2\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://venturebeat.com/ai/forget-gpt-5-openai-launches-new-ai-model-family-o1-claiming-phd-level-performance/\"\u003eOpenAI o1\u003c/a\u003e uses inference-time scaling to solve the systematic and structured reasoning problem and allows the model to pause and review its results as it gradually solves the problem. While OpenAI has not released much detail about the underlying mechanism of o1, its results show promising directions for improving the reasoning abilities of foundational models.\u003c/p\u003e\n\n\n\n\u003cp\u003eInspired by o1, the researchers designed LLaVA-o1 to perform stage-by-stage reasoning. Instead of generating a direct reasoning chain, LLaVA-o1 breaks down the reasoning process into four distinct stages:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e The model first provides a high-level summary of the question, outlining the core problem it needs to address.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCaption:\u003c/strong\u003e  If an image is present, the model describes the relevant parts, focusing on elements related to the question.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eReasoning:\u003c/strong\u003e  Building on the summary, the model performs structured, logical reasoning to derive a preliminary answer.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eConclusion:\u003c/strong\u003e Finally, the model presents a concise summary of the answer based on the preceding reasoning.\u003c/p\u003e\n\n\n\n\u003cp\u003eOnly the conclusion stage is visible to the user; the other three stages represent the model’s internal reasoning process, similar to the hidden reasoning trace of o1. This structured approach allows LLaVA-o1 to manage its reasoning process independently, leading to improved performance on complex tasks.\u003c/p\u003e\n\n\n\n\u003cp\u003e“This structured approach enables the model to independently manage its reasoning process, improving its adaptability and performance on complex reasoning tasks,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1200\" height=\"531\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/stage-level-beam-search.jpg?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/11/stage-level-beam-search.jpg 1200w, https://venturebeat.com/wp-content/uploads/2024/11/stage-level-beam-search.jpg?resize=300,133 300w, https://venturebeat.com/wp-content/uploads/2024/11/stage-level-beam-search.jpg?resize=768,340 768w, https://venturebeat.com/wp-content/uploads/2024/11/stage-level-beam-search.jpg?resize=800,354 800w, https://venturebeat.com/wp-content/uploads/2024/11/stage-level-beam-search.jpg?resize=400,177 400w, https://venturebeat.com/wp-content/uploads/2024/11/stage-level-beam-search.jpg?resize=750,332 750w, https://venturebeat.com/wp-content/uploads/2024/11/stage-level-beam-search.jpg?resize=578,256 578w, https://venturebeat.com/wp-content/uploads/2024/11/stage-level-beam-search.jpg?resize=930,412 930w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eStage-level beam search (right) vs other inference-time scaling techniques Source: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eLLaVA-o1 also introduces a novel inference-time scaling technique called “stage-level beam search.” Stage-level beam search generates multiple candidate outputs at each reasoning stage. It then selects the best candidate at each stage to continue the generation process. This is in contrast to the classic best-of-N approach, in which the model is prompted to generate multiple complete responses before selecting one.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Notably, it is the structured output design of LLaVA-o1 that makes this approach feasible, enabling efficient and accurate verification at each stage,” the researchers write. “This validates the effectiveness of structured output in improving inference time scaling.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-training-llava-o1\"\u003eTraining LLaVA-o1\u003c/h2\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"800\" height=\"566\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/Llava-o1-training-data.jpg?w=800\" alt=\"Llava o1 training data\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/11/Llava-o1-training-data.jpg 800w, https://venturebeat.com/wp-content/uploads/2024/11/Llava-o1-training-data.jpg?resize=300,212 300w, https://venturebeat.com/wp-content/uploads/2024/11/Llava-o1-training-data.jpg?resize=768,543 768w, https://venturebeat.com/wp-content/uploads/2024/11/Llava-o1-training-data.jpg?resize=400,283 400w, https://venturebeat.com/wp-content/uploads/2024/11/Llava-o1-training-data.jpg?resize=750,531 750w, https://venturebeat.com/wp-content/uploads/2024/11/Llava-o1-training-data.jpg?resize=578,409 578w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eLLaVA-o1 training data is annotated with GPT-4o Source: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eTo train LLaVA-o1, the researchers compiled a new dataset of around 100,000 image-question-answer pairs obtained from several widely used VQA datasets. The dataset covers a variety of tasks, from multi-turn question answering to chart interpretation and geometric reasoning.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers used \u003ca href=\"https://venturebeat.com/ai/gpt-4o-first-reactions-essentially-agi/\"\u003eGPT-4o\u003c/a\u003e to generate the detailed four-stage reasoning processes for each example, including the summary, caption, reasoning and conclusion stages. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers then fine-tuned \u003ca href=\"https://venturebeat.com/ai/meta-llama-3-2-vision-models-to-rival-anthropic-openai/\"\u003eLlama-3.2-11B-Vision-Instruct\u003c/a\u003e on this dataset to obtain the final LLaVA-o1 model. The researchers have not released the model but plan to release the dataset, called the LLaVA-o1-100k.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-llava-o1-in-action\"\u003eLLaVA-o1 in action\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers evaluated LLaVA-o1 on several multimodal reasoning benchmarks.  Despite being trained on only 100,000 examples, LLaVA-o1 showed significant performance improvements over the base Llama model, with an average benchmark score increase of 6.9%.  \u003c/p\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"800\" height=\"623\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/LLaVA-o1-results.jpg?w=770\" alt=\"LLaVA-o1 results\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/11/LLaVA-o1-results.jpg 800w, https://venturebeat.com/wp-content/uploads/2024/11/LLaVA-o1-results.jpg?resize=300,234 300w, https://venturebeat.com/wp-content/uploads/2024/11/LLaVA-o1-results.jpg?resize=768,598 768w, https://venturebeat.com/wp-content/uploads/2024/11/LLaVA-o1-results.jpg?resize=770,600 770w, https://venturebeat.com/wp-content/uploads/2024/11/LLaVA-o1-results.jpg?resize=400,312 400w, https://venturebeat.com/wp-content/uploads/2024/11/LLaVA-o1-results.jpg?resize=750,584 750w, https://venturebeat.com/wp-content/uploads/2024/11/LLaVA-o1-results.jpg?resize=578,450 578w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eLLaVA-o1 vs other open and closed models Source: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eFurthermore, stage-level beam search led to additional performance gains, demonstrating the effectiveness of inference-time scaling. Due to computational resource constraints, the researchers were only able to test the technique with a beam size of 2. They expect even greater improvements with larger beam sizes.\u003c/p\u003e\n\n\n\n\u003cp\u003eImpressively, LLaVA-o1 outperformed not only other open-source models of the same size or larger but also some closed-source models like \u003ca href=\"https://venturebeat.com/ai/openai-unveils-gpt-4o-mini-a-smaller-much-cheaper-multimodal-ai-model/\"\u003eGPT-4-o-mini\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/googles-gemini-1-5-pro-leaps-ahead-in-ai-race-challenging-gpt-4o/\"\u003eGemini 1.5 Pro\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003e“LLaVA-o1 establishes a new standard for multimodal reasoning in VLMs, offering robust performance and scalability, especially in inference time,” the researchers write. “Our work paves the way for future research on structured reasoning in VLMs, including potential expansions with external verifiers and the use of reinforcement learning to further enhance complex multimodal reasoning capabilities.”\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-11-22T23:26:42Z",
  "modifiedTime": "2024-11-22T23:26:51Z"
}
