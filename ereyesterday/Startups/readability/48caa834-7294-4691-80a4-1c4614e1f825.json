{
  "id": "48caa834-7294-4691-80a4-1c4614e1f825",
  "title": "Show HN: Can I run this LLM? (locally)",
  "link": "https://can-i-run-this-llm-blue.vercel.app/",
  "description": "One of the most frequent questions one faces while running LLMs locally is: I have xx RAM and yy GPU, Can I run zz LLM model ? I have vibe coded a simple application to help you with just that.Update: A lot of great feedback for me to improve the app. Thank you all. Comments URL: https://news.ycombinator.com/item?id=43304436 Points: 28 # Comments: 32",
  "author": "asasidh",
  "published": "Sat, 08 Mar 2025 23:08:20 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "",
  "length": 0,
  "excerpt": "",
  "siteName": "",
  "favicon": "",
  "text": "",
  "image": "",
  "html": "",
  "readingTime": "Less than 1 min",
  "publishedTime": null,
  "modifiedTime": null
}
