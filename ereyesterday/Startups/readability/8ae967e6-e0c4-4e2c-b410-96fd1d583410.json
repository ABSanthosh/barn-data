{
  "id": "8ae967e6-e0c4-4e2c-b410-96fd1d583410",
  "title": "Unintended consequences: U.S. election results herald reckless AI development",
  "link": "https://venturebeat.com/ai/unintended-consequences-u-s-election-results-herald-reckless-ai-development/",
  "description": "AI accelerationists have won as a consequence of the election, potentially sidelining those advocating for a more cautious approach.",
  "author": "Gary Grossman, Edelman",
  "published": "Sun, 22 Dec 2024 20:05:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "AI regulation",
    "AI regulatory compliance",
    "AI, ML and Deep Learning",
    "category-/News/Politics",
    "category-/Science",
    "Conversational AI",
    "Generative AI",
    "large language models",
    "NLP"
  ],
  "byline": "Gary Grossman, Edelman",
  "length": 8792,
  "excerpt": "AI accelerationists have won as a consequence of the election, potentially sidelining those advocating for a more cautious approach.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 22, 2024 12:05 PM Dall-E prompt by Grossman. Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More While the 2024 U.S. election focused on traditional issues like the economy and immigration, its quiet impact on AI policy could prove even more transformative. Without a single debate question or major campaign promise about AI, voters inadvertently tipped the scales in favor of accelerationists — those who advocate for rapid AI development with minimal regulatory hurdles. The implications of this acceleration are profound, heralding a new era of AI policy that prioritizes innovation over caution and signals a decisive shift in the debate between AI’s potential risks and rewards. The pro-business stance of President-elect Donald Trump leads many to assume that his administration will favor those developing and marketing AI and other advanced technologies. His party platform has little to say about AI. However, it does emphasize a policy approach focused on repealing AI regulations, particularly targeting what they described as “radical left-wing ideas” within existing executive orders of the outgoing administration. In contrast, the platform supported AI development aimed at fostering free speech and “human flourishing,” calling for policies that enable innovation in AI while opposing measures perceived to hinder technological progress. Early indications based on appointments to leading government positions underscore this direction. However, there is a larger story unfolding: The resolution of the intense debate over AI’s future. An intense debate Ever since ChatGPT appeared in November 2022, there has been a raging debate between those in the AI field who want to accelerate AI development and those who want to decelerate. Famously, in March 2023 the latter group proposed a six-month AI pause in development of the most advanced systems, warning in an open letter that AI tools present “profound risks to society and humanity.” This letter, spearheaded by the Future of Life Institute, was prompted by OpenAI’s release of the GPT-4 large language model (LLM), several months after ChatGPT launched. The letter was initially signed by more than 1,000 technology leaders and researchers, including Elon Musk, Apple Co-founder Steve Wozniak, 2020 Presidential candidate Andrew Yang, podcaster Lex Fridman, and AI pioneers Yoshua Bengio and Stuart Russell. The number of signees of the letter eventually swelled to more than 33,000. Collectively, they became known as “doomers,” a term to capture their concerns about potential existential risks from AI. Not everyone agreed. OpenAI CEO Sam Altman did not sign. Nor did Bill Gates and many others. Their reasons for not doing so varied, although many voiced concerns about potential harm from AI. This led to many conversations about the potential for AI to run amok, leading to disaster. It became fashionable for many in the AI field to talk about their assessment of the probability of doom, often referred to as an equation: p(doom). Nevertheless, work on AI development did not pause. For the record, my p(doom) in June 2023 was 5%. That might seem low, but it was not zero. I felt that the major AI labs were sincere in their efforts to stringently test new models prior to release and in providing significant guardrails for their use. Many observers concerned about AI dangers have rated existential risks higher than 5%, and some have rated much higher. AI safety researcher Roman Yampolskiy rated the probability of AI ending humanity at over 99%. That said, a study released early this year, well before the election and representing the views of more than 2,700 AI researchers, showed that “the median prediction for extremely bad outcomes, such as human extinction, was 5%.” Would you board a plane if there were a 5% chance it might crash? This is the dilemma AI researchers and policymakers face. Must go faster Others have been openly dismissive of worries about AI, pointing instead to what they perceived as the huge upside of the technology. These include Andrew Ng (who founded and led the Google Brain project) and Pedro Domingos (a professor of computer science and engineering at the University of Washington and author of “The Master Algorithm”). They argued, instead, that AI is part of the solution. As put forward by Ng, there are indeed existential dangers, such as climate change and future pandemics, and AI can be part of how these are addressed and mitigated. Ng argued that AI development should not be paused, but should instead go faster. This utopian view of technology has been echoed by others who are collectively known as “effective accelerationists” or “e/acc” for short. They argue that technology — and especially AI — is not the problem, but the solution to most, if not all, of the world’s issues. Startup accelerator Y Combinator CEO Garry Tan, along with other prominent Silicon Valley leaders, included the term “e/acc” in their usernames on X to show alignment to the vision. Reporter Kevin Roose at the New York Times captured the essence of these accelerationists by saying they have  an “all-gas, no-brakes approach.” A Substack newsletter from a couple years ago described the principles underlying effective accelerationism. Here is the summation they offer at the end of the article, plus a comment from OpenAI CEO Sam Altman. AI acceleration ahead The 2024 election outcome may be seen as a turning point, putting the accelerationist vision in a position to shape U.S. AI policy for the next several years. For example, the President-elect recently appointed technology entrepreneur and venture capitalist David Sacks as “AI czar.” Sacks, a vocal critic of AI regulation and a proponent of market-driven innovation, brings his experience as a technology investor to this role. He is one of the leading voices in the AI industry, and much of what he has said about AI aligns with the accelerationist viewpoints expressed by the incoming party platform. In response to the AI executive order from the Biden administration in 2023, Sacks tweeted: “The U.S. political and fiscal situation is hopelessly broken, but we have one unparalleled asset as a country: Cutting-edge innovation in AI driven by a completely free and unregulated market for software development. That just ended.” While the amount of influence Sacks will have on AI policy remains to be seen, his appointment signals a shift toward policies favoring industry self-regulation and rapid innovation. Elections have consequences I doubt most of the voting public gave much thought to AI policy implications when casting their votes. Nevertheless, in a very tangible way, the accelerationists have won as a consequence of the election, potentially sidelining those advocating for a more cautious approach by the federal government to mitigate AI’s long-term risks. As accelerationists chart the path forward, the stakes could not be higher. Whether this era ushers in unparalleled progress or unintended catastrophe remains to be seen. As AI development accelerates, the need for informed public discourse and vigilant oversight becomes ever more paramount. How we navigate this era will define not only technological progress but also our collective future. As a counterbalance to a lack of action at the federal level, it is possible that one or more states will adopt various regulations, which has already happened to some extent in California and Colorado. For instance, California’s AI safety bills focus on transparency requirements, while Colorado addresses AI discrimination in hiring practices, offering models for state-level governance. Now, all eyes will be on the voluntary testing and self-imposed guardrails at Anthropic, Google, OpenAI and other AI model developers. In summary, the accelerationist victory means less restrictions on AI innovation. This increased speed may indeed lead to faster innovation, but also raises the risk of unintended consequences. I’m now revising my p(doom) to 10%. What is yours? Gary Grossman is EVP of technology practice at Edelman and global lead of the Edelman AI Center of Excellence. DataDecisionMakers Welcome to the VentureBeat community! DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation. If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers. You might even consider contributing an article of your own! Read More From DataDecisionMakers",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/Cover-image.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-22T20:05:00+00:00\" datetime=\"2024-12-22T20:05:00+00:00\"\u003eDecember 22, 2024 12:05 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"400\" height=\"229\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/Cover-image.webp?w=400\" alt=\"Dall-E prompt by Grossman. \"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDall-E prompt by Grossman. \u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eWhile the 2024 U.S. election focused on traditional issues like the economy and immigration, its quiet impact on \u003ca href=\"https://venturebeat.com/ai/ai-regulation-in-peril-navigating-uncertain-times/\"\u003eAI policy\u003c/a\u003e could prove even more transformative. Without a single debate question or major campaign promise about AI, voters inadvertently tipped the scales in favor of accelerationists — those who advocate for rapid AI development with minimal regulatory hurdles. The implications of this acceleration are profound, heralding a new era of AI policy that prioritizes innovation over caution and signals a decisive shift in the debate between \u003ca href=\"https://venturebeat.com/ai/the-end-of-ai-scaling-may-not-be-nigh-heres-whats-next/\"\u003eAI’s potential risks and rewards\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe pro-business stance of President-elect Donald Trump leads many to assume that his administration will favor those developing and marketing AI and other advanced technologies. His party \u003ca href=\"https://prod-static.gop.com/media/RNC2024-Platform.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplatform\u003c/a\u003e has little to say about AI. However, it does emphasize a policy approach focused on repealing AI regulations, particularly targeting what they described as “radical left-wing ideas” within existing executive orders of the outgoing administration. In contrast, the platform supported AI development aimed at fostering free speech and “human flourishing,” calling for policies that enable innovation in AI while opposing measures perceived to hinder technological progress.\u003c/p\u003e\n\n\n\n\u003cp\u003eEarly indications based on appointments to leading government positions underscore this direction. However, there is a larger story unfolding: The resolution of the intense debate over \u003ca href=\"https://venturebeat.com/ai/large-language-overkill-how-slms-can-beat-their-bigger-resource-intensive-cousins/\"\u003eAI’s future\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-an-intense-debate\"\u003eAn intense debate\u003c/h2\u003e\n\n\n\n\u003cp\u003eEver since \u003ca href=\"https://venturebeat.com/ai/chatgpts-second-birthday-what-will-gen-ai-and-the-world-look-like-in-another-2-years/\"\u003eChatGPT\u003c/a\u003e appeared in November 2022, there has been a raging debate between those in the AI field who want to accelerate AI development and those who want to decelerate.\u003c/p\u003e\n\n\n\n\u003cp\u003eFamously, in March 2023 the latter group proposed a six-month AI pause in development of the most advanced systems, warning in \u003ca href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ean open letter\u003c/a\u003e that AI tools present “profound risks to society and humanity.” This letter, spearheaded by the \u003ca href=\"https://futureoflife.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eFuture of Life Institute\u003c/a\u003e, was prompted by OpenAI’s release of the GPT-4 large language model (LLM), several months after ChatGPT launched.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe letter was initially signed by more than 1,000 technology leaders and researchers, including Elon Musk, Apple Co-founder Steve Wozniak, 2020 Presidential candidate Andrew Yang, podcaster Lex Fridman, and AI pioneers Yoshua Bengio and Stuart Russell. The number of signees of the letter eventually swelled to more than 33,000. Collectively, they became known as “doomers,” a term to capture their concerns about potential existential risks from AI.\u003c/p\u003e\n\n\n\n\u003cp\u003eNot everyone agreed. OpenAI CEO Sam Altman did not sign. Nor did Bill Gates and many others. Their reasons for not doing so varied, although many voiced concerns about potential harm from AI. This led to many conversations about the potential for AI to run amok, leading to disaster. It became fashionable for many in the AI field to talk about their \u003ca href=\"https://www.nytimes.com/2023/12/06/business/dealbook/silicon-valley-artificial-intelligence.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eassessment of the probability of doom\u003c/a\u003e, often referred to as an equation: p(doom). Nevertheless, work on AI development did not pause.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor the record, \u003ca href=\"https://venturebeat.com/ai/ai-doom-ai-boom-and-the-possible-destruction-of-humanity/\"\u003emy p(doom) in June 2023 was 5%\u003c/a\u003e. That might seem low, but it was not zero. I felt that the major AI labs were sincere in their efforts to stringently test new models prior to release and in providing significant guardrails for their use.\u003c/p\u003e\n\n\n\n\u003cp\u003eMany observers concerned about AI dangers have rated existential risks higher than 5%, and some have rated much higher. AI safety researcher Roman Yampolskiy rated the probability of AI \u003ca href=\"https://www.youtube.com/watch?v=NNr6gPelJ3E\u0026amp;t=113s\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eending humanity at over 99%\u003c/a\u003e. That said, a \u003ca href=\"https://arxiv.org/html/2401.02843v2\" target=\"_blank\" rel=\"noreferrer noopener\"\u003estudy\u003c/a\u003e released early this year, well before the election and representing the views of more than 2,700 AI researchers, showed that “the median prediction for extremely bad outcomes, such as human extinction, was 5%.” Would you board a plane if there were a 5% chance it might crash? This is the dilemma AI researchers and policymakers face.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-must-go-faster\"\u003eMust go faster\u003c/h2\u003e\n\n\n\n\u003cp\u003eOthers have been openly dismissive of worries about AI, pointing instead to what they perceived as the huge upside of the technology. These include Andrew Ng (who founded and led the Google Brain project) and Pedro Domingos (a professor of computer science and engineering at the University of Washington and author of “\u003ca href=\"https://www.hachettebookgroup.com/titles/pedro-domingos/the-master-algorithm/9780465094271/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eThe Master Algorithm\u003c/a\u003e”). They argued, instead, that AI is part of the solution. As put forward by Ng, there are indeed existential dangers, such as climate change and future pandemics, and AI can be part of how these are addressed and mitigated.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"624\" height=\"336\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/Domingos-Ng-Twitter.jpg\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/Domingos-Ng-Twitter.jpg 624w, https://venturebeat.com/wp-content/uploads/2024/12/Domingos-Ng-Twitter.jpg?resize=300,162 300w, https://venturebeat.com/wp-content/uploads/2024/12/Domingos-Ng-Twitter.jpg?resize=400,215 400w, https://venturebeat.com/wp-content/uploads/2024/12/Domingos-Ng-Twitter.jpg?resize=578,311 578w\" sizes=\"(max-width: 624px) 100vw, 624px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eNg argued that AI development should not be paused, but should instead go faster. This utopian view of technology has been echoed by others who are collectively known as “effective accelerationists” or “e/acc” for short. They argue that technology — and especially AI — is not the problem, but the solution to most, if not all, of the world’s issues. Startup accelerator \u003ca href=\"https://www.ycombinator.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eY Combinator\u003c/a\u003e CEO Garry Tan, along with other prominent Silicon Valley leaders, included the term “e/acc” in their usernames on X to show alignment to the vision. Reporter Kevin Roose at the New York Times \u003ca href=\"https://www.nytimes.com/2023/12/10/technology/ai-acceleration.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecaptured the essence\u003c/a\u003e of these accelerationists by saying they have  an “all-gas, no-brakes approach.”\u003c/p\u003e\n\n\n\n\u003cp\u003eA Substack \u003ca href=\"https://beff.substack.com/p/notes-on-eacc-principles-and-tenets\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enewsletter\u003c/a\u003e from a couple years ago described the principles underlying effective accelerationism. Here is the summation they offer at the end of the article, plus a comment from OpenAI CEO Sam Altman.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"684\" height=\"529\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/Accelerate.jpg\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/Accelerate.jpg 684w, https://venturebeat.com/wp-content/uploads/2024/12/Accelerate.jpg?resize=300,232 300w, https://venturebeat.com/wp-content/uploads/2024/12/Accelerate.jpg?resize=400,309 400w, https://venturebeat.com/wp-content/uploads/2024/12/Accelerate.jpg?resize=578,447 578w\" sizes=\"(max-width: 684px) 100vw, 684px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-ai-acceleration-ahead\"\u003eAI acceleration ahead\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe 2024 election outcome may be seen as a turning point, putting the accelerationist vision in a position to shape U.S. AI policy for the next several years. For example, the President-elect recently appointed technology entrepreneur and venture capitalist David Sacks as “AI czar.”\u003c/p\u003e\n\n\n\n\u003cp\u003eSacks, a vocal critic of AI regulation and a proponent of market-driven innovation, brings his experience as a technology investor to this role. He is one of the leading voices in the AI industry, and much of what he has said about AI aligns with the accelerationist viewpoints expressed by the incoming party platform.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn response to the AI executive order from the Biden administration in 2023, Sacks \u003ca href=\"https://x.com/DavidSacks/status/1719436981086900530\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etweeted\u003c/a\u003e: “The U.S. political and fiscal situation is hopelessly broken, but we have one unparalleled asset as a country: Cutting-edge innovation in AI driven by a completely free and unregulated market for software development. That just ended.” While the amount of influence Sacks will have on AI policy remains to be seen, his appointment signals a shift toward policies favoring industry self-regulation and rapid innovation.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-elections-have-consequences\"\u003eElections have consequences\u003c/h2\u003e\n\n\n\n\u003cp\u003eI doubt most of the voting public gave much thought to AI policy implications when casting their votes. Nevertheless, in a very tangible way, the accelerationists have won as a consequence of the election, potentially sidelining those advocating for a more cautious approach by the federal government to mitigate AI’s long-term risks.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs accelerationists chart the path forward, the stakes could not be higher. Whether this era ushers in unparalleled progress or unintended catastrophe remains to be seen. As AI development accelerates, the need for informed public discourse and vigilant oversight becomes ever more paramount. How we navigate this era will define not only technological progress but also our collective future.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs a counterbalance to a lack of action at the federal level, it is possible that one or more states will adopt various regulations, which has already happened to some extent in \u003ca href=\"https://calmatters.org/economy/technology/2024/09/california-ai-safety-regulations-bills/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCalifornia\u003c/a\u003e and \u003ca href=\"https://natlawreview.com/article/colorado-becomes-first-state-enact-comprehensive-ai-legislation\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eColorado\u003c/a\u003e. For instance, California’s AI safety bills focus on transparency requirements, while Colorado addresses AI discrimination in hiring practices, offering models for state-level governance. Now, all eyes will be on the voluntary testing and self-imposed guardrails at Anthropic, Google, OpenAI and other AI model developers.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn summary, the accelerationist victory means less restrictions on AI innovation. This increased speed may indeed lead to faster innovation, but also raises the risk of unintended consequences. I’m now revising my p(doom) to 10%. What is yours?\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eGary Grossman is EVP of technology practice at \u003ca href=\"https://www.edelman.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eEdelman\u003c/a\u003e and global lead of the Edelman AI Center of Excellence.\u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2736392\"\u003e\n\u003cp\u003e\u003cstrong\u003eDataDecisionMakers\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWelcome to the VentureBeat community!\u003c/p\u003e\n\n\n\n\u003cp\u003eDataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might even consider \u003ca rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://venturebeat.com/guest-posts/\"\u003econtributing an article\u003c/a\u003e of your own!\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca rel=\"noreferrer noopener\" href=\"https://venturebeat.com/category/DataDecisionMakers/\" target=\"_blank\"\u003eRead More From DataDecisionMakers\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2024-12-22T20:05:00Z",
  "modifiedTime": "2024-12-21T20:58:17Z"
}
