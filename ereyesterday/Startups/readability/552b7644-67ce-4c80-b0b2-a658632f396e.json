{
  "id": "552b7644-67ce-4c80-b0b2-a658632f396e",
  "title": "DeepSeek unveils new technique for smarter, scalable AI reward models",
  "link": "https://venturebeat.com/ai/deepseek-unveils-new-technique-for-smarter-scalable-ai-reward-models/",
  "description": "Reward models holding back AI? DeepSeek's SPCT creates self-guiding critiques, promising more scalable intelligence for enterprise LLMs.",
  "author": "Ben Dickson",
  "published": "Tue, 08 Apr 2025 22:33:13 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI applications",
    "AI research",
    "AI, ML and Deep Learning",
    "category-/Computers \u0026 Electronics",
    "category-/Science/Computer Science",
    "Deepseek AI",
    "gemma 2",
    "gen AI",
    "Generative AI",
    "generative reward modeling (GRM)",
    "gpt-4o",
    "large language models",
    "large language models (LLMs)",
    "LLMs",
    "reinforcement learning",
    "research",
    "reward models (RMs)",
    "Self-Principled Critique Tuning (SPCT)"
  ],
  "byline": "Ben Dickson",
  "length": 9777,
  "excerpt": "Reward models holding back AI? DeepSeek's SPCT creates self-guiding critiques, promising more scalable intelligence for enterprise LLMs.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "April 8, 2025 3:33 PM Image credit: VentureBeat with Ideogram Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More DeepSeek AI, a Chinese research lab gaining recognition for its powerful open-source language models such as DeepSeek-R1, has introduced a significant advancement in reward modeling for large language models (LLMs).  Their new technique, Self-Principled Critique Tuning (SPCT), aims to create generalist and scalable reward models (RMs). This could potentially lead to more capable AI applications for open-ended tasks and domains where current models can’t capture the nuances and complexities of their environment and users. The crucial role and current limits of reward models Reinforcement learning (RL) has become a cornerstone in developing state-of-the-art LLMs. In RL, models are fine-tuned based on feedback signals that indicate the quality of their responses.  Reward models are the critical component that provides these signals. Essentially, an RM acts as a judge, evaluating LLM outputs and assigning a score or “reward” that guides the RL process and teaches the LLM to produce more useful responses. However, current RMs often face limitations. They typically excel in narrow domains with clear-cut rules or easily verifiable answers. For example, current state-of-the-art reasoning models such as DeepSeek-R1 underwent an RL phase, in which they were trained on math and coding problems where the ground truth is clearly defined. However, creating a reward model for complex, open-ended, or subjective queries in general domains remains a major hurdle. In the paper explaining their new technique, researchers at DeepSeek AI write, “Generalist RM requires to generate high-quality rewards beyond specific domains, where the criteria for rewards are more diverse and complex, and there are often no explicit reference or ground truth.”  They highlight four key challenges in creating generalist RMs capable of handling broader tasks: Input flexibility: The RM must handle various input types and be able to evaluate one or more responses simultaneously. Accuracy: It must generate accurate reward signals across diverse domains where the criteria are complex and the ground truth is often unavailable.  Inference-time scalability: The RM should produce higher-quality rewards when more computational resources are allocated during inference. Learning scalable behaviors: For RMs to scale effectively at inference time, they need to learn behaviors that allow for improved performance as more computation is used. Different types of reward models Credit: arXiv Reward models can be broadly classified by their “reward generation paradigm” (e.g., scalar RMs outputting a single score, generative RMs producing textual critiques) and their “scoring pattern” (e.g., pointwise scoring assigns individual scores to each response, pairwise selects the better of two responses). These design choices affect the model’s suitability for generalist tasks, particularly its input flexibility and potential for inference-time scaling.  For instance, simple scalar RMs struggle with inference-time scaling because they will generate the same score repeatedly, while pairwise RMs can’t easily rate single responses.  The researchers propose that “pointwise generative reward modeling” (GRM), where the model generates textual critiques and derives scores from them, can offer the flexibility and scalability required for generalist requirements. The DeepSeek team conducted preliminary experiments on models like GPT-4o and Gemma-2-27B, and found that “certain principles could guide reward generation within proper criteria for GRMs, improving the quality of rewards, which inspired us that inference-time scalability of RM might be achieved by scaling the generation of high-quality principles and accurate critiques.”  Training RMs to generate their own principles Based on these findings, the researchers developed Self-Principled Critique Tuning (SPCT), which trains the GRM to generate principles and critiques based on queries and responses dynamically.  The researchers propose that principles should be a “part of reward generation instead of a preprocessing step.” This way, the GRMs could generate principles on the fly based on the task they are evaluating and then generate critiques based on the principles.  “This shift enables [the] principles to be generated based on the input query and responses, adaptively aligning [the] reward generation process, and the quality and granularity of the principles and corresponding critiques could be further improved with post-training on the GRM,” the researchers write. Self-Principled Critique Tuning (SPCT) Credit: arXiv SPCT involves two main phases: Rejective fine-tuning: This phase trains the GRM to generate principles and critiques for various input types using the correct format. The model generates principles, critiques and rewards for given queries/responses. Trajectories (generation attempts) are accepted only if the predicted reward aligns with the ground truth (correctly identifying the better response, for instance) and rejected otherwise. This process is repeated and the model is fine-tuned on the filtered examples to improve its principle/critique generation capabilities. Rule-based RL: In this phase, the model is further fine-tuned through outcome-based reinforcement learning. The GRM generates principles and critiques for each query, and the reward signals are calculated based on simple accuracy rules (e.g., did it pick the known best response?). Then the model is updated. This encourages the GRM to learn how to generate effective principles and accurate critiques dynamically and in a scalable way. “By leveraging rule-based online RL, SPCT enables GRMs to learn to adaptively posit principles and critiques based on the input query and responses, leading to better outcome rewards in general domains,” the researchers write. To tackle the inference-time scaling challenge (getting better results with more compute), the researchers run the GRM multiple times for the same input, generating different sets of principles and critiques. The final reward is determined by voting (aggregating the sample scores). This allows the model to consider a broader range of perspectives, leading to potentially more accurate and nuanced final judgments as it is provided with more resources. However, some generated principles/critiques might be low-quality or biased due to model limitations or randomness. To address this, the researchers introduced a “meta RM”—a separate, lightweight scalar RM trained specifically to predict whether a principle/critique generated by the primary GRM will likely lead to a correct final reward.  During inference, the meta RM evaluates the generated samples and filters out the low-quality judgments before the final voting, further enhancing scaling performance. Putting SPCT into practice with DeepSeek-GRM The researchers applied SPCT to Gemma-2-27B, Google’s open-weight model, creating DeepSeek-GRM-27B. They evaluated it against several strong baseline RMs (including LLM-as-a-Judge, scalar RMs, and semi-scalar RMs) and public models (like GPT-4o and Nemotron-4-340B-Reward) across multiple benchmarks. They found that DeepSeek-GRM-27B outperformed baseline methods trained on the same data. SPCT significantly improved the quality and, crucially, the inference-time scalability compared to standard fine-tuning. The performance of DeepSeek-GRM (trained with SPCT) continues to improve with inference-time scaling Credit: arXiv When scaled at inference time by generating more samples, DeepSeek-GRM-27B’s performance increased substantially, surpassing even much larger models like Nemotron-4-340B-Reward and GPT-4o. The meta RM further improved the scaling, achieving the best results by filtering judgments.  “With larger-scale sampling, DeepSeek-GRM could judge more accurately upon principles with higher diversity, and output rewards with finer granularity,” the researchers write. Interestingly, SPCT showed less bias across different domains compared to scalar RMs, which often performed well on verifiable tasks but poorly elsewhere. Implications for the enterprise Developing more generalist and scalable reward models can be promising for enterprise AI applications. Potential areas that can benefit from generalist RMs include creative tasks and applications where the model must adapt to dynamic environments such as evolving customer preferences.  Despite the strong results, DeepSeek-GRM still lags behind specialized scalar RMs on purely verifiable tasks where explicit reasoning generation might be less efficient than direct scoring. Efficiency also remains a challenge compared to non-generative RMs.  The DeepSeek team suggests future work will focus on efficiency improvements and deeper integration. As they conclude, “Future directions could include integrating GRMs into online RL pipelines as versatile interfaces of reward systems, exploring inference-time co-scaling with policy models, or serving as robust offline evaluators for foundation models.”  Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/04/deepseek-reward-model.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-04-08T22:33:13+00:00\" datetime=\"2025-04-08T22:33:13+00:00\"\u003eApril 8, 2025 3:33 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"421\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/deepseek-reward-model.webp?w=750\" alt=\"deepseek reward model\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with Ideogram\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://deepseek.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDeepSeek AI\u003c/a\u003e, a Chinese research lab gaining recognition for its powerful open-source language models such as DeepSeek-R1, has introduced a significant advancement in reward modeling for large language models (LLMs). \u003c/p\u003e\n\n\n\n\u003cp\u003eTheir new technique, Self-Principled Critique Tuning (SPCT), aims to create generalist and scalable reward models (RMs). This could potentially lead to more capable AI applications for open-ended tasks and domains where current models can’t capture the nuances and complexities of their environment and users.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-crucial-role-and-current-limits-of-reward-models\"\u003eThe crucial role and current limits of reward models\u003c/h2\u003e\n\n\n\n\u003cp\u003eReinforcement learning (RL) has become a cornerstone in developing state-of-the-art LLMs. In RL, models are fine-tuned based on feedback signals that indicate the quality of their responses. \u003c/p\u003e\n\n\n\n\u003cp\u003eReward models are the critical component that provides these signals. Essentially, an RM acts as a judge, evaluating LLM outputs and assigning a score or “reward” that guides the RL process and teaches the LLM to produce more useful responses.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, current RMs often face limitations. They typically excel in narrow domains with clear-cut rules or easily verifiable answers. For example, current state-of-the-art reasoning models such as \u003ca href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/\"\u003eDeepSeek-R1 underwent an RL phase\u003c/a\u003e, in which they were trained on math and coding problems where the ground truth is clearly defined.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, creating a reward model for complex, open-ended, or subjective queries in general domains remains a major hurdle. In \u003ca href=\"http://arxiv.org/abs/2504.02495\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe paper\u003c/a\u003e explaining their new technique, researchers at DeepSeek AI write, “Generalist RM requires to generate high-quality rewards beyond specific domains, where the criteria for rewards are more diverse and complex, and there are often no explicit reference or ground truth.” \u003c/p\u003e\n\n\n\n\u003cp\u003eThey highlight four key challenges in creating generalist RMs capable of handling broader tasks:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eInput flexibility:\u003c/strong\u003e The RM must handle various input types and be able to evaluate one or more responses simultaneously.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cspan\u003e\u003cstrong\u003eAccuracy:\u003c/strong\u003e It must generate accurate reward signals across diverse domains where the criteria are complex and the ground truth is often unavailable.\u003c/span\u003e \u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eInference-time scalability:\u003c/strong\u003e The RM should produce higher-quality rewards when more computational resources are allocated during inference.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eLearning scalable behaviors:\u003c/strong\u003e For RMs to scale effectively at inference time, they need to learn behaviors that allow for improved performance as more computation is used.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1294\" height=\"620\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_28cd78.png?w=800\" alt=\"Different types of reward models\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_28cd78.png 1294w, https://venturebeat.com/wp-content/uploads/2025/04/image_28cd78.png?resize=300,144 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_28cd78.png?resize=768,368 768w, https://venturebeat.com/wp-content/uploads/2025/04/image_28cd78.png?resize=800,383 800w, https://venturebeat.com/wp-content/uploads/2025/04/image_28cd78.png?resize=400,192 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_28cd78.png?resize=750,359 750w, https://venturebeat.com/wp-content/uploads/2025/04/image_28cd78.png?resize=578,277 578w, https://venturebeat.com/wp-content/uploads/2025/04/image_28cd78.png?resize=930,446 930w\" sizes=\"(max-width: 1294px) 100vw, 1294px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eDifferent types of reward models Credit: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eReward models can be broadly classified by their “reward generation paradigm” (e.g., scalar RMs outputting a single score, generative RMs producing textual critiques) and their “scoring pattern” (e.g., pointwise scoring assigns individual scores to each response, pairwise selects the better of two responses). These design choices affect the model’s suitability for generalist tasks, particularly its \u003cstrong\u003einput flexibility\u003c/strong\u003e and potential for \u003cstrong\u003einference-time scaling\u003c/strong\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor instance, simple scalar RMs struggle with inference-time scaling because they will generate the same score repeatedly, while pairwise RMs can’t easily rate single responses. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers propose that “pointwise generative reward modeling” (GRM), where the model generates textual critiques and derives scores from them, can offer the flexibility and scalability required for generalist requirements.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe DeepSeek team conducted preliminary experiments on models like GPT-4o and Gemma-2-27B, and found that “certain principles could guide reward generation within proper criteria for GRMs, improving the quality of rewards, which inspired us that inference-time scalability of RM might be achieved by scaling the generation of high-quality principles and accurate critiques.” \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-training-rms-to-generate-their-own-principles\"\u003eTraining RMs to generate their own principles\u003c/h2\u003e\n\n\n\n\u003cp\u003eBased on these findings, the researchers developed Self-Principled Critique Tuning (SPCT), which trains the GRM to generate principles and critiques based on queries and responses dynamically. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers propose that principles should be a “part of reward generation instead of a preprocessing step.” This way, the GRMs could generate principles on the fly based on the task they are evaluating and then generate critiques based on the principles. \u003c/p\u003e\n\n\n\n\u003cp\u003e“This shift enables [the] principles to be generated based on the input query and responses, adaptively aligning [the] reward generation process, and the quality and granularity of the principles and corresponding critiques could be further improved with post-training on the GRM,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1256\" height=\"828\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_de71b7.png?w=800\" alt=\"SPCT\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_de71b7.png 1256w, https://venturebeat.com/wp-content/uploads/2025/04/image_de71b7.png?resize=300,198 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_de71b7.png?resize=768,506 768w, https://venturebeat.com/wp-content/uploads/2025/04/image_de71b7.png?resize=800,527 800w, https://venturebeat.com/wp-content/uploads/2025/04/image_de71b7.png?resize=400,264 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_de71b7.png?resize=750,494 750w, https://venturebeat.com/wp-content/uploads/2025/04/image_de71b7.png?resize=578,381 578w, https://venturebeat.com/wp-content/uploads/2025/04/image_de71b7.png?resize=930,613 930w\" sizes=\"(max-width: 1256px) 100vw, 1256px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eSelf-Principled Critique Tuning (SPCT) Credit: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eSPCT involves two main phases:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eRejective fine-tuning:\u003c/strong\u003e This phase trains the GRM to generate principles and critiques for various input types using the correct format. The model generates principles, critiques and rewards for given queries/responses. Trajectories (generation attempts) are accepted only if the predicted reward aligns with the ground truth (correctly identifying the better response, for instance) and rejected otherwise. This process is repeated and the model is fine-tuned on the filtered examples to improve its principle/critique generation capabilities.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eRule-based RL:\u003c/strong\u003e In this phase, the model is further fine-tuned through outcome-based reinforcement learning. The GRM generates principles and critiques for each query, and the reward signals are calculated based on simple accuracy rules (e.g., did it pick the known best response?). Then the model is updated. This encourages the GRM to learn how to generate effective principles and accurate critiques dynamically and in a scalable way.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003e“By leveraging rule-based online RL, SPCT enables GRMs to learn to adaptively posit principles and critiques based on the input query and responses, leading to better outcome rewards in general domains,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo tackle the inference-time scaling challenge (getting better results with more compute), the researchers run the GRM multiple times for the same input, generating different sets of principles and critiques. The final reward is determined by voting (aggregating the sample scores). This allows the model to consider a broader range of perspectives, leading to potentially more accurate and nuanced final judgments as it is provided with more resources.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, some generated principles/critiques might be low-quality or biased due to model limitations or randomness. To address this, the researchers introduced a “meta\u003cem\u003e \u003c/em\u003eRM”—a separate, lightweight scalar RM trained specifically to predict whether a principle/critique generated by the primary GRM will likely lead to a correct final reward. \u003c/p\u003e\n\n\n\n\u003cp\u003eDuring inference, the meta RM evaluates the generated samples and filters out the low-quality judgments before the final voting, further enhancing scaling performance.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-putting-spct-into-practice-with-deepseek-grm\"\u003ePutting SPCT into practice with DeepSeek-GRM\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers applied SPCT to \u003ca href=\"https://venturebeat.com/ai/google-gemma-2-27-billion-lightweight-model/\"\u003eGemma-2-27B\u003c/a\u003e, Google’s open-weight model, creating DeepSeek-GRM-27B. They evaluated it against several strong baseline RMs (including LLM-as-a-Judge, scalar RMs, and semi-scalar RMs) and public models (like GPT-4o and Nemotron-4-340B-Reward) across multiple benchmarks.\u003c/p\u003e\n\n\n\n\u003cp\u003eThey found that DeepSeek-GRM-27B outperformed baseline methods trained on the same data. SPCT significantly improved the quality and, crucially, the inference-time scalability compared to standard fine-tuning.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"656\" height=\"410\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_2430ca.png\" alt=\"DeepSeek-GRM\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_2430ca.png 656w, https://venturebeat.com/wp-content/uploads/2025/04/image_2430ca.png?resize=300,188 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_2430ca.png?resize=400,250 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_2430ca.png?resize=578,361 578w\" sizes=\"(max-width: 656px) 100vw, 656px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eThe performance of DeepSeek-GRM (trained with SPCT) continues to improve with inference-time scaling Credit: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhen scaled at inference time by generating more samples, DeepSeek-GRM-27B’s performance increased substantially, surpassing even much larger models like \u003ca href=\"https://venturebeat.com/ai/nvidias-nemotron-4-340b-model-redefines-synthetic-data-generation-rivals-gpt-4/\"\u003eNemotron-4-340B-Reward\u003c/a\u003e and GPT-4o. The meta RM further improved the scaling, achieving the best results by filtering judgments. \u003c/p\u003e\n\n\n\n\u003cp\u003e“With larger-scale sampling, DeepSeek-GRM could judge more accurately upon principles with higher diversity, and output rewards with finer granularity,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cp\u003eInterestingly, SPCT showed less bias across different domains compared to scalar RMs, which often performed well on verifiable tasks but poorly elsewhere.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-implications-for-the-enterprise\"\u003eImplications for the enterprise\u003c/h2\u003e\n\n\n\n\u003cp\u003eDeveloping more generalist and scalable reward models can be promising for enterprise AI applications. Potential areas that can benefit from generalist RMs include creative tasks and applications where the model must adapt to dynamic environments such as evolving customer preferences. \u003c/p\u003e\n\n\n\n\u003cp\u003eDespite the strong results, DeepSeek-GRM still lags behind specialized scalar RMs on purely verifiable tasks where explicit reasoning generation might be less efficient than direct scoring. Efficiency also remains a challenge compared to non-generative RMs. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe DeepSeek team suggests future work will focus on efficiency improvements and deeper integration. As they conclude, “Future directions could include integrating GRMs into online RL pipelines as versatile interfaces of reward systems, exploring inference-time co-scaling with policy models, or serving as robust offline evaluators for foundation models.” \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-04-08T22:33:13Z",
  "modifiedTime": "2025-04-08T22:33:25Z"
}
