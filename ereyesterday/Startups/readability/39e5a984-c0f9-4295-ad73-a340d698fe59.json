{
  "id": "39e5a984-c0f9-4295-ad73-a340d698fe59",
  "title": "Analyzing Modern Nvidia GPU Cores",
  "link": "https://arxiv.org/abs/2503.20481",
  "description": "Article URL: https://arxiv.org/abs/2503.20481 Comments URL: https://news.ycombinator.com/item?id=43900463 Points: 25 # Comments: 1",
  "author": "mfiguiere",
  "published": "Mon, 05 May 2025 23:38:56 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "[Submitted on 26 Mar 2025]",
  "length": 1716,
  "excerpt": "GPUs are the most popular platform for accelerating HPC workloads, such as artificial intelligence and science simulations. However, most microarchitectural research in academia relies on GPU core pipeline designs based on architectures that are more than 15 years old. This paper reverse engineers modern NVIDIA GPU cores, unveiling many key aspects of its design and explaining how GPUs leverage hardware-compiler techniques where the compiler guides hardware during execution. In particular, it reveals how the issue logic works including the policy of the issue scheduler, the structure of the register file and its associated cache, and multiple features of the memory pipeline. Moreover, it analyses how a simple instruction prefetcher based on a stream buffer fits well with modern NVIDIA GPUs and is likely to be used. Furthermore, we investigate the impact of the register file cache and the number of register file read ports on both simulation accuracy and performance. By modeling all these new discovered microarchitectural details, we achieve 18.24% lower mean absolute percentage error (MAPE) in execution cycles than previous state-of-the-art simulators, resulting in an average of 13.98% MAPE with respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that this new model stands for other NVIDIA architectures, such as Turing. Finally, we show that the software-based dependence management mechanism included in modern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in terms of performance and area.",
  "siteName": "arXiv.org",
  "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png",
  "text": "View PDF Abstract:GPUs are the most popular platform for accelerating HPC workloads, such as artificial intelligence and science simulations. However, most microarchitectural research in academia relies on GPU core pipeline designs based on architectures that are more than 15 years old. This paper reverse engineers modern NVIDIA GPU cores, unveiling many key aspects of its design and explaining how GPUs leverage hardware-compiler techniques where the compiler guides hardware during execution. In particular, it reveals how the issue logic works including the policy of the issue scheduler, the structure of the register file and its associated cache, and multiple features of the memory pipeline. Moreover, it analyses how a simple instruction prefetcher based on a stream buffer fits well with modern NVIDIA GPUs and is likely to be used. Furthermore, we investigate the impact of the register file cache and the number of register file read ports on both simulation accuracy and performance. By modeling all these new discovered microarchitectural details, we achieve 18.24% lower mean absolute percentage error (MAPE) in execution cycles than previous state-of-the-art simulators, resulting in an average of 13.98% MAPE with respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that this new model stands for other NVIDIA architectures, such as Turing. Finally, we show that the software-based dependence management mechanism included in modern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in terms of performance and area. Submission history From: Rodrigo Huerta [view email] [v1] Wed, 26 Mar 2025 12:10:53 UTC (246 KB)",
  "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"content-inner\"\u003e\n    \n    \n                \n    \u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2503.20481\"\u003eView PDF\u003c/a\u003e\u003c/p\u003e\u003cblockquote\u003e\n            \u003cspan\u003eAbstract:\u003c/span\u003eGPUs are the most popular platform for accelerating HPC workloads, such as artificial intelligence and science simulations. However, most microarchitectural research in academia relies on GPU core pipeline designs based on architectures that are more than 15 years old.\n\u003cbr/\u003eThis paper reverse engineers modern NVIDIA GPU cores, unveiling many key aspects of its design and explaining how GPUs leverage hardware-compiler techniques where the compiler guides hardware during execution. In particular, it reveals how the issue logic works including the policy of the issue scheduler, the structure of the register file and its associated cache, and multiple features of the memory pipeline. Moreover, it analyses how a simple instruction prefetcher based on a stream buffer fits well with modern NVIDIA GPUs and is likely to be used. Furthermore, we investigate the impact of the register file cache and the number of register file read ports on both simulation accuracy and performance.\n\u003cbr/\u003eBy modeling all these new discovered microarchitectural details, we achieve 18.24% lower mean absolute percentage error (MAPE) in execution cycles than previous state-of-the-art simulators, resulting in an average of 13.98% MAPE with respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that this new model stands for other NVIDIA architectures, such as Turing. Finally, we show that the software-based dependence management mechanism included in modern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in terms of performance and area.\n    \u003c/blockquote\u003e\n\n    \n    \n  \u003c/div\u003e\u003cdiv\u003e\n      \u003ch2\u003eSubmission history\u003c/h2\u003e\u003cp\u003e From: Rodrigo Huerta [\u003ca href=\"https://arxiv.org/show-email/03496778/2503.20481\" rel=\"nofollow\"\u003eview email\u003c/a\u003e]      \u003cbr/\u003e    \u003cstrong\u003e[v1]\u003c/strong\u003e\n        Wed, 26 Mar 2025 12:10:53 UTC (246 KB)\u003cbr/\u003e\n\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "2 min read",
  "publishedTime": null,
  "modifiedTime": null
}
