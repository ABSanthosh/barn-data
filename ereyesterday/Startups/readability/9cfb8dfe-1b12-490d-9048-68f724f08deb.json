{
  "id": "9cfb8dfe-1b12-490d-9048-68f724f08deb",
  "title": "Migrating to Postgres",
  "link": "https://engineering.usemotion.com/migrating-to-postgres-3c93dff9c65d",
  "description": "Article URL: https://engineering.usemotion.com/migrating-to-postgres-3c93dff9c65d Comments URL: https://news.ycombinator.com/item?id=43989497 Points: 87 # Comments: 41",
  "author": "shenli3514",
  "published": "Wed, 14 May 2025 21:39:45 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "Sean Callahan",
  "length": 11240,
  "excerpt": "Since early 2022, Motion was on CockroachDB. Cockroach has many qualities going for it: effortless horizontal scaling, especially when dealing with multi-region set ups, extremely high availability…",
  "siteName": "Motion Engineering Blog",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*sphTgcj9aI9rf4ZFYYR7dg.png",
  "text": "Migrating to PostgresSince early 2022, Motion was on CockroachDB. Cockroach has many qualities going for it: effortless horizontal scaling, especially when dealing with multi-region set ups, extremely high availability, and a SQL-compatible interface. Early on, there were concerns about the eventuality of a multi-region setup (mandated by GDPR), and how exactly a traditional set up on Postgres would scale.However, as Motion grew, so did our usage and costs. By 2024, Motion’s CockroachDB bill had 5x-ed to the mid 6 figures, and some of the cracks were beginning to show. None of our customers had required data localization yet, and we were still in a single region doing fairly simple transactional queries — so why pay the cost of a distributed database at all?Fortunately for us, we had an ORM that made testing head to head relatively trivial.MigrationsAs the size of our database grew, we would frequently get into situations where Prisma would simply time out when applying migrations. We’d then have to log into Cockroach and manually run migrations one by one, concurrently of course.The deploy was blocked for ~2 hours while we figured out why Cockroach was timing outWhen I finally moved us to Postgres, I tested a very similar migration head to head (the name of the column is different, just for demo purposes). Here. is Postgres applying this migration on a rewound instance of our database (to control for data size) in ten seconds.Adding a new “foobar” column on the TeamMember table and running the Prisma migration.The timeouts were leading to operational shortcuts. By 2024, the timeouts had become so acute that even really strong developers were actively doing things outside the DB purely out of fear of locking the entire system.Even worse, the migration issues were proving to be a blocker to upgrade even Cockroach versions. We were stuck on version 22 well past the end of life, which made support even slower to reply to us.In the end, we ended up just giving up and opting to move to Postgres, remaining on the defunct version 22 until the end.ETLThe timeouts began affecting things outside of migrations — namely, ETL.A typical error from Airbyte stallingWe’d become accustomed to waking to pages like this one:Even when the ETL jobs worked and did not time out, the performance was brutal.Unfortunately, there’s very little support for any real ETL solutions that offer CRDB replication. As of this writing, there is still only Airbyte’s connector (which was in alpha in 2024), and we discovered only after implementing that the connector had a memory leak.Query SpeedsThe query head to heads were interesting. Some queries were indeed faster on Cockroach, thanks to their optimizer:An example query where Cockroach was fasterFor queries like the above, Cockroach would resolve in ~13 seconds while Postgres would take up to 20 seconds. It seems like the Cockroach query planner innately understands these queries and does aggregation while Postgres naively ends up doing a near full table scan. (This advantage seems to go away when comparing Drizzle instead of the SQL generated by Prisma. More on that in a separate blog post.)But there were many situations when the “magic” of the Cockroach query planner seemed to cut the other way. Here is the Prisma code and the generated SQL, for which Cockroach opts to do a full table scan while Postgres does not:Note the `AND 1=1` at the end, resulting in a scan of the entire tableThe vast majority of our real world queries seemed to follow this general pattern where Prisma generates very convoluted SQL with numerous inclusions of various columns and joins. That, combined with the magical Cockroach optimizer, ultimately resulted in extremely high latencies. Here’s a real example of a query that is twenty times faster in Postgres than Cockroach.SELECT \"teamTasks\".\"id\", \"teamTasks\".\"name\", \"teamTasks\".\"description\", \"teamTasks\".\"dueDate\", \"teamTasks\".\"duration\", \"teamTasks\".\"completedTime\", \"teamTasks\".\"assigneeUserId\", \"teamTasks\".\"priorityLevel\", \"teamTasks\".\"statusId\", \"teamTasks\".\"projectId\", \"teamTasks\".\"createdByUserId\", \"teamTasks\".\"createdTime\", \"teamTasks\".\"updatedTime\", \"teamTasks\".\"lastInteractedTime\", \"teamTasks\".\"archivedTime\", \"teamTasks\".\"workspaceId\", \"teamTasks\".\"minimumDuration\", \"teamTasks\".\"scheduledStart\", \"teamTasks\".\"scheduledEnd\", \"teamTasks\".\"startDate\", \"teamTasks\".\"isChunkedTask\", \"teamTasks\".\"isUnfit\", \"teamTasks\".\"type\", \"teamTasks\".\"needsReschedule\", \"teamTasks\".\"deadlineType\", \"teamTasks\".\"schedule\", \"teamTasks\".\"parentChunkTaskId\", \"teamTasks\".\"parentRecurringTaskId\", \"teamTasks\".\"rank\", \"teamTasks\".\"isFixedTimeTask\", \"teamTasks\".\"slug\", \"teamTasks\".\"previousSlugs\", \"teamTasks\".\"endDate\", \"teamTasks\".\"isBusy\", \"teamTasks\".\"isAutoScheduled\", \"teamTasks\".\"ignoreWarnOnPastDue\", \"teamTasks\".\"scheduleOverridden\", \"teamTasks\".\"snoozeUntil\", \"teamTasks\".\"manuallyStarted\", \"teamTasks_chunks\".\"data\" AS \"chunks\", \"teamTasks_labels\".\"data\" AS \"labels\", \"teamTasks_blockedTasks\".\"data\" AS \"blockedTasks\", \"teamTasks_blockingTasks\".\"data\" AS \"blockingTasks\"FROM \"TeamTask\" \"teamTasks\" LEFT JOIN LATERAL ( SELECT coalesce(json_agg(json_build_array(\"teamTasks_chunks\".\"id\")), '[]'::json) AS \"data\" FROM \"TeamTask\" \"teamTasks_chunks\" WHERE \"teamTasks_chunks\".\"parentChunkTaskId\" = \"teamTasks\".\"id\") \"teamTasks_chunks\" ON TRUE LEFT JOIN LATERAL ( SELECT coalesce(json_agg(json_build_array(\"teamTasks_labels\".\"labelId\")), '[]'::json) AS \"data\" FROM \"TeamTaskLabel\" \"teamTasks_labels\" WHERE \"teamTasks_labels\".\"taskId\" = \"teamTasks\".\"id\") \"teamTasks_labels\" ON TRUE LEFT JOIN LATERAL ( SELECT coalesce(json_agg(json_build_array(\"teamTasks_blockedTasks\".\"blockingId\")), '[]'::json) AS \"data\" FROM \"TeamTaskBlocker\" \"teamTasks_blockedTasks\" WHERE \"teamTasks_blockedTasks\".\"blockedId\" = \"teamTasks\".\"id\") \"teamTasks_blockedTasks\" ON TRUE LEFT JOIN LATERAL ( SELECT coalesce(json_agg(json_build_array(\"teamTasks_blockingTasks\".\"blockedId\")), '[]'::json) AS \"data\" FROM \"TeamTaskBlocker\" \"teamTasks_blockingTasks\" WHERE \"teamTasks_blockingTasks\".\"blockingId\" = \"teamTasks\".\"id\") \"teamTasks_blockingTasks\" ON TRUEWHERE (\"teamTasks\".\"workspaceId\" in( SELECT \"workspaceId\" FROM \"WorkspaceMember\" WHERE (\"WorkspaceMember\".\"userId\" = '{{user_id}}' AND \"WorkspaceMember\".\"deletedTime\" IS NULL)) and(\"teamTasks\".\"workspaceId\" in('{{workspace_id}}') and(\"teamTasks\".\"id\" in( SELECT \"taskId\" FROM \"TeamTaskLabel\" WHERE \"TeamTaskLabel\".\"labelId\" in('{{label_id}}')) OR \"teamTasks\".\"id\" NOT in( SELECT \"taskId\" FROM \"TeamTaskLabel\")) AND \"teamTasks\".\"archivedTime\" IS NULL AND \"teamTasks\".\"completedTime\" IS NULL))Most of our queries on the Team Tasks table were, on average, three times slower on Cockroach compared to Postgres.UI/UX IssuesThere were a handful of UI issues that also plagued us throughout the process.Unused IndicesThe UI for unused indices would show a mostly used indices, which periodically led to confusion by developers. We never got to the bottom of whether this was due to us using Prisma or not.Cockroach’s list of recommendations, advocating developers dropped indexes that are still used2. Cancelling running queriesBy now we knew that running expensive queries on Cockroach was extremely scary. But since it’s a distributed cluster, cancelling a query isn’t so simple. With Postgres, you can simply hit cancel on TablePlus (or your preferred SQL client). On Cockroach, you have to actually log into the console and cancel the query, and pray all nodes cancel before they fall over. (At least once, they did not. You can probably guess what happened.)3. SupportFirst, the support portal is a totally different website that doesn’t share auth with the main portal. Second, you have to re-input a lot of data they already know about you (cluster ID, etc). And by the time they respond it’s typically been a week. Normally that’s fine, but once they rolled out a bug which of course took us down immediately. That was not the time to log into a separate portal and input mundane details.VPC IssuesThroughout the over two years we were using Cockroach, we consistently ran into periodic Tailscale connectivity issues.getaddrinfo ENOTFOUND internal-motion-dev-2022-10-grc.gcp-us-central1.cockroachlabs.cloudPrismaClientInitializationError: Can't reach database server at 'internal-motion-prod-gnp.gcp-us-central1.cockroachlabs.cloud':'26257'No matter what we did though, these issues would suddenly arise, and, an hour later, just as suddenly disappear. These issues persisted in every environment (Airbyte connectors, CI, and our local TablePlus clients). We never solved it, and have never had similar issues with Postgres.The MigrationBy Jan 2024, our largest table had roughly 100 million rows. As mentioned earlier, while there were many tools built to import data into Cockroach, there were no ETL tools other than a very alpha Airbyte connector that kept timing out due to a memory leak.So I decided to build a custom ETL solution.Around this time, Bun was gaining popularity, so this migration was a convenient excuse to satisfy my curiosity. The Bun script roughly did the following steps:Read the database schema and all of its table informationDump the data of each table to a dedicated file on diskSpawn a Bun child process for each table in our schemaEach child process would then initiate a streaming connection of the data from the dumped data for that tableThe stream was just a CSV stream of all the rows in the tableThe “sink” for our stream would just insert the rows into PostgresI got to this point relatively quickly and should have known this was going too well to be true. It wasn’t until I started to run test migrations on our production database that I learned that Cockroach used slightly different byte encoding in JSON and array columns than Postgres did. The following few weeks involved using Csv-js to put together a custom CSV parsing pipeline to transform all of the data from Cockroach to something that was compatible with Postgres, but also identical from a user perspective.When the night of the migration came, I spun up the largest VM I could get my hands on GCP (128 core VM), turned on Motion’s maintenance mode, and let it rip. The production migration script from start to finish took roughly 15 minutes to run through the entire DB.Post MigrationEverything you read above was done by one person (me), over the course of several weeks. In total, we were down for just under an hour from midnight to 1am pacific time — and of course, there was zero data loss. We could have been even more aggressive and limited the downtime to just 15 minutes, but we opted to be super safe and gradually ramp traffic back up.Afterwards, we saw an immediate 33% drop in aggregated request latencies. But the exciting part was just beginning. Thanks to the Postgres ecosystem and tools like PGAnalyze, we fixed half a dozen unoptimized queries in just a few hours after the migration.Despite being conservative and over provisioning our postgres cluster, we still ended up saving the business a little over $110,000 a year (would be even higher when factoring in the continued growth of traffic to the system).",
  "image": "https://miro.medium.com/v2/resize:fit:1084/1*dxp1_R2A5K0eWlUjF8UV7A@2x.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"5426\" data-testid=\"storyTitle\"\u003eMigrating to Postgres\u003c/h2\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/@scalahansolo?source=post_page---byline--3c93dff9c65d---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Sean Callahan\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*UE43O9jCIGdEE415dA0FZQ.jpeg\" width=\"32\" height=\"32\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cp id=\"a150\"\u003eSince early 2022, Motion was on \u003ca href=\"https://www.cockroachlabs.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCockroachDB\u003c/a\u003e. Cockroach has many qualities going for it: effortless horizontal scaling, especially when dealing with multi-region set ups, extremely high availability, and a SQL-compatible interface. Early on, there were concerns about the eventuality of a multi-region setup (mandated by GDPR), and how exactly a traditional set up on Postgres would scale.\u003c/p\u003e\u003cp id=\"2af4\"\u003eHowever, as Motion grew, so did our usage and costs. By 2024, Motion’s CockroachDB bill had 5x-ed to the mid 6 figures, and some of the cracks were beginning to show. None of our customers had required data localization yet, and we were still in a single region doing fairly simple transactional queries — so why pay the cost of a distributed database at all?\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"67b3\"\u003eFortunately for us, we had an ORM that made testing head to head relatively trivial.\u003c/p\u003e\u003ch2 id=\"6d75\"\u003eMigrations\u003c/h2\u003e\u003cp id=\"2cd0\"\u003eAs the size of our database grew, we would frequently get into situations where Prisma would simply time out when applying migrations. We’d then have to log into Cockroach and manually run migrations one by one, concurrently of course.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eThe deploy was blocked for ~2 hours while we figured out why Cockroach was timing out\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"3da2\"\u003eWhen I finally moved us to Postgres, I tested a very similar migration head to head (the name of the column is different, just for demo purposes). Here. is Postgres applying this migration on a rewound instance of our database (to control for data size) in \u003cem\u003eten seconds\u003c/em\u003e.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eAdding a new “foobar” column on the TeamMember table and running the Prisma migration.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"de41\"\u003eThe timeouts were leading to operational shortcuts. By 2024, the timeouts had become so acute that even really strong\u003cem\u003e \u003c/em\u003edevelopers were actively doing things outside the DB purely out of fear of locking the entire system.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"b43d\"\u003eEven worse, the migration issues were proving to be a blocker to upgrade even Cockroach versions. We were stuck on version 22 well past the end of life, which made support \u003cem\u003eeven slower\u003c/em\u003e to reply to us.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"288a\"\u003eIn the end, we ended up just giving up and opting to move to Postgres, remaining on the defunct version 22 until the end.\u003c/p\u003e\u003ch2 id=\"65d9\"\u003eETL\u003c/h2\u003e\u003cp id=\"abba\"\u003eThe timeouts began affecting things outside of migrations — namely, ETL.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eA typical error from Airbyte stalling\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"31f4\"\u003eWe’d become accustomed to waking to pages like this one:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"6779\"\u003eEven when the ETL jobs worked and did not time out, the performance was \u003cem\u003ebrutal\u003c/em\u003e.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"080a\"\u003eUnfortunately, there’s \u003cem\u003every\u003c/em\u003e little support for any real ETL solutions that offer CRDB replication. As of this writing, there is still only \u003ca href=\"https://docs.airbyte.com/integrations/sources/cockroachdb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAirbyte’s connector\u003c/a\u003e (which was in alpha in 2024), and we discovered only after implementing that the connector had a memory leak.\u003c/p\u003e\u003ch2 id=\"8d5c\"\u003eQuery Speeds\u003c/h2\u003e\u003cp id=\"4921\"\u003eThe query head to heads were interesting. Some queries were indeed faster on Cockroach, thanks to their optimizer:\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eAn example query where Cockroach was faster\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"2aea\"\u003eFor queries like the above, Cockroach would resolve in ~13 seconds while Postgres would take up to 20 seconds. It seems like the Cockroach query planner innately understands these queries and does aggregation while Postgres naively ends up doing a near full table scan. (This advantage seems to go away when comparing \u003ca href=\"https://orm.drizzle.team/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDrizzle\u003c/a\u003e instead of the SQL generated by Prisma. More on that in a separate blog post.)\u003c/p\u003e\u003cp id=\"a0ba\"\u003eBut there were many situations when the “magic” of the Cockroach query planner seemed to cut the other way. Here is the Prisma code and the generated SQL, for which Cockroach opts to do a full table scan while Postgres does not:\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eNote the `AND 1=1` at the end, resulting in a scan of the entire table\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"f75b\"\u003eThe \u003cem\u003evast\u003c/em\u003e majority of our real world queries seemed to follow this general pattern where Prisma generates very convoluted SQL with numerous inclusions of various columns and joins. That, combined with the magical Cockroach optimizer, ultimately resulted in extremely high latencies. Here’s a real example of a query that is\u003cem\u003e twenty\u003c/em\u003e times faster in Postgres than Cockroach.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"e599\"\u003eSELECT\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;id\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;name\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;description\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;dueDate\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;duration\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;completedTime\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;assigneeUserId\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;priorityLevel\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;statusId\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;projectId\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;createdByUserId\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;createdTime\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;updatedTime\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;lastInteractedTime\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;archivedTime\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;workspaceId\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;minimumDuration\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;scheduledStart\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;scheduledEnd\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;startDate\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;isChunkedTask\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;isUnfit\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;type\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;needsReschedule\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;deadlineType\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;schedule\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;parentChunkTaskId\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;parentRecurringTaskId\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;rank\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;isFixedTimeTask\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;slug\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;previousSlugs\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;endDate\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;isBusy\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;isAutoScheduled\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;ignoreWarnOnPastDue\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;scheduleOverridden\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;snoozeUntil\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks\u0026#34;.\u0026#34;manuallyStarted\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks_chunks\u0026#34;.\u0026#34;data\u0026#34; AS \u0026#34;chunks\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks_labels\u0026#34;.\u0026#34;data\u0026#34; AS \u0026#34;labels\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks_blockedTasks\u0026#34;.\u0026#34;data\u0026#34; AS \u0026#34;blockedTasks\u0026#34;,\u003cbr/\u003e \u0026#34;teamTasks_blockingTasks\u0026#34;.\u0026#34;data\u0026#34; AS \u0026#34;blockingTasks\u0026#34;\u003cbr/\u003eFROM\u003cbr/\u003e \u0026#34;TeamTask\u0026#34; \u0026#34;teamTasks\u0026#34;\u003cbr/\u003e LEFT JOIN LATERAL (\u003cbr/\u003e  SELECT\u003cbr/\u003e   coalesce(json_agg(json_build_array(\u0026#34;teamTasks_chunks\u0026#34;.\u0026#34;id\u0026#34;)), \u0026#39;[]\u0026#39;::json) AS \u0026#34;data\u0026#34;\u003cbr/\u003e  FROM\u003cbr/\u003e   \u0026#34;TeamTask\u0026#34; \u0026#34;teamTasks_chunks\u0026#34;\u003cbr/\u003e  WHERE\u003cbr/\u003e   \u0026#34;teamTasks_chunks\u0026#34;.\u0026#34;parentChunkTaskId\u0026#34; = \u0026#34;teamTasks\u0026#34;.\u0026#34;id\u0026#34;) \u0026#34;teamTasks_chunks\u0026#34; ON TRUE\u003cbr/\u003e LEFT JOIN LATERAL (\u003cbr/\u003e  SELECT\u003cbr/\u003e   coalesce(json_agg(json_build_array(\u0026#34;teamTasks_labels\u0026#34;.\u0026#34;labelId\u0026#34;)), \u0026#39;[]\u0026#39;::json) AS \u0026#34;data\u0026#34;\u003cbr/\u003e  FROM\u003cbr/\u003e   \u0026#34;TeamTaskLabel\u0026#34; \u0026#34;teamTasks_labels\u0026#34;\u003cbr/\u003e  WHERE\u003cbr/\u003e   \u0026#34;teamTasks_labels\u0026#34;.\u0026#34;taskId\u0026#34; = \u0026#34;teamTasks\u0026#34;.\u0026#34;id\u0026#34;) \u0026#34;teamTasks_labels\u0026#34; ON TRUE\u003cbr/\u003e LEFT JOIN LATERAL (\u003cbr/\u003e  SELECT\u003cbr/\u003e   coalesce(json_agg(json_build_array(\u0026#34;teamTasks_blockedTasks\u0026#34;.\u0026#34;blockingId\u0026#34;)), \u0026#39;[]\u0026#39;::json) AS \u0026#34;data\u0026#34;\u003cbr/\u003e  FROM\u003cbr/\u003e   \u0026#34;TeamTaskBlocker\u0026#34; \u0026#34;teamTasks_blockedTasks\u0026#34;\u003cbr/\u003e  WHERE\u003cbr/\u003e   \u0026#34;teamTasks_blockedTasks\u0026#34;.\u0026#34;blockedId\u0026#34; = \u0026#34;teamTasks\u0026#34;.\u0026#34;id\u0026#34;) \u0026#34;teamTasks_blockedTasks\u0026#34; ON TRUE\u003cbr/\u003e LEFT JOIN LATERAL (\u003cbr/\u003e  SELECT\u003cbr/\u003e   coalesce(json_agg(json_build_array(\u0026#34;teamTasks_blockingTasks\u0026#34;.\u0026#34;blockedId\u0026#34;)), \u0026#39;[]\u0026#39;::json) AS \u0026#34;data\u0026#34;\u003cbr/\u003e  FROM\u003cbr/\u003e   \u0026#34;TeamTaskBlocker\u0026#34; \u0026#34;teamTasks_blockingTasks\u0026#34;\u003cbr/\u003e  WHERE\u003cbr/\u003e   \u0026#34;teamTasks_blockingTasks\u0026#34;.\u0026#34;blockingId\u0026#34; = \u0026#34;teamTasks\u0026#34;.\u0026#34;id\u0026#34;) \u0026#34;teamTasks_blockingTasks\u0026#34; ON TRUE\u003cbr/\u003eWHERE (\u0026#34;teamTasks\u0026#34;.\u0026#34;workspaceId\u0026#34; in(\u003cbr/\u003e  SELECT\u003cbr/\u003e   \u0026#34;workspaceId\u0026#34; FROM \u0026#34;WorkspaceMember\u0026#34;\u003cbr/\u003e  WHERE (\u0026#34;WorkspaceMember\u0026#34;.\u0026#34;userId\u0026#34; = \u0026#39;{{user_id}}\u0026#39;\u003cbr/\u003e   AND \u0026#34;WorkspaceMember\u0026#34;.\u0026#34;deletedTime\u0026#34; IS NULL))\u003cbr/\u003e and(\u0026#34;teamTasks\u0026#34;.\u0026#34;workspaceId\u0026#34; in(\u0026#39;{{workspace_id}}\u0026#39;)\u003cbr/\u003e  and(\u0026#34;teamTasks\u0026#34;.\u0026#34;id\u0026#34; in(\u003cbr/\u003e    SELECT\u003cbr/\u003e     \u0026#34;taskId\u0026#34; FROM \u0026#34;TeamTaskLabel\u0026#34;\u003cbr/\u003e    WHERE\u003cbr/\u003e     \u0026#34;TeamTaskLabel\u0026#34;.\u0026#34;labelId\u0026#34; in(\u0026#39;{{label_id}}\u0026#39;))\u003cbr/\u003e    OR \u0026#34;teamTasks\u0026#34;.\u0026#34;id\u0026#34; NOT in(\u003cbr/\u003e     SELECT\u003cbr/\u003e      \u0026#34;taskId\u0026#34; FROM \u0026#34;TeamTaskLabel\u0026#34;))\u003cbr/\u003e    AND \u0026#34;teamTasks\u0026#34;.\u0026#34;archivedTime\u0026#34; IS NULL\u003cbr/\u003e    AND \u0026#34;teamTasks\u0026#34;.\u0026#34;completedTime\u0026#34; IS NULL))\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"bcdb\"\u003eMost of our queries on the Team Tasks table were, on average, \u003cem\u003ethree times\u003c/em\u003e slower on Cockroach compared to Postgres.\u003c/p\u003e\u003ch2 id=\"102e\"\u003eUI/UX Issues\u003c/h2\u003e\u003cp id=\"e7ee\"\u003eThere were a handful of UI issues that also plagued us throughout the process.\u003c/p\u003e\u003col\u003e\u003cli id=\"9b00\"\u003eUnused Indices\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"095f\"\u003eThe UI for unused indices would show a mostly used indices, which periodically led to confusion by developers. We never got to the bottom of whether this was due to us using Prisma or not.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eCockroach’s list of recommendations, advocating developers dropped indexes that are still used\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"7479\"\u003e2. Cancelling running queries\u003c/p\u003e\u003cp id=\"ff64\"\u003eBy now we knew that running expensive queries on Cockroach was \u003cem\u003eextremely\u003c/em\u003e scary. But since it’s a distributed cluster, cancelling a query isn’t so simple. With Postgres, you can simply hit cancel on TablePlus (or your preferred SQL client). On Cockroach, you have to actually log into the console and cancel the query, and pray all nodes cancel before they fall over. (At least once, they did not. You can probably guess what happened.)\u003c/p\u003e\u003cp id=\"c0a8\"\u003e3. Support\u003c/p\u003e\u003cp id=\"8334\"\u003eFirst, the support portal is a totally different website that doesn’t share auth with the main portal. Second, you have to re-input a lot of data they already know about you (cluster ID, etc). And by the time they respond it’s typically been a week. Normally that’s fine, but once they rolled out a bug which of course took us down immediately. That was \u003cem\u003enot\u003c/em\u003e the time to log into a separate portal and input mundane details.\u003c/p\u003e\u003ch2 id=\"960a\"\u003eVPC Issues\u003c/h2\u003e\u003cp id=\"bf47\"\u003eThroughout the over two years we were using Cockroach, we consistently ran into periodic Tailscale connectivity issues.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"666a\"\u003egetaddrinfo ENOTFOUND internal-motion-dev-2022-10-grc.gcp-us-central1.cockroachlabs.cloud\u003cbr/\u003ePrismaClientInitializationError: Can\u0026#39;t reach database server at \u0026#39;internal-motion-prod-gnp.gcp-us-central1.cockroachlabs.cloud\u0026#39;:\u0026#39;26257\u0026#39;\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"423c\"\u003eNo matter what we did though, these issues would suddenly arise, and, an hour later, just as suddenly disappear. These issues persisted in every environment (Airbyte connectors, CI, and our local TablePlus clients). We never solved it, and have never had similar issues with Postgres.\u003c/p\u003e\u003ch2 id=\"84c9\"\u003eThe Migration\u003c/h2\u003e\u003cp id=\"07b6\"\u003eBy Jan 2024, our largest table had roughly 100 million rows. As mentioned earlier, while there were many tools built to \u003cem\u003eimport\u003c/em\u003e data into Cockroach, there were no ETL tools other than a very alpha Airbyte connector that kept timing out due to a memory leak.\u003c/p\u003e\u003cp id=\"b599\"\u003eSo I decided to build a custom ETL solution.\u003c/p\u003e\u003cp id=\"ec0b\"\u003eAround this time, \u003ca href=\"https://bun.sh/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBun\u003c/a\u003e was gaining popularity, so this migration was a convenient excuse to satisfy my curiosity. The Bun script roughly did the following steps:\u003c/p\u003e\u003cul\u003e\u003cli id=\"fe34\"\u003eRead the database schema and all of its table information\u003c/li\u003e\u003cli id=\"8db5\"\u003eDump the data of each table to a dedicated file on disk\u003c/li\u003e\u003cli id=\"ee4f\"\u003eSpawn a Bun \u003ca href=\"https://bun.sh/docs/api/spawn\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003echild process\u003c/a\u003e for each table in our schema\u003c/li\u003e\u003cli id=\"002d\"\u003eEach child process would then initiate a streaming connection of the data from the dumped data for that table\u003c/li\u003e\u003cli id=\"241d\"\u003eThe stream was just a CSV stream of all the rows in the table\u003c/li\u003e\u003cli id=\"c4fd\"\u003eThe “sink” for our stream would just insert the rows into Postgres\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"8275\"\u003eI got to this point relatively quickly and should have known this was going too well to be true. It wasn’t until I started to run test migrations on our production database that I learned that Cockroach used slightly different byte encoding in JSON and array columns than Postgres did. The following few weeks involved using \u003ca href=\"https://csv.js.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCsv-js\u003c/a\u003e to put together a custom CSV parsing pipeline to transform all of the data from Cockroach to something that was compatible with Postgres, but also identical from a user perspective.\u003c/p\u003e\u003cp id=\"5bdd\"\u003eWhen the night of the migration came, I spun up the largest VM I could get my hands on GCP (128 core VM), turned on Motion’s maintenance mode, and let it rip. The production migration script from start to finish took roughly 15 minutes to run through the entire DB.\u003c/p\u003e\u003ch2 id=\"ce3a\"\u003ePost Migration\u003c/h2\u003e\u003cp id=\"c9de\"\u003eEverything you read above was done by \u003cem\u003eone\u003c/em\u003e person (me), over the course of several weeks. In total, we were down for just under an hour from midnight to 1am pacific time — and of course, there was zero data loss. We could have been even more aggressive and limited the downtime to just 15 minutes, but we opted to be super safe and gradually ramp traffic back up.\u003c/p\u003e\u003cp id=\"07bd\"\u003eAfterwards, we saw an immediate 33% drop in aggregated request latencies. But the exciting part was just beginning. Thanks to the Postgres ecosystem and tools like \u003ca href=\"https://pganalyze.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePGAnalyze\u003c/a\u003e, we fixed half a dozen unoptimized queries in just a few hours after the migration.\u003c/p\u003e\u003cp id=\"e288\"\u003eDespite being conservative and \u003cem\u003eover \u003c/em\u003eprovisioning our postgres cluster, we still ended up saving the business a little over $110,000 a year (would be even higher when factoring in the continued growth of traffic to the system).\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2025-05-13T18:53:28.673Z",
  "modifiedTime": null
}
