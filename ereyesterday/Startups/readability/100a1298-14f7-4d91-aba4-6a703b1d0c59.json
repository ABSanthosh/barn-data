{
  "id": "100a1298-14f7-4d91-aba4-6a703b1d0c59",
  "title": "Meta Introduces Spirit LM open source model that combines text and speech inputs/outputs",
  "link": "https://venturebeat.com/ai/meta-introduces-spirit-lm-open-source-model-that-combines-text-and-speech-inputs-outputs/",
  "description": "Spirit LM Expressive incorporates emotional cues into its speech generation and can detect and reflect anger, surprise, or joy.",
  "author": "Carl Franzen",
  "published": "Sat, 19 Oct 2024 00:05:21 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI audio",
    "AI speech",
    "AI, ML and Deep Learning",
    "category-/Science/Computer Science",
    "Conversational AI",
    "LLMs",
    "Meta",
    "NLP",
    "speech-to-text",
    "spirit lm",
    "Text-to-Speech"
  ],
  "byline": "Carl Franzen",
  "length": 5862,
  "excerpt": "Spirit LM Expressive incorporates emotional cues into its speech generation and can detect and reflect anger, surprise, or joy.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "October 18, 2024 5:05 PM Credit: VentureBeat made with ChatGPT Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Just in time for Halloween 2024, Meta has unveiled Meta Spirit LM, the company’s first open-source multimodal language model capable of seamlessly integrating text and speech inputs and outputs. As such, it competes directly with OpenAI’s GPT-4o (also natively multimodal) and other multimodal models such as Hume’s EVI 2, as well as dedicated text-to-speech and speech-to-text offerings such as ElevenLabs. Designed by Meta’s Fundamental AI Research (FAIR) team, Spirit LM aims to address the limitations of existing AI voice experiences by offering a more expressive and natural-sounding speech generation, while learning tasks across modalities like automatic speech recognition (ASR), text-to-speech (TTS), and speech classification. Unfortunately for entrepreneurs and business leaders, the model is only currently available for non-commercial usage under Meta’s FAIR Noncommercial Research License, which e grants users the right to use, reproduce, modify, and create derivative works of the Meta Spirit LM models, but only for noncommercial purposes. Any distribution of these models or derivatives must also comply with the noncommercial restriction. A new approach to text and speech Traditional AI models for voice rely on automatic speech recognition to process spoken input before synthesizing it with a language model, which is then converted into speech using text-to-speech techniques. While effective, this process often sacrifices the expressive qualities inherent to human speech, such as tone and emotion. Meta Spirit LM introduces a more advanced solution by incorporating phonetic, pitch, and tone tokens to overcome these limitations. Meta has released two versions of Spirit LM: • Spirit LM Base: Uses phonetic tokens to process and generate speech. • Spirit LM Expressive: Includes additional tokens for pitch and tone, allowing the model to capture more nuanced emotional states, such as excitement or sadness, and reflect those in the generated speech. Both models are trained on a combination of text and speech datasets, allowing Spirit LM to perform cross-modal tasks like speech-to-text and text-to-speech, while maintaining the natural expressiveness of speech in its outputs. Open-source noncommercial — only available for research In line with Meta’s commitment to open science, the company has made Spirit LM fully open-source, providing researchers and developers with the model weights, code, and supporting documentation to build upon. Meta hopes that the open nature of Spirit LM will encourage the AI research community to explore new methods for integrating speech and text in AI systems. The release also includes a research paper detailing the model’s architecture and capabilities. Mark Zuckerberg, Meta’s CEO, has been a strong advocate for open-source AI, stating in a recent open letter that AI has the potential to “increase human productivity, creativity, and quality of life” while accelerating advancements in areas like medical research and scientific discovery. Applications and future potential Meta Spirit LM is designed to learn new tasks across various modalities, such as: • Automatic Speech Recognition (ASR): Converting spoken language into written text. • Text-to-Speech (TTS): Generating spoken language from written text. • Speech Classification: Identifying and categorizing speech based on its content or emotional tone. The Spirit LM Expressive model goes a step further by incorporating emotional cues into its speech generation. For instance, it can detect and reflect emotional states like anger, surprise, or joy in its output, making the interaction with AI more human-like and engaging. This has significant implications for applications like virtual assistants, customer service bots, and other interactive AI systems where more nuanced and expressive communication is essential. A broader effort Meta Spirit LM is part of a broader set of research tools and models that Meta FAIR is releasing to the public. This includes an update to Meta’s Segment Anything Model 2.1 (SAM 2.1) for image and video segmentation, which has been used across disciplines like medical imaging and meteorology, and research on enhancing the efficiency of large language models. Meta’s overarching goal is to achieve advanced machine intelligence (AMI), with an emphasis on developing AI systems that are both powerful and accessible. The FAIR team has been sharing its research for more than a decade, aiming to advance AI in a way that benefits not just the tech community, but society as a whole. Spirit LM is a key component of this effort, supporting open science and reproducibility while pushing the boundaries of what AI can achieve in natural language processing. What’s next for Spirit LM? With the release of Meta Spirit LM, Meta is taking a significant step forward in the integration of speech and text in AI systems. By offering a more natural and expressive approach to AI-generated speech, and making the model open-source, Meta is enabling the broader research community to explore new possibilities for multimodal AI applications. Whether in ASR, TTS, or beyond, Spirit LM represents a promising advance in the field of machine learning, with the potential to power a new generation of more human-like AI interactions. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/10/spirit-lm.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-10-19T00:05:21+00:00\" datetime=\"2024-10-19T00:05:21+00:00\"\u003eOctober 18, 2024 5:05 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"429\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/spirit-lm.png?w=750\" alt=\"comic book style pointillism image of a startled programmer watching a happy ghost emerge from a computer screen\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with ChatGPT\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eJust in time for Halloween 2024, Meta has unveiled \u003ca href=\"https://speechbot.github.io/spiritlm/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMeta Spirit LM\u003c/a\u003e, the company’s first open-source multimodal language model capable of seamlessly integrating text and speech inputs and outputs. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs such, it competes directly with\u003ca href=\"https://venturebeat.com/ai/openai-announces-new-free-model-gpt-4o-and-chatgpt-for-desktop/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e OpenAI’s GPT-4o (also natively multimodal)\u003c/a\u003e and other multimodal models such as \u003ca href=\"https://venturebeat.com/ai/who-needs-gpt-4o-voice-mode-humes-evi-2-is-here-with-emotionally-inflected-voice-ai-and-api/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHume’s EVI 2\u003c/a\u003e, as well as dedicated text-to-speech and speech-to-text offerings such as \u003ca href=\"https://venturebeat.com/ai/elevenlabs-launches-ios-app-that-turns-any-text-into-audio-narration-with-ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eElevenLabs\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eDesigned by Meta’s Fundamental AI Research (FAIR) team, Spirit LM aims to address the limitations of existing AI voice experiences by offering a more expressive and natural-sounding speech generation, while learning tasks across modalities like automatic speech recognition (ASR), text-to-speech (TTS), and speech classification.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnfortunately for entrepreneurs and business leaders, the model is only currently available for non-commercial usage under\u003ca href=\"https://github.com/facebookresearch/spiritlm/blob/main/LICENSE\"\u003e Meta’s FAIR Noncommercial Research License\u003c/a\u003e, which e grants users the right to use, reproduce, modify, and create derivative works of the Meta Spirit LM models, but only for noncommercial purposes. Any distribution of these models or derivatives must also comply with the noncommercial restriction.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-a-new-approach-to-text-and-speech\"\u003eA new approach to text and speech\u003c/h2\u003e\n\n\n\n\u003cp\u003eTraditional AI models for voice rely on automatic speech recognition to process spoken input before synthesizing it with a language model, which is then converted into speech using text-to-speech techniques. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile effective, this process often sacrifices the expressive qualities inherent to human speech, such as tone and emotion. Meta Spirit LM introduces a more advanced solution by incorporating phonetic, pitch, and tone tokens to overcome these limitations.\u003c/p\u003e\n\n\n\n\u003cp\u003eMeta has released two versions of Spirit LM:\u003c/p\u003e\n\n\n\n\u003cp\u003e• \u003cstrong\u003eSpirit LM Base\u003c/strong\u003e: Uses phonetic tokens to process and generate speech.\u003c/p\u003e\n\n\n\n\u003cp\u003e• \u003cstrong\u003eSpirit LM Expressive\u003c/strong\u003e: Includes additional tokens for pitch and tone, allowing the model to capture more nuanced emotional states, such as excitement or sadness, and reflect those in the generated speech.\u003c/p\u003e\n\n\n\n\u003cp\u003eBoth models are trained on a combination of text and speech datasets, allowing Spirit LM to perform cross-modal tasks like speech-to-text and text-to-speech, while maintaining the natural expressiveness of speech in its outputs.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-open-source-noncommercial-only-available-for-research\"\u003eOpen-source noncommercial — only available for research\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn line with Meta’s commitment to open science, the company has made Spirit LM fully open-source, providing researchers and developers with the model weights, code, and supporting documentation to build upon. \u003c/p\u003e\n\n\n\n\u003cp\u003eMeta hopes that the open nature of Spirit LM will encourage the AI research community to explore new methods for integrating speech and text in AI systems. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe release also includes a \u003ca href=\"https://arxiv.org/pdf/2402.05755\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eresearch paper\u003c/a\u003e detailing the model’s architecture and capabilities.\u003c/p\u003e\n\n\n\n\u003cp\u003eMark Zuckerberg, Meta’s CEO, has been a strong advocate for open-source AI, stating in a recent open letter that AI has the potential to “increase human productivity, creativity, and quality of life” while accelerating advancements in areas like medical research and scientific discovery.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-applications-and-future-potential\"\u003eApplications and future potential\u003c/h2\u003e\n\n\n\n\u003cp\u003eMeta Spirit LM is designed to learn new tasks across various modalities, such as:\u003c/p\u003e\n\n\n\n\u003cp\u003e• \u003cstrong\u003eAutomatic Speech Recognition (ASR)\u003c/strong\u003e: Converting spoken language into written text.\u003c/p\u003e\n\n\n\n\u003cp\u003e• \u003cstrong\u003eText-to-Speech (TTS)\u003c/strong\u003e: Generating spoken language from written text.\u003c/p\u003e\n\n\n\n\u003cp\u003e• \u003cstrong\u003eSpeech Classification\u003c/strong\u003e: Identifying and categorizing speech based on its content or emotional tone.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe \u003cstrong\u003eSpirit LM Expressive\u003c/strong\u003e model goes a step further by incorporating emotional cues into its speech generation. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor instance, it can detect and reflect emotional states like anger, surprise, or joy in its output, making the interaction with AI more human-like and engaging. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis has significant implications for applications like virtual assistants, customer service bots, and other interactive AI systems where more nuanced and expressive communication is essential.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-a-broader-effort\"\u003eA broader effort\u003c/h2\u003e\n\n\n\n\u003cp\u003eMeta Spirit LM is part of a broader set of research tools and models that Meta FAIR is releasing to the public. This includes an update to Meta’s Segment Anything Model 2.1 (SAM 2.1) for image and video segmentation, which has been used across disciplines like medical imaging and meteorology, and research on enhancing the efficiency of large language models.\u003c/p\u003e\n\n\n\n\u003cp\u003eMeta’s overarching goal is to achieve advanced machine intelligence (AMI), with an emphasis on developing AI systems that are both powerful and accessible. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe FAIR team has been sharing its research for more than a decade, aiming to advance AI in a way that benefits not just the tech community, but society as a whole. Spirit LM is a key component of this effort, supporting open science and reproducibility while pushing the boundaries of what AI can achieve in natural language processing.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-s-next-for-spirit-lm\"\u003eWhat’s next for Spirit LM?\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith the release of Meta Spirit LM, Meta is taking a significant step forward in the integration of speech and text in AI systems. \u003c/p\u003e\n\n\n\n\u003cp\u003eBy offering a more natural and expressive approach to AI-generated speech, and making the model open-source, Meta is enabling the broader research community to explore new possibilities for multimodal AI applications. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhether in ASR, TTS, or beyond, Spirit LM represents a promising advance in the field of machine learning, with the potential to power a new generation of more human-like AI interactions.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-10-19T00:05:21Z",
  "modifiedTime": "2024-10-19T00:05:28Z"
}
