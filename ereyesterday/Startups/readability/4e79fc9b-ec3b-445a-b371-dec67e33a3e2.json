{
  "id": "4e79fc9b-ec3b-445a-b371-dec67e33a3e2",
  "title": "Despite intense AI arms race, we’re in for a multi-modal future",
  "link": "https://venturebeat.com/ai/despite-heated-ai-arms-race-were-in-for-a-multi-modal-future/",
  "description": "There will be no single model that will rule the universe, neither next year nor next decade. Instead, the future of AI will be multi-model.",
  "author": "Zack Kass",
  "published": "Sun, 29 Dec 2024 21:05:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "AI, ML and Deep Learning",
    "category-/News",
    "Conversational AI",
    "Generative AI",
    "large language models",
    "multimodal ai",
    "multimodal large language model",
    "NLP"
  ],
  "byline": "Zack Kass",
  "length": 8800,
  "excerpt": "There will be no single model that will rule the universe, neither next year nor next decade. Instead, the future of AI will be multi-model.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 29, 2024 1:05 PM VentureBeat/Midjourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Every week — sometimes every day—a new state-of-the-art AI model is born to the world. As we move into 2025, the pace at which new models are being released is dizzying, if not exhausting. The curve of the rollercoaster is continuing to grow exponentially, and fatigue and wonder have become constant companions. Each release highlights why this particular model is better than all others, with endless collections of benchmarks and bar charts filling our feeds as we scramble to keep up. The number of large foundation models released each year has been exploding since 2020Charlie Giattino, Edouard Mathieu, Veronika Samborska and Max Roser (2023) – “Artificial Intelligence” Published online at OurWorldinData.org. Eighteen months ago, the vast majority of developers and businesses were using a single AI model. Today, the opposite is true. It is rare to find a business of significant scale that is confining itself to the capabilities of a single model. Companies are wary of vendor lock-in, particularly for a technology which has quickly become a core part of both long-term corporate strategy and short-term bottom-line revenue. It is increasingly risky for teams to put all their bets on a single large language model (LLM). But despite this fragmentation, many model providers still champion the view that AI will be a winner-takes-all market. They claim that the expertise and compute required to train best-in-class models is scarce, defensible and self-reinforcing. From their perspective, the hype bubble for building AI models will eventually collapse, leaving behind a single, giant artificial general intelligence (AGI) model that will be used for anything and everything. To exclusively own such a model would mean to be the most powerful company in the world. The size of this prize has kicked off an arms race for more and more GPUs, with a new zero added to the number of training parameters every few months.  Deep Thought, the monolithic AGI from the Hitchhiker’s Guide to the UniverseBBC, Hitchhiker’s Guide to the Galaxy, television series (1981). Still image retrieved for commentary purposes. We believe this view is mistaken. There will be no single model that will rule the universe, neither next year nor next decade. Instead, the future of AI will be multi-model.  Language models are fuzzy commodities  The Oxford Dictionary of Economics defines a commodity as a “standardized good which is bought and sold at scale and whose units are interchangeable.” Language models are commodities in two important senses:  The models themselves are becoming more interchangeable on a wider set of tasks;  The research expertise required to produce these models is becoming more distributed and accessible, with frontier labs barely outpacing each other and independent researchers in the open-source community nipping at their heels.  Commodities describing commodities (Credit: Not Diamond) But while language models are commoditizing, they are doing so unevenly. There is a large core of capabilities for which any model, from GPT-4 all the way down to Mistral Small, is perfectly suited to handle. At the same time, as we move towards the margins and edge cases, we see greater and greater differentiation, with some model providers explicitly specializing in code generation, reasoning, retrieval-augmented generation (RAG) or math. This leads to endless handwringing, reddit-searching, evaluation and fine-tuning to find the right model for each job.  AI models are commoditizing around core capabilities and specializing at the edges. Credit: Not Diamond And so while language models are commodities, they are more accurately described as fuzzy commodities. For many use cases, AI models will be nearly interchangeable, with metrics like price and latency determining which model to use. But at the edge of capabilities, the opposite will happen: Models will continue to specialize, becoming more and more differentiated. As an example, Deepseek-V2.5 is stronger than GPT-4o on coding in C#, despite being a fraction of the size and 50 times cheaper.  Both of these dynamics — commoditization and specialization — uproot the thesis that a single model will be best-suited to handle every possible use case. Rather, they point towards a progressively fragmented landscape for AI.  Multi-modal orchestration and routing There is an apt analogy for the market dynamics of language models: The human brain. The structure of our brains has remained unchanged for 100,000 years, and brains are far more similar than they are dissimilar. For the vast majority of our time on Earth, most people learned the same things and had similar capabilities.  But then something changed. We developed the ability to communicate in language — first in speech, then in writing. Communication protocols facilitate networks, and as humans began to network with each other, we also began to specialize to greater and greater degrees. We became freed from the burden of needing to be generalists across all domains, to be self-sufficient islands. Paradoxically, the collective riches of specialization have also meant that the average human today is a far stronger generalist than any of our ancestors.  On a sufficiently wide enough input space, the universe always tends towards specialization. This is true all the way from molecular chemistry, to biology, to human society. Given sufficient variety, distributed systems will always be more computationally efficient than monoliths. We believe the same will be true of AI. The more we can leverage the strengths of multiple models instead of relying on just one, the more those models can specialize, expanding the frontier for capabilities.   Multi-model systems can allow for greater specialization, capability and efficiency. Source: Not Diamond An increasingly important pattern for leveraging the strengths of diverse models is routing — dynamically sending queries to the best-suited model, while also leveraging cheaper, faster models when doing so doesn’t degrade quality. Routing allows us to take advantage of all the benefits of specialization — higher accuracy with lower costs and latency — without giving up any of the robustness of generalization. A simple demonstration of the power of routing can be seen in the fact that most of the world’s top models are themselves routers: They are built using Mixture of Expert architectures that route each next-token generation to a few dozen expert sub-models. If it’s true that LLMs are exponentially proliferating fuzzy commodities, then routing must become an essential part of every AI stack.  There is a view that LLMs will plateau as they reach human intelligence — that as we fully saturate capabilities, we will coalesce around a single general model in the same way that we have coalesced around AWS, or the iPhone. Neither of those platforms (or their competitors) have 10X’d their capabilities in the past couple years — so we might as well get comfortable in their ecosystems. We believe, however, that AI will not stop at human-level intelligence; it will carry on far past any limits we might even imagine. As it does so, it will become increasingly fragmented and specialized, just as any other natural system would.  We cannot overstate how much AI model fragmentation is a very good thing. Fragmented markets are efficient markets: They give power to buyers, maximize innovation and minimize costs. And to the extent that we can leverage networks of smaller, more specialized models rather than send everything through the internals of a single giant model, we move towards a much safer, more interpretable and more steerable future for AI.  The greatest inventions have no owners. Ben Franklin’s heirs do not own electricity. Turing’s estate does not own all computers. AI is undoubtedly one of humanity’s greatest inventions; we believe its future will be — and should be — multi-model.  Zack Kass is the former head of go-to-market at OpenAI. Tomás Hernando Kofman is the co-Founder and CEO of Not Diamond.  DataDecisionMakers Welcome to the VentureBeat community! DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation. If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers. You might even consider contributing an article of your own! Read More From DataDecisionMakers",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/nuneybits_Abstract_art_of_robots_in_an_office_optimistic_088da783-ac0e-4ecc-87d8-74540d3af662-transformed.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-29T21:05:00+00:00\" datetime=\"2024-12-29T21:05:00+00:00\"\u003eDecember 29, 2024 1:05 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/nuneybits_Abstract_art_of_robots_in_an_office_optimistic_088da783-ac0e-4ecc-87d8-74540d3af662-transformed.webp?w=750\" alt=\"VentureBeat/Midjourney\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eVentureBeat/Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eEvery week — sometimes every day—a new \u003ca href=\"https://venturebeat.com/ai/how-advanced-foundation-models-will-expand-what-ai-can-do-and-other-predictions-for-2025/\"\u003estate-of-the-art AI model\u003c/a\u003e is born to the world. As we move into 2025, the pace at which new models are being released is dizzying, if not exhausting. The curve of the rollercoaster is continuing to grow exponentially, and fatigue and wonder have become constant companions. Each release highlights why \u003cem\u003ethis\u003c/em\u003e particular model is better than all others, with endless collections of benchmarks and bar charts filling our feeds as we scramble to keep up.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1431\" height=\"954\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image-1.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image-1.png 1431w, https://venturebeat.com/wp-content/uploads/2024/12/image-1.png?resize=300,200 300w, https://venturebeat.com/wp-content/uploads/2024/12/image-1.png?resize=768,512 768w, https://venturebeat.com/wp-content/uploads/2024/12/image-1.png?resize=800,533 800w, https://venturebeat.com/wp-content/uploads/2024/12/image-1.png?resize=400,267 400w, https://venturebeat.com/wp-content/uploads/2024/12/image-1.png?resize=750,500 750w, https://venturebeat.com/wp-content/uploads/2024/12/image-1.png?resize=578,385 578w, https://venturebeat.com/wp-content/uploads/2024/12/image-1.png?resize=930,620 930w\" sizes=\"(max-width: 1431px) 100vw, 1431px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eThe number of large foundation models released each year has been exploding since 2020\u003cbr/\u003eCharlie Giattino, Edouard Mathieu, Veronika Samborska and Max Roser (2023) – “Artificial Intelligence” Published online at OurWorldinData.org. \u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eEighteen months ago, the vast majority of developers and businesses were using a \u003ca href=\"https://venturebeat.com/ai/large-language-overkill-how-slms-can-beat-their-bigger-resource-intensive-cousins/\"\u003esingle AI model\u003c/a\u003e. Today, the opposite is true. It is rare to find a business of significant scale that is confining itself to the capabilities of a single model. Companies are wary of vendor lock-in, particularly for a technology which has quickly become a core part of both long-term corporate strategy and short-term bottom-line revenue. It is increasingly risky for teams to put all their bets on a single large language model (LLM).\u003c/p\u003e\n\n\n\n\u003cp\u003eBut despite this fragmentation, many model providers still champion the view that AI will be a winner-takes-all market. They claim that the expertise and compute required to train best-in-class models is scarce, defensible and self-reinforcing. From their perspective, the hype bubble for \u003ca href=\"https://venturebeat.com/ai/weve-come-a-long-way-from-rpa-how-ai-agents-are-revolutionizing-automation/\"\u003ebuilding AI models\u003c/a\u003e will eventually collapse, leaving behind a single, giant artificial general intelligence (AGI) model that will be used for anything and everything. To exclusively own such a model would mean to be the most powerful company in the world. The size of this prize has kicked off an arms race for more and more GPUs, with a new zero added to the number of training parameters every few months. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1431\" height=\"990\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image-2.jpg?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image-2.jpg 1431w, https://venturebeat.com/wp-content/uploads/2024/12/image-2.jpg?resize=300,208 300w, https://venturebeat.com/wp-content/uploads/2024/12/image-2.jpg?resize=768,531 768w, https://venturebeat.com/wp-content/uploads/2024/12/image-2.jpg?resize=800,553 800w, https://venturebeat.com/wp-content/uploads/2024/12/image-2.jpg?resize=400,277 400w, https://venturebeat.com/wp-content/uploads/2024/12/image-2.jpg?resize=750,519 750w, https://venturebeat.com/wp-content/uploads/2024/12/image-2.jpg?resize=578,400 578w, https://venturebeat.com/wp-content/uploads/2024/12/image-2.jpg?resize=930,643 930w\" sizes=\"(max-width: 1431px) 100vw, 1431px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eDeep Thought, the monolithic AGI from the Hitchhiker’s Guide to the Universe\u003cbr/\u003eBBC, Hitchhiker’s Guide to the Galaxy, television series (1981). Still image retrieved for commentary purposes.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWe believe this view is mistaken. There will be no single model that will rule the universe, neither next year nor next decade. Instead, the future of AI will be multi-model. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-language-models-are-fuzzy-commodities\"\u003eLanguage models are fuzzy commodities \u003c/h2\u003e\n\n\n\n\u003cp\u003eThe \u003cem\u003eOxford Dictionary of Economics\u003c/em\u003e defines a commodity as a “standardized good which is bought and sold at scale and whose units are interchangeable.” Language models are commodities in two important senses: \u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eThe models themselves are becoming more interchangeable on a wider set of tasks; \u003c/li\u003e\n\n\n\n\u003cli\u003eThe research expertise required to produce these models is becoming more distributed and accessible, with frontier labs barely outpacing each other and independent researchers in the open-source community nipping at their heels. \u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1431\" height=\"441\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image-3.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image-3.png 1431w, https://venturebeat.com/wp-content/uploads/2024/12/image-3.png?resize=300,92 300w, https://venturebeat.com/wp-content/uploads/2024/12/image-3.png?resize=768,237 768w, https://venturebeat.com/wp-content/uploads/2024/12/image-3.png?resize=800,247 800w, https://venturebeat.com/wp-content/uploads/2024/12/image-3.png?resize=400,123 400w, https://venturebeat.com/wp-content/uploads/2024/12/image-3.png?resize=750,231 750w, https://venturebeat.com/wp-content/uploads/2024/12/image-3.png?resize=578,178 578w, https://venturebeat.com/wp-content/uploads/2024/12/image-3.png?resize=930,287 930w\" sizes=\"(max-width: 1431px) 100vw, 1431px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eCommodities describing commodities (Credit: Not Diamond)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eBut while language models are commoditizing, they are doing so unevenly. There is a large core of capabilities for which any model, from GPT-4 all the way down to Mistral Small, is perfectly suited to handle. At the same time, as we move towards the margins and edge cases, we see greater and greater differentiation, with some model providers explicitly specializing in code generation, reasoning, retrieval-augmented generation (RAG) or math. This leads to endless handwringing, reddit-searching, evaluation and fine-tuning to find the right model for each job. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1342\" height=\"1008\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image-4.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image-4.png 1342w, https://venturebeat.com/wp-content/uploads/2024/12/image-4.png?resize=300,225 300w, https://venturebeat.com/wp-content/uploads/2024/12/image-4.png?resize=768,577 768w, https://venturebeat.com/wp-content/uploads/2024/12/image-4.png?resize=800,600 800w, https://venturebeat.com/wp-content/uploads/2024/12/image-4.png?resize=400,300 400w, https://venturebeat.com/wp-content/uploads/2024/12/image-4.png?resize=750,563 750w, https://venturebeat.com/wp-content/uploads/2024/12/image-4.png?resize=578,434 578w, https://venturebeat.com/wp-content/uploads/2024/12/image-4.png?resize=930,699 930w\" sizes=\"(max-width: 1342px) 100vw, 1342px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eAI models are commoditizing around core capabilities and specializing at the edges. Credit: Not Diamond\u003cbr/\u003e\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAnd so while language models are commodities, they are more accurately described as \u003cem\u003efuzzy commodities\u003c/em\u003e. For many use cases, AI models will be nearly interchangeable, with metrics like price and latency determining which model to use. But at the edge of capabilities, the opposite will happen: Models will continue to specialize, becoming more and more differentiated. As an example, \u003ca href=\"https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/\"\u003eDeepseek-V2.5\u003c/a\u003e is stronger than GPT-4o on coding in C#, despite being a fraction of the size and 50 times cheaper. \u003c/p\u003e\n\n\n\n\u003cp\u003eBoth of these dynamics — commoditization and specialization — uproot the thesis that a single model will be best-suited to handle every possible use case. Rather, they point towards a progressively fragmented landscape for AI. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-multi-modal-orchestration-and-routing\"\u003eMulti-modal orchestration and routing\u003c/h2\u003e\n\n\n\n\u003cp\u003eThere is an apt analogy for the market dynamics of language models: The human brain. The structure of our brains has remained unchanged for 100,000 years, and brains are far more similar than they are dissimilar. For the vast majority of our time on Earth, most people learned the same things and had similar capabilities. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut then something changed. We developed the ability to communicate in language — first in speech, then in writing. Communication protocols facilitate networks, and as humans began to network with each other, we also began to specialize to greater and greater degrees. We became freed from the burden of needing to be generalists across all domains, to be self-sufficient islands. Paradoxically, the collective riches of specialization have also meant that the average human today is a far stronger generalist than any of our ancestors. \u003c/p\u003e\n\n\n\n\u003cp\u003eOn a sufficiently wide enough input space, the universe always tends towards specialization. This is true all the way from molecular chemistry, to biology, to human society. Given sufficient variety, distributed systems will always be more computationally efficient than monoliths. We believe the same will be true of AI. The more we can leverage the strengths of multiple models instead of relying on just one, the more those models can specialize, expanding the frontier for capabilities. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1278\" height=\"988\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image-5.png?w=776\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image-5.png 1278w, https://venturebeat.com/wp-content/uploads/2024/12/image-5.png?resize=300,232 300w, https://venturebeat.com/wp-content/uploads/2024/12/image-5.png?resize=768,594 768w, https://venturebeat.com/wp-content/uploads/2024/12/image-5.png?resize=776,600 776w, https://venturebeat.com/wp-content/uploads/2024/12/image-5.png?resize=400,309 400w, https://venturebeat.com/wp-content/uploads/2024/12/image-5.png?resize=750,580 750w, https://venturebeat.com/wp-content/uploads/2024/12/image-5.png?resize=578,447 578w, https://venturebeat.com/wp-content/uploads/2024/12/image-5.png?resize=930,719 930w\" sizes=\"(max-width: 1278px) 100vw, 1278px\"/\u003e\u003cfigcaption\u003e\u003cem\u003e Multi-model systems can allow for greater specialization, capability and efficiency. Source: Not Diamond\u003cbr/\u003e\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAn increasingly important pattern for leveraging the strengths of diverse models is routing — dynamically sending queries to the best-suited model, while also leveraging cheaper, faster models when doing so doesn’t degrade quality. Routing allows us to take advantage of all the benefits of specialization — higher accuracy with lower costs and latency — without giving up any of the robustness of generalization. \u003c/p\u003e\n\n\n\n\u003cp\u003eA simple demonstration of the power of routing can be seen in the fact that most of the world’s top models are themselves routers: They are built using \u003ca href=\"https://arxiv.org/pdf/2407.06204\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMixture of Expert\u003c/a\u003e architectures that route each next-token generation to a few dozen expert sub-models. If it’s true that LLMs are exponentially proliferating fuzzy commodities, then routing must become an essential part of every AI stack. \u003c/p\u003e\n\n\n\n\u003cp\u003eThere is a view that LLMs will plateau as they reach human intelligence — that as we fully saturate capabilities, we will coalesce around a single general model in the same way that we have coalesced around AWS, or the iPhone. Neither of those platforms (or their competitors) have 10X’d their capabilities in the past couple years — so we might as well get comfortable in their ecosystems. We believe, however, that AI will not stop at human-level intelligence; it will carry on far past any limits we might even imagine. As it does so, it will become increasingly fragmented and specialized, just as any other natural system would. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe cannot overstate how much AI model fragmentation is a very good thing. Fragmented markets are efficient markets: They give power to buyers, maximize innovation and minimize costs. And to the extent that we can leverage networks of smaller, more specialized models rather than send everything through the internals of a single giant model, we move towards a much safer, more interpretable and more steerable future for AI. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe greatest inventions have no owners. Ben Franklin’s heirs do not own electricity. Turing’s estate does not own all computers. AI is undoubtedly one of humanity’s greatest inventions; we believe its future will be — and should be — multi-model. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eZack Kass is the former head of go-to-market at \u003c/em\u003e\u003ca href=\"https://openai.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eOpenAI\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. \u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eTomás Hernando Kofman is the co-Founder and CEO of \u003c/em\u003e\u003ca href=\"https://notdiamond.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eNot Diamond\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2736392\"\u003e\n\u003cp\u003e\u003cstrong\u003eDataDecisionMakers\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWelcome to the VentureBeat community!\u003c/p\u003e\n\n\n\n\u003cp\u003eDataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might even consider \u003ca rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://venturebeat.com/guest-posts/\"\u003econtributing an article\u003c/a\u003e of your own!\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca rel=\"noreferrer noopener\" href=\"https://venturebeat.com/category/DataDecisionMakers/\" target=\"_blank\"\u003eRead More From DataDecisionMakers\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2024-12-29T21:05:00Z",
  "modifiedTime": "2024-12-28T22:35:37Z"
}
