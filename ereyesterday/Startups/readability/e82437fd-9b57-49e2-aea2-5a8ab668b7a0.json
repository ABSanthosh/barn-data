{
  "id": "e82437fd-9b57-49e2-aea2-5a8ab668b7a0",
  "title": "Learn how GE Healthcare used AWS to build a new AI model that interprets MRIs",
  "link": "https://venturebeat.com/ai/learn-how-ge-healthcare-used-aws-to-build-a-new-ai-model-that-interprets-mris/",
  "description": "Here's how GE Healthcare overcame challenges of 2D and created the industry’s first full-body 3D MRI research foundation model.",
  "author": "Taryn Plumb",
  "published": "Mon, 23 Dec 2024 21:54:53 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI in healthcare",
    "AI in medicine",
    "AI, ML and Deep Learning",
    "category-/Health/Medical Facilities \u0026 Services",
    "GE Healthcare",
    "Generative AI",
    "large language models",
    "MRI",
    "MRI radiation systems"
  ],
  "byline": "Taryn Plumb",
  "length": 9695,
  "excerpt": "Here's how GE Healthcare overcame challenges of 2D and created the industry’s first full-body 3D MRI research foundation model.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 23, 2024 1:54 PM VentureBeat/Ideogram Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More MRI images are understandably complex and data-heavy.  Because of this, developers training large language models (LLMs) for MRI analysis have had to slice captured images into 2D. But this results in just an approximation of the original image, thus limiting the model’s ability to analyze intricate anatomical structures. This creates challenges in complex cases involving brain tumors, skeletal disorders or cardiovascular diseases.  But GE Healthcare appears to have overcome this massive hurdle, introducing the industry’s first full-body 3D MRI research foundation model (FM) at this year’s AWS re:Invent. For the first time, models can use full 3D images of the entire body.  GE Healthcare’s FM was built on AWS from the ground up — there are very few models specifically designed for medical imaging like MRIs — and is based on more than 173,000 images from over 19,000 studies. Developers say they have been able to train the model with five times less compute than previously required.  GE Healthcare has not yet commercialized the foundation model; it is still in an evolutionary research phase. An early evaluator, Mass General Brigham, is set to begin experimenting with it soon.  “Our vision is to put these models into the hands of technical teams working in healthcare systems, giving them powerful tools for developing research and clinical applications faster, and also more cost-effectively,” GE HealthCare chief AI officer Parry Bhatia told VentureBeat.  Enabling real-time analysis of complex 3D MRI data While this is a groundbreaking development, generative AI and LLMs are not new territory for the company. The team has been working with advanced technologies for more than 10 years, Bhatia explained.  One of its flagship products is AIR Recon DL, a deep learning-based reconstruction algorithm that allows radiologists to more quickly achieve crisp images. The algorithm removes noise from raw images and improves signal-to-noise ratio, cutting scan times by up to 50%. Since 2020, 34 million patients have been scanned with AIR Recon DL.  GE Healthcare began working on its MRI FM at the beginning of 2024. Because the model is multimodal, it can support image-to-text searching, link images and words, and segment and classify diseases. The goal is to give healthcare professionals more details in one scan than ever before, said Bhatia, leading to faster, more accurate diagnosis and treatment. “The model has significant potential to enable real-time analysis of 3D MRI data, which can improve medical procedures like biopsies, radiation therapy and robotic surgery,” Dan Sheeran, GM for health care and life sciences at AWS, told VentureBeat.  Already, it has outperformed other publicly-available research models in tasks including classification of prostate cancer and Alzheimer’s disease. It has exhibited accuracy up to 30% in matching MRI scans with text descriptions in image retrieval — which might not sound all that impressive, but it’s a big improvement over the 3% capability exhibited by similar models.  “It has come to a stage where it’s giving some really robust results,” said Bhatia. “The implications are huge.” Doing more with (much less) data The MRI process requires a few different types of datasets to support various techniques that map the human body, Bhatia explained.  What’s known as a T1-weighted imaging technique, for instance, highlights fatty tissue and decreases the signal of water, while T2-weighted imaging enhances water signals. The two methods are complementary and create a full picture of the brain to help clinicians detect abnormalities like tumors, trauma or cancer.  “MRI images come in all different shapes and sizes, similar to how you would have books in different formats and sizes, right?” said Bhatia.  To overcome challenges presented by diverse datasets, developers introduced a “resize and adapt” strategy so that the model could process and react to different variations. Also, data may be missing in some areas — an image may be incomplete, for instance — so they taught the model simply to ignore those instances.  “Instead of getting stuck, we taught the model to skip over the gaps and focus on what was available,” said Bhatia. “Think of this as solving a puzzle with some missing pieces.” The developers also employed semi-supervised student-teacher learning, which is particularly helpful when there is limited data. With this method, two different neural networks are trained on both labeled and unlabeled data, with the teacher creating labels that help the student learn and predict future labels.  “We’re now using a lot of these self-supervised technologies, which don’t require huge amounts of data or labels to train large models,” said Bhatia. “It reduces the dependencies, where you can learn more from these raw images than in the past.” This helps to ensure that the model performs well in hospitals with fewer resources, older machines and different kinds of datasets, Bhatia explained.  He also underscored the importance of the models’ multimodality. “A lot of technology in the past was unimodal,” said Bhatia. “It would look only into the image, into the text. But now they’re becoming multi-modal, they can go from image to text, text to image, so that you can bring in a lot of things that were done with separate models in the past and really unify the workflow.”  He emphasized that researchers only use datasets that they have rights to; GE Healthcare has partners who license de-identified data sets, and they’re careful to adhere to compliance standards and policies. Using AWS SageMaker to tackle computation, data challenges Undoubtedly, there are many challenges when building such sophisticated models — such as limited computational power for 3D images that are gigabytes in size. “It’s a massive 3D volume of data,” said Bhatia. “You need to bring it into the memory of the model, which is a really complex problem.” To help overcome this, GE Healthcare built on Amazon SageMaker, which provides high-speed networking and distributed training capabilities across multiple GPUs, and leveraged Nvidia A100 and tensor core GPUs for large-scale training.  “Because of the size of the data and the size of the models, they cannot send it into a single GPU,” Bhatia explained. SageMaker allowed them to customize and scale operations across multiple GPUs that could interact with one another.  Developers also used Amazon FSx in Amazon S3 object storage, which allowed for faster reading and writing for datasets.  Bhatia pointed out that another challenge is cost optimization; with Amazon’s elastic compute cloud (EC2), developers were able to move unused or infrequently used data to lower-cost storage tiers.  “Leveraging Sagemaker for training these large models — mainly for efficient, distributed training across multiple high-performance GPU clusters — was one of the critical components that really helped us to move faster,” said Bhatia.  He emphasized that all components were built from a data integrity and compliance perspective that took into account HIPAA and other regulatory regulations and frameworks.  Ultimately, “these technologies can really streamline, help us innovate faster, as well as improve overall operational efficiencies by reducing the administrative load, and eventually drive better patient care — because now you’re providing more personalized care.” Serving as a basis for other specialized fine-tuned models While the model for now is specific to the MRI domain, researchers see great opportunities to expand into other areas of medicine.  Sheeran pointed out that, historically, AI in medical imaging has been constrained by the need to develop custom models for specific conditions in specific organs, requiring expert annotation for each image used in training.  But that approach is “inherently limited” due to the different ways diseases manifest across individuals, and introduces generalizability challenges.  “What we truly need is thousands of such models and the ability to rapidly create new ones as we encounter novel information,” he said. High-quality labeled datasets for each model are also essential.  Now with generative AI, instead of training discrete models for each disease/organ combination, developers can pre-train a single foundation model that can serve as a basis for other specialized fine-tuned models downstream.  For instance, GE Healthcare’s model could be expanded into areas such as radiation therapy, where radiologists spend significant time manually marking organs that might be at risk. It could also help reduce scan time during x-rays and other procedures that currently require patients to sit still in a machine for extended periods, said Bhatia.  Sheeran marveled that “we’re not just expanding access to medical imaging data through cloud-based tools; we’re changing how that data can be utilized to drive AI advancements in healthcare.” Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/a-cartoonish-ai-robot-with-a-lab-coat-an_JCnRymIrTqmbCYvf5fs81A_uhWgRUHcQG2GZLi9LlOivQ-transformed.jpeg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-23T21:54:53+00:00\" datetime=\"2024-12-23T21:54:53+00:00\"\u003eDecember 23, 2024 1:54 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"400\" height=\"224\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/a-cartoonish-ai-robot-with-a-lab-coat-an_JCnRymIrTqmbCYvf5fs81A_uhWgRUHcQG2GZLi9LlOivQ-transformed.jpeg?w=400\" alt=\"VentureBeat/Ideogram\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eVentureBeat/Ideogram\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eMRI images are understandably complex and data-heavy. \u003c/p\u003e\n\n\n\n\u003cp\u003eBecause of this, developers \u003ca href=\"https://venturebeat.com/ai/google-ai-agents-multimodal-ai-enterprise-search-will-dominate-in-2025/\"\u003etraining large language models\u003c/a\u003e (LLMs) for MRI analysis have had to slice captured images into 2D. But this results in just an approximation of the original image, thus limiting the model’s ability to analyze intricate anatomical structures. This creates challenges in complex cases involving \u003ca href=\"https://venturebeat.com/ai/exclusive-how-piramidal-is-using-ai-to-decode-the-human-brain/\"\u003ebrain tumors\u003c/a\u003e, skeletal disorders or cardiovascular diseases. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut \u003ca href=\"https://www.gehealthcare.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGE Healthcare\u003c/a\u003e appears to have overcome this massive hurdle, introducing the industry’s first full-body 3D MRI research foundation model (FM) at this year’s \u003ca href=\"https://reinvent.awsevents.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAWS re:Invent\u003c/a\u003e. For the first time, models can use full 3D images of the entire body. \u003c/p\u003e\n\n\n\n\u003cp\u003eGE Healthcare’s FM was built on AWS from the ground up — there are very few models specifically designed for medical imaging like MRIs — and is based on more than 173,000 images from over 19,000 studies. Developers say they have been able to train the model with five times less compute than previously required. \u003c/p\u003e\n\n\n\n\u003cp\u003eGE Healthcare has not yet commercialized the foundation model; it is still in an evolutionary research phase. An early evaluator, \u003ca href=\"https://www.massgeneralbrigham.org/en\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMass General Brigham\u003c/a\u003e, is set to begin experimenting with it soon. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Our vision is to put these models into the hands of technical teams working in healthcare systems, giving them powerful tools for developing research and clinical applications faster, and also more cost-effectively,” GE HealthCare chief AI officer Parry Bhatia told VentureBeat. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-enabling-real-time-analysis-of-complex-3d-mri-data\"\u003eEnabling real-time analysis of complex 3D MRI data\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile this is a groundbreaking development, generative AI and LLMs are not new territory for the company. The team has been working with advanced technologies for more than 10 years, Bhatia explained. \u003c/p\u003e\n\n\n\n\u003cp\u003eOne of its flagship products is \u003ca href=\"https://www.gehealthcare.com/products/magnetic-resonance-imaging/air-recon-dl?\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAIR Recon DL\u003c/a\u003e, a deep learning-based reconstruction algorithm that allows radiologists to more quickly achieve crisp images. The algorithm removes noise from raw images and improves signal-to-noise ratio, cutting scan times by up to 50%. Since 2020, 34 million patients have been scanned with AIR Recon DL. \u003c/p\u003e\n\n\n\n\u003cp\u003eGE Healthcare began working on its MRI FM at the beginning of 2024. Because the model is multimodal, it can support image-to-text searching, link images and words, and segment and classify diseases. The goal is to give \u003ca href=\"https://venturebeat.com/ai/how-kaiser-permanente-is-using-gen-ai-to-paradoxically-make-care-more-human-again/\"\u003ehealthcare professionals\u003c/a\u003e more details in one scan than ever before, said Bhatia, leading to faster, more accurate diagnosis and treatment.\u003c/p\u003e\n\n\n\n\u003cp\u003e“The model has significant potential to enable real-time analysis of 3D MRI data, which can improve medical procedures like biopsies, radiation therapy and robotic surgery,” Dan Sheeran, GM for health care and life sciences at AWS, told VentureBeat. \u003c/p\u003e\n\n\n\n\u003cp\u003eAlready, it has outperformed other publicly-available research models in tasks including classification of prostate cancer and Alzheimer’s disease. It has exhibited accuracy up to 30% in matching MRI scans with text descriptions in image retrieval — which might not sound all that impressive, but it’s a big improvement over the 3% capability exhibited by similar models. \u003c/p\u003e\n\n\n\n\u003cp\u003e“It has come to a stage where it’s giving some really robust results,” said Bhatia. “The implications are huge.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-doing-more-with-much-less-data\"\u003eDoing more with (much less) data\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe \u003ca href=\"https://venturebeat.com/ai/gen-ais-impact-on-healthcare-cutting-edge-applications-and-their-challenges/\"\u003eMRI process\u003c/a\u003e requires a few different types of datasets to support various techniques that map the human body, Bhatia explained. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhat’s known as a T1-weighted imaging technique, for instance, highlights fatty tissue and decreases the signal of water, while T2-weighted imaging enhances water signals. The two methods are complementary and create a full picture of the brain to help clinicians detect abnormalities like tumors, trauma or cancer. \u003c/p\u003e\n\n\n\n\u003cp\u003e“MRI images come in all different shapes and sizes, similar to how you would have books in different formats and sizes, right?” said Bhatia. \u003c/p\u003e\n\n\n\n\u003cp\u003eTo overcome challenges presented by diverse datasets, developers introduced a “resize and adapt” strategy so that the model could process and react to different variations. Also, data may be missing in some areas — an image may be incomplete, for instance — so they taught the model simply to ignore those instances. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Instead of getting stuck, we taught the model to skip over the gaps and focus on what was available,” said Bhatia. “Think of this as solving a puzzle with some missing pieces.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe developers also employed semi-supervised student-teacher learning, which is particularly helpful when there is limited data. With this method, two different neural networks are trained on both labeled and unlabeled data, with the teacher creating labels that help the student learn and predict future labels. \u003c/p\u003e\n\n\n\n\u003cp\u003e“We’re now using a lot of these self-supervised technologies, which don’t require huge amounts of data or labels to train large models,” said Bhatia. “It reduces the dependencies, where you can learn more from these raw images than in the past.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThis helps to ensure that the model performs well in hospitals with fewer resources, older machines and different kinds of datasets, Bhatia explained. \u003c/p\u003e\n\n\n\n\u003cp\u003eHe also underscored the importance of the models’ multimodality. “A lot of technology in the past was unimodal,” said Bhatia. “It would look only into the image, into the text. But now they’re becoming multi-modal, they can go from image to text, text to image, so that you can bring in a lot of things that were done with separate models in the past and really unify the workflow.” \u003c/p\u003e\n\n\n\n\u003cp\u003eHe emphasized that researchers only use datasets that they have rights to; GE Healthcare has partners who license de-identified data sets, and they’re careful to adhere to compliance standards and policies.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-using-aws-sagemaker-to-tackle-computation-data-challenges\"\u003eUsing AWS SageMaker to tackle computation, data challenges\u003c/h2\u003e\n\n\n\n\u003cp\u003eUndoubtedly, there are many challenges when building such sophisticated models — such as limited computational power for 3D images that are gigabytes in size.\u003c/p\u003e\n\n\n\n\u003cp\u003e“It’s a massive 3D volume of data,” said Bhatia. “You need to bring it into the memory of the model, which is a really complex problem.”\u003c/p\u003e\n\n\n\n\u003cp\u003eTo help overcome this, GE Healthcare built on \u003ca href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAmazon SageMaker\u003c/a\u003e, which provides high-speed networking and distributed training capabilities across multiple GPUs, and leveraged Nvidia A100 and tensor core GPUs for large-scale training. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Because of the size of the data and the size of the models, they cannot send it into a single GPU,” Bhatia explained. SageMaker allowed them to customize and scale operations across multiple GPUs that could interact with one another. \u003c/p\u003e\n\n\n\n\u003cp\u003eDevelopers also used \u003ca href=\"https://aws.amazon.com/fsx/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAmazon FSx\u003c/a\u003e in \u003ca href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAmazon S3\u003c/a\u003e object storage, which allowed for faster reading and writing for datasets. \u003c/p\u003e\n\n\n\n\u003cp\u003eBhatia pointed out that another challenge is cost optimization; with Amazon’s elastic compute cloud (EC2), developers were able to move unused or infrequently used data to lower-cost storage tiers. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Leveraging Sagemaker for training these large models — mainly for efficient, distributed training across multiple high-performance GPU clusters — was one of the critical components that really helped us to move faster,” said Bhatia. \u003c/p\u003e\n\n\n\n\u003cp\u003eHe emphasized that all components were built from a data integrity and compliance perspective that took into account HIPAA and other regulatory regulations and frameworks. \u003c/p\u003e\n\n\n\n\u003cp\u003eUltimately, “these technologies can really streamline, help us innovate faster, as well as improve overall operational efficiencies by reducing the administrative load, and eventually drive better patient care — because now you’re providing more personalized care.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-serving-as-a-basis-for-other-specialized-fine-tuned-models\"\u003eServing as a basis for other specialized fine-tuned models\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile the model for now is specific to the MRI domain, researchers see great opportunities to expand into other areas of medicine. \u003c/p\u003e\n\n\n\n\u003cp\u003eSheeran pointed out that, historically, AI in medical imaging has been constrained by the need to develop custom models for specific conditions in specific organs, requiring expert annotation for each image used in training. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut that approach is “inherently limited” due to the different ways diseases manifest across individuals, and introduces generalizability challenges. \u003c/p\u003e\n\n\n\n\u003cp\u003e“What we truly need is thousands of such models and the ability to rapidly create new ones as we encounter novel information,” he said. High-quality labeled datasets for each model are also essential. \u003c/p\u003e\n\n\n\n\u003cp\u003eNow with generative AI, instead of training discrete models for each disease/organ combination, developers can pre-train a single foundation model that can serve as a basis for other specialized fine-tuned models downstream. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor instance, GE Healthcare’s model could be expanded into areas such as radiation therapy, where radiologists spend significant time manually marking organs that might be at risk. It could also help reduce scan time during x-rays and other procedures that currently require patients to sit still in a machine for extended periods, said Bhatia. \u003c/p\u003e\n\n\n\n\u003cp\u003eSheeran marveled that “we’re not just expanding access to medical imaging data through cloud-based tools; we’re changing how that data can be utilized to drive AI advancements in healthcare.”\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2024-12-23T21:54:53Z",
  "modifiedTime": "2024-12-23T21:55:00Z"
}
