{
  "id": "602d52e7-5ff3-4679-8dbf-cb29d39357ef",
  "title": "Alibaba’s new open source model QwQ-32B matches DeepSeek-R1 with way smaller compute requirements",
  "link": "https://venturebeat.com/ai/alibabas-new-open-source-model-qwq-32b-matches-deepseek-r1-with-way-smaller-compute-requirements/",
  "description": "While DeepSeek-R1 operates with 671 billion parameters, QwQ-32B achieves comparable performance with a much smaller footprint.",
  "author": "Carl Franzen",
  "published": "Wed, 05 Mar 2025 23:06:56 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI, ML and Deep Learning",
    "alibaba",
    "category-/Science/Computer Science",
    "China",
    "Conversational AI",
    "Deepseek R1",
    "GPUs",
    "inference",
    "Large Reasoning Models (LRMs)",
    "LLM reasoning",
    "NLP",
    "Nvidia",
    "o1",
    "o1-preview",
    "OpenAI",
    "openai o1",
    "Qwen",
    "Qwen 2.5",
    "qwen team",
    "qwen with questions",
    "qwq",
    "qwq-32b",
    "reasoning AI",
    "reasoning models"
  ],
  "byline": "Carl Franzen",
  "length": 9574,
  "excerpt": "While DeepSeek-R1 operates with 671 billion parameters, QwQ-32B achieves comparable performance with a much smaller footprint.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "March 5, 2025 3:06 PM Credit: VentureBeat made with Midjourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Qwen Team, a division of Chinese e-commerce giant Alibaba developing its growing family of open-source Qwen large language models (LLMs), has introduced QwQ-32B, a new 32-billion-parameter reasoning model designed to improve performance on complex problem-solving tasks through reinforcement learning (RL). The model is available as open-weight on Hugging Face and on ModelScope under an Apache 2.0 license. This means it’s available for commercial and research uses, so enterprises can employ it immediately to power their products and applications (even ones they charge customers to use). It can also be accessed for individual users via Qwen Chat. Quan-with-Questions was Alibaba’s answer to OpenAI’s original reasoning model o1 QwQ, short for Qwen-with-Questions, was first introduced by Alibaba in November 2024 as an open-source reasoning model aimed at competing with OpenAI’s o1-preview. At launch, the model was designed to enhance logical reasoning and planning by reviewing and refining its own responses during inference, a technique that made it particularly effective in math and coding tasks. The initial version of QwQ featured 32 billion parameters and a 32,000-token context length, with Alibaba highlighting its ability to outperform o1-preview in mathematical benchmarks like AIME and MATH, as well as scientific reasoning tasks such as GPQA. Despite its strengths, QwQ’s early iterations struggled with programming benchmarks like LiveCodeBench, where OpenAI’s models maintained an edge. Additionally, as with many emerging reasoning models, QwQ faced challenges such as language mixing and occasional circular reasoning loops. However, Alibaba’s decision to release the model under an Apache 2.0 license ensured that developers and enterprises could freely adapt and commercialize it, distinguishing it from proprietary alternatives like OpenAI’s o1. Since QwQ’s initial release, the AI landscape has evolved rapidly. The limitations of traditional LLMs have become more apparent, with scaling laws yielding diminishing returns in performance improvements. This shift has fueled interest in large reasoning models (LRMs) — a new category of AI systems that use inference-time reasoning and self-reflection to enhance accuracy. These include OpenAI’s o3 series and the massively successful DeepSeek-R1 from rival Chinese lab DeepSeek, an offshoot of Hong Kong quantitative analysis firm High-Flyer Capital Management. A new report from web traffic analytics and research firm SimilarWeb found that since the launch of R1 back in January 2024, DeepSeek has rocketed up the charts to become the most-visited AI model-providing website behind OpenAI. Credit: SimilarWeb, AI Global Global Sector Trends on Generative AI QwQ-32B, Alibaba’s latest iteration, builds on these advancements by integrating RL and structured self-questioning, positioning it as a serious competitor in the growing field of reasoning-focused AI. Scaling up performance with multi-stage reinforcement learning Traditional instruction-tuned models often struggle with difficult reasoning tasks, but the Qwen Team’s research suggests that RL can significantly improve a model’s ability to solve complex problems. QwQ-32B builds on this idea by implementing a multi-stage RL training approach to enhance mathematical reasoning, coding proficiency and general problem-solving. The model has been benchmarked against leading alternatives such as DeepSeek-R1, o1-mini and DeepSeek-R1-Distilled-Qwen-32B, demonstrating competitive results despite having fewer parameters than some of these models. For example, while DeepSeek-R1 operates with 671 billion parameters (with 37 billion activated), QwQ-32B achieves comparable performance with a much smaller footprint — typically requiring 24 GB of vRAM on a GPU (Nvidia’s H100s have 80GB) compared to more than 1500 GB of vRAM for running the full DeepSeek R1 (16 Nvidia A100 GPUs) — highlighting the efficiency of Qwen’s RL approach. QwQ-32B follows a causal language model architecture and includes several optimizations: 64 transformer layers with RoPE, SwiGLU, RMSNorm and Attention QKV bias; Generalized query attention (GQA) with 40 attention heads for queries and 8 for key-value pairs; Extended context length of 131,072 tokens, allowing for better handling of long-sequence inputs; Multi-stage training including pretraining, supervised fine-tuning and RL. The RL process for QwQ-32B was executed in two phases: Math and coding focus: The model was trained using an accuracy verifier for mathematical reasoning and a code execution server for coding tasks. This approach ensured that generated answers were validated for correctness before being reinforced. General capability enhancement: In a second phase, the model received reward-based training using general reward models and rule-based verifiers. This stage improved instruction following, human alignment and agent reasoning without compromising its math and coding capabilities. What it means for enterprise decision-makers For enterprise leaders—including CEOs, CTOs, IT leaders, team managers and AI application developers—QwQ-32B represents a potential shift in how AI can support business decision-making and technical innovation. With its RL-driven reasoning capabilities, the model can provide more accurate, structured and context-aware insights, making it valuable for use cases such as automated data analysis, strategic planning, software development and intelligent automation. Companies looking to deploy AI solutions for complex problem-solving, coding assistance, financial modeling or customer service automation may find QwQ-32B’s efficiency an attractive option. Additionally, its open-weight availability allows organizations to fine-tune and customize the model for domain-specific applications without proprietary restrictions, making it a flexible choice for enterprise AI strategies. The fact that it comes from a Chinese e-commerce giant may raise some security and bias concerns for some non-Chinese users, especially when using the Qwen Chat interface. But as with DeepSeek-R1, the fact that the model is available on Hugging Face for download and offline usage and fine-tuning or retraining suggests that these can be overcome fairly easily. And it is a viable alternative to DeepSeek-R1. Early reactions from AI power users and influencers The release of QwQ-32B has already gained attention from the AI research and development community, with several developers and industry professionals sharing their initial impressions on X (formerly Twitter): Hugging Face’s Vaibhav Srivastav (@reach_vb) highlighted QwQ-32B’s speed in inference thanks to provider Hyperbolic Labs, calling it “blazingly fast” and comparable to top-tier models. He also noted that the model “beats DeepSeek-R1 and OpenAI o1-mini with Apache 2.0 license.” AI news and rumor publisher Chubby (@kimmonismus) was impressed by the model’s performance, emphasizing that QwQ-32B sometimes outperforms DeepSeek-R1, despite being 20 times smaller. “Holy moly! Qwen cooked!” they wrote. Yuchen Jin (@Yuchenj_UW), co-founder and CTO of Hyperbolic Labs, celebrated the release by noting the efficiency gains. “Small models are so powerful! Alibaba Qwen released QwQ-32B, a reasoning model that beats DeepSeek-R1 (671B) and OpenAI o1-mini!” Another Hugging Face team member, Erik Kaunismäki (@ErikKaum) emphasized the ease of deployment, sharing that the model is available for one-click deployment on Hugging Face endpoints, making it accessible to developers without extensive setup. Agentic capabilities QwQ-32B incorporates agentic capabilities, allowing it to dynamically adjust reasoning processes based on environmental feedback. For optimal performance, Qwen Team recommends using the following inference settings: Temperature: 0.6 TopP: 0.95 TopK: Between 20-40 YaRN Scaling: Recommended for handling sequences longer than 32,768 tokens The model supports deployment using vLLM, a high-throughput inference framework. However, current implementations of vLLM only support static YaRN scaling, which maintains a fixed scaling factor regardless of input length. Future developments Qwen’s team sees QwQ-32B as the first step in scaling RL to enhance reasoning capabilities. Looking ahead, the team plans to: Further explore scaling RL to improve model intelligence; Integrate agents with RL for long-horizon reasoning; Continue developing foundation models optimized for RL; Move toward artificial general intelligence (AGI) through more advanced training techniques. With QwQ-32B, Qwen Team is positioning RL as a key driver of the next generation of AI models, demonstrating that scaling can produce highly performant and effective reasoning systems. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/03/cfr0z3n_minimalist_corporate_memphis_flat_illustration_a_robo_0132573d-abf5-4d21-aedd-5cc6b6f17164_3.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-03-05T23:06:56+00:00\" datetime=\"2025-03-05T23:06:56+00:00\"\u003eMarch 5, 2025 3:06 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2025/03/cfr0z3n_minimalist_corporate_memphis_flat_illustration_a_robo_0132573d-abf5-4d21-aedd-5cc6b6f17164_3.png?w=750\" alt=\"An orange and purple humanoid robot types on a desktop computer surrounded by question mark bubbles\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://qwenlm.github.io/about/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eQwen Team\u003c/a\u003e, a division of Chinese e-commerce giant \u003ca href=\"https://www.alibaba.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAlibaba\u003c/a\u003e developing its growing family of open-source Qwen large language models (LLMs), has introduced \u003ca href=\"https://qwenlm.github.io/blog/qwq-32b/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eQwQ-32B\u003c/a\u003e, a new 32-billion-parameter reasoning model designed to improve performance on complex problem-solving tasks through reinforcement learning (RL).\u003c/p\u003e\n\n\n\n\u003cp\u003eThe model is available as open-weight on \u003ca href=\"https://huggingface.co/Qwen/QwQ-32B\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e and on \u003ca href=\"https://modelscope.cn/models/Qwen/QwQ-32B\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eModelScope\u003c/a\u003e under an Apache 2.0 license. This means it’s available for commercial and research uses, so enterprises can employ it immediately to power their products and applications (even ones they charge customers to use).\u003c/p\u003e\n\n\n\n\u003cp\u003eIt can also be accessed for individual users via \u003ca href=\"https://chat.qwen.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eQwen Chat\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-quan-with-questions-was-alibaba-s-answer-to-openai-s-original-reasoning-model-o1\"\u003eQuan-with-Questions was Alibaba’s answer to OpenAI’s original reasoning model o1\u003c/h2\u003e\n\n\n\n\u003cp\u003eQwQ, short for Qwen-with-Questions, was first introduced by \u003ca href=\"https://venturebeat.com/ai/alibaba-releases-qwen-with-questions-an-open-reasoning-model-that-beats-o1-preview/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAlibaba in November 2024\u003c/a\u003e as an open-source reasoning model aimed at competing with \u003ca href=\"https://venturebeat.com/ai/forget-gpt-5-openai-launches-new-ai-model-family-o1-claiming-phd-level-performance/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI’s o1-preview.\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAt launch, the model was designed to enhance logical reasoning and planning by reviewing and refining its own responses during inference, a technique that made it particularly effective in math and coding tasks. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe initial version of QwQ featured 32 billion parameters and a 32,000-token context length, with Alibaba highlighting its ability to outperform o1-preview in mathematical benchmarks like AIME and MATH, as well as scientific reasoning tasks such as GPQA.\u003c/p\u003e\n\n\n\n\u003cp\u003eDespite its strengths, QwQ’s early iterations struggled with programming benchmarks like LiveCodeBench, where OpenAI’s models maintained an edge. Additionally, as with many emerging reasoning models, QwQ faced challenges such as language mixing and occasional circular reasoning loops. \u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, Alibaba’s decision to release the model under an Apache 2.0 license ensured that developers and enterprises could freely adapt and commercialize it, distinguishing it from proprietary alternatives like OpenAI’s o1.\u003c/p\u003e\n\n\n\n\u003cp\u003eSince QwQ’s initial release, the AI landscape has evolved rapidly. The limitations of traditional LLMs have become more apparent, with scaling laws yielding diminishing returns in performance improvements.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis shift has fueled interest in large reasoning models (LRMs) — a new category of AI systems that use inference-time reasoning and self-reflection to enhance accuracy. These include \u003ca href=\"https://venturebeat.com/ai/its-here-openais-o3-mini-advanced-reasoning-model-arrives-to-counter-deepseeks-rise/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI’s o3 series\u003c/a\u003e and the massively successful \u003ca href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/\"\u003eDeepSeek-R1\u003c/a\u003e from rival Chinese lab DeepSeek, an offshoot of Hong Kong quantitative analysis firm High-Flyer Capital Management. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://www.similarweb.com/corp/wp-content/uploads/2025/03/attachment-Global-AI-Tracker-022825.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eA new report\u003c/a\u003e from web traffic analytics and research firm SimilarWeb found that since the launch of R1 back in January 2024, DeepSeek has rocketed up the charts to become the most-visited AI model-providing website behind OpenAI.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"858\" height=\"590\" src=\"https://venturebeat.com/wp-content/uploads/2025/03/visits-to-ai-sites.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/03/visits-to-ai-sites.png 858w, https://venturebeat.com/wp-content/uploads/2025/03/visits-to-ai-sites.png?resize=300,206 300w, https://venturebeat.com/wp-content/uploads/2025/03/visits-to-ai-sites.png?resize=768,528 768w, https://venturebeat.com/wp-content/uploads/2025/03/visits-to-ai-sites.png?resize=800,550 800w, https://venturebeat.com/wp-content/uploads/2025/03/visits-to-ai-sites.png?resize=400,275 400w, https://venturebeat.com/wp-content/uploads/2025/03/visits-to-ai-sites.png?resize=750,516 750w, https://venturebeat.com/wp-content/uploads/2025/03/visits-to-ai-sites.png?resize=578,397 578w\" sizes=\"(max-width: 858px) 100vw, 858px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eCredit\u003c/em\u003e: \u003cem\u003eSimilarWeb, AI Global Global Sector Trends on Generative AI\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eQwQ-32B, Alibaba’s latest iteration, builds on these advancements by integrating RL and structured self-questioning, positioning it as a serious competitor in the growing field of reasoning-focused AI.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-scaling-up-performance-with-multi-stage-reinforcement-learning\"\u003eScaling up performance with multi-stage reinforcement learning\u003c/h2\u003e\n\n\n\n\u003cp\u003eTraditional instruction-tuned models often struggle with difficult reasoning tasks, but the Qwen Team’s research suggests that RL can significantly improve a model’s ability to solve complex problems.\u003c/p\u003e\n\n\n\n\u003cp\u003eQwQ-32B builds on this idea by implementing a multi-stage RL training approach to enhance mathematical reasoning, coding proficiency and general problem-solving.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe model has been benchmarked against leading alternatives such as DeepSeek-R1, o1-mini and DeepSeek-R1-Distilled-Qwen-32B, demonstrating competitive results despite having fewer parameters than some of these models.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1954\" height=\"1036\" src=\"https://venturebeat.com/wp-content/uploads/2025/03/qwq-32B-benchmarks.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/03/qwq-32B-benchmarks.png 1954w, https://venturebeat.com/wp-content/uploads/2025/03/qwq-32B-benchmarks.png?resize=300,159 300w, https://venturebeat.com/wp-content/uploads/2025/03/qwq-32B-benchmarks.png?resize=768,407 768w, https://venturebeat.com/wp-content/uploads/2025/03/qwq-32B-benchmarks.png?resize=800,424 800w, https://venturebeat.com/wp-content/uploads/2025/03/qwq-32B-benchmarks.png?resize=1536,814 1536w, https://venturebeat.com/wp-content/uploads/2025/03/qwq-32B-benchmarks.png?resize=400,212 400w, https://venturebeat.com/wp-content/uploads/2025/03/qwq-32B-benchmarks.png?resize=750,398 750w, https://venturebeat.com/wp-content/uploads/2025/03/qwq-32B-benchmarks.png?resize=578,306 578w, https://venturebeat.com/wp-content/uploads/2025/03/qwq-32B-benchmarks.png?resize=930,493 930w\" sizes=\"(max-width: 1954px) 100vw, 1954px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eFor example, while DeepSeek-R1 operates with 671 billion parameters (with 37 billion activated), QwQ-32B achieves comparable performance with a much smaller footprint — typically requiring \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1iko20f/hardware_requirements_and_advice_for_model_32b/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e24 GB of vRAM on a GPU\u003c/a\u003e (Nvidia’s H100s have 80GB) compared to more than \u003ca href=\"https://apxml.com/posts/gpu-requirements-deepseek-r1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e1500 GB of vRAM\u003c/a\u003e for running the full DeepSeek R1 (16 Nvidia A100 GPUs) — highlighting the efficiency of Qwen’s RL approach.\u003c/p\u003e\n\n\n\n\u003cp\u003eQwQ-32B follows a causal language model architecture and includes several optimizations:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e64 transformer layers with RoPE, SwiGLU, RMSNorm and Attention QKV bias;\u003c/li\u003e\n\n\n\n\u003cli\u003eGeneralized query attention (GQA) with 40 attention heads for queries and 8 for key-value pairs;\u003c/li\u003e\n\n\n\n\u003cli\u003eExtended context length of 131,072 tokens, allowing for better handling of long-sequence inputs;\u003c/li\u003e\n\n\n\n\u003cli\u003eMulti-stage training including pretraining, supervised fine-tuning and RL.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eThe RL process for QwQ-32B was executed in two phases:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eMath and coding focus:\u003c/strong\u003e The model was trained using an accuracy verifier for mathematical reasoning and a code execution server for coding tasks. This approach ensured that generated answers were validated for correctness before being reinforced.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eGeneral capability enhancement: \u003c/strong\u003eIn a second phase, the model received reward-based training using general reward models and rule-based verifiers. This stage improved instruction following, human alignment and agent reasoning without compromising its math and coding capabilities.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003ch2 id=\"h-what-it-means-for-enterprise-decision-makers\"\u003eWhat it means for enterprise decision-makers\u003c/h2\u003e\n\n\n\n\u003cp\u003eFor enterprise leaders—including CEOs, CTOs, IT leaders, team managers and AI application developers—QwQ-32B represents a potential shift in how AI can support business decision-making and technical innovation.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith its RL-driven reasoning capabilities, the model can provide more accurate, structured and context-aware insights, making it valuable for use cases such as automated data analysis, strategic planning, software development and intelligent automation.\u003c/p\u003e\n\n\n\n\u003cp\u003eCompanies looking to deploy AI solutions for complex problem-solving, coding assistance, financial modeling or customer service automation may find QwQ-32B’s efficiency an attractive option. Additionally, its open-weight availability allows organizations to fine-tune and customize the model for domain-specific applications without proprietary restrictions, making it a flexible choice for enterprise AI strategies.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe fact that it comes from a Chinese e-commerce giant may raise some security and bias concerns for some non-Chinese users, especially when using the Qwen Chat interface. But as with DeepSeek-R1, the fact that the model is available on Hugging Face for download and offline usage and fine-tuning or retraining suggests that these can be overcome fairly easily. And it is a viable alternative to DeepSeek-R1.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-early-reactions-from-ai-power-users-and-influencers\"\u003eEarly reactions from AI power users and influencers\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe release of QwQ-32B has already gained attention from the AI research and development community, with several developers and industry professionals sharing their initial impressions on X (formerly Twitter):\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eHugging Face’s \u003ca href=\"https://x.com/reach_vb/status/1897389320946024876\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eVaibhav Srivastav (@reach_vb)\u003c/a\u003e highlighted QwQ-32B’s speed in inference thanks to provider \u003ca href=\"https://x.com/reach_vb/status/1897389320946024876\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHyperbolic Labs\u003c/a\u003e, calling it “blazingly fast” and comparable to top-tier models. He also noted that the model “beats DeepSeek-R1 and OpenAI o1-mini with Apache 2.0 license.”\u003c/li\u003e\n\n\n\n\u003cli\u003eAI news and rumor publisher \u003ca href=\"https://x.com/kimmonismus/status/1897367820276494438\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eChubby (@kimmonismus)\u003c/a\u003e was impressed by the model’s performance, emphasizing that QwQ-32B sometimes outperforms DeepSeek-R1, despite being 20 times smaller. “Holy moly! Qwen cooked!” they \u003ca href=\"https://x.com/kimmonismus/status/1897363452039155876\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ewrote\u003c/a\u003e.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003ca href=\"https://x.com/Yuchenj_UW/status/1897386174605586736\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eYuchen Jin (@Yuchenj_UW),\u003c/a\u003e co-founder and CTO of Hyperbolic Labs\u003cstrong\u003e,\u003c/strong\u003e celebrated the release by noting the efficiency gains. “Small models are so powerful! Alibaba Qwen released QwQ-32B, a reasoning model that beats DeepSeek-R1 (671B) and OpenAI o1-mini!”\u003c/li\u003e\n\n\n\n\u003cli\u003eAnother Hugging Face team member, \u003ca href=\"https://x.com/ErikKaum/status/1897381718539296950\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eErik Kaunismäki (@ErikKaum)\u003c/a\u003e emphasized the ease of deployment, sharing that the model is available for one-click deployment on Hugging Face endpoints, making it accessible to developers without extensive setup.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch2 id=\"h-agentic-capabilities\"\u003eAgentic capabilities\u003c/h2\u003e\n\n\n\n\u003cp\u003eQwQ-32B incorporates agentic capabilities, allowing it to dynamically adjust reasoning processes based on environmental feedback.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor optimal performance, Qwen Team recommends using the following inference settings:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTemperature\u003c/strong\u003e: 0.6\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eTopP\u003c/strong\u003e: 0.95\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eTopK\u003c/strong\u003e: Between 20-40\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eYaRN Scaling\u003c/strong\u003e: Recommended for handling sequences longer than 32,768 tokens\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eThe model supports deployment using vLLM, a high-throughput inference framework. However, current implementations of vLLM only support static YaRN scaling, which maintains a fixed scaling factor regardless of input length.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-future-developments\"\u003eFuture developments\u003c/h2\u003e\n\n\n\n\u003cp\u003eQwen’s team sees QwQ-32B as the first step in scaling RL to enhance reasoning capabilities. Looking ahead, the team plans to:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eFurther explore scaling RL to improve model intelligence;\u003c/li\u003e\n\n\n\n\u003cli\u003eIntegrate agents with RL for long-horizon reasoning;\u003c/li\u003e\n\n\n\n\u003cli\u003eContinue developing foundation models optimized for RL;\u003c/li\u003e\n\n\n\n\u003cli\u003eMove toward artificial general intelligence (AGI) through more advanced training techniques.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eWith QwQ-32B, Qwen Team is positioning RL as a key driver of the next generation of AI models, demonstrating that scaling can produce highly performant and effective reasoning systems.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-03-05T23:06:56Z",
  "modifiedTime": "2025-03-06T00:04:01Z"
}
