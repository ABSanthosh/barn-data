{
  "id": "88019cae-f77c-4b6d-a0ea-fc5e7c5b16c3",
  "title": "Here’s how OpenAI o1 might lose ground to open source models",
  "link": "https://venturebeat.com/ai/heres-how-openai-o1-might-lose-ground-to-open-source-models/",
  "description": "o1 does not reveal its reasoning chain, which makes it difficult to get consistent results and correct the model's responses and logic.",
  "author": "Ben Dickson",
  "published": "Tue, 10 Dec 2024 23:09:17 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI, ML and Deep Learning",
    "alibaba",
    "category-/Science",
    "Deepseek R1",
    "large language models",
    "LLM reasoning",
    "LLMs",
    "o1",
    "open source AI",
    "open source LLMs",
    "OpenAI",
    "openai o1"
  ],
  "byline": "Ben Dickson",
  "length": 4638,
  "excerpt": "o1 does not reveal its reasoning chain, which makes it difficult to get consistent results and correct the model's responses and logic.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 10, 2024 3:09 PM Image Credit: StableDiffusion, via VentureBeat Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More OpenAI has ushered in a new reasoning paradigm in large language models (LLMs) with its o1 model, which recently got a major upgrade. However, while OpenAI has a strong lead in reasoning models, it might lose some ground to open source rivals that are quickly emerging. Models like o1, sometimes referred to as large reasoning models (LRMs), use extra inference-time compute cycles to “think” more, review their responses and correct their answers. This enables them to solve complex reasoning problems that classic LLMs struggle with and makes them especially useful for tasks such as coding, math and data analysis.  However, in recent days, developers have shown mixed reactions to o1, especially after the updated release. Some have posted examples of o1 accomplishing incredible tasks while others have expressed frustration over the model’s confusing responses. Developers have experienced all kinds of problems from making illogical changes to code or ignoring instructions. Secrecy around o1 details Part of the confusion is due to OpenAI’s secrecy and refusal to show the details of how o1 works. The secret sauce behind the success of LRMs is the extra tokens that the model generates as it reaches the final response, referred to as the model’s “thoughts” or “reasoning chain.” For example, if you prompt a classic LLM to generate code for a task, it will immediately generate the code. In contrast, an LRM will generate reasoning tokens that examine the problem, plan the structure of code, and generate multiple solutions before emitting the final answer. o1 hides the thinking process and only shows the final response along with a message that displays how long the model thought and possibly a high overview of the reasoning process. This is partly to avoid cluttering the response and providing a smoother user experience. But more importantly, OpenAI considers the reasoning chain as a trade secret and wants to make it difficult for competitors to replicate o1’s capabilities. The costs of training new models continue to grow and profit margins are not keeping pace, which is pushing some AI labs to become more secretive in order to extend their lead. Even Apollo research, which did the red-teaming of the model, was not given access to its reasoning chain. This lack of transparency has led users to make all kinds of speculations, including accusing OpenAI of degrading the model to cut inference costs. Open-source models fully transparent On the other hand, open source alternatives such as Alibaba’s Qwen with Questions and Marco-o1 show the full reasoning chain of their models. Another alternative is DeepSeek R1, which is not open source but still reveals the reasoning tokens. Seeing the reasoning chain enables developers to troubleshoot their prompts and find ways to improve the model’s responses by adding additional instructions or in-context examples. Visibility into the reasoning process is especially important when you want to integrate the model’s responses into applications and tools that expect consistent results. Moreover, having control over the underlying model is important in enterprise applications. Private models and the scaffolding that supports them, such as the safeguards and filters that test their inputs and outputs, are constantly changing. While this may result in better overall performance, it can break many prompts and applications that were built on top of them. In contrast, open source models give full control of the model to the developer, which can be a more robust option for enterprise applications, where performance on very specific tasks is more important than general skills. QwQ and R1 are still in preview versions and o1 has the lead in terms of accuracy and ease of use. And for many uses, such as making general ad hoc prompts and one-time requests, o1 can still be a better option than the open source alternatives.  But the open-source community is quick to catch up with private models and we can expect more models to emerge in the coming months. They can turn into a suitable alternative where visibility and control are crucial. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/01/The_universe_of_open_source_large_language_models_is_small_in_number-e1706200036796.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-10T23:09:17+00:00\" datetime=\"2024-12-10T23:09:17+00:00\"\u003eDecember 10, 2024 3:09 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"375\" src=\"https://venturebeat.com/wp-content/uploads/2024/01/The_universe_of_open_source_large_language_models_is_small_in_number-e1706200036796.png?w=750\" alt=\"Examples of open source LLMs\"/\u003e\u003c/p\u003e\u003cdiv\u003e\u003cp\u003e\u003cem\u003eImage Credit: StableDiffusion, via VentureBeat\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eOpenAI has ushered in a new reasoning paradigm in large language models (LLMs) with its \u003ca href=\"https://venturebeat.com/ai/openai-launches-full-o1-model-with-34-reduced-error-rate-debuts-chatgpt-pro/\"\u003eo1 model\u003c/a\u003e, which recently got a major upgrade. However, while OpenAI has a strong lead in reasoning models, it might lose some ground to \u003ca href=\"https://venturebeat.com/ai/openai-faces-critical-test-as-chinese-models-close-the-gap-in-ai-leadership/\"\u003eopen source rivals\u003c/a\u003e that are quickly emerging.\u003c/p\u003e\n\n\n\n\u003cp\u003eModels like o1, sometimes referred to as large reasoning models (LRMs), use extra inference-time compute cycles to “think” more, review their responses and correct their answers. This enables them to solve complex reasoning problems that classic LLMs struggle with and makes them especially useful for tasks such as coding, math and data analysis. \u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, in recent days, developers have shown mixed reactions to o1, especially after the updated release. Some have posted examples of o1 accomplishing incredible tasks while others have \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1h7xret/why_we_need_an_open_source_o1/?rdt=63963\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eexpressed frustration\u003c/a\u003e over the model’s confusing responses. Developers have experienced all kinds of problems from making illogical changes to code or ignoring instructions.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-secrecy-around-o1-details\"\u003eSecrecy around o1 details\u003c/h2\u003e\n\n\n\n\u003cp\u003ePart of the confusion is due to OpenAI’s secrecy and refusal to show the details of how o1 works. The secret sauce behind the success of LRMs is the extra tokens that the model generates as it reaches the final response, referred to as the model’s “thoughts” or “reasoning chain.” For example, if you prompt a classic LLM to generate code for a task, it will immediately generate the code. In contrast, an LRM will generate reasoning tokens that examine the problem, plan the structure of code, and generate multiple solutions before emitting the final answer.\u003c/p\u003e\n\n\n\n\u003cp\u003eo1 hides the thinking process and only shows the final response along with a message that displays how long the model thought and possibly a high overview of the reasoning process. This is partly to avoid cluttering the response and providing a smoother user experience. But more importantly, OpenAI considers the reasoning chain as a trade secret and wants to make it difficult for competitors to replicate o1’s capabilities. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe costs of training new models continue to grow and profit margins are not keeping pace, which is pushing some AI labs to become more secretive in order to extend their lead. Even Apollo research, which did the \u003ca href=\"https://www.apolloresearch.ai/research/scheming-reasoning-evaluations\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ered-teaming of the model\u003c/a\u003e, was not given access to its reasoning chain.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis lack of transparency has led users to make all kinds of speculations, including accusing OpenAI of degrading the model to cut inference costs.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-open-source-models-fully-transparent\"\u003eOpen-source models fully transparent\u003c/h2\u003e\n\n\n\n\u003cp\u003eOn the other hand, open source alternatives such as Alibaba’s \u003ca href=\"https://venturebeat.com/ai/alibaba-releases-qwen-with-questions-an-open-reasoning-model-that-beats-o1-preview/\"\u003eQwen with Questions\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/alibaba-researchers-unveil-marco-o1-an-llm-with-advanced-reasoning-capabilities/\"\u003eMarco-o1\u003c/a\u003e show the full reasoning chain of their models. Another alternative is \u003ca href=\"https://venturebeat.com/ai/deepseeks-first-reasoning-model-r1-lite-preview-turns-heads-beating-openai-o1-performance/\"\u003eDeepSeek R1\u003c/a\u003e, which is not open source but still reveals the reasoning tokens. Seeing the reasoning chain enables developers to troubleshoot their prompts and find ways to improve the model’s responses by adding additional instructions or in-context examples.\u003c/p\u003e\n\n\n\n\u003cp\u003eVisibility into the reasoning process is especially important when you want to integrate the model’s responses into applications and tools that expect consistent results. Moreover, having control over the underlying model is important in enterprise applications. Private models and the scaffolding that supports them, such as the safeguards and filters that test their inputs and outputs, are constantly changing. While this may result in better overall performance, it can break many prompts and applications that were built on top of them. In contrast, open source models give full control of the model to the developer, which can be a more robust option for enterprise applications, where performance on very specific tasks is more important than general skills.\u003c/p\u003e\n\n\n\n\u003cp\u003eQwQ and R1 are still in preview versions and o1 has the lead in terms of accuracy and ease of use. And for many uses, such as making general ad hoc prompts and one-time requests, o1 can still be a better option than the open source alternatives. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the open-source community is quick to catch up with private models and we can expect more models to emerge in the coming months. They can turn into a suitable alternative where visibility and control are crucial.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-12-10T23:09:17Z",
  "modifiedTime": "2024-12-11T00:26:20Z"
}
