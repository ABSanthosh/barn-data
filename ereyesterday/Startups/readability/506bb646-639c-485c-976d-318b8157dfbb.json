{
  "id": "506bb646-639c-485c-976d-318b8157dfbb",
  "title": "Faking a JPEG",
  "link": "https://www.ty-penguin.org.uk/~auj/blog/2025/03/25/fake-jpeg/",
  "description": "Article URL: https://www.ty-penguin.org.uk/~auj/blog/2025/03/25/fake-jpeg/ Comments URL: https://news.ycombinator.com/item?id=44537631 Points: 57 # Comments: 9",
  "author": "todsacerdoti",
  "published": "Fri, 11 Jul 2025 22:57:25 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "",
  "length": 6841,
  "excerpt": "Creating something that seems like a JPEG, very quickly",
  "siteName": "",
  "favicon": "",
  "text": "25th March 2025: Faking a JPEG Click to expand I've been wittering on about Spigot for a while. It's small web application which generates a fake hierarchy of web pages, on the fly, using a Markov Chain to make gibberish content for aggressive web crawlers to ingest. Spigot has been sitting there, doing its thing, for a few months now, serving over a million pages per day. I've not really been keeping track of what it's up to, but every now and then I look at its logs to see what crawlers are hitting it. Sadly, two of the hardest-hitting crawlers go to extreme lengths to hide their identity, generating random, and unlikely, browser signatures (e.g. Firefox version 134.0, 64 bit, on Windows 98!) and accessing from random addresses. It seems quite likely that this is being done via a botnet - illegally abusing thousands of people's devices. Sigh. Where I can identify a heavy hitter, I add it to the list on Spigot's front page so I can track crawler behaviour over time. Anyway... a couple of weeks ago, I noticed a new heavy hitter, \"ImageSiftBot\". None of Spigot's output contained images, but ImageSiftBot was busily hitting it with thousands of requests per hour, desperately looking for images to ingest. I felt sorry for its thankless quest and started thinking about how I could please it. My primary aim, for Spigot, is that it should sit there, doing its thing, without eating excessive CPU on my server. Generating images on the fly isn't trivial, in terms of CPU load. If I want to create a bunch of pixels, in a form that a crawler would believe, I pretty much have to supply compressed data. And compressing on the fly is CPU intensive. That's not going to be great for Spigot, and is a complete waste when we're just generating throw-away garbage in any case. I got to thinking: compression tends to increase the entropy of a bit stream. If a file doesn't look to have random content then it's compressible, and an optimally compressed set of data would be more or less indistinguishable from random data. JPEGs are pretty well compressed. So the compressed data in a JPEG will look random, right? If I had a template for a JPEG file, which contained the \"structured\" parts (info on size, colour depth, etc) and tags indicating where highly compressed data goes, I could construct something that looks like a JPEG by just filling out the \"compressed\" areas with random data. That's a very low-CPU operation. The recipient would see something that looks like a JPEG and would treat the random data as something to decompress. I read up a bit on the structure of JPEG files and discovered they can be quite complex. But that doesn't matter much. A JPEG file is made up of chunks. Each chunk has a marker and a length (sometimes implicitly zero, sometimes only determined by reading the chunk's content, looking for the next marker). So, parsing a JPEG is relatively simple. And I've got loads of JPEGs. So: what if I scan a bunch of existing files, discarding the \"comment\" chunks, noting just the length of the \"pixel data\" chunks and keeping the rest? How big would the result be? I've currently got 514 JPEGs on my web site, totalling about 150MBytes of data. If I scan all of them, keeping just the \"structured\" chunks and noting the \"pixel\" chunk lengths, the resulting data set is under 500KBytes - a drop in the ocean. That gives me 514 realistic templates, of various sizes, colour depths, etc. Generating a JPEG would come down to: template = random.choice(template_list) for chunk in template.chunks: if chunk.type is pixel_data: output(random.randbytes(chunk.length)) else: output(chunk.data) And that it! So I knocked together some test code. And discovered it's not quite as simple as that. Real pixel data isn't quite random - it's Huffman coded, and there's a bit of structure there. If I fill out the pixel chunks with purely random data, the decoder notices places where the data is incorrect. I'm sure someone with more brains/time/inclination than me would be able to parse the other chunks in the template to determine exactly what Huffman codes can be inserted into the pixel chunks, still without needing to actually do compression. But that's where I bowed out. Because... every JPEG viewer I've tried accepts my garbage data and displays an image. Even though the decoder notes issues, it still emits pixel data. And that might just be good enough to inconvenience web crawlers. I bet most of them don't care about errors, so long as they don't result in a broken image. Even if they do care about errors, they still have to grab the data and try to decode it before they can tell it's broken. And that's increasing their costs, which is fine by me. The image at the top of this page is an example, generated on the fly by from the code. Your browser will probably display it, despite it being a faulty JPEG. Back to efficiency: How quickly can I generated these garbage images? As I said, I'm using templates based on images from my site. I usually optimise images for the web, resulting in JPEGs having a range of sizes, but mostly around 1280x960 pixels and 200-300KBytes. A quick test shows I can generate around 900 such images per second on my web server using this method (in Python). That's around 190MBytes/second and very substantially faster than my web server's connection to the Internet. Nice! I've wired the above into Spigot and around 60% of Spigot-generated pages will now have a garbage JPEG in them. As with Spigot, I seed the random number generator for an image with a value derived from the URL. So, while the above image was generated on the fly, if you reload it, you'll get the same image. ImageSiftBot is very happy with this and grabbed around 15,000 garbage images today. I expect it'll ramp its rate up over the next few days as it finds more links. Meta's bot, AmazonBot, and GPTBot are also getting excited! I need to tidy up the Python class that does this, but will release it in due course. It's under 100 lines of code (but could do with more comments!). [2025-03-26] Now released on GitHub [2025-03-28] After thinking a lot about Huffman codes, I've added a bit mask against the generated pixel data. \"AND\"-ing every generated byte with 0x6D ensures that no strings of three or more 1's appear in the bit stream. This massively reduces the probability (from \u003e 90% to \u003c 4%) of generating a JPEG that has invalid Huffman codes, without requiring a lot more CPU. The focus is to make generating the garbage as cheap as possible for me and as expensive as possible for the abusive web crawler. After examining how JPEG uses Huffman codes, it wouldn't be excessively difficult to generate perfectly valid Huffman streams. But it would eat a lot more CPU for very little gain.",
  "image": "https://www.ty-penguin.org.uk/~auj/pengfold.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\n\u003ch2\u003e25th March 2025: Faking a JPEG\u003c/h2\u003e\n\n \n\n\n\n        \n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://www.ty-penguin.org.uk/~auj/spigot/pics/2025/03/25/fake-jpeg/banner.jpg\" id=\"gallery-expanded-1\" alt=\"\" onclick=\"gallery_show_popup(this);\"/\u003e\n        \u003cspan id=\"gallery-caption-1\"\u003eClick to expand\u003c/span\u003e\n    \u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eI\u0026#39;ve been \u003ca href=\"https://www.ty-penguin.org.uk/~auj/blog/2024/09/13/spigot/\"\u003ewittering\u003c/a\u003e on \n\u003ca href=\"https://www.ty-penguin.org.uk/~auj/blog/2025/01/02/spigot/\"\u003eabout\u003c/a\u003e Spigot for a while. \nIt\u0026#39;s small \u003ca href=\"https://www.ty-penguin.org.uk/~auj/spigot/\"\u003eweb application\u003c/a\u003e which generates a \nfake hierarchy of web pages, on the fly, using a Markov \nChain to make gibberish content for aggressive web crawlers\nto ingest.\u003c/p\u003e\n\u003cp\u003eSpigot has been sitting there, doing its thing, for a few\nmonths now, serving over a million pages per day. I\u0026#39;ve\nnot really been keeping track of what it\u0026#39;s up to, but every now\nand then I look at its logs to see what crawlers are hitting it.\u003c/p\u003e\n\u003cp\u003eSadly, two of the hardest-hitting crawlers go to extreme lengths \nto hide their identity, generating random, and unlikely, browser \nsignatures (e.g. Firefox version 134.0, 64 bit, on Windows 98!) and\naccessing from random addresses. It seems quite likely that this\nis being done via a botnet - illegally abusing thousands of people\u0026#39;s \ndevices. Sigh.\u003c/p\u003e\n\u003cp\u003eWhere I can identify a heavy hitter, I add it to the list on\nSpigot\u0026#39;s front page so I can track crawler behaviour over time.\u003c/p\u003e\n\u003cp\u003eAnyway... a couple of weeks ago, I noticed a new heavy hitter,\n\u0026#34;ImageSiftBot\u0026#34;. None of Spigot\u0026#39;s output contained images, but \nImageSiftBot was busily hitting it with thousands of requests \nper hour, desperately looking for images to ingest. I felt sorry\nfor its thankless quest and started thinking about how I could \nplease it.\u003c/p\u003e\n\u003cp\u003eMy primary aim, for Spigot, is that it should sit there, doing its\nthing, without eating excessive CPU on my server. Generating images \non the fly isn\u0026#39;t trivial, in terms of CPU load. If I want to create\na bunch of pixels, in a form that a crawler would believe, I pretty\nmuch have to supply compressed data. And compressing on the fly is\nCPU intensive. That\u0026#39;s not going to be great for Spigot, and is a \ncomplete waste when we\u0026#39;re just generating throw-away garbage in any\ncase.\u003c/p\u003e\n\u003cp\u003eI got to thinking: compression tends to increase the entropy of \na bit stream. If a file doesn\u0026#39;t look to have random content then\nit\u0026#39;s compressible, and an optimally compressed set of data would\nbe more or less indistinguishable from random data. JPEGs are\npretty well compressed. So the compressed data in a JPEG will \nlook random, right?\u003c/p\u003e\n\u003cp\u003eIf I had a template for a JPEG file, which contained the \u0026#34;structured\u0026#34;\nparts (info on size, colour depth, etc) and tags indicating where \nhighly compressed data goes, I could construct something that looks\nlike a JPEG by just filling out the \u0026#34;compressed\u0026#34; areas with random \ndata. That\u0026#39;s a very low-CPU operation. The recipient would see \nsomething that looks like a JPEG and would treat the random data \nas something to decompress.\u003c/p\u003e\n\u003cp\u003eI read up a bit on the structure of JPEG files and discovered they\ncan be quite complex. But that doesn\u0026#39;t matter much. A JPEG file is\nmade up of chunks. Each chunk has a marker and a length (sometimes\nimplicitly zero, sometimes only determined by reading the chunk\u0026#39;s\ncontent, looking for the next marker). So, parsing a JPEG is \nrelatively simple. And I\u0026#39;ve got loads of JPEGs. So: what if I\nscan a bunch of existing files, discarding the \u0026#34;comment\u0026#34; chunks,\nnoting just the length of the \u0026#34;pixel data\u0026#34; chunks and keeping the \nrest? How big would the result be?\u003c/p\u003e\n\u003cp\u003eI\u0026#39;ve currently got 514 JPEGs on my web site, totalling about 150MBytes\nof data. If I scan all of them, keeping just the \u0026#34;structured\u0026#34; chunks \nand noting the \u0026#34;pixel\u0026#34; chunk lengths, the resulting data set is under\n500KBytes - a drop in the ocean. That gives me 514 realistic templates, of\nvarious sizes, colour depths, etc.\u003c/p\u003e\n\u003cp\u003eGenerating a JPEG would come down to:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003ccode\u003e\u003cspan\u003etemplate\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003erandom\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003echoice\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003etemplate_list\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003efor\u003c/span\u003e \u003cspan\u003echunk\u003c/span\u003e \u003cspan\u003ein\u003c/span\u003e \u003cspan\u003etemplate\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003echunks\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n   \u003cspan\u003eif\u003c/span\u003e \u003cspan\u003echunk\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003etype\u003c/span\u003e \u003cspan\u003eis\u003c/span\u003e \u003cspan\u003epixel_data\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n       \u003cspan\u003eoutput\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erandom\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003erandbytes\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003echunk\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elength\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e \n   \u003cspan\u003eelse\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n       \u003cspan\u003eoutput\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003echunk\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edata\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eAnd that it!\u003c/p\u003e\n\u003cp\u003eSo I knocked together some test code. And discovered it\u0026#39;s not quite\nas simple as that. Real pixel data isn\u0026#39;t quite random - it\u0026#39;s Huffman\ncoded, and there\u0026#39;s a bit of structure there. If I fill out the pixel\nchunks with purely random data, the decoder notices places where the\ndata is incorrect. I\u0026#39;m sure someone with more brains/time/inclination\nthan me would be able to parse the other chunks in the template to\ndetermine exactly what Huffman codes can be inserted into the pixel\nchunks, still without needing to actually \u003cem\u003edo\u003c/em\u003e compression.\u003c/p\u003e\n\u003cp\u003eBut that\u0026#39;s where I bowed out. Because... every JPEG viewer I\u0026#39;ve tried\naccepts my garbage data and displays an image. Even though the decoder\nnotes issues, it still emits pixel data. And that might just be good\nenough to inconvenience web crawlers. I bet most of them don\u0026#39;t care\nabout errors, so long as they don\u0026#39;t result in a broken image. Even if\nthey \u003cem\u003edo\u003c/em\u003e care about errors, they still have to grab the data and try \nto decode it before they can tell it\u0026#39;s broken. And that\u0026#39;s increasing\ntheir costs, which is fine by me.\u003c/p\u003e\n\u003cp\u003eThe image at the top of this page is an example, generated on the fly by\nfrom the code. Your browser will probably display it, despite it being a\nfaulty JPEG.\u003c/p\u003e\n\u003cp\u003eBack to efficiency: How quickly can I generated these garbage images?  As I\nsaid, I\u0026#39;m using templates based on images from my site. I usually optimise\nimages for the web, resulting in JPEGs having a range of sizes, but mostly\naround 1280x960 pixels and 200-300KBytes. A quick test shows I can generate\naround 900 such images per second on my web server using this method (in\nPython). That\u0026#39;s around 190MBytes/second and very substantially faster than\nmy web server\u0026#39;s connection to the Internet. Nice!\u003c/p\u003e\n\u003cp\u003eI\u0026#39;ve wired the above into Spigot and around 60% of Spigot-generated\npages will now have a garbage JPEG in them. As with Spigot, I seed the\nrandom number generator for an image with a value derived from the URL. \nSo, while the above image was generated on the fly, if you reload it,\nyou\u0026#39;ll get the same image.\u003c/p\u003e\n\u003cp\u003eImageSiftBot is \u003cem\u003every\u003c/em\u003e happy with this and grabbed around 15,000 garbage \nimages today. I expect it\u0026#39;ll ramp its rate up over the next few days as it\nfinds more links. Meta\u0026#39;s bot, AmazonBot, and GPTBot are also getting\nexcited!\u003c/p\u003e\n\u003cp\u003eI need to tidy up the Python class that does this, but will release it \nin due course. It\u0026#39;s under 100 lines of code (but could do with more\ncomments!).\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e[2025-03-26]\u003c/em\u003e Now released on \u003ca href=\"https://github.com/gw1urf/fakejpeg\"\u003eGitHub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e[2025-03-28]\u003c/em\u003e After thinking a lot about Huffman codes, I\u0026#39;ve added a bit\nmask against the generated pixel data. \u0026#34;AND\u0026#34;-ing every generated byte with\n0x6D ensures that no strings of three or more 1\u0026#39;s appear in the bit stream.\nThis massively reduces the probability (from \u0026gt; 90% to \u0026lt; 4%) of generating a\nJPEG that has invalid Huffman codes, without requiring a lot more CPU. \u003c/p\u003e\n\u003cp\u003eThe focus is to make generating the garbage as cheap as possible for me\nand as expensive as possible for the abusive web crawler. After examining\nhow JPEG uses Huffman codes, it \u003cem\u003ewouldn\u0026#39;t\u003c/em\u003e be excessively difficult to \ngenerate perfectly valid Huffman streams. But it would eat a lot more CPU\nfor very little gain.\u003c/p\u003e\n\n\n\n\n\n\n\n\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": null,
  "modifiedTime": null
}
