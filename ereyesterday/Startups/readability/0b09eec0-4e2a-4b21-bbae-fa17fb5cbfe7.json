{
  "id": "0b09eec0-4e2a-4b21-bbae-fa17fb5cbfe7",
  "title": "Sakana AI’s TreeQuest: Deploy multi-model teams that outperform individual LLMs by 30%",
  "link": "https://venturebeat.com/ai/sakana-ais-treequest-deploy-multi-model-teams-that-outperform-individual-llms-by-30/",
  "description": "Sakana AI's new inference-time scaling technique uses Monte-Carlo Tree Search to orchestrate multiple LLMs to collaborate on complex tasks.",
  "author": "Ben Dickson",
  "published": "Thu, 03 Jul 2025 22:00:19 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI research",
    "AI, ML and Deep Learning",
    "inference-time scaling",
    "large language models",
    "large language models (LLMs)",
    "Large Reasoning Models (LRMs)",
    "LLM reasoning",
    "LLMs",
    "research",
    "sakana ai"
  ],
  "byline": "Ben Dickson",
  "length": 8072,
  "excerpt": "Sakana AI's new inference-time scaling technique uses Monte-Carlo Tree Search to orchestrate multiple LLMs to collaborate on complex tasks.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "July 3, 2025 3:00 PM Image credit: VentureBeat with ChatGPT Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now Japanese AI lab Sakana AI has introduced a new technique that allows multiple large language models (LLMs) to cooperate on a single task, effectively creating a “dream team” of AI agents. The method, called Multi-LLM AB-MCTS, enables models to perform trial-and-error and combine their unique strengths to solve problems that are too complex for any individual model. For enterprises, this approach provides a means to develop more robust and capable AI systems. Instead of being locked into a single provider or model, businesses could dynamically leverage the best aspects of different frontier models, assigning the right AI for the right part of a task to achieve superior results. The power of collective intelligence Frontier AI models are evolving rapidly. However, each model has its own distinct strengths and weaknesses derived from its unique training data and architecture. One might excel at coding, while another excels at creative writing. Sakana AI’s researchers argue that these differences are not a bug, but a feature. “We see these biases and varied aptitudes not as limitations, but as precious resources for creating collective intelligence,” the researchers state in their blog post. They believe that just as humanity’s greatest achievements come from diverse teams, AI systems can also achieve more by working together. “By pooling their intelligence, AI systems can solve problems that are insurmountable for any single model.” Thinking longer at inference time Sakana AI’s new algorithm is an “inference-time scaling” technique (also referred to as “test-time scaling”), an area of research that has become very popular in the past year. While most of the focus in AI has been on “training-time scaling” (making models bigger and training them on larger datasets), inference-time scaling improves performance by allocating more computational resources after a model is already trained.  One common approach involves using reinforcement learning to prompt models to generate longer, more detailed chain-of-thought (CoT) sequences, as seen in popular models such as OpenAI o3 and DeepSeek-R1. Another, simpler method is repeated sampling, where the model is given the same prompt multiple times to generate a variety of potential solutions, similar to a brainstorming session. Sakana AI’s work combines and advances these ideas. “Our framework offers a smarter, more strategic version of Best-of-N (aka repeated sampling),” Takuya Akiba, research scientist at Sakana AI and co-author of the paper, told VentureBeat. “It complements reasoning techniques like long CoT through RL. By dynamically selecting the search strategy and the appropriate LLM, this approach maximizes performance within a limited number of LLM calls, delivering better results on complex tasks.” How adaptive branching search works The core of the new method is an algorithm called Adaptive Branching Monte Carlo Tree Search (AB-MCTS). It enables an LLM to effectively perform trial-and-error by intelligently balancing two different search strategies: “searching deeper” and “searching wider.” Searching deeper involves taking a promising answer and repeatedly refining it, while searching wider means generating completely new solutions from scratch. AB-MCTS combines these approaches, allowing the system to improve a good idea but also to pivot and try something new if it hits a dead end or discovers another promising direction. To accomplish this, the system uses Monte Carlo Tree Search (MCTS), a decision-making algorithm famously used by DeepMind’s AlphaGo. At each step, AB-MCTS uses probability models to decide whether it’s more strategic to refine an existing solution or generate a new one. Different test-time scaling strategies Source: Sakana AI The researchers took this a step further with Multi-LLM AB-MCTS, which not only decides “what” to do (refine vs. generate) but also “which” LLM should do it. At the start of a task, the system doesn’t know which model is best suited for the problem. It begins by trying a balanced mix of available LLMs and, as it progresses, learns which models are more effective, allocating more of the workload to them over time. Putting the AI ‘dream team’ to the test The researchers tested their Multi-LLM AB-MCTS system on the ARC-AGI-2 benchmark. ARC (Abstraction and Reasoning Corpus) is designed to test a human-like ability to solve novel visual reasoning problems, making it notoriously difficult for AI.  The team used a combination of frontier models, including o4-mini, Gemini 2.5 Pro, and DeepSeek-R1. The collective of models was able to find correct solutions for over 30% of the 120 test problems, a score that significantly outperformed any of the models working alone. The system demonstrated the ability to dynamically assign the best model for a given problem. On tasks where a clear path to a solution existed, the algorithm quickly identified the most effective LLM and used it more frequently. AB-MCTS vs individual models Source: Sakana AI More impressively, the team observed instances where the models solved problems that were previously impossible for any single one of them. In one case, a solution generated by the o4-mini model was incorrect. However, the system passed this flawed attempt to DeepSeek-R1 and Gemini-2.5 Pro, which were able to analyze the error, correct it, and ultimately produce the right answer.  “This demonstrates that Multi-LLM AB-MCTS can flexibly combine frontier models to solve previously unsolvable problems, pushing the limits of what is achievable by using LLMs as a collective intelligence,” the researchers write. AB-MTCS can select different models at different stages of solving a problem Source: Sakana AI “In addition to the individual pros and cons of each model, the tendency to hallucinate can vary significantly among them,” Akiba said. “By creating an ensemble with a model that is less likely to hallucinate, it could be possible to achieve the best of both worlds: powerful logical capabilities and strong groundedness. Since hallucination is a major issue in a business context, this approach could be valuable for its mitigation.” From research to real-world applications To help developers and businesses apply this technique, Sakana AI has released the underlying algorithm as an open-source framework called TreeQuest, available under an Apache 2.0 license (usable for commercial purposes). TreeQuest provides a flexible API, allowing users to implement Multi-LLM AB-MCTS for their own tasks with custom scoring and logic. “While we are in the early stages of applying AB-MCTS to specific business-oriented problems, our research reveals significant potential in several areas,” Akiba said.  Beyond the ARC-AGI-2 benchmark, the team was able to successfully apply AB-MCTS to tasks like complex algorithmic coding and improving the accuracy of machine learning models.  “AB-MCTS could also be highly effective for problems that require iterative trial-and-error, such as optimizing performance metrics of existing software,” Akiba said. “For example, it could be used to automatically find ways to improve the response latency of a web service.” The release of a practical, open-source tool could pave the way for a new class of more powerful and reliable enterprise AI applications. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/07/ChatGPT-Image-Jul-3-2025-09_58_19-PM.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-07-03T22:00:19+00:00\" datetime=\"2025-07-03T22:00:19+00:00\"\u003eJuly 3, 2025 3:00 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"500\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/ChatGPT-Image-Jul-3-2025-09_58_19-PM.png?w=750\" alt=\"Image credit: VentureBeat with ChatGPT\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with ChatGPT\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\u003c/em\u003e \u003cem\u003e\u003ca href=\"https://venturebeat.com/newsletters/\"\u003eSubscribe Now\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eJapanese AI lab \u003ca href=\"https://sakana.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSakana AI\u003c/a\u003e has introduced a new technique that allows multiple large language models (LLMs) to cooperate on a single task, effectively creating a “dream team” of AI agents. The method, called \u003ca href=\"https://arxiv.org/abs/2503.04412\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMulti-LLM AB-MCTS\u003c/a\u003e, enables models to perform trial-and-error and combine their unique strengths to solve problems that are too complex for any individual model.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor enterprises, this approach provides a means to develop more robust and capable AI systems. Instead of being locked into a single provider or model, businesses could dynamically leverage the best aspects of different frontier models, assigning the right AI for the right part of a task to achieve superior results.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-power-of-collective-intelligence\"\u003eThe power of collective intelligence\u003c/h2\u003e\n\n\n\n\u003cp\u003eFrontier AI models are evolving rapidly. However, each model has its own distinct strengths and weaknesses derived from its unique training data and architecture. One might excel at coding, while another excels at creative writing. Sakana AI’s researchers argue that these differences are not a bug, but a feature.\u003c/p\u003e\n\n\n\n\u003cp\u003e“We see these biases and varied aptitudes not as limitations, but as precious resources for creating collective intelligence,” the researchers state in their \u003ca href=\"https://sakana.ai/ab-mcts/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eblog post\u003c/a\u003e. They believe that just as humanity’s greatest achievements come from diverse teams, AI systems can also achieve more by working together. “By pooling their intelligence, AI systems can solve problems that are insurmountable for any single model.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-thinking-longer-at-inference-time\"\u003eThinking longer at inference time\u003c/h2\u003e\n\n\n\n\u003cp\u003eSakana AI’s new algorithm is an “inference-time scaling” technique (also referred to as “\u003ca href=\"https://venturebeat.com/ai/how-test-time-scaling-unlocks-hidden-reasoning-abilities-in-small-language-models-and-allows-them-to-outperform-llms/\"\u003etest-time scaling\u003c/a\u003e”), an area of research that has become very popular in the past year. While most of the focus in AI has been on “training-time scaling” (making models bigger and training them on larger datasets), inference-time scaling improves performance by allocating more computational resources after a model is already trained. \u003c/p\u003e\n\n\n\n\u003cp\u003eOne common approach involves using reinforcement learning to prompt models to generate longer, more detailed \u003ca href=\"https://venturebeat.com/ai/dont-believe-reasoning-models-chains-of-thought-says-anthropic/\"\u003echain-of-thought\u003c/a\u003e (CoT) sequences, as seen in popular models such as OpenAI o3 and \u003ca href=\"https://venturebeat.com/ai/deepseek-r1-0528-arrives-in-powerful-open-source-challenge-to-openai-o3-and-google-gemini-2-5-pro/\"\u003eDeepSeek-R1\u003c/a\u003e. Another, simpler method is repeated sampling, where the model is given the same prompt multiple times to generate a variety of potential solutions, similar to a brainstorming session. Sakana AI’s work combines and advances these ideas.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Our framework offers a smarter, more strategic version of Best-of-N (aka repeated sampling),” Takuya Akiba, research scientist at Sakana AI and co-author of the paper, told VentureBeat. “It complements reasoning techniques like long CoT through RL. By dynamically selecting the search strategy and the appropriate LLM, this approach maximizes performance within a limited number of LLM calls, delivering better results on complex tasks.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-adaptive-branching-search-works\"\u003eHow adaptive branching search works\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe core of the new method is an algorithm called Adaptive Branching Monte Carlo Tree Search (AB-MCTS). It enables an LLM to effectively perform trial-and-error by intelligently balancing two different search strategies: “searching deeper” and “searching wider.” Searching deeper involves taking a promising answer and repeatedly refining it, while searching wider means generating completely new solutions from scratch. AB-MCTS combines these approaches, allowing the system to improve a good idea but also to pivot and try something new if it hits a dead end or discovers another promising direction.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo accomplish this, the system uses \u003ca href=\"https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMonte Carlo Tree Search\u003c/a\u003e (MCTS), a decision-making algorithm famously used by \u003ca href=\"https://venturebeat.com/ai/google-deepmind-alphazero-chess-shogi-go/\"\u003eDeepMind’s AlphaGo\u003c/a\u003e. At each step, AB-MCTS uses probability models to decide whether it’s more strategic to refine an existing solution or generate a new one.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" height=\"422\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png 3130w, https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png?resize=300,158 300w, https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png?resize=768,406 768w, https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png?resize=800,422 800w, https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png?resize=1536,811 1536w, https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png?resize=2048,1082 2048w, https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png?resize=400,211 400w, https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png?resize=750,396 750w, https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png?resize=578,305 578w, https://venturebeat.com/wp-content/uploads/2025/07/image_ac751e.png?resize=930,491 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eDifferent test-time scaling strategies Source: Sakana AI\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe researchers took this a step further with Multi-LLM AB-MCTS, which not only decides “what” to do (refine vs. generate) but also “which” LLM should do it. At the start of a task, the system doesn’t know which model is best suited for the problem. It begins by trying a balanced mix of available LLMs and, as it progresses, learns which models are more effective, allocating more of the workload to them over time.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-putting-the-ai-dream-team-to-the-test\"\u003ePutting the AI ‘dream team’ to the test\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers tested their Multi-LLM AB-MCTS system on the \u003ca href=\"https://arcprize.org/blog/arc-agi-2-technical-report\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eARC-AGI-2 benchmark\u003c/a\u003e. ARC (Abstraction and Reasoning Corpus) is designed to test a human-like ability to solve novel visual reasoning problems, making it notoriously difficult for AI. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe team used a combination of frontier models, including \u003ca href=\"https://venturebeat.com/ai/openai-launches-o3-and-o4-mini-ai-models-that-think-with-images-and-use-tools-autonomously/\"\u003eo4-mini\u003c/a\u003e, \u003ca href=\"https://venturebeat.com/ai/beyond-benchmarks-gemini-2-5-pro-is-probably-the-best-reasoning-model-yet/\"\u003eGemini 2.5 Pro\u003c/a\u003e, and DeepSeek-R1.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe collective of models was able to find correct solutions for over 30% of the 120 test problems, a score that significantly outperformed any of the models working alone. The system demonstrated the ability to dynamically assign the best model for a given problem. On tasks where a clear path to a solution existed, the algorithm quickly identified the most effective LLM and used it more frequently.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" height=\"414\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image_fae32e.png?w=800\" alt=\"AB-MCTS vs individual models (source: Sakana AI)\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image_fae32e.png 2048w, https://venturebeat.com/wp-content/uploads/2025/07/image_fae32e.png?resize=300,155 300w, https://venturebeat.com/wp-content/uploads/2025/07/image_fae32e.png?resize=768,398 768w, https://venturebeat.com/wp-content/uploads/2025/07/image_fae32e.png?resize=800,414 800w, https://venturebeat.com/wp-content/uploads/2025/07/image_fae32e.png?resize=1536,796 1536w, https://venturebeat.com/wp-content/uploads/2025/07/image_fae32e.png?resize=400,207 400w, https://venturebeat.com/wp-content/uploads/2025/07/image_fae32e.png?resize=750,389 750w, https://venturebeat.com/wp-content/uploads/2025/07/image_fae32e.png?resize=578,299 578w, https://venturebeat.com/wp-content/uploads/2025/07/image_fae32e.png?resize=930,482 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eAB-MCTS vs individual models Source: Sakana AI\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eMore impressively, the team observed instances where the models solved problems that were previously impossible for any single one of them. In one case, a solution generated by the o4-mini model was incorrect. However, the system passed this flawed attempt to DeepSeek-R1 and Gemini-2.5 Pro, which were able to analyze the error, correct it, and ultimately produce the right answer. \u003c/p\u003e\n\n\n\n\u003cp\u003e“This demonstrates that Multi-LLM AB-MCTS can flexibly combine frontier models to solve previously unsolvable problems, pushing the limits of what is achievable by using LLMs as a collective intelligence,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" height=\"170\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image_579cf6.png?w=800\" alt=\"AB-MTCS can select different models at different stages of solving a problem (source: Sakana AI)\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image_579cf6.png 2048w, https://venturebeat.com/wp-content/uploads/2025/07/image_579cf6.png?resize=300,64 300w, https://venturebeat.com/wp-content/uploads/2025/07/image_579cf6.png?resize=768,163 768w, https://venturebeat.com/wp-content/uploads/2025/07/image_579cf6.png?resize=800,170 800w, https://venturebeat.com/wp-content/uploads/2025/07/image_579cf6.png?resize=1536,326 1536w, https://venturebeat.com/wp-content/uploads/2025/07/image_579cf6.png?resize=400,85 400w, https://venturebeat.com/wp-content/uploads/2025/07/image_579cf6.png?resize=750,159 750w, https://venturebeat.com/wp-content/uploads/2025/07/image_579cf6.png?resize=578,122 578w, https://venturebeat.com/wp-content/uploads/2025/07/image_579cf6.png?resize=930,197 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eAB-MTCS can select different models at different stages of solving a problem Source: Sakana AI\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e“In addition to the individual pros and cons of each model, the tendency to hallucinate can vary significantly among them,” Akiba said. “By creating an ensemble with a model that is less likely to hallucinate, it could be possible to achieve the best of both worlds: powerful logical capabilities and strong groundedness. Since hallucination is a major issue in a business context, this approach could be valuable for its mitigation.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-from-research-to-real-world-applications\"\u003eFrom research to real-world applications\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo help developers and businesses apply this technique, Sakana AI has released the underlying algorithm as an open-source framework called \u003ca href=\"https://github.com/SakanaAI/treequest\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eTreeQuest\u003c/a\u003e, available under an Apache 2.0 license (usable for commercial purposes). TreeQuest provides a flexible API, allowing users to implement Multi-LLM AB-MCTS for their own tasks with custom scoring and logic.\u003c/p\u003e\n\n\n\n\u003cp\u003e“While we are in the early stages of applying AB-MCTS to specific business-oriented problems, our research reveals significant potential in several areas,” Akiba said. \u003c/p\u003e\n\n\n\n\u003cp\u003eBeyond the ARC-AGI-2 benchmark, the team was able to successfully apply AB-MCTS to tasks like complex algorithmic coding and improving the accuracy of machine learning models. \u003c/p\u003e\n\n\n\n\u003cp\u003e“AB-MCTS could also be highly effective for problems that require iterative trial-and-error, such as optimizing performance metrics of existing software,” Akiba said. “For example, it could be used to automatically find ways to improve the response latency of a web service.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe release of a practical, open-source tool could pave the way for a new class of more powerful and reliable enterprise AI applications.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2025-07-03T22:00:19Z",
  "modifiedTime": "2025-07-03T22:00:30Z"
}
