{
  "id": "996f58c5-5e1c-4985-9354-13a433172772",
  "title": "Meta makes its MobileLLM open for researchers, posting full weights",
  "link": "https://venturebeat.com/ai/meta-makes-its-mobilellm-open-for-researchers-posting-full-weights/",
  "description": "Developers and researchers interested in testing MobileLLM can now access the models on Hugging Face, fully integrated with the Transformers library.",
  "author": "Carl Franzen",
  "published": "Thu, 31 Oct 2024 19:15:39 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI, ML and Deep Learning",
    "Conversational AI",
    "LLaMA",
    "Meta",
    "Mobile",
    "MobileLLM",
    "NLP",
    "noncommercial",
    "Open source"
  ],
  "byline": "Carl Franzen",
  "length": 4803,
  "excerpt": "Developers and researchers interested in testing MobileLLM can now access the models on Hugging Face, fully integrated with the Transformers library.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "October 31, 2024 12:15 PM Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Meta AI has announced the open-source release of MobileLLM, a set of language models optimized for mobile devices, with model checkpoints and code now accessible on Hugging Face. However, it is presently only available under a Creative Commons 4.0 non-commercial license, meaning enterprises can’t use it on commercial products. Originally described in a research paper published in July 2024 and covered by VentureBeat, MobileLLM is now fully available with open weights, marking a significant milestone for efficient, on-device AI. The release of these open weights makes MobileLLM a more direct, if roundabout, competitor to Apple Intelligence, Apple’s on-device/private cloud hybrid AI solution made up of multiple models, shipping out to users of its iOS 18 operating system in the U.S. and outside the EU this week. However, being restricted to research use and requiring downloading and installation from Hugging Face, it’s likely to remain limited to a computer science and academic audience for now. More efficiency for mobile devices MobileLLM aims to tackle the challenges of deploying AI models on smartphones and other resource-constrained devices. With parameter counts ranging from 125 million to 1 billion, these models are designed to operate within the limited memory and energy capacities typical of mobile hardware. By emphasizing architecture over sheer size, Meta’s research suggests that well-designed compact models can deliver robust AI performance directly on devices. Resolving scaling issues The design philosophy behind MobileLLM deviates from traditional AI scaling laws that emphasize width and large parameter counts. Meta AI’s research instead focuses on deep, thin architectures to maximize performance, improving how abstract concepts are captured by the model. Yann LeCun, Meta’s Chief AI Scientist, highlighted the importance of these depth-focused strategies in enabling advanced AI on everyday hardware. MobileLLM incorporates several innovations aimed at making smaller models more effective: • Depth Over Width: The models employ deep architectures, shown to outperform wider but shallower ones in small-scale scenarios. • Embedding Sharing Techniques: These maximize weight efficiency, crucial for maintaining compact model architecture. • Grouped Query Attention: Inspired by work from Ainslie et al. (2023), this method optimizes attention mechanisms. • Immediate Block-wise Weight Sharing: A novel strategy to reduce latency by minimizing memory movement, helping keep execution efficient on mobile devices. Performance Metrics and Comparisons Despite their compact size, MobileLLM models excel on benchmark tasks. The 125 million and 350 million parameter versions show 2.7% and 4.3% accuracy improvements over previous state-of-the-art (SOTA) models in zero-shot tasks. Remarkably, the 350M version even matches the API calling performance of the much larger Meta Llama-2 7B model. These gains demonstrate that well-architected smaller models can handle complex tasks effectively. Designed for smartphones and the edge MobileLLM’s release aligns with Meta AI’s broader efforts to democratize access to advanced AI technology. With the increasing demand for on-device AI due to cloud costs and privacy concerns, models like MobileLLM are set to play a pivotal role. The models are optimized for devices with memory constraints of 6-12 GB, making them practical for integration into popular smartphones like the iPhone and Google Pixel. Open but non-commercial Meta AI’s decision to open-source MobileLLM reflects the company’s stated commitment to collaboration and transparency. Unfortunately, the licensing terms prohibit commercial usage for now, so only researchers can benefit. By sharing both the model weights and pre-training code, they invite the research community to build on and refine their work. This could accelerate innovation in the field of small language models (SLMs), making high-quality AI accessible without reliance on extensive cloud infrastructure. Developers and researchers interested in testing MobileLLM can now access the models on Hugging Face, fully integrated with the Transformers library. As these compact models evolve, they promise to redefine how advanced AI operates on everyday devices. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/10/smartphone-releasing-magic-emoji.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-10-31T19:15:39+00:00\" datetime=\"2024-10-31T19:15:39+00:00\"\u003eOctober 31, 2024 12:15 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"429\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/smartphone-releasing-magic-emoji.png?w=750\" alt=\"\"/\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eMeta AI has announced the open-source release of \u003ca href=\"https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95\"\u003eMobileLLM\u003c/a\u003e, a set of language models optimized for mobile devices, with model checkpoints and code now accessible on Hugging Face. However, it is presently only \u003ca href=\"https://spdx.org/licenses/CC-BY-NC-4.0\"\u003eavailable under a Creative Commons 4.0 non-commercial license\u003c/a\u003e, meaning enterprises can’t use it on commercial products.\u003c/p\u003e\n\n\n\n\u003cp\u003eOriginally \u003ca href=\"https://arxiv.org/pdf/2402.14905\"\u003edescribed in a research paper published in July 2024\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/meta-ai-develops-compact-language-model-for-mobile-devices/\"\u003ecovered by VentureBeat\u003c/a\u003e, MobileLLM is now fully available with open weights, marking a significant milestone for efficient, on-device AI.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe release of these open weights makes MobileLLM a more direct, if roundabout, competitor to Apple Intelligence, Apple’s on-device/private cloud hybrid AI solution made up of multiple models, shipping out to users of its \u003ca href=\"https://www.theverge.com/2024/10/28/24272995/apple-intelligence-now-available-ios-18-1-mac-ipad\"\u003eiOS 18 operating system in the U.S. and outside the EU\u003c/a\u003e this week. However, being restricted to research use and requiring downloading and installation from Hugging Face, it’s likely to remain limited to a computer science and academic audience for now.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-more-efficiency-for-mobile-devices\"\u003eMore efficiency for mobile devices\u003c/h2\u003e\n\n\n\n\u003cp\u003eMobileLLM aims to tackle the challenges of deploying AI models on smartphones and other resource-constrained devices. \u003c/p\u003e\n\n\n\n\u003cp\u003eWith parameter counts ranging from 125 million to 1 billion, these models are designed to operate within the limited memory and energy capacities typical of mobile hardware. \u003c/p\u003e\n\n\n\n\u003cp\u003eBy emphasizing architecture over sheer size, Meta’s research suggests that well-designed compact models can deliver robust AI performance directly on devices.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-resolving-scaling-issues\"\u003eResolving scaling issues\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe design philosophy behind MobileLLM deviates from traditional AI scaling laws that emphasize width and large parameter counts. \u003c/p\u003e\n\n\n\n\u003cp\u003eMeta AI’s research instead focuses on deep, thin architectures to maximize performance, improving how abstract concepts are captured by the model. \u003c/p\u003e\n\n\n\n\u003cp\u003eYann LeCun, Meta’s Chief AI Scientist, highlighted the importance of these depth-focused strategies in enabling advanced AI on everyday hardware.\u003c/p\u003e\n\n\n\n\u003cp\u003eMobileLLM incorporates several innovations aimed at making smaller models more effective:\u003c/p\u003e\n\n\n\n\u003cp\u003e• \u003cstrong\u003eDepth Over Width:\u003c/strong\u003e The models employ deep architectures, shown to outperform wider but shallower ones in small-scale scenarios.\u003c/p\u003e\n\n\n\n\u003cp\u003e• \u003cstrong\u003eEmbedding Sharing Techniques:\u003c/strong\u003e These maximize weight efficiency, crucial for maintaining compact model architecture.\u003c/p\u003e\n\n\n\n\u003cp\u003e• \u003cstrong\u003eGrouped Query Attention:\u003c/strong\u003e Inspired by work from Ainslie et al. (2023), this method optimizes attention mechanisms.\u003c/p\u003e\n\n\n\n\u003cp\u003e• \u003cstrong\u003eImmediate Block-wise Weight Sharing:\u003c/strong\u003e A novel strategy to reduce latency by minimizing memory movement, helping keep execution efficient on mobile devices.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePerformance Metrics and Comparisons\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eDespite their compact size, MobileLLM models excel on benchmark tasks. The 125 million and 350 million parameter versions show 2.7% and 4.3% accuracy improvements over previous state-of-the-art (SOTA) models in zero-shot tasks. \u003c/p\u003e\n\n\n\n\u003cp\u003eRemarkably, the 350M version even matches the API calling performance of the much larger Meta Llama-2 7B model. \u003c/p\u003e\n\n\n\n\u003cp\u003eThese gains demonstrate that well-architected smaller models can handle complex tasks effectively.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-designed-for-smartphones-and-the-edge\"\u003eDesigned for smartphones and the edge\u003c/h2\u003e\n\n\n\n\u003cp\u003eMobileLLM’s release aligns with Meta AI’s broader efforts to democratize access to advanced AI technology. \u003c/p\u003e\n\n\n\n\u003cp\u003eWith the increasing demand for on-device AI due to cloud costs and privacy concerns, models like MobileLLM are set to play a pivotal role. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe models are optimized for devices with memory constraints of 6-12 GB, making them practical for integration into popular smartphones like the iPhone and Google Pixel.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-open-but-non-commercial\"\u003eOpen but non-commercial\u003c/h2\u003e\n\n\n\n\u003cp\u003eMeta AI’s decision to open-source MobileLLM reflects the company’s stated commitment to collaboration and transparency. Unfortunately, the licensing terms \u003ca href=\"https://spdx.org/licenses/CC-BY-NC-4.0\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eprohibit commercial usage for now\u003c/a\u003e, so only researchers can benefit.\u003c/p\u003e\n\n\n\n\u003cp\u003eBy sharing both the model weights and pre-training code, they invite the research community to build on and refine their work. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis could accelerate innovation in the field of small language models (SLMs), making high-quality AI accessible without reliance on extensive cloud infrastructure.\u003c/p\u003e\n\n\n\n\u003cp\u003eDevelopers and researchers interested in testing MobileLLM can now access the models on Hugging Face, fully integrated with the Transformers library. As these compact models evolve, they promise to redefine how advanced AI operates on everyday devices.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-10-31T19:15:39Z",
  "modifiedTime": "2024-10-31T19:15:47Z"
}
