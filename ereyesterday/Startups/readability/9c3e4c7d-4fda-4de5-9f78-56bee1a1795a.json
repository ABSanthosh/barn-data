{
  "id": "9c3e4c7d-4fda-4de5-9f78-56bee1a1795a",
  "title": "The AI paradox: How tomorrow’s cutting-edge tools can become dangerous cyber threats (and what to do to prepare)",
  "link": "https://venturebeat.com/security/the-ai-paradox-how-tomorrows-cutting-edge-tools-can-become-dangerous-cyber-threats-and-how-to-prepare/",
  "description": "AI agents will bring enterprises to the next level, but the same applies to related vulnerabilities. Here are key tips to follow.",
  "author": "Zac Amos, ReHack",
  "published": "Sun, 02 Feb 2025 23:35:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "Security",
    "AI agents",
    "AI, ML and Deep Learning",
    "category-/Business \u0026 Industrial",
    "category-/Computers \u0026 Electronics",
    "Cloud and Data Storage Security",
    "Conversational AI",
    "Data Security and Privacy",
    "Generative AI",
    "large language models",
    "Network Security and Privacy",
    "NLP"
  ],
  "byline": "Zac Amos, ReHack",
  "length": 6451,
  "excerpt": "AI agents will bring enterprises to the next level, but the same applies to related vulnerabilities. Here are key tips to follow.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More AI is changing the way businesses operate. While much of this shift is positive, it introduces some unique cybersecurity concerns. Next-generation AI applications like agentic AI pose a particularly noteworthy risk to organizations’ security posture. What is agentic AI? Agentic AI refers to AI models that can act autonomously, often automating entire roles with little to no human input. Advanced chatbots are among the most prominent examples, but AI agents can also appear in applications like business intelligence, medical diagnoses and insurance adjustments. In all use cases, this technology combines generative models, natural language processing (NLP) and other machine learning (ML) functions to perform multi-step tasks independently. It’s easy to see the value in such a solution. Understandably, Gartner predicts that one-third of all generative AI interactions will use these agents by 2028. The unique security risks of agentic AI Agentic AI adoption will surge as businesses seek to complete a larger range of tasks without a larger workforce. As promising as that is, though, giving an AI model so much power has serious cybersecurity implications. AI agents typically require access to vast amounts of data. Consequently, they are prime targets for cybercriminals, as attackers could focus efforts on a single application to expose a considerable amount of information. It would have a similar effect to whaling — which led to $12.5 billion in losses in 2021 alone — but may be easier, as AI models could be more susceptible than experienced professionals. Agentic AI’s autonomy is another concern. While all ML algorithms introduce some risks, conventional use cases require human authorization to do anything with their data. Agents, on the other hand, can act without clearance. As a result, any accidental privacy exposures or mistakes like AI hallucinations may slip through without anyone noticing. This lack of supervision makes existing AI threats like data poisoning all the more dangerous. Attackers can corrupt a model by altering just 0.01% of its training dataset, and doing so is possible with minimal investment. That’s damaging in any context, but a poisoned agent’s faulty conclusions would reach much farther than one where humans review outputs first. How to improve AI agent cybersecurity In light of these threats, cybersecurity strategies need to adapt before businesses implement agentic AI applications. Here are four critical steps toward that goal. 1. Maximize visibility The first step is to ensure security and operations teams have full visibility into an AI agent’s workflow. Every task the model completes, each device or app it connects to and all data it can access should be evident. Revealing these factors will make it easier to spot potential vulnerabilities. Automated network mapping tools may be necessary here. Only 23% of IT leaders say they have full visibility into their cloud environments and 61% use multiple detection tools, leading to duplicate records. Admins must address these issues first to gain the necessary insight into what their AI agents can access. Employ the principle of least privilege Once it’s clear what the agent can interact with, businesses must restrict those privileges. The principle of least privilege — which holds that any entity can only see and use what it absolutely needs — is essential. Any database or application an AI agent can interact with is a potential risk. Consequently, organizations can minimize relevant attack surfaces and prevent lateral movement by limiting these permissions as much as possible. Anything that does not directly contribute to an AI’s value-driving purpose should be off-limits. Limit sensitive information Similarly, network admins can prevent privacy breaches by removing sensitive details from the datasets their agentive AI can access. Many AI agents’ work naturally involves private data. More than 50% of generative AI spending will go toward chatbots, which may gather information on customers. However, not all of these details are necessary. While an agent should learn from past customer interactions, it does not need to store names, addresses or payment details. Programming the system to scrub unnecessary personally identifiable information from AI-accessible data will minimize the damage in the event of a breach. Watch for suspicious behavior Businesses need to take care when programming agentive AI, too. Apply it to a single, small use case first and use a diverse team to review the model for signs of bias or hallucinations during training. When it comes time to deploy the agent, roll it out slowly and monitor it for suspicious behavior. Real-time responsiveness is crucial in this monitoring, as agentive AI’s risks mean any breaches could have dramatic consequences. Thankfully, automated detection and response solutions are highly effective, saving an average of $2.22 million in data breach costs. Organizations can slowly expand their AI agents after a successful trial, but they must continue to monitor all applications. As cybersecurity advances, so must cybersecurity strategies AI’s rapid advancement holds significant promise for modern businesses, but its cybersecurity risks are rising just as quickly. Enterprises’ cyber defenses must scale up and advance alongside generative AI use cases. Failure to keep up with these changes could cause damage that outweighs the technology’s benefits. Agentive AI will take ML to new heights, but the same applies to related vulnerabilities. While that does not render this technology too unsafe to invest in, it does warrant extra caution. Businesses must follow these essential security steps as they roll out new AI applications. Zac Amos is features editor at ReHack. DataDecisionMakers Welcome to the VentureBeat community! DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation. If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers. You might even consider contributing an article of your own! Read More From DataDecisionMakers",
  "image": "https://venturebeat.com/wp-content/uploads/2025/02/a-3d-render-of-a-sleep-depiction-of-mode_3x_jieckQbKYhaSXnstcTA_7uPpDWENT_WLGoSxjA0jlQ-transformed.jpeg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eAI is changing the way businesses operate. While much of this shift is positive, it introduces some unique cybersecurity concerns. Next-generation AI applications like \u003ca href=\"https://venturebeat.com/ai/clever-architecture-over-raw-compute-deepseek-shatters-the-bigger-is-better-approach-to-ai-development/\"\u003eagentic AI\u003c/a\u003e pose a particularly noteworthy risk to organizations’ security posture.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-is-agentic-ai\"\u003eWhat is agentic AI?\u003c/h2\u003e\n\n\n\n\u003cp\u003eAgentic AI refers to AI models that can act autonomously, often automating entire roles with little to no human input. Advanced chatbots are among the most prominent examples, but AI agents can also appear in applications like business intelligence, medical diagnoses and insurance adjustments.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn all use cases, this technology combines generative models, natural language processing (NLP) and other machine learning (ML) functions to perform multi-step tasks independently. It’s easy to see the value in such a solution. Understandably, Gartner predicts that \u003ca href=\"https://www.gartner.com/en/newsroom/press-releases/2024-03-11-gartner-predicts-one-third-of-interactions-with-genai-services-will-use-action-models-and-autonomous-agents-for-task-completion-by-2028\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eone-third\u003c/a\u003e of all generative AI interactions will use these agents by 2028.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-unique-security-risks-of-agentic-ai\"\u003eThe unique security risks of agentic AI\u003c/h2\u003e\n\n\n\n\u003cp\u003eAgentic AI adoption will surge as businesses seek to complete a larger range of tasks without a larger workforce. As promising as that is, though, giving an AI model so much power has serious cybersecurity implications.\u003c/p\u003e\n\n\n\n\u003cp\u003eAI agents typically require access to vast amounts of data. Consequently, they are prime targets for cybercriminals, as attackers could focus efforts on a single application to expose a considerable amount of information. It would have a similar effect to whaling — which led to \u003ca href=\"https://digital.va.gov/general/call-me-ishmael-all-about-whaling-scams/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e$12.5 billion in losses\u003c/a\u003e in 2021 alone — but may be easier, as AI models could be more susceptible than experienced professionals.\u003c/p\u003e\n\n\n\n\u003cp\u003eAgentic AI’s autonomy is another concern. While all ML algorithms introduce some risks, conventional use cases require human authorization to do anything with their data. Agents, on the other hand, can act without clearance. As a result, any accidental privacy exposures or \u003ca href=\"https://www.ibm.com/topics/ai-hallucinations\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emistakes like AI hallucinations\u003c/a\u003e may slip through without anyone noticing.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis lack of supervision makes existing AI threats like data poisoning all the more dangerous. Attackers can corrupt a model by altering just \u003ca href=\"https://arxiv.org/html/2302.10149v2\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e0.01% of its training dataset\u003c/a\u003e, and doing so is possible with minimal investment. That’s damaging in any context, but a poisoned agent’s faulty conclusions would reach much farther than one where humans review outputs first.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-to-improve-ai-agent-cybersecurity\"\u003eHow to improve AI agent cybersecurity\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn light of these threats, cybersecurity strategies need to adapt before businesses implement \u003ca href=\"https://venturebeat.com/ai/we-asked-openais-o1-about-the-top-ai-trends-in-2025-heres-a-look-into-our-conversation/\"\u003eagentic AI\u003c/a\u003e applications. Here are four critical steps toward that goal.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-1-maximize-visibility\"\u003e1. Maximize visibility\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe first step is to ensure security and operations teams have full visibility into an AI agent’s workflow. Every task the model completes, each device or app it connects to and all data it can access should be evident. Revealing these factors will make it easier to spot potential vulnerabilities.\u003c/p\u003e\n\n\n\n\u003cp\u003eAutomated network mapping tools may be necessary here. Only \u003ca href=\"https://cloudsecurityalliance.org/artifacts/the-state-of-security-remediation-survey-report\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e23% of IT leaders\u003c/a\u003e say they have full visibility into their cloud environments and 61% use multiple detection tools, leading to duplicate records. Admins must address these issues first to gain the necessary insight into what their AI agents can access.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-employ-the-principle-of-least-privilege\"\u003eEmploy the principle of least privilege\u003c/h3\u003e\n\n\n\n\u003cp\u003eOnce it’s clear what the agent can interact with, businesses must restrict those privileges. The principle of least privilege — which holds that any entity can only see and use what it absolutely needs — is essential.\u003c/p\u003e\n\n\n\n\u003cp\u003eAny database or application an AI agent can interact with is a potential risk. Consequently, organizations can minimize relevant attack surfaces and prevent lateral movement by limiting these permissions as much as possible. Anything that does not directly contribute to an AI’s value-driving purpose should be off-limits.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-limit-sensitive-information\"\u003eLimit sensitive information\u003c/h3\u003e\n\n\n\n\u003cp\u003eSimilarly, network admins can prevent privacy breaches by removing sensitive details from the datasets their agentive AI can access. Many AI agents’ work naturally involves private data. More than \u003ca href=\"https://kopiustech.com/insights-innovation/what-is-generative-ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e50% of generative AI spending\u003c/a\u003e will go toward chatbots, which may gather information on customers. However, not all of these details are necessary.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile an agent should learn from past customer interactions, it does not need to store names, addresses or payment details. Programming the system to scrub unnecessary personally identifiable information from AI-accessible data will minimize the damage in the event of a breach.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-watch-for-suspicious-behavior\"\u003eWatch for suspicious behavior\u003c/h3\u003e\n\n\n\n\u003cp\u003eBusinesses need to take care when programming agentive AI, too. Apply it to a single, small use case first and use a diverse team to review the model for signs of bias or hallucinations during training. When it comes time to deploy the agent, roll it out slowly and monitor it for suspicious behavior.\u003c/p\u003e\n\n\n\n\u003cp\u003eReal-time responsiveness is crucial in this monitoring, as agentive AI’s risks mean any breaches could have dramatic consequences. Thankfully, automated detection and response solutions are highly effective, saving \u003ca href=\"https://www.ibm.com/reports/data-breach\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ean average of $2.22 million\u003c/a\u003e in data breach costs. Organizations can slowly expand their AI agents after a successful trial, but they must continue to monitor all applications.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-as-cybersecurity-advances-so-must-cybersecurity-strategies\"\u003eAs cybersecurity advances, so must cybersecurity strategies\u003c/h2\u003e\n\n\n\n\u003cp\u003eAI’s rapid advancement holds significant promise for modern businesses, but its \u003ca href=\"https://venturebeat.com/security/deepseek-helps-speed-up-threat-detection-while-raising-national-security-concerns/\"\u003ecybersecurity risks\u003c/a\u003e are rising just as quickly. Enterprises’ cyber defenses must scale up and advance alongside generative AI use cases. Failure to keep up with these changes could cause damage that outweighs the technology’s benefits.\u003c/p\u003e\n\n\n\n\u003cp\u003eAgentive AI will take ML to new heights, but the same applies to related vulnerabilities. While that does not render this technology too unsafe to invest in, it does warrant extra caution. Businesses must follow these essential security steps as they roll out new AI applications.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eZac Amos is features editor at \u003ca href=\"https://rehack.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eReHack\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\n\n\n\n\u003cdiv id=\"boilerplate_2736392\"\u003e\n\u003cp\u003e\u003cstrong\u003eDataDecisionMakers\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWelcome to the VentureBeat community!\u003c/p\u003e\n\n\n\n\u003cp\u003eDataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might even consider \u003ca rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://venturebeat.com/guest-posts/\"\u003econtributing an article\u003c/a\u003e of your own!\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca rel=\"noreferrer noopener\" href=\"https://venturebeat.com/category/DataDecisionMakers/\" target=\"_blank\"\u003eRead More From DataDecisionMakers\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-02-02T23:35:00Z",
  "modifiedTime": "2025-02-02T23:44:32Z"
}
