{
  "id": "a02770ba-d2b5-4f35-a918-3348eb1a3ed0",
  "title": "Show HN: Wall-mounted diffusion mirror that turns reflections into paintings",
  "link": "https://www.matthieulc.com/posts/pablonet/",
  "description": "Article URL: https://www.matthieulc.com/posts/pablonet/ Comments URL: https://news.ycombinator.com/item?id=41929804 Points: 57 # Comments: 19",
  "author": "cataPhil",
  "published": "Wed, 23 Oct 2024 22:24:21 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "",
  "length": 2208,
  "excerpt": "Matthieu Le Cauchois's website",
  "siteName": "",
  "favicon": "",
  "text": "The debate about whether internet-fitted AIs can be creative always seemed besides the point to me. Making art is hard. But art is mostly about surfacing the inner world, and only in part about skill. It’s unfortunate that art selects so strongly for skill. Can we decorrelate the two? It seems so. Cheap interpolative* creativity used by 8 billion non-artists surely surfaces new views of the world.For these reasons and since I suck at art I’ve been very excited about the various AI-driven art forms popping up. A couple of months ago I started playing with real-time diffusion of my webcam feed using StreamDiffusion. Specifically, with the intent of generating pretty visuals and hoping to elicit new/interesting feelings. Although it’s very fun, the laptop form-factor breaks the illusion. It feels all temporary and geeky. So, I recently built an LCD frame that can be hanged to a wall with minimal illusion breakers. What I really like about this setup is that making it a proper object opens up new channels of interaction. It’s no longer just a screen, it has permanence. You can leave it there, come back to it at a different time, in a different mood, with different lighting, objects, friends, etc. Infrared light in the darkPlus, it looks pretty:The main issue with the current setup is the low frame rate. I managed to increase it using TensorRT and compressing the images in and out, but there’s still a lot of room for improvement.For those of you that are interested in creating your own or contributing, here are the details:Code for the client and server here.RunPod for hosting the server.Client runs on a Raspberry Pi 5.This 10.1\" Pi screen.This infrared light.This generic frame.This infrared Pi camera.I used a puncher to cut a hole in the frame’s cardboard for the camera (drills didn’t work).I spent hours playing with different preprocessing filters, not just prompting. In general I found the two to be equally important. Without preprocessing, img2img often looked too realistic. To get the blue Picasso style seen in this post, I ended up using a mix of canny edge detection, blue coloration and blurring.*Technically, learning in high dimension always amounts to extrapolation.",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"content\"\u003e\u003carticle\u003e\u003cp\u003e\u003cimg src=\"https://www.matthieulc.com/posts/pablonet/main.jpeg\"/\u003e\u003c/p\u003e\u003cp\u003eThe debate about whether internet-fitted AIs can be creative always seemed besides the point to me. Making art is hard. But art is mostly about surfacing the inner world, and only in part about skill. It’s unfortunate that art selects so strongly for skill. Can we decorrelate the two? It seems so. Cheap interpolative* creativity used by 8 billion non-artists surely surfaces new views of the world.\u003c/p\u003e\u003cp\u003eFor these reasons and since I suck at art I’ve been very excited about the various AI-driven art forms popping up. A couple of months ago I started playing with real-time diffusion of my webcam feed using \u003ca href=\"https://github.com/cumulo-autumn/StreamDiffusion\"\u003eStreamDiffusion\u003c/a\u003e. Specifically, with the intent of generating pretty visuals and hoping to elicit new/interesting feelings. Although it’s very fun, the laptop form-factor breaks the illusion. It feels all temporary and geeky. So, I recently built an LCD frame that can be hanged to a wall with minimal illusion breakers. What I really like about this setup is that making it a proper object opens up new channels of interaction. It’s no longer just a screen, it has permanence. You can leave it there, come back to it at a different time, in a different mood, with different lighting, objects, friends, etc.\u003c/p\u003e\u003cdiv\u003e\u003cvideo width=\"300\" height=\"550\" object-fit=\"fill\" controls=\"\"\u003e\n\u003csource src=\"https://www.matthieulc.com/posts/pablonet/trimmed.mp4\" type=\"video/mp4\"/\u003e\u003c/video\u003e\u003cp\u003eInfrared light in the dark\u003c/p\u003e\u003c/div\u003e\u003cp\u003ePlus, it looks pretty:\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://www.matthieulc.com/posts/pablonet/stairs.jpeg\"/\u003e\u003c/p\u003e\u003cp\u003eThe main issue with the current setup is the low frame rate. I managed to increase it using TensorRT and compressing the images in and out, but there’s still a lot of room for improvement.\u003c/p\u003e\u003cp\u003eFor those of you that are interested in creating your own or contributing, here are the details:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCode for the client and server \u003ca href=\"https://github.com/mlecauchois/pablonet\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://www.runpod.io/\"\u003eRunPod\u003c/a\u003e for hosting the server.\u003c/li\u003e\u003cli\u003eClient runs on a \u003ca href=\"https://www.raspberrypi.com/products/raspberry-pi-5/\"\u003eRaspberry Pi 5\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eThis \u003ca href=\"https://www.amazon.fr/HMTECH-Raspberry-Moniteur-portable-Raspbian/dp/B098762GVK\"\u003e10.1\u0026#34; Pi screen\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eThis \u003ca href=\"https://www.amazon.fr/dp/B0BG5HM2Q8?ref=ppx_yo2ov_dt_b_fed_asin_title\"\u003einfrared light\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eThis generic \u003ca href=\"https://www.leroymerlin.fr/produits/decoration-eclairage/decoration-murale/cadre-photo/cadre-noir/cadre-milo-21-x-29-7-cm-noir-inspire-71670942.html\"\u003eframe\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eThis infrared \u003ca href=\"https://www.raspberrypi.com/products/pi-noir-camera-v2/\"\u003ePi camera\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eI used a puncher to cut a hole in the frame’s cardboard for the camera (drills didn’t work).\u003c/li\u003e\u003cli\u003eI spent hours playing with different preprocessing filters, not just prompting. In general I found the two to be equally important. Without preprocessing, img2img often looked too realistic. To get the blue Picasso style seen in this post, I ended up using a mix of canny edge detection, blue coloration and blurring.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cimg src=\"https://www.matthieulc.com/posts/pablonet/behind.png\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://www.matthieulc.com/posts/pablonet/closeup.png\"/\u003e\u003c/p\u003e\u003chr/\u003e\u003cp\u003e*Technically, \u003ca href=\"https://arxiv.org/abs/2110.09485\"\u003elearning in high dimension always amounts to extrapolation\u003c/a\u003e.\u003c/p\u003e\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": null,
  "modifiedTime": null
}
