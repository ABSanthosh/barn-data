{
  "id": "5b6822b1-025a-4af5-839c-a427a0c82132",
  "title": "Hidden costs in AI deployment: Why Claude models may be 20-30% more expensive than GPT in enterprise settings",
  "link": "https://venturebeat.com/ai/hidden-costs-in-ai-deployment-why-claude-models-may-be-20-30-more-expensive-than-gpt-in-enterprise-settings/",
  "description": "It is a well-known fact that different model families can use different tokenizers. However, there has been limited analysis on how the process of “tokenization” itself varies across these tokenizers. Do all tokenizers result in the same number of tokens for a given input text? If not, how different are the generated tokens? How significant […]",
  "author": "Lavanya Gupta",
  "published": "Thu, 01 May 2025 20:14:04 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI deployment",
    "AI, ML and Deep Learning",
    "Anthropic",
    "API",
    "Byte Pair Encoding (BPE)",
    "ChatGPT",
    "Claude",
    "claude 3.5 sonnet",
    "context window utilization",
    "context windows",
    "Enterprise",
    "enterprise ai",
    "gpt-4o",
    "model families",
    "natural language",
    "OpenAI",
    "Python",
    "tokenization inefficiency"
  ],
  "byline": "Lavanya Gupta",
  "length": 6881,
  "excerpt": "It is a well-known fact that different model families can use different tokenizers. However, there has been limited analysis on how the process of “tokenization” itself varies across these tokenizers. Do all tokenizers result in the same number of tokens for a given input text? If not, how different are the generated tokens? How significant […]",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "May 1, 2025 1:14 PM Credit: VentureBeat using DALL-E 3 It is a well-known fact that different model families can use different tokenizers. However, there has been limited analysis on how the process of “tokenization” itself varies across these tokenizers. Do all tokenizers result in the same number of tokens for a given input text? If not, how different are the generated tokens? How significant are the differences? In this article, we explore these questions and examine the practical implications of tokenization variability. We present a comparative story of two frontier model families: OpenAI’s ChatGPT vs Anthropic’s Claude. Although their advertised “cost-per-token” figures are highly competitive, experiments reveal that Anthropic models can be 20–30% more expensive than GPT models. API Pricing — Claude 3.5 Sonnet vs GPT-4o As of June 2024, the pricing structure for these two advanced frontier models is highly competitive. Both Anthropic’s Claude 3.5 Sonnet and OpenAI’s GPT-4o have identical costs for output tokens, while Claude 3.5 Sonnet offers a 40% lower cost for input tokens. Source: Vantage Despite lower input token rates of the Anthropic model, we observed that the total costs of running experiments (on a given set of fixed prompts) with GPT-4o is much cheaper when compared to Claude Sonnet-3.5.Why? The Anthropic tokenizer tends to break down the same input into more tokens compared to OpenAI’s tokenizer. This means that, for identical prompts, Anthropic models produce considerably more tokens than their OpenAI counterparts. As a result, while the per-token cost for Claude 3.5 Sonnet’s input may be lower, the increased tokenization can offset these savings, leading to higher overall costs in practical use cases.  This hidden cost stems from the way Anthropic’s tokenizer encodes information, often using more tokens to represent the same content. The token count inflation has a significant impact on costs and context window utilization. Domain-dependent tokenization inefficiency Different types of domain content are tokenized differently by Anthropic’s tokenizer, leading to varying levels of increased token counts compared to OpenAI’s models. The AI research community has noted similar tokenization differences here. We tested our findings on three popular domains, namely: English articles, code (Python) and math. DomainModel InputGPT TokensClaude Tokens% Token OverheadEnglish articles7789~16%Code (Python)6078~30%Math114138~21% % Token Overhead of Claude 3.5 Sonnet Tokenizer (relative to GPT-4o) Source: Lavanya Gupta When comparing Claude 3.5 Sonnet to GPT-4o, the degree of tokenizer inefficiency varies significantly across content domains. For English articles, Claude’s tokenizer produces approximately 16% more tokens than GPT-4o for the same input text. This overhead increases sharply with more structured or technical content: for mathematical equations, the overhead stands at 21%, and for Python code, Claude generates 30% more tokens. This variation arises because some content types, such as technical documents and code, often contain patterns and symbols that Anthropic’s tokenizer fragments into smaller pieces, leading to a higher token count. In contrast, more natural language content tends to exhibit a lower token overhead. Other practical implications of tokenizer inefficiency Beyond the direct implication on costs, there is also an indirect impact on the context window utilization.  While Anthropic models claim a larger context window of 200K tokens, as opposed to OpenAI’s 128K tokens, due to verbosity, the effective usable token space may be smaller for Anthropic models. Hence, there could potentially be a small or large difference in the “advertised” context window sizes vs the “effective” context window sizes. Implementation of tokenizers GPT models use Byte Pair Encoding (BPE), which merges frequently co-occurring character pairs to form tokens. Specifically, the latest GPT models use the open-source o200k_base tokenizer. The actual tokens used by GPT-4o (in the tiktoken tokenizer) can be viewed here. JSON { #reasoning \"o1-xxx\": \"o200k_base\", \"o3-xxx\": \"o200k_base\", # chat \"chatgpt-4o-\": \"o200k_base\", \"gpt-4o-xxx\": \"o200k_base\", # e.g., gpt-4o-2024-05-13 \"gpt-4-xxx\": \"cl100k_base\", # e.g., gpt-4-0314, etc., plus gpt-4-32k \"gpt-3.5-turbo-xxx\": \"cl100k_base\", # e.g, gpt-3.5-turbo-0301, -0401, etc. } Unfortunately, not much can be said about Anthropic tokenizers as their tokenizer is not as directly and easily available as GPT. Anthropic released their Token Counting API in Dec 2024. However, it was soon demised in later 2025 versions. Latenode reports that “Anthropic uses a unique tokenizer with only 65,000 token variations, compared to OpenAI’s 100,261 token variations for GPT-4.” This Colab notebook contains Python code to analyze the tokenization differences between GPT and Claude models. Another tool that enables interfacing with some common, publicly available tokenizers validates our findings.The ability to proactively estimate token counts (without invoking the actual model API) and budget costs is crucial for AI enterprises.  Key Takeaways Anthropic’s competitive pricing comes with hidden costs:While Anthropic’s Claude 3.5 Sonnet offers 40% lower input token costs compared to OpenAI’s GPT-4o, this apparent cost advantage can be misleading due to differences in how input text is tokenized. Hidden “tokenizer inefficiency”:Anthropic models are inherently more verbose. For businesses that process large volumes of text, understanding this discrepancy is crucial when evaluating the true cost of deploying models. Domain-dependent tokenizer inefficiency:When choosing between OpenAI and Anthropic models, evaluate the nature of your input text. For natural language tasks, the cost difference may be minimal, but technical or structured domains may lead to significantly higher costs with Anthropic models. Effective context window:Due to the verbosity of Anthropic’s tokenizer, its larger advertised 200K context window may offer less effective usable space than OpenAI’s 128K, leading to a potential gap between advertised and actual context window. Anthropic did not respond to VentureBeat’s requests for comment by press time. We’ll update the story if they respond. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/05/ChatGPT-Image-May-1-2025-11_45_21-AM.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-05-01T20:14:04+00:00\" datetime=\"2025-05-01T20:14:04+00:00\"\u003eMay 1, 2025 1:14 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"500\" src=\"https://venturebeat.com/wp-content/uploads/2025/05/ChatGPT-Image-May-1-2025-11_45_21-AM.png?w=750\" alt=\"Credit: VentureBeat using DALL-E 3\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat using DALL-E 3\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003eIt is a well-known fact that different model families can use different tokenizers. However, there has been limited analysis on how the process of \u003cstrong\u003e“\u003c/strong\u003etokenization\u003cstrong\u003e”\u003c/strong\u003e itself varies across these tokenizers. Do all tokenizers result in the same number of tokens for a given input text? If not, how different are the generated tokens? How significant are the differences?\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this article, we explore these questions and examine the practical implications of tokenization variability. We present a comparative story of two frontier model families: \u003ca href=\"https://openai.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI\u003c/a\u003e’s ChatGPT vs \u003ca href=\"https://www.anthropic.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAnthropic\u003c/a\u003e’s Claude. Although their advertised “cost-per-token” figures are highly competitive, experiments reveal that Anthropic models can be 20–30% more expensive than GPT models.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcjYkbaat45UUmrmJac7xr-F8GAU4A57MqFpL3YvypnpWsKETErWZVbgaCjqceVNY2Pm6McXBjN1l80iSYet85-hbzAaHIctFp2qcOW4inuoJvUlTSJ5K3R0Av0i9GIy7zv3ltyJg?key=uYyVfXEMcQuopeQSrSvTcEex\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-api-pricing-claude-3-5-sonnet-vs-gpt-4o\"\u003e\u003cstrong\u003eAPI Pricing — Claude 3.5 Sonnet vs GPT-4o\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs of June 2024, the pricing structure for these two advanced frontier models is highly competitive. Both Anthropic’s Claude 3.5 Sonnet and OpenAI’s GPT-4o have identical costs for output tokens, while Claude 3.5 Sonnet offers a 40% lower cost for input tokens.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"624\" height=\"145\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdAK7hYA9Qdvc4NrG1F7yz33bal2r4qGEliCW8xhvfOE8o0qMZTqOeB_84NzBmXJp0-GDj3L9eAMc2Ww6D3f8Lb0HunNU6DBhquEthh61oTKnz-gh4HP8EhsiYc9lSOLpO6j9LcwQ?key=uYyVfXEMcQuopeQSrSvTcEex\"/\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eSource: \u003ca href=\"https://www.vantage.sh/blog/aws-bedrock-claude-vs-azure-openai-gpt-ai-cost\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eVantage\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cdiv\u003e\u003cp\u003eDespite lower input token rates of the Anthropic model, we observed that the total costs of running experiments (on a given set of fixed prompts) with GPT-4o is much cheaper when compared to Claude Sonnet-3.5.\u003c/p\u003e\u003cp\u003eWhy?\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe Anthropic tokenizer tends to break down the same input into more tokens compared to OpenAI’s tokenizer.\u003cstrong\u003e \u003c/strong\u003eThis means that, for identical prompts, Anthropic models produce considerably more tokens than their OpenAI counterparts. As a result, while the per-token cost for Claude 3.5 Sonnet’s input may be lower, the increased tokenization can offset these savings, leading to higher overall costs in practical use cases. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis hidden cost stems from the way Anthropic’s tokenizer encodes information, often using more tokens to represent the same content. The token count inflation has a significant impact on costs and context window utilization.\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-domain-dependent-tokenization-inefficiency\"\u003e\u003cstrong\u003eDomain-dependent tokenization inefficiency\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eDifferent types of domain content are tokenized differently by Anthropic’s tokenizer, leading to varying levels of increased token counts compared to OpenAI’s models. The AI research community has noted similar tokenization differences \u003ca href=\"https://github.com/gkamradt/LLMTest_NeedleInAHaystack/issues/25#issue-2172585625\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e. We tested our findings on three popular domains, namely: English articles, code (Python) and math.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eDomain\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003eModel Input\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003eGPT Tokens\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003eClaude Tokens\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003e% Token Overhead\u003c/strong\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eEnglish articles\u003c/td\u003e\u003ctd\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdjCIsR-wwC_VvwAgm3cw0Cys1IRIk9OuXBUYe2ydbwPRMDzBVPsom7vL-hk2VgDsYL65GcUgcwh_owIGVCu-0PKegRJpPWuf8utXK_lVOt42uwg7vIpRCy-Fn1KVpPPlRQUqXZ?key=uYyVfXEMcQuopeQSrSvTcEex\" width=\"119\" height=\"48\"/\u003e\u003c/td\u003e\u003ctd\u003e77\u003c/td\u003e\u003ctd\u003e89\u003c/td\u003e\u003ctd\u003e~16%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eCode (Python)\u003c/td\u003e\u003ctd\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXepTz9vd3dAyEY855SAs63QEohyV9qA2Ty_8Jm1PMUDwVhVYsQavqCY1_H7qVzFnRWFhJPGBHrx9NVkaOd7wOEvAf7xTQ-NZgbk8z9ZSZbgsI5RyPAD_4BAikrjqzTN_UROpBvT4Q?key=uYyVfXEMcQuopeQSrSvTcEex\" width=\"119\" height=\"65\"/\u003e\u003c/td\u003e\u003ctd\u003e60\u003c/td\u003e\u003ctd\u003e78\u003c/td\u003e\u003ctd\u003e~30%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eMath\u003c/td\u003e\u003ctd\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfcD-nA_oc7iUh0_bo6gK9E-9R9JHmvwl4B0KmCYAKbFTKuULgTn-oztgx6PKOzh7-gJRbdSp8ViM7tBx7tEYXBCjX4QyqYEVScPF6qYEbJdgJveBri2CmQMjAQxlrqx5gS7rV-JQ?key=uYyVfXEMcQuopeQSrSvTcEex\" width=\"119\" height=\"11\"/\u003e\u003c/td\u003e\u003ctd\u003e114\u003c/td\u003e\u003ctd\u003e138\u003c/td\u003e\u003ctd\u003e~21%\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e\u003cem\u003e% Token Overhead of Claude 3.5 Sonnet Tokenizer (relative to GPT-4o) Source: Lavanya Gupta\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen comparing Claude 3.5 Sonnet to GPT-4o, the degree of tokenizer inefficiency varies significantly across content domains. For English articles, Claude’s tokenizer produces approximately 16% more tokens than GPT-4o for the same input text. This overhead increases sharply with more structured or technical content: for mathematical equations, the overhead stands at 21%, and for Python code, Claude generates 30% more tokens.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis variation arises because some content types, such as technical documents and code, often contain patterns and symbols that Anthropic’s tokenizer fragments into smaller pieces, leading to a higher token count. In contrast, more natural language content tends to exhibit a lower token overhead.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-other-practical-implications-of-tokenizer-inefficiency\"\u003e\u003cstrong\u003eOther practical implications of tokenizer inefficiency\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eBeyond the direct implication on costs, there is also an indirect impact on the context window utilization.  While Anthropic models claim a larger context window of 200K tokens, as opposed to OpenAI’s 128K tokens, due to verbosity, the effective usable token space may be smaller for Anthropic models. Hence, there could potentially be a small or large difference in the “advertised” context window sizes vs the “effective” context window sizes.\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-implementation-of-tokenizers\"\u003e\u003cstrong\u003eImplementation of tokenizers\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eGPT models use \u003ca href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding\"\u003eByte Pair Encoding (BPE\u003c/a\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e)\u003c/a\u003e, which merges frequently co-occurring character pairs to form tokens. Specifically, the latest GPT models use the open-source o200k_base tokenizer. The actual tokens used by GPT-4o (in the tiktoken tokenizer) can be viewed\u003ca href=\"https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e here\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003eJSON\n \n{\n    #reasoning\n    \u0026#34;o1-xxx\u0026#34;: \u0026#34;o200k_base\u0026#34;,\n    \u0026#34;o3-xxx\u0026#34;: \u0026#34;o200k_base\u0026#34;,\n\n    # chat\n    \u0026#34;chatgpt-4o-\u0026#34;: \u0026#34;o200k_base\u0026#34;,\n    \u0026#34;gpt-4o-xxx\u0026#34;: \u0026#34;o200k_base\u0026#34;,  # e.g., gpt-4o-2024-05-13\n    \u0026#34;gpt-4-xxx\u0026#34;: \u0026#34;cl100k_base\u0026#34;,  # e.g., gpt-4-0314, etc., plus gpt-4-32k\n    \u0026#34;gpt-3.5-turbo-xxx\u0026#34;: \u0026#34;cl100k_base\u0026#34;,  # e.g, gpt-3.5-turbo-0301, -0401, etc.\n}\n\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eUnfortunately, not much can be said about Anthropic tokenizers as their tokenizer is not as directly and easily available as GPT. Anthropic \u003ca href=\"https://docs.anthropic.com/en/release-notes/api#december-17th-2024\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ereleased their Token Counting API in Dec 2024\u003c/a\u003e. However, it was soon demised in later 2025 versions.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003e\u003ca href=\"https://latenode.com/blog/ai-anthropic-claude-3-overview\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLatenode\u003c/a\u003e reports that “Anthropic uses a unique tokenizer with only 65,000 token variations, compared to OpenAI’s 100,261 token variations for GPT-4.” This \u003ca href=\"https://colab.research.google.com/drive/1jYl4aW65Ko1e8QBca2b5hJvxrASJ-dHf#scrollTo=pOHNGlhTv3Zy\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eColab notebook\u003c/a\u003e contains Python code to analyze the tokenization differences between GPT and Claude models. Another \u003ca href=\"https://tokenizer.model.box/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etool\u003c/a\u003e that enables interfacing with some common, publicly available tokenizers validates our findings.\u003c/p\u003e\u003cp\u003eThe ability to proactively estimate token counts (without invoking the actual model API) and budget costs is crucial for AI enterprises. \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch2 id=\"h-key-takeaways\"\u003e\u003cstrong\u003eKey Takeaways\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAnthropic’s competitive pricing comes with hidden costs:\u003cbr/\u003e\u003c/strong\u003eWhile Anthropic’s Claude 3.5 Sonnet offers 40% lower input token costs compared to OpenAI’s GPT-4o, this apparent cost advantage can be misleading due to differences in how input text is tokenized.\u003cbr/\u003e\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eHidden “tokenizer inefficiency”:\u003cbr/\u003e\u003c/strong\u003eAnthropic models are inherently more \u003cstrong\u003everbose\u003c/strong\u003e. For businesses that process large volumes of text, understanding this discrepancy is crucial when evaluating the true cost of deploying models.\u003cbr/\u003e\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eDomain-dependent tokenizer inefficiency:\u003cbr/\u003e\u003c/strong\u003eWhen choosing between OpenAI and Anthropic models, \u003cstrong\u003eevaluate the nature of your input text\u003c/strong\u003e. For natural language tasks, the cost difference may be minimal, but technical or structured domains may lead to significantly higher costs with Anthropic models.\u003cbr/\u003e\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eEffective context window:\u003cbr/\u003e\u003c/strong\u003eDue to the verbosity of Anthropic’s tokenizer, its larger advertised 200K context window may offer less effective usable space than OpenAI’s 128K, leading to a\u003cstrong\u003e \u003c/strong\u003epotential\u003cstrong\u003e gap between advertised and actual context window\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eAnthropic did not respond to VentureBeat’s requests for comment by press time. We’ll update the story if they respond.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-05-01T20:14:04Z",
  "modifiedTime": "2025-05-01T20:14:17Z"
}
