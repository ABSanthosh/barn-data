{
  "id": "fa22d318-be77-4282-bb30-d583fdc11b6b",
  "title": "Salesforce launches Agentforce Testing Center to put agents through paces",
  "link": "https://venturebeat.com/ai/salesforce-launches-agentforce-testing-center-to-put-agents-through-paces/",
  "description": "Salesforce announced Testing Center, a platform that lets enterprises test AI agents responses without touching sensitive information.",
  "author": "Emilia David",
  "published": "Thu, 21 Nov 2024 21:47:54 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AgentForce",
    "Agentforce Testing Center",
    "Agentic AI",
    "AI agent",
    "AI agents",
    "AI model evaluation",
    "AI, ML and Deep Learning",
    "category-/Business \u0026 Industrial",
    "category-/Computers \u0026 Electronics/Enterprise Technology",
    "category-/Computers \u0026 Electronics/Software",
    "data",
    "Data Storage and Cloud",
    "Enterprise",
    "model evaluation",
    "observability",
    "Salesforce",
    "sandbox"
  ],
  "byline": "Emilia David",
  "length": 4271,
  "excerpt": "Salesforce announced Testing Center, a platform that lets enterprises test AI agents responses without touching sensitive information.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "November 21, 2024 1:47 PM Credit: VentureBeat generated by MidJourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More The next phase of agentic AI may just be evaluation and monitoring, as enterprises want to make the agents they’re beginning to deploy more observable. While AI agent benchmarks can be misleading, there’s a lot of value in seeing if the agent is working the way they want to. To this end, companies are beginning to offer platforms where customers can sandbox AI agents or evaluate their performance. Salesforce released its agent evaluation platform, Agentforce Testing Center, in a limited pilot Wednesday. General availability is expected in December. Testing Center lets enterprises observe and prototype AI agents to ensure they access the workflows and data they need.  Testing Center’s new capabilities include AI-generated tests for Agentforce, Sandboxes for Agentforce and Data Cloud and monitoring and observability for Agentforce.  AI-generated tests allow companies to use AI models to generate “hundreds of synthetic interactions” to test if agents end up in how often they answer the way companies want. As the name suggests, sandboxes offer an isolated environment to test agents while mirroring a company’s data to reflect better how the agent will work for them. Monitoring and observability let enterprises bring an audit trail to the sandbox when the agents go into production.  Patrick Stokes, executive vice president of product and industries marketing at Salesforce, told VentureBeat that the Testing Center is part of a new class of agents the company calls Agent Lifecycle Management.  “We are positioning what we think will be a big new subcategory of agents,” Stokes said. “When we say lifecycle, we mean the whole thing from genesis to development all the way through deployment, and then iterations of your deployment as you go forward.” Stokes said that right now, the Testing Center doesn’t have workflow-specific insights where developers can see the specific choices in API, data or model the agents used. However, Salesforce collects that kind of data on its Einstein Trust Layer. “What we’re doing is building developer tools to expose that metadata to our customers so that they can actually use it to better build their agents,” Stokes said. Salesforce is hanging its hat on AI agents, focusing a lot of its energy on its agentic offering Agentforce. Salesforce customers can use preset agents or build customized agents on Agentforce to connect to their instances.  Evaluating agents AI agents touch many points in an organization, and since good agentic ecosystems aim to automate a big chunk of workflows, making sure they work well becomes essential.  If an agent decides to tap the wrong API, it could spell disaster for a business. AI agents are stochastic in nature, like the models that power them, and consider each potential probability before coming up with an outcome. Stokes said Salesforce tests agents by barraging the agent with versions of the same utterances or questions. Its responses are scored as pass or fail, allowing the agent to learn and evolve within a safe environment that human developers can control.  Platforms that help enterprises evaluate AI agents are fast becoming a new type of product offering. In June, customer experience AI company Sierra launched an AI agent benchmark called TAU-bench to look at the performance of conversational agents. Automation company UiPath released its Agent Builder platform in October which also offered a means to evaluate agent performance before full deployment.  Testing AI applications is nothing new. Other than benchmarking model performances, many AI model repositories like AWS Bedrock and Microsoft Azure already let customers test out foundation models in a controlled environment to see which one works best for their use cases.  VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/11/crimedy7_illustration_of_a_robot_taking_a_test_in_a_school_sett_12a0f62d-a5ef-4a72-b8d0-0e60b59fe1cb.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-11-21T21:47:54+00:00\" datetime=\"2024-11-21T21:47:54+00:00\"\u003eNovember 21, 2024 1:47 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/crimedy7_illustration_of_a_robot_taking_a_test_in_a_school_sett_12a0f62d-a5ef-4a72-b8d0-0e60b59fe1cb.png?w=750\" alt=\"Credit: VentureBeat generated by MidJourney\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat generated by MidJourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eThe next phase of agentic AI may just be evaluation and monitoring, as enterprises want to make the agents they’re beginning to deploy more observable.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile \u003ca href=\"https://venturebeat.com/ai/ai-agent-benchmarks-are-misleading-study-warns/\"\u003eAI agent benchmarks can be misleading\u003c/a\u003e, there’s a lot of value in seeing if the agent is working the way they want to. To this end, companies are beginning to offer platforms where customers can sandbox AI agents or evaluate their performance.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://www.salesforce.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSalesforce\u003c/a\u003e released its agent evaluation platform, Agentforce Testing Center, in a limited pilot Wednesday. General availability is expected in December. Testing Center lets enterprises observe and prototype AI agents to ensure they access the workflows and data they need. \u003c/p\u003e\n\n\n\n\u003cp\u003eTesting Center’s new capabilities include AI-generated tests for Agentforce, Sandboxes for Agentforce and Data Cloud and monitoring and observability for Agentforce. \u003c/p\u003e\n\n\n\n\u003cp\u003eAI-generated tests allow companies to use AI models to generate “hundreds of synthetic interactions” to test if agents end up in how often they answer the way companies want. As the name suggests, sandboxes offer an isolated environment to test agents while mirroring a company’s data to reflect better how the agent will work for them. Monitoring and observability let enterprises bring an audit trail to the sandbox when the agents go into production. \u003c/p\u003e\n\n\n\n\u003cp\u003ePatrick Stokes, executive vice president of product and industries marketing at Salesforce, told VentureBeat that the Testing Center is part of a new class of agents the company calls Agent Lifecycle Management. \u003c/p\u003e\n\n\n\n\u003cp\u003e“We are positioning what we think will be a big new subcategory of agents,” Stokes said. “When we say lifecycle, we mean the whole thing from genesis to development all the way through deployment, and then iterations of your deployment as you go forward.”\u003c/p\u003e\n\n\n\n\u003cp\u003eStokes said that right now, the Testing Center doesn’t have workflow-specific insights where developers can see the specific choices in API, data or model the agents used. However, Salesforce collects that kind of data on its Einstein Trust Layer.\u003c/p\u003e\n\n\n\n\u003cp\u003e“What we’re doing is building developer tools to expose that metadata to our customers so that they can actually use it to better build their agents,” Stokes said.\u003c/p\u003e\n\n\n\n\u003cp\u003eSalesforce is hanging its hat on AI agents, focusing a lot of its energy on its \u003ca href=\"https://venturebeat.com/ai/salesforces-agentforce-the-ai-assistants-that-want-to-run-your-entire-business/\"\u003eagentic offering Agentforce\u003c/a\u003e. Salesforce customers can use preset agents or build customized agents on Agentforce to connect to their instances. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-evaluating-agents\"\u003eEvaluating agents\u003c/h2\u003e\n\n\n\n\u003cp\u003eAI agents touch many points in an organization, and since good agentic ecosystems aim to automate a big chunk of workflows, making sure they work well \u003ca href=\"https://venturebeat.com/ai/how-to-get-started-with-ai-agents-and-do-it-right/\"\u003ebecomes essential\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eIf an agent decides to tap the wrong API, it could spell disaster for a business. AI agents are stochastic in nature, like the models that power them, and consider each potential probability before coming up with an outcome. Stokes said Salesforce tests agents by barraging the agent with versions of the same utterances or questions. Its responses are scored as pass or fail, allowing the agent to learn and evolve within a safe environment that human developers can control. \u003c/p\u003e\n\n\n\n\u003cp\u003ePlatforms that help enterprises evaluate AI agents are fast becoming a new type of product offering. In June, customer experience AI company \u003ca href=\"https://sierra.ai/\"\u003eSierra\u003c/a\u003e \u003ca href=\"https://venturebeat.com/ai/sierras-new-benchmark-reveals-how-well-ai-agents-perform-at-real-work/\"\u003elaunched an AI agent benchmark\u003c/a\u003e called TAU-bench to look at the performance of conversational agents. Automation company \u003ca href=\"https://www.uipath.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eUiPath\u003c/a\u003e released its \u003ca href=\"https://www.uipath.com/newsroom/uipath-unveils-vision-for-future-agentic-automation\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAgent Builder platform in October\u003c/a\u003e which also offered a means to evaluate agent performance before full deployment. \u003c/p\u003e\n\n\n\n\u003cp\u003eTesting AI applications is nothing new. Other than benchmarking model performances, many AI model repositories like AWS Bedrock and Microsoft Azure already let customers test out foundation models in a controlled environment to see which one works best for their use cases. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-11-21T21:47:54Z",
  "modifiedTime": "2024-11-21T21:48:02Z"
}
