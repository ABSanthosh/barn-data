{
  "id": "9a9a38dd-91f0-4916-81d4-c8e379e238f3",
  "title": "Do reasoning models really “think” or not? Apple research sparks lively debate, response",
  "link": "https://venturebeat.com/ai/do-reasoning-models-really-think-or-not-apple-research-sparks-lively-debate-response/",
  "description": "Ultimately, the big takeaway for ML researchers is that before proclaiming an AI milestone—or obituary—make sure the test itself isn’t flawed",
  "author": "Carl Franzen",
  "published": "Fri, 13 Jun 2025 22:02:22 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI, ML and Deep Learning",
    "Anthropic",
    "Apple",
    "Apple AI",
    "apple inc",
    "Apple Intelligence",
    "apple ml",
    "Apple Research",
    "Claude",
    "Conversational AI",
    "Large Reasoning Models (LRMs)",
    "machine learning",
    "NLP",
    "OpenAI",
    "reasoning llms",
    "reasoning models",
    "Siri"
  ],
  "byline": "Carl Franzen",
  "length": 13029,
  "excerpt": "Ultimately, the big takeaway for ML researchers is that before proclaiming an AI milestone—or obituary—make sure the test itself isn’t flawed",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "June 13, 2025 3:02 PM Credit: VentureBeat made with Midjourney Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy. Learn more Apple’s machine-learning group set off a rhetorical firestorm earlier this month with its release of “The Illusion of Thinking,” a 53-page research paper arguing that so-called large reasoning models (LRMs) or reasoning large language models (reasoning LLMs) such as OpenAI’s “o” series and Google’s Gemini-2.5 Pro and Flash Thinking don’t actually engage in independent “thinking” or “reasoning” from generalized first principles learned from their training data. Instead, the authors contend, these reasoning LLMs are actually performing a kind of “pattern matching” and their apparent reasoning ability seems to fall apart once a task becomes too complex, suggesting that their architecture and performance is not a viable path to improving generative AI to the point that it is artificial generalized intelligence (AGI), which OpenAI defines as a model that outperforms humans at most economically valuable work, or superintelligence, AI even smarter than human beings can comprehend. ACT NOW: Come discuss the latest LLM advances and research at VB Transform on June 24-25 in SF — limited tickets available. REGISTER NOW Unsurprisingly, the paper immediately circulated widely among the machine learning community on X and many readers’ initial reactions were to declare that Apple had effectively disproven much of the hype around this class of AI: “Apple just proved AI ‘reasoning’ models like Claude, DeepSeek-R1, and o3-mini don’t actually reason at all,” declared Ruben Hassid, creator of EasyGen, an LLM-driven LinkedIn post auto writing tool. “They just memorize patterns really well.” But now today, a new paper has emerged, the cheekily titled “The Illusion of The Illusion of Thinking” — importantly, co-authored by a reasoning LLM itself, Claude Opus 4 and Alex Lawsen, a human being and independent AI researcher and technical writer — that includes many criticisms from the larger ML community about the paper and effectively argues that the methodologies and experimental designs the Apple Research team used in their initial work are fundamentally flawed. While we here at VentureBeat are not ML researchers ourselves and not prepared to say the Apple Researchers are wrong, the debate has certainly been a lively one and the issue about the capabilities of LRMs or reasoner LLMs compared to human thinking seems far from settled. How the Apple Research study was designed — and what it found Using four classic planning problems — Tower of Hanoi, Blocks World, River Crossing and Checkers Jumping — Apple’s researchers designed a battery of tasks that forced reasoning models to plan multiple moves ahead and generate complete solutions. These games were chosen for their long history in cognitive science and AI research and their ability to scale in complexity as more steps or constraints are added. Each puzzle required the models to not just produce a correct final answer, but to explain their thinking along the way using chain-of-thought prompting. As the puzzles increased in difficulty, the researchers observed a consistent drop in accuracy across multiple leading reasoning models. In the most complex tasks, performance plunged to zero. Notably, the length of the models’ internal reasoning traces—measured by the number of tokens spent thinking through the problem—also began to shrink. Apple’s researchers interpreted this as a sign that the models were abandoning problem-solving altogether once the tasks became too hard, essentially “giving up.” The timing of the paper’s release, just ahead of Apple’s annual Worldwide Developers Conference (WWDC), added to the impact. It quickly went viral across X, where many interpreted the findings as a high-profile admission that current-generation LLMs are still glorified autocomplete engines, not general-purpose thinkers. This framing, while controversial, drove much of the initial discussion and debate that followed. Critics take aim on X Among the most vocal critics of the Apple paper was ML researcher and X user @scaling01 (aka “Lisan al Gaib”), who posted multiple threads dissecting the methodology. In one widely shared post, Lisan argued that the Apple team conflated token budget failures with reasoning failures, noting that “all models will have 0 accuracy with more than 13 disks simply because they cannot output that much!” For puzzles like Tower of Hanoi, he emphasized, the output size grows exponentially, while the LLM context windows remain fixed, writing “just because Tower of Hanoi requires exponentially more steps than the other ones, that only require quadratically or linearly more steps, doesn’t mean Tower of Hanoi is more difficult” and convincingly showed that models like Claude 3 Sonnet and DeepSeek-R1 often produced algorithmically correct strategies in plain text or code—yet were still marked wrong. Another post highlighted that even breaking the task down into smaller, decomposed steps worsened model performance—not because the models failed to understand, but because they lacked memory of previous moves and strategy. “The LLM needs the history and a grand strategy,” he wrote, suggesting the real problem was context-window size rather than reasoning. I raised another important grain of salt myself on X: Apple never benchmarked the model performance against human performance on the same tasks. “Am I missing it, or did you not compare LRMs to human perf[ormance] on [the] same tasks?? If not, how do you know this same drop-off in perf doesn’t happen to people, too?” I asked the researchers directly in a thread tagging the paper’s authors. I also emailed them about this and many other questions, but they have yet to respond. Others echoed that sentiment, noting that human problem solvers also falter on long, multistep logic puzzles, especially without pen-and-paper tools or memory aids. Without that baseline, Apple’s claim of a fundamental “reasoning collapse” feels ungrounded. Several researchers also questioned the binary framing of the paper’s title and thesis—drawing a hard line between “pattern matching” and “reasoning.” Alexander Doria aka Pierre-Carl Langlais, an LLM trainer at energy efficient French AI startup Pleias, said the framing misses the nuance, arguing that models might be learning partial heuristics rather than simply matching patterns. Ok I guess I have to go through that Apple paper. My main issue is the framing which is super binary: \"Are these models capable of generalizable reasoning, or are they leveraging different forms of pattern matching?\" Or what if they only caught genuine yet partial heuristics. pic.twitter.com/GZE3eG7WlM— Alexander Doria (@Dorialexander) June 8, 2025 Ethan Mollick, the AI focused professor at University of Pennsylvania’s Wharton School of Business, called the idea that LLMs are “hitting a wall” premature, likening it to similar claims about “model collapse” that didn’t pan out. Meanwhile, critics like @arithmoquine were more cynical, suggesting that Apple—behind the curve on LLMs compared to rivals like OpenAI and Google—might be trying to lower expectations,” coming up with research on “how it’s all fake and gay and doesn’t matter anyway” they quipped, pointing out Apple’s reputation with now poorly performing AI products like Siri. In short, while Apple’s study triggered a meaningful conversation about evaluation rigor, it also exposed a deep rift over how much trust to place in metrics when the test itself might be flawed. A measurement artifact, or a ceiling? In other words, the models may have understood the puzzles but ran out of “paper” to write the full solution. “Token limits, not logic, froze the models,” wrote Carnegie Mellon researcher Rohan Paul in a widely shared thread summarizing the follow-up tests. Yet not everyone is ready to clear LRMs of the charge. Some observers point out that Apple’s study still revealed three performance regimes — simple tasks where added reasoning hurts, mid-range puzzles where it helps, and high-complexity cases where both standard and “thinking” models crater. Others view the debate as corporate positioning, noting that Apple’s own on-device “Apple Intelligence” models trail rivals on many public leaderboards. The rebuttal: “The Illusion of the Illusion of Thinking” In response to Apple’s claims, a new paper titled “The Illusion of the Illusion of Thinking” was released on arXiv by independent researcher and technical writer Alex Lawsen of the nonprofit Open Philanthropy, in collaboration with Anthropic’s Claude Opus 4. The paper directly challenges the original study’s conclusion that LLMs fail due to an inherent inability to reason at scale. Instead, the rebuttal presents evidence that the observed performance collapse was largely a by-product of the test setup—not a true limit of reasoning capability. Lawsen and Claude demonstrate that many of the failures in the Apple study stem from token limitations. For example, in tasks like Tower of Hanoi, the models must print exponentially many steps — over 32,000 moves for just 15 disks — leading them to hit output ceilings. The rebuttal points out that Apple’s evaluation script penalized these token-overflow outputs as incorrect, even when the models followed a correct solution strategy internally. The authors also highlight several questionable task constructions in the Apple benchmarks. Some of the River Crossing puzzles, they note, are mathematically unsolvable as posed, and yet model outputs for these cases were still scored. This further calls into question the conclusion that accuracy failures represent cognitive limits rather than structural flaws in the experiments. To test their theory, Lawsen and Claude ran new experiments allowing models to give compressed, programmatic answers. When asked to output a Lua function that could generate the Tower of Hanoi solution—rather than writing every step line-by-line—models suddenly succeeded on far more complex problems. This shift in format eliminated the collapse entirely, suggesting that the models didn’t fail to reason. They simply failed to conform to an artificial and overly strict rubric. Why it matters for enterprise decision-makers The back-and-forth underscores a growing consensus: evaluation design is now as important as model design. Requiring LRMs to enumerate every step may test their printers more than their planners, while compressed formats, programmatic answers or external scratchpads give a cleaner read on actual reasoning ability. The episode also highlights practical limits developers face as they ship agentic systems—context windows, output budgets and task formulation can make or break user-visible performance. For enterprise technical decision makers building applications atop reasoning LLMs, this debate is more than academic. It raises critical questions about where, when, and how to trust these models in production workflows—especially when tasks involve long planning chains or require precise step-by-step output. If a model appears to “fail” on a complex prompt, the problem may not lie in its reasoning ability, but in how the task is framed, how much output is required, or how much memory the model has access to. This is particularly relevant for industries building tools like copilots, autonomous agents, or decision-support systems, where both interpretability and task complexity can be high. Understanding the constraints of context windows, token budgets, and the scoring rubrics used in evaluation is essential for reliable system design. Developers may need to consider hybrid solutions that externalize memory, chunk reasoning steps, or use compressed outputs like functions or code instead of full verbal explanations. Most importantly, the paper’s controversy is a reminder that benchmarking and real-world application are not the same. Enterprise teams should be cautious of over-relying on synthetic benchmarks that don’t reflect practical use cases—or that inadvertently constrain the model’s ability to demonstrate what it knows. Ultimately, the big takeaway for ML researchers is that before proclaiming an AI milestone—or obituary—make sure the test itself isn’t putting the system in a box too small to think inside. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/06/cfr0z3n_pen_drawing_black_ink_on_white_background_technical_sch_27601b38-b730-4322-98fb-371d1318a22f.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-06-13T22:02:22+00:00\" datetime=\"2025-06-13T22:02:22+00:00\"\u003eJune 13, 2025 3:02 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2025/06/cfr0z3n_pen_drawing_black_ink_on_white_background_technical_sch_27601b38-b730-4322-98fb-371d1318a22f.png?w=750\" alt=\"Black stylized Apple Inc logo surrounded by glitchy artifacts on white backdrop\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy. \u003ca href=\"http://vbtransform.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLearn more\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eApple’s machine-learning group set off a rhetorical firestorm earlier this month with its release of “\u003ca href=\"https://machinelearning.apple.com/research/illusion-of-thinking\"\u003eThe Illusion of Thinking\u003c/a\u003e,” a 53-page research paper arguing that so-called large reasoning models (LRMs) or reasoning large language models (reasoning LLMs) such as OpenAI’s “o” series and Google’s Gemini-2.5 Pro and Flash Thinking don’t actually engage in independent “thinking” or “reasoning” from generalized first principles learned from their training data.\u003c/p\u003e\n\n\n\n\u003cp\u003eInstead, the authors contend, these reasoning LLMs are actually performing a kind of “pattern matching” and their apparent reasoning ability seems to fall apart once a task becomes too complex, suggesting that their architecture and performance is not a viable path to improving generative AI to the point that it is artificial generalized intelligence (AGI), which OpenAI defines as a model that outperforms humans at most economically valuable work, or superintelligence, AI even smarter than human beings can comprehend.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-act-now-come-discuss-the-latest-llm-advances-and-research-at-vb-transform-on-june-24-25-in-sf-limited-tickets-available-register-now\"\u003e\u003cem\u003eACT NOW: Come discuss the latest LLM advances and research at VB Transform on June 24-25 in SF — limited tickets available\u003c/em\u003e. \u003ca href=\"https://www.vbtransform.com/?utm_source=vb\u0026amp;utm_medium=article\u0026amp;utm_content=transformpromo\u0026amp;utm_campaign=vbarticles\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eREGISTER NOW\u003c/a\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eUnsurprisingly, the paper immediately circulated widely among the machine learning community on X and many readers’ initial reactions were to declare that Apple had effectively disproven much of the hype around this class of AI: “Apple just proved AI ‘reasoning’ models like Claude, DeepSeek-R1, and o3-mini don’t actually reason at all,” \u003ca href=\"https://x.com/RubenHssd/status/1931389580105925115\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edeclared Ruben Hassid\u003c/a\u003e, creator of EasyGen, an LLM-driven LinkedIn post auto writing tool. “They just memorize patterns really well.”\u003c/p\u003e\n\n\n\n\u003cp\u003eBut now today, \u003ca href=\"https://arxiv.org/pdf/2506.09250\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ea new paper has emerged\u003c/a\u003e, the cheekily titled “\u003ca href=\"https://arxiv.org/pdf/2506.09250\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eThe Illusion of The Illusion of Thinking\u003c/a\u003e” — importantly, co-authored by a reasoning LLM itself, Claude Opus 4 and Alex Lawsen, a human being and independent AI researcher and technical writer — that includes many criticisms from the larger ML community about the paper and effectively argues that the methodologies and experimental designs the Apple Research team used in their initial work are fundamentally flawed.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile we here at VentureBeat are not ML researchers ourselves and not prepared to say the Apple Researchers are wrong, the debate has certainly been a lively one and the issue about the capabilities of LRMs or reasoner LLMs compared to human thinking seems far from settled.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-the-apple-research-study-was-designed-and-what-it-found\"\u003eHow the Apple Research study was designed — and what it found\u003c/h2\u003e\n\n\n\n\u003cp\u003eUsing four classic planning problems — Tower of Hanoi, Blocks World, River Crossing and Checkers Jumping — Apple’s researchers designed a battery of tasks that forced reasoning models to plan multiple moves ahead and generate complete solutions. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" height=\"482\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/06/Gs7EI0SXsAAVpXk.jpg?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/06/Gs7EI0SXsAAVpXk.jpg 1458w, https://venturebeat.com/wp-content/uploads/2025/06/Gs7EI0SXsAAVpXk.jpg?resize=300,181 300w, https://venturebeat.com/wp-content/uploads/2025/06/Gs7EI0SXsAAVpXk.jpg?resize=768,462 768w, https://venturebeat.com/wp-content/uploads/2025/06/Gs7EI0SXsAAVpXk.jpg?resize=800,482 800w, https://venturebeat.com/wp-content/uploads/2025/06/Gs7EI0SXsAAVpXk.jpg?resize=400,241 400w, https://venturebeat.com/wp-content/uploads/2025/06/Gs7EI0SXsAAVpXk.jpg?resize=750,452 750w, https://venturebeat.com/wp-content/uploads/2025/06/Gs7EI0SXsAAVpXk.jpg?resize=578,348 578w, https://venturebeat.com/wp-content/uploads/2025/06/Gs7EI0SXsAAVpXk.jpg?resize=930,560 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThese games were chosen for their long history in cognitive science and AI research and their ability to scale in complexity as more steps or constraints are added. Each puzzle required the models to not just produce a correct final answer, but to explain their thinking along the way using chain-of-thought prompting.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs the puzzles increased in difficulty, the researchers observed a consistent drop in accuracy across multiple leading reasoning models. In the most complex tasks, performance plunged to zero. Notably, the length of the models’ internal reasoning traces—measured by the number of tokens spent thinking through the problem—also began to shrink. Apple’s researchers interpreted this as a sign that the models were abandoning problem-solving altogether once the tasks became too hard, essentially “giving up.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe timing of the paper’s release,\u003ca href=\"https://developer.apple.com/wwdc25/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e just ahead of Apple’s annual Worldwide Developers Conference (WWDC)\u003c/a\u003e, added to the impact. It quickly went viral across X, where many interpreted the findings as a high-profile admission that current-generation LLMs are still glorified autocomplete engines, not general-purpose thinkers. This framing, while controversial, drove much of the initial discussion and debate that followed.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-critics-take-aim-on-x\"\u003eCritics take aim on X\u003c/h2\u003e\n\n\n\n\u003cp\u003eAmong the most vocal critics of the Apple paper \u003ca href=\"https://x.com/scaling01\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ewas ML researcher and X user @scaling01\u003c/a\u003e (aka “Lisan al Gaib”), who posted multiple threads dissecting the methodology. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn \u003ca href=\"https://x.com/scaling01/status/1931783050511126954\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eone widely shared post\u003c/a\u003e, Lisan argued that the Apple team conflated token budget failures with reasoning failures, noting that “all models will have 0 accuracy with more than 13 disks simply because they cannot output that much!” \u003c/p\u003e\n\n\n\n\u003cp\u003eFor puzzles like Tower of Hanoi, he emphasized, the output size grows exponentially, while the LLM context windows remain fixed, writing “just because Tower of Hanoi requires exponentially more steps than the other ones, that only require quadratically or linearly more steps, doesn’t mean Tower of Hanoi is more difficult” and convincingly showed that models like Claude 3 Sonnet and DeepSeek-R1 often produced algorithmically correct strategies in plain text or code—yet were still marked wrong.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://x.com/scaling01/status/1931882132256751724\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAnother post\u003c/a\u003e highlighted that even breaking the task down into smaller, decomposed steps worsened model performance—not because the models failed to understand, but because they lacked memory of previous moves and strategy. \u003c/p\u003e\n\n\n\n\u003cp\u003e“The LLM needs the history and a grand strategy,” he wrote, suggesting the real problem was context-window size rather than reasoning.\u003c/p\u003e\n\n\n\n\u003cp\u003eI raised \u003ca href=\"https://x.com/carlfranzen/status/1931696990447944008/photo/1\"\u003eanother important grain of salt myself on X\u003c/a\u003e: Apple never benchmarked the model performance against human performance on the same tasks. “Am I missing it, or did you not compare LRMs to human perf[ormance] on [the] same tasks?? If not, how do you know this same drop-off in perf doesn’t happen to people, too?” I asked the researchers directly in a thread tagging the paper’s authors. I also emailed them about this and many other questions, but they have yet to respond.\u003c/p\u003e\n\n\n\n\u003cp\u003eOthers echoed that sentiment, noting that human problem solvers also falter on long, multistep logic puzzles, especially without pen-and-paper tools or memory aids. Without that baseline, Apple’s claim of a fundamental “reasoning collapse” feels ungrounded.\u003c/p\u003e\n\n\n\n\u003cp\u003eSeveral researchers also questioned the binary framing of the paper’s title and thesis—drawing a hard line between “pattern matching” and “reasoning.” \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://x.com/Dorialexander/status/1931624658387833263\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAlexander Doria \u003c/a\u003eaka Pierre-Carl Langlais, an LLM trainer at energy efficient French AI startup \u003ca href=\"https://pleias.fr/\"\u003ePleias\u003c/a\u003e, said the framing \u003cem\u003emisses the nuance\u003c/em\u003e, arguing that models might be learning partial heuristics rather than simply matching patterns. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cdiv\u003e\n\u003cblockquote data-width=\"500\" data-dnt=\"true\"\u003e\u003cdiv lang=\"en\" dir=\"ltr\"\u003e\u003cp\u003eOk I guess I have to go through that Apple paper. \u003c/p\u003e\u003cp\u003eMy main issue is the framing which is super binary: \u0026#34;Are these models capable of generalizable reasoning, or are they leveraging different forms of pattern matching?\u0026#34; Or what if they only caught genuine yet partial heuristics. \u003ca href=\"https://t.co/GZE3eG7WlM\"\u003epic.twitter.com/GZE3eG7WlM\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e— Alexander Doria (@Dorialexander) \u003ca href=\"https://twitter.com/Dorialexander/status/1931624658387833263?ref_src=twsrc%5Etfw\"\u003eJune 8, 2025\u003c/a\u003e\u003c/blockquote\u003e\n\u003c/div\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eEthan Mollick, the AI focused professor at University of Pennsylvania’s Wharton School of Business,  called the idea that LLMs are “hitting a wall” premature, likening it to similar claims about “model collapse” that didn’t pan out.\u003c/p\u003e\n\n\n\n\u003cp\u003eMeanwhile, critics like \u003ca href=\"https://x.com/arithmoquine/status/1931256646598082948\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e@arithmoquine \u003c/a\u003ewere more cynical, suggesting that Apple—behind the curve on LLMs compared to rivals like OpenAI and Google—might be trying to lower expectations,” coming up with research on “how it’s all fake and gay and doesn’t matter anyway” they quipped, pointing out Apple’s reputation with now poorly performing AI products like Siri.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn short, while Apple’s study triggered a meaningful conversation about evaluation rigor, it also exposed a deep rift over how much trust to place in metrics when the test itself might be flawed.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-a-measurement-artifact-or-a-ceiling\"\u003eA measurement artifact, or a ceiling?\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn other words, the models may have understood the puzzles but ran out of “paper” to write the full solution.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Token limits, not logic, froze the models,” wrote Carnegie Mellon researcher Rohan Paul in\u003ca href=\"https://x.com/rohanpaul_ai/status/1933296859730301353\"\u003e a widely shared thread summarizing the follow-up tests.\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eYet not everyone is ready to clear LRMs of the charge. Some observers point out that Apple’s study still revealed three performance regimes — simple tasks where added reasoning hurts, mid-range puzzles where it helps, and high-complexity cases where both standard and “thinking” models crater.\u003c/p\u003e\n\n\n\n\u003cp\u003eOthers view the debate as corporate positioning, noting that Apple’s own on-device “Apple Intelligence” models trail rivals on many public leaderboards.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-rebuttal-the-illusion-of-the-illusion-of-thinking\"\u003eThe rebuttal: “The Illusion of the Illusion of Thinking”\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn response to Apple’s claims, a new paper titled “\u003ca href=\"https://arxiv.org/pdf/2506.09250\"\u003eThe Illusion of the Illusion of Thinking\u003c/a\u003e” was released on arXiv by independent researcher and technical writer \u003ca href=\"https://www.openphilanthropy.org/team/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAlex Lawsen of the nonprofit Open Philanthropy\u003c/a\u003e, in collaboration with Anthropic’s Claude Opus 4. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe paper directly challenges the original study’s conclusion that LLMs fail due to an inherent inability to reason at scale. Instead, the rebuttal presents evidence that the observed performance collapse was largely a by-product of the test setup—not a true limit of reasoning capability.\u003c/p\u003e\n\n\n\n\u003cp\u003eLawsen and Claude demonstrate that many of the failures in the Apple study stem from token limitations. For example, in tasks like Tower of Hanoi, the models must print exponentially many steps — over 32,000 moves for just 15 disks — leading them to hit output ceilings. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe rebuttal points out that Apple’s evaluation script penalized these token-overflow outputs as incorrect, even when the models followed a correct solution strategy internally.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe authors also highlight several questionable task constructions in the Apple benchmarks. Some of the River Crossing puzzles, they note, are mathematically unsolvable as posed, and yet model outputs for these cases were still scored. This further calls into question the conclusion that accuracy failures represent cognitive limits rather than structural flaws in the experiments.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo test their theory, Lawsen and Claude ran new experiments allowing models to give compressed, programmatic answers. When asked to output a Lua function that could generate the Tower of Hanoi solution—rather than writing every step line-by-line—models suddenly succeeded on far more complex problems. This shift in format eliminated the collapse entirely, suggesting that the models didn’t fail to reason. They simply failed to conform to an artificial and overly strict rubric.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-why-it-matters-for-enterprise-decision-makers\"\u003eWhy it matters for enterprise decision-makers\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe back-and-forth underscores a growing consensus: evaluation design is now as important as model design.\u003c/p\u003e\n\n\n\n\u003cp\u003eRequiring LRMs to enumerate every step may test their printers more than their planners, while compressed formats, programmatic answers or external scratchpads give a cleaner read on actual reasoning ability.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe episode also highlights practical limits developers face as they ship agentic systems—context windows, output budgets and task formulation can make or break user-visible performance.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor enterprise technical decision makers building applications atop reasoning LLMs, this debate is more than academic. It raises critical questions about where, when, and how to trust these models in production workflows—especially when tasks involve long planning chains or require precise step-by-step output.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf a model appears to “fail” on a complex prompt, the problem may not lie in its reasoning ability, but in how the task is framed, how much output is required, or how much memory the model has access to. This is particularly relevant for industries building tools like copilots, autonomous agents, or decision-support systems, where both interpretability and task complexity can be high.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnderstanding the constraints of context windows, token budgets, and the scoring rubrics used in evaluation is essential for reliable system design. Developers may need to consider hybrid solutions that externalize memory, chunk reasoning steps, or use compressed outputs like functions or code instead of full verbal explanations.\u003c/p\u003e\n\n\n\n\u003cp\u003eMost importantly, the paper’s controversy is a reminder that benchmarking and real-world application are not the same. Enterprise teams should be cautious of over-relying on synthetic benchmarks that don’t reflect practical use cases—or that inadvertently constrain the model’s ability to demonstrate what it knows.\u003c/p\u003e\n\n\n\n\u003cp\u003eUltimately, the big takeaway for ML researchers is that before proclaiming an AI milestone—or obituary—make sure the test itself isn’t putting the system in a box too small to think inside.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": "2025-06-13T22:02:22Z",
  "modifiedTime": "2025-06-13T22:02:33Z"
}
