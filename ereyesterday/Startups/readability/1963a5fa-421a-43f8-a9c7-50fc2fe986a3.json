{
  "id": "1963a5fa-421a-43f8-a9c7-50fc2fe986a3",
  "title": "Exclusive: Scale AI’s Spam, Security Woes Plagued the Company While Serving Google",
  "link": "https://www.inc.com/sam-blum/exclusive-scale-ais-spam-security-woes-while-serving-google/91205895",
  "description": "How the startup that just scored a $14 billion investment from Meta struggled to contain ‘spammy behavior’ from unqualified contributors as it trained Gemini.",
  "author": "Sam Blum",
  "published": "Wed, 25 Jun 2025 17:21:22 -0400",
  "source": "https://www.inc.com/rss/",
  "categories": [
    "Technology"
  ],
  "byline": "Sam Blum",
  "length": 8778,
  "excerpt": "How the startup that just scored a $14 billion investment from Meta struggled to contain 'spammy behavior' from unqualified contributors as it trained Gemini.",
  "siteName": "Inc",
  "favicon": "https://www.inc.com/_public/icons/apple-icon.png",
  "text": "Earlier this month, after Meta invested $14 billion in Scale AI and hired its CEO, Alexandr Wang, speculation quickly surfaced that Scale could lose some of its marquee clients. As Reuters later reported, Google was the first company to sever its relationship with Scale, citing Meta’s now 49 percent ownership stake in the startup.  But that split was just the latest chapter in a long relationship between Google and Scale, which was fraught with lapses in security protocols for at least 11 months between March 2023 and April 2024, according to a trove of internal documents obtained by Inc.  Scale AI’s early efforts to train Google AI programs were meant to be exclusively staffed by experts in various specialties. Instead the program, called “Bulba Experts,” became flooded with “spam,” according to a series of daily action items maintained by project leads working for Scale subsidiary Remotasks. Scale’s client identities are publicly confidential, and known internally at the company by various aliases. “Bulba,” an apparent riff on the Pokémon name “Bulbasaur,” was the name for Google’s Bard AI program, which was later rebranded to Gemini.  The cache of logs reveals the pressure to serve a major client like Google in the heat of the post-ChatGPT AI boom. The documents showcase efforts made by team leaders to clamp down on “spammers,” who were working as independent contractors, otherwise called “contributors” within the company’s parlance. They often abused the system by submitting transparently shoddy work that managed to evade detection.  Variations of the word “spam” appear across 83 pages of the documents. One entry offers a definition of “spammy behavior” as “writing gibberish, writing incorrect information, GPT-generated thought processes.”  Often, the spammers were paid, because it was too difficult to ferret them all out due to their sheer numbers, former Scale contractors say.  The documents offer a window into the hiccups that occurred during Scale’s relationship with Google between 2023 and 2024. They spell out many concerns, including contributors flooding the platform from abroad and sometimes using ChatGPT to complete projects, ostensibly as a means of overcoming a language barrier when the project required a native command of English. Oftentimes, projects required advanced degrees that many participants did not have, former Scale contractors claim.  In a statement to Inc., Scale AI spokesperson Joe Osborne said: “This story is filled with so many inaccuracies, it’s hard to keep track. What these documents show, and what we explained to Inc ahead of publishing, is that we had clear safeguards in place to detect and remove spam before anything goes to customers.”  A Google spokesperson didn’t return a request for comment.  ‘It was a mess’ The documents, titled “Bulba Experts SSOT Overview,” are essentially a running agenda for team leaders running data-labeling projects. (SSOT is a common term in data science, meaning Single Source of Truth.)  Gemini, in addition to other prominent AI tools developed by OpenAI, Meta, and the U.S. Department of Defense couldn’t have been commercially deployed without Scale AI’s services.  The company, co-founded in 2016 by Wang and Lucy Guo, provides data-training for AI systems via legions of contract workers, who label images, tweak text and otherwise ensure AI tools are fit for market. The contract workers train raw models through the platforms Remotasks and Outlier AI, which are both owned by Scale AI. AI models need quality data to function properly.  Despite not having the qualifications and inputting data sometimes described as “gibberish” in the documents, many of the spammers were compensated. “People made so much money,” says a former Remotasks history contributor on the Bulba project. “They just hired everybody who could breathe.”  The episode raises the question of whether or not Google at one point had vital data muddied by workers who lacked the credentials required by the Bulba program. It also calls into question Scale AI’s security and vetting protocols.  “It was a mess. They had no authentication at the beginning,” says the former contributor.  Google used Remotasks to develop AI tools across a range of topics during the time period in question, the documents indicate. They include accounting, physics, biology, chemistry, human resources, history, computer science, economics, data science, finance, marketing, law and various other topics.  Contributors to these training programs were meant to have a sophisticated level of domain expertise, sometimes requiring an advanced degree in their respective areas. But according to conversations with former Remotasks contractors, security and vetting of Remotasks’ workforce was often nonexistent.  “There were people in my domain that had no experience, no concept of it,” says a former Queue Manager (QM) for Remotasks, who reviewed and approved the work of individual contributors and asked for anonymity to protect relationships. The labor pool’s lack of experience was a persistent problem for the 11 months covered by the documents, the former QM says. “I would do all-nighters looking for and removing” unqualified contributors, the former QM says.   The Bulba initiative was big, which meant unqualified contributors could swoop in and often avoid getting caught. And if they did get caught, they’d sometimes use a VPN to start the process over again.   Spammers “could get away with just totally submitting garbage and there weren’t enough people to track them down,” says the former QM. Often, the spamming would involve inputting gibberish, or using ChatGPT to help improve an answer. Sometimes, those contributors still managed to get paid, despite using a different AI tool developed by OpenAI, a leading competitor to Google.  “I would personally go through all of the people in my domain and check their tasks and make sure they weren’t doing this, but a lot of it slipped through their cracks. So, yeah, a lot of it was paid work,” says the former QM, who worked for Remotasks during the entirety of the period covered by the documents.  Another QM says the inundation of contributors was a result of poor communication with Scale AI’s Allocations team, which was responsible for assigning contributors to projects: “One morning I hopped online to check messages and I noticed that the Allocations Department had dumped 800 spammers into our team and they obliterated the queue spamming all of the tasks.”  Gibberish, ChatGPT and a marketplace for Remotasks accounts  Though the Bulba training programs called for fluency in American English, an influx of taskers from developing countries had descended onto the Remotasks platform by the summer of 2023, the documents indicate.   “There were no background checks whatsoever,” says the former QM. “For example, the clients would have requirements for people working on projects to have certain degrees. But there were no verification checks…Often it was people that weren’t native English speakers.”  It created a problem for those monitoring and approving the tasks. An entry from the logs on July 3, 2023 reads: “Spammers: Please let’s not allow anymore taskers from EG [Egypt], KE [Kenya], ZA [South Africa], VE [Venezuela], PK [Pakistan], and any other developing nations.” A document called “Experts Suspicious Activity Guidelines” advises supervisors to remove workers from Kenya, Egypt, Pakistan, and other African countries, and to vet entries using ZeroGPT, a tool that detects usage of ChatGPT.  The Suspicious Activity Guidelines also advise supervisors to watch out for accounts that stay active for inordinate amounts of time. “Abuse: if taskers are logging hours but not working or if taskers are logging 18+ hours in a given day.”  Another problem involved individual contributors selling their accounts. One entry from August 2023, is titled “Members of the Expert Community Selling their Accounts” and has a few links listed below that title, one of which is titled “Spam-finder doc, including instructions, process, and methodology.”  Contractors suspected that certain accounts were being used by multiple people for prolonged periods of time, according to a former QM, who noticed that some accounts would stay logged on to the platform for “eighteen hours straight … which doesn’t seem humanly feasible. The idea was that there may be multiple people using that account.”  Presently, there are still live Reddit threads and Facebook pages offering Remotasks and Outlier AI accounts for sale. The early-rate deadline for the 2025 Inc. Power Partner Awards is this Friday, June 27, at 11:59 p.m. PT. Apply now.",
  "image": "https://img-cdn.inc.com/image/upload/f_webp,q_auto,c_fit/vip/2025/06/scale-ai-alexandr-wang-inc-GettyImages-1160440757.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\u003cp\u003eEarlier this month, after \u003ca href=\"https://www.inc.com/sam-blum/alexandr-wang-scales-up-why-mark-zuckerberg-picked-a-founder-to-fuel-his-ai-ambitions/91202658\"\u003eMeta invested $14 billion in Scale AI\u003c/a\u003e and \u003ca href=\"https://www.inc.com/kit-eaton/heres-what-you-need-to-know-about-the-blockbuster-meta-scale-ai-deal/91202170\"\u003ehired its CEO, Alexandr Wang\u003c/a\u003e, speculation quickly surfaced that Scale could lose some of its marquee clients. As Reuters later reported, Google was the \u003ca href=\"https://www.reuters.com/business/google-scale-ais-largest-customer-plans-split-after-meta-deal-sources-say-2025-06-13/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003efirst company to sever its relationship with Scale\u003c/a\u003e, citing Meta’s now 49 percent ownership stake in the startup. \u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\n\n\n\n\u003cp\u003eBut that split was just the latest chapter in a long relationship between Google and Scale, which was fraught with lapses in security protocols for at least 11 months between March 2023 and April 2024, according to a trove of internal documents obtained by Inc. \u003c/p\u003e\n\n\n\n\u003cp\u003eScale \u003ca href=\"https://www.fastcompany.com/section/artificial-intelligence\" data-internallinksmanager029f6b8e52c=\"10\"\u003eAI\u003c/a\u003e’s early efforts to train Google AI programs were meant to be exclusively staffed by experts in various specialties. Instead the program, called “Bulba Experts,” became flooded with “spam,” according to a series of daily action items maintained by project leads working for Scale subsidiary Remotasks. Scale’s client identities are publicly confidential, and \u003ca href=\"https://www.inc.com/sam-blum/a-scale-ai-subsidiary-targeted-small-businesses-for-data-to-train-an-ai-entrepreneurs-threatened-legal-action-to-get-paid.html\"\u003eknown internally at the company by various aliases.\u003c/a\u003e “Bulba,” an apparent riff on the Pokémon name “Bulbasaur,” was the name for Google’s Bard AI program, \u003ca href=\"https://www.inc.com/sam-blum/mystery-client-behind-scale-ai-small-biz-program-revealed-as-google.html\"\u003ewhich was later rebranded to Gemini. \u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe cache of logs reveals the pressure to serve a major client like Google in the heat of the post-ChatGPT AI boom. The documents showcase efforts made by team leaders to clamp down on “spammers,” who were working as independent contractors, otherwise called “contributors” within the company’s parlance. They often abused the system by submitting transparently shoddy work that managed to evade detection. \u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\n\n\n\n\u003cp\u003eVariations of the word “spam” appear across 83 pages of the documents. One entry offers a definition of “spammy behavior” as “writing gibberish, writing incorrect information, GPT-generated thought processes.” \u003c/p\u003e\n\n\n\n\u003cp\u003eOften, the spammers were paid, because it was too difficult to ferret them all out due to their sheer numbers, former Scale contractors say. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe documents offer a window into the hiccups that occurred during Scale’s relationship with Google between 2023 and 2024. They spell out many concerns, including contributors flooding the platform from abroad and sometimes using ChatGPT to complete projects, ostensibly as a means of overcoming a language barrier when the project required a native command of English. Oftentimes, projects required advanced degrees that many participants did not have, former Scale contractors claim. \u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\n\n\n\n\u003cp\u003eIn a statement to Inc., Scale AI spokesperson Joe Osborne said: “This story is filled with so many inaccuracies, it’s hard to keep track. What these documents show, and what we explained to Inc ahead of publishing, is that we had clear safeguards in place to detect and remove spam before anything goes to customers.” \u003c/p\u003e\n\n\n\n\u003cp\u003eA Google spokesperson didn’t return a request for comment. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-it-was-a-mess\"\u003e‘It was a mess’\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe documents, titled “Bulba Experts SSOT Overview,” are essentially a running agenda for team leaders running data-labeling projects. (SSOT is a common term in data science, meaning Single Source of Truth.) \u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\n\n\n\n\u003cp\u003eGemini, in addition to other prominent AI tools developed by OpenAI, Meta, and the U.S. Department of Defense couldn’t have been commercially deployed without Scale AI’s services. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe company, co-founded in 2016 by Wang and \u003ca href=\"https://www.inc.com/sam-blum/lucy-guo-doesnt-shy-away-from-controversy-it-finds-her/91153376\"\u003eLucy Guo\u003c/a\u003e, provides data-training for AI systems via legions of contract workers, who label images, tweak text and otherwise ensure AI tools are fit for market. The contract workers train raw models through the platforms Remotasks and Outlier AI, which are both owned by Scale AI. AI models need quality data to function properly. \u003c/p\u003e\n\n\n\n\u003cp\u003eDespite not having the qualifications and inputting data sometimes described as “gibberish” in the documents, many of the spammers were compensated. “People made so much money,” says a former Remotasks history contributor on the Bulba project. “They just hired everybody who could breathe.” \u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\n\n\n\n\u003cp\u003eThe episode raises the question of whether or not Google at one point had vital data muddied by workers who lacked the credentials required by the Bulba program. It also calls into question Scale AI’s security and vetting protocols. \u003c/p\u003e\n\n\n\n\u003cp\u003e“It was a mess. They had no authentication at the beginning,” says the former contributor. \u003c/p\u003e\n\n\n\n\u003cp\u003eGoogle used Remotasks to develop AI tools across a range of topics during the time period in question, the documents indicate. They include accounting, physics, biology, chemistry, human resources, history, computer science, economics, data science, finance, \u003ca href=\"https://www.fastcompany.com/section/marketing\" data-internallinksmanager029f6b8e52c=\"7\"\u003emarketing\u003c/a\u003e, law and various other topics. \u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\n\n\n\n\u003cp\u003eContributors to these training programs were meant to have a sophisticated level of domain expertise, sometimes requiring an advanced degree in their respective areas. But according to conversations with former Remotasks contractors, security and vetting of Remotasks’ workforce was often nonexistent. \u003c/p\u003e\n\n\n\n\u003cp\u003e“There were people in my domain that had no experience, no concept of it,” says a former Queue Manager (QM) for Remotasks, who reviewed and approved the work of individual contributors and asked for anonymity to protect relationships. The labor pool’s lack of experience was a persistent problem for the 11 months covered by the documents, the former QM says. “I would do all-nighters looking for and removing” unqualified contributors, the former QM says.  \u003c/p\u003e\n\n\n\n\u003cp\u003eThe Bulba initiative was big, which meant unqualified contributors could swoop in and often avoid getting caught. And if they did get caught, they’d sometimes use a VPN to start the process over again.  \u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\n\n\n\n\u003cp\u003eSpammers “could get away with just totally submitting garbage and there weren’t enough people to track them down,” says the former QM. Often, the spamming would involve inputting gibberish, or using ChatGPT to help improve an answer. Sometimes, those contributors still managed to get paid, despite using a different AI tool developed by OpenAI, a leading competitor to Google. \u003c/p\u003e\n\n\n\n\u003cp\u003e“I would personally go through all of the people in my domain and check their tasks and make sure they weren’t doing this, but a lot of it slipped through their cracks. So, yeah, a lot of it was paid work,” says the former QM, who worked for Remotasks during the entirety of the period covered by the documents. \u003c/p\u003e\n\n\n\n\u003cp\u003eAnother QM says the inundation of contributors was a result of poor communication with Scale AI’s Allocations team, which was responsible for assigning contributors to projects: “One morning I hopped online to check messages and I noticed that the Allocations Department had dumped 800 spammers into our team and they obliterated the queue spamming all of the tasks.” \u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\n\n\n\n\u003ch2 id=\"h-gibberish-chatgpt-and-a-marketplace-for-remotasks-accounts-nbsp\"\u003eGibberish, ChatGPT and a marketplace for Remotasks accounts\u003cstrong\u003e \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThough the Bulba training programs called for fluency in American English, an influx of taskers from developing countries had descended onto the Remotasks platform by the summer of 2023, the documents indicate.  \u003c/p\u003e\n\n\n\n\u003cp\u003e“There were no background checks whatsoever,” says the former QM. “For example, the clients would have requirements for people working on projects to have certain degrees. But there were no verification checks…Often it was people that weren’t native English speakers.” \u003c/p\u003e\n\n\n\n\u003cp\u003eIt created a problem for those monitoring and approving the tasks. An entry from the logs on July 3, 2023 reads: “Spammers: Please let’s not allow anymore taskers from EG [Egypt], KE [Kenya], ZA [South Africa], VE [Venezuela], PK [Pakistan], and any other developing nations.”\u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\n\n\n\n\u003cp\u003eA document called “Experts Suspicious Activity Guidelines” advises supervisors to remove workers from Kenya, Egypt, Pakistan, and other African countries, and to vet entries using ZeroGPT, a tool that detects usage of ChatGPT. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe Suspicious Activity Guidelines also advise supervisors to watch out for accounts that stay active for inordinate amounts of time. “Abuse: if taskers are logging hours but not working or if taskers are logging 18+ hours in a given day.” \u003c/p\u003e\n\n\n\n\u003cp\u003eAnother problem involved individual contributors selling their accounts. One entry from August 2023, is titled “Members of the Expert Community Selling their Accounts” and has a few links listed below that title, one of which is titled “Spam-finder doc, including instructions, process, and methodology.” \u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\n\n\n\n\u003cp\u003eContractors suspected that certain accounts were being used by multiple people for prolonged periods of time, according to a former QM, who noticed that some accounts would stay logged on to the platform for “eighteen hours straight … which doesn’t seem humanly feasible. The idea was that there may be multiple people using that account.” \u003c/p\u003e\n\n\n\n\u003cp\u003ePresently, there are still live \u003ca href=\"https://www.reddit.com/r/remotaskonline/comments/15nsxjy/i_am_selling_remotask_verified_accounts_in_the_us/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eReddit threads\u003c/a\u003e and \u003ca href=\"https://www.facebook.com/groups/518817254305090\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eFacebook pages\u003c/a\u003e offering Remotasks and Outlier AI accounts for sale.\u003c/p\u003e\n\n\n\n\u003c/div\u003e\u003cdiv\u003e\u003cp\u003e\u003cem\u003eThe early-rate deadline for the 2025 \u003ca href=\"https://incpowerpartners.secure-platform.com/a\"\u003eInc. Power Partner Awards\u003c/a\u003e is this Friday, June 27, at 11:59 p.m. PT. \u003ca href=\"https://incpowerpartners.secure-platform.com/a\"\u003eApply now\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-06-25T21:21:22Z",
  "modifiedTime": "2025-06-26T01:06:40Z"
}
