{
  "id": "27e53bf1-4b83-4c71-a50e-1f15876671aa",
  "title": "Beyond transformers: Nvidia’s MambaVision aims to unlock faster, cheaper enterprise computer vision",
  "link": "https://venturebeat.com/ai/beyond-transformers-nvidias-mambavision-aims-to-unlock-faster-cheaper-enterprise-computer-vision/",
  "description": "Nvidia is updating its computer vision models with new versions of MambaVision that combine the best of Mamba and transformers to improve efficiency.",
  "author": "Sean Michael Kerner",
  "published": "Tue, 25 Mar 2025 22:35:25 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "category-/Computers \u0026 Electronics",
    "category-/Science",
    "computer vision",
    "enterprise ai",
    "Generative AI",
    "Hugging Face",
    "hybrid model",
    "Mamba",
    "MambaVision",
    "Nvidia",
    "SSM",
    "Structured State Space Models (SSM)",
    "Transformers"
  ],
  "byline": "Sean Michael Kerner",
  "length": 6821,
  "excerpt": "Nvidia is updating its computer vision models with new versions of MambaVision that combine the best of Mamba and transformers to improve efficiency.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "March 25, 2025 3:35 PM Credit: Image generated by VentureBeat with StableDiffusion 3.5 Large Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Transformer-based large language models (LLMs) are the foundation of the modern generative AI landscape. Transformers aren’t the only way to do gen AI, though. Over the course of the last year, Mamba, an approach that uses Structured State Space Models (SSM), has also picked up adoption as an alternative approach from multiple vendors, including AI21 and AI silicon giant Nvidia.  Nvidia first discussed the concept of Mamba-powered models in 2024 when it initially released the MambaVision research and some early models. This week, Nvidia is expanding on its initial effort with a series of updated MambaVision models available on Hugging Face. MambaVision, as the name implies, is a Mamba-based model family for computer vision and image recognition tasks. The promise of MambaVision for enterprise is that it could improve the efficiency and accuracy of vision operations, at potentially lower costs, thanks to lower computational requirements. What are SSMs and how do they compare to transformers? SSMs are a neural network architecture class that processes sequential data differently from traditional transformers.  While transformers use attention mechanisms to process all tokens in relation to each other, SSMs model sequence data as a continuous dynamic system. Mamba is a specific SSM implementation developed to address the limitations of earlier SSM models. It introduces selective state space modelling that dynamically adapts to input data and hardware-aware design for efficient GPU utilization. Mamba aims to provide comparable performance to transformers on many tasks while using fewer computational resources Nvidia using hybrid architecture with MambaVision to revolutionize Computer Vision Traditional Vision Transformers (ViT) have dominated high-performance computer vision for the last several years, but at significant computational cost. Pure Mamba-based approaches, while more efficient, have struggled to match Transformer performance on complex vision tasks requiring global context understanding. MambaVision bridges this gap by adopting a hybrid approach. Nvidia’s MambaVision is a hybrid model that strategically combines Mamba’s efficiency with the Transformer’s modelling power.  The architecture’s innovation lies in its redesigned Mamba formulation specifically engineered for visual feature modeling, augmented by strategic placement of self-attention blocks in the final layers to capture complex spatial dependencies. Unlike conventional vision models that rely exclusively on either attention mechanisms or convolutional approaches, MambaVision’s hierarchical architecture employs both paradigms simultaneously. The model processes visual information through sequential scan-based operations from Mamba while leveraging self-attention to model global context — effectively getting the best of both worlds. MambaVision now has 740 million parameters The new set of MambaVision models released on Hugging Face is available under the Nvidia Source Code License-NC, which is an open license. The initial variants of MambaVision released in 2024 include the T and T2 variants, which were trained on the ImageNet-1K library. The new models released this week include the L/L2 and L3 variants, which are scaled-up models. “Since the initial release, we’ve significantly enhanced MambaVision, scaling it up to an impressive 740 million parameters,” Ali Hatamizadeh, Senior Research Scientist at Nvidia wrote in a Hugging Face discussion post. “We’ve also expanded our training approach by utilizing the larger ImageNet-21K dataset and have introduced native support for higher resolutions, now handling images at 256 and 512 pixels compared to the original 224 pixels.” According to Nvidia, the improved scale in the new MambaVision models also improves performance. Independent AI consultant Alex Fazio explained to VentureBeat that the new MambaVision models’ training on larger datasets makes them much better at handling more diverse and complex tasks.  He noted that the new models include high-resolution variants perfect for detailed image analysis. Fazio said that the lineup has also expanded with advanced configurations offering more flexibility and scalability for different workloads. “In terms of benchmarks, the 2025 models are expected to outperform the 2024 ones because they generalize better across larger datasets and tasks, Fazio said.  Enterprise implications of MambaVision For enterprises building computer vision applications, MambaVision’s balance of performance and efficiency opens new possibilities Reduced inference costs: The improved throughput means lower GPU compute requirements for similar performance levels compared to Transformer-only models. Edge deployment potential: While still large, MambaVision’s architecture is more amenable to optimization for edge devices than pure Transformer approaches. Improved downstream task performance: The gains on complex tasks like object detection and segmentation translate directly to better performance for real-world applications like inventory management, quality control, and autonomous systems. Simplified deployment: NVIDIA has released MambaVision with Hugging Face integration, making implementation straightforward with just a few lines of code for both classification and feature extraction. What this means for enterprise AI strategy MambaVision represents an opportunity for enterprises to deploy more efficient computer vision systems that maintain high accuracy. The model’s strong performance means that it can potentially serve as a versatile foundation for multiple computer vision applications across industries. MambaVision is still somewhat of an early effort, but it does represent a glimpse into the future of computer vision models. MambaVision highlights how architectural innovation—not just scale—continues to drive meaningful improvements in AI capabilities. Understanding these architectural advances is becoming increasingly crucial for technical decision-makers to make informed AI deployment choices. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/03/faster_computer_vi_image_smk.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-03-25T22:35:25+00:00\" datetime=\"2025-03-25T22:35:25+00:00\"\u003eMarch 25, 2025 3:35 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"422\" src=\"https://venturebeat.com/wp-content/uploads/2025/03/faster_computer_vi_image_smk.jpg?w=750\" alt=\"Credit: Image generated by VentureBeat with StableDiffusion 3.5 Large\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: Image generated by VentureBeat with StableDiffusion 3.5 Large\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"https://venturebeat.com/business/why-transformers-offer-more-than-meets-the-eye/\" target=\"_blank\"\u003eTransformer-\u003c/a\u003ebased\u003c/span\u003e large language models (LLMs) are the foundation of the modern generative AI landscape.\u003c/p\u003e\n\n\n\n\u003cp\u003eTransformers aren’t the only way to do \u003cspan\u003egen AI, though. Over the course of the last year, Mamba, an approach that uses\u003ca href=\"https://venturebeat.com/ai/ai21-debuts-jamba-1-5-boosting-hybrid-ssm-transformer-model-to-enable-agentic-ai/\" target=\"_blank\"\u003e Structured State Space Models (SSM\u003c/a\u003e), has also picked up adoption as an alternative approach from multiple vendors,\u003c/span\u003e including \u003ca href=\"https://www.ai21.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAI21 \u003c/a\u003eand AI silicon giant \u003ca href=\"https://www.nvidia.com/en-us/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eNvidia.\u003c/a\u003e \u003c/p\u003e\n\n\n\n\u003cp\u003eNvidia first discussed the concept of Mamba-powered models in 2024 when it initially released the\u003ca href=\"https://arxiv.org/pdf/2407.08083\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e MambaVision research\u003c/a\u003e and some early models. This week, Nvidia is expanding on its initial effort with a series of updated MambaVision models available on\u003ca href=\"https://huggingface.co/nvidia/MambaVision-L3-256-21K\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e Hugging Face\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eMambaVision, as the name implies, is a Mamba-based model family for computer vision and image recognition tasks. The promise of MambaVision for enterprise is that it could improve the efficiency and accuracy of vision operations, at potentially lower costs, thanks to lower computational requirements.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-are-ssms-and-how-do-they-compare-to-transformers\"\u003eWhat are SSMs and how do they compare to transformers?\u003c/h2\u003e\n\n\n\n\u003cp\u003eSSMs are a neural network architecture class that processes sequential data differently from traditional transformers. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile transformers use attention mechanisms to process all tokens in relation to each other, SSMs model sequence data as a continuous dynamic system.\u003c/p\u003e\n\n\n\n\u003cp\u003eMamba is a specific SSM implementation developed to address the limitations of earlier SSM models. It introduces selective state space modelling that dynamically adapts to input data and hardware-aware design for efficient GPU utilization. Mamba aims to provide comparable performance to transformers on many tasks while using fewer computational resources\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-nvidia-using-hybrid-architecture-with-mambavision-to-revolutionize-computer-vision\"\u003eNvidia using hybrid architecture with MambaVision to revolutionize Computer Vision\u003c/h2\u003e\n\n\n\n\u003cp\u003eTraditional Vision Transformers (ViT) have dominated high-performance computer vision for the last several years, but at significant computational cost. Pure Mamba-based approaches, while more efficient, have struggled to match Transformer performance on complex vision tasks requiring global context understanding.\u003c/p\u003e\n\n\n\n\u003cp\u003eMambaVision bridges this gap by adopting a hybrid approach. Nvidia’s MambaVision is a hybrid model that strategically combines Mamba’s efficiency with the Transformer’s modelling power. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe architecture’s innovation lies in its redesigned Mamba formulation specifically engineered for visual feature modeling, augmented by strategic placement of self-attention blocks in the final layers to capture complex spatial dependencies.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnlike conventional vision models that rely exclusively on either attention mechanisms or convolutional approaches, MambaVision’s hierarchical architecture employs both paradigms simultaneously. The model processes visual information through sequential scan-based operations from Mamba while leveraging self-attention to model global context — effectively getting the best of both worlds.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-mambavision-now-has-740-million-parameters\"\u003eMambaVision now has 740 million parameters\u003c/h2\u003e\n\n\n\n\u003cp\u003e\u003cspan\u003eThe new set of MambaVision models released on\u003ca href=\"https://huggingface.co/nvidia/MambaVision-L3-256-21K\" target=\"_blank\"\u003e Huggi\u003c/a\u003e\u003c/span\u003eng Face is available under the Nvidia Source Code License-NC, which is an open license.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe initial variants of MambaVision released in 2024 include the T and T2 variants, which were trained on the ImageNet-1K library. The new models released this week include the L/L2 and L3 variants, which are scaled-up models.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Since the initial release, we’ve significantly enhanced MambaVision, scaling it up to an impressive 740 million parameters,” Ali Hatamizadeh, Senior Research Scientist at Nvidia wrote in a Hugging Face\u003ca href=\"https://huggingface.co/nvidia/MambaVision-L3-512-21K/discussions/1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e discussion post\u003c/a\u003e. “We’ve also expanded our training approach by utilizing the larger ImageNet-21K dataset and have introduced native support for higher resolutions, now handling images at 256 and 512 pixels compared to the original 224 pixels.”\u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Nvidia, the improved scale in the new MambaVision models also improves performance.\u003c/p\u003e\n\n\n\n\u003cp\u003eIndependent AI consultant\u003ca href=\"https://github.com/alexfazio\"\u003e Alex Fazio\u003c/a\u003e explained to VentureBeat that the new MambaVision models’ training on larger datasets makes them much better at handling more diverse and complex tasks. \u003c/p\u003e\n\n\n\n\u003cp\u003eHe noted that the new models include high-resolution variants perfect for detailed image analysis. Fazio said that the lineup has also expanded with advanced configurations offering more flexibility and scalability for different workloads.\u003c/p\u003e\n\n\n\n\u003cp\u003e“In terms of benchmarks, the 2025 models are expected to outperform the 2024 ones because they generalize better across larger datasets and tasks, Fazio said. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-enterprise-implications-of-mambavision\"\u003e\u003cstrong\u003eEnterprise implications of MambaVision\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eFor enterprises building computer vision applications, MambaVision’s balance of performance and efficiency opens new possibilities\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eReduced inference costs\u003c/strong\u003e: The improved throughput means lower GPU compute requirements for similar performance levels compared to Transformer-only models.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eEdge deployment potential\u003c/strong\u003e: While still large, MambaVision’s architecture is more amenable to optimization for edge devices than pure Transformer approaches.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eImproved downstream task performance\u003c/strong\u003e: The gains on complex tasks like object detection and segmentation translate directly to better performance for real-world applications like inventory management, quality control, and autonomous systems.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eSimplified deployment\u003c/strong\u003e: NVIDIA has released MambaVision with Hugging Face integration, making implementation straightforward with just a few lines of code for both classification and feature extraction.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-this-means-for-enterprise-ai-strategy\"\u003eWhat this means for enterprise AI strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eMambaVision represents an opportunity for enterprises to deploy more efficient computer vision systems that maintain high accuracy. The model’s strong performance means that it can potentially serve as a versatile foundation for multiple computer vision applications across industries.\u003c/p\u003e\n\n\n\n\u003cp\u003eMambaVision is still somewhat of an early effort, but it does represent a glimpse into the future of computer vision models.\u003c/p\u003e\n\n\n\n\u003cp\u003eMambaVision highlights how architectural innovation—not just scale—continues to drive meaningful improvements in AI capabilities. Understanding these architectural advances is becoming increasingly crucial for technical decision-makers to make informed AI deployment choices.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-03-25T22:35:25Z",
  "modifiedTime": "2025-03-25T22:35:32Z"
}
