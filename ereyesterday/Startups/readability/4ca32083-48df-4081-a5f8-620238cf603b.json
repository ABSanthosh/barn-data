{
  "id": "4ca32083-48df-4081-a5f8-620238cf603b",
  "title": "Synthetic data has its limits — why human-sourced data can help prevent AI model collapse",
  "link": "https://venturebeat.com/ai/synthetic-data-has-its-limits-why-human-sourced-data-can-help-prevent-ai-model-collapse/",
  "description": "With model degradation, AI development could stall, leaving AI systems unable to ingest new data and essentially becoming “stuck in time.”",
  "author": "Rick Song, Persona",
  "published": "Sat, 14 Dec 2024 20:05:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "AI, ML and Deep Learning",
    "category-/Science/Computer Science",
    "Conversational AI",
    "Generative AI",
    "large language models",
    "model collapse",
    "NLP",
    "Synthetic Data"
  ],
  "byline": "Rick Song, Persona",
  "length": 6696,
  "excerpt": "With model degradation, AI development could stall, leaving AI systems unable to ingest new data and essentially becoming “stuck in time.”",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 14, 2024 12:05 PM VentureBeat/Ideogram Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More My, how quickly the tables turn in the tech world. Just two years ago, AI was lauded as the “next transformational technology to rule them all.” Now, instead of reaching Skynet levels and taking over the world, AI is, ironically, degrading.  Once the harbinger of a new era of intelligence, AI is now tripping over its own code, struggling to live up to the brilliance it promised. But why exactly? The simple fact is that we’re starving AI of the one thing that makes it truly smart: human-generated data. To feed these data-hungry models, researchers and organizations have increasingly turned to synthetic data. While this practice has long been a staple in AI development, we’re now crossing into dangerous territory by over-relying on it, causing a gradual degradation of AI models. And this isn’t just a minor concern about ChatGPT producing sub-par results — the consequences are far more dangerous. When AI models are trained on outputs generated by previous iterations, they tend to propagate errors and introduce noise, leading to a decline in output quality. This recursive process turns the familiar cycle of “garbage in, garbage out” into a self-perpetuating problem, significantly reducing the effectiveness of the system. As AI drifts further from human-like understanding and accuracy, it not only undermines performance but also raises critical concerns about the long-term viability of relying on self-generated data for continued AI development. But this isn’t just a degradation of technology; it’s a degradation of reality, identity, and data authenticity — posing serious risks to humanity and society. The ripple effects could be profound, leading to a rise in critical errors. As these models lose accuracy and reliability, the consequences could be dire — think medical misdiagnosis, financial losses and even life-threatening accidents. Another major implication is that AI development could completely stall, leaving AI systems unable to ingest new data and essentially becoming “stuck in time.” This stagnation would not only hinder progress but also trap AI in a cycle of diminishing returns, with potentially catastrophic effects on technology and society. But, practically speaking, what can enterprises do to ensure the safety of their customers and users? Before we answer that question, we need to understand how this all works. When a model collapses, reliability goes out the window The more AI-generated content spreads online, the faster it will infiltrate datasets and, subsequently, the models themselves. And it’s happening at an accelerated rate, making it increasingly difficult for developers to filter out anything that is not pure, human-created training data. The fact is, using synthetic content in training can trigger a detrimental phenomenon known as “model collapse” or “model autophagy disorder (MAD).” Model collapse is the degenerative process in which AI systems progressively lose their grasp on the true underlying data distribution they’re meant to model. This often occurs when AI is trained recursively on content it generated, leading to a number of issues: Loss of nuance: Models begin to forget outlier data or less-represented information, crucial for a comprehensive understanding of any dataset. Reduced diversity: There is a noticeable decrease in the diversity and quality of the outputs produced by the models. Amplification of biases: Existing biases, particularly against marginalized groups, may be exacerbated as the model overlooks the nuanced data that could mitigate these biases. Generation of nonsensical outputs: Over time, models may start producing outputs that are completely unrelated or nonsensical. A case in point: A study published in Nature highlighted the rapid degeneration of language models trained recursively on AI-generated text. By the ninth iteration, these models were found to be producing entirely irrelevant and nonsensical content, demonstrating the rapid decline in data quality and model utility. Safeguarding AI’s future: Steps enterprises can take today Enterprise organizations are in a unique position to shape the future of AI responsibly, and there are clear, actionable steps they can take to keep AI systems accurate and trustworthy: Invest in data provenance tools: Tools that trace where each piece of data comes from and how it changes over time give companies confidence in their AI inputs. With clear visibility into data origins, organizations can avoid feeding models unreliable or biased information. Deploy AI-powered filters to detect synthetic content: Advanced filters can catch AI-generated or low-quality content before it slips into training datasets. These filters help ensure that models are learning from authentic, human-created information rather than synthetic data that lacks real-world complexity. Partner with trusted data providers: Strong relationships with vetted data providers give organizations a steady supply of authentic, high-quality data. This means AI models get real, nuanced information that reflects actual scenarios, which boosts both performance and relevance. Promote digital literacy and awareness: By educating teams and customers on the importance of data authenticity, organizations can help people recognize AI-generated content and understand the risks of synthetic data. Building awareness around responsible data use fosters a culture that values accuracy and integrity in AI development. The future of AI depends on responsible action. Enterprises have a real opportunity to keep AI grounded in accuracy and integrity. By choosing real, human-sourced data over shortcuts, prioritizing tools that catch and filter out low-quality content, and encouraging awareness around digital authenticity, organizations can set AI on a safer, smarter path. Let’s focus on building a future where AI is both powerful and genuinely beneficial to society. Rick Song is the CEO and co-founder of Persona. DataDecisionMakers Welcome to the VentureBeat community! DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation. If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers. You might even consider contributing an article of your own! Read More From DataDecisionMakers",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/a-photo-of-a-factory-floor-with-multiple_smWxvYaVQ0SXIgUYy4sa6g_wWO91VftSACaTLVLgS6JTA-transformed.jpeg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-14T20:05:00+00:00\" datetime=\"2024-12-14T20:05:00+00:00\"\u003eDecember 14, 2024 12:05 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"421\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/a-photo-of-a-factory-floor-with-multiple_smWxvYaVQ0SXIgUYy4sa6g_wWO91VftSACaTLVLgS6JTA-transformed.jpeg?w=750\" alt=\"VentureBeat/Ideogram\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eVentureBeat/Ideogram\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eMy, how quickly the tables turn in the tech world. Just two years ago, AI was lauded as the “next \u003ca href=\"https://venturebeat.com/ai/the-end-of-ai-scaling-may-not-be-nigh-heres-whats-next/\"\u003etransformational technology\u003c/a\u003e to rule them all.” Now, instead of reaching Skynet levels and taking over the world, AI is, ironically, degrading. \u003c/p\u003e\n\n\n\n\u003cp\u003eOnce the harbinger of a new era of intelligence, AI is now tripping over its own code, struggling to live up to the brilliance it promised. \u003cem\u003eBut why exactly?\u003c/em\u003e The simple fact is that we’re starving AI of the one thing that makes it truly smart: human-generated data.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo feed these data-hungry models, researchers and organizations have increasingly turned to synthetic data. While this practice has long been a staple in \u003ca href=\"https://venturebeat.com/ai/getting-started-with-ai-agents-part-1-capturing-processes-roles-and-connections/\"\u003eAI development\u003c/a\u003e, we’re now crossing into dangerous territory by over-relying on it, causing a gradual degradation of AI models. And this isn’t just a minor concern about \u003ca href=\"https://venturebeat.com/ai/chatgpts-second-birthday-what-will-gen-ai-and-the-world-look-like-in-another-2-years/\"\u003eChatGPT\u003c/a\u003e producing sub-par results — the consequences are far more dangerous.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen AI models are trained on outputs generated by previous iterations, they tend to propagate errors and introduce noise, leading to a decline in output quality. This recursive process turns the familiar cycle of “garbage in, garbage out” into a self-perpetuating problem, significantly reducing the effectiveness of the system. As AI drifts further from \u003ca href=\"https://venturebeat.com/ai/agi-is-coming-faster-than-we-think-we-must-get-ready-now/\"\u003ehuman-like understanding\u003c/a\u003e and accuracy, it not only undermines performance but also raises critical concerns about the long-term viability of relying on self-generated data for continued AI development.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut this isn’t just a degradation of technology; it’s a degradation of reality, identity, and data authenticity — posing serious risks to humanity and society. The ripple effects could be profound, leading to a rise in critical errors. As these models lose accuracy and reliability, the consequences could be dire — think medical misdiagnosis, financial losses and even life-threatening accidents.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnother major implication is that AI development could completely stall, leaving \u003ca href=\"https://venturebeat.com/ai/heres-the-one-thing-you-should-never-outsource-to-an-ai-model/\"\u003eAI systems\u003c/a\u003e unable to ingest new data and essentially becoming “stuck in time.” This stagnation would not only hinder progress but also trap AI in a cycle of diminishing returns, with potentially catastrophic effects on technology and society.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut, practically speaking, what can enterprises do to ensure the safety of their customers and users? Before we answer that question, we need to understand how this all works.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-when-a-model-collapses-reliability-goes-out-the-window\"\u003eWhen a model collapses, reliability goes out the window\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe more AI-generated content spreads online, the faster it will infiltrate datasets and, subsequently, the models themselves. And it’s happening at an accelerated rate, making it increasingly difficult for developers to filter out anything that is not pure, human-created training data. The fact is, using synthetic content in training can trigger a detrimental phenomenon known as “model collapse” or “\u003ca href=\"https://arxiv.org/abs/2307.01850\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emodel autophagy disorder\u003c/a\u003e (MAD).”\u003c/p\u003e\n\n\n\n\u003cp\u003eModel collapse is the degenerative process in which AI systems progressively lose their grasp on the true underlying data distribution they’re meant to model. This often occurs when AI is trained recursively on content it generated, leading to a number of issues:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLoss of nuance\u003c/strong\u003e: Models begin to forget outlier data or less-represented information, crucial for a comprehensive understanding of any dataset.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eReduced diversity\u003c/strong\u003e: There is a noticeable decrease in the diversity and quality of the outputs produced by the models.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eAmplification of biases\u003c/strong\u003e: Existing biases, particularly against marginalized groups, may be exacerbated as the model overlooks the nuanced data that could mitigate these biases.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eGeneration of nonsensical outputs\u003c/strong\u003e: Over time, models may start producing outputs that are completely unrelated or nonsensical.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eA case in point: A study published in \u003cem\u003eNature\u003c/em\u003e highlighted the rapid degeneration of language models trained recursively on AI-generated text. By the ninth iteration, these models were found to be producing entirely irrelevant and nonsensical content, demonstrating the rapid decline in data quality and model utility.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-safeguarding-ai-s-future-steps-enterprises-can-take-today\"\u003eSafeguarding AI’s future: Steps enterprises can take today\u003c/h2\u003e\n\n\n\n\u003cp\u003eEnterprise organizations are in a unique position to shape the future of AI responsibly, and there are clear, actionable steps they can take to keep AI systems accurate and trustworthy:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eInvest in data provenance tools\u003c/strong\u003e: Tools that trace where each piece of data comes from and how it changes over time give companies confidence in their AI inputs. With clear visibility into data origins, organizations can avoid feeding models unreliable or biased information.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eDeploy AI-powered filters to detect synthetic content: \u003c/strong\u003eAdvanced filters can catch \u003ca href=\"https://venturebeat.com/ai/our-brains-are-vector-databases-heres-why-thats-helpful-when-using-ai/\"\u003eAI-generated\u003c/a\u003e or low-quality content before it slips into training datasets. These filters help ensure that models are learning from authentic, human-created information rather than synthetic data that lacks real-world complexity.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003ePartner with trusted data providers: \u003c/strong\u003eStrong relationships with vetted data providers give organizations a steady supply of authentic, high-quality data. This means AI models get real, nuanced information that reflects actual scenarios, which boosts both performance and relevance.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003ePromote digital literacy and awareness\u003c/strong\u003e: By educating teams and customers on the importance of data authenticity, organizations can help people recognize AI-generated content and understand the risks of synthetic data. Building awareness around responsible data use fosters a culture that values accuracy and integrity in AI development.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eThe future of AI depends on responsible action. Enterprises have a real opportunity to keep AI grounded in accuracy and integrity. By choosing real, human-sourced data over shortcuts, prioritizing tools that catch and filter out low-quality content, and encouraging awareness around digital authenticity, organizations can set AI on a safer, smarter path. Let’s focus on building a future where AI is both powerful and genuinely beneficial to society.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eRick Song is the CEO and co-founder of \u003ca href=\"https://withpersona.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePersona\u003c/a\u003e. \u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2736392\"\u003e\n\u003cp\u003e\u003cstrong\u003eDataDecisionMakers\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWelcome to the VentureBeat community!\u003c/p\u003e\n\n\n\n\u003cp\u003eDataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might even consider \u003ca rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://venturebeat.com/guest-posts/\"\u003econtributing an article\u003c/a\u003e of your own!\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca rel=\"noreferrer noopener\" href=\"https://venturebeat.com/category/DataDecisionMakers/\" target=\"_blank\"\u003eRead More From DataDecisionMakers\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-12-14T20:05:00Z",
  "modifiedTime": "2024-12-14T18:30:24Z"
}
