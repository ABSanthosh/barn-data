{
  "id": "24d7b4f8-6b87-45fc-96f4-74deefada788",
  "title": "QwenLong-L1 solves long-context reasoning challenge that stumps current LLMs",
  "link": "https://venturebeat.com/ai/qwenlong-l1-solves-long-context-reasoning-challenge-that-stumps-current-llms/",
  "description": "Alibaba's QwenLong-L1 helps LLMs deeply understand long documents, unlocking advanced reasoning for practical enterprise applications.",
  "author": "Ben Dickson",
  "published": "Fri, 30 May 2025 23:39:01 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI Reasoning",
    "AI, ML and Deep Learning",
    "alibaba qwen",
    "large language models",
    "Large Reasoning Models (LRMs)",
    "LLM reasoning",
    "LLMs",
    "Qwen",
    "reasoning llms",
    "reasoning models",
    "Supervised fine-tuning (SFT)"
  ],
  "byline": "Ben Dickson",
  "length": 6880,
  "excerpt": "Alibaba's QwenLong-L1 helps LLMs deeply understand long documents, unlocking advanced reasoning for practical enterprise applications.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "May 30, 2025 4:39 PM Image credit: VentureBeat with Ideogram Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Alibaba Group has introduced QwenLong-L1, a new framework that enables large language models (LLMs) to reason over extremely long inputs. This development could unlock a new wave of enterprise applications that require models to understand and draw insights from extensive documents such as detailed corporate filings, lengthy financial statements, or complex legal contracts. The challenge of long-form reasoning for AI Recent advances in large reasoning models (LRMs), particularly through reinforcement learning (RL), have significantly improved their problem-solving capabilities. Research shows that when trained with RL fine-tuning, LRMs acquire skills similar to human “slow thinking,” where they develop sophisticated strategies to tackle complex tasks. However, these improvements are primarily seen when models work with relatively short pieces of text, typically around 4,000 tokens. The ability of these models to scale their reasoning to much longer contexts (e.g., 120,000 tokens) remains a major challenge. Such long-form reasoning requires a robust understanding of the entire context and the ability to perform multi-step analysis. “This limitation poses a significant barrier to practical applications requiring interaction with external knowledge, such as deep research, where LRMs must collect and process information from knowledge-intensive environments,” the developers of QwenLong-L1 write in their paper. The researchers formalize these challenges into the concept of “long-context reasoning RL.” Unlike short-context reasoning, which often relies on knowledge already stored within the model, long-context reasoning RL requires models to retrieve and ground relevant information from lengthy inputs accurately. Only then can they generate chains of reasoning based on this incorporated information.  Training models for this through RL is tricky and often results in inefficient learning and unstable optimization processes. Models struggle to converge on good solutions or lose their ability to explore diverse reasoning paths. QwenLong-L1: A multi-stage approach QwenLong-L1 is a reinforcement learning framework designed to help LRMs transition from proficiency with short texts to robust generalization across long contexts. The framework enhances existing short-context LRMs through a carefully structured, multi-stage process: Warm-up Supervised Fine-Tuning (SFT): The model first undergoes an SFT phase, where it is trained on examples of long-context reasoning. This stage establishes a solid foundation, enabling the model to ground information accurately from long inputs. It helps develop fundamental capabilities in understanding context, generating logical reasoning chains, and extracting answers. Curriculum-Guided Phased RL: At this stage, the model is trained through multiple phases, with the target length of the input documents gradually increasing. This systematic, step-by-step approach helps the model stably adapt its reasoning strategies from shorter to progressively longer contexts. It avoids the instability often seen when models are abruptly trained on very long texts. Difficulty-Aware Retrospective Sampling: The final training stage incorporates challenging examples from the preceding training phases, ensuring the model continues to learn from the hardest problems. This prioritizes difficult instances and encourages the model to explore more diverse and complex reasoning paths. QwenLong-L1 process Source: arXiv Beyond this structured training, QwenLong-L1 also uses a distinct reward system. While training for short-context reasoning tasks often relies on strict rule-based rewards (e.g., a correct answer in a math problem), QwenLong-L1 employs a hybrid reward mechanism. This combines rule-based verification, which ensures precision by checking for strict adherence to correctness criteria, with an “LLM-as-a-judge.” This judge model compares the semanticity of the generated answer with the ground truth, allowing for more flexibility and better handling of the diverse ways correct answers can be expressed when dealing with long, nuanced documents. Putting QwenLong-L1 to the test The Alibaba team evaluated QwenLong-L1 using document question-answering (DocQA) as the primary task. This scenario is highly relevant to enterprise needs, where AI must understand dense documents to answer complex questions.  Experimental results across seven long-context DocQA benchmarks showed QwenLong-L1’s capabilities. Notably, the QWENLONG-L1-32B model (based on DeepSeek-R1-Distill-Qwen-32B) achieved performance comparable to Anthropic’s Claude-3.7 Sonnet Thinking, and outperformed models like OpenAI’s o3-mini and Qwen3-235B-A22B. The smaller QWENLONG-L1-14B model also outperformed Google’s Gemini 2.0 Flash Thinking and Qwen3-32B.  Source: arXiv An important finding relevant to real-world applications is how RL training results in the model developing specialized long-context reasoning behaviors. The paper notes that models trained with QwenLong-L1 become better at “grounding” (linking answers to specific parts of a document), “subgoal setting” (breaking down complex questions), “backtracking” (recognizing and correcting their own mistakes mid-reasoning), and “verification” (double-checking their answers). For instance, while a base model might get sidetracked by irrelevant details in a financial document or get stuck in a loop of over-analyzing unrelated information, the QwenLong-L1 trained model demonstrated an ability to engage in effective self-reflection. It could successfully filter out these distractor details, backtrack from incorrect paths, and arrive at the correct answer. Techniques like QwenLong-L1 could significantly expand the utility of AI in the enterprise. Potential applications include legal tech (analyzing thousands of pages of legal documents), finance (deep research on annual reports and financial filings for risk assessment or investment opportunities) and customer service (analyzing long customer interaction histories to provide more informed support). The researchers have released the code for the QwenLong-L1 recipe and the weights for the trained models. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/05/Robot-reading-script.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-05-30T23:39:01+00:00\" datetime=\"2025-05-30T23:39:01+00:00\"\u003eMay 30, 2025 4:39 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"421\" src=\"https://venturebeat.com/wp-content/uploads/2025/05/Robot-reading-script.webp?w=750\" alt=\"Image credit: VentureBeat with Ideogram\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with Ideogram\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://www.alibabagroup.com/en-US/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAlibaba Group\u003c/a\u003e has introduced \u003ca href=\"https://github.com/Tongyi-Zhiwen/QwenLong-L1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eQwenLong-L1\u003c/a\u003e, a new framework that enables large language models (LLMs) to reason over extremely long inputs. This development could unlock a new wave of enterprise applications that require models to understand and draw insights from extensive documents such as detailed corporate filings, lengthy financial statements, or complex legal contracts.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-challenge-of-long-form-reasoning-for-ai\"\u003eThe challenge of long-form reasoning for AI\u003c/h2\u003e\n\n\n\n\u003cp\u003eRecent advances in large reasoning models (LRMs), particularly through \u003ca href=\"https://venturebeat.com/ai/deepseek-r1s-bold-bet-on-reinforcement-learning-how-it-outpaced-openai-at-3-of-the-cost/\"\u003ereinforcement learning\u003c/a\u003e (RL), have significantly improved their problem-solving capabilities. Research shows that when trained with RL fine-tuning, LRMs acquire skills similar to human “\u003ca href=\"https://venturebeat.com/ai/deepminds-talker-reasoner-framework-brings-system-2-thinking-to-ai-agents/\"\u003eslow thinking\u003c/a\u003e,” where they develop sophisticated strategies to tackle complex tasks.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, these improvements are primarily seen when models work with relatively short pieces of text, typically around 4,000 tokens. The ability of these models to scale their reasoning to much longer contexts (e.g., 120,000 tokens) remains a major challenge. Such long-form reasoning requires a robust understanding of the entire context and the ability to perform multi-step analysis. “This limitation poses a significant barrier to practical applications requiring interaction with external knowledge, such as deep research, where LRMs must collect and process information from knowledge-intensive environments,” the developers of QwenLong-L1 write in their \u003ca href=\"https://arxiv.org/abs/2505.17667\"\u003epaper\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers formalize these challenges into the concept of “long-context reasoning RL.” Unlike short-context reasoning, which often relies on knowledge already stored within the model, long-context reasoning RL requires models to retrieve and ground relevant information from lengthy inputs accurately. Only then can they generate chains of reasoning based on this incorporated information. \u003c/p\u003e\n\n\n\n\u003cp\u003eTraining models for this through RL is tricky and often results in inefficient learning and unstable optimization processes. Models struggle to converge on good solutions or lose their ability to explore diverse reasoning paths.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-qwenlong-l1-a-multi-stage-approach\"\u003eQwenLong-L1: A multi-stage approach\u003c/h2\u003e\n\n\n\n\u003cp\u003eQwenLong-L1 is a reinforcement learning framework designed to help LRMs transition from proficiency with short texts to robust generalization across long contexts. The framework enhances existing short-context LRMs through a carefully structured, multi-stage process:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eWarm-up Supervised Fine-Tuning (SFT): \u003c/strong\u003eThe model first undergoes an SFT phase, where it is trained on examples of long-context reasoning. This stage establishes a solid foundation, enabling the model to ground information accurately from long inputs. It helps develop fundamental capabilities in understanding context, generating logical reasoning chains, and extracting answers.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCurriculum-Guided Phased RL: \u003c/strong\u003eAt this stage, the model is trained through multiple phases, with the target length of the input documents gradually increasing. This systematic, step-by-step approach helps the model stably adapt its reasoning strategies from shorter to progressively longer contexts. It avoids the instability often seen when models are abruptly trained on very long texts.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eDifficulty-Aware Retrospective Sampling: \u003c/strong\u003eThe final training stage incorporates challenging examples from the preceding training phases, ensuring the model continues to learn from the hardest problems. This prioritizes difficult instances and encourages the model to explore more diverse and complex reasoning paths.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1228\" height=\"274\" src=\"https://venturebeat.com/wp-content/uploads/2025/05/image_a1cc38.png?w=800\" alt=\"QwenLong-L1 process (source: arXiv)\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/05/image_a1cc38.png 1228w, https://venturebeat.com/wp-content/uploads/2025/05/image_a1cc38.png?resize=300,67 300w, https://venturebeat.com/wp-content/uploads/2025/05/image_a1cc38.png?resize=768,171 768w, https://venturebeat.com/wp-content/uploads/2025/05/image_a1cc38.png?resize=800,179 800w, https://venturebeat.com/wp-content/uploads/2025/05/image_a1cc38.png?resize=400,89 400w, https://venturebeat.com/wp-content/uploads/2025/05/image_a1cc38.png?resize=750,167 750w, https://venturebeat.com/wp-content/uploads/2025/05/image_a1cc38.png?resize=578,129 578w, https://venturebeat.com/wp-content/uploads/2025/05/image_a1cc38.png?resize=930,208 930w\" sizes=\"(max-width: 1228px) 100vw, 1228px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eQwenLong-L1 process Source: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eBeyond this structured training, QwenLong-L1 also uses a distinct reward system. While training for short-context reasoning tasks often relies on strict rule-based rewards (e.g., a correct answer in a math problem), QwenLong-L1 employs a hybrid reward mechanism. This combines rule-based verification, which ensures precision by checking for strict adherence to correctness criteria, with an “\u003ca href=\"https://cameronrwolfe.substack.com/p/finetuned-judge\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLLM-as-a-judge\u003c/a\u003e.” This judge model compares the semanticity of the generated answer with the ground truth, allowing for more flexibility and better handling of the diverse ways correct answers can be expressed when dealing with long, nuanced documents.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-putting-qwenlong-l1-to-the-test\"\u003ePutting QwenLong-L1 to the test\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Alibaba team evaluated QwenLong-L1 using document question-answering (DocQA) as the primary task. This scenario is highly relevant to enterprise needs, where AI must understand dense documents to answer complex questions. \u003c/p\u003e\n\n\n\n\u003cp\u003eExperimental results across seven long-context DocQA benchmarks showed QwenLong-L1’s capabilities. Notably, the QWENLONG-L1-32B model (based on \u003ca href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDeepSeek-R1-Distill-Qwen-32B\u003c/a\u003e) achieved performance comparable to Anthropic’s \u003ca href=\"https://venturebeat.com/ai/anthropics-claude-3-7-sonnet-takes-aim-at-openai-and-deepseek-in-ais-next-big-battle/\"\u003eClaude-3.7 Sonnet Thinking\u003c/a\u003e, and outperformed models like OpenAI’s \u003ca href=\"https://venturebeat.com/ai/its-here-openais-o3-mini-advanced-reasoning-model-arrives-to-counter-deepseeks-rise/\"\u003eo3-mini\u003c/a\u003e and Qwen3-235B-A22B. The smaller QWENLONG-L1-14B model also outperformed Google’s \u003ca href=\"https://venturebeat.com/ai/gemini-2-0-flash-thinking-now-has-memory-and-google-apps-integration/\"\u003eGemini 2.0 Flash Thinking\u003c/a\u003e and Qwen3-32B. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1296\" height=\"454\" src=\"https://venturebeat.com/wp-content/uploads/2025/05/image_45bf10.png?w=800\" alt=\"Source: arXiv\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/05/image_45bf10.png 1296w, https://venturebeat.com/wp-content/uploads/2025/05/image_45bf10.png?resize=300,105 300w, https://venturebeat.com/wp-content/uploads/2025/05/image_45bf10.png?resize=768,269 768w, https://venturebeat.com/wp-content/uploads/2025/05/image_45bf10.png?resize=800,280 800w, https://venturebeat.com/wp-content/uploads/2025/05/image_45bf10.png?resize=400,140 400w, https://venturebeat.com/wp-content/uploads/2025/05/image_45bf10.png?resize=750,263 750w, https://venturebeat.com/wp-content/uploads/2025/05/image_45bf10.png?resize=578,202 578w, https://venturebeat.com/wp-content/uploads/2025/05/image_45bf10.png?resize=930,326 930w\" sizes=\"(max-width: 1296px) 100vw, 1296px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eSource: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAn important finding relevant to real-world applications is how RL training results in the model developing specialized long-context reasoning behaviors. The paper notes that models trained with QwenLong-L1 become better at “grounding” (linking answers to specific parts of a document), “subgoal setting” (breaking down complex questions), “backtracking” (recognizing and correcting their own mistakes mid-reasoning), and “verification” (double-checking their answers).\u003c/p\u003e\n\n\n\n\u003cp\u003eFor instance, while a base model might get sidetracked by irrelevant details in a financial document or get stuck in a loop of over-analyzing unrelated information, the QwenLong-L1 trained model demonstrated an ability to engage in effective self-reflection. It could successfully filter out these distractor details, backtrack from incorrect paths, and arrive at the correct answer.\u003c/p\u003e\n\n\n\n\u003cp\u003eTechniques like QwenLong-L1 could significantly expand the utility of AI in the enterprise. Potential applications include legal tech (analyzing thousands of pages of legal documents), finance (deep research on annual reports and financial filings for risk assessment or investment opportunities) and customer service (analyzing long customer interaction histories to provide more informed support). The researchers have released the \u003ca href=\"https://github.com/Tongyi-Zhiwen/QwenLong-L1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecode for the QwenLong-L1 recipe\u003c/a\u003e and the \u003ca href=\"https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eweights for the trained models\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-05-30T23:39:01Z",
  "modifiedTime": "2025-05-30T23:39:12Z"
}
