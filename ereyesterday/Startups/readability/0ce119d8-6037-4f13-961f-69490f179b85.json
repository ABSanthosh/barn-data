{
  "id": "0ce119d8-6037-4f13-961f-69490f179b85",
  "title": "‘This is a game changer’: Runway releases new AI facial expression motion capture feature Act-One",
  "link": "https://venturebeat.com/ai/this-is-a-game-changer-runway-releases-new-ai-facial-expression-motion-capture-feature-act-one/",
  "description": "The new feature includes safeguards to detect and block attempts to generate content featuring public figures without authorization.",
  "author": "Carl Franzen",
  "published": "Tue, 22 Oct 2024 21:35:17 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI video",
    "AI video creation",
    "ai video generation",
    "AI video platform",
    "AI, ML and Deep Learning",
    "Conversational AI",
    "gen-3",
    "gen-3 alpha",
    "gen-3 alpha turbo",
    "NLP",
    "runway",
    "Runway AI model",
    "runway gen-3 alpha",
    "RunwayML",
    "video AI"
  ],
  "byline": "Carl Franzen",
  "length": 8227,
  "excerpt": "The new feature includes safeguards to detect and block attempts to generate content featuring public figures without authorization.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More AI video has come incredibly far in the years since the first models debuted in late 2022, increasing in realism, resolution, fidelity, prompt adherence (how well they match the text prompt or description of the video that the user typed) and number. But one area that remains a limitation to many AI video creators — myself included — is in depicting realistic facial expressions in AI generated characters. Most appear quite limited and difficult to control. But no longer: today, Runway, the New York City-headquartered AI startup backed by Google and others, announced a new feature “Act-One,” that allows users to record video of themselves or actors from any video camera — even the one on a smartphone — and then transfers the subject’s facial expressions to that of an AI generated character with uncanny accuracy. The free-to-use tool is gradually rolling out “gradually” to users starting today, according to Runway’s blog post on the feature. While anyone with a Runway account can access it, it will be limited to those who have enough credits to generate new videos on the company’s Gen-3 Alpha video generation model introduced earlier this year, which supports text-to-video, image-to-video, and video-to-video AI creation pipelines (e.g. the user can type in a scene description, upload an image or a video, or use a combination of these inputs and Gen-3 Alpha will use what its given to guide its generation of a new scene). Despite limited availability right now at the time of this posting, the burgeoning scene of AI video creators online is already applauding the new feature. As Allen T. remarked on his X account “This is a game changer!” It also comes on the heels of Runway’s move into Hollywood film production last month, when it announced it had inked a deal with Lionsgate, the studio behind the John Wick and Hunger Games movie franchises, to create a custom AI video generation model based on the studio’s catalog of more than 20,000 titles. Simplifying a traditionally complex and equipment-heavy creative proccess Traditionally, facial animation requires extensive and often cumbersome processes, including motion capture equipment, manual face rigging, and multiple reference footages. Anyone interested in filmmaking has likely caught sight of some of the intricacy and difficulty of this process to date on set or when viewing behind the scenes footage of effects-heavy and motion-capture films such as The Lord of the Rings series, Avatar, or Rise of the Planet of the Apes, wherein actors are seen covered in ping pong ball markers and their faces dotted with marker and blocked by head-mounted apparatuses. Accurately modeling intricate facial expressions is what led David Fincher and his production team on The Curious Case of Benjamin Button to develop whole new 3D modeling processes and ultimately won them an Academy Award, as reported in a prior VentureBeat report. Yet in the last few years, new software and AI-based startups such as Move have sought to reduce the equipment necessary to perform accurate motion capture — though that company in particular has concentrated primarily on full-body, more broad movements, whereas Runway’s Act-One is focused more on modeling facial expressions. With Act-One, Runway aims to make this complex process far more accessible. The new tool allows creators to animate characters in a variety of styles and designs, without the need for motion-capture gear or character rigging. Instead, users can rely on a simple driving video to transpose performances—including eye-lines, micro-expressions, and nuanced pacing—onto a generated character, or even multiple characters in different styles. As Runway wrote on its X account: “Act-One is able to translate the performance from a single input video across countless different character designs and in many different styles.” The feature is focused “mostly” on the face “for now,” according to Cristóbal Valenzuela, co-founder and CEO of Runway, who responded to VentureBeat’s questions via direct message on X. Runway’s approach offers significant advantages for animators, game developers, and filmmakers alike. The model accurately captures the depth of an actor’s performance while remaining versatile across different character designs and proportions. This opens up exciting possibilities for creating unique characters that express genuine emotion and personality. Cinematic realism across camera angles One of Act-One’s key strengths lies in its ability to deliver cinematic-quality, realistic outputs from various camera angles and focal lengths. This flexibility enhances creators’ ability to tell emotionally resonant stories through character performances that were previously hard to achieve without expensive equipment and multi-step workflows. The tool’s ability to faithfully capture the emotional depth and performance style of an actor, even in complex scenes. This shift allows creators to bring their characters to life in new ways, unlocking the potential for richer storytelling across both live-action and animated formats. While Runway previously supported video-to-video AI conversion as previously mentioned in this piece, which did allow users to upload footage of themselves and have Gen-3 Alpha or other prior Runway AI video models such as Gen-2 “reskin” them with AI effects, the new Act-One feature is optimized for facial mapping and effects. As Valenzuela told VentureBeat via DM on X: “The consistency and performance is unmatched with Act-One.” Enabling more expansive video storytelling A single actor, using only a consumer-grade camera, can now perform multiple characters, with the model generating distinct outputs for each. This capability is poised to transform narrative content creation, particularly in indie film production and digital media, where high-end production resources are often limited. In a public post on X, Valenzuela noted a shift in how the industry approaches generative models. “We are now beyond the threshold of asking ourselves if generative models can generate consistent videos. A good model is now the new baseline. The difference lies in what you do with the model—how you think about its applications and use cases, and what you ultimately build,” Valenzuela wrote. Safety and protection for public figure impersonations As with all of Runway’s releases, Act-One comes equipped with a comprehensive suite of safety measures. These include safeguards to detect and block attempts to generate content featuring public figures without authorization, as well as technical tools to verify voice usage rights. Continuous monitoring also ensures that the platform is used responsibly, preventing potential misuse of the tool. Runway’s commitment to ethical development aligns with its broader mission to expand creative possibilities while maintaining a strong focus on safety and content moderation. Looking ahead As Act-One gradually rolls out, Runway is eager to see how artists, filmmakers, and other creators will harness this new tool to bring their ideas to life. With Act -ne, complex animation techniques are now within reach for a broader audience of creators, enabling more people to explore new forms of storytelling and artistic expression. By reducing the technical barriers traditionally associated with character animation, the company hopes to inspire new levels of creativity across the digital media landscape. It also helps Runway stand out and differentiate its AI video creation platform against the likes of an increasing swath of competitors, including Luma AI from the U.S. and Hailuo and Kling from China, as well as open source rivals such as Genmo’s Mochi 1, which also just debuted today. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/10/runway-face-act-one_27c272.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eAI video has come incredibly far in the years since the \u003ca href=\"https://aibusiness.com/nlp/ai-video-generation-the-supreme-list\"\u003efirst models debuted in late 2022\u003c/a\u003e, increasing in realism, resolution, fidelity, prompt adherence (how well they match the text prompt or description of the video that the user typed) and number.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut one area that remains a limitation to many AI video creators — \u003ca href=\"https://x.com/carlfranzen/status/1838392392853914062\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emyself included\u003c/a\u003e — is in depicting realistic facial expressions in AI generated characters. Most appear quite limited and difficult to control.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut no longer: today, \u003ca href=\"https://runwayml.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRunway\u003c/a\u003e, the New York City-headquartered \u003ca href=\"https://venturebeat.com/ai/runway-draws-fresh-141-million-as-next-level-generative-ai-video-begins-to-emerge/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAI startup backed by Google\u003c/a\u003e and others,\u003ca href=\"https://runwayml.com/research/introducing-act-one\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e announced a new feature “Act-One,”\u003c/a\u003e that allows users to record video of themselves or actors from any video camera — even the one on a \u003ca href=\"https://x.com/runwayml/status/1848785910248218880\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esmartphone\u003c/a\u003e — and then transfers the subject’s facial expressions to that of an AI generated character with uncanny accuracy.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1920\" height=\"1080\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/image_ce090c.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/10/image_ce090c.png 1920w, https://venturebeat.com/wp-content/uploads/2024/10/image_ce090c.png?resize=300,169 300w, https://venturebeat.com/wp-content/uploads/2024/10/image_ce090c.png?resize=768,432 768w, https://venturebeat.com/wp-content/uploads/2024/10/image_ce090c.png?resize=800,450 800w, https://venturebeat.com/wp-content/uploads/2024/10/image_ce090c.png?resize=1536,864 1536w, https://venturebeat.com/wp-content/uploads/2024/10/image_ce090c.png?resize=400,225 400w, https://venturebeat.com/wp-content/uploads/2024/10/image_ce090c.png?resize=750,422 750w, https://venturebeat.com/wp-content/uploads/2024/10/image_ce090c.png?resize=578,325 578w, https://venturebeat.com/wp-content/uploads/2024/10/image_ce090c.png?resize=930,523 930w\" sizes=\"(max-width: 1920px) 100vw, 1920px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe free-to-use tool is gradually rolling out “gradually” to users starting today, according to \u003ca href=\"https://runwayml.com/research/introducing-act-one\"\u003eRunway’s blog post on the feature\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile anyone with a Runway account can access it, it will be limited to those who have enough credits to generate new videos on the company’s \u003ca href=\"https://venturebeat.com/ai/runways-gen-3-alpha-ai-video-model-now-available-but-theres-a-catch/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGen-3 Alpha video generation model \u003c/a\u003eintroduced earlier this year, which supports text-to-video, image-to-video, and video-to-video AI creation pipelines (e.g. the user can type in a scene description, upload an image or a video, or use a combination of these inputs and Gen-3 Alpha will use what its given to guide its generation of a new scene). \u003c/p\u003e\n\n\n\n\u003cp\u003eDespite limited availability right now at the time of this posting, the burgeoning scene of AI video creators online is already applauding the new feature.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs \u003ca href=\"https://x.com/Mr_AllenT/status/1848794034942448004\"\u003eAllen T. \u003c/a\u003e remarked on his X account “This is a game changer!”\u003c/p\u003e\n\n\n\n\u003cp\u003eIt also comes on the heels of \u003ca href=\"https://venturebeat.com/ai/runway-inks-deal-with-lionsgate-in-first-team-up-for-ai-provider-and-major-movie-studio/\"\u003eRunway’s move into Hollywood film production\u003c/a\u003e last month, when it announced it had inked a deal with Lionsgate, the studio behind the \u003cem\u003eJohn Wick\u003c/em\u003e and \u003cem\u003eHunger Games\u003c/em\u003e movie franchises, to create a custom AI video generation model based on the studio’s catalog of more than 20,000 titles. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-simplifying-a-traditionally-complex-and-equipment-heavy-creative-proccess\"\u003eSimplifying a traditionally complex and equipment-heavy creative proccess\u003c/h2\u003e\n\n\n\n\u003cp\u003eTraditionally, facial animation requires extensive and often cumbersome processes, including motion capture equipment, manual face rigging, and multiple reference footages. \u003c/p\u003e\n\n\n\n\u003cp\u003eAnyone interested in filmmaking has likely caught sight of some of the intricacy and difficulty of this process to date on set or when viewing behind the scenes footage of effects-heavy and motion-capture films such as \u003cem\u003eThe Lord of the Rings\u003c/em\u003e series, \u003cem\u003eAvatar\u003c/em\u003e, or \u003cem\u003eRise of the Planet of the Apes\u003c/em\u003e, wherein actors are seen covered in ping pong ball markers and their faces dotted with marker and blocked by head-mounted apparatuses.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\n\u003ciframe title=\"Turning Human Motion-Capture into Realistic Apes in Dawn of the Planet of the Apes | WIRED\" width=\"500\" height=\"281\" src=\"https://www.youtube.com/embed/4NU9ikjqjC0?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003c/p\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAccurately modeling intricate facial expressions is what led David Fincher and his production team on \u003ca href=\"https://www.researchgate.net/publication/262174852_The_Curious_Case_of_Benjamin_Button\"\u003e\u003cem\u003eThe Curious Case of Benjamin Button \u003c/em\u003eto develop whole new 3D modeling processes\u003c/a\u003e and ultimately won them an \u003ca href=\"https://www.designative.info/2009/02/26/oscar-winner-for-best-visual-effects-the-curious-case-of-benjamin-button-shaped-with-autodesk-technology/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAcademy Award\u003c/a\u003e, as reported in a \u003ca href=\"https://venturebeat.com/ai/hollywoods-strike-battle-over-ai-and-3d-scanning-has-been-decades-in-the-making/\"\u003eprior VentureBeat report\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eYet in the last few years, new software and AI-based startups such as \u003ca href=\"https://venturebeat.com/ai/3d-motion-capture-app-move-ai-raises-10m-in-seed-funding/\"\u003eMove\u003c/a\u003e have sought to reduce the equipment necessary to perform accurate motion capture — though that company in particular has concentrated primarily on full-body, more broad movements, whereas Runway’s Act-One is focused more on modeling facial expressions.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith Act-One, Runway aims to make this complex process far more accessible. The new tool allows creators to animate characters in a variety of styles and designs, without the need for motion-capture gear or character rigging.\u003c/p\u003e\n\n\n\n\u003cp\u003eInstead, users can rely on a simple driving video to transpose performances—including eye-lines, micro-expressions, and nuanced pacing—onto a generated character, or even multiple characters in different styles. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs Runway \u003ca href=\"https://x.com/runwayml/status/1848785911867121739\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ewrote on its X account:\u003c/a\u003e “Act-One is able to translate the performance from a single input video across countless different character designs and in many different styles.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe feature is focused “mostly” on the face “for now,” according to Cristóbal Valenzuela, co-founder and CEO of Runway, who responded to VentureBeat’s questions via direct message on X.\u003c/p\u003e\n\n\n\n\u003cp\u003eRunway’s approach offers significant advantages for animators, game developers, and filmmakers alike. The model accurately captures the depth of an actor’s performance while remaining versatile across different character designs and proportions. This opens up exciting possibilities for creating unique characters that express genuine emotion and personality.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-cinematic-realism-across-camera-angles\"\u003eCinematic realism across camera angles\u003c/h2\u003e\n\n\n\n\u003cp\u003eOne of Act-One’s key strengths lies in its ability to deliver cinematic-quality, realistic outputs from various camera angles and focal lengths. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis flexibility enhances creators’ ability to tell emotionally resonant stories through character performances that were previously hard to achieve without expensive equipment and multi-step workflows. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe tool’s ability to faithfully capture the emotional depth and performance style of an actor, even in complex scenes.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis shift allows creators to bring their characters to life in new ways, unlocking the potential for richer storytelling across both live-action and animated formats.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile Runway previously supported video-to-video AI conversion as previously mentioned in this piece, which did allow users to upload footage of themselves and have Gen-3 Alpha or other prior Runway AI video models such as Gen-2 “reskin” them with AI effects, the new Act-One feature is optimized for facial mapping and effects.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs Valenzuela told VentureBeat via DM on X: “The consistency and performance is unmatched with Act-One.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-enabling-more-expansive-video-storytelling\"\u003eEnabling more expansive video storytelling\u003c/h2\u003e\n\n\n\n\u003cp\u003eA single actor, using only a consumer-grade camera, can now perform multiple characters, with the model generating distinct outputs for each. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis capability is poised to transform narrative content creation, particularly in indie film production and digital media, where high-end production resources are often limited.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a \u003ca href=\"https://x.com/c_valenzuelab/status/1848812052707741727\"\u003epublic post on X, Valenzuela noted\u003c/a\u003e a shift in how the industry approaches generative models. “We are now beyond the threshold of asking ourselves if generative models can generate consistent videos. A good model is now the new baseline. The difference lies in what you do with the model—how you think about its applications and use cases, and what you ultimately build,” Valenzuela wrote.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-safety-and-protection-for-public-figure-impersonations\"\u003eSafety and protection for public figure impersonations\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs with all of Runway’s releases, Act-One comes equipped with a comprehensive suite of safety measures. \u003c/p\u003e\n\n\n\n\u003cp\u003eThese include safeguards to detect and block attempts to generate content featuring public figures without authorization, as well as technical tools to verify voice usage rights. \u003c/p\u003e\n\n\n\n\u003cp\u003eContinuous monitoring also ensures that the platform is used responsibly, preventing potential misuse of the tool.\u003c/p\u003e\n\n\n\n\u003cp\u003eRunway’s commitment to ethical development aligns with its broader mission to expand creative possibilities while maintaining a strong focus on safety and content moderation.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-looking-ahead\"\u003eLooking ahead\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs Act-One gradually rolls out, Runway is eager to see how artists, filmmakers, and other creators will harness this new tool to bring their ideas to life.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith Act -ne, complex animation techniques are now within reach for a broader audience of creators, enabling more people to explore new forms of storytelling and artistic expression.\u003c/p\u003e\n\n\n\n\u003cp\u003eBy reducing the technical barriers traditionally associated with character animation, the company hopes to inspire new levels of creativity across the digital media landscape.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt also helps Runway stand out and differentiate its AI video creation platform against the likes of an increasing swath of competitors, including \u003ca href=\"https://venturebeat.com/ai/ai-video-rivalry-intensifies-as-luma-announces-dream-machine-api-hours-after-runway/\"\u003eLuma AI\u003c/a\u003e from the U.S. and \u003ca href=\"https://venturebeat.com/ai/hailuo-gets-feature-competitive-launching-image-to-video-ai-generation-capability/\"\u003eHailuo\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/what-you-need-to-know-about-kling-the-ai-video-generator-rival-to-sora-thats-wowing-creators/\"\u003eKling\u003c/a\u003e from China, as well as open source rivals such as \u003ca href=\"https://venturebeat.com/ai/video-ai-startup-genmo-launches-mochi-1-an-open-source-model-to-rival-runway-kling-and-others/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGenmo’s Mochi 1\u003c/a\u003e, which also just debuted today.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2024-10-22T21:35:17Z",
  "modifiedTime": "2024-10-22T21:49:08Z"
}
