{
  "id": "51758814-35b0-4a03-aa24-354b641570ec",
  "title": "How we ensure data privacy and safety in the age of AI",
  "link": "https://www.intercom.com/blog/data-privacy-security-ai-chatbots/",
  "description": "With AI transforming customer support, organizations must prioritize data protection. At Intercom, we've built our AI products with security-first principles to safeguard customer information while delivering exceptional service. Here's how.",
  "author": "Thibault Candebat",
  "published": "Wed, 12 Feb 2025 16:00:49 +0000",
  "source": "https://www.intercom.com/blog/feed",
  "categories": [
    "AI \u0026 Automation",
    "AI agent",
    "data privacy",
    "data security",
    "Fin AI Agent"
  ],
  "byline": "Thibault Candebat,Norm Provost",
  "length": 14309,
  "excerpt": "Discover how Intercom safeguards customer data through robust privacy controls, LLM threat protection, and industry-leading security measures.",
  "siteName": "The Intercom Blog",
  "favicon": "https://blog.intercomassets.com/blog/wp-content/uploads/2018/10/cropped-cropped-site-icon-2-192x192.png",
  "text": "Generative AI has transformed customer service, with AI agents leveraging large language models (LLMs) to understand and resolve customer queries. While this significant change has brought with it multiple benefits for support teams and customers, it has also come with new data security and privacy risks. In the age of AI, handling customer data correctly isn’t just a regulatory requirement, it’s the foundation of trust between you and your customers. At Intercom, we’ve designed our AI-powered products, such as our AI agent, Fin, with data protection as a first principle, implementing multiple layers of security controls, accuracy checks, and privacy safeguards that exceed industry standards. Here, we take a transparent look at everything we’re doing to ensure our customers’ complete peace of mind. Protecting customers’ privacy Ensuring our customers trust us to handle their data safely has always been a core part of our philosophy. In this new AI era where we’re using third-party service providers to build AI-powered products and features, robust protections and compliance protocols are more important than ever. While this introduces additional complexity, we’re working hard to ensure it doesn’t impact our customers’ experience. Here’s how we’re keeping our customers safe while minimizing any red tape they have to deal with. Hosting infrastructure tailored to our global customer base: To ensure customers around the world can securely make use of our AI features, we’ve introduced new hosting infrastructure options. Most notably, our European customers can now access Intercom’s AI-powered products and features, including Fin AI Agent, Copilot, and inbox AI features, with data processed securely within Europe for enhanced data privacy. Existing European customers who were previously using US data processing will be automatically transitioned to the new EU-hosted infrastructure. Constant evaluation of third-party service providers: To provide the best possible customer experience, we’re committed to only working with the best third-party service providers. This means we’re constantly testing and evaluating which providers are performing best and returning the most accurate, high-quality results. Metrics we test for include resolution rates, hallucination rates, and latency in responses, amongst others. For the most up-to-date list of third-party service providers we’re working with, take a look here. Careful restrictions when it comes to data handling: None of our third-party LLM providers store any conversation data. Customer data is only temporarily used to generate responses before being deleted. In addition, all third-party LLM providers are contractually prohibited from using conversation data to train or improve their models. Access to our customers’ data is strictly limited to what’s necessary for providing the relevant service. Robust contractual protections to ensure compliance: Business Associate Agreements (BAAs) are in place with all third-party LLM providers we use for HIPAA-compliant customers. We also require these providers to implement appropriate technical and organizational measures to protect personal data to the standard set out by the applicable Data Protection legislation. Mitigating LLM-focused threats Data security is more important than ever when working with AI because, unlike traditional software systems where threats are often well-documented and predictable, LLMs can be vulnerable in ways that are still being discovered and understood by the security community. To stay ahead, our security teams are always thinking beyond conventional security measures and anticipating potential vulnerabilities before they’re exploited, continuously monitoring for new types of attacks, and maintaining security systems that can adapt as rapidly as the threats themselves evolve. Here are a few of the most important threats, identified in the OWASP LLM Top 10 threats list, that could put customers at risk and the steps we’ve taken to actively secure our AI-powered products against them. Direct prompt injection Direct prompt injection involves malicious actors attempting to manipulate the LLM supporting an AI agent by injecting harmful or misleading prompts. These attacks can potentially cause the AI agent to generate unintended or harmful responses. While prompt injections can’t be universally prevented, we’ve applied a series of guardrails to reduce the likelihood of prompt injection exploitation and eliminate any real business impact if a prompt injection does occur. The guardrails we’ve put in place include: Defensive prompting: We include protective instructions whenever the LLM reads user input, safeguarding against attempts to manipulate or override the system’s core safety measures. Monitoring and adaptation: We track the emergence of new prompt injection techniques and any instances where Fin’s AI features behave in unexpected ways. We then modify our defensive prompting strategy accordingly to counteract any new threats. Version upgrades: We assess and migrate to newer versions of our LLM providers’ generative models, which are increasingly robust against prompt injection attacks. Rapid response: Over the past year, we’ve observed very few successful attempts at bypassing the safeguards put in place to protect our AI-powered features. None of these attempts constituted a reputational threat to our customers, and all incidents were mitigated in a timely fashion. Data poisoning Data poisoning involves the injection of malicious data into the training set. We’ve secured against this happening by taking these steps: Data validation: We’ve put rigorous data validation protocols and platform controls in place to ensure the integrity and quality of Knowledge Base data. Limiting context: In each LLM prompt, we only include the context that the LLM needs to perform a specific operation. This reduces the likelihood of incorporating any poisoned data at scale. Evasion attacks Evasion attacks involve adversaries crafting inputs that bypass AI features’ security measures and produce harmful outputs. Our mitigation strategies to avoid this include: Adversarial testing: We run industry-standard LLM vulnerability scanning against the models we use in production to identify and mitigate potential evasion techniques. Robust prompt design: We’ve invested a significant amount of time in designing our prompts to be robust against manipulation, ensuring Fin’s outputs remain consistent and secure. Regular updates: Security measures are regularly updated to address new evasion techniques as they emerge. Insecure integrations Insecure integrations involve vulnerabilities arising from the integration of the LLM or AI features with other systems or applications. To ensure an ironclad integration, we’ve implemented the following: Secure API design: APIs used to integrate the LLM with Intercom and/or other systems are designed with security best practices in mind, including secure defaults like secure proxy to LLM providers and complementary customer control implementation for specific features. Penetration testing: Regular penetration testing is conducted to identify and address integration vulnerabilities. Security reviews: Comprehensive security reviews are performed for all integrations to ensure they meet our stringent security standards. Data leakage Data leakage occurs when sensitive or confidential information is inadvertently exposed through AI-powered features’ responses. To protect against this, we’ve employed the following strategies: Granular opt-in: We design our AI features in such a way that customers must explicitly opt in to making data sources available for LLM processing. This means customers have fine-grained control of the content they’re comfortable feeding to AI-backed features. Data leak prevention by design: We operate under the assumption that the data going into an LLM prompt can come out as part of the LLM response. All features that surface an LLM response ensure that the data being fed to the LLM is already authorized to be read by the relevant parties involved. Access controls: Strict access controls are enforced to limit the exposure of sensitive data within the LLM’s operational environments. Intercom’s LLM providers implement a zero data retention policy on customer messages and returned responses, and we mandate that no data is used for training or fine-tuning of their LLMs. Auditing and logging: Comprehensive logging and auditing mechanisms are in place to track data access and usage, ensuring any anomalies are quickly identified and addressed. Prompts are logged in access controlled repositories and access is gated according to business needs. Our LLM providers are considered key sub-processors and are therefore subject to periodic User Access Reviews and annual security audits as part of our risk management and compliance efforts. Maximizing protection with performance assurance at every phase of the development cycle Our commitment to delivering exceptional quality is reflected in our AI products’ performance, which consistently surpass publicly available LLMs. Post-deployment, our AI-powered products are continuously monitored in production to ensure ongoing excellence. Our engineering and ML teams collaborate closely on rigorously testing every model and prompt modification, reviewing metrics, and analyzing customer reports to quickly address any emerging issues. Backtesting Before any updates are introduced, they undergo thorough scrutiny in our secured development environment. This ensures enhancements in key performance areas without compromising our AI products’ reliability. Our backtesting process includes: Minimizing hallucinations: We ensure that responses are grounded in the provided context, rather than being influenced by the LLM’s pre-existing knowledge. Ensuring quality: Both automated and manual quality checks are conducted to maintain the high standard of Fin’s responses. A/B testing We deploy each model or prompt change to a controlled subset of requests to evaluate: Maximizing resolution rates: This ensures that the updates improve the accuracy and usefulness of responses. Customer satisfaction: Assessing feedback and CSAT scores allows us to validate the positive impact of changes. Comprehensive performance assessment: Conducting large-scale evaluations helps to ensure sustained performance and quality. Optimizing latency and performance: We rigorously test for improved responsiveness without sacrificing accuracy. Red teaming To safeguard against vulnerabilities, we conduct extensive internal penetration testing, including: Guardrail testing: This involves regular simulations of real-world attack scenarios that incorporate the broad functionality of the Intercom platform to validate the robustness of our defenses. Hallucination assessments: We perform systematic evaluations of the effects a hallucination may have on downstream product behavior, ensuring a defense-in-depth approach when handling the LLM responses. Our high conversation volume has enabled us to build a substantial dataset of real-world hallucinations, which we use to validate our models against. This real-world testing capability, available only to established players like Intercom that processes millions of conversations, provides an additional layer of quality assurance. Vulnerability scanning: We apply industry-standard LLM vulnerability scans to identify and mitigate potential weaknesses. If a model fails to meet our stringent security criteria, it isn’t deployed. Regular auditing Insufficient auditing can lead to undetected security incidents and non-compliance with regulatory requirements. To guard against this, we perform the following: Comprehensive logging: Detailed logging of all LLM interactions is maintained to provide a clear audit trail. Regular audits: Regular security audits are conducted to ensure compliance with regulatory requirements and internal policies. Performance monitoring: Key operational metrics are continuously tracked, including resolution rates, deflection rates, user feedback (both positive and negative), distribution of response types and outcomes, and response characteristics like length and complexity. Incident response: An incident response plan is in place to quickly address any security incidents identified through auditing. The future of customer service depends on trust As AI continues to reshape customer service, trusting the providers you work with is critical. And that trust can’t be built on promises alone – vendors need to walk the walk and demonstrate a consistent commitment to ensuring data safety. This space is moving fast, and we’re adapting and learning all the time. At Intercom, we’re committed to leading the way in setting the highest standards of trust when it comes to privacy and security. Product \u0026 Design 6 min read Security at Intercom: How our InfoSec team protects our customers’ data and trust ​​At Intercom, we think deeply about how to deliver the safest, most secure experience possible for our customers and their customers. We’re committed to meeting the highest standards of security to protect our customers’ trust and love for our product. Here’s how we do it. AI \u0026 Automation 2 min read Fin 2: Powered by Anthropic’s Claude LLM Fin 2 now runs on Anthropic’s Claude, a cutting-edge LLM which provides the levels of intelligence, performance, and reliability that lets us deliver even more value to our customers. Des Traynor Co-founder \u0026 Chief Strategy Officer, Intercom Product \u0026 Design 8 min read How we ensure the highest standards of data privacy and compliance within Intercom Your data is our most critical asset. We protect it throughout its lifecycle with robust security practices, tailored role-specific staff training, and rigorous compliance with regulations. Penny Gray Staff Technical Programme Manager, Intercom",
  "image": "https://blog.intercomassets.com/blog/wp-content/uploads/2025/02/FinAIAgent-Privacy-Security-scaled.jpg.optimal.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\n\n\n\t\t\t\u003cmain id=\"main\" role=\"main\" itemscope=\"\" itemprop=\"mainContentOfPage\"\u003e\n\n\t\u003carticle itemscope=\"\" itemtype=\"https://schema.org/BlogPosting\"\u003e\n\t\t\n\n\t\t\u003cmeta itemscope=\"\" itemprop=\"mainEntityOfPage\" itemtype=\"https://schema.org/WebPage\" itemid=\"https://www.intercom.com/blog/data-privacy-security-ai-chatbots/\"/\u003e\n\n\t\t\n\t\t\t\n\t\t\t\u003cfigure itemprop=\"image\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"\u003e\n\t\t\t\t\u003cmeta itemprop=\"url\" content=\"https://blog.intercomassets.com/blog/wp-content/uploads/2025/02/FinAIAgent-Privacy-Security-scaled.jpg.optimal.jpg\"/\u003e\n\t\t\t\t\u003cmeta itemprop=\"width\" content=\"2560\"/\u003e\n\t\t\t\t\u003cmeta itemprop=\"height\" content=\"1195\"/\u003e\n\t\t\t\t\u003cimg width=\"1800\" height=\"840\" src=\"https://blog.intercomassets.com/blog/wp-content/uploads/2025/02/FinAIAgent-Privacy-Security-1800x840.jpg.optimal.jpg\" alt=\"Fin AI Agent Data Privacy and Security\" decoding=\"async\" loading=\"lazy\"/\u003e\t\t\t\u003c/figure\u003e\n\n\t\t\n\t\t\u003cheader\u003e\n\t\t\t\n\t\t\u003c/header\u003e\n\n\t\t\n\n\t\t\u003cdiv\u003e\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\u003cp\u003eGenerative AI has transformed customer service, with AI agents leveraging large language models (LLMs) to understand and resolve customer queries.\u003c/p\u003e\n\u003cp\u003eWhile this significant change has brought with it multiple benefits for support teams and customers, it has also come with new data security and privacy risks. In the age of AI, handling customer data correctly isn’t just a regulatory requirement, it’s the foundation of trust between you and your customers.\u003c/p\u003e\n\u003cp\u003eAt Intercom, we’ve designed our AI-powered products, such as our AI agent, Fin, with data protection as a first principle, implementing multiple layers of security controls, accuracy checks, and privacy safeguards that exceed industry standards.\u003c/p\u003e\n\u003cp\u003eHere, we take a transparent look at everything we’re doing to ensure our customers’ complete peace of mind.\u003c/p\u003e\n\u003ch2\u003eProtecting customers’ privacy\u003c/h2\u003e\n\u003cp\u003eEnsuring our customers trust us to handle their data safely has always been a core part of our philosophy. In this new AI era where we’re using third-party service providers to build AI-powered products and features, robust protections and compliance protocols are more important than ever.\u003c/p\u003e\n\u003cp\u003eWhile this introduces additional complexity, we’re working hard to ensure it doesn’t impact our customers’ experience. Here’s how we’re keeping our customers safe while minimizing any red tape they have to deal with.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHosting infrastructure tailored to our global customer base:\u003c/strong\u003e To ensure customers around the world can securely make use of our AI features, we’ve introduced new hosting infrastructure options. Most notably, our European customers can now access Intercom’s AI-powered products and features, including \u003ca href=\"https://www.intercom.com/fin\"\u003eFin AI Agent\u003c/a\u003e, \u003ca href=\"https://www.intercom.com/support-for-agents/ai-copilot\"\u003eCopilot\u003c/a\u003e, and inbox AI features, with data processed securely within Europe for enhanced data privacy. Existing European customers who were previously using US data processing will be automatically transitioned to the new EU-hosted infrastructure.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConstant evaluation of third-party service providers:\u003c/strong\u003e To provide the best possible customer experience, we’re committed to only working with the best third-party service providers. This means we’re constantly testing and evaluating which providers are performing best and returning the most accurate, high-quality results. Metrics we test for include resolution rates, hallucination rates, and latency in responses, amongst others. For the most up-to-date list of third-party service providers we’re working with, take a look \u003ca href=\"https://www.intercom.com/legal/subprocessors-list?redirect_from=/legal/security-third-parties\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCareful restrictions when it comes to data handling:\u003c/strong\u003e None of our third-party LLM providers store any conversation data. Customer data is only temporarily used to generate responses before being deleted. In addition, all third-party LLM providers are contractually prohibited from using conversation data to train or improve their models. Access to our customers’ data is strictly limited to what’s necessary for providing the relevant service.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRobust contractual protections to ensure compliance:\u003c/strong\u003e Business Associate Agreements (BAAs) are in place with all third-party LLM providers we use for HIPAA-compliant customers. We also require these providers to implement appropriate technical and organizational measures to protect personal data to the standard set out by the applicable Data Protection legislation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eMitigating LLM-focused threats\u003c/h2\u003e\n\u003cp\u003eData security is more important than ever when working with AI because, unlike traditional software systems where threats are often well-documented and predictable, LLMs can be vulnerable in ways that are still being discovered and understood by the security community.\u003c/p\u003e\n\u003cp\u003eTo stay ahead, our security teams are always thinking beyond conventional security measures and anticipating potential vulnerabilities before they’re exploited, continuously monitoring for new types of attacks, and maintaining security systems that can adapt as rapidly as the threats themselves evolve.\u003c/p\u003e\n\u003cp\u003eHere are a few of the most important threats, identified in the \u003ca href=\"https://owasp.org/www-project-top-10-for-large-language-model-applications/\"\u003eOWASP LLM Top 10 threats list\u003c/a\u003e, that could put customers at risk and the steps we’ve taken to actively secure our AI-powered products against them.\u003c/p\u003e\n\u003ch3\u003eDirect prompt injection\u003c/h3\u003e\n\u003cp\u003eDirect prompt injection involves malicious actors attempting to manipulate the LLM supporting an AI agent by injecting harmful or misleading prompts. These attacks can potentially cause the AI agent to generate unintended or harmful responses.\u003c/p\u003e\n\u003cp\u003eWhile prompt injections can’t be universally prevented, we’ve applied a series of guardrails to reduce the likelihood of prompt injection exploitation and eliminate any real business impact if a prompt injection does occur. The guardrails we’ve put in place include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDefensive prompting:\u003c/strong\u003e We include protective instructions whenever the LLM reads user input, safeguarding against attempts to manipulate or override the system’s core safety measures.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMonitoring and adaptation:\u003c/strong\u003e We track the emergence of new prompt injection techniques and any instances where Fin’s AI features behave in unexpected ways. We then modify our defensive prompting strategy accordingly to counteract any new threats.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVersion upgrades:\u003c/strong\u003e We assess and migrate to newer versions of our LLM providers’ generative models, which are increasingly robust against prompt injection attacks.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRapid response:\u003c/strong\u003e Over the past year, we’ve observed very few successful attempts at bypassing the safeguards put in place to protect our AI-powered features. None of these attempts constituted a reputational threat to our customers, and all incidents were mitigated in a timely fashion.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eData poisoning\u003c/h3\u003e\n\u003cp\u003eData poisoning involves the injection of malicious data into the training set.\u003c/p\u003e\n\u003cp\u003eWe’ve secured against this happening by taking these steps:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eData validation:\u003c/strong\u003e We’ve put rigorous data validation protocols and platform controls in place to ensure the integrity and quality of Knowledge Base data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLimiting context:\u003c/strong\u003e In each LLM prompt, we only include the context that the LLM needs to perform a specific operation. This reduces the likelihood of incorporating any poisoned data at scale.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eEvasion attacks\u003c/h3\u003e\n\u003cp\u003eEvasion attacks involve adversaries crafting inputs that bypass AI features’ security measures and produce harmful outputs.\u003c/p\u003e\n\u003cp\u003eOur mitigation strategies to avoid this include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAdversarial testing:\u003c/strong\u003e We run industry-standard LLM vulnerability scanning against the models we use in production to identify and mitigate potential evasion techniques.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRobust prompt design:\u003c/strong\u003e We’ve invested a significant amount of time in designing our prompts to be robust against manipulation, ensuring Fin’s outputs remain consistent and secure.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRegular updates:\u003c/strong\u003e Security measures are regularly updated to address new evasion techniques as they emerge.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eInsecure integrations\u003c/h3\u003e\n\u003cp\u003eInsecure integrations involve vulnerabilities arising from the integration of the LLM or AI features with other systems or applications.\u003c/p\u003e\n\u003cp\u003eTo ensure an ironclad integration, we’ve implemented the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSecure API design:\u003c/strong\u003e APIs used to integrate the LLM with Intercom and/or other systems are designed with security best practices in mind, including secure defaults like secure proxy to LLM providers and complementary customer control implementation for specific features.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePenetration testing:\u003c/strong\u003e Regular penetration testing is conducted to identify and address integration vulnerabilities.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSecurity reviews:\u003c/strong\u003e Comprehensive security reviews are performed for all integrations to ensure they meet our stringent security standards.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eData leakage\u003c/h3\u003e\n\u003cp\u003eData leakage occurs when sensitive or confidential information is inadvertently exposed through AI-powered features’ responses.\u003c/p\u003e\n\u003cp\u003eTo protect against this, we’ve employed the following strategies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGranular opt-in:\u003c/strong\u003e We design our AI features in such a way that customers must explicitly opt in to making data sources available for LLM processing. This means customers have fine-grained control of the content they’re comfortable feeding to AI-backed features.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData leak prevention by design:\u003c/strong\u003e We operate under the assumption that the data going into an LLM prompt can come out as part of the LLM response. All features that surface an LLM response ensure that the data being fed to the LLM is already authorized to be read by the relevant parties involved.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccess controls:\u003c/strong\u003e Strict access controls are enforced to limit the exposure of sensitive data within the LLM’s operational environments. Intercom’s LLM providers implement a zero data retention policy on customer messages and returned responses, and we mandate that no data is used for training or fine-tuning of their LLMs.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuditing and logging:\u003c/strong\u003e Comprehensive logging and auditing mechanisms are in place to track data access and usage, ensuring any anomalies are quickly identified and addressed. Prompts are logged in access controlled repositories and access is gated according to business needs. Our LLM providers are considered key sub-processors and are therefore subject to periodic User Access Reviews and annual security audits as part of our risk management and compliance efforts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eMaximizing protection with performance assurance at every phase of the development cycle\u003c/h2\u003e\n\u003cp\u003eOur commitment to delivering exceptional quality is reflected in our AI products’ performance, which consistently surpass publicly available LLMs.\u003c/p\u003e\n\u003cp\u003ePost-deployment, our AI-powered products are continuously monitored in production to ensure ongoing excellence. Our engineering and ML teams collaborate closely on rigorously testing every model and prompt modification, reviewing metrics, and analyzing customer reports to quickly address any emerging issues.\u003c/p\u003e\n\u003ch3\u003eBacktesting\u003c/h3\u003e\n\u003cp\u003eBefore any updates are introduced, they undergo thorough scrutiny in our secured development environment. This ensures enhancements in key performance areas without compromising our AI products’ reliability. Our backtesting process includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMinimizing hallucinations:\u003c/strong\u003e We ensure that responses are grounded in the provided context, rather than being influenced by the LLM’s pre-existing knowledge.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEnsuring quality:\u003c/strong\u003e Both automated and manual quality checks are conducted to maintain the high standard of Fin’s responses.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eA/B testing\u003c/h3\u003e\n\u003cp\u003eWe deploy each model or prompt change to a controlled subset of requests to evaluate:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMaximizing resolution rates:\u003c/strong\u003e This ensures that the updates improve the accuracy and usefulness of responses.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustomer satisfaction:\u003c/strong\u003e Assessing feedback and CSAT scores allows us to validate the positive impact of changes.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eComprehensive performance assessment:\u003c/strong\u003e Conducting large-scale evaluations helps to ensure sustained performance and quality.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOptimizing latency and performance:\u003c/strong\u003e We rigorously test for improved responsiveness without sacrificing accuracy.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eRed teaming\u003c/h3\u003e\n\u003cp\u003eTo safeguard against vulnerabilities, we conduct extensive internal penetration testing, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGuardrail testing:\u003c/strong\u003e This involves regular simulations of real-world attack scenarios that incorporate the broad functionality of the Intercom platform to validate the robustness of our defenses.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHallucination assessments:\u003c/strong\u003e We perform systematic evaluations of the effects a hallucination may have on downstream product behavior, ensuring a defense-in-depth approach when handling the LLM responses. Our high conversation volume has enabled us to build a substantial dataset of real-world hallucinations, which we use to validate our models against. This real-world testing capability, available only to established players like Intercom that processes millions of conversations, provides an additional layer of quality assurance.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVulnerability scanning:\u003c/strong\u003e We apply industry-standard LLM vulnerability scans to identify and mitigate potential weaknesses. If a model fails to meet our stringent security criteria, it isn’t deployed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eRegular auditing\u003c/h3\u003e\n\u003cp\u003eInsufficient auditing can lead to undetected security incidents and non-compliance with regulatory requirements. To guard against this, we perform the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eComprehensive logging:\u003c/strong\u003e Detailed logging of all LLM interactions is maintained to provide a clear audit trail.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRegular audits:\u003c/strong\u003e Regular security audits are conducted to ensure compliance with regulatory requirements and internal policies.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePerformance monitoring:\u003c/strong\u003e Key operational metrics are continuously tracked, including resolution rates, deflection rates, user feedback (both positive and negative), distribution of response types and outcomes, and response characteristics like length and complexity.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIncident response:\u003c/strong\u003e An incident response plan is in place to quickly address any security incidents identified through auditing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe future of customer service depends on trust\u003c/h2\u003e\n\u003cp\u003eAs AI continues to reshape customer service, trusting the providers you work with is critical. And that trust can’t be built on promises alone – vendors need to walk the walk and demonstrate a consistent commitment to ensuring data safety.\u003c/p\u003e\n\u003cp\u003eThis space is moving fast, and we’re adapting and learning all the time. At Intercom, we’re committed to leading the way in setting the highest standards of trust when it comes to privacy and security.\u003c/p\u003e\n\u003cp\u003e\n\n\t\u003ca href=\"https://www.intercom.com/fin\" data-ctatype=\"horizontal\" data-ctaname=\"Fin2 AI Agent (Horizontal)\" data-ctacategory=\"\" data-tracking-identifier=\"Fin2 AI Agent (Horizontal)\" data-tracking-action=\"Blog Ad Click\" data-tracking-type=\"Product Ad\" data-tracking-text=\"\" data-tracking-location=\"https://www.intercom.com/fin\" target=\"_blank\"\u003e\n\t\t\u003cimg decoding=\"async\" src=\"https://blog.intercomassets.com/blog/wp-content/uploads/2024/10/Fin2-CTA-Horizontal-1200.jpg.optimal.jpg\" alt=\"Fin2 AI Agent blog CTA - Horizontal\"/\u003e\n\t\u003c/a\u003e\n\n\t\u003c/p\u003e\n\n\t\t\t\n\u003cp\u003e\u003ca href=\"https://transformation.intercom.com/?utm_source=ii-blog\u0026amp;utm_medium=internal\u0026amp;utm_campaign=20250128_all_wc_all_global_enus_all_all_all_cst25\u0026amp;utm_content=ad\" data-ctatype=\"vertical\" data-ctaname=\"CS Transformation Report (Vertical)\" data-ctacategory=\"Report\" data-tracking-identifier=\"CS Transformation Report (Vertical)\" data-tracking-action=\"Blog Ad Click\" data-tracking-type=\"Guides and Reports\" data-tracking-text=\"\" data-tracking-location=\"https://transformation.intercom.com/\" target=\"_blank\"\u003e\n\t\t\u003cimg src=\"https://blog.intercomassets.com/blog/wp-content/uploads/2025/01/CS-Transformation-Report-CTA-vertical.jpg.optimal.jpg\" alt=\"The 2025 Customer Service Transformation Report – blog CTA vertical\"/\u003e\n\t\u003c/a\u003e\n\n\t\u003c/p\u003e\n\n\t\t\t\n\t\t\u003c/div\u003e\n\n\t\u003c/article\u003e\n\n\t\n\t\u003csection\u003e\n\t\t\n\n\t\t\n\u003carticle itemscope=\"\" itemtype=\"http://schema.org/BlogPosting\"\u003e\n\t\n\n\t\u003cmeta itemscope=\"\" itemprop=\"mainEntityOfPage\" itemtype=\"https://schema.org/WebPage\" itemid=\"https://www.intercom.com/blog/intercom-for-enterprise-security/\"/\u003e\n\n\t\n\t\u003cfigure itemprop=\"image\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"\u003e\n\t\t\u003ca itemprop=\"url\" href=\"https://www.intercom.com/blog/intercom-for-enterprise-security/\"\u003e\n\t\t\t\u003cimg width=\"530\" height=\"240\" src=\"https://blog.intercomassets.com/blog/wp-content/uploads/2022/02/1-intercom-for-enterprise@1x-530x240.jpg.optimal.jpg\" alt=\"intercom-for-enterprise\" decoding=\"async\" loading=\"lazy\"/\u003e\t\t\t\u003cmeta itemprop=\"url\" content=\"https://blog.intercomassets.com/blog/wp-content/uploads/2022/02/1-intercom-for-enterprise@1x.jpg.optimal.jpg\"/\u003e\n\t\t\t\u003cmeta itemprop=\"width\" content=\"1800\"/\u003e\n\t\t\t\u003cmeta itemprop=\"height\" content=\"843\"/\u003e\n\t\t\u003c/a\u003e\n\t\u003c/figure\u003e\n\n\t\u003cp\u003e\n\t\t\u003ca href=\"https://www.intercom.com/blog/product-and-design/\"\u003eProduct \u0026amp; Design\u003c/a\u003e\n\t\t\u003cspan\u003e6 min read\u003c/span\u003e\n\t\u003c/p\u003e\n\n\t\u003cheader itemprop=\"headline\"\u003e\n\t\t\u003ctime itemprop=\"datePublished\" datetime=\"2022-02-22T17:44:31+00:00\"\u003e\u003c/time\u003e\n\t\t\u003ctime itemprop=\"dateModified\" datetime=\"2022-03-14T12:40:23+00:00\"\u003e\u003c/time\u003e\n\t\t\u003ch2\u003e\n\t\t\t\u003ca href=\"https://www.intercom.com/blog/intercom-for-enterprise-security/\" itemprop=\"url\" rel=\"bookmark\"\u003e\n\t\t\t\tSecurity at Intercom: How our InfoSec team protects our customers’ data and trust\t\t\t\u003c/a\u003e\n\t\t\u003c/h2\u003e\n\t\u003c/header\u003e\n\n\t\u003cp\u003e​​At Intercom, we think deeply about how to deliver the safest, most secure experience possible for our customers and their customers. We’re committed to meeting the highest standards of security to protect our customers’ trust and love for our product. Here’s how we do it.\u003c/p\u003e\n\n\t\n\n\u003c/article\u003e\n\n\u003carticle itemscope=\"\" itemtype=\"http://schema.org/BlogPosting\"\u003e\n\t\n\n\t\u003cmeta itemscope=\"\" itemprop=\"mainEntityOfPage\" itemtype=\"https://schema.org/WebPage\" itemid=\"https://www.intercom.com/blog/fin-2-powered-by-anthropic-claude-llm/\"/\u003e\n\n\t\n\t\u003cfigure itemprop=\"image\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"\u003e\n\t\t\u003ca itemprop=\"url\" href=\"https://www.intercom.com/blog/fin-2-powered-by-anthropic-claude-llm/\"\u003e\n\t\t\t\u003cimg width=\"530\" height=\"240\" src=\"https://blog.intercomassets.com/blog/wp-content/uploads/2024/10/Blog-Anthropic-Intercom-Paul-Adams-530x240.jpg.optimal.jpg\" alt=\"Paul Adams Intercom Anthropic at Pioneer 2024\" decoding=\"async\" loading=\"lazy\"/\u003e\t\t\t\u003cmeta itemprop=\"url\" content=\"https://blog.intercomassets.com/blog/wp-content/uploads/2024/10/Blog-Anthropic-Intercom-Paul-Adams.jpg.optimal.jpg\"/\u003e\n\t\t\t\u003cmeta itemprop=\"width\" content=\"1800\"/\u003e\n\t\t\t\u003cmeta itemprop=\"height\" content=\"840\"/\u003e\n\t\t\u003c/a\u003e\n\t\u003c/figure\u003e\n\n\t\u003cp\u003e\n\t\t\u003ca href=\"https://www.intercom.com/blog/ai-ml/\"\u003eAI \u0026amp; Automation\u003c/a\u003e\n\t\t\u003cspan\u003e2 min read\u003c/span\u003e\n\t\u003c/p\u003e\n\n\t\u003cheader itemprop=\"headline\"\u003e\n\t\t\u003ctime itemprop=\"datePublished\" datetime=\"2024-10-10T05:55:38+01:00\"\u003e\u003c/time\u003e\n\t\t\u003ctime itemprop=\"dateModified\" datetime=\"2024-10-10T16:11:40+01:00\"\u003e\u003c/time\u003e\n\t\t\u003ch2\u003e\n\t\t\t\u003ca href=\"https://www.intercom.com/blog/fin-2-powered-by-anthropic-claude-llm/\" itemprop=\"url\" rel=\"bookmark\"\u003e\n\t\t\t\tFin 2: Powered by Anthropic’s Claude LLM\t\t\t\u003c/a\u003e\n\t\t\u003c/h2\u003e\n\t\u003c/header\u003e\n\n\t\u003cp\u003eFin 2 now runs on Anthropic’s Claude, a cutting-edge LLM which provides the levels of intelligence, performance, and reliability that lets us deliver even more value to our customers.\u003c/p\u003e\n\n\t\n\u003cdiv itemprop=\"author\" itemscope=\"\" itemtype=\"http://schema.org/Person\"\u003e\n\t\t\t\u003cp itemprop=\"name\"\u003e\n\t\t\t\t\u003ca href=\"https://www.intercom.com/blog/author/des/\" title=\"Posts by Des Traynor\" rel=\"author\"\u003eDes Traynor\u003c/a\u003e\t\t\t\u003c/p\u003e\n\n\t\t\t\u003cp\u003e\n\t\t\t\tCo-founder \u0026amp; Chief Strategy Officer, Intercom\t\t\t\u003c/p\u003e\n\n\t\t\t\u003cp\u003e\u003cimg src=\"https://blog.intercomassets.com/blog/wp-content/uploads/2019/06/Des-Traynor-96x96.jpg.optimal.jpg\" alt=\"Des Traynor\"/\u003e\n\t\t\t\t\t\u003c/p\u003e\u003c/div\u003e\n\u003c/article\u003e\n\n\u003carticle itemscope=\"\" itemtype=\"http://schema.org/BlogPosting\"\u003e\n\t\n\n\t\u003cmeta itemscope=\"\" itemprop=\"mainEntityOfPage\" itemtype=\"https://schema.org/WebPage\" itemid=\"https://www.intercom.com/blog/data-privacy-compliance/\"/\u003e\n\n\t\n\t\u003cfigure itemprop=\"image\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"\u003e\n\t\t\u003ca itemprop=\"url\" href=\"https://www.intercom.com/blog/data-privacy-compliance/\"\u003e\n\t\t\t\u003cimg width=\"530\" height=\"240\" src=\"https://blog.intercomassets.com/blog/wp-content/uploads/2022/10/Intercom-for-Enterprise-Engage-530x240.jpg.optimal.jpg\" alt=\"Intercom for Enterprise data privacy\" decoding=\"async\" loading=\"lazy\"/\u003e\t\t\t\u003cmeta itemprop=\"url\" content=\"https://blog.intercomassets.com/blog/wp-content/uploads/2022/10/Intercom-for-Enterprise-Engage.jpg.optimal.jpg\"/\u003e\n\t\t\t\u003cmeta itemprop=\"width\" content=\"1800\"/\u003e\n\t\t\t\u003cmeta itemprop=\"height\" content=\"842\"/\u003e\n\t\t\u003c/a\u003e\n\t\u003c/figure\u003e\n\n\t\u003cp\u003e\n\t\t\u003ca href=\"https://www.intercom.com/blog/product-and-design/\"\u003eProduct \u0026amp; Design\u003c/a\u003e\n\t\t\u003cspan\u003e8 min read\u003c/span\u003e\n\t\u003c/p\u003e\n\n\t\u003cheader itemprop=\"headline\"\u003e\n\t\t\u003ctime itemprop=\"datePublished\" datetime=\"2022-10-19T12:21:00+01:00\"\u003e\u003c/time\u003e\n\t\t\u003ctime itemprop=\"dateModified\" datetime=\"2023-05-04T13:31:28+01:00\"\u003e\u003c/time\u003e\n\t\t\u003ch2\u003e\n\t\t\t\u003ca href=\"https://www.intercom.com/blog/data-privacy-compliance/\" itemprop=\"url\" rel=\"bookmark\"\u003e\n\t\t\t\tHow we ensure the highest standards of data privacy and compliance within Intercom\t\t\t\u003c/a\u003e\n\t\t\u003c/h2\u003e\n\t\u003c/header\u003e\n\n\t\u003cp\u003eYour data is our most critical asset. We protect it throughout its lifecycle with robust security practices, tailored role-specific staff training, and rigorous compliance with regulations. \u003c/p\u003e\n\n\t\n\u003cdiv itemprop=\"author\" itemscope=\"\" itemtype=\"http://schema.org/Person\"\u003e\n\t\t\t\u003cp itemprop=\"name\"\u003e\n\t\t\t\t\u003ca href=\"https://www.intercom.com/blog/author/pennymerelle/\" title=\"Posts by Penny Gray\" rel=\"author\"\u003ePenny Gray\u003c/a\u003e\t\t\t\u003c/p\u003e\n\n\t\t\t\u003cp\u003e\n\t\t\t\tStaff Technical Programme Manager, Intercom\t\t\t\u003c/p\u003e\n\n\t\t\t\u003cp\u003e\u003cimg src=\"https://blog.intercomassets.com/blog/wp-content/uploads/2017/05/penny-96x96.jpg.optimal.jpg\" alt=\"Penny Gray\"/\u003e\n\t\t\t\t\t\u003c/p\u003e\u003c/div\u003e\n\u003c/article\u003e\n\n\t\u003c/section\u003e\n\n\t\n\n\t\t\u003c/main\u003e\n\n\t\t\t\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2025-02-12T16:00:49Z",
  "modifiedTime": "2025-02-12T16:01:42Z"
}
