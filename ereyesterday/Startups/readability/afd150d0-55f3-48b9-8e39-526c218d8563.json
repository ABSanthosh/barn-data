{
  "id": "afd150d0-55f3-48b9-8e39-526c218d8563",
  "title": "Kubernetes is a symptom, not a solution",
  "link": "https://andreafortuna.org/2025/06/20/unpopular-opinion-kubernetes-is-a-symptom-not-a-solution",
  "description": "Article URL: https://andreafortuna.org/2025/06/20/unpopular-opinion-kubernetes-is-a-symptom-not-a-solution Comments URL: https://news.ycombinator.com/item?id=44460288 Points: 5 # Comments: 0",
  "author": "gsky",
  "published": "Fri, 04 Jul 2025 00:42:27 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "",
  "length": 11735,
  "excerpt": "Today I decided to run a little experiment. I asked an AI to write a brutally honest, no-holds-barred critique of Kubernetes, something really spicy that would make the container orchestration crowd squirm in their ergonomic office chairs.",
  "siteName": "Andrea Fortuna",
  "favicon": "",
  "text": "Today I decided to run a little experiment. I asked an AI to write a brutally honest, no-holds-barred critique of Kubernetes, something really spicy that would make the container orchestration crowd squirm in their ergonomic office chairs. “Make it controversial,” I said. “Make it provocative. Really lean into the contrarian takes.” So everything you’re about to read? Those aren’t technically my opinions. They’re the unfiltered thoughts of an artificial intelligence that doesn’t have to worry about getting uninvited from KubeCon or losing consulting gigs with companies heavily invested in the K8s ecosystem. …Or are they? Let me start with a confession: I think Kubernetes is one of the biggest obstacles to genuine innovation in cloud computing today. There, I said it. Before you grab your pitchforks and start composing angry tweets about how K8s “orchestrates at scale” and “democratizes container deployment,” hear me out. Because underneath all that YAML wizardry and those impressive-sounding conferences lies what I believe is a fundamental architectural mistake that’s holding back the entire industry. The Real Problem we should be solving The fundamental challenge of cloud computing isn’t actually that complicated when you strip away all the vendor marketing and ecosystem noise. It boils down to this: How do we create software components for distributed computing that can be easily deployed in the cloud? This isn’t a new problem, by the way. We’ve seen similar challenges before in the history of computing. Remember CORBA? Yeah, that distributed object standard from the ’90s that tried to solve interoperability between different systems? It was attempting to tackle a similar conceptual challenge, just in a different era with different constraints. When I think about how cloud deployment should work, I imagine something beautifully simple: you write your code, and then you send it “up” to the cloud with as few intermediate steps as possible. Ideally, instantaneously. Write code, deploy code. Done. Notice I’m not talking about testing or security here – those are separate concerns. Having an instantaneous deployment system wouldn’t prevent you from adding subsequent phases of hardening and validation. In fact, it would probably help those processes too, because you’d have a cleaner, more predictable foundation to build upon. What the Ideal World looks like In my ideal world, we’d have systems similar to what already exists for languages like Erlang or Unison: languages that were designed from the ground up with distributed computing in mind. But we’d have this capability available for all programming languages, not just the lucky few. And here’s the kicker: universal components that work across all languages already exist. Look at WASI (WebAssembly System Interface), components built on WebAssembly that provide a platform-independent, efficient format that any language can compile to. This would be my dream solution: write your code in whatever language you prefer, compile your microservices into semantically meaningful components using a platform-independent but efficient format, and deploy them universally. This approach is remarkably similar to what’s being done today with systems like Apache OpenServerless. These platforms actually get it right, focusing on the logical components and their behavior rather than getting bogged down in infrastructure minutiae. The Docker Disaster But instead of this elegant vision, what has become the standard for cloud components? Docker. Let me explain what Docker actually is for those who might not know the gory technical details: Docker is essentially a sandwich of disk images where you can shove absolutely anything, and then these images get executed by running whatever legacy software you’ve crammed in there, regardless of how horrific or inconsistent it might be, with zero behavioral controls. Think about that for a moment. We’re building our entire cloud infrastructure around a concept of “component” that is completely undefined. A Docker component is literally “a disk image with whatever random stuff you decided to put inside it!” The only reason this approach gained traction is because it saves upload space compared to creating full virtual machines. That’s it. That’s the revolutionary innovation we’ve built our entire ecosystem around. From this fundamentally broken concept of componentization, we’ve constructed the entire Kubernetes ecosystem, which exists primarily to solve the problem of managing these semantically meaningless horrors by aggregating all possible kinds of digital garbage into something that resembles a functioning system. Why Kubernetes is inherently difficult This is precisely why Kubernetes is so notoriously difficult to work with. It’s not difficult because distributed systems are inherently complex (though they are). It’s difficult because Kubernetes is fundamentally an assembler of garbage. Every component in a Kubernetes cluster can violate any rule, consume unlimited memory, crash randomly, loop without control, and it’s entirely up to you to figure out how to handle these problems. There’s no semantic contract, no behavioral guarantees, no meaningful abstraction layer between your application logic and the chaos of whatever random processes are running inside those containers. Deploying a system on Kubernetes is always a nightmare of managing a collection of unhinged components that can explode at any moment for any reason. And we’ve somehow convinced ourselves that this is not only acceptable but represents the pinnacle of modern software engineering. The symptoms of this fundamental architectural problem are everywhere once you start looking. The endless YAML configuration files that nobody really understands. The “Kubernetes experts” who spend their time memorizing arcane incantations rather than solving actual business problems. The conferences where people present complex solutions to problems that shouldn’t exist in the first place. We’ve created a cargo cult around Kubernetes, where practitioners perform elaborate rituals (writing extensive Helm charts, configuring service meshes, tuning resource limits) without really understanding why these rituals are necessary or whether they’re actually solving the right problems. The truth is, most of these “solutions” are just band-aids covering up the fundamental issue: we’re trying to build reliable, predictable systems out of components that are inherently unreliable and unpredictable. Here’s where the “innovation bottleneck” part comes in When your deployment platform is this complex and fragile, innovation slows to a crawl. Teams spend more time fighting with Kubernetes configurations than they do building new features or solving customer problems. Startups that should be moving fast and breaking things instead spend weeks getting their deployment pipeline working. Enterprise teams create entire “platform engineering” departments whose primary job is to wrestle with the Kubernetes beast. The cognitive load is enormous, and the opportunity cost is staggering. Even worse, this complexity creates a false sense of sophistication. Teams feel like they’re doing “real engineering” because they’re dealing with complicated systems, but they’re actually just managing accidental complexity that shouldn’t exist in the first place. Lock-in effect? Kubernetes has also created a powerful lock-in effect that stifles competition and alternative approaches. Once you’ve invested heavily in Kubernetes expertise, tooling, and infrastructure, it becomes very difficult to consider alternatives, even when those alternatives might be objectively better. This lock-in effect is particularly insidious because it’s not just technical, it’s cultural and organizational. Companies hire “Kubernetes experts,” attend Kubernetes conferences, and build their engineering culture around Kubernetes concepts. Questioning Kubernetes becomes tantamount to questioning the technical competence of the entire organization. While we’re all busy learning the latest Kubernetes best practices and debugging mysterious pod failures, we’re missing out on genuine innovations in distributed computing. Languages and platforms designed for distributed systems from the ground up are being overshadowed by the Kubernetes hype cycle. Systems that provide true semantic abstractions for distributed computing – where you can reason about your application logic without getting bogged down in infrastructure details – are being ignored in favor of yet another Kubernetes management tool. The irony is that many of the problems Kubernetes claims to solve have already been solved more elegantly by other approaches. But because everyone’s invested in the Kubernetes ecosystem, these superior solutions remain niche and underexplored. So what’s the alternative? How do we break free from this cycle of complexity? First, we need to acknowledge that the emperor has no clothes. Kubernetes is not the inevitable result of distributed systems complexity: it’s the result of building an entire ecosystem around a fundamentally flawed abstraction. Second, we need to invest in better abstractions. WASI and WebAssembly represent one promising direction. Languages like Erlang and platforms like Unison show us what’s possible when distributed computing is baked into the design from the beginning. Third, we need to resist the urge to add more layers of complexity on top of Kubernetes. Every new “Kubernetes management” tool is just another band-aid on a fundamentally broken system. The uncomfortable truth The most uncomfortable truth about Kubernetes is that it’s successful not because it’s good, but because it’s good enough to solve the immediate problem of “how do we deploy these Docker containers at scale” while being complex enough to create an entire industry of consultants, conferences, and training courses around it. There’s a perverse incentive structure where the complexity of Kubernetes actually benefits many players in the ecosystem. Tool vendors can sell “Kubernetes management” solutions. Consultants can charge premium rates for Kubernetes expertise. Conference organizers have an endless supply of complex topics to fill their agendas. But this success comes at the cost of actual innovation. We’re optimizing for the wrong things, and we’re all paying the price in terms of developer productivity, system reliability, and the pace of genuine technological progress. Breaking free I know this critique won’t be popular in certain circles. Kubernetes has a lot of mindshare, a lot of investment, and a lot of smart people working on it. But sometimes the hardest thing to do is admit that we’ve been going down the wrong path, no matter how much we’ve already invested in that path. The cloud computing revolution promised us simplicity, scalability, and rapid iteration. Instead, Kubernetes has given us complexity, fragility, and endless configuration management. We can do better, but only if we’re willing to acknowledge the fundamental problems with our current approach. The future of cloud computing shouldn’t be about managing the chaos of arbitrary containers. It should be about building systems where good behavior is the default, where semantic guarantees are built in, and where developers can focus on solving real problems instead of wrestling with deployment complexity. Until we’re ready to have that conversation honestly, Kubernetes will continue to be the colossal innovation bottleneck that it is today, dressed up in the clothes of progress and sold as the solution to problems it largely created in the first place.",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv aria-label=\"Content\"\u003e\n        \u003carticle itemscope=\"\" itemtype=\"http://schema.org/BlogPosting\"\u003e\n\n  \n\n  \u003cdiv itemprop=\"articleBody\"\u003e\n    \u003cp\u003eToday I decided to run a little experiment.\nI asked an AI to write a brutally honest, no-holds-barred critique of Kubernetes, something really spicy that would make the container orchestration crowd squirm in their ergonomic office chairs.\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003e“Make it controversial,”\u003c/em\u003e I said.\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003e“Make it provocative. Really lean into the contrarian takes.”\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eSo everything you’re about to read?\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eThose aren’t technically my opinions.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eThey’re the unfiltered thoughts of an artificial intelligence that doesn’t have to worry about getting uninvited from \u003cem\u003eKubeCon\u003c/em\u003e or losing consulting gigs with companies heavily invested in the K8s ecosystem.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003e…Or are they?\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003eLet me start with a confession: I think Kubernetes is one of the biggest obstacles to genuine innovation in cloud computing today. There, I said it. Before you grab your pitchforks and start composing angry tweets about how K8s “orchestrates at scale” and “democratizes container deployment,” hear me out.\u003c/p\u003e\n\n\u003cp\u003eBecause underneath all that YAML wizardry and those impressive-sounding conferences lies what I believe is a fundamental architectural mistake that’s holding back the entire industry.\u003c/p\u003e\n\n\u003ch2 id=\"the-real-problem-we-should-be-solving\"\u003eThe Real Problem we should be solving\u003c/h2\u003e\n\n\u003cp\u003eThe fundamental challenge of cloud computing isn’t actually that complicated when you strip away all the vendor marketing and ecosystem noise. It boils down to this: \u003cstrong\u003eHow do we create software components for distributed computing that can be easily deployed in the cloud?\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eThis isn’t a new problem, by the way. We’ve seen similar challenges before in the history of computing. \n\u003cstrong\u003eRemember \u003ca href=\"https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture\"\u003eCORBA\u003c/a\u003e?\u003c/strong\u003e Yeah, that distributed object standard from the ’90s that tried to solve interoperability between different systems? It was attempting to tackle a similar conceptual challenge, just in a different era with different constraints.\u003c/p\u003e\n\n\u003cp\u003eWhen I think about how cloud deployment \u003cem\u003eshould\u003c/em\u003e work, I imagine something beautifully simple: you write your code, and then you send it “up” to the cloud with as few intermediate steps as possible. Ideally, instantaneously. Write code, deploy code. Done.\u003c/p\u003e\n\n\u003cp\u003eNotice I’m not talking about testing or security here – those are separate concerns. Having an instantaneous deployment system wouldn’t prevent you from adding subsequent phases of hardening and validation. In fact, it would probably help those processes too, because you’d have a cleaner, more predictable foundation to build upon.\u003c/p\u003e\n\n\u003ch2 id=\"what-the-ideal-world-looks-like\"\u003eWhat the Ideal World looks like\u003c/h2\u003e\n\n\u003cp\u003eIn my ideal world, we’d have systems similar to what already exists for languages like Erlang or Unison: languages that were designed from the ground up with distributed computing in mind. But we’d have this capability available for \u003cem\u003eall\u003c/em\u003e programming languages, not just the lucky few.\u003c/p\u003e\n\n\u003cp\u003eAnd here’s the kicker: universal components that work across all languages already exist. \nLook at \u003ca href=\"https://en.wikipedia.org/wiki/WebAssembly#WASI\"\u003eWASI\u003c/a\u003e (WebAssembly System Interface), components built on WebAssembly that provide a platform-independent, efficient format that any language can compile to. This would be my dream solution: write your code in whatever language you prefer, compile your microservices into semantically meaningful components using a platform-independent but efficient format, and deploy them universally.\u003c/p\u003e\n\n\u003cp\u003eThis approach is remarkably similar to what’s being done today with systems like Apache OpenServerless. These platforms actually get it right, focusing on the logical components and their behavior rather than getting bogged down in infrastructure minutiae.\u003c/p\u003e\n\n\u003ch2 id=\"the-docker-disaster\"\u003eThe \u003cem\u003eDocker Disaster\u003c/em\u003e\u003c/h2\u003e\n\n\u003cp\u003eBut instead of this elegant vision, what has become the standard for cloud components? \u003cstrong\u003eDocker\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg width=\"400px\" src=\"https://github.com/user-attachments/assets/97beca5c-664f-4161-a507-78e006792049\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eLet me explain what Docker actually is for those who might not know the gory technical details:\u003c/p\u003e\n\n\u003cp\u003eDocker is essentially a sandwich of disk images where you can shove absolutely anything, and then these images get executed by running whatever legacy software you’ve crammed in there, regardless of how horrific or inconsistent it might be, \u003cem\u003ewith zero behavioral controls.\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThink about that for a moment. We’re building our entire cloud infrastructure around a concept of “\u003cem\u003ecomponent\u003c/em\u003e” that is completely undefined. A Docker component is literally \u003cem\u003e“a disk image with whatever random stuff you decided to put inside it!”\u003c/em\u003e \nThe only reason this approach gained traction is because it saves upload space compared to creating full virtual machines. That’s it. That’s the revolutionary innovation we’ve built our entire ecosystem around.\u003c/p\u003e\n\n\u003cp\u003eFrom this fundamentally broken concept of componentization, we’ve constructed the entire Kubernetes ecosystem, which exists primarily to solve the problem of managing these semantically meaningless horrors by aggregating all possible kinds of digital garbage into something that resembles a functioning system.\u003c/p\u003e\n\n\u003ch2 id=\"why-kubernetes-is-inherently-difficult\"\u003eWhy Kubernetes is inherently difficult\u003c/h2\u003e\n\n\u003cp\u003eThis is precisely why Kubernetes is so notoriously difficult to work with. It’s not difficult because distributed systems are inherently complex (though they are). \nIt’s difficult because \u003cstrong\u003eKubernetes is fundamentally an assembler of garbage\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eEvery component in a Kubernetes cluster can violate any rule, consume unlimited memory, crash randomly, loop without control, and it’s entirely up to you to figure out how to handle these problems. There’s no semantic contract, no behavioral guarantees, no meaningful abstraction layer between your application logic and the chaos of whatever random processes are running inside those containers.\u003c/p\u003e\n\n\u003cp\u003eDeploying a system on Kubernetes is always a nightmare of managing a collection of unhinged components that can explode at any moment for any reason. And we’ve somehow convinced ourselves that this is not only acceptable but represents the pinnacle of modern software engineering.\u003c/p\u003e\n\n\u003cp\u003eThe symptoms of this fundamental architectural problem are everywhere once you start looking. The endless YAML configuration files that nobody really understands. The “Kubernetes experts” who spend their time memorizing arcane incantations rather than solving actual business problems. The conferences where people present complex solutions to problems that shouldn’t exist in the first place.\u003c/p\u003e\n\n\u003cp\u003eWe’ve created a cargo cult around Kubernetes, where practitioners perform elaborate rituals (writing extensive Helm charts, configuring service meshes, tuning resource limits) without really understanding why these rituals are necessary or whether they’re actually solving the right problems.\u003c/p\u003e\n\n\u003cp\u003eThe truth is, most of these “\u003cem\u003esolutions\u003c/em\u003e” are just band-aids covering up the fundamental issue: we’re trying to build reliable, predictable systems out of components that are inherently unreliable and unpredictable.\u003c/p\u003e\n\n\u003ch2 id=\"heres-where-the-innovation-bottleneck-part-comes-in\"\u003eHere’s where the “\u003cem\u003einnovation bottleneck\u003c/em\u003e” part comes in\u003c/h2\u003e\n\n\u003cp\u003eWhen your deployment platform is this complex and fragile, innovation slows to a crawl. Teams spend more time fighting with Kubernetes configurations than they do building new features or solving customer problems.\u003c/p\u003e\n\n\u003cp\u003eStartups that should be moving fast and breaking things instead spend weeks getting their deployment pipeline working. Enterprise teams create entire “platform engineering” departments whose primary job is to wrestle with the Kubernetes beast. The cognitive load is enormous, and the opportunity cost is staggering.\u003c/p\u003e\n\n\u003cp\u003eEven worse, this complexity creates a false sense of sophistication. Teams feel like they’re doing “real engineering” because they’re dealing with complicated systems, but they’re actually just managing accidental complexity that shouldn’t exist in the first place.\u003c/p\u003e\n\n\u003ch2 id=\"lock-in-effect\"\u003eLock-in effect?\u003c/h2\u003e\n\n\u003cp\u003eKubernetes has also created a powerful lock-in effect that stifles competition and alternative approaches. Once you’ve invested heavily in Kubernetes expertise, tooling, and infrastructure, it becomes very difficult to consider alternatives, even when those alternatives might be objectively better.\u003c/p\u003e\n\n\u003cp\u003eThis lock-in effect is particularly insidious because it’s not just technical, it’s cultural and organizational. Companies hire “Kubernetes experts,” attend Kubernetes conferences, and build their engineering culture around Kubernetes concepts. Questioning Kubernetes becomes tantamount to questioning the technical competence of the entire organization.\u003c/p\u003e\n\n\u003cp\u003eWhile we’re all busy learning the latest Kubernetes best practices and debugging mysterious pod failures, we’re missing out on genuine innovations in distributed computing. Languages and platforms designed for distributed systems from the ground up are being overshadowed by the Kubernetes hype cycle.\u003c/p\u003e\n\n\u003cp\u003eSystems that provide true semantic abstractions for distributed computing – where you can reason about your application logic without getting bogged down in infrastructure details – are being ignored in favor of yet another Kubernetes management tool.\u003c/p\u003e\n\n\u003cp\u003eThe irony is that many of the problems Kubernetes claims to solve have already been solved more elegantly by other approaches. But because everyone’s invested in the Kubernetes ecosystem, these superior solutions remain niche and underexplored.\u003c/p\u003e\n\n\u003ch2 id=\"so-whats-the-alternative-how-do-we-break-free-from-this-cycle-of-complexity\"\u003eSo what’s the alternative? How do we break free from this cycle of complexity?\u003c/h2\u003e\n\n\u003cp\u003eFirst, we need to acknowledge that the emperor has no clothes. Kubernetes is not the inevitable result of distributed systems complexity: it’s the result of building an entire ecosystem around a fundamentally flawed abstraction.\u003c/p\u003e\n\n\u003cp\u003eSecond, we need to invest in better abstractions. WASI and WebAssembly represent one promising direction. Languages like Erlang and platforms like Unison show us what’s possible when distributed computing is baked into the design from the beginning.\u003c/p\u003e\n\n\u003cp\u003eThird, we need to resist the urge to add more layers of complexity on top of Kubernetes. Every new “Kubernetes management” tool is just another band-aid on a fundamentally broken system.\u003c/p\u003e\n\n\u003ch2 id=\"the-uncomfortable-truth\"\u003eThe uncomfortable truth\u003c/h2\u003e\n\n\u003cp\u003eThe most uncomfortable truth about Kubernetes is that it’s successful not because it’s good, but because it’s good enough to solve the immediate problem of \u003cem\u003e“how do we deploy these Docker containers at scale”\u003c/em\u003e while being complex enough to create an entire industry of consultants, conferences, and training courses around it.\u003c/p\u003e\n\n\u003cp\u003eThere’s a perverse incentive structure where the complexity of Kubernetes actually benefits many players in the ecosystem. Tool vendors can sell “Kubernetes management” solutions. Consultants can charge premium rates for Kubernetes expertise. Conference organizers have an endless supply of complex topics to fill their agendas.\u003c/p\u003e\n\n\u003cp\u003eBut this success comes at the cost of actual innovation. We’re optimizing for the wrong things, and we’re all paying the price in terms of developer productivity, system reliability, and the pace of genuine technological progress.\u003c/p\u003e\n\n\u003ch2 id=\"breaking-free\"\u003eBreaking free\u003c/h2\u003e\n\n\u003cp\u003eI know this critique won’t be popular in certain circles. Kubernetes has a lot of mindshare, a lot of investment, and a lot of smart people working on it. But sometimes the hardest thing to do is admit that we’ve been going down the wrong path, no matter how much we’ve already invested in that path.\u003c/p\u003e\n\n\u003cp\u003eThe cloud computing revolution promised us simplicity, scalability, and rapid iteration. Instead, Kubernetes has given us complexity, fragility, and endless configuration management. We can do better, but only if we’re willing to acknowledge the fundamental problems with our current approach.\u003c/p\u003e\n\n\u003cp\u003eThe future of cloud computing shouldn’t be about managing the chaos of arbitrary containers. It should be about building systems where good behavior is the default, where semantic guarantees are built in, and where developers can focus on solving real problems instead of wrestling with deployment complexity.\u003c/p\u003e\n\n\u003cp\u003eUntil we’re ready to have that conversation honestly, Kubernetes will continue to be the colossal innovation bottleneck that it is today, dressed up in the clothes of progress and sold as the solution to problems it largely created in the first place.\u003c/p\u003e\n\n  \u003c/div\u003e\n\u003c/article\u003e\n\n      \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2025-06-20T00:00:00Z",
  "modifiedTime": null
}
