{
  "id": "6764605d-4090-41f9-aca2-b8841ac9a681",
  "title": "AI2 closes the gap between closed-source and open-source post-training",
  "link": "https://venturebeat.com/ai/ai2-closes-the-gap-between-closed-source-and-open-source-post-training/",
  "description": "Ai2 released Tülu 3, a model that makes fine-tuning open-source LLMs easier and get its performance closer to closed LLMs like GPT-4o.",
  "author": "Emilia David",
  "published": "Sat, 23 Nov 2024 00:02:27 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Data Infrastructure",
    "AI, ML and Deep Learning",
    "AI2",
    "Allen Institute for Artificial Intelligence",
    "Anthropic",
    "ChatGPT",
    "Claude",
    "data",
    "datasets",
    "enterprises",
    "fine tuning",
    "fine-tuning",
    "Open source",
    "open source AI",
    "open-source AI",
    "OpenAI",
    "Tulu 3"
  ],
  "byline": "Emilia David",
  "length": 3926,
  "excerpt": "Ai2 released Tülu 3, a model that makes fine-tuning open-source LLMs easier and get its performance closer to closed LLMs like GPT-4o.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "November 22, 2024 4:02 PM Credit: VentureBeat made with OpenAI ChatGPT Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More The Allen Institute for AI (Ai2) claims to have narrowed the gap between closed-source and open-sourced post-training with the release of its new model training family, Tülu 3, bringing the argument that open-source models will thrive in the enterprise space.  Tülu 3 brings open-source models up to par with OpenAI’s GPT models, Claude from Anthropic and Google’s Gemini. It allows researchers, developers and enterprises to fine-tune open-source models without losing data and core skills of the model and get it close to the quality of closed-source models.  Ai2 said it released Tülu 3 with all of the data, data mixes, recipes, code, infrastructure and evaluation frameworks. The company needed to create new datasets and training methods to improve Tülu’s performance, including “training directly on verifiable problems with reinforcement learning.” “Our best models result from a complex training process that integrates partial details from proprietary methods with novel techniques and established academic research,” Ai2 said in a blog post. “Our success is rooted in careful data curation, rigorous experimentation, innovative methodologies and improved training infrastructure.” Tülu 3 will be available in a range of sizes.  Open-source for enterprises Open-source models often lagged behind closed-sourced models in enterprise adoption, although more companies anecdotally reported choosing more open-source large language models (LLMs) for projects.  Ai2’s thesis is that improving fine-tuning with open-source models like Tülu 3 will increase the number of enterprises and researchers picking open-source models because they can be confident it can perform as well as a Claude or Gemini.  The company points out that Tülu 3 and Ai2’s other models are fully open source, noting that big model trainers like Anthropic and Meta, who claim to be open source, have “none of their training data nor training recipes are transparent to users.” The Open Source Initiative recently published the first version of its open-source AI definition, but some organizations and model providers don’t fully follow the definition in their licenses.  Enterprises care about the transparency of models, but many choose open-source models not so much for research or data openness but because it’s the best fit for their use cases.  Tülu 3 offers enterprises more of a choice when looking for open-source models to bring into their stack and fine-tune with their data.  Ai2’s other models, OLMoE and Molmo, are also open source which the company said has started to outperform other leading models like GPT-4o and Claude.  Other Tülu 3 features Ai2 said Tülu 3 lets companies mix and match their data during fine-tuning.  “The recipes help you balance the datasets, so if you want to build a model that can code, but also follow instructions precisely and speak in multiple languages, you just select the particular datasets and follow the steps in the recipe,” Ai2 said.  Mixing and matching datasets can make it easier for developers to move from a smaller model to a larger weighted one and keep its post-training settings. The company said the infrastructure code it released with Tülu 3 allows enterprises to build out that pipeline when moving through model sizes.  The evaluation framework from Ai2 offers a way for developers to specify settings in what they want to see out of the model.  VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/08/robot-fine-tuning.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-11-23T00:02:27+00:00\" datetime=\"2024-11-23T00:02:27+00:00\"\u003eNovember 22, 2024 4:02 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"429\" src=\"https://venturebeat.com/wp-content/uploads/2024/08/robot-fine-tuning.png?w=750\" alt=\"AI golden age sci-fi style art of humanoid robot wearing glasses repairing watch with small. jeweler\u0026#39;s screwdriver\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with OpenAI ChatGPT\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://allenai.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eThe Allen Institute for AI (Ai2)\u003c/a\u003e claims to have narrowed the gap between closed-source and open-sourced post-training with the release of its new model training family, Tülu 3, bringing the argument that open-source models will thrive in the enterprise space. \u003c/p\u003e\n\n\n\n\u003cp\u003eTülu 3 brings open-source models up to par with OpenAI’s GPT models, Claude from Anthropic and Google’s Gemini. It allows researchers, developers and enterprises to fine-tune open-source models without losing data and core skills of the model and get it close to the quality of closed-source models. \u003c/p\u003e\n\n\n\n\u003cp\u003eAi2 said it released Tülu 3 with all of the data, data mixes, recipes, code, infrastructure and evaluation frameworks. The company needed to create new datasets and training methods to improve Tülu’s performance, including “training directly on verifiable problems with reinforcement learning.”\u003c/p\u003e\n\n\n\n\u003cp\u003e“Our best models result from a complex training process that integrates partial details from proprietary methods with novel techniques and established academic research,” Ai2 said in a \u003ca href=\"https://allenai.org/blog/tulu-3?includeDrafts\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eblog post\u003c/a\u003e. “Our success is rooted in careful data curation, rigorous experimentation, innovative methodologies and improved training infrastructure.”\u003c/p\u003e\n\n\n\n\u003cp\u003eTülu 3 will be available in a range of sizes. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfbLmOW854LIJsBXl5hUOXyce55Pz3dzbWTKuB4QzxO4j8UH4g17Ty4vylF3Xjpdbc2gcnhLz3-OTykXaqkEMv01TWnljb3kYxmHGzrFsHSY8IwhOl2F87V17VhwLh4t_MYg4ln?key=MCKwtUsTpoVRLW6DsYkYVAVD\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-open-source-for-enterprises\"\u003eOpen-source for enterprises\u003c/h2\u003e\n\n\n\n\u003cp\u003eOpen-source models often lagged behind closed-sourced models in enterprise adoption, although more companies \u003ca href=\"https://venturebeat.com/ai/the-enterprise-verdict-on-ai-models-why-open-source-will-win/\"\u003eanecdotally reported choosing more open-source large language models (LLMs\u003c/a\u003e) for projects. \u003c/p\u003e\n\n\n\n\u003cp\u003eAi2’s thesis is that improving fine-tuning with open-source models like Tülu 3 will increase the number of enterprises and researchers \u003ca href=\"https://venturebeat.com/security/what-open-source-ai-models-should-your-enterprise-use-endor-labs-analyzes-them-all/\"\u003epicking open-source models\u003c/a\u003e because they can be confident it can perform as well as a Claude or Gemini. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe company points out that Tülu 3 and Ai2’s other models are fully open source, noting that big model trainers like Anthropic and Meta, who claim to be open source, have “none of their training data nor training recipes are transparent to users.” The Open Source Initiative recently published the first version of its \u003ca href=\"https://opensource.org/ai/open-source-ai-definition\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eopen-source AI definition\u003c/a\u003e, but some organizations and model providers don’t fully follow the definition in their licenses. \u003c/p\u003e\n\n\n\n\u003cp\u003eEnterprises care about the transparency of models, but many choose open-source models not so much for research or data openness but because it’s the best fit for their use cases. \u003c/p\u003e\n\n\n\n\u003cp\u003eTülu 3 offers enterprises more of a choice when looking for open-source models to bring into their stack and fine-tune with their data. \u003c/p\u003e\n\n\n\n\u003cp\u003eAi2’s other models, \u003ca href=\"https://venturebeat.com/ai/ai2s-new-model-aims-to-be-open-and-powerful-yet-cost-effective/\"\u003eOLMoE\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/ai2s-new-molmo-open-source-ai-models-beat-gpt-4o-claude-on-some-benchmarks/\"\u003eMolmo\u003c/a\u003e, are also open source which the company said has started to outperform other leading models like GPT-4o and Claude. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-other-tulu-3-features\"\u003eOther Tülu 3 features\u003c/h2\u003e\n\n\n\n\u003cp\u003eAi2 said Tülu 3 lets companies mix and match their data during fine-tuning. \u003c/p\u003e\n\n\n\n\u003cp\u003e“The recipes help you balance the datasets, so if you want to build a model that can code, but also follow instructions precisely and speak in multiple languages, you just select the particular datasets and follow the steps in the recipe,” Ai2 said. \u003c/p\u003e\n\n\n\n\u003cp\u003eMixing and matching datasets can make it easier for developers to move from a smaller model to a larger weighted one and keep its post-training settings. The company said the infrastructure code it released with Tülu 3 allows enterprises to build out that pipeline when moving through model sizes. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe evaluation framework from Ai2 offers a way for developers to specify settings in what they want to see out of the model. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-11-23T00:02:27Z",
  "modifiedTime": "2024-11-23T00:02:36Z"
}
