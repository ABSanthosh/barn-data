{
  "id": "70ac6589-ad6c-4629-8d5a-1b662ad03728",
  "title": "GPUHammer: Rowhammer attacks on GPU memories are practical",
  "link": "https://gpuhammer.com/",
  "description": "Article URL: https://gpuhammer.com/ Comments URL: https://news.ycombinator.com/item?id=44577268 Points: 32 # Comments: 12",
  "author": "jonbaer",
  "published": "Wed, 16 Jul 2025 00:05:45 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "",
  "length": 9887,
  "excerpt": "",
  "siteName": "GPUHammer",
  "favicon": "https://gpuhammer.com/apple-touch-icon.png",
  "text": "GPUHammer: Rowhammer Attacks on GPU Memories are Practical # Chris (Shaopeng) Lin‚Ä†, Joyce Qu‚Ä†, Gururaj Saileshwar, from University of Toronto Published at USENIX Security 2025 (link to paper). Artifact available on GitHub and Zenodo. ‚Ä† equal contribution TL;DR # GPUHammer is the first attack to show Rowhammer bit flips on GPU memories, specifically on a GDDR6 memory in an NVIDIA A6000 GPU. Our attacks induce bit flips across all tested DRAM banks, despite in-DRAM defenses like TRR, using user-level CUDA code. These bit flips allow a malicious GPU user to tamper with another user‚Äôs data on the GPU in shared, time-sliced environments. In a proof-of-concept, we use these bit flips to tamper with a victim‚Äôs DNN models and degrade model accuracy from 80% to 0.1%, using a single bit flip. Enabling Error Correction Codes (ECC) can mitigate this risk, but ECC can introduce up to a 10% slowdown for ML inference workloads on an A6000 GPU. üîç What‚Äôs New in Rowhammer for GPUs? # Rowhammer is a hardware vulnerability where rapidly activating a memory row introduces bit flips in adjacent memory rows. Since 2014, this vulnerability has been widely studied in CPUs and in CPU-based memories like DDR3, DDR4, and LPDDR4. However, with critical AI and ML workloads now running on discrete GPUs in the cloud, it is vital to assess the vulnerability of GPU memories to Rowhammer attacks. Rowhammer is uniquely more challenging on GPU-based GDDR memories for the following reasons: ‚è±Ô∏è GDDR6 has higher latency and faster refresh than CPU-based DDR4, making hammering harder. üß© Unknown DRAM address mappings in GDDR memories complicate crafting effective patterns. üõ°Ô∏è In-DRAM mitigations in GDDR are opaque and undocumented. Despite this, GPUHammer overcomes these barriers and launches successful attacks on GDDR6. Step 1: Reverse Engineering GPU DRAM Mappings # To craft effective Rowhammer attacks, we first need to identify memory addresses that map to the same DRAM bank on an NVIDIA GPU. Unlike CPUs, NVIDIA GPUs do not expose the physical addresses to user-level CUDA code, making it challenging to identify and hammer DRAM rows in the same bank. However, observing that the NVIDIA GPU driver consistently maps virtual memory to the same physical memory, we reverse engineer the virtual memory offsets to DRAM bank mappings. Inspired by the DRAMA technique, we use timing differences between memory accesses to the same bank vs. different banks. One key obstacle, as shown in Fig. 1, is the Non-Uniform Memory Access (NUMA) effect in memory access latencies for addresses accessed in pairs, which makes it hard to pinpoint same-bank addresses. Based on the insight that addresses in the same DRAM bank must have similar latency when accessed in isolation, we filter out such addresses contributing to the NUMA effects and clearly identify addresses mapping to the same DRAM bank (see Fig. 2). This accurately identifies same-bank address pairs needed for crafting Rowhammer patterns. Fig. 1: Naively accessing pairs of addresses: different-bank latencies (320-370ns) overlap with same-bank latencies (370-380ns) due to NUMA effects. Fig 2: After filtering addresses with different latencies when accessed in isolation, same and different bank address pairs are distinguishable. Step-2: Maximizing Hammering Intensity # GPU memory accesses are up to 4√ó slower slower than CPUs, making it hard to reach the activation rate needed for Rowhammer attacks using naive, single-threaded hammering like in CPUs (Fig. 3, green line). To overcome this, we exploit the GPU‚Äôs SIMT parallelism, launching multiple threads and warps in parallel. This multi-threaded, multi-warp approach (Fig. 3, orange line) eliminates idle time in the memory controller and reaches the maximum possible hammering rate. Fig. 4 shows how these strategies maximize memory controller utilization. Fig. 3: Number of activations in a single refresh period (tREFW) for single-thread, multi-thread, and multi-warp hammering. Fig. 4: Memory controller utilization with (a) single-thread, (b) multi-thread, and (c) multi-warp hammering. Multi-warp hammering minimizes idle time, maximizing activation rates. Step-3: Synchronizing to Refreshes, Defeating Mitigations # Prior works like SMASH and BlackSmith show that synchronizing hammering to refreshes (REF) is key to bypassing in-DRAM defenses. However, CUDA‚Äôs synchronization primitives (like __syncthreads()) used to synchronize threads can reorder warp execution. Instead, we use implicit per-warp delays, aligned such that REF commands overlap with our inserted delays, to align our hammering with refreshes and defeat in-DRAM mitigations like TRR while preserving warp execution order. Fig. 5: Per-warp delays inserted using adds for synchronizing to REF. When the delays overlap, the REF is inserted in alignment with the hammering pattern. üí• What Did We Break? # We ran GPUHammer on an NVIDIA RTX A6000 (48 GB GDDR6) across four DRAM banks and observed 8 distinct single-bit flips, and bit-flips across all tested banks (see Fig. 6). The minimum activation count ( TRH) to induce a flip was ~12K, consistent with prior DDR4 findings. Using these flips, we performed the first ML accuracy degradation attack using Rowhammer on a GPU. Prior work shows that flipping the most significant bit of a floating-point exponent in FP16 model weights can drastically reduce model accuracy. Based on this insight, we show that in a time-shared GPU setup, an attacker can position victim data into vulnerable DRAM rows via memory massaging and force the bit flips at such locations. In our proof-of-concept (Fig. 7), with just a single bit flip, the accuracy of an ML model is degraded below 1% for all five tested ImageNet DNN models, resulting in up to 80% accuracy loss. Fig. 6: Number of bit-flips in 4 banks on RTX A6000 with GDDR6. We observed bit-flips on each bank. Fig. 7: Accuracy degradation attack on ImageNet models on NVIDIA A6000 GPU. We report the top-1 / top-5 accuracy without (Base Acc) and with (Degraded Acc) the bit-flip, and the Relative Accuracy Drop (RAD) for the top-1 accuracy. ‚ùì FAQs # What GPUs are vulnerable? Am I affected? # We confirmed Rowhammer bit flips on NVIDIA A6000 GPUs with GDDR6 memory. Other GDDR6 GPUs, such as the RTX 3080, did not show bit flips in our tests, possibly due to variations in DRAM vendor, chip characteristics, or operating conditions like temperature. We also observed no flips on an A100 GPU with HBM memory. Why test so few GPUs? Isn‚Äôt that a small sample size? # Unlike CPUs, where DRAM modules can be easily swapped out for testing, GPU DRAM is soldered in, making large-scale testing expensive (GPUs can cost thousands of dollars). Our attack code is extensible to other Ampere GPUs and beyond, and we encourage future work to expand the test coverage. How can GPUHammer be mitigated? # Admins can mitigate GPUHammer by enabling ECC via nvidia-smi -e 1 (followed by a reboot). All observed bit flips so far are single-bit, which ECC can correct. However, enabling ECC may reduce performance by up to 10% and memory capacity by 6.25% on the A6000. Still, this doesn‚Äôt eliminate the root cause, a hardware flaw, which would require redesigning GDDR6 with principled mitigations like PRAC, which has been proposed for DDR5 and beyond, or probabilistic mitigations like PRIDE. Are newer GPUs like the H100 or RTX 5090 affected? # Currently, no. H100 (HBM3) and RTX 5090 (GDDR7) feature on-die ECC, which likely masks single-bit flips. However, future Rowhammer patterns causing multi-bit flips may bypass such ECC, as shown in attacks like ECCploit. Did you disclose this to NVIDIA? What was their response? # Yes. We responsibly disclosed GPUHammer to NVIDIA on January 15, 2025, and also to major cloud service providers (AWS, Azure, GCP, etc.). NVIDIA confirmed the issue and recommended enabling ECC as a mitigation. Further Information # Please refer to our paper, which appears at USENIX Security 2025. The artifacts are available on GitHub and Zenodo. To cite our paper, please use: @inproceedings{lin2025gpuhammer, author = {Chris S. Lin and Joyce Qu and Gururaj Saileshwar}, title = {GPUHammer: Rowhammer Attacks on GPU Memories are Practical}, publisher = {USENIX Association}, booktitle = {Proceedings of the 34th USENIX Conference on Security Symposium}, year = {2025}, series = {SEC '25}, address = {USA}, location = {Seattle, WA, USA}, } Acknowledgements # This research was supported by Natural Sciences and Engineering Research Council of Canada (NSERC) under funding reference number RGPIN-2023-04796, and an NSERC-CSE Research Communities Grant under funding reference number ALLRP-588144-23. Any research, opinions, or positions expressed in this work are solely those of the authors and do not represent the official views of NSERC, the Communications Security Establishment Canada, or the Government of Canada.",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\n  \n  \u003csection\u003e\u003cp\u003e\n  \u003cimg src=\"https://gpuhammer.com/gpuhammer_icon.png\" alt=\"gpuhammer icon\" width=\"20%\"/\u003e\n\u003c/p\u003e\n\n\u003ch2\u003eGPUHammer: Rowhammer Attacks on GPU Memories are Practical \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#gpuhammer-rowhammer-attacks-on-gpu-memories-are-practical\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eChris (Shaopeng) Lin\u003csup\u003e‚Ä†\u003c/sup\u003e, Joyce Qu\u003csup\u003e‚Ä†\u003c/sup\u003e, Gururaj Saileshwar, \u003cem\u003efrom University of Toronto\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePublished at \u003cem\u003eUSENIX Security 2025\u003c/em\u003e (link to \u003ca href=\"https://gururaj-s.github.io/assets/pdf/SEC25_GPUHammer.pdf\" target=\"_blank\"\u003epaper\u003c/a\u003e). Artifact available on \u003ca href=\"https://github.com/sith-lab/gpuhammer\" target=\"_blank\"\u003eGitHub\u003c/a\u003e and \u003ca href=\"https://zenodo.org/records/15694512\" target=\"_blank\"\u003eZenodo\u003c/a\u003e.\n\u003csup\u003e‚Ä†\u003c/sup\u003e equal contribution\u003c/p\u003e\n\u003chr/\u003e\n\n\n\u003ch2\u003eTL;DR \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#tldr\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://gururaj-s.github.io/assets/pdf/SEC25_GPUHammer.pdf\" target=\"_blank\"\u003eGPUHammer\u003c/a\u003e is the first attack to show \u003cstrong\u003eRowhammer bit flips on GPU memories\u003c/strong\u003e, specifically on a \u003cstrong\u003eGDDR6 memory in an NVIDIA A6000 GPU\u003c/strong\u003e. Our attacks \u003cstrong\u003einduce bit flips across all tested DRAM banks\u003c/strong\u003e, despite in-DRAM defenses like TRR, using user-level CUDA code. These bit flips allow a malicious GPU user to \u003cstrong\u003etamper with another user‚Äôs data on the GPU\u003c/strong\u003e in shared, time-sliced environments. In a proof-of-concept, we use these bit flips to tamper with a victim‚Äôs DNN models and \u003cstrong\u003edegrade model accuracy from 80% to 0.1%, using a single bit flip\u003c/strong\u003e. Enabling Error Correction Codes (ECC) can mitigate this risk, but ECC can introduce up to a 10% slowdown for ML inference workloads on an A6000 GPU.\u003c/p\u003e\n\n\n\u003ch2\u003eüîç What‚Äôs New in Rowhammer for GPUs? \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#-whats-new-in-rowhammer-for-gpus\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://users.ece.cmu.edu/~yoonguk/papers/kim-isca14.pdf\" target=\"_blank\"\u003eRowhammer\u003c/a\u003e is a hardware vulnerability where rapidly activating a memory row introduces bit flips in adjacent memory rows.\nSince 2014, this vulnerability has been widely studied in CPUs and in CPU-based memories like DDR3, DDR4, and LPDDR4.\nHowever, with critical AI and ML workloads now running on discrete GPUs in the cloud, it is vital to assess the vulnerability of GPU memories to Rowhammer attacks.\u003c/p\u003e\n\u003cp\u003eRowhammer is uniquely more challenging on GPU-based GDDR memories for the following reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e‚è±Ô∏è  GDDR6 has higher latency and faster refresh than CPU-based DDR4, making hammering harder.\u003c/li\u003e\n\u003cli\u003eüß© Unknown DRAM address mappings in GDDR memories complicate crafting effective patterns.\u003c/li\u003e\n\u003cli\u003eüõ°Ô∏è  In-DRAM mitigations in GDDR are opaque and undocumented.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDespite this, GPUHammer overcomes these barriers and launches successful attacks on GDDR6.\u003c/p\u003e\n\n\n\u003ch3\u003eStep 1: Reverse Engineering GPU DRAM Mappings \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#step-1-reverse-engineering-gpu-dram-mappings\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eTo craft effective Rowhammer attacks, we first need to identify memory addresses that map to the same DRAM bank on an NVIDIA GPU. Unlike CPUs, NVIDIA GPUs do not expose the physical addresses to user-level CUDA code, making it challenging to identify and hammer DRAM rows in the same bank. However, observing that the NVIDIA GPU driver consistently maps virtual memory to the same physical memory, we reverse engineer the virtual memory offsets to DRAM bank mappings. Inspired by the \u003ca href=\"https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_pessl.pdf\" target=\"_blank\"\u003eDRAMA\u003c/a\u003e technique, we use timing differences between memory accesses to the same bank vs. different banks.\u003c/p\u003e\n\u003cp\u003eOne key obstacle, as shown in \u003cstrong\u003eFig. 1\u003c/strong\u003e, is the \u003cstrong\u003eNon-Uniform Memory Access (NUMA)\u003c/strong\u003e effect in memory access latencies for addresses accessed in pairs, which makes it hard to pinpoint same-bank addresses. Based on the insight that addresses in the same DRAM bank must have similar latency when accessed  in isolation, we filter out such addresses contributing to the NUMA effects and clearly identify addresses mapping to the same DRAM bank (see \u003cstrong\u003eFig. 2\u003c/strong\u003e). This accurately identifies same-bank address pairs needed for crafting Rowhammer patterns.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003cp\u003e\u003cstrong\u003eFig. 1:\u003c/strong\u003e  Naively accessing pairs of addresses: different-bank latencies (320-370ns) overlap with same-bank latencies (370-380ns) due to NUMA effects.\u003c/p\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cp\u003e\u003cstrong\u003eFig 2:\u003c/strong\u003e After filtering addresses with different latencies when accessed in isolation, same and different bank address pairs are distinguishable.  \u003c/p\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cimg src=\"https://gpuhammer.com/fig1.png\" alt=\"fig1\"/\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003cimg src=\"https://gpuhammer.com/fig2.png\" alt=\"fig2\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch3\u003eStep-2: Maximizing Hammering Intensity \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#step-2-maximizing-hammering-intensity\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eGPU memory accesses are up to \u003ca href=\"https://chipsandcheese.com/p/measuring-gpu-memory-latency\" target=\"_blank\"\u003e4√ó slower\u003c/a\u003e slower than CPUs, making it hard to reach the activation rate needed for Rowhammer attacks using naive, \u003cstrong\u003esingle-threaded\u003c/strong\u003e hammering like in CPUs (\u003cstrong\u003eFig. 3\u003c/strong\u003e, green line). To overcome this, we exploit the GPU‚Äôs \u003ca href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/#simt-architecture\" target=\"_blank\"\u003eSIMT parallelism\u003c/a\u003e, launching multiple threads and warps in parallel. This \u003cstrong\u003emulti-threaded, multi-warp approach\u003c/strong\u003e (\u003cstrong\u003eFig. 3\u003c/strong\u003e, orange line) eliminates idle time in the memory controller and reaches the maximum possible hammering rate. \u003cstrong\u003eFig. 4\u003c/strong\u003e shows how these strategies maximize memory controller utilization.\u003c/p\u003e\n\n\u003ccenter\u003e\n\u003cdiv\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003cp\u003e\u003cstrong\u003eFig. 3:\u003c/strong\u003e Number of activations in a single refresh period (tREFW) for single-thread, multi-thread, and multi-warp hammering.\u003c/p\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cp\u003e\u003cimg src=\"https://gpuhammer.com/fig3.png\" alt=\"fig3\"/\u003e\u003c/p\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\u003c/center\u003e\n\u003ccenter\u003e\n\u003cp\u003e\u003cstrong\u003eFig. 4:\u003c/strong\u003e Memory controller utilization with (a) single-thread,\n(b) multi-thread, and (c) multi-warp hammering. Multi-warp\nhammering minimizes idle time, maximizing activation rates.\n\u003cimg src=\"https://gpuhammer.com/fig4.png\" alt=\"fig4\"/\u003e\u003c/p\u003e\n\u003c/center\u003e\n\n\u003ch3\u003eStep-3: Synchronizing to Refreshes, Defeating Mitigations \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#step-3-synchronizing-to-refreshes-defeating-mitigations\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003ePrior works like \u003ca href=\"https://www.usenix.org/conference/usenixsecurity21/presentation/ridder\" target=\"_blank\"\u003eSMASH\u003c/a\u003e and \u003ca href=\"https://comsec.ethz.ch/wp-content/files/blacksmith_sp22.pdf\" target=\"_blank\"\u003eBlackSmith\u003c/a\u003e show that synchronizing hammering to refreshes (REF) is key to bypassing in-DRAM defenses.\nHowever, CUDA‚Äôs synchronization primitives (like \u003ccode\u003e__syncthreads()\u003c/code\u003e) used to synchronize threads can reorder warp execution. Instead, we use implicit per-warp delays, aligned such that REF commands overlap with our inserted delays, to align our hammering with refreshes and defeat in-DRAM mitigations like TRR while preserving warp execution order.\u003c/p\u003e\n\n\u003ccenter\u003e\n\u003cp\u003e\u003cstrong\u003eFig. 5:\u003c/strong\u003e Per-warp delays inserted using \u003ccode\u003eadds\u003c/code\u003e for synchronizing\nto REF. When the delays overlap, the REF\nis inserted in alignment with the hammering pattern.\n\u003cimg src=\"https://gpuhammer.com/fig5.png\" alt=\"fig5\"/\u003e\u003c/p\u003e\n\u003c/center\u003e\n\n\u003ch2\u003eüí• What Did We Break? \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#-what-did-we-break\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eWe ran GPUHammer on an \u003cstrong\u003eNVIDIA RTX A6000 (48 GB GDDR6)\u003c/strong\u003e across four DRAM banks and observed \u003cstrong\u003e8 distinct single-bit flips\u003c/strong\u003e, and bit-flips \u003cstrong\u003eacross all tested banks\u003c/strong\u003e (see Fig. 6). The \u003cstrong\u003eminimum activation count ( T\u003csub\u003eRH\u003c/sub\u003e)\u003c/strong\u003e to induce a flip was ~12K, consistent with prior DDR4 findings.\u003c/p\u003e\n\n\n\n\n\u003cp\u003eUsing these flips, we performed the \u003cstrong\u003efirst ML accuracy degradation attack using Rowhammer on a GPU\u003c/strong\u003e. \u003ca href=\"%28https://www.usenix.org/conference/usenixsecurity19/presentation/hong%29\"\u003ePrior work\u003c/a\u003e shows that \u003cstrong\u003eflipping the most significant bit of a floating-point exponent\u003c/strong\u003e in FP16 model weights can drastically reduce model accuracy. Based on this insight, we show that in a time-shared GPU setup, an attacker can position victim data into vulnerable DRAM rows via memory massaging and force the bit flips at such locations.\u003c/p\u003e\n\u003cp\u003eIn our proof-of-concept (\u003cstrong\u003eFig. 7\u003c/strong\u003e), with just \u003cstrong\u003ea single bit flip\u003c/strong\u003e, the accuracy of an ML model is \u003cstrong\u003edegraded below 1%\u003c/strong\u003e for all five tested ImageNet DNN models, resulting in up to \u003cstrong\u003e80% accuracy loss\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003cp\u003e\u003cstrong\u003eFig. 6:\u003c/strong\u003e Number of bit-flips in 4 banks on RTX A6000 with GDDR6. We observed bit-flips on each bank.\u003c/p\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cp\u003e\u003cstrong\u003eFig. 7:\u003c/strong\u003e Accuracy degradation attack on ImageNet models on NVIDIA A6000 GPU. We report the top-1 / top-5 accuracy without (Base Acc) and with (Degraded Acc) the bit-flip, and the Relative Accuracy Drop (RAD) for the top-1 accuracy.\u003c/p\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cimg src=\"https://gpuhammer.com/fig6.png\" alt=\"fig6\"/\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003cimg src=\"https://gpuhammer.com/fig8.png\" alt=\"fig7\"/\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch2\u003e‚ùì FAQs \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#-faqs\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\u003ch4\u003eWhat GPUs are vulnerable? Am I affected? \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#what-gpus-are-vulnerable-am-i-affected\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eWe confirmed Rowhammer bit flips on \u003cstrong\u003eNVIDIA A6000 GPUs with GDDR6\u003c/strong\u003e memory. Other GDDR6 GPUs, such as the \u003cstrong\u003eRTX 3080\u003c/strong\u003e, did not show bit flips in our tests, possibly due to variations in DRAM vendor, chip characteristics, or operating conditions like temperature. We also observed \u003cstrong\u003eno flips on an A100\u003c/strong\u003e GPU with HBM memory.\u003c/p\u003e\n\n\n\u003ch4\u003eWhy test so few GPUs? Isn‚Äôt that a small sample size? \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#why-test-so-few-gpus-isnt-that-a-small-sample-size\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eUnlike CPUs, where DRAM modules can be easily swapped out for testing, \u003cstrong\u003eGPU DRAM is soldered in\u003c/strong\u003e, making large-scale testing expensive (GPUs can cost thousands of dollars). Our attack code is \u003cstrong\u003eextensible to other Ampere GPUs and beyond\u003c/strong\u003e, and we encourage future work to expand the test coverage.\u003c/p\u003e\n\n\n\u003ch4\u003eHow can GPUHammer be mitigated? \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#how-can-gpuhammer-be-mitigated\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eAdmins can mitigate GPUHammer by \u003cstrong\u003eenabling ECC\u003c/strong\u003e via \u003ccode\u003envidia-smi -e 1\u003c/code\u003e (followed by a reboot). All observed bit flips so far are \u003cstrong\u003esingle-bit\u003c/strong\u003e, which ECC can correct. However, enabling ECC may \u003cstrong\u003ereduce performance by up to 10%\u003c/strong\u003e and \u003cstrong\u003ememory capacity by 6.25%\u003c/strong\u003e on the A6000. Still, this doesn‚Äôt eliminate the root cause, a hardware flaw, which would require redesigning GDDR6 with principled mitigations like  \u003ca href=\"https://arxiv.org/abs/2501.18861\" target=\"_blank\"\u003ePRAC\u003c/a\u003e, which has been proposed for DDR5 and beyond, or probabilistic mitigations like \u003ca href=\"https://gururaj-s.github.io/assets/pdf/ISCA24_Jaleel.pdf\" target=\"_blank\"\u003ePRIDE\u003c/a\u003e.\u003c/p\u003e\n\n\n\u003ch4\u003eAre newer GPUs like the H100 or RTX 5090 affected? \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#are-newer-gpus-like-the-h100-or-rtx-5090-affected\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eCurrently, \u003cstrong\u003eno\u003c/strong\u003e. H100 (HBM3) and RTX 5090 (GDDR7) feature \u003cstrong\u003eon-die ECC\u003c/strong\u003e, which likely masks single-bit flips. However, future Rowhammer patterns causing multi-bit flips  may bypass such ECC, as shown in attacks like \u003ca href=\"https://www.vusec.net/projects/eccploit/\" target=\"_blank\"\u003eECCploit\u003c/a\u003e.\u003c/p\u003e\n\n\n\u003ch4\u003eDid you disclose this to NVIDIA? What was their response? \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#did-you-disclose-this-to-nvidia-what-was-their-response\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eYes. We \u003cstrong\u003eresponsibly disclosed\u003c/strong\u003e GPUHammer to NVIDIA on \u003cstrong\u003eJanuary 15, 2025\u003c/strong\u003e, and also to major cloud service providers (AWS, Azure, GCP, etc.). NVIDIA confirmed the issue and recommended enabling ECC as a mitigation.\u003c/p\u003e\n\n\u003ch2\u003eFurther Information \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#further-information\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003ePlease refer to our \u003ca href=\"https://gururaj-s.github.io/assets/pdf/SEC25_GPUHammer.pdf\" target=\"_blank\"\u003epaper\u003c/a\u003e, which appears at \u003cem\u003eUSENIX Security 2025\u003c/em\u003e. The artifacts are available on \u003ca href=\"https://github.com/sith-lab/gpuhammer\" target=\"_blank\"\u003eGitHub\u003c/a\u003e and \u003ca href=\"https://zenodo.org/records/15694512\" target=\"_blank\"\u003eZenodo\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo cite our paper, please use:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"bibtex\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e@inproceedings\u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003elin2025gpuhammer\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003eauthor\u003c/span\u003e       \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{Chris S. Lin and Joyce Qu and Gururaj Saileshwar}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003etitle\u003c/span\u003e        \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{GPUHammer: Rowhammer Attacks on GPU Memories are Practical}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003epublisher\u003c/span\u003e    \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{USENIX Association}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003ebooktitle\u003c/span\u003e    \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{Proceedings of the 34th USENIX Conference on Security Symposium}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003eyear\u003c/span\u003e         \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{2025}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003eseries\u003c/span\u003e       \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{SEC \u0026#39;25}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003eaddress\u003c/span\u003e      \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{USA}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003elocation\u003c/span\u003e     \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{Seattle, WA, USA}\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003eAcknowledgements \n    \n    \n    \u003cspan\u003e\n        \u003ca href=\"#acknowledgements\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eThis research was supported by Natural Sciences and Engineering Research Council of Canada (NSERC) under funding reference number RGPIN-2023-04796, and an NSERC-CSE Research Communities Grant under funding reference number ALLRP-588144-23.\nAny research, opinions, or positions expressed in this work are solely those of the authors and do not represent the official views of NSERC, the Communications Security Establishment Canada, or the Government of Canada.\u003c/p\u003e\n\u003cdiv\u003e\n  \u003cp\u003e\u003cimg src=\"https://gpuhammer.com/Cse_badge.png\" alt=\"CSE logo\"/\u003e\n  \u003c/p\u003e\n  \u003cp\u003e\u003cimg src=\"https://gpuhammer.com/NSERC_RGB.png\" alt=\"NSERC logo\"/\u003e\n  \u003c/p\u003e\n\u003c/div\u003e\n\u003c/section\u003e\n\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": null,
  "modifiedTime": null
}
