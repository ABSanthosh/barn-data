{
  "id": "dc2f6014-47ab-48ba-8430-0fae1acbb44b",
  "title": "Study finds LLMs can identify their own mistakes",
  "link": "https://venturebeat.com/ai/study-finds-llms-can-identify-their-own-mistakes/",
  "description": "It turns out that LLMs encode quite a bit of knowledge about the truthfulness of their answers, even when they give the wrong one.",
  "author": "Ben Dickson",
  "published": "Tue, 29 Oct 2024 22:17:22 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI Hallucinations",
    "AI research",
    "AI, ML and Deep Learning",
    "Anthropic",
    "Apple",
    "Apple AI research",
    "category-/Science",
    "datasets",
    "Google",
    "Google Deepmind",
    "Google Research",
    "interpretable AI",
    "large language models",
    "LLaMA 2",
    "LLM hallucinations",
    "LLMs",
    "Mistral 7B",
    "natural language inference",
    "open source models",
    "OpenAI",
    "research",
    "technion"
  ],
  "byline": "Ben Dickson",
  "length": 6152,
  "excerpt": "It turns out that LLMs encode quite a bit of knowledge about the truthfulness of their answers, even when they give the wrong one.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "October 29, 2024 3:17 PM Image credit: VentureBeat with DALL-E 3 Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More A well-known problem of large language models (LLMs) is their tendency to generate incorrect or nonsensical outputs, often called “hallucinations.” While much research has focused on analyzing these errors from a user’s perspective, a new study by researchers at Technion, Google Research and Apple investigates the inner workings of LLMs, revealing that these models possess a much deeper understanding of truthfulness than previously thought. The term hallucination lacks a universally accepted definition and encompasses a wide range of LLM errors. For their study, the researchers adopted a broad interpretation, considering hallucinations to encompass all errors produced by an LLM, including factual inaccuracies, biases, common-sense reasoning failures, and other real-world errors. Most previous research on hallucinations has focused on analyzing the external behavior of LLMs and examining how users perceive these errors. However, these methods offer limited insight into how errors are encoded and processed within the models themselves. Some researchers have explored the internal representations of LLMs, suggesting they encode signals of truthfulness. However, previous efforts were mostly focused on examining the last token generated by the model or the last token in the prompt. Since LLMs typically generate long-form responses, this practice can miss crucial details. The new study takes a different approach. Instead of just looking at the final output, the researchers analyze “exact answer tokens,” the response tokens that, if modified, would change the correctness of the answer. The researchers conducted their experiments on four variants of Mistral 7B and Llama 2 models across 10 datasets spanning various tasks, including question answering, natural language inference, math problem-solving, and sentiment analysis. They allowed the models to generate unrestricted responses to simulate real-world usage. Their findings show that truthfulness information is concentrated in the exact answer tokens.  “These patterns are consistent across nearly all datasets and models, suggesting a general mechanism by which LLMs encode and process truthfulness during text generation,” the researchers write. To predict hallucinations, they trained classifier models, which they call “probing classifiers,” to predict features related to the truthfulness of generated outputs based on the internal activations of the LLMs. The researchers found that training classifiers on exact answer tokens significantly improves error detection. “Our demonstration that a trained probing classifier can predict errors suggests that LLMs encode information related to their own truthfulness,” the researchers write. Generalizability and skill-specific truthfulness The researchers also investigated whether a probing classifier trained on one dataset could detect errors in others. They found that probing classifiers do not generalize across different tasks. Instead, they exhibit “skill-specific” truthfulness, meaning they can generalize within tasks that require similar skills, such as factual retrieval or common-sense reasoning, but not across tasks that require different skills, such as sentiment analysis. “Overall, our findings indicate that models have a multifaceted representation of truthfulness,” the researchers write. “They do not encode truthfulness through a single unified mechanism but rather through multiple mechanisms, each corresponding to different notions of truth.” Further experiments showed that these probing classifiers could predict not only the presence of errors but also the types of errors the model is likely to make. This suggests that LLM representations contain information about the specific ways in which they might fail, which can be useful for developing targeted mitigation strategies. Finally, the researchers investigated how the internal truthfulness signals encoded in LLM activations align with their external behavior. They found a surprising discrepancy in some cases: The model’s internal activations might correctly identify the right answer, yet it consistently generates an incorrect response. This finding suggests that current evaluation methods, which solely rely on the final output of LLMs, may not accurately reflect their true capabilities. It raises the possibility that by better understanding and leveraging the internal knowledge of LLMs, we might be able to unlock hidden potential and significantly reduce errors. Future implications The study’s findings can help design better hallucination mitigation systems. However, the techniques it uses require access to internal LLM representations, which is mainly feasible with open-source models.  The findings, however, have broader implications for the field. The insights gained from analyzing internal activations can help develop more effective error detection and mitigation techniques. This work is part of a broader field of studies that aims to better understand what is happening inside LLMs and the billions of activations that happen at each inference step. Leading AI labs such as OpenAI, Anthropic and Google DeepMind have been working on various techniques to interpret the inner workings of language models. Together, these studies can help build more robots and reliable systems. “Our findings suggest that LLMs’ internal representations provide useful insights into their errors, highlight the complex link between the internal processes of models and their external outputs, and hopefully pave the way for further improvements in error detection and mitigation,” the researchers write. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/10/Robot-peering-inside-head-of-another-robot.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-10-29T22:17:22+00:00\" datetime=\"2024-10-29T22:17:22+00:00\"\u003eOctober 29, 2024 3:17 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"422\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/Robot-peering-inside-head-of-another-robot.jpg?w=750\" alt=\"Robot peering inside head of another robot\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with DALL-E 3\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eA well-known problem of large language models (LLMs) is their tendency to generate incorrect or nonsensical outputs, often called “\u003ca href=\"https://venturebeat.com/ai/galileo-hallucination-index-identifies-gpt-4-as-best-performing-llm-for-different-use-cases/\"\u003ehallucinations\u003c/a\u003e.” While much research has focused on analyzing these errors from a user’s perspective, a \u003ca href=\"https://arxiv.org/abs/2410.02707\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew study\u003c/a\u003e by researchers at \u003ca href=\"https://www.technion.ac.il/en/home-2/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eTechnion\u003c/a\u003e, \u003ca href=\"http://research.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle Research\u003c/a\u003e and \u003ca href=\"https://machinelearning.apple.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eApple\u003c/a\u003e investigates the inner workings of LLMs, revealing that these models possess a much deeper understanding of truthfulness than previously thought.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe term hallucination lacks a universally accepted definition and encompasses a wide range of LLM errors. For their study, the researchers adopted a broad interpretation, considering hallucinations to encompass all errors produced by an LLM, including factual inaccuracies, biases, common-sense reasoning failures, and other real-world errors.\u003c/p\u003e\n\n\n\n\u003cp\u003eMost previous research on hallucinations has focused on analyzing the external behavior of LLMs and examining how users perceive these errors. However, these methods offer limited insight into how errors are encoded and processed within the models themselves.\u003c/p\u003e\n\n\n\n\u003cp\u003eSome researchers have explored the internal representations of LLMs, suggesting they encode signals of truthfulness. However, previous efforts were mostly focused on examining the last token generated by the model or the last token in the prompt. Since LLMs typically generate long-form responses, this practice can miss crucial details.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe new study takes a different approach. Instead of just looking at the final output, the researchers analyze “exact answer tokens,” the response tokens that, if modified, would change the correctness of the answer.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers conducted their experiments on four variants of \u003ca href=\"https://venturebeat.com/ai/mistral-ai-europe-startup-releases-mistral-7b-model/\"\u003eMistral 7B\u003c/a\u003e and \u003ca href=\"https://venturebeat.com/ai/llama-2-how-to-access-and-use-metas-versatile-open-source-chatbot-right-now/\"\u003eLlama 2\u003c/a\u003e models across 10 datasets spanning various tasks, including question answering, natural language inference, math problem-solving, and sentiment analysis. They allowed the models to generate unrestricted responses to simulate real-world usage. Their findings show that truthfulness information is concentrated in the exact answer tokens. \u003c/p\u003e\n\n\n\n\u003cp\u003e“These patterns are consistent across nearly all datasets and models, suggesting a general mechanism by which LLMs encode and process truthfulness during text generation,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo predict hallucinations, they trained classifier models, which they call “probing classifiers,” to predict features related to the truthfulness of generated outputs based on the internal activations of the LLMs. The researchers found that training classifiers on exact answer tokens significantly improves error detection.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Our demonstration that a trained probing classifier can predict errors suggests that LLMs encode information related to their own truthfulness,” the researchers write.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-generalizability-and-skill-specific-truthfulness\"\u003eGeneralizability and skill-specific truthfulness\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers also investigated whether a probing classifier trained on one dataset could detect errors in others. They found that probing classifiers do not generalize across different tasks. Instead, they exhibit “skill-specific” truthfulness, meaning they can generalize within tasks that require similar skills, such as factual retrieval or common-sense reasoning, but not across tasks that require different skills, such as sentiment analysis.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Overall, our findings indicate that models have a multifaceted representation of truthfulness,” the researchers write. “They do not encode truthfulness through a single unified mechanism but rather through multiple mechanisms, each corresponding to different notions of truth.”\u003c/p\u003e\n\n\n\n\u003cp\u003eFurther experiments showed that these probing classifiers could predict not only the presence of errors but also the types of errors the model is likely to make. This suggests that LLM representations contain information about the specific ways in which they might fail, which can be useful for developing targeted mitigation strategies.\u003c/p\u003e\n\n\n\n\u003cp\u003eFinally, the researchers investigated how the internal truthfulness signals encoded in LLM activations align with their external behavior. They found a surprising discrepancy in some cases: The model’s internal activations might correctly identify the right answer, yet it consistently generates an incorrect response.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis finding suggests that current evaluation methods, which solely rely on the final output of LLMs, may not accurately reflect their true capabilities. It raises the possibility that by better understanding and leveraging the internal knowledge of LLMs, we might be able to unlock hidden potential and significantly reduce errors.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-future-implications\"\u003eFuture implications\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe study’s findings can help design better hallucination mitigation systems. However\u003ca href=\"https://venturebeat.com/ai/2023-was-a-great-year-for-open-source-llms/\"\u003e,\u003c/a\u003e the techniques it uses require access to internal LLM representations, which is mainly feasible with \u003ca href=\"https://venturebeat.com/ai/2023-was-a-great-year-for-open-source-llms/\"\u003eopen-source models\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe findings, however, have broader implications for the field. The insights gained from analyzing internal activations can help develop more effective error detection and mitigation techniques. This work is part of a broader field of studies that aims to better understand what is happening inside LLMs and the billions of activations that happen at each inference step. Leading AI labs such as OpenAI, Anthropic and Google DeepMind have been working on various techniques to \u003ca href=\"https://venturebeat.com/ai/deepminds-gemma-scope-peers-under-the-hood-of-large-language-models/\"\u003einterpret the inner workings of language models\u003c/a\u003e. Together, these studies can help build more robots and reliable systems.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Our findings suggest that LLMs’ internal representations provide useful insights into their errors, highlight the complex link between the internal processes of models and their external outputs, and hopefully pave the way for further improvements in error detection and mitigation,” the researchers write.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-10-29T22:17:22Z",
  "modifiedTime": "2024-10-29T22:17:28Z"
}
