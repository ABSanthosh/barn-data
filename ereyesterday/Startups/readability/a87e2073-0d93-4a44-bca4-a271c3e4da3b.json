{
  "id": "a87e2073-0d93-4a44-bca4-a271c3e4da3b",
  "title": "CAPTCHAs are over (in ticketing)",
  "link": "https://behind.pretix.eu/2025/05/23/captchas-are-over/",
  "description": "Article URL: https://behind.pretix.eu/2025/05/23/captchas-are-over/ Comments URL: https://news.ycombinator.com/item?id=44084677 Points: 6 # Comments: 1",
  "author": "pabs3",
  "published": "Sun, 25 May 2025 00:37:08 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "",
  "length": 12275,
  "excerpt": "Behind the scenes blog of pretix, your friendly ticketing system from next door.",
  "siteName": "",
  "favicon": "",
  "text": "One of the issues in ticketing is that many events have much more demand for tickets than they can supply. Obviously, this is a good problem to have (better than empty halls), but it attracts certain types of bad actors trying to get as many tickets as possible in order to resell them (“ticket scalping”). While this is possible of course just by buying tickets like a regular customer, many of them use computer programs (“bots”) to enhance either their chance of claiming a ticket or their scale of operation by buying more tickets. The naive economic solution to the problem would be raising ticket prices step by step until it is no longer attractive for scalpers to resell your ticket, because the original price of the ticket is already the maximum that people are willing to pay, creating an economic equilibrium. Most organizers, including for-profit organizations, do not want to choose this option due to ethical concerns or concerns about community building. Therefore, whenever this topic comes up, someone quickly suggests a technical solution – usually a CAPTCHA. I believe that CAPTCHAs no longer provide any meaningful protection in this scenario. Here’s why: We have run out of problems CAPTCHAs were invented over 20 years ago to distinguish between human and non-human users of a website. The basic idea is this: Ask the user to solve a problem that is easy to solve for a human but hard to solve for a computer. The most popular early problem used in CAPTCHAs was text recognition, often under artificially bad conditions. This often looked like this: These days, a state-of-the-art machine learning model is able to easily solve this problem and recognize the words. Any attempt to make the text even more unreadable will make it too hard for the average human user as well, making it unsuitable. Around ten years ago, Google and others therefore switched to one of the problems that remained hard at the time: Image recognition. I don’t need to explain this to you, you have all clicked on way to many motorcycles yourselves: However, in 2025, this is also no longer a challenge to computers. A modern image detection model will be able to point out the motorcycles in this screenshot faster than any human. To make it harder, a good CAPTCHA always needs a second type of problem since we need to provide accessibility for humans who cannot see. This has always been the right thing to do, but in Europe it’s now also becoming the law. A typical choice is an audio recognition task: Have you seen what modern speech recognition models can do? They can certainly understand audio better than any non-native speaker of most languages. Unfortunately, I don’t see any new approaches of this type replacing the current ones. We are running out of suitable problems that humans can solve better than computers. Of course, there are still a lot of things that humans can do better than computers – but not things that all humans can do better than computers and that can be tested over the internet with a few simple clicks in a few seconds. If AI is the problem, surely it is also the solution What I’m saying above is not new information. That’s why people working on bot protection have moved to solutions other than having the user solve puzzles many years ago. One of these approaches is behavior analysis using machine learning. In 2014, Google presented the one-click CAPTCHA and later, with reCAPTCHA v3, the “invisible CAPTCHA”. Similar services are offered by other companies like Cloudflare, Akamai, and others. They rely on analyzing everything they know about you and feeding it into a large machine-learning model to compare you with behavior they know from humans and bots. That’s a clever idea, but it comes with two significant drawbacks: First, it’s a privacy nightmare. The entire idea only works if you’re collecting a lot of data. Given the types of companies who offer this service, they most likely not only collect data about how people behave on your website, but how people behave on other websites as well. This requires the creation of extensive centralized profiles about the people visiting your site. Second, at least in ticketing, there’s not really a good way to handle false positives. Say the model makes a wrong decision and puts an entire class of valid human users into the “bot” bucket, for example, everyone who has never visited one of the sites protected by the company before, or everyone who uses a specific assistive technology. In some scenarios, e.g. when protecting an online forum against spam, you can place submissions with a high bot score into manual review. However, in high-demand ticketing, you need to make a binary choice: Either you sell the user a ticket, or you don’t. Risking to exclude an entire class of users is not only unethical, but also a possible legal risk. So what do these solutions usually do? If your bot score is too high, they show you a classical CAPTCHA again. This however, as we’ve seen above, is no longer a real hurdle to bots. Many signals are not accessibility-safe Say we want to build a solution that is built on behavior analysis only of the user’s behavior on our site, not across half the internet. What information do we have to work with? Fifteen years ago, bots used to be scripts that used HTTP libraries to interact with web pages. So we could look at JavaScript execution behavior, HTTP header fingerprinting, timing differences in loading resources, and similar signals. These days, bots are using real browsers that load web pages just like regular users. The bots use the APIs provided by browsers to analyze the page and control what the browser does. Now, I’ve already mentioned that we need to build solutions in a way that are accessible to all users. This includes users who make use of assistive technology such as screen readers. And what does a screen reader do? It interacts with the browser through the APIs the browser provides to analyze the page and control what the browser does. So all the differences we might expect between an “average user” and a bot, such as the lack of mouse movement before a button is clicked, we will also see when someone is using assistive technology (or, in this example, just a touch screen). Proof of Work doesn’t work for ticketing The current trend in bot prevention is using a proof of work scheme, popularized by solutions like Friendly Captcha or Anubis. The idea between proof of work for spam protection goes back to 1997 and is quite different than the CAPTCHAs presented above. We’re no longer looking for problems that humans can solve better than computers. We’re now looking for problems that are costly for computers to solve. The typical example is brute-forcing a hash function. Any computer can do that, but – depending on the capability of the computer – it will take the computer a certain amount of time. Computing time costs power, and power costs money. The idea is to make spamming too expensive for the spammers. If you need to spend 0.0001 € in power to access a website, a human user will probably not care. For someone who is trying to spam millions of messages or scrape the entire internet, however, it will add up to a significant total cost and the spamming or scraping might not be worth it any more. The ethically questionable idea of burning power on useless computations (even if just a little) aside, I don’t consider this economic argument plausible for ticketing. If a spammer needs to spend 0.0001 € in power to access the site only to gain a marginal profit of 0.00005 €, they are losing money with every site access. However, if a ticket scalper needs to spend 0.0001 € in power to buy a ticket that they will later sell at a 200 € profit, this will not stop them. The economics problem affects traditional CAPTCHAs as well This economic argument also applies to all the traditional CAPTCHAs we’ve talked about. While modern machine learning can easily recognize characters, spoken words, or images of motorcycles, every run of the machine learning model comes at a cost. Google’s cloud vision API, for example, costs 0.0015 $ per picture for text detection and 0.00225 € for object localization. Even if we did find new problems that are still too hard for computers, there are companies who employ gig workers in low-price countries to solve CAPTCHAs for you. It’s not hard to find CAPTCHA bypass providers that use a combination of AI and low-paid workers to solve CAPTCHAs for you at speed and very low cost: So what’s left? I believe that the possibility to reliably tell a bot from a human through something simple like a CAPTCHA is a thing of the past in an industry like ticketing where the financial motivation for circumvention is very high. One tactic that can still be quite effective is limiting resale possibilities by strongly personalizing tickets, including ID card verification of at least a relevant sample of sold tickets at the event entrance. This still allows scalpers to sell something like “bot as a service” in advance of the ticket sale but limits any type of “buy-and-resell-later” schemes. Of course it also harms real buyers who want to go to a concert with a +1 but do not yet know who they will bring. A related option is to strongly bind purchase limits to other resources that are not easy to acquire quickly in large amounts, such as allowing only X tickets per (verified) phone number or – as you will need to collect payment information for the ticket anyway – per verified credit card or verified bank account. People are able to easily get multiple phone numbers and bank accounts, but getting dozens or hundreds of them becomes a lot more effort. A sufficiently motivated bad actor will still be able to acquire many of these, but it will increase the cost involved significantly and also many possible circumvention techniques (like buying stolen credit card numbers) are significantly more illegal than solving CAPTCHAs with AI and might be beyond what at least some of the actors are ready to do. The BAP theorem In computer science, there is the “CAP theorem”. It states that when designing a database system, you can choose between the three properties “consistency”, “availability”, and “partition tolerance”. And while it is possible to combine two of them, it is impossible to combine all three – you always need to give one up. I’m proposing a similar theorem for bot protection, the “BAP theorem”, stating that you can only combine two of the following three properties: Bot-resistance Accessibility Privacy-friendliness The possible combinations of these properties would be: BA: Bot-resistant and accessible, but not privacy-friendly. In this category we find all approaches based on heavily personalized tickets, possibly with strong identification methods. If there is a good accessibility fallback, then large-scale behaviour analysis also falls under this category. BP: Bot-resistant and privacy-friendly, but not accessible. This category contains all approaches trying to use the remaining bits and pieces that are hard for AI and easy for humans, such as heavily relying on mouse movements, any puzzles involving complex user interaction, or any tricks playing with the limitations of the browser APIs. AP: Accessible and privacy-friendly, but not bot-resistant. This category is easy and includes many things down to the trivial case of no protection at all, or any attempt to fight off at least very simple bots without harming the other goals. Conclusion Since accessibility is required by law, we’re basically left with the first or last option. As much as I hope that I’ve missed something significant, I feel the conclusion is inevitable: Events will need to decide whether they want to protect against bots, or preserve high privacy standards. You will not be able to do both. It remains, unfortunately, very hard to solve social problems with technology. Looking at social solutions, some have tried making ticket scalping illegal – with varying success. If your country is not on that list, lobbying for it should be part of your solution strategy – but will most likely will be a very long road to a solution.",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv role=\"main\"\u003e\n\n    \u003carticle\u003e\n\n        \n\n\n\n        \n\n        \n\n        \u003csection\u003e\n            \u003cp\u003eOne of the issues in ticketing is that many events have much more demand for tickets than they can supply.\nObviously, this is a good problem to have (better than empty halls), but it attracts certain types of bad actors trying to get as many tickets as possible in order to resell them (“ticket scalping”).\nWhile this is possible of course just by buying tickets like a regular customer, many of them use computer programs (“bots”) to enhance either their chance of claiming a ticket or their scale of operation by buying more tickets.\u003c/p\u003e\n\n\u003cp\u003eThe naive economic solution to the problem would be raising ticket prices step by step until it is no longer attractive for scalpers to resell your ticket, because the original price of the ticket is already the maximum that people are willing to pay, creating an \u003ca href=\"https://en.wikipedia.org/wiki/Economic_equilibrium\"\u003eeconomic equilibrium\u003c/a\u003e.\nMost organizers, including for-profit organizations, do not want to choose this option due to ethical concerns or concerns about community building.\u003c/p\u003e\n\n\u003cp\u003eTherefore, whenever this topic comes up, someone quickly suggests a technical solution – usually a \u003ca href=\"https://en.wikipedia.org/wiki/CAPTCHA\"\u003eCAPTCHA\u003c/a\u003e.\nI believe that CAPTCHAs no longer provide any meaningful protection in this scenario.\nHere’s why:\u003c/p\u003e\n\n\u003ch3 id=\"we-have-run-out-of-problems\"\u003eWe have run out of problems\u003c/h3\u003e\n\n\u003cp\u003eCAPTCHAs were invented over 20 years ago to distinguish between human and non-human users of a website.\nThe basic idea is this:\nAsk the user to solve a problem that is easy to solve for a human but hard to solve for a computer.\nThe most popular early problem used in CAPTCHAs was \u003ca href=\"https://en.wikipedia.org/wiki/Optical_character_recognition\"\u003etext recognition\u003c/a\u003e, often under artificially bad conditions.\nThis often looked like this:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://behind.pretix.eu/assets/images/2025-05-captchas/recaptcha-v1.jpg\" alt=\"reCAPTCHA v1 using two distorted words\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThese days, a state-of-the-art machine learning model is able to \u003cem\u003eeasily\u003c/em\u003e solve this problem and recognize the words.\nAny attempt to make the text even more unreadable will make it too hard for the average human user as well, making it unsuitable.\u003c/p\u003e\n\n\u003cp\u003eAround ten years ago, Google and others therefore switched to one of the problems that remained hard at the time: Image recognition.\nI don’t need to explain this to you, you have all clicked on way to many motorcycles yourselves:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://behind.pretix.eu/assets/images/2025-05-captchas/recaptcha-img.png\" alt=\"reCAPTCHA v2 asking to select all squares with motorcycles\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eHowever, in 2025, this is also no longer a challenge to computers.\nA modern image detection model will be able to point out the motorcycles in this screenshot faster than any human.\u003c/p\u003e\n\n\u003cp\u003eTo make it harder, a good CAPTCHA always needs a second type of problem since we need to provide accessibility for humans who cannot see.\nThis has always been the right thing to do, but in Europe it’s now also becoming \u003ca href=\"https://en.wikipedia.org/wiki/European_Accessibility_Act\"\u003ethe law\u003c/a\u003e.\nA typical choice is an audio recognition task:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://behind.pretix.eu/assets/images/2025-05-captchas/recaptcha-audio.png\" alt=\"reCAPTCHA v2 asking to type the content of an audio snippet\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eHave you seen what modern speech recognition models can do?\nThey can certainly understand audio better than any non-native speaker of most languages.\u003c/p\u003e\n\n\u003cp\u003eUnfortunately, I don’t see any new approaches of this type replacing the current ones.\nWe are running out of suitable problems that humans can solve better than computers.\nOf course, there are still a lot of things that humans can do better than computers – but not things that \u003cem\u003eall\u003c/em\u003e humans can do better than computers and that can be tested over the internet with a few simple clicks in a few seconds.\u003c/p\u003e\n\n\u003ch3 id=\"if-ai-is-the-problem-surely-it-is-also-the-solution\"\u003eIf AI is the problem, surely it is also the solution\u003c/h3\u003e\n\n\u003cp\u003eWhat I’m saying above is not new information.\nThat’s why people working on bot protection have moved to solutions other than having the user solve puzzles many years ago.\nOne of these approaches is behavior analysis using machine learning.\u003c/p\u003e\n\n\u003cp\u003eIn 2014, Google presented the \u003ca href=\"https://security.googleblog.com/2014/12/are-you-robot-introducing-no-captcha.html\"\u003eone-click CAPTCHA\u003c/a\u003e and later, with reCAPTCHA v3, the “invisible CAPTCHA”.\nSimilar services are offered by other companies like Cloudflare, Akamai, and others.\u003c/p\u003e\n\n\u003cp\u003eThey rely on analyzing everything they know about you and feeding it into a large machine-learning model to compare you with behavior they know from humans and bots.\nThat’s a clever idea, but it comes with two significant drawbacks:\u003c/p\u003e\n\n\u003cp\u003eFirst, it’s a privacy nightmare.\nThe entire idea only works if you’re collecting a lot of data.\nGiven the types of companies who offer this service, they most likely not only collect data about how people behave on \u003cem\u003eyour\u003c/em\u003e website, but how people behave on other websites as well.\nThis requires the creation of extensive centralized profiles about the people visiting your site.\u003c/p\u003e\n\n\u003cp\u003eSecond, at least in ticketing, there’s not really a good way to handle false positives.\nSay the model makes a wrong decision and puts an entire class of valid human users into the “bot” bucket, for example, everyone who has never visited one of the sites protected by the company before, or everyone who uses a specific assistive technology.\u003c/p\u003e\n\n\u003cp\u003eIn some scenarios, e.g. when protecting an online forum against spam, you can place submissions with a high bot score into manual review.\nHowever, in high-demand ticketing, you need to make a binary choice:\nEither you sell the user a ticket, or you don’t.\nRisking to exclude an entire class of users is not only unethical, but also a possible legal risk.\u003c/p\u003e\n\n\u003cp\u003eSo what do these solutions usually do?\nIf your bot score is too high, they show you a classical CAPTCHA again.\nThis however, as we’ve seen above, is no longer a real hurdle to bots.\u003c/p\u003e\n\n\u003ch3 id=\"many-signals-are-not-accessibility-safe\"\u003eMany signals are not accessibility-safe\u003c/h3\u003e\n\n\u003cp\u003eSay we want to build a solution that is built on behavior analysis only of the user’s behavior on our site, not across half the internet.\nWhat information do we have to work with?\u003c/p\u003e\n\n\u003cp\u003eFifteen years ago, bots used to be scripts that used HTTP libraries to interact with web pages.\nSo we could look at JavaScript execution behavior, HTTP header fingerprinting, timing differences in loading resources, and similar signals.\nThese days, bots are using \u003ca href=\"https://pptr.dev/\"\u003ereal browsers\u003c/a\u003e that load web pages just like regular users.\nThe bots use the APIs provided by browsers to analyze the page and control what the browser does.\u003c/p\u003e\n\n\u003cp\u003eNow, I’ve already mentioned that we need to build solutions in a way that are accessible to all users.\nThis includes users who make use of \u003ca href=\"https://en.wikipedia.org/wiki/Assistive_technology\"\u003eassistive technology\u003c/a\u003e such as screen readers.\nAnd what does a screen reader do?\nIt interacts with the browser through the APIs the browser provides to analyze the page and control what the browser does.\u003c/p\u003e\n\n\u003cp\u003eSo all the differences we might expect between an “average user” and a bot, such as the lack of mouse movement before a button is clicked, we will also see when someone is using assistive technology (or, in this example, just a touch screen).\u003c/p\u003e\n\n\u003ch3 id=\"proof-of-work-doesnt-work-for-ticketing\"\u003eProof of Work doesn’t work for ticketing\u003c/h3\u003e\n\n\u003cp\u003eThe current trend in bot prevention is using a \u003ca href=\"https://en.wikipedia.org/wiki/Proof_of_work\"\u003eproof of work\u003c/a\u003e scheme, popularized by solutions like \u003ca href=\"https://friendlycaptcha.com/\"\u003eFriendly Captcha\u003c/a\u003e or \u003ca href=\"https://anubis.techaro.lol/\"\u003eAnubis\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe idea between proof of work for spam protection goes back to 1997 and is quite different than the CAPTCHAs presented above.\nWe’re no longer looking for problems that humans can solve better than computers.\nWe’re now looking for problems that are \u003cstrong\u003ecostly\u003c/strong\u003e for computers to solve.\nThe typical example is brute-forcing a hash function.\nAny computer can do that, but – depending on the capability of the computer – it will take the computer a certain amount of time.\nComputing time costs power, and power costs money.\u003c/p\u003e\n\n\u003cp\u003eThe idea is to make spamming too expensive for the spammers.\nIf you need to spend 0.0001 € in power to access a website, a human user will probably not care.\nFor someone who is trying to spam millions of messages or scrape the entire internet, however, it will add up to a significant total cost and the spamming or scraping might not be worth it any more.\u003c/p\u003e\n\n\u003cp\u003eThe ethically questionable idea of burning power on useless computations (even if just a little) aside, I don’t consider this economic argument plausible for ticketing.\nIf a spammer needs to spend 0.0001 € in power to access the site only to gain a marginal profit of 0.00005 €, they are losing money with every site access.\nHowever, if a ticket scalper needs to spend 0.0001 € in power to buy a ticket that they will later sell at a 200 € profit, this will not stop them.\u003c/p\u003e\n\n\u003ch3 id=\"the-economics-problem-affects-traditional-captchas-as-well\"\u003eThe economics problem affects traditional CAPTCHAs as well\u003c/h3\u003e\n\n\u003cp\u003eThis economic argument also applies to all the traditional CAPTCHAs we’ve talked about.\nWhile modern machine learning can easily recognize characters, spoken words, or images of motorcycles, every run of the machine learning model comes at a cost.\nGoogle’s cloud vision API, for example, costs 0.0015 $ per picture for text detection and 0.00225 € for object localization.\u003c/p\u003e\n\n\u003cp\u003eEven if we did find new problems that are still too hard for computers, there are companies who employ gig workers in low-price countries to solve CAPTCHAs for you.\nIt’s not hard to find CAPTCHA bypass providers that use a combination of AI and low-paid workers to solve CAPTCHAs for you at speed and very low cost:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://behind.pretix.eu/assets/images/2025-05-captchas/captcha-solving-services.png\" alt=\"reCAPTCHA v2 asking to type the content of an audio snippet\"/\u003e\u003c/p\u003e\n\n\u003ch3 id=\"so-whats-left\"\u003eSo what’s left?\u003c/h3\u003e\n\n\u003cp\u003eI believe that the possibility to reliably tell a bot from a human through something simple like a CAPTCHA is a thing of the past in an industry like ticketing where the financial motivation for circumvention is very high.\u003c/p\u003e\n\n\u003cp\u003eOne tactic that can still be quite effective is limiting resale possibilities by strongly personalizing tickets, including ID card verification of at least a relevant sample of sold tickets at the event entrance.\nThis still allows scalpers to sell something like “bot as a service” in advance of the ticket sale but limits any type of “buy-and-resell-later” schemes.\nOf course it also harms real buyers who want to go to a concert with a +1 but do not yet know who they will bring.\u003c/p\u003e\n\n\u003cp\u003eA related option is to strongly bind purchase limits to other resources that are not easy to acquire quickly in large amounts, such as allowing only X tickets per (verified) phone number or – as you will need to collect payment information for the ticket anyway – per verified credit card or verified bank account.\nPeople are able to easily get multiple phone numbers and bank accounts, but getting dozens or hundreds of them becomes a lot more effort.\nA sufficiently motivated bad actor will still be able to acquire many of these, but it will increase the cost involved significantly and also many possible circumvention techniques (like buying stolen credit card numbers) are significantly more illegal than solving CAPTCHAs with AI and might be beyond what at least some of the actors are ready to do.\u003c/p\u003e\n\n\u003ch3 id=\"the-bap-theorem\"\u003eThe BAP theorem\u003c/h3\u003e\n\n\u003cp\u003eIn computer science, there is the “\u003ca href=\"https://en.wikipedia.org/wiki/CAP_theorem\"\u003eCAP theorem\u003c/a\u003e”.\nIt states that when designing a database system, you can choose between the three properties “consistency”, “availability”, and “partition tolerance”.\nAnd while it is possible to combine two of them, it is impossible to combine all three – you always need to give one up.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://behind.pretix.eu/assets/images/2025-05-captchas/cap.svg\" alt=\"CAP theorem shown as a Venn Diagram, based on CC-BY-SA diagram of Moond on Wikimedia\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eI’m proposing a similar theorem for bot protection, the “BAP theorem”, stating that you can only combine two of the following three properties:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eB\u003c/strong\u003eot-resistance\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eA\u003c/strong\u003eccessibility\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eP\u003c/strong\u003erivacy-friendliness\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cimg src=\"https://behind.pretix.eu/assets/images/2025-05-captchas/bap.svg\" alt=\"Variation of the Venn Diagram with B = bot-resistant, A = accessible, P = privacy-friendly\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThe possible combinations of these properties would be:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003eBA\u003c/strong\u003e: Bot-resistant and accessible, but not privacy-friendly. In this category we find all approaches based on heavily personalized tickets, possibly with strong identification methods. If there is a good accessibility fallback, then large-scale behaviour analysis also falls under this category.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003eBP\u003c/strong\u003e: Bot-resistant and privacy-friendly, but not accessible. This category contains all approaches trying to use the remaining bits and pieces that are hard for AI and easy for humans, such as heavily relying on mouse movements, any puzzles involving complex user interaction, or any tricks playing with the limitations of the browser APIs.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003eAP\u003c/strong\u003e: Accessible and privacy-friendly, but not bot-resistant. This category is easy and includes many things down to the trivial case of no protection at all, or any attempt to fight off at least very simple bots without harming the other goals.\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\n\u003cp\u003eSince accessibility is \u003ca href=\"https://en.wikipedia.org/wiki/European_Accessibility_Act\"\u003erequired by law\u003c/a\u003e, we’re basically left with the first or last option.\nAs much as I hope that I’ve missed something significant, I feel the conclusion is inevitable:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eEvents will need to decide whether they want to protect against bots, or preserve high privacy standards. You will not be able to do both.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eIt remains, unfortunately, very hard to solve social problems with technology.\nLooking at social solutions, some have tried \u003ca href=\"https://en.wikipedia.org/wiki/Ticket_resale#International_responses\"\u003emaking ticket scalping illegal\u003c/a\u003e – with varying success.\nIf your country is not on that list, lobbying for it should be part of your solution strategy – but will most likely will be a very long road to a solution.\u003c/p\u003e\n\n\n        \u003c/section\u003e\n\n        \n\n        \n\n    \u003c/article\u003e\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": null,
  "modifiedTime": null
}
