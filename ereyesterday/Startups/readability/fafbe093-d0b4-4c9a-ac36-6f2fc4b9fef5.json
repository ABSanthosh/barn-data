{
  "id": "fafbe093-d0b4-4c9a-ac36-6f2fc4b9fef5",
  "title": "Flux is fast and it's open source",
  "link": "https://replicate.com/blog/flux-is-fast-and-open-source",
  "description": "Article URL: https://replicate.com/blog/flux-is-fast-and-open-source Comments URL: https://news.ycombinator.com/item?id=41824390 Points: 3 # Comments: 0",
  "author": "smusamashah",
  "published": "Sun, 13 Oct 2024 01:28:01 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "",
  "length": 3237,
  "excerpt": "FLUX is now much faster on Replicate, and we’ve made our optimizations open-source so you can see exactly how they work and build upon them.",
  "siteName": "",
  "favicon": "https://d31rfu1d3w8e4q.cloudfront.net/static/apple-touch-icon.1adc51db122a.png",
  "text": "FLUX is now much faster on Replicate, and we’ve made our optimizations open-source so you can see exactly how they work and build upon them. Here are the end-to-end speeds: FLUX.1 [schnell] at 512x512 and 4 steps: 0.29 seconds (P90: 0.49 seconds) FLUX.1 [schnell] at 1024x1024 and 4 steps: 0.72 seconds (P90: 0.95 seconds) FLUX.1 [dev] at 1024x1024 and 28 steps: 3.03 seconds (P90: 3.90 seconds) This is from the west coast of the US using the Python client. Here’s a demo of FLUX.1 [schnell]. (It’s live, just start typing!) Here’s the full app, and source code, if you’d like to check it out. How did we do it? Most of the models on Replicate are contributed by our community, but we maintain the FLUX models in collaboration with Black Forest Labs. We’ve done two main things to make FLUX faster: We optimized the model. We used Alex Redden’s flux-fp8-api as a starting point, then optimized it with torch.compile and used fast CuDNN attention kernels in the nightly Torch builds. We added a new synchronous HTTP API that makes all image models much faster on Replicate. The quantization in flux-fp8-api slightly changes the output of the model, but we have found it has little impact on the quality. We’ve created a tool that compares the output of thousands of prompts on FLUX.1 [schnell] and FLUX.1 [dev]. We’re not cherry picking. Take a look for yourself. You can disable this by setting the go_fast input on the model to false. We want to be open with you about how we’re optimizing the models. It’s notoriously hard to compare output between models and providers, and it’s often unclear whether providers are doing things that impact the quality of the model. We’re just going to tell you how we did it and let you disable any optimizations. That means you’re not wondering whether the output you’re getting is the best quality it can be. Most importantly, the code is open-source, so you can see exactly how it works: github.com/replicate/cog-flux Open-source should be fast too Open-source models are often slow out of the box. Model providers then optimize these models to make them fast and release them behind proprietary APIs, without contributing the improvements back to the community. We want to change that. We think open-source should be fast too. We’re open-sourcing all the improvements we make to FLUX. We’re also collaborating with the AI Compiler Study Group and other AI researchers to make an open-source fast version of FLUX. Making the FLUX optimizations open-source is not just the right thing to do, it also means all the experts in the world can collaborate together to make it the fastest. Pull requests welcome. It’s going to get faster New techniques are coming out all the time to make models faster, and by collaborating with the community, you can be sure that they’re going to be on Replicate as fast as possible. Stay tuned. Do more with FLUX You can do more than just run FLUX on Replicate. You can: Fine-tune FLUX on your own data (training and running trained models is going to much faster soon too!) Edit the code and deploy a custom version, if you’re doing something advanced Try out the models and compare outputs on our new playground Follow us on X to keep up to speed.",
  "image": "https://og.replicate.com/api/blog/flux-is-fast-and-open-source",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n  \u003cp\u003eFLUX is now much faster on Replicate, and we’ve made our optimizations open-source so you can see exactly how they work and build upon them.\u003c/p\u003e\n\u003cp\u003eHere are the end-to-end speeds:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://replicate.com/black-forest-labs/flux-schnell\"\u003eFLUX.1 [schnell]\u003c/a\u003e at 512x512 and 4 steps: 0.29 seconds (P90: 0.49 seconds)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://replicate.com/black-forest-labs/flux-schnell\"\u003eFLUX.1 [schnell]\u003c/a\u003e at 1024x1024 and 4 steps: 0.72 seconds (P90: 0.95 seconds)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://replicate.com/black-forest-labs/flux-dev\"\u003eFLUX.1 [dev]\u003c/a\u003e at 1024x1024 and 28 steps: 3.03 seconds (P90: 3.90 seconds)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is from the west coast of the US using the Python client.\u003c/p\u003e\n\u003cp\u003eHere’s a demo of FLUX.1 [schnell]. (It’s live, just start typing!)\u003c/p\u003e\n\n\n\u003cp\u003eHere’s \u003ca href=\"https://fast-flux-demo.replicate.workers.dev/\"\u003ethe full app, and source code\u003c/a\u003e, if you’d like to check it out.\u003c/p\u003e\n\u003ch2 id=\"how-did-we-do-it\"\u003eHow did we do it?\u003c/h2\u003e\n\u003cp\u003eMost of the models on Replicate are contributed by our community, but we maintain the FLUX models in collaboration with \u003ca href=\"https://blackforestlabs.ai/\"\u003eBlack Forest Labs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe’ve done two main things to make FLUX faster:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe optimized the model. We used Alex Redden’s \u003ca href=\"https://github.com/aredden/flux-fp8-api\"\u003eflux-fp8-api\u003c/a\u003e as a starting point, then optimized it with \u003ccode\u003etorch.compile\u003c/code\u003e and used fast CuDNN attention kernels in the nightly Torch builds.\u003c/li\u003e\n\u003cli\u003eWe added a new \u003ca href=\"https://replicate.com/changelog/2024-10-09-synchronous-api\"\u003esynchronous HTTP API\u003c/a\u003e that makes all image models much faster on Replicate.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe quantization in flux-fp8-api slightly changes the output of the model, but we have found it has little impact on the quality.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://flux-quality-comparison.vercel.app/\"\u003e\n\u003cimg src=\"https://d31rfu1d3w8e4q.cloudfront.net/static/blog/flux-is-fast/slow.webp\" title=\"Guess which is which!\"/\u003e\n\u003c/a\u003e\n\u003ca href=\"https://flux-quality-comparison.vercel.app/\"\u003e\n\u003cimg src=\"https://d31rfu1d3w8e4q.cloudfront.net/static/blog/flux-is-fast/fast.webp\" title=\"Guess which is which!\"/\u003e\n\u003c/a\u003e\n\u003c/p\u003e\n\n\u003cp\u003eWe’ve created a tool that compares the output of thousands of prompts on FLUX.1 [schnell] and FLUX.1 [dev]. We’re not cherry picking. \u003ca href=\"https://flux-quality-comparison.vercel.app/\"\u003eTake a look for yourself.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eYou can disable this by setting the \u003ccode\u003ego_fast\u003c/code\u003e input on the model to \u003ccode\u003efalse\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eWe want to be open with you about how we’re optimizing the models. It’s notoriously hard to compare output between models and providers, and it’s often unclear whether providers are doing things that impact the quality of the model.\u003c/p\u003e\n\u003cp\u003eWe’re just going to tell you how we did it and let you disable any optimizations. That means you’re not wondering whether the output you’re getting is the best quality it can be.\u003c/p\u003e\n\u003cp\u003eMost importantly, the code is open-source, so you can see exactly how it works: \u003ca href=\"https://github.com/replicate/cog-flux\"\u003egithub.com/replicate/cog-flux\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"open-source-should-be-fast-too\"\u003eOpen-source should be fast too\u003c/h2\u003e\n\u003cp\u003eOpen-source models are often slow out of the box. Model providers then optimize these models to make them fast and release them behind proprietary APIs, without contributing the improvements back to the community.\u003c/p\u003e\n\u003cp\u003eWe want to change that. We think open-source should be fast too.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/replicate/cog-flux\"\u003eWe’re open-sourcing all the improvements we make to FLUX.\u003c/a\u003e We’re also collaborating with the \u003ca href=\"https://github.com/ai-compiler-study\"\u003eAI Compiler Study Group\u003c/a\u003e and other AI researchers to make an open-source fast version of FLUX. \u003c/p\u003e\n\u003cp\u003eMaking the FLUX optimizations open-source is not just the right thing to do, it also means all the experts in the world can collaborate together to make it the fastest. Pull requests welcome.\u003c/p\u003e\n\u003ch2 id=\"its-going-to-get-faster\"\u003eIt’s going to get faster\u003c/h2\u003e\n\u003cp\u003eNew techniques are coming out all the time to make models faster, and by collaborating with the community, you can be sure that they’re going to be on Replicate as fast as possible. Stay tuned.\u003c/p\u003e\n\u003ch2 id=\"do-more-with-flux\"\u003eDo more with FLUX\u003c/h2\u003e\n\u003cp\u003eYou can do more than just run FLUX on Replicate. You can:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://replicate.com/blog/fine-tune-flux\"\u003eFine-tune FLUX on your own data\u003c/a\u003e (training and running trained models is going to much faster soon too!)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/replicate/cog-flux\"\u003eEdit the code and deploy a custom version\u003c/a\u003e, if you’re doing something advanced\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://replicate.com/playground/\"\u003eTry out the models and compare outputs on our new playground\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://x.com/replicate\"\u003eFollow us on X\u003c/a\u003e to keep up to speed.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": null,
  "modifiedTime": null
}
