{
  "id": "0fb2a68a-36af-4cb2-a9df-75706593f6ad",
  "title": "A new paradigm for AI: How ‘thinking as optimization’ leads to better general-purpose models",
  "link": "https://venturebeat.com/ai/a-new-paradigm-for-ai-how-thinking-as-optimization-leads-to-better-general-purpose-models/",
  "description": "A new AI model learns to \"think\" longer on hard problems, achieving more robust reasoning and better generalization to novel, unseen tasks.",
  "author": "Ben Dickson",
  "published": "Fri, 11 Jul 2025 22:26:03 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI research",
    "AI, ML and Deep Learning",
    "large language models",
    "large language models (LLMs)",
    "LLM reasoning",
    "LLMs",
    "reasoning models",
    "research",
    "System 2 Reasoning"
  ],
  "byline": "Ben Dickson",
  "length": 9378,
  "excerpt": "A new AI model learns to \"think\" longer on hard problems, achieving more robust reasoning and better generalization to novel, unseen tasks.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "July 11, 2025 3:26 PM Image credit: VentureBeat with Imagen 4 Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now Researchers at the University of Illinois Urbana-Champaign and the University of Virginia have developed a new model architecture that could lead to more robust AI systems with more powerful reasoning capabilities.  Called an energy-based transformer (EBT), the architecture shows a natural ability to use inference-time scaling to solve complex problems. For the enterprise, this could translate into cost-effective AI applications that can generalize to novel situations without the need for specialized fine-tuned models. The challenge of System 2 thinking In psychology, human thought is often divided into two modes: System 1, which is fast and intuitive, and System 2, which is slow, deliberate and analytical. Current large language models (LLMs) excel at System 1-style tasks, but the AI industry is increasingly focused on enabling System 2 thinking to tackle more complex reasoning challenges. Reasoning models use various inference-time scaling techniques to improve their performance on difficult problems. One popular method is reinforcement learning (RL), used in models like DeepSeek-R1 and OpenAI’s “o-series” models, where the AI is rewarded for producing reasoning tokens until it reaches the correct answer. Another approach, often called best-of-n, involves generating multiple potential answers and using a verification mechanism to select the best one.  However, these methods have significant drawbacks. They are often limited to a narrow range of easily verifiable problems, like math and coding, and can degrade performance on other tasks such as creative writing. Furthermore, recent evidence suggests that RL-based approaches might not be teaching models new reasoning skills, instead just making them more likely to use successful reasoning patterns they already know. This limits their ability to solve problems that require true exploration and are beyond their training regime. Energy-based models (EBM) The architecture proposes a different approach based on a class of models known as energy-based models (EBMs). The core idea is simple: Instead of directly generating an answer, the model learns an “energy function” that acts as a verifier. This function takes an input (like a prompt) and a candidate prediction and assigns a value, or “energy,” to it. A low energy score indicates high compatibility, meaning the prediction is a good fit for the input, while a high energy score signifies a poor match. Applying this to AI reasoning, the researchers propose in a paper that devs should view “thinking as an optimization procedure with respect to a learned verifier, which evaluates the compatibility (unnormalized probability) between an input and candidate prediction.” The process begins with a random prediction, which is then progressively refined by minimizing its energy score and exploring the space of possible solutions until it converges on a highly compatible answer. This approach is built on the principle that verifying a solution is often much easier than generating one from scratch. This “verifier-centric” design addresses three key challenges in AI reasoning. First, it allows for dynamic compute allocation, meaning models can “think” for longer on harder problems and shorter on easy problems. Second, EBMs can naturally handle the uncertainty of real-world problems where there isn’t one clear answer. Third, they act as their own verifiers, eliminating the need for external models. Unlike other systems that use separate generators and verifiers, EBMs combine both into a single, unified model. A key advantage of this arrangement is better generalization. Because verifying a solution on new, out-of-distribution (OOD) data is often easier than generating a correct answer, EBMs can better handle unfamiliar scenarios. Despite their promise, EBMs have historically struggled with scalability. To solve this, the researchers introduce EBTs, which are specialized transformer models designed for this paradigm. EBTs are trained to first verify the compatibility between a context and a prediction, then refine predictions until they find the lowest-energy (most compatible) output. This process effectively simulates a thinking process for every prediction. The researchers developed two EBT variants: A decoder-only model inspired by the GPT architecture, and a bidirectional model similar to BERT. Energy-based transformer (source: GitHub) The architecture of EBTs make them flexible and compatible with various inference-time scaling techniques. “EBTs can generate longer CoTs, self-verify, do best-of-N [or] you can sample from many EBTs,” Alexi Gladstone, a PhD student in computer science at the University of Illinois Urbana-Champaign and lead author of the paper, told VentureBeat. “The best part is, all of these capabilities are learned during pretraining.” EBTs in action The researchers compared EBTs against established architectures: the popular transformer++ recipe for text generation (discrete modalities) and the diffusion transformer (DiT) for tasks like video prediction and image denoising (continuous modalities). They evaluated the models on two main criteria: “Learning scalability,” or how efficiently they train, and “thinking scalability,” which measures how performance improves with more computation at inference time. During pretraining, EBTs demonstrated superior efficiency, achieving an up to 35% higher scaling rate than Transformer++ across data, batch size, parameters and compute. This means EBTs can be trained faster and more cheaply.  At inference, EBTs also outperformed existing models on reasoning tasks. By “thinking longer” (using more optimization steps) and performing “self-verification” (generating multiple candidates and choosing the one with the lowest energy), EBTs improved language modeling performance by 29% more than Transformer++. “This aligns with our claims that because traditional feed-forward transformers cannot dynamically allocate additional computation for each prediction being made, they are unable to improve performance for each token by thinking for longer,” the researchers write. For image denoising, EBTs achieved better results than DiTs while using 99% fewer forward passes.  Crucially, the study found that EBTs generalize better than the other architectures. Even with the same or worse pretraining performance, EBTs outperformed existing models on downstream tasks. The performance gains from System 2 thinking were most substantial on data that was further out-of-distribution (different from the training data), suggesting that EBTs are particularly robust when faced with novel and challenging tasks. The researchers suggest that “the benefits of EBTs’ thinking are not uniform across all data but scale positively with the magnitude of distributional shifts, highlighting thinking as a critical mechanism for robust generalization beyond training distributions.” The benefits of EBTs are important for two reasons. First, they suggest that at the massive scale of today’s foundation models, EBTs could significantly outperform the classic transformer architecture used in LLMs. The authors note that “at the scale of modern foundation models trained on 1,000X more data with models 1,000X larger, we expect the pretraining performance of EBTs to be significantly better than that of the Transformer++ recipe.” Second, EBTs show much better data efficiency. This is a critical advantage in an era where high-quality training data is becoming a major bottleneck for scaling AI. “As data has become one of the major limiting factors in further scaling, this makes EBTs especially appealing,” the paper concludes.  Despite its different inference mechanism, the EBT architecture is highly compatible with the transformer, making it possible to use them as a drop-in replacement for current LLMs.  “EBTs are very compatible with current hardware/inference frameworks,” Gladstone said, including speculative decoding using feed-forward models on both GPUs or TPUs. He said he is also confident they can run on specialized accelerators such as LPUs and optimization algorithms such as FlashAttention-3, or can be deployed through common inference frameworks like vLLM. For developers and enterprises, the strong reasoning and generalization capabilities of EBTs could make them a powerful and reliable foundation for building the next generation of AI applications. “Thinking longer can broadly help on almost all enterprise applications, but I think the most exciting will be those requiring more important decisions, safety or applications with limited data,” Gladstone said. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/07/Energy-based-transformer.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-07-11T22:26:03+00:00\" datetime=\"2025-07-11T22:26:03+00:00\"\u003eJuly 11, 2025 3:26 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"409\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/Energy-based-transformer.png?w=750\" alt=\"Image credit: VentureBeat with Imagen 4\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with Imagen 4\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\u003c/em\u003e \u003cem\u003e\u003ca href=\"https://venturebeat.com/newsletters/\"\u003eSubscribe Now\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eResearchers at the University of Illinois Urbana-Champaign and the University of Virginia have developed a new model architecture that could lead to more robust AI systems with more powerful reasoning capabilities. \u003c/p\u003e\n\n\n\n\u003cp\u003eCalled an \u003ca href=\"https://github.com/alexiglad/ebt\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eenergy-based transformer\u003c/a\u003e (EBT), the architecture shows a natural ability to use inference-time scaling to solve complex problems. For the enterprise, this could translate into cost-effective AI applications that can generalize to novel situations without the need for specialized fine-tuned models.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-challenge-of-system-2-thinking\"\u003eThe challenge of System 2 thinking\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn psychology, human thought is often divided into two modes: System 1, which is fast and intuitive, and \u003ca href=\"https://venturebeat.com/ai/meta-researchers-distill-system-2-thinking-into-llms-improving-performance-on-complex-reasoning/\"\u003eSystem 2\u003c/a\u003e, which is slow, deliberate and analytical. Current large language models (LLMs) excel at System 1-style tasks, but the AI industry is increasingly focused on enabling System 2 thinking to tackle more complex reasoning challenges.\u003c/p\u003e\n\n\n\n\u003cp\u003eReasoning models use various \u003ca href=\"https://venturebeat.com/ai/how-test-time-scaling-unlocks-hidden-reasoning-abilities-in-small-language-models-and-allows-them-to-outperform-llms/\"\u003einference-time scaling techniques\u003c/a\u003e to improve their performance on difficult problems. One popular method is reinforcement learning (RL), used in models like \u003ca href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/\"\u003eDeepSeek-R1\u003c/a\u003e and OpenAI’s “\u003ca href=\"https://venturebeat.com/ai/five-breakthroughs-that-make-openais-o3-a-turning-point-for-ai-and-one-big-challenge/\"\u003eo-series\u003c/a\u003e” models, where the AI is rewarded for producing reasoning tokens until it reaches the correct answer. Another approach, often called best-of-n, involves generating multiple potential answers and using a verification mechanism to select the best one. \u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, these methods have significant drawbacks. They are often limited to a narrow range of easily verifiable problems, like math and coding, and can degrade performance on other tasks such as creative writing. Furthermore, \u003ca href=\"https://arxiv.org/abs/2504.13837\" target=\"_blank\" rel=\"noreferrer noopener\"\u003erecent evidence\u003c/a\u003e suggests that RL-based approaches might not be teaching models new reasoning skills, instead just making them more likely to use successful reasoning patterns they already know. This limits their ability to solve problems that require true exploration and are beyond their training regime.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-energy-based-models-ebm\"\u003eEnergy-based models (EBM)\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe architecture proposes a different approach based on a class of models known as energy-based models (EBMs). The core idea is simple: Instead of directly generating an answer, the model learns an “energy function” that acts as a verifier. This function takes an input (like a prompt) and a candidate prediction and assigns a value, or “energy,” to it. A low energy score indicates high compatibility, meaning the prediction is a good fit for the input, while a high energy score signifies a poor match.\u003c/p\u003e\n\n\n\n\u003cp\u003eApplying this to AI reasoning, the researchers propose in \u003ca href=\"https://arxiv.org/abs/2507.02092\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ea paper\u003c/a\u003e that devs should view “thinking as an optimization procedure with respect to a learned verifier, which evaluates the compatibility (unnormalized probability) between an input and candidate prediction.” The process begins with a random prediction, which is then progressively refined by minimizing its energy score and exploring the space of possible solutions until it converges on a highly compatible answer. This approach is built on the principle that verifying a solution is often much easier than generating one from scratch.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" height=\"419\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image_b7d491.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image_b7d491.png 986w, https://venturebeat.com/wp-content/uploads/2025/07/image_b7d491.png?resize=300,157 300w, https://venturebeat.com/wp-content/uploads/2025/07/image_b7d491.png?resize=768,402 768w, https://venturebeat.com/wp-content/uploads/2025/07/image_b7d491.png?resize=800,419 800w, https://venturebeat.com/wp-content/uploads/2025/07/image_b7d491.png?resize=400,209 400w, https://venturebeat.com/wp-content/uploads/2025/07/image_b7d491.png?resize=750,392 750w, https://venturebeat.com/wp-content/uploads/2025/07/image_b7d491.png?resize=578,302 578w, https://venturebeat.com/wp-content/uploads/2025/07/image_b7d491.png?resize=930,487 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThis “verifier-centric” design addresses three key challenges in AI reasoning. First, it allows for dynamic compute allocation, meaning models can “think” for longer on harder problems and shorter on easy problems. Second, EBMs can naturally handle the uncertainty of real-world problems where there isn’t one clear answer. Third, they act as their own verifiers, eliminating the need for external models. \u003c/p\u003e\n\n\n\n\u003cp\u003eUnlike other systems that use separate generators and verifiers, EBMs combine both into a single, unified model. A key advantage of this arrangement is better generalization. Because verifying a solution on new, out-of-distribution (OOD) data is often easier than generating a correct answer, EBMs can better handle unfamiliar scenarios.\u003c/p\u003e\n\n\n\n\u003cp\u003eDespite their promise, EBMs have historically struggled with scalability. To solve this, the researchers introduce EBTs, which are specialized \u003ca href=\"https://bdtechtalks.com/2022/05/02/what-is-the-transformer/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etransformer models\u003c/a\u003e designed for this paradigm. EBTs are trained to first verify the compatibility between a context and a prediction, then refine predictions until they find the lowest-energy (most compatible) output. This process effectively simulates a thinking process for every prediction. The researchers developed two EBT variants: A decoder-only model inspired by the GPT architecture, and a bidirectional model similar to BERT.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" height=\"263\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png 9491w, https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png?resize=300,99 300w, https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png?resize=768,253 768w, https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png?resize=800,263 800w, https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png?resize=1536,506 1536w, https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png?resize=2048,674 2048w, https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png?resize=400,132 400w, https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png?resize=750,247 750w, https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png?resize=578,190 578w, https://venturebeat.com/wp-content/uploads/2025/07/image_41149a.png?resize=930,306 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eEnergy-based transformer (source: GitHub)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe architecture of EBTs make them flexible and compatible with various inference-time scaling techniques. “EBTs can generate longer CoTs, self-verify, do best-of-N [or] you can sample from many EBTs,” Alexi Gladstone, a PhD student in computer science at the University of Illinois Urbana-Champaign and lead author of the paper, told VentureBeat. “The best part is, all of these capabilities are learned during pretraining.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-ebts-in-action\"\u003eEBTs in action\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers compared EBTs against established architectures: the popular \u003ca href=\"https://arxiv.org/abs/2003.04974\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etransformer++\u003c/a\u003e recipe for text generation (discrete modalities) and the diffusion transformer (DiT) for tasks like video prediction and image denoising (continuous modalities). They evaluated the models on two main criteria: “Learning scalability,” or how efficiently they train, and “thinking scalability,” which measures how performance improves with more computation at inference time.\u003c/p\u003e\n\n\n\n\u003cp\u003eDuring pretraining, EBTs demonstrated superior efficiency, achieving an up to 35% higher scaling rate than Transformer++ across data, batch size, parameters and compute. This means EBTs can be trained faster and more cheaply. \u003c/p\u003e\n\n\n\n\u003cp\u003eAt inference, EBTs also outperformed existing models on reasoning tasks. By “thinking longer” (using more optimization steps) and performing “self-verification” (generating multiple candidates and choosing the one with the lowest energy), EBTs improved language modeling performance by 29% more than Transformer++. “This aligns with our claims that because traditional feed-forward transformers cannot dynamically allocate additional computation for each prediction being made, they are unable to improve performance for each token by thinking for longer,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor image denoising, EBTs achieved better results than DiTs while using 99% fewer forward passes. \u003c/p\u003e\n\n\n\n\u003cp\u003eCrucially, the study found that EBTs generalize better than the other architectures. Even with the same or worse pretraining performance, EBTs outperformed existing models on downstream tasks. The performance gains from System 2 thinking were most substantial on data that was further out-of-distribution (different from the training data), suggesting that EBTs are particularly robust when faced with novel and challenging tasks. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers suggest that “the benefits of EBTs’ thinking are not uniform across all data but scale positively with the magnitude of distributional shifts, highlighting thinking as a critical mechanism for robust generalization beyond training distributions.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe benefits of EBTs are important for two reasons. First, they suggest that at the massive scale of today’s foundation models, EBTs could significantly outperform the classic transformer architecture used in LLMs. The authors note that “at the scale of modern foundation models trained on 1,000X more data with models 1,000X larger, we expect the pretraining performance of EBTs to be significantly better than that of the Transformer++ recipe.”\u003c/p\u003e\n\n\n\n\u003cp\u003eSecond, EBTs show much better data efficiency. This is a critical advantage in an era where high-quality training data is becoming a major bottleneck for scaling AI. “As data has become one of the major limiting factors in further scaling, this makes EBTs especially appealing,” the paper concludes. \u003c/p\u003e\n\n\n\n\u003cp\u003eDespite its different inference mechanism, the EBT architecture is highly compatible with the transformer, making it possible to use them as a drop-in replacement for current LLMs. \u003c/p\u003e\n\n\n\n\u003cp\u003e“EBTs are very compatible with current hardware/inference frameworks,” Gladstone said, including speculative decoding using feed-forward models on both GPUs or TPUs. He said he is also confident they can run on specialized accelerators such as LPUs and optimization algorithms such as \u003ca href=\"https://venturebeat.com/ai/flashattention-3-unleashes-the-power-of-h100-gpus-for-llms/\"\u003eFlashAttention-3\u003c/a\u003e, or can be deployed through common inference frameworks like vLLM.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor developers and enterprises, the strong reasoning and generalization capabilities of EBTs could make them a powerful and reliable foundation for building the next generation of AI applications. “Thinking longer can broadly help on almost all enterprise applications, but I think the most exciting will be those requiring more important decisions, safety or applications with limited data,” Gladstone said.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-07-11T22:26:03Z",
  "modifiedTime": "2025-07-11T22:26:11Z"
}
