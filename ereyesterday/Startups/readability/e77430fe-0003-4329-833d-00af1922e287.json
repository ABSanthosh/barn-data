{
  "id": "e77430fe-0003-4329-833d-00af1922e287",
  "title": "DeepCoder delivers top coding performance in efficient 14B open model",
  "link": "https://venturebeat.com/ai/deepcoder-delivers-top-coding-performance-in-efficient-14b-open-model/",
  "description": "DeepCoder-14B competes with frontier models like o3 and o1—and the weights, code, and optimization platform are open source.",
  "author": "Ben Dickson",
  "published": "Thu, 10 Apr 2025 22:19:54 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI Coding",
    "AI research",
    "AI, ML and Deep Learning",
    "Codeforces",
    "Deepseek R1",
    "Group Relative Policy Optimization (GRPO)",
    "HumanEval+",
    "large language models",
    "large language models (LLMs)",
    "LiveCodeBench (LCB)",
    "LLMs",
    "reinforcement learning",
    "research",
    "Together AI"
  ],
  "byline": "Ben Dickson",
  "length": 7128,
  "excerpt": "DeepCoder-14B competes with frontier models like o3 and o1—and the weights, code, and optimization platform are open source.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "April 10, 2025 3:19 PM A robot writing codeImage Credit: Venturebeat made with Ideogram Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Researchers at Together AI and Agentica have released DeepCoder-14B, a new coding model that delivers impressive performance comparable to leading proprietary models like OpenAI’s o3-mini.  Built on top of DeepSeek-R1, this model gives more flexibility to integrate high-performance code generation and reasoning capabilities into real-world applications. Importantly, the teams have fully open-sourced the model, its training data, code, logs and system optimizations, which can help researchers improve their work and accelerate progress. Competitive coding capabilities in a smaller package The research team’s experiments show that DeepCoder-14B performs strongly across several challenging coding benchmarks, including LiveCodeBench (LCB), Codeforces and HumanEval+. “Our model demonstrates strong performance across all coding benchmarks… comparable to the performance of o3-mini (low) and o1,” the researchers write in a blog post that describes the model. Interestingly, despite being trained primarily on coding tasks, the model shows improved mathematical reasoning, scoring 73.8% on the AIME 2024 benchmark, a 4.1% improvement over its base model (DeepSeek-R1-Distill-Qwen-14B). This suggests that the reasoning skills developed through RL on code can be generalized effectively to other domains. Credit: Together AI The most striking aspect is achieving this level of performance with only 14 billion parameters. This makes DeepCoder significantly smaller and potentially more efficient to run than many frontier models. Innovations driving DeepCoder’s performance While developing the model, the researchers solved some of the key challenges in training coding models using reinforcement learning (RL). The first challenge was curating the training data. Reinforcement learning requires reliable reward signals indicating the model’s output is correct. As the researchers point out, “Unlike math—where abundant high-quality, verifiable data is readily available on the Internet—the coding domain suffers from a relative scarcity of such data.”  To address this problem, the DeepCoder team implemented a strict pipeline that gathers examples from different datasets and filters them for validity, complexity and duplication. This process yielded 24,000 high-quality problems, providing a solid foundation for effective RL training. The team also designed a straightforward reward function that only provides a positive signal if the generated code passes all sampled unit tests for the problem within a specific time limit. Combined with the high-quality training examples, this outcome-focused reward system prevents the model from learning tricks like printing memorized answers for public tests or optimizing for simple edge cases without solving the core problem. The model’s core training algorithm is based on Group Relative Policy Optimization (GRPO), a reinforcement learning algorithm that proved very successful in DeepSeek-R1. However, the team made several modifications to the algorithm to make it more stable and allow the model to continue improving as the training extends for a longer time. GRPO+ enables DeepCoder-14 to continue for longer durations without collapsing Credit: Together AI Finally, the team extended the model’s context window iteratively, first training it on shorter reasoning sequences and gradually increasing the length. They also developed a filtering method to avoid penalizing the model when it created reasoning chains that exceeded the context limits when solving a hard prompt.  DeepCoder was trained on 32K context problems but was also able to solve 64K tasks Credit: Together AI The researchers explain the core idea: “To preserve long-context reasoning while enabling efficient training, we incorporated overlong filtering… This technique masks out truncated sequences during training so that models aren’t penalized for generating thoughtful but lengthy outputs that exceed the current context limit.”  The training was gradually scaled from a 16K to a 32K context window, and the resulting model could also solve problems that required up to 64K tokens. Optimizing long-context RL training Training large models with RL, especially on tasks requiring long generated sequences like coding or complex reasoning, is computationally intensive and slow. A major bottleneck is the “sampling” step, where the model generates potentially thousands of tokens per example in the batch. Variations in response length mean some responses finish much later than others, leaving GPUs idle and slowing down the entire training loop.  To accelerate this, the team developed verl-pipeline, an optimized extension of the open-source verl library for reinforcement learning from human feedback (RLHF). The key innovation, which they call “One-Off Pipelining,” rearranges the response sampling and model updates to reduce the bottlenecks and accelerator idle time. One-Off Pipelining Their experiments showed that one-off pipelining provided up to a 2x speedup for coding RL tasks compared to baseline implementations. This optimization was crucial for training DeepCoder within a reasonable timeframe (2.5 weeks on 32 H100s) and is now open-sourced as part of verl-pipeline for the community to use and build upon.  Enterprise impact The researchers have made all the artifacts for training and running DeepCoder-14B available on GitHub and Hugging Face under a permissive license. “By fully sharing our dataset, code, and training recipe, we empower the community to reproduce our work and make RL training accessible to all,” the researchers write. DeepCoder-14B powerfully illustrates a broader, accelerating trend in the AI landscape: the rise of highly capable yet efficient and openly accessible models.  For the enterprise world, this shift signifies more options and higher accessibility of advanced models. Cutting-edge performance is no longer solely the domain of hyperscalers or those willing to pay premium API fees. Models like DeepCoder can empower organizations of all sizes to leverage sophisticated code generation and reasoning, customize solutions to their specific needs, and securely deploy them within their environments.  This trend can lower the barrier to entry for AI adoption and foster a more competitive and innovative ecosystem, where progress is driven through open source collaboration. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/03/a_robot_working_as_a_programmer_writing_on_a_mo.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-04-10T22:19:54+00:00\" datetime=\"2025-04-10T22:19:54+00:00\"\u003eApril 10, 2025 3:19 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"422\" src=\"https://venturebeat.com/wp-content/uploads/2024/03/a_robot_working_as_a_programmer_writing_on_a_mo.jpg?w=750\" alt=\"A robot writing code\"/\u003e\u003c/p\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eA robot writing code\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eImage Credit: Venturebeat made with Ideogram\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eResearchers at \u003ca href=\"https://www.together.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eTogether AI\u003c/a\u003e and \u003ca href=\"https://agentica-project.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAgentica\u003c/a\u003e have released DeepCoder-14B, a new coding model that delivers impressive performance comparable to leading proprietary models like \u003ca href=\"https://venturebeat.com/ai/its-here-openais-o3-mini-advanced-reasoning-model-arrives-to-counter-deepseeks-rise/\"\u003eOpenAI’s o3-mini\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eBuilt on top of DeepSeek-R1, this model gives more flexibility to integrate high-performance code generation and reasoning capabilities into real-world applications. Importantly, the teams have fully open-sourced the model, its training data, code, logs and system optimizations, which can help researchers improve their work and accelerate progress.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-competitive-coding-capabilities-in-a-smaller-package\"\u003eCompetitive coding capabilities in a smaller package\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe research team’s experiments show that DeepCoder-14B performs strongly across several challenging coding benchmarks, including LiveCodeBench (LCB), Codeforces and HumanEval+.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Our model demonstrates strong performance across all coding benchmarks… comparable to the performance of o3-mini (low) and o1,” the researchers write in a \u003ca href=\"https://www.together.ai/blog/deepcoder\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eblog post\u003c/a\u003e that describes the model.\u003c/p\u003e\n\n\n\n\u003cp\u003eInterestingly, despite being trained primarily on coding tasks, the model shows improved mathematical reasoning, scoring 73.8% on the AIME 2024 benchmark, a 4.1% improvement over its base model (DeepSeek-R1-Distill-Qwen-14B). This suggests that the reasoning skills developed through RL on code can be generalized effectively to other domains.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1812\" height=\"728\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_89b587.png?w=800\" alt=\"DeepCoder-14B performance\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_89b587.png 1812w, https://venturebeat.com/wp-content/uploads/2025/04/image_89b587.png?resize=300,121 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_89b587.png?resize=768,309 768w, https://venturebeat.com/wp-content/uploads/2025/04/image_89b587.png?resize=800,321 800w, https://venturebeat.com/wp-content/uploads/2025/04/image_89b587.png?resize=1536,617 1536w, https://venturebeat.com/wp-content/uploads/2025/04/image_89b587.png?resize=400,161 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_89b587.png?resize=750,301 750w, https://venturebeat.com/wp-content/uploads/2025/04/image_89b587.png?resize=578,232 578w, https://venturebeat.com/wp-content/uploads/2025/04/image_89b587.png?resize=930,374 930w\" sizes=\"(max-width: 1812px) 100vw, 1812px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eCredit: Together AI\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe most striking aspect is achieving this level of performance with only 14 billion parameters. This makes DeepCoder significantly smaller and potentially more efficient to run than many frontier models.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-innovations-driving-deepcoder-s-performance\"\u003eInnovations driving DeepCoder’s performance\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile developing the model, the researchers solved some of the key challenges in \u003ca href=\"https://venturebeat.com/ai/code-in-pre-training-data-improves-llms-performance-at-non-coding-tasks/\"\u003etraining coding models\u003c/a\u003e using reinforcement learning (RL).\u003c/p\u003e\n\n\n\n\u003cp\u003eThe first challenge was curating the training data. Reinforcement learning requires reliable reward signals indicating the model’s output is correct. As the researchers point out, “Unlike math—where abundant high-quality, verifiable data is readily available on the Internet—the coding domain suffers from a relative scarcity of such data.” \u003c/p\u003e\n\n\n\n\u003cp\u003eTo address this problem, the DeepCoder team implemented a strict pipeline that gathers examples from different datasets and filters them for validity, complexity and duplication. This process yielded 24,000 high-quality problems, providing a solid foundation for effective RL training.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe team also designed a straightforward reward function that only provides a positive signal if the generated code passes all sampled unit tests for the problem within a specific time limit. Combined with the high-quality training examples, this outcome-focused reward system prevents the model from learning tricks like printing memorized answers for public tests or optimizing for simple edge cases without solving the core problem.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe model’s core training algorithm is based on Group Relative Policy Optimization (GRPO), a reinforcement learning algorithm that proved \u003ca href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/\"\u003every successful in DeepSeek-R1\u003c/a\u003e. However, the team made several modifications to the algorithm to make it more stable and allow the model to continue improving as the training extends for a longer time.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"2048\" height=\"1529\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_39acb9.png?w=800\" alt=\"GRPO+\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_39acb9.png 2048w, https://venturebeat.com/wp-content/uploads/2025/04/image_39acb9.png?resize=300,224 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_39acb9.png?resize=768,573 768w, https://venturebeat.com/wp-content/uploads/2025/04/image_39acb9.png?resize=800,597 800w, https://venturebeat.com/wp-content/uploads/2025/04/image_39acb9.png?resize=1536,1147 1536w, https://venturebeat.com/wp-content/uploads/2025/04/image_39acb9.png?resize=400,299 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_39acb9.png?resize=750,560 750w, https://venturebeat.com/wp-content/uploads/2025/04/image_39acb9.png?resize=578,432 578w, https://venturebeat.com/wp-content/uploads/2025/04/image_39acb9.png?resize=930,694 930w\" sizes=\"(max-width: 2048px) 100vw, 2048px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eGRPO+ enables DeepCoder-14 to continue for longer durations without collapsing Credit: Together AI\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eFinally, the team extended the model’s context window iteratively, first training it on shorter reasoning sequences and gradually increasing the length. They also developed a filtering method to avoid penalizing the model when it created reasoning chains that exceeded the context limits when solving a hard prompt. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"2048\" height=\"1083\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_4c875d.png?w=800\" alt=\"iterative context extension\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_4c875d.png 2048w, https://venturebeat.com/wp-content/uploads/2025/04/image_4c875d.png?resize=300,159 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_4c875d.png?resize=768,406 768w, https://venturebeat.com/wp-content/uploads/2025/04/image_4c875d.png?resize=800,423 800w, https://venturebeat.com/wp-content/uploads/2025/04/image_4c875d.png?resize=1536,812 1536w, https://venturebeat.com/wp-content/uploads/2025/04/image_4c875d.png?resize=400,212 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_4c875d.png?resize=750,397 750w, https://venturebeat.com/wp-content/uploads/2025/04/image_4c875d.png?resize=578,306 578w, https://venturebeat.com/wp-content/uploads/2025/04/image_4c875d.png?resize=930,492 930w\" sizes=\"(max-width: 2048px) 100vw, 2048px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eDeepCoder was trained on 32K context problems but was also able to solve 64K tasks Credit: Together AI\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe researchers explain the core idea: “To preserve long-context reasoning while enabling efficient training, we incorporated overlong filtering… This technique masks out truncated sequences during training so that models aren’t penalized for generating thoughtful but lengthy outputs that exceed the current context limit.” \u003c/p\u003e\n\n\n\n\u003cp\u003eThe training was gradually scaled from a 16K to a 32K context window, and the resulting model could also solve problems that required up to 64K tokens.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-optimizing-long-context-rl-training\"\u003eOptimizing long-context RL training\u003c/h2\u003e\n\n\n\n\u003cp\u003eTraining large models with RL, especially on tasks requiring long generated sequences like coding or complex reasoning, is computationally intensive and slow. A major bottleneck is the “sampling” step, where the model generates potentially thousands of tokens per example in the batch. Variations in response length mean some responses finish much later than others, leaving GPUs idle and slowing down the entire training loop. \u003c/p\u003e\n\n\n\n\u003cp\u003eTo accelerate this, the team developed verl-pipeline, an optimized extension of the open-source verl library for \u003ca href=\"https://bdtechtalks.com/2023/01/16/what-is-rlhf/\"\u003ereinforcement learning from human feedback\u003c/a\u003e (RLHF). The key innovation, which they call “One-Off Pipelining,” rearranges the response sampling and model updates to reduce the bottlenecks and accelerator idle time.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1027\" height=\"267\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_3a530d.png?w=800\" alt=\"One-Off Pipelining\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_3a530d.png 1027w, https://venturebeat.com/wp-content/uploads/2025/04/image_3a530d.png?resize=300,78 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_3a530d.png?resize=768,200 768w, https://venturebeat.com/wp-content/uploads/2025/04/image_3a530d.png?resize=800,208 800w, https://venturebeat.com/wp-content/uploads/2025/04/image_3a530d.png?resize=400,104 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_3a530d.png?resize=750,195 750w, https://venturebeat.com/wp-content/uploads/2025/04/image_3a530d.png?resize=578,150 578w, https://venturebeat.com/wp-content/uploads/2025/04/image_3a530d.png?resize=930,242 930w\" sizes=\"auto, (max-width: 1027px) 100vw, 1027px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eOne-Off Pipelining\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eTheir experiments showed that one-off pipelining provided up to a 2x speedup for coding RL tasks compared to baseline implementations. This optimization was crucial for training DeepCoder within a reasonable timeframe (2.5 weeks on 32 H100s) and is now open-sourced as part of verl-pipeline for the community to use and build upon. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-enterprise-impact\"\u003eEnterprise impact\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers have made all the artifacts for training and running DeepCoder-14B available on \u003ca href=\"https://github.com/agentica-project/rllm\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGitHub\u003c/a\u003e and \u003ca href=\"https://huggingface.co/agentica-org/DeepCoder-14B-Preview\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e under a permissive license.\u003c/p\u003e\n\n\n\n\u003cp\u003e“By fully sharing our dataset, code, and training recipe, we empower the community to reproduce our work and make RL training accessible to all,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cp\u003eDeepCoder-14B powerfully illustrates a broader, accelerating trend in the AI landscape: the rise of highly capable yet efficient and openly accessible models. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor the enterprise world, this shift signifies more options and higher accessibility of advanced models. Cutting-edge performance is no longer solely the domain of hyperscalers or those willing to pay premium API fees. Models like DeepCoder can empower organizations of all sizes to leverage sophisticated code generation and reasoning, customize solutions to their specific needs, and securely deploy them within their environments. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis trend can lower the barrier to entry for AI adoption and foster a more competitive and innovative ecosystem, where progress is driven through open source collaboration.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-04-10T22:19:54Z",
  "modifiedTime": "2025-04-10T22:20:03Z"
}
