{
  "id": "3fada71f-778f-4c3f-ae8a-65ab90e35d46",
  "title": "s3: The new RAG framework that trains search agents with minimal data",
  "link": "https://venturebeat.com/ai/s3-the-new-rag-framework-that-trains-search-agents-with-minimal-data/",
  "description": "S3 decouples RAG search from generation, boosting efficiency and generalization for enterprise LLM applications with minimal data.",
  "author": "Ben Dickson",
  "published": "Wed, 28 May 2025 22:51:11 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI, ML and Deep Learning",
    "Claude 3 Haiku",
    "Fine-tuning large language models",
    "Gain Beyond RAG (GBR)",
    "large language models",
    "large language models (LLMs)",
    "LLMs",
    "Open source",
    "Qwen2.5-7B-Instruct",
    "reinforcement learning",
    "retrieval augmented generation",
    "Retrieval-augmented generation (RAG)",
    "S3",
    "search agent",
    "search engines",
    "Search-R1"
  ],
  "byline": "Ben Dickson",
  "length": 7028,
  "excerpt": "S3 decouples RAG search from generation, boosting efficiency and generalization for enterprise LLM applications with minimal data.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "May 28, 2025 3:51 PM Image credit: VentureBeat with Gemini Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Researchers at University of Illinois Urbana-Champaign have introduced s3, an open-source framework designed to build retrieval-augmented generation (RAG) systems more efficiently than current methods.  s3 can benefit developers creating real-world large language model (LLM) applications, as it simplifies and reduces the cost of creating retriever models within RAG architectures. RAG retrieval The effectiveness of any RAG system hinges on the quality of its retrieval component. In their paper, the researchers categorize the evolution of RAG approaches into three distinct phases. “Classic RAG” systems rely on static retrieval methods with fixed queries, where retrieval quality is disconnected from the ultimate generation performance. These architectures struggle with queries requiring contextual or multi-hop reasoning. A subsequent phase, dubbed “Pre-RL-Zero,” introduces more active LLM participation during inference. These techniques involved multi-turn interactions, interleaving query generation, retrieval, and reasoning. However, they typically depend on zero-shot prompting and lack trainable components to optimize retrieval through direct outcome signals. The most recent phase, “RL-Zero,” leverages reinforcement learning (RL) to train models to act as search agents, improving through outcome-based feedback like answer correctness. An example is Search-R1, which trains the model to interleave reasoning with search queries and retrieved context. Despite their advancements, existing RL-Zero approaches often optimize retrieval using search-centric metrics that ignore downstream utility. Moreover, they require fine-tuning the LLM, which is costly and error-prone. By entangling retrieval with generation, they limit real search utility and compatibility with frozen or proprietary models.  Different types of RAG Source: arXiv As the researchers put it, “This motivates a shift toward a modular framework where search and generation are cleanly separated, and optimization focuses purely on search quality with respect to downstream utility.” s3 The s3 framework addresses this challenge with a model-agnostic approach. The main idea is to train a search agent with structured, multi-turn access to external knowledge. This search agent improves the quality of the retrieval stage without affecting the LLM that generates the final answer. In s3, a dedicated searcher LLM iteratively interacts with a search engine. It generates queries based on the prompt, retrieves relevant documents, selects a useful subset of evidence, and decides whether to continue searching for more information. Once the search concludes, a separate, frozen generator LLM consumes this accumulated evidence to produce the final answer. s3 framework Source: arXiv A core innovation of s3 is its reward signal, Gain Beyond RAG (GBR). GBR quantifies the improvement in the generator’s accuracy when conditioned on documents retrieved by s3, compared to a baseline that retrieves the top documents matching the query. This reward incentivizes the searcher to find documents that truly enhance the generator’s output quality.  “s3 decouples the retriever (searcher) from the generator. This lets companies plug in any off-the-shelf or proprietary LLM—whether GPT-4, Claude, or an internal model—without having to fine-tune it,” Patrick (Pengcheng) Jiang, lead author of the paper and doctoral student at UIUC, told VentureBeat. “For enterprises with regulatory or contractual constraints on model modification, or those that rely on closed-source LLM APIs, this modularity makes s3 highly practical. It allows them to enhance search quality without touching their generation infrastructure.” s3 in action The researchers tested s3 across six general-domain question-answering benchmarks, comparing it against three categories of RAG systems: End-to-end fine-tuning (e.g., Search-R1), static retrieval with frozen generators (such as classic RAG pipelines) and active retrieval with frozen generators (e.g., combining documents obtained by Search-R1 with a frozen LLM). In their experiments, they used Qwen2.5-7B-Instruct as the base model for the searcher and Qwen2.5-14B-Instruct and Claude 3 Haiku as the frozen generator LLMs. s3 surpassed static, zero-shot and end-to-end tuned baselines on most benchmarks and achieved an average score. Its data efficiency is particularly noteworthy: s3 achieved strong gains with only 2.4k training examples, significantly less than the 70k examples required by DeepRetrieval (a static retrieval framework) or the 170k needed by Search-R1, while outperforming both in context quality and final answer performance. s3 vs other RAG techniques Source: GitHub “Many enterprises lack large-scale annotated QA datasets or the GPU infrastructure to fine-tune end-to-end LLM systems. s3 lowers the barrier by enabling strong retrieval performance with minimal supervision and compute,” Jiang said. “This means faster prototyping, reduced costs and quicker time-to-deployment for AI-powered search applications.” The findings suggest a fundamental shift in optimization strategy. As the researchers note in the paper, most of the performance gain in RAG stems from “improving the search capability instead of aligning generation outputs,” which implies that focusing RL on search strategy rather than combined generation alignment yields better results. Another crucial finding for enterprise applications is s3’s ability to generalize to domains it has not been trained on. s3 showed zero-shot success on medical QA despite training only on general QA, suggesting that “reinforcement-learned search skills generalize more reliably than generation-tuned approaches,” according to the researchers.  This cross-domain adaptability makes s3 well-suited for specialized enterprise applications that often deal with proprietary or bespoke datasets without requiring extensive domain-specific training data. This means that a single trained searcher could serve different departments (e.g., legal, HR, customer support) or adapt to evolving content such as new product documents.  “We see immediate potential in healthcare, enterprise knowledge management, and scientific research support, where high retrieval quality is critical and labeled data is often scarce,” Jiang said. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/05/Robot-processing-documents.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-05-28T22:51:11+00:00\" datetime=\"2025-05-28T22:51:11+00:00\"\u003eMay 28, 2025 3:51 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"409\" src=\"https://venturebeat.com/wp-content/uploads/2025/05/Robot-processing-documents.png?w=750\" alt=\"Image credit: VentureBeat with Gemini\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with Gemini\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eResearchers at \u003ca href=\"https://illinois.edu/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eUniversity of Illinois Urbana-Champaign\u003c/a\u003e have introduced \u003ca href=\"https://github.com/pat-jj/s3\"\u003es3\u003c/a\u003e, an open-source framework designed to build retrieval-augmented generation (RAG) systems more efficiently than current methods. \u003c/p\u003e\n\n\n\n\u003cp\u003es3 can benefit developers creating real-world large language model (LLM) applications, as it simplifies and reduces the cost of creating retriever models within RAG architectures.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-rag-retrieval\"\u003eRAG retrieval\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe effectiveness of any RAG system hinges on the quality of its retrieval component. In \u003ca href=\"https://arxiv.org/abs/2505.14146\"\u003etheir paper\u003c/a\u003e, the researchers categorize the evolution of \u003ca href=\"https://venturebeat.com/ai/how-agentic-rag-can-be-a-game-changer-for-data-processing-and-retrieval/\"\u003eRAG\u003c/a\u003e approaches into three distinct phases. \u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003e“Classic RAG” systems rely on static retrieval methods with fixed queries, where retrieval quality is disconnected from the ultimate generation performance. These architectures struggle with queries requiring contextual or multi-hop reasoning.\u003c/li\u003e\n\n\n\n\u003cli\u003eA subsequent phase, dubbed “Pre-RL-Zero,” introduces more active LLM participation during inference. These techniques involved multi-turn interactions, interleaving query generation, retrieval, and reasoning. However, they typically depend on zero-shot prompting and lack trainable components to optimize retrieval through direct outcome signals.\u003c/li\u003e\n\n\n\n\u003cli\u003eThe most recent phase, “RL-Zero,” leverages \u003ca href=\"https://venturebeat.com/ai/deepseek-r1s-bold-bet-on-reinforcement-learning-how-it-outpaced-openai-at-3-of-the-cost/\"\u003ereinforcement learning\u003c/a\u003e (RL) to train models to act as search agents, improving through outcome-based feedback like answer correctness. An example is \u003ca href=\"https://venturebeat.com/ai/beyond-rag-search-r1-integrates-search-engines-directly-into-reasoning-models/\"\u003eSearch-R1\u003c/a\u003e, which trains the model to interleave reasoning with search queries and retrieved context.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eDespite their advancements, existing RL-Zero approaches often optimize retrieval using search-centric metrics that ignore downstream utility. Moreover, they require \u003ca href=\"https://bdtechtalks.com/2023/07/10/llm-fine-tuning/\"\u003efine-tuning the LLM\u003c/a\u003e, which is costly and error-prone. By entangling retrieval with generation, they limit real search utility and compatibility with frozen or proprietary models. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1398\" height=\"350\" src=\"https://venturebeat.com/wp-content/uploads/2025/05/image_1b7d4b.png?w=800\" alt=\"Different types of RAG (source: arXiv)\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/05/image_1b7d4b.png 1398w, https://venturebeat.com/wp-content/uploads/2025/05/image_1b7d4b.png?resize=300,75 300w, https://venturebeat.com/wp-content/uploads/2025/05/image_1b7d4b.png?resize=768,192 768w, https://venturebeat.com/wp-content/uploads/2025/05/image_1b7d4b.png?resize=800,200 800w, https://venturebeat.com/wp-content/uploads/2025/05/image_1b7d4b.png?resize=400,100 400w, https://venturebeat.com/wp-content/uploads/2025/05/image_1b7d4b.png?resize=750,188 750w, https://venturebeat.com/wp-content/uploads/2025/05/image_1b7d4b.png?resize=578,145 578w, https://venturebeat.com/wp-content/uploads/2025/05/image_1b7d4b.png?resize=930,233 930w\" sizes=\"(max-width: 1398px) 100vw, 1398px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eDifferent types of RAG Source: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAs the researchers put it, “This motivates a shift toward a modular framework where search and generation are cleanly separated, and optimization focuses purely on search quality with respect to downstream utility.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-s3\"\u003es3\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe s3 framework addresses this challenge with a model-agnostic approach. The main idea is to train a search agent with structured, multi-turn access to external knowledge. This search agent improves the quality of the retrieval stage without affecting the LLM that generates the final answer.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn s3, a dedicated searcher LLM iteratively interacts with a search engine. It generates queries based on the prompt, retrieves relevant documents, selects a useful subset of evidence, and decides whether to continue searching for more information. Once the search concludes, a separate, frozen generator LLM consumes this accumulated evidence to produce the final answer.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"5674\" height=\"3052\" src=\"https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png?w=800\" alt=\"s3 framework (source: arXiv)\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png 5674w, https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png?resize=300,161 300w, https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png?resize=768,413 768w, https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png?resize=800,430 800w, https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png?resize=1536,826 1536w, https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png?resize=2048,1102 2048w, https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png?resize=400,215 400w, https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png?resize=750,403 750w, https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png?resize=578,311 578w, https://venturebeat.com/wp-content/uploads/2025/05/image_80ac1e.png?resize=930,500 930w\" sizes=\"(max-width: 5674px) 100vw, 5674px\"/\u003e\u003cfigcaption\u003e\u003cem\u003es3 framework Source: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eA core innovation of s3 is its reward signal, Gain Beyond RAG (GBR). GBR quantifies the improvement in the generator’s accuracy when conditioned on documents retrieved by s3, compared to a baseline that retrieves the top documents matching the query. This reward incentivizes the searcher to find documents that truly enhance the generator’s output quality. \u003c/p\u003e\n\n\n\n\u003cp\u003e“s3 decouples the retriever (searcher) from the generator. This lets companies plug in any off-the-shelf or proprietary LLM—whether GPT-4, Claude, or an internal model—without having to fine-tune it,” Patrick (Pengcheng) Jiang, lead author of the paper and doctoral student at UIUC, told VentureBeat. “For enterprises with regulatory or contractual constraints on model modification, or those that rely on closed-source LLM APIs, this modularity makes s3 highly practical. It allows them to enhance search quality without touching their generation infrastructure.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-s3-in-action\"\u003es3 in action\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers tested s3 across six general-domain question-answering benchmarks, comparing it against three categories of RAG systems: End-to-end fine-tuning (e.g., Search-R1), static retrieval with frozen generators (such as classic RAG pipelines) and active retrieval with frozen generators (e.g., combining documents obtained by Search-R1 with a frozen LLM). In their experiments, they used Qwen2.5-7B-Instruct as the base model for the searcher and Qwen2.5-14B-Instruct and \u003ca href=\"https://venturebeat.com/ai/anthropic-releases-claude-3-haiku-an-ai-model-built-for-speed-and-affordability/\"\u003eClaude 3 Haiku\u003c/a\u003e as the frozen generator LLMs.\u003c/p\u003e\n\n\n\n\u003cp\u003es3 surpassed static, zero-shot and end-to-end tuned baselines on most benchmarks and achieved an average score. Its data efficiency is particularly noteworthy: s3 achieved strong gains with only 2.4k training examples, significantly less than the 70k examples required by DeepRetrieval (a static retrieval framework) or the 170k needed by Search-R1, while outperforming both in context quality and final answer performance.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"2400\" height=\"900\" src=\"https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png?w=800\" alt=\"s3 vs other RAG techniques (source: GitHub)\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png 2400w, https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png?resize=300,113 300w, https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png?resize=768,288 768w, https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png?resize=800,300 800w, https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png?resize=1536,576 1536w, https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png?resize=2048,768 2048w, https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png?resize=400,150 400w, https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png?resize=750,281 750w, https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png?resize=578,217 578w, https://venturebeat.com/wp-content/uploads/2025/05/image_0e2b88.png?resize=930,349 930w\" sizes=\"(max-width: 2400px) 100vw, 2400px\"/\u003e\u003cfigcaption\u003e\u003cem\u003es3 vs other RAG techniques Source: GitHub\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e“Many enterprises lack large-scale annotated QA datasets or the GPU infrastructure to fine-tune end-to-end LLM systems. s3 lowers the barrier by enabling strong retrieval performance with minimal supervision and compute,” Jiang said. “This means faster prototyping, reduced costs and quicker time-to-deployment for AI-powered search applications.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe findings suggest a fundamental shift in optimization strategy. As the researchers note in the paper, most of the performance gain in RAG stems from “improving the search capability instead of aligning generation outputs,” which implies that focusing RL on search strategy rather than combined generation alignment yields better results.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnother crucial finding for enterprise applications is s3’s ability to generalize to domains it has not been trained on. s3 showed zero-shot success on medical QA despite training only on general QA, suggesting that “reinforcement-learned search skills generalize more reliably than generation-tuned approaches,” according to the researchers. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis cross-domain adaptability makes s3 well-suited for specialized enterprise applications that often deal with proprietary or bespoke datasets without requiring extensive domain-specific training data. This means that a single trained searcher could serve different departments (e.g., legal, HR, customer support) or adapt to evolving content such as new product documents. \u003c/p\u003e\n\n\n\n\u003cp\u003e“We see immediate potential in healthcare, enterprise knowledge management, and scientific research support, where high retrieval quality is critical and labeled data is often scarce,” Jiang said.\u003c/p\u003e\n\n\n\n\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-05-28T22:51:11Z",
  "modifiedTime": "2025-05-28T22:51:31Z"
}
