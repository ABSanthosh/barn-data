{
  "id": "1ae201b9-dc95-44a7-b765-3957e7cc92af",
  "title": "Show HN: Eyesite - experimental website combining computer vision and web design",
  "link": "https://blog.andykhau.com/blog/eyesite",
  "description": "I wanted Apple Vision Pros, but I don’t have $3,500 in my back pocket. So I made Apple Vision Pros at home.This was just a fun little project I made. Currently, the website doesn't work on screens less than 1200x728 (Sorry mobile users!) It also might struggle on lower end devices.For best results, have a webcam pointing right at you. I tested my website with a MacBook camera.Any comments, questions, or suggestions are greatly appreciated!blog: https://blog.andykhau.com/blog/eyesitecheck it out: https://eyesite.andykhau.com/github: https://github.com/akchro/eyesite Comments URL: https://news.ycombinator.com/item?id=44253307 Points: 4 # Comments: 2",
  "author": "akchro",
  "published": "Thu, 12 Jun 2025 00:37:27 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "Andy Khau",
  "length": 3194,
  "excerpt": "Development journal of eyesite.",
  "siteName": "",
  "favicon": "",
  "text": "I wanted Apple Vision Pros, but I don’t have $3,500 in my back pocket. So I made Apple Vision Pros at home. I was interested in making a project that combined computer vision with web design—a website that users could physically interact with. This inspired me to make Eyesite, because who needs a mouse when you have your eyes? Eye tracking Luckily, there is already a Javascript library for eye tracking called WebGazer.js. We can achieve decent eye tracking through calibration: Make the user look at a point and click. This maps the current gaze to a point on the screen. Feed the gaze/coordinate mapping into WebGazer to calibrate. Repeat 9x times on the corners, sides, and center to get good mapping data. I found that it was best to get 5 mappings per point for better eye tracking accuracy. Calibration in debug mode. The top right shows how WebGazer tracks your eyes and face. The red dot is where it thinks I’m looking. Website Interaction Now that we have eye tracking, we can make some cool things with it! I decided to use the user’s gaze as a mouse and have them click with spacebar—kind of like how Apple Vision Pros have you look and pinch. Although I had the main functionality, it was far from finished. There were many considerations with making the experience as smooth and immersive as possible. The “Invisible” Mouse Initially, the user could see “where” they were looking at through a red dot.Main page in debug mode. This created some problems. First, the red dot was distracting, and users would unconsciously look at it instead of my buttons. Second, the red dot revealed how inaccurate the eye tracking was, which ruined the immersion. Ultimately, I decided to remove the “eye cursor” and also make the user’s mouse invisible. It made you really feel like you were controlling the website with your eyes rather than moving a mouse around. You can turn on debug mode to see your eye cursor and mouse. User feedback Since we don’t have a mouse, we need some way for the user to know they are looking at something. To do this… we track the user’s gaze (how surprising). We hid the eye cursor, but we still have the x and y coordinates of the user’s gaze. Each button component has checks to see if that gaze is within its borders. When the component detects the user is looking at it, it responds with a slight glow and pop. Large UI Admittedly, the eye tracking is not the best. You can really see how jittery it is with debug mode on. So I decided to make the UI huge. I also added a screen size restriction so the site is only usable on displays that meet a minimum size threshold (Sorry mobile users! It wouldn’t work on your phone anyway). The large size of the button accounts for the jitteriness of the eye tracking. Conclusion Those were a few details about Eyesite. If you are interested, you can see the source code. Small warning: this project was just a small demo and isn’t a shining example of clean code or best practices. This was a really fun project to make, and super cool to use too. If you want to make your own computer vision project or improve this one, I encourage you to do so! You can find the project at https://github.com/akchro/eyesite.",
  "image": "https://blog.andykhau.com/api/content/blogs/eyesite/thumbnail.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eI wanted Apple Vision Pros, but I don’t have $3,500 in my back pocket. So I made Apple Vision Pros at home.\u003c/p\u003e\n\u003cp\u003eI was interested in making a project that combined computer vision with web design—a website that users could \u003cem\u003ephysically\u003c/em\u003e interact with. This inspired me to make \u003ca href=\"https://eyesite.andykhau.com/\"\u003eEyesite\u003c/a\u003e, because who needs a mouse when you have your eyes?\u003c/p\u003e\n\u003ch2\u003eEye tracking\u003c/h2\u003e\n\u003cp\u003eLuckily, there is already a Javascript library for eye tracking called \u003ca href=\"https://webgazer.cs.brown.edu/\"\u003eWebGazer.js\u003c/a\u003e. We can achieve decent eye tracking through calibration:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eMake the user look at a point and click. This maps the current gaze to a point on the screen.\u003c/li\u003e\n\u003cli\u003eFeed the gaze/coordinate mapping into WebGazer to calibrate.\u003c/li\u003e\n\u003cli\u003eRepeat 9x times on the corners, sides, and center to get good mapping data.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eI found that it was best to get 5 mappings per point for better eye tracking accuracy.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://blog.andykhau.com/api/content/blogs/eyesite/calibration.jpg\" alt=\"Calibration screen\"/\u003e\u003cbr/\u003e\u003cem\u003eCalibration in debug mode. The top right shows how WebGazer tracks your eyes and face. The red dot is where it thinks I’m looking.\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003eWebsite Interaction\u003c/h2\u003e\n\u003cp\u003eNow that we have eye tracking, we can make some cool things with it! I decided to use the user’s gaze as a mouse and have them click with spacebar—kind of like how Apple Vision Pros have you look and pinch. Although I had the main functionality, it was far from finished. There were many considerations with making the experience as smooth and immersive as possible.\u003cbr/\u003e\u003cimg src=\"https://blog.andykhau.com/api/content/blogs/eyesite/gazepage.jpg\" alt=\"Main page\"/\u003e\u003c/p\u003e\n\u003ch2\u003eThe “Invisible” Mouse\u003c/h2\u003e\n\u003cp\u003eInitially, the user could see “where” they were looking at through a red dot.\u003cbr/\u003e\u003cimg src=\"https://blog.andykhau.com/api/content/blogs/eyesite/gazedebug.jpg\" alt=\"Gaze page in debug mode\"/\u003e\u003cbr/\u003e\u003cem\u003eMain page in debug mode.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThis created some problems. First, the red dot was distracting, and users would unconsciously look at it instead of my buttons. Second, the red dot revealed how inaccurate the eye tracking was, which ruined the immersion.\u003c/p\u003e\n\u003cp\u003eUltimately, I decided to remove the “eye cursor” and also make the user’s mouse invisible. It made you really feel like you were controlling the website with your eyes rather than moving a mouse around. You can turn on debug mode to see your eye cursor and mouse.\u003c/p\u003e\n\u003ch2\u003eUser feedback\u003c/h2\u003e\n\u003cp\u003eSince we don’t have a mouse, we need some way for the user to know they are looking at something. To do this… we track the user’s gaze (how surprising). We hid the eye cursor, but we still have the x and y coordinates of the user’s gaze. Each button component has checks to see if that gaze is within its borders. When the component detects the user is looking at it, it responds with a slight glow and pop.\u003cbr/\u003e\u003cimg src=\"https://blog.andykhau.com/api/content/blogs/eyesite/inandout.gif\" alt=\"Eye cursor going in and out\"/\u003e\u003c/p\u003e\n\u003ch2\u003eLarge UI\u003c/h2\u003e\n\u003cp\u003eAdmittedly, the eye tracking is not the best. You can really see how jittery it is with debug mode on. So I decided to make the UI huge. I also added a screen size restriction so the site is only usable on displays that meet a minimum size threshold (Sorry mobile users! It wouldn’t work on your phone anyway).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://blog.andykhau.com/api/content/blogs/eyesite/jitter.gif\" alt=\"Eye cursor jittering\"/\u003e\u003cbr/\u003e\u003cem\u003eThe large size of the button accounts for the jitteriness of the eye tracking.\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eThose were a few details about Eyesite. If you are interested, you can see the source code. Small warning: this project was just a small demo and isn’t a shining example of clean code or best practices. \u003c/p\u003e\n\u003cp\u003eThis was a really fun project to make, and super cool to use too. If you want to make your own computer vision project or improve this one, I encourage you to do so! You can find the project at \u003ca href=\"https://github.com/akchro/eyesite\"\u003ehttps://github.com/akchro/eyesite\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-06-02T00:00:00Z",
  "modifiedTime": null
}
