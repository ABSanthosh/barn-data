{
  "id": "c55dcc07-9e72-447e-a320-c681cb961317",
  "title": "Navigating embedding vectors",
  "link": "https://uxdesign.cc/navigating-embedding-vectors-09fb846149fa?source=rss----138adf9c44c---4",
  "description": "",
  "author": "Tom Hatton",
  "published": "Tue, 04 Mar 2025 08:44:34 GMT",
  "source": "https://uxdesign.cc/feed",
  "categories": [
    "artificial-intelligence",
    "product-design",
    "ai",
    "design",
    "ux"
  ],
  "byline": "Tom Hatton",
  "length": 14007,
  "excerpt": "As of March 2025, we still lack meaningful control over AI-generated outputs. From a user experience point of view, most of the time this is acceptable. However, when using AI tools to help with…",
  "siteName": "UX Collective",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*dn6MbbIIlwobt0jnUcrt_Q.png",
  "text": "Navigating embedding vectorsAI, feedback \u0026 the need for greater user control.As of March 2025, we still lack meaningful control over AI-generated outputs. From a user experience point of view, most of the time this is acceptable. However, when using AI tools to help with complex information discovery or nuanced creative tasks, the prompting process quickly becomes convoluted, imprecise and frustrating. Technically, this shouldn’t need to be the case.Every time we revise a prompt, a new cycle of input and output tokens is generated. This is an awkward way of working when you are honing in towards a final output. The back and forth text prompting needed to direct AI tools is inefficient, quickly strays from naturally constructed phrases and previous incorrect responses pollute the attention mechanism.This lack of predictability currently prevents users from gaining an intuitive working knowledge of AI tools, which in turn limits the models’ capabilities.What if?What if we had customisable UI controls that would allow users to navigate towards a desired output without having to use imprecise language prompts?Older electronic products had direct mechanical feedback between a user’s input and a corresponding action. This experience feels distant when using current AI tools. But does this need to be the case? Dieter Rams. World Receiver T 1000 Radio, 1963. Brooklyn Museum.Why is this better?This isn’t just about convenience — it’s about creating a more natural way for users to collaborate with AI tools and harness their power. The most efficient way for users to solve problems is to learn by doing. The most natural way is by trial, error and refinement. Rewriting a prompt resets all the input token embeddings which means that users lose any sense of control when working with AI tools.A more sensible approach would be to allow users to move through the AI model space and let them navigate to a desired outcome.Wireflow: Enhancing AI prompts with a control panel and concept vector sliders.Erm, I still don’t get itTo illustrate this concept more clearly, let’s use an analogy. Imagine a game where the multi-dimensional geometry of an AI model is represented by inter-galactic space. Each time you prompt a spaceship pops up somewhere in this inter-galactic space. You have a destination in mind — say a specific star system that you want to explore. At the moment, the only way to navigate towards your star system is to prompt. Each time you do so, the spaceship teleports to another somewhat random position. You are unsure if your new prompt will appear closer or further away to your destination. Your prompts balloon in length, and your uncertainty increases as each additional word has less impact on the spaceship’s position.If, on the other hand, you had navigational controls, instead of blindly jumping about the universe, you could increase or decrease various values and more effectively learn which values move you towards your destination.You might find that you need to re-prompt a couple of times first to start closer to your destination. But when you’re closing in, being able to navigate through the vector space with sliders is significantly more effective.(“But what about prompt weightings? By adding + and — to words in a prompt it is possible to change their importance!” \u003e This is a useful hack but it isn’t intuitive or efficient. With successive, lengthy prompts users are still blindly guessing with new token embeddings.)What’s needed for a UI control panel?UI controls would need to be inferred from each prompt.The input embeddings go through many cycles of attention processing, so controls would need to directly alter the prompt’s final input embedding vectors — prior to the output content generation process.Proposed and existing data flow through an LLM attention head.So, how could this work?When a prompt is being processed a copy of the final input vector embeddings would need to be stored prior to the output generation. From these copied embeddings it should be possible to infer the most relevant values to provide as controls. It should also be possible to allow users to input their own values.If a user needs to fine-tune an output, they could adjust controls which would shift the token embeddings. These new embeddings would be fed directly into the output generation, skipping the input prompt generation.While I’m at the edge of my knowledge of ML models, it seems that mathematically it might be possible to effect a change in the token embeddings by altering the Value ‘V’ in the equation below.This mathematical equation describes the attention head layer within a Large Language Model. Query (Q) relates to the token generation of the input prompt. Key (K) maps the input prompt to the model space. Value (V) is a weighting layer that intentionally guides output generation.Where this approach works bestWorking With Near-Known \u0026 Unknown Information — When new information can shift a user’s initial intention. Eg, Travel Planning \u003e If a user wrote an initial prompt for a personalised travel itinerary, they could then shift subjective parameters to tailor the plan without having to re-write long prompts.Content Generation — The tasks that stand to gain the most are when prompting during the creative process, when it’s beneficial for the “temperature” parameter to be higher. Eg, when using image generation tools users either have a conscious target in mind that they are trying to match, or they will discover what ‘feels right’ as they use the generative tool. Endless prompting harms the creative process and is computationally expensive. Concept vector sliders should expand a user’s creative flow state rather than frustrate it.Deep Research | Searching Within Complex Vertical Databases — Interrogating data with nuanced vector based search would be useful for particular scientific experiments that involve large databases. Eg, for research studies attempting to map animal communication, it might be useful to explore the contextual differences in the way animals communicate. The same sound pattern might be being made, but being expressed differently depending on comfort and safety vs threat and danger. Navigating a database with UI sliders that control various embedding vectors and provide feedback analysis on search terms could be useful.Generative AI: Two Example Use Cases1. Writing | Feedback \u0026 Modulation ControlBefore making style changes to text, it would be useful for writers to receive feedback. As I’m writing this article for example, when I’m deep in a writing flow, I’m unsure if I’m keeping an acceptable level of complexity and tone across sections. Variance of course is ok, but feedback would be helpful.Then when making style changes, users need more precise control. Default commands, such as Apple Intelligence’s ‘Friendly’ , ‘Professional’ , ‘Concise’ , or Gemini’s ‘Rephrase’, ‘Shorten’, ‘Elaborate’ offer little feedback or control. How ‘Friendly’ or ‘Professional’ is the text to begin with? And then when applying the change, how much more ‘Friendly’ , ‘Professional’, ‘Shorter’ or ‘Longer’ does the user want it to be? Also, perhaps there are more nuanced stylistic changes that I’d like to explore.An initial mock up of how a simple control panel could function within Google Docs existing UI.So wait, what’s new?Feedback — Users can quickly review a text based on customisable values.User Interface Controls — Following feedback, users can then make informed and confident changes along several nuanced concept vectors at once. Without a multi-step prompt dialogue. Using these concept sliders users can pinpoint a specific intention that might be difficult, or inefficient to describe with words.Easier Development, Deployment \u0026 Modulation of Personal Styles — A fully customisable control panel can help users create and deploy a personal style and then modulate it for a given context.The impact of document analytics and vector sliders like this would be considerable. Instead of giving full agency to AI to re-write texts, using a copilot to quickly analyse and variably modulate text could help users to be more intentional with their writing and improve their writing skills rather than loosing it to AI.2. Multi-Media Content GenerationCompared to text based LLMs text to media generation tools currently suffer from an even greater lack of traction between intention, prompt and output. This is because they have huge dual model spaces with a text input analysis as well as an output vector space which have to be matched together.As well as media labelling issues and black holes within training data (eg. there are hardly any images of wine glasses that are full to the brim), another significant problem is a UX one.Users lack intuition of how to prompt text-image models effectively. With vector sliders users would have greater certainty in knowing whether a desired outcome is even achievable in the model and not a prompt failure. By removing the uncertainty involved with prompts, users would increasingly enjoy working with generative AI tools and be more effective with less overall prompt attempts. Efficiencies in text prompting can only be beneficial from a business standpoint.Mock up of a text to image generator to shown the usefulness of subjective concept vectors.I’m almost lost again, what’s new?Two Step Prompts | Text + Concept Vector Sliders — With a more straightforward initial prompt, users could now make further changes using subjective concept vectors. In the above example ‘atmosphere’ is added to the image. There is feedback of how atmospheric the images is, which informs a user when changing this value.Control Panels Change the Final Input Embeddings — This is crucial. When users decide to make a change they would now be able to carefully fine tune an existing prompt without reshuffling all the vector embeddings.It took over an hour of repeated prompts to Adobe Firefly to get the three images for the above mock-up. Every time I re-prompted Firefly I felt as though I was playing roulette. I was never certain of what any of Firefly’s controls or presets were doing. Perhaps it’s a skill issue, but even after finding an image to use as a firm compositional lock and as a style transfer, I was frustrated with an inability to nudge the image in any meaningful non-random way.It definitely feels that something is going wrong. These models are incredibly powerful, and they should be able to handle incremental changes and nuanced inference. There is obviously a lot of untapped potential with the combination of LLMs and diffusion models.Doing more with less. Why this is worth pursuing.Part of the problem with prompt engineering is that users have to communicate to an AI that has an unknown exposure to the world. Users don’t know what information they need to provide to an AI or how that information should be provided. To make matters worse, models frequently change, and in turn, their sensitivities to words and phrases change.If users had greater model space control, this would ease some of these tensions. Users could write shorter prompts to establish a baseline which they could re-define with concept vectors. A multi-step user interface means shorter, less perfect, and more efficient prompts with increased fine control of the output for the ‘last mile’ of accuracy.A two-step process, of prompting and then fine-tuning final input embeddings, should also be more computationally efficient. From a UX perspective it would be more satisfying because this method is in-sync with how we think and work — particularly when working through unknown problems and when needing Generative AI to perform at higher ‘temperatures’ (hallucinations) for creative work.NotesThe ideas in this article can be seen as part of wider evolving research and discussions surrounding Large Concept Models that are being developed by Meta. Essentially this is an LLM model that is specifically organised around conceptually related terms. This approach should make navigating concepts more predictable and reliable from a user experience interaction. Articles for further reading:- Mehul Gupta’s Meta Large Concept Models (LCM): End of LLMs?- Vishal Rajpjut’s ‘Forget LLMs, It’s Time For Large Concept Models (LCMs)’ .I first encountered Concept Activation Vectors (CAVs) in 2020, while working alongside Nord Projects on a research project for GoogleAI. This project, which explored subjectivity, style, and inference in images, won an Interaction Award (IxDA).The idea of identifying and working with subjective inference, which Nord Projects explored, has stayed with me ever since. It has influenced the central ideas of this piece and shaped my thinking on how similar concepts could be applied as user controls within LLM and GenAI models.ReferencesAttention In Transformers, step-by-step Grant Sanderson, (3Blue1Brown Youtube Channel)https://www.youtube.com/watch?v=eMlx5fFNoYcLarge Language Models II: Attention, Transformers and LLMsMitul Tiwarihttps://www.linkedin.com/pulse/large-language-models-ii-attention-transformers-llms-mitul-tiwari-zg0uf/Attention Is All You NeedAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhinhttps://arxiv.org/abs/1706.03762What Is ChatGPT Doing … and Why Does It Work Stephen Wolframhttps://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/King — Man + Woman is Queen; but why?Piotr Migdałhttps://p.migdal.pl/blog/2017/01/king-man-woman-queen-whyDon’t Use Cosine Similarity CarelesslyPiotr Migdałhttps://p.migdal.pl/blog/2025/01/dont-use-cosine-similarityOpen sourcing the Embedding Projector: a tool for visualizing high dimensional dataDaniel Smilkov and the Big Picture grouphttps://research.google/blog/open-sourcing-the-embedding-projector-a-tool-for-visualizing-high-dimensional-data/How AI ‘Understands’ Images (CLIP)Mike Pound, (Computerphile)https://www.youtube.com/watch?v=KcSXcpluDe4",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*OFqzepqmNSSYG8mAGNCFvA.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e\u003ch2 id=\"5c63\" data-testid=\"storyTitle\"\u003eNavigating embedding vectors\u003c/h2\u003e\u003c/p\u003e\u003cdiv\u003e\u003ch2 id=\"89d9\"\u003eAI, feedback \u0026amp; the need for greater user control.\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/@tomhatton.studio?source=post_page---byline--09fb846149fa---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Tom Hatton\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*Y87gWhDUWjCB0eg1yRIAOA.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://uxdesign.cc/?source=post_page---byline--09fb846149fa---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"UX Collective\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*mDhF9X4VO0rCrJvWFatyxg.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"3f46\"\u003eAs of March 2025, we still lack meaningful control over AI-generated outputs. From a user experience point of view, most of the time this is acceptable. However, when using AI tools to help with complex information discovery or nuanced creative tasks, the prompting process quickly becomes convoluted, imprecise and frustrating. Technically, this shouldn’t need to be the case.\u003c/p\u003e\u003cp id=\"ab65\"\u003eEvery time we revise a prompt, a new cycle of input and output tokens is generated. This is an awkward way of working when you are honing in towards a final output. The back and forth text prompting needed to direct AI tools is inefficient, quickly strays from naturally constructed phrases and previous incorrect responses pollute the attention mechanism.\u003c/p\u003e\u003cp id=\"db0e\"\u003eThis lack of predictability currently prevents users from gaining an intuitive working knowledge of AI tools, which in turn limits the models’ capabilities.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"3489\"\u003eWhat if?\u003c/h2\u003e\u003cp id=\"a433\"\u003eWhat if we had customisable UI controls that would allow users to navigate towards a desired output without having to use imprecise language prompts?\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cstrong\u003eOlder electronic products had direct mechanical feedback between a user’s input and a corresponding action. This experience feels distant when using current AI tools. But does this need to be the case?\u003c/strong\u003e\u003ca href=\"https://www.brooklynmuseum.org/en-GB/objects/170035\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e Dieter Rams. World Receiver T 1000 Radio, 1963. Brooklyn Museum.\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"9ebb\"\u003eWhy is this better?\u003c/h2\u003e\u003cp id=\"f86b\"\u003eThis isn’t just about convenience — it’s about creating a more natural way for users to collaborate with AI tools and harness their power. The most efficient way for users to solve problems is to learn by doing. The most natural way is by trial, error and refinement. Rewriting a prompt resets all the input token embeddings which means that users lose any sense of control when working with AI tools.\u003c/p\u003e\u003cp id=\"c8ab\"\u003eA more sensible approach would be to allow users to move through the AI model space and let them navigate to a desired outcome.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eWireflow: Enhancing AI prompts with a control panel and concept vector sliders.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"d614\"\u003e\u003cstrong\u003eErm, I still don’t get it\u003c/strong\u003e\u003c/h2\u003e\u003cp id=\"e19d\"\u003eTo illustrate this concept more clearly, let’s use an analogy. Imagine a game where the multi-dimensional geometry of an AI model is represented by inter-galactic space. Each time you prompt a spaceship pops up somewhere in this inter-galactic space. You have a destination in mind — say a specific star system that you want to explore. At the moment, the only way to navigate towards your star system is to prompt. Each time you do so, the spaceship teleports to another somewhat random position. You are unsure if your new prompt will appear closer or further away to your destination. Your prompts balloon in length, and your uncertainty increases as each additional word has less impact on the spaceship’s position.\u003c/p\u003e\u003cp id=\"3ce2\"\u003eIf, on the other hand, you had navigational controls, instead of blindly jumping about the universe, you could increase or decrease various values and more effectively learn which values move you towards your destination.\u003c/p\u003e\u003cp id=\"2c1a\"\u003eYou might find that you need to re-prompt a couple of times first to start closer to your destination. But when you’re closing in, being able to navigate through the vector space with sliders is significantly more effective.\u003c/p\u003e\u003cp id=\"16c3\"\u003e\u003cem\u003e(“But what about prompt weightings? By adding + and — to words in a prompt it is possible to change their importance!” \u0026gt; This is a useful hack but it isn’t intuitive or efficient. With successive, lengthy prompts users are still blindly guessing with new token embeddings.)\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"4f6a\"\u003eWhat’s needed for a UI control panel?\u003c/h2\u003e\u003cul\u003e\u003cli id=\"dd37\"\u003e\u003cstrong\u003eUI controls would need to be inferred from each prompt\u003c/strong\u003e.\u003c/li\u003e\u003cli id=\"e2ce\"\u003eThe input embeddings go through many cycles of attention processing, so \u003cstrong\u003econtrols would need to directly alter the prompt’s final input embedding vectors\u003c/strong\u003e\u003cem\u003e — prior to the output content generation process.\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cfigcaption\u003eProposed and existing data flow through an LLM attention head.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"a626\"\u003eSo, how could this work?\u003c/h2\u003e\u003cp id=\"4d1d\"\u003eWhen a prompt is being processed a copy of the final input vector embeddings would need to be stored prior to the output generation. From these copied embeddings it should be possible to infer the most relevant values to provide as controls.\u003cem\u003e It should also be possible to allow users to input their own values.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"6098\"\u003eIf a user needs to fine-tune an output, they could adjust controls which would shift the token embeddings. These new embeddings would be fed directly into the output generation, skipping the input prompt generation.\u003c/p\u003e\u003cp id=\"6fde\"\u003eWhile I’m at the edge of my knowledge of ML models, it seems that mathematically it might be possible to effect a change in the token embeddings by altering the Value ‘V’ in the equation below.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eThis mathematical equation describes the attention head layer within a Large Language Model. Query (Q) relates to the token generation of the input prompt. Key (K) maps the input prompt to the model space. Value (V) is a weighting layer that intentionally guides output generation.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"160f\"\u003eWhere this approach works best\u003c/h2\u003e\u003col\u003e\u003cli id=\"17d1\"\u003e\u003cstrong\u003eWorking With Near-Known \u0026amp; Unknown Information \u003c/strong\u003e— When new information can shift a user’s initial intention. \u003cem\u003eEg, Travel Planning \u0026gt; If a user wrote an initial prompt for a personalised travel itinerary, they could then shift subjective parameters to tailor the plan without having to re-write long prompts.\u003c/em\u003e\u003c/li\u003e\u003cli id=\"0a42\"\u003e\u003cstrong\u003eContent Generation \u003c/strong\u003e— The tasks that stand to gain the most are when prompting during the creative process, when it’s beneficial for the “temperature” parameter to be higher. \u003cem\u003eEg, when using image generation tools users either have a conscious target in mind that they are trying to match, or they will discover what ‘feels right’ as they use the generative tool. Endless prompting harms the creative process and is computationally expensive. Concept vector sliders should expand a user’s creative flow state rather than frustrate it.\u003c/em\u003e\u003c/li\u003e\u003cli id=\"9d93\"\u003e\u003cstrong\u003eDeep Research | Searching Within Complex Vertical Databases — \u003c/strong\u003eInterrogating data with nuanced vector based search would be useful for particular scientific experiments that involve large databases. \u003cem\u003eEg, for research studies attempting to map animal communication, it might be useful to explore the contextual differences in the way animals communicate. The same sound pattern might be being made, but being expressed differently depending on comfort and safety vs threat and danger. Navigating a database with UI sliders that control various embedding vectors and provide feedback analysis on search terms could be useful.\u003c/em\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"62dd\"\u003eGenerative AI: Two Example Use Cases\u003c/h2\u003e\u003ch2 id=\"8ef4\"\u003e1. Writing | Feedback \u0026amp; Modulation Control\u003c/h2\u003e\u003cp id=\"2aab\"\u003eBefore making style changes to text, it would be useful for writers to receive feedback. As I’m writing this article for example, when I’m deep in a writing flow, I’m unsure if I’m keeping an acceptable level of complexity and tone across sections. Variance of course is ok, but feedback would be helpful.\u003c/p\u003e\u003cp id=\"c51c\"\u003eThen when making style changes, users need more precise control. Default commands, such as Apple Intelligence’s ‘Friendly’ , ‘Professional’ , ‘Concise’ , or Gemini’s ‘Rephrase’, ‘Shorten’, ‘Elaborate’ offer little feedback or control. \u003cstrong\u003eHow ‘Friendly’ or ‘Professional’ is the text to begin with?\u003c/strong\u003e And then when applying the change, \u003cstrong\u003ehow much more ‘Friendly’ , ‘Professional’, ‘Shorter’ or ‘Longer’ does the user want it to be? \u003c/strong\u003e\u003cem\u003eAlso, perhaps there are more nuanced stylistic changes that I’d like to explore.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eAn initial mock up of how a simple control panel could function within Google Docs existing UI.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"169b\"\u003e\u003cstrong\u003eSo wait, what’s new?\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli id=\"9c2c\"\u003e\u003cstrong\u003eFeedback\u003c/strong\u003e — \u003cem\u003eUsers can quickly review a text based on customisable values.\u003c/em\u003e\u003c/li\u003e\u003cli id=\"2303\"\u003e\u003cstrong\u003eUser Interface Controls\u003c/strong\u003e — \u003cem\u003eFollowing feedback, users can then make informed and confident changes along several nuanced concept vectors at once. Without a multi-step prompt dialogue. Using these concept sliders users can pinpoint a specific intention that might be difficult, or inefficient to describe with words.\u003c/em\u003e\u003c/li\u003e\u003cli id=\"199f\"\u003e\u003cstrong\u003eEasier Development, Deployment \u0026amp; Modulation of Personal Styles\u003c/strong\u003e \u003cem\u003e— A fully customisable control panel can help users create and deploy a personal style and then modulate it for a given context.\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"0c5c\"\u003eThe impact of document analytics and vector sliders like this would be considerable. Instead of giving full agency to AI to re-write texts, using a copilot to quickly analyse and variably modulate text could help users to be more intentional with their writing and improve their writing skills rather than loosing it to AI.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"08e6\"\u003e2. Multi-Media Content Generation\u003c/h2\u003e\u003cp id=\"d253\"\u003eCompared to text based LLMs text to media generation tools currently suffer from an even greater lack of traction between intention, prompt and output. This is because they have huge dual model spaces with a text input analysis as well as an output vector space which have to be matched together.\u003c/p\u003e\u003cp id=\"458d\"\u003eAs well as media labelling issues and black holes within training data \u003cem\u003e(eg. there are hardly any images of \u003c/em\u003e\u003ca href=\"https://www.reddit.com/r/ChatGPT/comments/1gas25l/your_mission_should_you_choose_to_accept_it_is_to/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003ewine glasses that are full to the brim\u003c/em\u003e\u003c/a\u003e\u003cem\u003e),\u003c/em\u003e another significant problem is a UX one.\u003c/p\u003e\u003cp id=\"dc7e\"\u003eUsers lack intuition of how to prompt text-image models effectively. With vector sliders users would have greater certainty in knowing whether a desired outcome is even achievable in the model and not a prompt failure. By removing the uncertainty involved with prompts, users would increasingly enjoy working with generative AI tools and be more effective with less overall prompt attempts. \u003cem\u003eEfficiencies in text prompting can only be beneficial from a business standpoint.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eMock up of a text to image generator to shown the usefulness of subjective concept vectors.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"762f\"\u003e\u003cstrong\u003eI’m almost lost again, what’s new?\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli id=\"490f\"\u003e\u003cstrong\u003eTwo Step Prompts | Text + Concept Vector Sliders \u003c/strong\u003e— With a more straightforward initial prompt, users could now make further changes using subjective concept vectors. \u003cem\u003eIn the above example ‘atmosphere’ is added to the image. There is feedback of how atmospheric the images is, which informs a user when changing this value.\u003c/em\u003e\u003c/li\u003e\u003cli id=\"a1e9\"\u003e\u003cstrong\u003eControl Panels Change the Final Input Embeddings\u003c/strong\u003e — This is crucial. When users decide to make a change they would now be able to carefully fine tune an existing prompt without reshuffling all the vector embeddings.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"a105\"\u003eIt took over an hour of repeated prompts to Adobe Firefly to get the three images for the above mock-up. Every time I re-prompted Firefly I felt as though I was playing roulette. I was never certain of what any of Firefly’s controls or presets were doing. Perhaps it’s a skill issue, but even after finding an image to use as a firm compositional lock and as a style transfer, I was frustrated with an inability to nudge the image in any meaningful non-random way.\u003c/p\u003e\u003cp id=\"fc6a\"\u003eIt definitely feels that something is going wrong. These models are incredibly powerful, and they should be able to handle incremental changes and nuanced inference. There is obviously a lot of untapped potential with the combination of LLMs and diffusion models.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"c338\"\u003eDoing more with less. Why this is worth pursuing.\u003c/h2\u003e\u003cp id=\"8f1f\"\u003ePart of the problem with prompt engineering is that users have to communicate to an AI that has an unknown exposure to the world. Users don’t know what information they need to provide to an AI or how that information should be provided. To make matters worse, models frequently change, and in turn, their sensitivities to words and phrases change.\u003c/p\u003e\u003cp id=\"f720\"\u003eIf users had greater model space control, this would ease some of these tensions. Users could write shorter prompts to establish a baseline which they could re-define with concept vectors. A multi-step user interface means shorter, less perfect, and more efficient prompts with increased fine control of the output for the ‘last mile’ of accuracy.\u003c/p\u003e\u003cp id=\"514f\"\u003eA two-step process, of prompting and then fine-tuning final input embeddings, should also be more computationally efficient. From a UX perspective it would be more satisfying because this method is in-sync with how we think and work — particularly when working through unknown problems and when needing Generative AI to perform at higher ‘temperatures’ \u003cem\u003e(hallucinations)\u003c/em\u003e for creative work.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"fa88\"\u003eNotes\u003c/h2\u003e\u003col\u003e\u003cli id=\"d46f\"\u003e\u003cem\u003eThe ideas in this article can be seen as part of wider evolving research and discussions surrounding Large Concept Models that are being developed by Meta. Essentially this is an LLM model that is specifically organised around conceptually related terms. This approach should make navigating concepts more predictable and reliable from a user experience interaction. Articles for further reading:\u003cbr/\u003e- Mehul Gupta’s \u003c/em\u003e\u003ca href=\"https://medium.com/data-science-in-your-pocket/meta-large-concept-models-lcm-end-of-llms-68cb0c5cd5cf\" rel=\"noopener\"\u003e\u003cem\u003eMeta Large Concept Models (LCM): End of LLMs?\u003c/em\u003e\u003c/a\u003e\u003cem\u003e\u003cbr/\u003e- Vishal Rajpjut’s \u003c/em\u003e\u003ca href=\"https://medium.com/aiguys/forget-llms-its-time-for-large-concept-models-lcms-05b75fe43185\" rel=\"noopener\"\u003e\u003cem\u003e‘Forget LLMs, It’s Time For Large Concept Models (LCMs)’\u003c/em\u003e\u003c/a\u003e\u003cem\u003e .\u003c/em\u003e\u003c/li\u003e\u003cli id=\"702a\"\u003e\u003cem\u003eI first encountered Concept Activation Vectors (CAVs) in 2020, while working alongside \u003c/em\u003e\u003ca href=\"https://nordprojects.co/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eNord Projects\u003c/em\u003e\u003c/a\u003e\u003cem\u003e on a \u003c/em\u003e\u003ca href=\"https://nordprojects.co/projects/cavstudio/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eresearch project\u003c/em\u003e\u003c/a\u003e\u003cem\u003e for GoogleAI. This project, which explored subjectivity, style, and inference in images, won\u003c/em\u003e\u003ca href=\"https://awards.ixda.org/projects/mood-board-search-enabling-ai-powered-creative-expression.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003e an Interaction Award (IxDA)\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003cbr/\u003eThe idea of identifying and working with subjective inference, which Nord Projects explored, has stayed with me ever since. It has influenced the central ideas of this piece and shaped my thinking on how similar concepts could be applied as user controls within LLM and GenAI models.\u003c/em\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"d8b8\"\u003eReferences\u003c/h2\u003e\u003cul\u003e\u003cli id=\"5cde\"\u003e\u003cstrong\u003eAttention In Transformers, step-by-step \u003c/strong\u003e\u003cbr/\u003e\u003cem\u003eGrant Sanderson, (3Blue1Brown Youtube Channel)\u003c/em\u003e\u003cbr/\u003e\u003ca href=\"https://www.youtube.com/watch?v=eMlx5fFNoYc\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://www.youtube.com/watch?v=eMlx5fFNoYc\u003c/a\u003e\u003c/li\u003e\u003cli id=\"dd04\"\u003e\u003cstrong\u003eLarge Language Models II: Attention, Transformers and LLMs\u003c/strong\u003e\u003cbr/\u003e\u003cem\u003eMitul Tiwari\u003c/em\u003e\u003cbr/\u003e\u003ca href=\"https://www.linkedin.com/pulse/large-language-models-ii-attention-transformers-llms-mitul-tiwari-zg0uf/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://www.linkedin.com/pulse/large-language-models-ii-attention-transformers-llms-mitul-tiwari-zg0uf/\u003c/a\u003e\u003c/li\u003e\u003cli id=\"1902\"\u003e\u003cstrong\u003eAttention Is All You Need\u003c/strong\u003e\u003cbr/\u003e\u003cem\u003eAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\u003cbr/\u003e\u003c/em\u003e\u003ca href=\"https://arxiv.org/abs/1706.03762\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://arxiv.org/abs/1706.03762\u003c/a\u003e\u003c/li\u003e\u003cli id=\"50a6\"\u003e\u003cstrong\u003eWhat Is ChatGPT Doing … and Why Does It Work \u003cbr/\u003e\u003c/strong\u003e\u003cem\u003eStephen Wolfram\u003c/em\u003e\u003cstrong\u003e\u003cbr/\u003e\u003c/strong\u003e\u003ca href=\"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\u003c/a\u003e\u003c/li\u003e\u003cli id=\"59a0\"\u003e\u003cstrong\u003eKing — Man + Woman is Queen; but why?\u003c/strong\u003e\u003cbr/\u003e\u003cem\u003ePiotr Migdał\u003c/em\u003e\u003cbr/\u003e\u003ca href=\"https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://p.migdal.pl/blog/2017/01/king-man-woman-queen-why\u003c/a\u003e\u003c/li\u003e\u003cli id=\"43e2\"\u003e\u003cstrong\u003eDon’t Use Cosine Similarity Carelessly\u003c/strong\u003e\u003cbr/\u003e\u003cem\u003ePiotr Migdał\u003cbr/\u003e\u003c/em\u003e\u003ca href=\"https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity\u003c/a\u003e\u003c/li\u003e\u003cli id=\"fa7b\"\u003e\u003cstrong\u003eOpen sourcing the Embedding Projector: a tool for visualizing high dimensional data\u003c/strong\u003e\u003cbr/\u003e\u003cem\u003eDaniel Smilkov and the Big Picture group\u003c/em\u003e\u003cbr/\u003e\u003ca href=\"https://research.google/blog/open-sourcing-the-embedding-projector-a-tool-for-visualizing-high-dimensional-data/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://research.google/blog/open-sourcing-the-embedding-projector-a-tool-for-visualizing-high-dimensional-data/\u003c/a\u003e\u003c/li\u003e\u003cli id=\"17a6\"\u003e\u003cstrong\u003eHow AI ‘Understands’ Images (CLIP)\u003c/strong\u003e\u003cbr/\u003e\u003cem\u003eMike Pound, (Computerphile)\u003cbr/\u003e\u003c/em\u003e\u003ca href=\"https://www.youtube.com/watch?v=KcSXcpluDe4\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://www.youtube.com/watch?v=KcSXcpluDe4\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2025-03-01T21:09:22.529Z",
  "modifiedTime": null
}
