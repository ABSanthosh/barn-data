{
  "id": "0271ab1b-185c-44a6-b68f-1850f09f349e",
  "title": "Analyze Usability Test Data in 4 Steps",
  "link": "https://www.nngroup.com/articles/analyze-usability-data/?utm_source=rss\u0026utm_medium=feed\u0026utm_campaign=rss-syndication",
  "description": "Analyze with confidence by collecting relevant data, critically assessing it, and forming testable explanations.",
  "author": "Maria Rosala",
  "published": "Fri, 02 May 2025 17:00:00 +0000",
  "source": "https://www.nngroup.com/feed/rss/",
  "categories": [
    "Article"
  ],
  "byline": "Maria Rosala",
  "length": 9358,
  "excerpt": "Analyze with confidence by collecting relevant data, critically assessing it, and forming testable explanations.",
  "siteName": "Nielsen Norman Group",
  "favicon": "",
  "text": "Summary:  Analyze with confidence by collecting relevant data, critically assessing it, and forming testable explanations. Why Analyzing Qualitative User Tests Is Complicated 4 Steps to Analyze Usability-Test Data Step 1: Collect Relevant Data Step 2: Assess for Accuracy Step 3. Explain the Data Step 4. Check for Good Fit Why Analyzing Qualitative User Tests Is Complicated Analyzing data from qualitative usability tests is often presented in textbooks as a straightforward process of cataloging usability issues, summarizing task performance, tallying errors, and summarizing time on task. However, it’s not always so easy. This is especially the case when you have: Unusual study designs (such as multiple prototype versions shown to each participant in a session) Complex research questions related to topics such as discoverability, comprehension, or how people solve problems Low-fidelity prototypes that don’t have all the features, content, or screens available Let’s imagine we’ve designed a new product-details page that we’d like to test. Participants will be asked to find specific products on a prototype and choose one that’s best for them. Below are some of our research questions. Is the product-comparison feature discoverable? What information about the product is important to users and what isn’t? Do people understand the product-overview information? Do people understand how the product works? Do they form the right mental model? It might be hard to answer such research questions by studying performance in one task. To answer these questions successfully, we’d need to: Gather multiple pieces of data (what people did and said at multiple points in the session) Weigh the data against the study design, recruitment particulars (who participated in our research), and even facilitation events (i.e., what the researcher did or said) Triangulate all this information to provide a trustworthy answer Analysis and Synthesis Moving from data to insights and recommendations requires a combination of two activities: analysis and synthesis. Analysis refers to the breaking down and inspection of complex information, whereas synthesis refers to the recombination of information into new meaningful forms, namely insights. (Note that when we talk about “analysis” as a stage in the research process, we’re really referring to both of these activities.) Analysis involves breaking down data into manageable units of information. Synthesis refers to rebuilding and combining them with other information to create insights, theories, and explanations. When we analyze qualitative data, analysis and synthesis don’t happen in a neat linear fashion. Sometimes, we move back and forth between the two. 4 Steps to Analyze Usability-Test Data To illustrate how we reach insights from usability test data and what the process of analysis and synthesis look like in this context, we propose a 4-step framework. Collect relevant data: Select data points from each session (observations and quotes) that are relevant to a research question. This step involves analysis and reducing the data set to something more manageable. Assess for accuracy: Examine each data point to assess its relevancy and accuracy. We’re still in the analysis phase here. Explain the data: Combine the data points (step 1), our assessments (step 2), and our expertise to provide reasonable explanations or answers to our research question. This step involves synthesis. Check for good fit: Check our explanations or answers to our research questions against the data we collected to ensure good fit. Does all the data we collected support the explanation? If not, we iterate upon the explanation (by revisiting step 3). Step 4 is where analysis occurs again. Even though this process seems linear, steps 3 and 4 are often iterative. In practice, we tend to form initial explanations based on a small subset of data — often data points that are easy to recall or stand out. However, when we test those explanations against the broader dataset, we may notice inconsistencies or overlooked patterns that challenge our initial thinking. This prompts us to refine our explanations. This back and forth between steps 3 and 4 can occur many times. The 4 steps for analyzing usability-test data require collecting relevant data points from the study, assessing each data point to determine its relevancy, forming explanations that answer our research questions, and finally, checking and refining our explanations against the data for robustness. Step 1: Collect Relevant Data Usability tests generate a lot of data. In the Collect stage, we start to gather all relevant data points or observations that may help us answer our research questions. (This is a little bit like picking apples in an orchard — we're picking apples that look promising.) To do this, we inspect session notes, transcripts, and recordings, if available. We note or code relevant observations and quotes. For example, if we were looking to understand whether a comparison feature on the product-details page is discoverable, we might revisit notes and recordings to collect data points on the participants’ behavior, in-the-moment comments, and answers to the facilitator’s followup questions when performing relevant tasks. Questions we might ask include:  Did participants use or notice the feature (as indicated, for instance, by hovering the mouse cursor over the feature)? Did participants mention the comparison feature or the need to compare while thinking out loud or in answer to some of the facilitator’s followup questions? If they didn’t use the comparison feature, how did participants perform the comparison task? This step involves analysis since we’re breaking down the full data set and extracting a smaller set of useful items. Step 2: Assess for Accuracy In step 2, we’re still in the analysis stage (rather than synthesis). In the Assess step, we scrutinize data points to assess how relevant each is and how much weight to place on it. Not every data point is treated equally. To continue our apple-picking analogy, this is like the apple picker inspecting each apple for bruises or other imperfections. For example, maybe a participant commented that they liked the comparison feature but they never used it. Or perhaps this comment was in answer to a leading question from the facilitator (e.g., Did you like the comparison feature?), Knowing these details makes us trust the data point a little more or less. Step 3. Explain the Data In step 3, we begin the synthesis process by pooling our observations, assessments, and domain knowledge to generate likely explanations (or hypotheses) for the data we’ve collected. Sometimes several explanations are possible given the data we collected. For example, if all our participants missed the comparison feature we designed, the following explanations might be generated. Possible explanation 1: The feature was not in the place people expected. Possible explanation 2: It was hard to see or notice the feature. Possible explanation 3: The feature wasn’t useful to participants in the task, so they didn’t look for it. Coming up with explanations requires UX knowledge and experience, as well as an element of imagination. For example, if a researcher has conducted research on similar designs in the past, they might have lots of memories of human behavior and knowledge of design best practices to draw from. Step 4. Check for Good Fit To narrow down our possible explanations and build confidence in our explanations for user behavior, we test our explanations against the existing data for a good fit. Does the existing data support the explanation? Can we find data points that conflict with it or cause us to doubt its accuracy? This step is like seeing if a puzzle piece fits neatly. For example, if participants didn’t use the comparison feature on the product-overview page and our interpretation was that users don’t need it, then we would expect that participants utilized another successful strategy to compare products. However, if participants struggled to compare, opened many product-overview pages in multiple tabs to compare, or complained that it was hard to compare the available products, we might reject the explanation since it seems inaccurate given the data. What’s happening here is a version of hypothesis testing. Our explanation drives predictions; if the data does not support our predictions, we reject or refine our explanation. Qualitative research often uncovers valuable insights and answers — but it can also raise just as many new questions that need further research! It’s okay for your analysis to end with reporting that you don’t really know the reasons behind the data and that more research is needed. However, the mark of a good analyst is exploring their data from multiple angles, testing multiple explanations against the data, and putting forward some sensible hypotheses to explore in future research. Conclusion Analyzing data from qualitative usability testing is often more complex than it’s portrayed. This type of data is rich, nuanced, and messy. A good analyst collects all the right data, evaluates it from multiple angles, seeks explanations that fit the data, and tests those explanations for good fit.",
  "image": "https://media.nngroup.com/media/articles/opengraph_images/Social-Card-4-Steps-Analyze-Usability-Data-opengraph.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp\u003e\u003cspan\u003e\n                  Summary: \n                \u003c/span\u003eAnalyze with confidence by collecting relevant data, critically assessing it, and forming testable explanations.\n              \u003c/p\u003e\u003cdiv\u003e\n              \u003cdiv\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-why-analyzing-qualitative-user-tests-is-complicated-1\"\u003eWhy Analyzing Qualitative User Tests Is Complicated\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-4-steps-to-analyze-usability-test-data-2\"\u003e4 Steps to Analyze Usability-Test Data\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-step-1-collect-relevant-data-3\"\u003eStep 1: Collect Relevant Data\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-step-2-assess-for-accuracy-4\"\u003eStep 2: Assess for Accuracy\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-step-3-explain-the-data-5\"\u003eStep 3. Explain the Data\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-step-4-check-for-good-fit-6\"\u003eStep 4. Check for Good Fit\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\u003ch2 id=\"toc-why-analyzing-qualitative-user-tests-is-complicated-1\"\u003eWhy Analyzing Qualitative User Tests Is Complicated\u003c/h2\u003e\n\u003cp\u003eAnalyzing data from qualitative usability tests is often presented in textbooks as a straightforward process of cataloging usability issues, summarizing task performance, tallying errors, and summarizing time on task. However, it’s not always so easy. This is especially the case when you have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUnusual study designs (such as multiple prototype versions shown to each participant in a session)\u003c/li\u003e\n\u003cli\u003eComplex \u003ca href=\"https://www.nngroup.com/videos/research-goals-questions-hypotheses/\"\u003eresearch questions\u003c/a\u003e related to topics such as \u003ca href=\"https://www.nngroup.com/videos/findability-vs-discoverability/\"\u003ediscoverability\u003c/a\u003e, comprehension, or how people solve problems\u003c/li\u003e\n\u003cli\u003eLow-fidelity prototypes that don’t have all the features, content, or screens available\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet’s imagine we’ve designed a new product-details page that we’d like to test. Participants will be asked to find specific products on a prototype and choose one that’s best for them. Below are some of our research questions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIs the product-comparison feature discoverable?\u003c/li\u003e\n\u003cli\u003eWhat information about the product is important to users and what isn’t?\u003c/li\u003e\n\u003cli\u003eDo people understand the product-overview information?\u003c/li\u003e\n\u003cli\u003eDo people understand how the product works? Do they form the right mental model?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt might be hard to answer such research questions by studying performance in one task. To answer these questions successfully, we’d need to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGather multiple pieces of data\u003c/strong\u003e (what people did and said at multiple points in the session)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWeigh the data against the study design\u003c/strong\u003e, recruitment particulars (who participated in our research), and even facilitation events (i.e., what the researcher did or said)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTriangulate \u003c/strong\u003eall this information to provide a trustworthy answer\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAnalysis and Synthesis\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://www.nngroup.com/articles/data-findings-insights-differences/\"\u003eMoving from data to insights\u003c/a\u003e and recommendations requires a combination of two activities: analysis and synthesis.\u003c/p\u003e\n\u003cp\u003eAnalysis refers to the breaking down and inspection of complex information, whereas synthesis refers to the recombination of information\u003cstrong\u003e \u003c/strong\u003einto new meaningful forms, namely insights. (Note that when we talk about “analysis” as a stage in the research process, we’re really referring to both of these activities.)\u003c/p\u003e\n\u003cfigure\u003e\u003cimg alt=\"Analysis shows documents being broken down into sticky notes. Synthesis shows these sticky notes being combined to form an insight.\" height=\"979\" loading=\"lazy\" src=\"https://media.nngroup.com/media/editor/2025/04/23/analysis-versus-synthesis.jpg\" width=\"1384\"/\u003e\n\u003cfigcaption\u003e\u003cem\u003eAnalysis involves breaking down data into manageable units of information. Synthesis refers to rebuilding and combining them with other information to create insights, theories, and explanations.\u003c/em\u003e\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eWhen we analyze qualitative data, analysis and synthesis don’t happen in a neat linear fashion. Sometimes, we move back and forth between the two.\u003c/p\u003e\n\u003ch2 id=\"toc-4-steps-to-analyze-usability-test-data-2\"\u003e4 Steps to Analyze Usability-Test Data\u003c/h2\u003e\n\u003cp\u003eTo illustrate how we reach insights from usability test data and what the process of analysis and synthesis look like in this context, we propose a \u003cstrong\u003e4-step framework\u003c/strong\u003e.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eCollect relevant data\u003c/strong\u003e: Select data points from each session (observations and quotes) that are relevant to a research question. This step involves analysis and reducing the data set to something more manageable.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAssess for accuracy\u003c/strong\u003e: Examine each data point to assess its relevancy and accuracy. We’re still in the analysis phase here.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExplain the data\u003c/strong\u003e: Combine the data points (step 1), our assessments (step 2), and our expertise to provide reasonable explanations or answers to our research question. This step involves synthesis.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCheck for good fit\u003c/strong\u003e: Check our explanations or answers to our research questions against the data we collected to ensure good fit. Does all the data we collected support the explanation? If not, we iterate upon the explanation (by revisiting step 3). Step 4 is where analysis occurs again.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eEven though this process seems linear, steps 3 and 4 are often iterative. In practice, we tend to form initial explanations based on a small subset of data — often data points that are easy to recall or stand out. However, when we test those explanations against the broader dataset, we may notice inconsistencies or overlooked patterns that challenge our initial thinking. This prompts us to refine our explanations. This back and forth between steps 3 and 4 can occur many times.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg alt=\"4 steps to analyze usability test data are shown: Step 1: Collect Relevant Data. Step 2: Assess for Accuracy. Step 3: Explain the Data, and Step 4: Check for Good Fit. In addition an arrow links step 4 back to step 3 to show that some iteration happens between explaining the data and checking the explanation for good fit.\" height=\"956\" loading=\"lazy\" src=\"https://media.nngroup.com/media/editor/2025/04/23/4-steps-analysis.jpg\" width=\"692\"/\u003e\n\u003cfigcaption\u003e\u003cem\u003eThe 4 steps for analyzing usability-test data require collecting relevant data points from the study, assessing each data point to determine its relevancy, forming explanations that answer our research questions, and finally, checking and refining our explanations against the data for robustness.\u003c/em\u003e\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2 id=\"toc-step-1-collect-relevant-data-3\"\u003eStep 1: Collect Relevant Data\u003c/h2\u003e\n\u003cp\u003eUsability tests generate a lot of data. In the \u003cem\u003eCollect\u003c/em\u003e stage, we start to gather all relevant data points or observations that may help us answer our research questions. (This is a little bit like picking apples in an orchard — we\u0026#39;re picking apples that look promising.) To do this, we inspect session notes, transcripts, and recordings, if available. We note or \u003ca href=\"https://www.nngroup.com/videos/coding-thematic-analysis/\"\u003ecode\u003c/a\u003e relevant observations and quotes.\u003c/p\u003e\n\u003cp\u003eFor example, if we were looking to understand whether a comparison feature on the product-details page is discoverable, we might revisit notes and recordings to collect data points on the participants’ behavior, in-the-moment comments, and answers to the facilitator’s followup questions when performing relevant tasks. Questions we might ask include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e Did participants use or notice the feature (as indicated, for instance, by hovering the mouse cursor over the feature)?\u003c/li\u003e\n\u003cli\u003eDid participants mention the comparison feature or the need to compare while thinking out loud or in answer to some of the facilitator’s followup questions?\u003c/li\u003e\n\u003cli\u003eIf they didn’t use the comparison feature, how did participants perform the comparison task?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis step involves analysis since we’re breaking down the full data set and extracting a smaller set of useful items.\u003c/p\u003e\n\u003ch2 id=\"toc-step-2-assess-for-accuracy-4\"\u003eStep 2: Assess for Accuracy\u003c/h2\u003e\n\u003cp\u003eIn step 2, we’re still in the analysis stage (rather than synthesis). In the \u003cem\u003eAssess\u003c/em\u003e step, we \u003ca href=\"https://www.nngroup.com/articles/usability-data-in-analysis/\"\u003escrutinize data points to assess how relevant\u003c/a\u003e each is and how much weight to place on it. Not every data point is treated equally. To continue our apple-picking analogy, this is like the apple picker inspecting each apple for bruises or other imperfections.\u003c/p\u003e\n\u003cp\u003eFor example, maybe a participant commented that they liked the comparison feature but they never used it. Or perhaps this comment was in answer to a leading question from the facilitator (e.g., \u003cem\u003eDid you like the comparison feature?),\u003c/em\u003e Knowing these details makes us trust the data point a little more or less.\u003c/p\u003e\n\u003ch2 id=\"toc-step-3-explain-the-data-5\"\u003eStep 3. Explain the Data\u003c/h2\u003e\n\u003cp\u003eIn step 3, we begin the synthesis process by pooling our observations, assessments, and domain knowledge to \u003cstrong\u003egenerate likely explanations\u003c/strong\u003e (or \u003ca href=\"https://www.nngroup.com/videos/research-goals-questions-hypotheses/\"\u003ehypotheses\u003c/a\u003e) for the data we’ve collected.\u003c/p\u003e\n\u003cp\u003eSometimes several explanations are possible given the data we collected. For example, if all our participants missed the comparison feature we designed, the following explanations might be generated.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePossible explanation 1:\u003c/strong\u003e The feature was not in the place people expected.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePossible explanation 2:\u003c/strong\u003e It was hard to see or notice the feature.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePossible explanation 3:\u003c/strong\u003e The feature wasn’t useful to participants in the task, so they didn’t look for it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eComing up with explanations requires UX knowledge and experience, as well as an element of imagination. For example, if a researcher has conducted research on similar designs in the past, they might have lots of memories of human behavior and knowledge of design best practices to draw from.\u003c/p\u003e\n\u003ch2 id=\"toc-step-4-check-for-good-fit-6\"\u003eStep 4. Check for Good Fit\u003c/h2\u003e\n\u003cp\u003eTo narrow down our possible explanations and build confidence in our explanations for user behavior, we \u003cstrong\u003etest our explanations against the existing data\u003c/strong\u003e for a good fit. Does the existing data support the explanation? Can we find data points that conflict with it or cause us to doubt its accuracy? This step is like seeing if a puzzle piece fits neatly.\u003c/p\u003e\n\u003cp\u003eFor example, if participants didn’t use the comparison feature on the product-overview page and our interpretation was that users don’t need it, then we would expect that participants utilized another successful strategy to compare products. However, if participants struggled to compare, opened many product-overview pages in multiple tabs to compare, or complained that it was hard to compare the available products, we might reject the explanation since it seems inaccurate given the data.\u003c/p\u003e\n\u003cp\u003eWhat’s happening here is \u003cstrong\u003ea version of hypothesis testing. \u003c/strong\u003eOur explanation drives predictions; if the data does not support our predictions, we reject or refine our explanation.\u003c/p\u003e\n\u003cp\u003eQualitative research often uncovers valuable insights and answers — but it can also raise just as many new questions that need further research! It’s okay for your analysis to end with reporting that you don’t really know the reasons behind the data and that more research is needed. However, the mark of a good analyst is exploring their data from multiple angles, testing multiple explanations against the data, and putting forward some sensible hypotheses to explore in future research.\u003c/p\u003e\n\u003ch3\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eAnalyzing data from qualitative usability testing is often more complex than it’s portrayed. This type of data is rich, nuanced, and messy. A good analyst collects all the right data, evaluates it from multiple angles, seeks explanations that fit the data, and tests those explanations for good fit.\u003c/p\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-05-02T17:00:00Z",
  "modifiedTime": null
}
