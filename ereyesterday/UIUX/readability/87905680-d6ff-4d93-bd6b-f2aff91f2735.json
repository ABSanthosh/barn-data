{
  "id": "87905680-d6ff-4d93-bd6b-f2aff91f2735",
  "title": "When AI takes the driver’s seat",
  "link": "https://uxdesign.cc/when-ai-takes-the-drivers-seat-452732f92448?source=rss----138adf9c44c---4",
  "description": "",
  "author": "Patrick Morgan",
  "published": "Tue, 11 Feb 2025 23:54:23 GMT",
  "source": "https://uxdesign.cc/feed",
  "categories": [
    "ux",
    "ai",
    "artificial-intelligence",
    "ui-design",
    "ui"
  ],
  "byline": "Patrick Morgan",
  "length": 5114,
  "excerpt": "Watching OpenAI’s Operator navigate the web, I can’t help but think about the early days of my career in the user testing lab at American Express. As a design engineer, I sat quietly watching…",
  "siteName": "UX Collective",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*dn6MbbIIlwobt0jnUcrt_Q.png",
  "text": "OpenAI Operator and the shift to AI-first interfaces.Made with MidjourneyHow do we create user-centered experiences when our users aren’t human?Watching OpenAI’s Operator navigate the web, I can’t help but think about the early days of my career in the user testing lab at American Express.As a design engineer, I sat quietly watching customers use our mobile banking prototypes. Each interaction told a story: the smile of delight when something worked intuitively, the furrowed brow of confusion when it didn’t, the forced politeness of someone clearly just telling us what they thought we wanted to hear. It was fascinating — and crucial — to see our work through human eyes.That kind of scene has defined software development for decades. But with AI agents like Operator, this human-centric approach to design is starting to feel almost quaint. We’re entering an era where much of the software we create won’t be used by humans at all, but by AI. This shift fundamentally changes how we think about interfaces and challenges our core assumptions about human-computer interaction.Source: OpenAI DemoAI is changing from copilot to driverAt first glance, Operator’s interface appears simple: a chat UI with an embedded browser. But this simplicity hides a seismic shift in software interaction.The core tension lies in the interface itself: humans need a visual interface to take action and monitor the AI, but the agent would be perfectly content with just some plain text or an API. As agents take the lead in software interaction, what becomes of our carefully crafted visual interfaces? How do we balance agent-first interaction while allowing humans to effectively take command?This evolution mirrors what we’ve seen with autonomous vehicles, where Tesla and Waymo have maintained controls like steering wheels and pedals while gradually increasing vehicles’ autonomy. But while physical vehicles face real-world complexity that slows their transition to full self-driving, software agents operate in more controlled digital environments. Here, the transition from providing assistance to taking the lead could happen far more rapidly.Building trust through collaboration and visual feedbackEven if these agents were ready for full autonomy today (which they aren’t), users aren’t ready to grant it. While there’s excitement about Operator’s potential, it will take time for people to adjust. There’s no foundation of understanding and trust.This need for trust-building is evident in the interface design, which prioritizes visibility into the agent’s actions. This visual feedback loop is crucial for building user confidence, especially compared to prior voice-only interfaces like the Rabbit R1, where course-correcting an agent’s actions proved much more challenging.The user can “take control” from the agent. Source: OpenAI DemoCreating a sandbox for agents to play and learnThe technical decisions behind Operator’s release reflect an emphasis on safety with careful consideration of capability and control.OpenAI opted for a remote browsing approach instead of enabling computer use directly on users’ local machines. While this rules out some capabilities, it creates a sandboxed environment where the agent can operate while maintaining security. It also enables the benefit of running many workflows in parallel by distributing tasks across browsers in the cloud.This is how a new workforce emergesAs renowned AI researcher Andrej Karpathy noted on Operator’s launch day: “I think 2025–2035 is the decade of agents…… you’ll spin up organizations of Operators for long-running tasks of your choice (eg running a whole company).”Despite the initial demo, Operator is not a basic tool for ordering groceries — it’s the prototype of a tool capable of running entire businesses.Even in its limited release, Operator represents a key milestone in OpenAI’s quest to unlock AI as an active participant in the digital ecosystem. By launching within the $200/month Pro tier, the company can gather valuable training data from power users while limiting initial exposure, then gradually expand the tool’s footprint as it improves.The strategy is clear: start small, gather data, improve capabilities, expand access, and apply to as many workflows as possible.Rethinking what the future holdsFor those of us who’ve spent our careers laser-focused on human users, it’s time to expand our thinking. Just as I once sat watching humans navigate banking software, I now find myself studying AI agents navigating the web. The fundamental questions remain similar: How do we ensure reliable, efficient interaction? How do we enable appropriate control? How do we build trust?The next generation of software will need to serve both human and AI users, often simultaneously. Success won’t just be about technical capability — it will be about finding the right balance between automation and oversight, between AI capability and human control. And perhaps most crucially, it will be about designing experiences that make this collaboration feel even better than prior software interfaces we’ve come to know and love.",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*BqrJL7U4cII-ktI4_ntqhg.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"c736\"\u003eOpenAI Operator and the shift to AI-first interfaces.\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/@itspatmorgan?source=post_page---byline--452732f92448---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Patrick Morgan\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*ZCSPZ40wwJSGiREsYF2qaA.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://uxdesign.cc/?source=post_page---byline--452732f92448---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"UX Collective\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*mDhF9X4VO0rCrJvWFatyxg.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cfigure\u003e\u003cfigcaption\u003eMade with Midjourney\u003c/figcaption\u003e\u003c/figure\u003e\u003cblockquote\u003e\u003cp id=\"c51f\"\u003e\u003cstrong\u003eHow do we create user-centered experiences when our users aren’t human?\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"2f02\"\u003eWatching \u003ca href=\"https://operator.chatgpt.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOpenAI’s Operator\u003c/a\u003e navigate the web, I can’t help but think about the early days of my career in the user testing lab at American Express.\u003c/p\u003e\u003cp id=\"bda5\"\u003eAs a design engineer, I sat quietly watching customers use our mobile banking prototypes. Each interaction told a story: the smile of delight when something worked intuitively, the furrowed brow of confusion when it didn’t, the forced politeness of someone clearly just telling us what they thought we wanted to hear. It was fascinating — and crucial — to see our work through human eyes.\u003c/p\u003e\u003cp id=\"cec0\"\u003eThat kind of scene has defined software development for decades. But with AI agents like Operator, this human-centric approach to design is starting to feel almost quaint. We’re entering an era where much of the software we create won’t be used by humans at all, but by AI. This shift fundamentally changes how we think about interfaces and challenges our core assumptions about human-computer interaction.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eSource: \u003ca href=\"https://youtu.be/gYqs-wUKZsM?si=rqBK4s0uV9PDBe8N\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOpenAI Demo\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"3fcc\"\u003eAI is changing from copilot to driver\u003c/h2\u003e\u003cp id=\"a7c2\"\u003eAt first glance, Operator’s interface appears simple: a chat UI with an embedded browser. But this simplicity hides a seismic shift in software interaction.\u003c/p\u003e\u003cp id=\"5a4c\"\u003eThe core tension lies in the interface itself: \u003cem\u003ehumans need a visual interface to take action and monitor the AI, but the agent would be perfectly content with just some plain text or an API\u003c/em\u003e. As agents take the lead in software interaction, what becomes of our carefully crafted visual interfaces? How do we balance agent-first interaction while allowing humans to effectively take command?\u003c/p\u003e\u003cp id=\"a58b\"\u003eThis evolution mirrors what we’ve seen with autonomous vehicles, where Tesla and Waymo have maintained controls like steering wheels and pedals while gradually increasing vehicles’ autonomy. But while physical vehicles face real-world complexity that slows their transition to full self-driving, software agents operate in more controlled digital environments. Here, the transition from providing assistance to taking the lead could happen far more rapidly.\u003c/p\u003e\u003ch2 id=\"3d7e\"\u003eBuilding trust through collaboration and visual feedback\u003c/h2\u003e\u003cp id=\"af1a\"\u003eEven if these agents were ready for full autonomy today (which they aren’t), users aren’t ready to grant it. While there’s excitement about Operator’s potential, it will take time for people to adjust. There’s no foundation of understanding and trust.\u003c/p\u003e\u003cp id=\"99cf\"\u003eThis need for trust-building is evident in the interface design, which prioritizes visibility into the agent’s actions. This visual feedback loop is crucial for building user confidence, especially compared to prior voice-only interfaces like the \u003ca href=\"https://www.theverge.com/2024/5/2/24147159/rabbit-r1-review-ai-gadget\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRabbit R1\u003c/a\u003e, where course-correcting an agent’s actions proved much more challenging.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eThe user can “take control” from the agent. Source: \u003ca href=\"https://www.youtube.com/live/CSE77wAdDLg?si=0MhjjrzeexZiSQ8y\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOpenAI Demo\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"c79d\"\u003eCreating a sandbox for agents to play and learn\u003c/h2\u003e\u003cp id=\"08bf\"\u003eThe technical decisions behind Operator’s release reflect an \u003ca href=\"https://openai.com/index/introducing-operator/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eemphasis on safety\u003c/a\u003e with careful consideration of capability and control.\u003c/p\u003e\u003cp id=\"7a52\"\u003eOpenAI opted for a remote browsing approach instead of enabling computer use directly on users’ local machines. While this rules out some capabilities, it creates a sandboxed environment where the agent can operate while maintaining security. It also enables the benefit of running many workflows in parallel by distributing tasks across browsers in the cloud.\u003c/p\u003e\u003ch2 id=\"25c7\"\u003eThis is how a new workforce emerges\u003c/h2\u003e\u003cp id=\"a7d9\"\u003eAs renowned AI researcher \u003ca href=\"https://x.com/karpathy/status/1882544526033924438\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAndrej Karpathy noted on Operator’s launch day\u003c/a\u003e: “\u003cem\u003eI think 2025–2035 is the decade of agents…… you’ll spin up organizations of Operators for long-running tasks of your choice (eg running a whole company).\u003c/em\u003e”\u003c/p\u003e\u003cp id=\"8d43\"\u003eDespite the \u003ca href=\"https://www.youtube.com/watch?v=CSE77wAdDLg\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003einitial demo\u003c/a\u003e, Operator is not a basic tool for ordering groceries — it’s the prototype of a tool capable of running entire businesses.\u003c/p\u003e\u003cp id=\"1fd0\"\u003eEven in its limited release, Operator represents a key milestone in OpenAI’s quest to unlock AI as an active participant in the digital ecosystem. By launching within the \u003ca href=\"https://openai.com/index/introducing-chatgpt-pro/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e$200/month Pro tier\u003c/a\u003e, the company can gather valuable training data from power users while limiting initial exposure, then gradually expand the tool’s footprint as it improves.\u003c/p\u003e\u003cp id=\"d61b\"\u003eThe strategy is clear: start small, gather data, improve capabilities, expand access, and apply to as many workflows as possible.\u003c/p\u003e\u003ch2 id=\"a1ee\"\u003eRethinking what the future holds\u003c/h2\u003e\u003cp id=\"1518\"\u003eFor those of us who’ve spent our careers laser-focused on human users, it’s time to \u003ca href=\"https://www.unknownarts.co/p/computational-thinking-is-the-new\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eexpand our thinking\u003c/a\u003e. Just as I once sat watching humans navigate banking software, I now find myself studying AI agents navigating the web. The fundamental questions remain similar: \u003cem\u003eHow do we ensure reliable, efficient interaction? How do we enable appropriate control? How do we build trust?\u003c/em\u003e\u003c/p\u003e\u003cp id=\"c733\"\u003eThe next generation of software will need to serve both human and AI users, often simultaneously. Success won’t just be about technical capability — it will be about finding the right balance between automation and oversight, between AI capability and human control. And perhaps most crucially, it will be about designing experiences that make this collaboration feel even better than prior software interfaces we’ve come to know and love.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-02-11T23:54:23.017Z",
  "modifiedTime": null
}
