{
  "id": "65abf2c8-3e40-415f-8a3a-aed831b9e487",
  "title": "When users change their behavior to game the algorithm",
  "link": "https://uxdesign.cc/the-observer-effect-7ffb8d079ca4?source=rss----138adf9c44c---4",
  "description": "",
  "author": "Simon Mauro Guido",
  "published": "Wed, 23 Apr 2025 19:40:10 GMT",
  "source": "https://uxdesign.cc/feed",
  "categories": [
    "algorithms",
    "social-media",
    "ux",
    "ai",
    "product-design"
  ],
  "byline": "Simon Mauro Guido",
  "length": 4764,
  "excerpt": "There was a time when social media was simple. You followed people, liked posts, and as a result you got shown more of the same. But now the feeds we scroll through are less about who we follow, and…",
  "siteName": "UX Collective",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*dn6MbbIIlwobt0jnUcrt_Q.png",
  "text": "How our awareness is breaking the social media algorithm.It is said that the eyes of the Mona Lisa (Leonardo Da Vinci) follow you around the room, an illusion now known as the ‘Mona Lisa Effect’.There was a time when social media was simple. You followed people, liked posts, and as a result you got shown more of the same. But now the feeds we scroll through are less about who we follow, and more about how we behave.If you watch a reel for longer than three seconds you can expect more like it. If you linger on a photo, pause mid-scroll, replay a TikTok – that’s the new like. Platforms today care far less about your explicit choices, and more about your passive ones. And honestly, it was revolutionary – at first.This behavioural model promised a more authentic insight into our preferences. After all, what we do is often far more telling than what we say. It’s clever, subtle, and even somewhat intimate. But as we’ve come to understand how these systems work, we’ve also begun to perform for them, whether consciously or not.Knowing you’re being watchedThere’s a well-documented psychological phenomenon known as the Hawthorne effect – named after a series of productivity experiments in the 1920s at the Hawthorne Works factory. Researchers found that workers altered their behaviour simply because they knew they were being observed. More broadly, this aligns with the observer effect in behavioural science: our awareness of surveillance alters our actions.The Hawthorne Works factory where productivity experiments took place (circa 1925). Source: Western Electric Company Photograph AlbumNow apply that to your online behaviour.We’ve started to understand how the algorithm thinks. We might clear our watch history to reset our feed. We might even find ourselves clicking on content not because we necessarily want to watch it, but because we want to train the algorithm. We avoid pausing too long on something we don’t want to see more of, or to be associated with going forward. The system still watches us, but we’re no longer behaving naturally. We’re gaming it.This feedback loop becomes flawed not because the algorithm isn’t smart, but because the data it collects is no longer clean. We’ve turned from subjects to strategists. And once that happens, how effective can behavioural-based content delivery really be?The tension at the heart of the algorithmThere’s actually a deeper problem here, and it’s not just technical.Modern recommendation systems rely heavily on inferred intent from passive signals. But when these signals are harvested without the users’ full understanding, it challenges core principles of ethical design, especially informed consent and autonomy.A 2020 report by the Ada Lovelace Institute highlighted how opaque algorithmic systems undermine user agency, particularly when platforms fail to explain how recommendations are made or allow users to meaningfully contest them.Do we really understand and consent to how our social media feeds are being populated? Source: Cottonbro StudioIt raises some uncomfortable questions:Is it ethical to personalise content based on signals users aren’t aware they’re giving?Are users being manipulated into feeding the system, rather than served by it?Do we have a duty to design for agency, and not just engagement?“People cannot be empowered in an environment where they do not understand how decisions about them are made.”— Ada Lovelace Institute, Rethinking DataWhen algorithms adapt based on our most subtle behaviours, especially without transparency, we edge into the territory of surveillance design, as explored by Tristan Harris and the Center for Humane Technology. And that should give all of us pause for thought.Tristan Harris, Center for Humane Technology. Source: Center for Humane TechnologyLooking to an ethical future of content deliveryIf the behavioural model is beginning to crack under the weight of our awareness, where do we go next?Do platforms double down, trying to outsmart the user? Do they return to less accurate, but more honest and explicit signals? Or is the future something else entirely – something slower, more intentional, and more ethical?We’re entering an era where the assumptions behind content delivery need to be revisited. If we change our behaviour when watched, and we now all know we’re being watched, then we’re often feeding the machine a performance rather than our preference.And any system built on performance, rather than authenticity, eventually loses its grip on reality. Is the user still even enjoying the experience?It’s time we designed systems that respect the user not just as a data point, but as a conscious, consensual ally. Because the future of UX can’t just be personalised, it also needs to be principled.",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*k5bm4EuN3bcCEn-fQPKgLQ.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"aeef\"\u003eHow our awareness is breaking the social media algorithm.\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/@smgdesign?source=post_page---byline--7ffb8d079ca4---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Simon Mauro Guido\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*NCClZlIWzlK0NGPDd8NhOw.png\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://uxdesign.cc/?source=post_page---byline--7ffb8d079ca4---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"UX Collective\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*mDhF9X4VO0rCrJvWFatyxg.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eIt is said that the eyes of the Mona Lisa (Leonardo Da Vinci) follow you around the room, an illusion now known as the ‘Mona Lisa Effect’.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"56fd\"\u003eThere was a time when social media was simple. You followed people, liked posts, and as a result you got shown more of the same. But now the feeds we scroll through are less about who we follow, and more about how we behave.\u003c/p\u003e\u003cp id=\"45ea\"\u003eIf you watch a reel for longer than three seconds you can expect more like it. If you linger on a photo, pause mid-scroll, replay a TikTok – that’s the new \u003cem\u003elike\u003c/em\u003e. Platforms today care far less about your explicit choices, and more about your passive ones. And honestly, it was revolutionary – at first.\u003c/p\u003e\u003cp id=\"8cbf\"\u003eThis behavioural model promised a more \u003cem\u003eauthentic\u003c/em\u003e insight into our preferences. After all, what we \u003cem\u003edo\u003c/em\u003e is often far more telling than what we \u003cem\u003esay\u003c/em\u003e. It’s clever, subtle, and even somewhat intimate. But as we’ve come to understand how these systems work, we’ve also begun to perform for them, whether consciously or not.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"dc0b\"\u003eKnowing you’re being watched\u003c/h2\u003e\u003cp id=\"3a1a\"\u003eThere’s a well-documented psychological phenomenon known as \u003ca href=\"https://en.wikipedia.org/wiki/Hawthorne_effect\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ethe Hawthorne effect\u003c/a\u003e – named after a series of productivity experiments in the 1920s at the Hawthorne Works factory. Researchers found that workers altered their behaviour simply because they knew they were being observed. More broadly, this aligns with the observer effect in behavioural science: our awareness of surveillance alters our actions.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eThe Hawthorne Works factory where productivity experiments took place (circa 1925). Source: Western Electric Company Photograph Album\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"2063\"\u003eNow apply that to your online behaviour.\u003c/p\u003e\u003cp id=\"2ae8\"\u003eWe’ve started to understand how the algorithm thinks. We might clear our watch history to \u003cem\u003ereset\u003c/em\u003e our feed. We might even find ourselves clicking on content not because we necessarily want to watch it, but because we want to train the algorithm. We avoid pausing too long on something we don’t want to see more of, or to be associated with going forward. The system still watches us, but we’re no longer behaving naturally. We’re gaming it.\u003c/p\u003e\u003cp id=\"b3c0\"\u003eThis feedback loop becomes flawed not because the algorithm isn’t smart, but because the data it collects is no longer clean. We’ve turned from subjects to strategists. And once that happens, how effective can behavioural-based content delivery really be?\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"2b75\"\u003eThe tension at the heart of the algorithm\u003c/h2\u003e\u003cp id=\"40c0\"\u003eThere’s actually a deeper problem here, and it’s not just technical.\u003c/p\u003e\u003cp id=\"ef1b\"\u003eModern recommendation systems rely heavily on inferred intent from passive signals. But when these signals are harvested without the users’ full understanding, it challenges core principles of ethical design, especially informed consent and autonomy.\u003c/p\u003e\u003cp id=\"c6aa\"\u003e\u003ca href=\"https://www.adalovelaceinstitute.org/report/rethinking-data/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eA 2020 report by the Ada Lovelace Institute\u003c/a\u003e highlighted how opaque algorithmic systems undermine user agency, particularly when platforms fail to explain how recommendations are made or allow users to meaningfully contest them.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eDo we really understand and consent to how our social media feeds are being populated? Source: Cottonbro Studio\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"809a\"\u003eIt raises some uncomfortable questions:\u003c/p\u003e\u003cul\u003e\u003cli id=\"d8b2\"\u003eIs it ethical to personalise content based on signals users aren’t aware they’re giving?\u003c/li\u003e\u003cli id=\"2670\"\u003eAre users being manipulated into feeding the system, rather than served by it?\u003c/li\u003e\u003cli id=\"8956\"\u003eDo we have a duty to design for agency, and not just engagement?\u003c/li\u003e\u003c/ul\u003e\u003cblockquote\u003e\u003cp id=\"74db\"\u003e\u003cstrong\u003e“People cannot be empowered in an environment where they do not understand how decisions about them are made.”\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"d732\"\u003e— Ada Lovelace Institute, Rethinking Data\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"1e20\"\u003eWhen algorithms adapt based on our most subtle behaviours, especially without transparency, we edge into the territory of surveillance design, as explored by \u003ca href=\"https://www.humanetech.com/problem\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTristan Harris and the Center for Humane Technology\u003c/a\u003e. And that should give all of us pause for thought.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eTristan Harris, Center for Humane Technology. Source: Center for Humane Technology\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"36d4\"\u003eLooking to an ethical future of content delivery\u003c/h2\u003e\u003cp id=\"9c7a\"\u003eIf the behavioural model is beginning to crack under the weight of our awareness, where do we go next?\u003c/p\u003e\u003cp id=\"e1ea\"\u003eDo platforms double down, trying to outsmart the user? Do they return to less accurate, but more honest and explicit signals? Or is the future something else entirely – something slower, more intentional, and more ethical?\u003c/p\u003e\u003cp id=\"6b09\"\u003eWe’re entering an era where the assumptions behind content delivery need to be revisited. If we change our behaviour when watched, and we now all know we’re being watched, then we’re often feeding the machine a performance rather than our preference.\u003c/p\u003e\u003cp id=\"e183\"\u003eAnd any system built on performance, rather than authenticity, eventually loses its grip on reality. Is the user still even enjoying the experience?\u003c/p\u003e\u003cp id=\"2203\"\u003eIt’s time we designed systems that respect the user not just as a data point, but as a conscious, consensual ally. Because the future of UX can’t just be personalised, it also needs to be principled.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-04-13T15:26:06.67Z",
  "modifiedTime": null
}
