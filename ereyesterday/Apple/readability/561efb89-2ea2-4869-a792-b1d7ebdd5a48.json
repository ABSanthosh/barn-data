{
  "id": "561efb89-2ea2-4869-a792-b1d7ebdd5a48",
  "title": "How to Run DeepSeek LLM Locally on Mac",
  "link": "https://osxdaily.com/2025/04/10/how-to-run-deepseek-llm-locally-on-mac/",
  "description": "If you follow AI news, or even tech news, you might have heard of DeepSeek by now, the powerful Chinese Large Language Model that has capabilities rivaling ChatGPT while having dramatically lower training costs. DeepSeek is designed for advanced reasoning, with general purpose natural language and ChatBot abilities, task competency, research, while also being excellent ... Read More",
  "author": "Paul Horowitz",
  "published": "Thu, 10 Apr 2025 20:52:18 +0000",
  "source": "http://feeds.feedburner.com/osxdaily",
  "categories": [
    "Mac OS",
    "Tips \u0026 Tricks",
    "AI",
    "artificial intelligence",
    "DeepSeek",
    "llama",
    "LLM",
    "Mac",
    "macOS",
    "Mistral",
    "Phi",
    "tips",
    "tricks"
  ],
  "byline": "",
  "length": 4975,
  "excerpt": "If you follow AI news, or even tech news, you might have heard of DeepSeek by now, the powerful Chinese Large Language Model that has capabilities rivaling ChatGPT while having dramatically lower t…",
  "siteName": "OS X Daily",
  "favicon": "https://cdn.osxdaily.com/wp-content/uploads/2019/09/cropped-apple-touch-icon-104-192x192.png",
  "text": "If you follow AI news, or even tech news, you might have heard of DeepSeek by now, the powerful Chinese Large Language Model that has capabilities rivaling ChatGPT while having dramatically lower training costs. DeepSeek is designed for advanced reasoning, with general purpose natural language and ChatBot abilities, task competency, research, while also being excellent for coding, code generation, and logic, making it a powerful AI tool in general, and potentially for your workflow. While you can run DeepSeek anywhere through the web, and you can also download and run DeepSeek through an app on your iPhone or iPad using the DeepSeek Cloud, another option is available for Mac users; running DeepSeek LLM locally on your Mac. DeepSeek being run locally can be useful for many Mac users, whether you’re a developer, researcher, or someone simply curious about exploring AI and LLM utilities. One of the most significant benefits of using and running DeepSeek locally is that it’s offline, giving you the benefits of the DeepSeek LLM without relying on cloud services (that are linked to China, whatever you make of that), and offering more privacy and potential freedom to customize and fine-tune the model for your particular use cases. You must have an Apple Silicon Mac to be able to run DeepSeek locally. In this case we’re going to use a free tool for Mac called LM Studio to be able to quickly setup and use DeepSeek on a Mac. While all of this sounds complicated, and AI and LLM tech can be overwhelming to newcomers, this setup process is really quite easy, as we will walk you through it. How to Run DeepSeek Locally on Mac Again you must have an Apple Silicon Mac with an M-series chip or better to be able to run DeepSeek locally. While this software requirement is strict, it’s also standard with running any other local model, even including Apple Intelligence, ChatGPT, Llama, or any of the other increasingly common LLM tools. You’ll also want at least 10GB of available disk space. If you do not have an Apple Silicon Mac, you can still use AI tools through your iPhone or iPad using various apps, or any Mac via the web. For our purposes here, we’re assuming you have an Apple Silicon Mac, in which case here’s how to get DeekSeek running locally: Get lmstudio free from lmstudio.ai Mount the disk image for LMStudio and copy the ‘LM Studio’ app from the disk image into your Applications folder to install it, then launch LM Studio directly from your Applications folder On first launch, you’ll be presented with an onboarding splash screen, click on the green button that says “Get your first LLM” At the download your first local LLM screen, click on the green “Download” button (optionally, uncheck the box for “Enable local LLM service on login” if you don’t want a daemon to run for DeekSeek every time you boot your Mac) Let DeekSeek LLM download, it’s several GB and may take a while, once DeekSeek has downloaded locally you’ll be ready to interact with it directly on your Mac Now you’re free to use and interact with DeekSeek locally on your Mac. If you wish to confirm that your experience is entirely local, turn off your wi-fi or otherwise disconnect from the internet, and you will see that you can still interact with LM Studio and DeekSeek as you wish. Something you will likely notice is that running any LLM locally is probably going to be slower than using a cloud-based LLM, and that’s because cloud LLM’s have huge amounts of powerful resources dedicated exclusively to running the models, whereas when you run an LLM locally it will be limited by the local resources of your Mac, including your processor, memory, and what else is running on your Mac. If you’re interested exclusively in speed and performance, you’ll likely want to use a cloud provider instead, though the latest most powerful Macs also are quite speedy. You can also use LM Studio on the Mac to run other LLM’s locally, aside from DeekSeek this includes Llama, Mistral, and Phi, but we’re obviously focusing on DeepSeek here. We have discussed other options along the lines of local models in the past, including running Llama LLM locally on the Mac (including an uncensored model!), but this approach to running DeepSeek (and other LLMs if you’re curious) locally is really quite easy, and the performance is good too. Thanks to CultOfMac for the inspiration. Personally I’m a huge fan of the Mac client for ChatGPT, which I use frequently, but there’s also Perplexity, and many other interesting clients and AI tools out there. That’s aside from the dozens of web-based or app-based options too, including Bing with ChatGPT, Google Gemini, X’s Grok, Facebook’s Llama, and many others. What do you think of running DeepSeek locally on your Mac? What do you think of LLM and AI tools in general? Do you have a preferred local LLM, or a preferred LLM client? Share your thoughts and experiences with DeekSeek, LLM’s, and AI in general in the comments!",
  "image": "https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cp\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" src=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio-610x420.jpg\" alt=\"Run DeepSeek locally on a Mac easily with LM Studio\" width=\"610\" height=\"420\" srcset=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio-610x420.jpg 610w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio-300x206.jpg 300w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio-768x529.jpg 768w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio-1536x1057.jpg 1536w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio.jpg 1668w\" sizes=\"(max-width: 610px) 100vw, 610px\" data-old-src=\"https://cdn.osxdaily.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif\" data-src=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio-610x420.jpg\" data-srcset=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio-610x420.jpg 610w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio-300x206.jpg 300w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio-768x529.jpg 768w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio-1536x1057.jpg 1536w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/run-deepseek-locally-mac-lm-studio.jpg 1668w\"/\u003e\u003c/p\u003e\n\u003cp\u003eIf you follow AI news, or even tech news, you might have heard of DeepSeek by now, the powerful Chinese Large Language Model that has capabilities rivaling ChatGPT while having dramatically lower training costs. DeepSeek is designed for advanced reasoning, with general purpose natural language and ChatBot abilities, task competency, research, while also being excellent for coding, code generation, and logic, making it a powerful AI tool in general, and potentially for your workflow. While you can run DeepSeek anywhere through the web, and you can also download and run DeepSeek through an app on your iPhone or iPad using the DeepSeek Cloud, another option is available for Mac users; running DeepSeek LLM locally on your Mac. \u003c/p\u003e\n\u003cp\u003eDeepSeek being run locally can be useful for many Mac users, whether you’re a developer, researcher, or someone simply curious about exploring AI and LLM utilities. One of the most significant benefits of using and running DeepSeek locally is that it’s offline, giving you the benefits of the DeepSeek LLM without relying on cloud services (that are linked to China, whatever you make of that), and offering more privacy and potential freedom to customize and fine-tune the model for your particular use cases. \u003c/p\u003e\n\u003cp\u003eYou must have an Apple Silicon Mac to be able to run DeepSeek locally. In this case we’re going to use a free tool for Mac called LM Studio to be able to quickly setup and use DeepSeek on a Mac. While all of this sounds complicated, and AI and LLM tech can be overwhelming to newcomers, this setup process is really quite easy, as we will walk you through it.\u003c/p\u003e\n\n\u003ch2\u003eHow to Run DeepSeek Locally on Mac\u003c/h2\u003e\n\u003cp\u003eAgain you must have an Apple Silicon Mac with an M-series chip or better to be able to run DeepSeek locally. While this software requirement is strict, it’s also standard with running any other local model, even including Apple Intelligence, ChatGPT, Llama, or any of the other increasingly common LLM tools. You’ll also want at least 10GB of available disk space. If you do not have an Apple Silicon Mac, you can still use AI tools through your iPhone or iPad using various apps, or any Mac via the web. For our purposes here, we’re assuming you have an Apple Silicon Mac, in which case here’s how to get DeekSeek running locally:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://lmstudio.ai\" target=\"_blank\" rel=\"nofollow\"\u003eGet lmstudio free from lmstudio.ai\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eMount the disk image for LMStudio and copy the ‘LM Studio’ app from the disk image into your Applications folder to install it, then launch LM Studio directly from your Applications folder\u003c/li\u003e\n\u003cli\u003eOn first launch, you’ll be presented with an onboarding splash screen, click on the green button that says “Get your first LLM”\u003c/li\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-610x375.jpg\" alt=\"Setup DeepSeek locally on Mac\" width=\"610\" height=\"375\" srcset=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-610x375.jpg 610w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-300x184.jpg 300w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-768x472.jpg 768w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-1536x943.jpg 1536w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-2048x1258.jpg 2048w\" sizes=\"(max-width: 610px) 100vw, 610px\" data-old-src=\"https://cdn.osxdaily.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif\" data-src=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-610x375.jpg\" data-srcset=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-610x375.jpg 610w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-300x184.jpg 300w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-768x472.jpg 768w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-1536x943.jpg 1536w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/lmstudio-onboarding-get-first-local-llm-mac-2048x1258.jpg 2048w\"/\u003e\u003c/p\u003e\n\u003cli\u003eAt the download your first local LLM screen, click on the green “Download” button (optionally, uncheck the box for “Enable local LLM service on login” if you don’t want a daemon to run for DeekSeek every time you boot your Mac)\u003c/li\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-610x375.jpg\" alt=\"Download DeepSeek locally on Mac\" width=\"610\" height=\"375\" srcset=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-610x375.jpg 610w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-300x184.jpg 300w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-768x472.jpg 768w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-1536x943.jpg 1536w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-2048x1258.jpg 2048w\" sizes=\"(max-width: 610px) 100vw, 610px\" data-old-src=\"https://cdn.osxdaily.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif\" data-src=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-610x375.jpg\" data-srcset=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-610x375.jpg 610w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-300x184.jpg 300w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-768x472.jpg 768w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-1536x943.jpg 1536w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/download-deekseek-llm-locally-mac-lm-studio-2048x1258.jpg 2048w\"/\u003e\u003c/p\u003e\n\u003cli\u003eLet DeekSeek LLM download, it’s several GB and may take a while, once DeekSeek has downloaded locally you’ll be ready to interact with it directly on your Mac\u003c/li\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-610x375.jpg\" alt=\"Running DeepSeek locally on Mac with LM Studio\" width=\"610\" height=\"375\" srcset=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-610x375.jpg 610w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-300x184.jpg 300w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-768x472.jpg 768w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-1536x943.jpg 1536w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-2048x1258.jpg 2048w\" sizes=\"auto, (max-width: 610px) 100vw, 610px\" data-old-src=\"https://cdn.osxdaily.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif\" data-src=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-610x375.jpg\" data-srcset=\"https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-610x375.jpg 610w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-300x184.jpg 300w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-768x472.jpg 768w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-1536x943.jpg 1536w, https://cdn.osxdaily.com/wp-content/uploads/2025/04/running-deepseek-locally-mac-2048x1258.jpg 2048w\"/\u003e\n\u003c/p\u003e\u003c/ol\u003e\n\u003cp\u003eNow you’re free to use and interact with DeekSeek locally on your Mac. \u003c/p\u003e\n\u003cp\u003eIf you wish to confirm that your experience is entirely local, turn off your wi-fi or otherwise disconnect from the internet, and you will see that you can still interact with LM Studio and DeekSeek as you wish. \u003c/p\u003e\n\u003cp\u003eSomething you will likely notice is that running any LLM locally is probably going to be slower than using a cloud-based LLM, and that’s because cloud LLM’s have huge amounts of powerful resources dedicated exclusively to running the models, whereas when you run an LLM locally it will be limited by the local resources of your Mac, including your processor, memory, and what else is running on your Mac. If you’re interested exclusively in speed and performance, you’ll likely want to use a cloud provider instead, though the latest most powerful Macs also are quite speedy.\u003c/p\u003e\n\u003cp\u003eYou can also use LM Studio on the Mac to run other LLM’s locally, aside from DeekSeek this includes Llama, Mistral, and Phi, but we’re obviously focusing on DeepSeek here. We have discussed other options along the lines of local models in the past, including \u003ca href=\"https://osxdaily.com/2024/08/22/how-run-llama-llm-mac-locally/\"\u003erunning Llama LLM locally on the Mac (including an uncensored model!)\u003c/a\u003e, but this approach to running DeepSeek (and other LLMs if you’re curious) locally is really quite easy, and the performance is good too. Thanks to \u003ca href=\"https://www.cultofmac.com/how-to/run-deepseek-locally-on-mac\"\u003eCultOfMac\u003c/a\u003e for the inspiration.\u003c/p\u003e\n\u003cp\u003ePersonally I’m a huge fan of \u003ca href=\"https://osxdaily.com/2024/06/26/chatgpt-for-mac-now-available-to-all/\"\u003ethe Mac client for ChatGPT\u003c/a\u003e, which I use frequently, but there’s also \u003ca href=\"https://osxdaily.com/2024/10/24/perplexity-ai-brings-more-artificial-intelligence-tools-to-mac/\"\u003ePerplexity\u003c/a\u003e, and many other interesting clients and \u003ca href=\"https://osxdaily.com/tag/ai/\"\u003eAI tools\u003c/a\u003e out there. That’s aside from the dozens of web-based or app-based options too, including Bing with ChatGPT, Google Gemini, X’s Grok, Facebook’s Llama, and many others.\u003c/p\u003e\n\u003cp\u003eWhat do you think of running DeepSeek locally on your Mac? What do you think of LLM and AI tools in general? Do you have a preferred local LLM, or a preferred LLM client? Share your thoughts and experiences with DeekSeek, LLM’s, and AI in general in the comments!\u003c/p\u003e\n\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-04-10T20:52:18Z",
  "modifiedTime": "2025-04-10T20:52:18Z"
}
