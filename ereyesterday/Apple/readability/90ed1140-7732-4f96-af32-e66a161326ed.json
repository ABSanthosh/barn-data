{
  "id": "90ed1140-7732-4f96-af32-e66a161326ed",
  "title": "Apple’s machine learning framework is getting support for NVIDIA’s CUDA platform",
  "link": "https://9to5mac.com/2025/07/15/apples-machine-learning-framework-is-getting-support-for-nvidia-gpus/",
  "description": "Apple’s MLX machine learning framework, originally designed for Apple Silicon, is getting a CUDA backend, which is a pretty big deal. Here’s why. more…",
  "author": "Marcus Mendes",
  "published": "Wed, 16 Jul 2025 00:22:22 +0000",
  "source": "https://9to5mac.com/feed",
  "categories": [
    "News"
  ],
  "byline": "Marcus Mendes",
  "length": 2260,
  "excerpt": "That means developers will soon be able to run MLX models directly on NVIDIA GPUs, which is a pretty big deal. Here’s why.",
  "siteName": "9to5Mac",
  "favicon": "https://9to5mac.com/wp-content/uploads/sites/6/2019/10/cropped-cropped-mac1-1.png?w=192",
  "text": "Apple’s MLX machine learning framework, originally designed for Apple Silicon, is getting a CUDA backend, which is a pretty big deal. Here’s why. The work is being led by developer @zcbenz on GitHub (via AppleInsider), who started prototyping CUDA support a few months ago. Since then, he split the project into smaller pieces, and gradually merged them into Apple’s MLX’s main branch. The backend is still a work in progress, but several core operations, like matrix multiplication, softmax, reduction, sorting, and indexing, are already supported and tested. Wait, what is CUDA? Basically, CUDA (or Compute Unified Device Architecture) is the Metal of NVIDIA hardware: a computing platform the company created specifically to run on its own GPUs and make the most of them for high-performance parallel computing tasks. For many, CUDA is the standard way to run machine learning workloads on NVIDIA GPUs, and it’s used throughout the ML ecosystem, from academic research to commercial deployment. Frameworks like PyTorch and TensorFlow, which are names increasingly familiar even outside of deep ML circles, all rely on CUDA to tap into GPU acceleration. So why is Apple’s MLX now supporting CUDA? MLX was originally optimized for Apple Silicon and Metal, but adding a CUDA backend changes that. Now, researchers and engineers can prototype CUDA-based models locally on a Mac using MLX, and then deploy them on large-scale NVIDIA GPU clusters, which still dominate machine learning training workloads. That said, there are still limitations, most of which are works in progress. For instance, not all MLX operators are implemented yet, and AMD GPU support is still further down the road. Still, bringing MLX code to NVIDIA GPUs without having to rewrite it opens the door to faster testing, experimentation, and research use cases, which is pretty much all an AI developer can hope to hear. If you want to try it yourself, the details are available on GitHub. Apple Watch deals 40mm Apple Watch SE 2: $169 (was $249) 42mm Apple Watch Series 10: $299 (was $399) 46mm Apple Watch Series 10: $329 (was $429) Add 9to5Mac to your Google News feed.  FTC: We use income earning auto affiliate links. More.",
  "image": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/03/nvidia-apple-intelligence.jpg?resize=1200%2C628\u0026quality=82\u0026strip=all\u0026ssl=1",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\n\u003cfigure\u003e\n\t\u003cimg width=\"1600\" height=\"800\" src=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/03/nvidia-apple-intelligence.jpg?quality=82\u0026amp;strip=all\u0026amp;w=1600\" alt=\"NVIDIA Apple Intelligence\" srcset=\"https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/03/nvidia-apple-intelligence.jpg?w=320\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 320w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/03/nvidia-apple-intelligence.jpg?w=640\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 640w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/03/nvidia-apple-intelligence.jpg?w=1024\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 1024w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/03/nvidia-apple-intelligence.jpg?w=1500\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 1500w\" decoding=\"async\" fetchpriority=\"high\"/\u003e\u003c/figure\u003e\n\n\u003cp\u003eApple’s MLX machine learning framework, originally designed for Apple Silicon, is getting a CUDA backend, which is a pretty big deal. Here’s why.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe work is being led by developer \u003ca href=\"https://github.com/ml-explore/mlx/pull/1983\"\u003e@zcbenz on GitHub\u003c/a\u003e (via \u003ca href=\"https://appleinsider.com/articles/25/07/15/apple-silicon-machine-learning-code-may-become-more-easily-portable-to-nvidia-hardware\"\u003e\u003cem\u003eAppleInsider\u003c/em\u003e\u003c/a\u003e), who started prototyping CUDA support a few months ago. Since then, he split the project into smaller pieces, and gradually merged them into Apple’s MLX’s main branch.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe backend is still a work in progress, but several core operations, like matrix multiplication, softmax, reduction, sorting, and indexing, are already supported and tested.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-wait-what-is-cuda\"\u003eWait, what is CUDA?\u003c/h2\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eBasically\u003c/em\u003e, \u003ca href=\"https://developer.nvidia.com/cuda-toolkit\"\u003eCUDA\u003c/a\u003e (or Compute Unified Device Architecture) is the \u003ca href=\"https://9to5mac.com/guides/metal/\"\u003eMetal\u003c/a\u003e of NVIDIA hardware: a computing platform the company created specifically to run on its own GPUs and make the most of them for high-performance parallel computing tasks.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor many, CUDA is the standard way to run machine learning workloads on NVIDIA GPUs, and it’s used throughout the ML ecosystem, from academic research to commercial deployment. Frameworks like PyTorch and TensorFlow, which are names increasingly familiar even outside of deep ML circles, all rely on CUDA to tap into GPU acceleration.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-so-why-is-apple-s-mlx-now-supporting-cuda\"\u003eSo why is Apple’s MLX now supporting CUDA?\u003c/h2\u003e\n\n\n\n\u003cp\u003eMLX was originally optimized for Apple Silicon and Metal, but adding a CUDA backend changes that. Now, researchers and engineers can prototype CUDA-based models locally on a Mac using MLX, and then deploy them on large-scale NVIDIA GPU clusters, which still dominate machine learning training workloads.\u003c/p\u003e\n\n\n\n\u003cp\u003eThat said, there are still limitations, most of which are works in progress. For instance, not all MLX operators are implemented yet, and AMD GPU support is still further down the road.\u003c/p\u003e\n\n\n\n\u003cp\u003eStill, bringing MLX code to NVIDIA GPUs without having to rewrite it opens the door to faster testing, experimentation, and research use cases, which is pretty much all an AI developer can hope to hear.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to try it yourself, the details are \u003ca href=\"https://github.com/ml-explore/mlx/pull/1983\"\u003eavailable on GitHub\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch4 id=\"h-apple-watch-deals\"\u003eApple Watch deals\u003c/h4\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.amazon.com/Apple-Smartwatch-Midnight-Aluminium-Detection/dp/B0DGJ4RTVT?tag=marcmendes-20\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e40mm Apple Watch SE 2\u003c/a\u003e: \u003cstrong\u003e$169\u003c/strong\u003e (was $249)\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003ca href=\"https://www.amazon.com/dp/B0DGHHJ1PJ/ref=twister_B0DGKWJLTY?_encoding=UTF8\u0026amp;th=1?tag=marcmendes-20\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e42mm Apple Watch Series 10\u003c/a\u003e: \u003cstrong\u003e$299\u003c/strong\u003e (was $399)\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003ca href=\"https://www.amazon.com/Apple-Smartwatch-Silver-Aluminium-Always/dp/B0DGJ745J6?tag=marcmendes-20\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e46mm Apple Watch Series 10\u003c/a\u003e: \u003cstrong\u003e$329\u003c/strong\u003e (was $429)\u003c/li\u003e\n\u003c/ul\u003e\n\t\u003cp\u003e\n\t\t\u003ca target=\"_blank\" rel=\"nofollow\" href=\"https://news.google.com/publications/CAAqBggKMLOFATDAGg?hl=en-US\u0026amp;gl=US\u0026amp;ceid=US:en\"\u003e\n\t\t\t\u003cem\u003eAdd 9to5Mac to your Google News feed.\u003c/em\u003e \n\t\t\t\t\t\u003c/a\u003e\n\t\u003c/p\u003e\n\t\u003cp\u003e\u003cem\u003eFTC: We use income earning auto affiliate links.\u003c/em\u003e \u003ca href=\"https://9to5mac.com/about/#affiliate\"\u003eMore.\u003c/a\u003e\u003c/p\u003e\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2025-07-16T00:22:22Z",
  "modifiedTime": "2025-07-16T01:18:22Z"
}
