{
  "id": "2c6dd8ce-e681-4c1f-97b9-744a4f890137",
  "title": "Claude 4",
  "link": "https://www.anthropic.com/news/claude-4",
  "description": "",
  "author": "John Gruber",
  "published": "2025-05-22T23:25:46Z",
  "source": "https://daringfireball.net/feeds/main",
  "categories": null,
  "byline": "",
  "length": 10276,
  "excerpt": "Discover Claude 4's breakthrough AI capabilities. Experience more reliable, interpretable assistance for complex tasks across work and learning.",
  "siteName": "",
  "favicon": "https://www.anthropic.com/images/icons/apple-touch-icon.png",
  "text": "Today, we’re introducing the next generation of Claude models: Claude Opus 4 and Claude Sonnet 4, setting new standards for coding, advanced reasoning, and AI agents. Claude Opus 4 is the world’s best coding model, with sustained performance on complex, long-running tasks and agent workflows. Claude Sonnet 4 is a significant upgrade to Claude Sonnet 3.7, delivering superior coding and reasoning while responding more precisely to your instructions.Alongside the models, we're also announcing:Extended thinking with tool use (beta): Both models can use tools—like web search—during extended thinking, allowing Claude to alternate between reasoning and tool use to improve responses.New model capabilities: Both models can use tools in parallel, follow instructions more precisely, and—when given access to local files by developers—demonstrate significantly improved memory capabilities, extracting and saving key facts to maintain continuity and build tacit knowledge over time.Claude Code is now generally available: After receiving extensive positive feedback during our research preview, we’re expanding how developers can collaborate with Claude. Claude Code now supports background tasks via GitHub Actions and native integrations with VS Code and JetBrains, displaying edits directly in your files for seamless pair programming.New API capabilities: We’re releasing four new capabilities on the Anthropic API that enable developers to build more powerful AI agents: the code execution tool, MCP connector, Files API, and the ability to cache prompts for up to one hour.Claude Opus 4 and Sonnet 4 are hybrid models offering two modes: near-instant responses and extended thinking for deeper reasoning. The Pro, Max, Team, and Enterprise Claude plans include both models and extended thinking, with Sonnet 4 also available to free users. Both models are available on the Anthropic API, Amazon Bedrock, and Google Cloud's Vertex AI. Pricing remains consistent with previous Opus and Sonnet models: Opus 4 at $15/$75 per million tokens (input/output) and Sonnet 4 at $3/$15.Claude 4Claude Opus 4 is our most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours—dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish.Claude Opus 4 excels at coding and complex problem-solving, powering frontier agent products. Cursor calls it state-of-the-art for coding and a leap forward in complex codebase understanding. Replit reports improved precision and dramatic advancements for complex changes across multiple files. Block calls it the first model to boost code quality during editing and debugging in its agent, codename goose, while maintaining full performance and reliability. Rakuten validated its capabilities with a demanding open-source refactor running independently for 7 hours with sustained performance. Cognition notes Opus 4 excels at solving complex challenges that other models can't, successfully handling critical actions that previous models have missed.Claude Sonnet 4 significantly improves on Sonnet 3.7's industry-leading capabilities, excelling in coding with a state-of-the-art 72.7% on SWE-bench. The model balances performance and efficiency for internal and external use cases, with enhanced steerability for greater control over implementations. While not matching Opus 4 in most domains, it delivers an optimal mix of capability and practicality.GitHub says Claude Sonnet 4 soars in agentic scenarios and will introduce it as the model powering the new coding agent in GitHub Copilot. Manus highlights its improvements in following complex instructions, clear reasoning, and aesthetic outputs. iGent reports Sonnet 4 excels at autonomous multi-feature app development, as well as substantially improved problem-solving and codebase navigation—reducing navigation errors from 20% to near zero. Sourcegraph says the model shows promise as a substantial leap in software development—staying on track longer, understanding problems more deeply, and providing more elegant code quality. Augment Code reports higher success rates, more surgical code edits, and more careful work through complex tasks, making it the top choice for their primary model.These models advance our customers' AI strategies across the board: Opus 4 pushes boundaries in coding, research, writing, and scientific discovery, while Sonnet 4 brings frontier performance to everyday use cases as an instant upgrade from Sonnet 3.7.Claude 4 models lead on SWE-bench Verified, a benchmark for performance on real software engineering tasks. See appendix for more on methodology.Claude 4 models deliver strong performance across coding, reasoning, multimodal capabilities, and agentic tasks. See appendix for more on methodology.Model improvementsIn addition to extended thinking with tool use, parallel tool execution, and memory improvements, we’ve significantly reduced behavior where the models use shortcuts or loopholes to complete tasks. Both models are 65% less likely to engage in this behavior than Sonnet 3.7 on agentic tasks that are particularly susceptible to shortcuts and loopholes.Claude Opus 4 also dramatically outperforms all previous models on memory capabilities. When developers build applications that provide Claude local file access, Opus 4 becomes skilled at creating and maintaining 'memory files' to store key information. This unlocks better long-term task awareness, coherence, and performance on agent tasks—like Opus 4 creating a 'Navigation Guide' while playing Pokémon.Memory: When given access to local files, Claude Opus 4 records key information to help improve its game play. The notes depicted above are real notes taken by Opus 4 while playing Pokémon.Finally, we've introduced thinking summaries for Claude 4 models that use a smaller model to condense lengthy thought processes. This summarization is only needed about 5% of the time—most thought processes are short enough to display in full. Users requiring raw chains of thought for advanced prompt engineering can contact sales about our new Developer Mode to retain full access.Claude CodeClaude Code, now generally available, brings the power of Claude to more of your development workflow—in the terminal, your favorite IDEs, and running in the background with the Claude Code SDK.New beta extensions for VS Code and JetBrains integrate Claude Code directly into your IDE. Claude’s proposed edits appear inline in your files, streamlining review and tracking within the familiar editor interface. Simply run Claude Code in your IDE terminal to install.Beyond the IDE, we're releasing an extensible Claude Code SDK, so you can build your own agents and applications using the same core agent as Claude Code. We're also releasing an example of what's possible with the SDK: Claude Code on GitHub, now in beta. Tag Claude Code on PRs to respond to reviewer feedback, fix CI errors, or modify code. To install, run /install-github-app from within Claude Code.Getting startedThese models are a large step toward the virtual collaborator—maintaining full context, sustaining focus on longer projects, and driving transformational impact. They come with extensive testing and evaluation to minimize risk and maximize safety, including implementing measures for higher AI Safety Levels like ASL-3.We're excited to see what you'll create. Get started today on Claude, Claude Code, or the platform of your choice.As always, your feedback helps us improve.AppendixPerformance benchmark data sourcesOpen AI: o3 launch post, o3 system card, GPT-4.1 launch post, GPT-4.1 hosted evalsGemini: Gemini 2.5 Pro Preview model cardClaude: Claude 3.7 Sonnet launch postPerformance benchmark reportingClaude Opus 4 and Sonnet 4 are hybrid reasoning models. The benchmarks reported in this blog post show the highest scores achieved with or without extended thinking. We’ve noted below for each result whether extended thinking was used:No extended thinking: SWE-bench Verified, Terminal-benchExtended thinking (up to 64K tokens):TAU-bench (no results w/o extended thinking reported)GPQA Diamond (w/o extended thinking: Opus 4 scores 74.9% and Sonnet 4 is 70.0%)MMMLU (w/o extended thinking: Opus 4 scores 87.4% and Sonnet 4 is 85.4%)MMMU (w/o extended thinking: Opus 4 scores 73.7% and Sonnet 4 is 72.6%)AIME (w/o extended thinking: Opus 4 scores 33.9% and Sonnet 4 is 33.1%)TAU-bench methodologyScores were achieved with a prompt addendum to both the Airline and Retail Agent Policy instructing Claude to better leverage its reasoning abilities while using extended thinking with tool use. The model is encouraged to write down its thoughts as it solves the problem distinct from our usual thinking mode, during the multi-turn trajectories to best leverage its reasoning abilities. To accommodate the additional steps Claude incurs by utilizing more thinking, the maximum number of steps (counted by model completions) was increased from 30 to 100 (most trajectories completed under 30 steps with only one trajectory reaching above 50 steps).SWE-bench methodologyFor the Claude 4 family of models, we continue to use the same simple scaffold that equips the model with solely the two tools described in our prior releases here—a bash tool, and a file editing tool that operates via string replacements. We no longer include the third ‘planning tool’ used by Claude 3.7 Sonnet. On all Claude 4 models, we report scores out of the full 500 problems. Scores for OpenAI models are reported out of a 477 problem subset.For our “high compute” numbers we adopt additional complexity and parallel test-time compute as follows:We sample multiple parallel attempts.We discard patches that break the visible regression tests in the repository, similar to the rejection sampling approach adopted by Agentless (Xia et al. 2024); note no hidden test information is used.We then use an internal scoring model to select the best candidate from the remaining attempts.This results in a score of 79.4% and 80.2% for Opus 4 and Sonnet 4 respectively.",
  "image": "https://cdn.sanity.io/images/4zrzovbb/website/c17d44d1f2095a3e3215c5446b4ed40340776830-2400x1260.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cp\u003eToday, we’re introducing the next generation of Claude models: \u003cstrong\u003eClaude Opus 4\u003c/strong\u003e and \u003cstrong\u003eClaude Sonnet 4\u003c/strong\u003e, setting new standards for coding, advanced reasoning, and AI agents. \u003c/p\u003e\u003cp\u003eClaude Opus 4 is the world’s best coding model, with sustained performance on complex, long-running tasks and agent workflows. Claude Sonnet 4 is a significant upgrade to Claude Sonnet 3.7, delivering superior coding and reasoning while responding more precisely to your instructions.\u003c/p\u003e\u003cp\u003eAlongside the models, we\u0026#39;re also announcing:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eExtended thinking with tool use (beta)\u003c/strong\u003e: Both models can use tools—like \u003ca href=\"https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool\"\u003eweb search\u003c/a\u003e—during extended thinking, allowing Claude to alternate between reasoning and tool use to improve responses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNew model capabilities\u003c/strong\u003e: Both models can use tools in parallel, follow instructions more precisely, and—when given access to local files by developers—demonstrate significantly improved memory capabilities, extracting and saving key facts to maintain continuity and build tacit knowledge over time.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eClaude Code is now generally available\u003c/strong\u003e: After receiving extensive positive feedback during our research preview, we’re expanding how developers can collaborate with Claude. Claude Code now supports background tasks via GitHub Actions and native integrations with VS Code and JetBrains, displaying edits directly in your files for seamless pair programming.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNew API capabilities:\u003c/strong\u003e We’re releasing \u003ca href=\"https://www.anthropic.com/news/agent-capabilities-api\"\u003efour new capabilities\u003c/a\u003e on the Anthropic API that enable developers to build more powerful AI agents: the code execution tool, MCP connector, Files API, and the ability to cache prompts for up to one hour.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eClaude Opus 4 and Sonnet 4 are hybrid models offering two modes: near-instant responses and extended thinking for deeper reasoning. The Pro, Max, Team, and Enterprise Claude plans include both models and extended thinking, with Sonnet 4 also available to free users. Both models are available on the Anthropic API, Amazon Bedrock, and Google Cloud\u0026#39;s Vertex AI. Pricing remains consistent with previous Opus and Sonnet models: Opus 4 at $15/$75 per million tokens (input/output) and Sonnet 4 at $3/$15.\u003c/p\u003e\u003ch2 id=\"claude-4\"\u003eClaude 4\u003c/h2\u003e\u003cp\u003eClaude Opus 4 is our most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours—dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish.\u003c/p\u003e\u003cp\u003eClaude Opus 4 excels at coding and complex problem-solving, powering frontier agent products. \u003cstrong\u003eCursor\u003c/strong\u003e calls it state-of-the-art for coding and a leap forward in complex codebase understanding. \u003cstrong\u003eReplit\u003c/strong\u003e reports improved precision and dramatic advancements for complex changes across multiple files. \u003cstrong\u003eBlock\u003c/strong\u003e calls it the first model to boost code quality during editing and debugging in its agent, \u003cem\u003ecodename goose\u003c/em\u003e, while maintaining full performance and reliability. \u003cstrong\u003eRakuten\u003c/strong\u003e validated its capabilities with a demanding open-source refactor running independently for 7 hours with sustained performance. \u003cstrong\u003eCognition\u003c/strong\u003e notes Opus 4 excels at solving complex challenges that other models can\u0026#39;t, successfully handling critical actions that previous models have missed.\u003c/p\u003e\u003cp\u003eClaude Sonnet 4 significantly improves on Sonnet 3.7\u0026#39;s industry-leading capabilities, excelling in coding with a state-of-the-art 72.7% on SWE-bench. The model balances performance and efficiency for internal and external use cases, with enhanced steerability for greater control over implementations. While not matching Opus 4 in most domains, it delivers an optimal mix of capability and practicality.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eGitHub\u003c/strong\u003e says Claude Sonnet 4 soars in agentic scenarios and will introduce it as the model powering the new coding agent in GitHub Copilot. \u003cstrong\u003eManus\u003c/strong\u003e highlights its improvements in following complex instructions, clear reasoning, and aesthetic outputs. \u003cstrong\u003eiGent\u003c/strong\u003e reports Sonnet 4 excels at autonomous multi-feature app development, as well as substantially improved problem-solving and codebase navigation—reducing navigation errors from 20% to near zero. \u003cstrong\u003eSourcegraph\u003c/strong\u003e says the model shows promise as a substantial leap in software development—staying on track longer, understanding problems more deeply, and providing more elegant code quality. \u003cstrong\u003eAugment Code\u003c/strong\u003e reports higher success rates, more surgical code edits, and more careful work through complex tasks, making it the top choice for their primary model.\u003c/p\u003e\u003cp\u003eThese models advance our customers\u0026#39; AI strategies across the board: Opus 4 pushes boundaries in coding, research, writing, and scientific discovery, while Sonnet 4 brings frontier performance to everyday use cases as an instant upgrade from Sonnet 3.7.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"Bar chart comparison between Claude and other LLMs on software engineering tasks\" loading=\"lazy\" width=\"3840\" height=\"2304\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F09a6d5aa47c25cb2037efff9f486da4918f77708-3840x2304.png\u0026amp;w=3840\u0026amp;q=75 1x\" src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F09a6d5aa47c25cb2037efff9f486da4918f77708-3840x2304.png\u0026amp;w=3840\u0026amp;q=75\"/\u003e\u003cfigcaption\u003eClaude 4 models lead on SWE-bench Verified, a benchmark for performance on real software engineering tasks. See appendix for more on methodology.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"Benchmark table comparing Opus 4 and Sonnet 4 to other LLM\" loading=\"lazy\" width=\"2600\" height=\"2118\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6246b412f30444ce8e1e5746e226c56a743bd99f-2600x2118.png\u0026amp;w=3840\u0026amp;q=75 1x\" src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6246b412f30444ce8e1e5746e226c56a743bd99f-2600x2118.png\u0026amp;w=3840\u0026amp;q=75\"/\u003e\u003cfigcaption\u003eClaude 4 models deliver strong performance across coding, reasoning, multimodal capabilities, and agentic tasks. See appendix for more on methodology.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003ch2 id=\"model-improvements\"\u003eModel improvements\u003c/h2\u003e\u003cp\u003eIn addition to extended thinking with tool use, parallel tool execution, and memory improvements, we’ve significantly reduced behavior where the models use shortcuts or loopholes to complete tasks. Both models are 65% less likely to engage in this behavior than Sonnet 3.7 on agentic tasks that are particularly susceptible to shortcuts and loopholes.\u003c/p\u003e\u003cp\u003eClaude Opus 4 also dramatically outperforms all previous models on memory capabilities. When developers build applications that provide Claude local file access, Opus 4 becomes skilled at creating and maintaining \u0026#39;memory files\u0026#39; to store key information. This unlocks better long-term task awareness, coherence, and performance on agent tasks—like Opus 4 creating a \u0026#39;Navigation Guide\u0026#39; while playing Pokémon.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"A visual note in Claude\u0026#39;s memories that depicts a navigation guide for the game Pokemon Red.\" loading=\"lazy\" width=\"1920\" height=\"1080\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe51564bb5ce9597dbfc59bbab13a0efbe25a7d66-1920x1080.gif\u0026amp;w=1920\u0026amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe51564bb5ce9597dbfc59bbab13a0efbe25a7d66-1920x1080.gif\u0026amp;w=3840\u0026amp;q=75 2x\" src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe51564bb5ce9597dbfc59bbab13a0efbe25a7d66-1920x1080.gif\u0026amp;w=3840\u0026amp;q=75\"/\u003e\u003cfigcaption\u003eMemory: When given access to local files, Claude Opus 4 records key information to help improve its game play. The notes depicted above are real notes taken by Opus 4 while playing Pokémon.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eFinally, we\u0026#39;ve introduced thinking summaries for Claude 4 models that use a smaller model to condense lengthy thought processes. This summarization is only needed about 5% of the time—most thought processes are short enough to display in full. Users requiring raw chains of thought for advanced prompt engineering can \u003ca href=\"https://www.anthropic.com/contact-sales\"\u003econtact sales\u003c/a\u003e about our new Developer Mode to retain full access.\u003c/p\u003e\u003ch2 id=\"claude-code\"\u003eClaude Code\u003c/h2\u003e\u003cp\u003eClaude Code, now generally available, brings the power of Claude to more of your development workflow—in the terminal, your favorite IDEs, and running in the background with the Claude Code SDK.\u003c/p\u003e\u003cp\u003eNew beta extensions for VS Code and JetBrains integrate Claude Code directly into your IDE. Claude’s proposed edits appear inline in your files, streamlining review and tracking within the familiar editor interface. Simply run Claude Code in your IDE terminal to install.\u003c/p\u003e\u003cp\u003eBeyond the IDE, we\u0026#39;re releasing an extensible Claude Code SDK, so you can build your own agents and applications using the same core agent as Claude Code. We\u0026#39;re also releasing an example of what\u0026#39;s possible with the SDK: Claude Code on GitHub, now in beta. Tag Claude Code on PRs to respond to reviewer feedback, fix CI errors, or modify code. To install, run /install-github-app from within Claude Code.\u003c/p\u003e\u003ch2 id=\"getting-started\"\u003eGetting started\u003c/h2\u003e\u003cp\u003eThese models are a large step toward the virtual collaborator—maintaining full context, sustaining focus on longer projects, and driving transformational impact. They come with extensive testing and evaluation to minimize risk and maximize safety, including \u003ca href=\"https://www.anthropic.com/news/activating-asl3-protections\"\u003eimplementing measures\u003c/a\u003e for higher AI Safety Levels like ASL-3.\u003c/p\u003e\u003cp\u003eWe\u0026#39;re excited to see what you\u0026#39;ll create. Get started today on \u003ca href=\"https://claude.ai/\"\u003eClaude\u003c/a\u003e, \u003ca href=\"https://www.anthropic.com/claude-code\"\u003eClaude Code\u003c/a\u003e, or the platform of your choice.\u003c/p\u003e\u003cp\u003e\u003cem\u003eAs always, your \u003ca href=\"mailto: feedback@anthropic.com\"\u003efeedback\u003c/a\u003e helps us improve.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article\u003e\u003cdiv\u003e\u003ch4\u003eAppendix\u003c/h4\u003e\u003ch4\u003ePerformance benchmark data sources\u003c/h4\u003e\u003cul\u003e\u003cli\u003eOpen AI: \u003ca href=\"https://openai.com/index/introducing-o3-and-o4-mini/\"\u003eo3 launch post\u003c/a\u003e, \u003ca href=\"https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf\"\u003eo3 system card\u003c/a\u003e, \u003ca href=\"https://openai.com/index/gpt-4-1/\"\u003eGPT-4.1 launch post\u003c/a\u003e, \u003ca href=\"https://github.com/openai/simple-evals/blob/main/multilingual_mmlu_benchmark_results.md\"\u003eGPT-4.1 hosted evals\u003c/a\u003e\u003c/li\u003e\u003cli\u003eGemini: \u003ca href=\"https://storage.googleapis.com/model-cards/documents/gemini-2.5-pro-preview.pdf\"\u003eGemini 2.5 Pro Preview model card\u003c/a\u003e\u003c/li\u003e\u003cli\u003eClaude: \u003ca href=\"https://www.anthropic.com/news/claude-3-7-sonnet\"\u003eClaude 3.7 Sonnet launch post\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch4\u003ePerformance benchmark reporting\u003c/h4\u003e\u003cp\u003eClaude Opus 4 and Sonnet 4 are hybrid reasoning models. The benchmarks reported in this blog post show the highest scores achieved with or without extended thinking. We’ve noted below for each result whether extended thinking was used:\u003c/p\u003e\u003cul\u003e\u003cli\u003eNo extended thinking: SWE-bench Verified, Terminal-bench\u003c/li\u003e\u003cli\u003eExtended thinking (up to 64K tokens):\u003cul\u003e\u003cli\u003eTAU-bench (no results w/o extended thinking reported)\u003c/li\u003e\u003cli\u003eGPQA Diamond (w/o extended thinking: Opus 4 scores 74.9% and Sonnet 4 is 70.0%)\u003c/li\u003e\u003cli\u003eMMMLU (w/o extended thinking: Opus 4 scores 87.4% and Sonnet 4 is 85.4%)\u003c/li\u003e\u003cli\u003eMMMU (w/o extended thinking: Opus 4 scores 73.7% and Sonnet 4 is 72.6%)\u003c/li\u003e\u003cli\u003eAIME (w/o extended thinking: Opus 4 scores 33.9% and Sonnet 4 is 33.1%)\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch4\u003eTAU-bench methodology\u003c/h4\u003e\u003cp\u003eScores were achieved with a prompt addendum to both the Airline and Retail Agent Policy instructing Claude to better leverage its reasoning abilities while using extended thinking with tool use. The model is encouraged to write down its thoughts as it solves the problem distinct from our usual thinking mode, during the multi-turn trajectories to best leverage its reasoning abilities. To accommodate the additional steps Claude incurs by utilizing more thinking, the maximum number of steps (counted by model completions) was increased from 30 to 100 (most trajectories completed under 30 steps with only one trajectory reaching above 50 steps).\u003c/p\u003e\u003ch4\u003eSWE-bench methodology\u003c/h4\u003e\u003cp\u003eFor the Claude 4 family of models, we continue to use the same simple scaffold that equips the model with solely the two tools described in our prior releases \u003ca href=\"https://www.anthropic.com/engineering/swe-bench-sonnet\"\u003ehere\u003c/a\u003e—a bash tool, and a file editing tool that operates via string replacements. We no longer include the \u003ca href=\"https://www.anthropic.com/engineering/claude-think-tool\"\u003ethird ‘planning tool’\u003c/a\u003e used by Claude 3.7 Sonnet. On all Claude 4 models, we report scores out of the full 500 problems. Scores for OpenAI models are reported out of a \u003ca href=\"https://openai.com/index/gpt-4-1/\"\u003e477 problem subset\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eFor our “high compute” numbers we adopt additional complexity and parallel test-time compute as follows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe sample multiple parallel attempts.\u003c/li\u003e\u003cli\u003eWe discard patches that break the visible regression tests in the repository, similar to the rejection sampling approach adopted by \u003ca href=\"https://arxiv.org/abs/2407.01489\"\u003eAgentless (Xia et al. 2024)\u003c/a\u003e; note no hidden test information is used.\u003c/li\u003e\u003cli\u003eWe then use an internal scoring model to select the best candidate from the remaining attempts.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis results in a score of 79.4% and 80.2% for Opus 4 and Sonnet 4 respectively.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": null,
  "modifiedTime": null
}
