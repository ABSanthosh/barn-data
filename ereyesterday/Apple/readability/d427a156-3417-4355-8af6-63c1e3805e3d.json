{
  "id": "d427a156-3417-4355-8af6-63c1e3805e3d",
  "title": "Notes on Early Mac Studio AI Benchmarks with Qwen3-235B-A22B and Qwen2.5-VL-72B",
  "link": "https://www.macstories.net/notes/notes-on-early-mac-studio-ai-benchmarks-with-qwen3-235b-a22b-and-qwen2-5-vl-72b/",
  "description": "I received a top-of-the-line Mac Studio (M3 Ultra, 512 GB of RAM, 8 TB of storage) on loan from Apple last week, and I thought I’d use this opportunity to revive something I’ve been mulling over for some time: more short-form blogging on MacStories in the form of brief “notes” with a dedicated Notes category on the […]",
  "author": "Federico Viticci",
  "published": "Tue, 20 May 2025 18:14:37 +0000",
  "source": "https://www.macstories.net/feed",
  "categories": [
    "notes",
    "AI",
    "artificial intelligence",
    "LLMs",
    "M3 Ultra",
    "mac studio",
    "Qwen"
  ],
  "byline": "Federico Viticci",
  "length": 5199,
  "excerpt": "I received a top-of-the-line Mac Studio (M3 Ultra, 512 GB of RAM, 8 TB of storage) on loan from Apple last week, and I thought I’d use this opportunity to revive something I’ve been mulling over for some time: more short-form blogging on MacStories in the form of brief “notes” with a dedicated Notes category on the",
  "siteName": "",
  "favicon": "https://www.macstories.net/app/themes/macstories4/images/apple-touch-icon-152x152-precomposed.png",
  "text": "I received a top-of-the-line Mac Studio (M3 Ultra, 512 GB of RAM, 8 TB of storage) on loan from Apple last week, and I thought I’d use this opportunity to revive something I’ve been mulling over for some time: more short-form blogging on MacStories in the form of brief “notes” with a dedicated Notes category on the site. Expect more of these “low-pressure”, quick posts in the future. I’ve been sent this Mac Studio as part of my ongoing experiments with assistive AI and automation, and one of the things I plan to do over the coming weeks and months is playing around with local LLMs that tap into the power of Apple Silicon and the incredible performance headroom afforded by the M3 Ultra and this computer’s specs. I have a lot to learn when it comes to local AI (my shortcuts and experiments so far have focused on cloud models and the Shortcuts app combined with the LLM CLI), but since I had to start somewhere, I downloaded LM Studio and Ollama, installed the llm-ollama plugin, and began experimenting with open-weights models (served from Hugging Face as well as the Ollama library) both in the GGUF format and Apple’s own MLX framework. I posted some of these early tests on Bluesky. I ran the massive Qwen3-235B-A22B model (a Mixture-of-Experts model with 235 billion parameters, 22 billion of which activated at once) with both GGUF and MLX using the beta version of the LM Studio app, and these were the results: GGUF: 16 tokens/second, ~133 GB of RAM used MLX: 24 tok/sec, ~124 GB RAM As you can see from these first benchmarks (both based on the 4-bit quant of Qwen3-235B-A22B), the Apple Silicon-optimized version of the model resulted in better performance both for token generation and memory usage. Regardless of the version, the Mac Studio absolutely didn’t care and I could barely hear the fans going. I also wanted to play around with the new generation of vision models (VLMs) to test modern OCR capabilities of these models. One of the tasks that has become kind of a personal AI eval for me lately is taking a long screenshot of a shortcut from the Shortcuts app (using CleanShot’s scrolling captures) and feed it either as a full-res PNG or PDF to an LLM. As I shared before, due to image compression, the vast majority of cloud LLMs either fail to accept the image as input or compresses the image so much that graphical artifacts lead to severe hallucinations in the text analysis of the image. Only o4-mini-high – thanks to its more agentic capabilities and tool-calling – was able to produce a decent output; even then, that was only possible because o4-mini-high decided to slice the image in multiple parts and iterate through each one with discrete pytesseract calls. The task took almost seven minutes to run in ChatGPT. This morning, I installed the 72-billion parameter version of Qwen2.5-VL, gave it a full-resolution screenshot of a 40-action shortcut, and let it run with Ollama and llm-ollama. After 3.5 minutes and around 100 GB RAM usage, I got a really good, Markdown-formatted analysis of my shortcut back from the model. To make the experience nicer, I even built a small local-scanning utility that lets me pick an image from Shortcuts and runs it through Qwen2.5-VL (72B) using the ‘Run Shell Script’ action on macOS. It worked beautifully on my first try. Amusingly, the smaller version of Qwen2.5-VL (32B) thought my photo of ergonomic mice was a “collection of seashells”. Fair enough: there’s a reason bigger models are heavier and costlier to run. Given my struggles with OCR and document analysis with cloud-hosted models, I’m very excited about the potential of local VLMs that bypass memory constraints thanks to the M3 Ultra and provide accurate results in just a few minutes without having to upload private images or PDFs anywhere. I’ve been writing a lot about this idea of “hybrid automation” that combines traditional Mac scripting tools, Shortcuts, and LLMs to unlock workflows that just weren’t possible before; I feel like the power of this Mac Studio is going to be an amazing accelerator for that. Next up on my list: understanding how to run MLX models with mlx-lm, investigating long-context models with dual-chunk attention support (looking at you, Qwen 2.5), and experimenting with Gemma 3. Fun times ahead! Access Extra Content and PerksFounded in 2015, Club MacStories has delivered exclusive content every week for nearly a decade. What started with weekly and monthly email newsletters has blossomed into a family of memberships designed every MacStories fan. Learn more here and from our Club FAQs. Club MacStories: Weekly and monthly newsletters via email and the web that are brimming with apps, tips, automation workflows, longform writing, early access to the MacStories Unwind podcast, periodic giveaways, and more; Club MacStories+: Everything that Club MacStories offers, plus an active Discord community, advanced search and custom RSS features for exploring the Club’s entire back catalog, bonus columns, and dozens of app discounts; Club Premier: All of the above and AppStories+, an extended version of our flagship podcast that’s delivered early, ad-free, and in high-bitrate audio.",
  "image": "https://cdn.macstories.net/cleanshot-2025-05-20-at-19-55-02-2x-1747763721300.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                        \u003cp id=\"p1\"\u003eI \u003ca href=\"https://bsky.app/profile/viticci.macstories.net/post/3lp7m6i5tls25\" rel=\"noopener noreferrer\"\u003ereceived\u003c/a\u003e a top-of-the-line Mac Studio (M3 Ultra, 512 GB of RAM, 8 TB of storage) on loan from Apple last week, and I thought I’d use this opportunity to revive something I’ve been mulling over for some time: more short-form blogging on MacStories in the form of brief “notes” with a dedicated \u003ca href=\"https://www.macstories.net/category/notes/\" rel=\"noopener noreferrer\"\u003eNotes\u003c/a\u003e category on the site. Expect more of these “low-pressure”, quick posts in the future.\u003c/p\u003e\n\u003cp id=\"p2\"\u003eI’ve been sent this Mac Studio as part of my \u003ca href=\"https://club.macstories.net/posts/automation-academy-how-i-built-a-smarter-web-clipper-for-obsidian-using-shortcuts-and-ai\" rel=\"noopener noreferrer\"\u003eongoing experiments\u003c/a\u003e with assistive AI and automation, and one of the things I plan to do over the coming weeks and months is playing around with local LLMs that tap into the power of Apple Silicon and the incredible performance headroom afforded by the M3 Ultra and this computer’s specs. I have \u003cem\u003ea lot\u003c/em\u003e to learn when it comes to local AI (my shortcuts and experiments so far have focused on cloud models and the Shortcuts app combined with the \u003ca href=\"https://llm.datasette.io/en/stable/\" rel=\"noopener noreferrer\"\u003eLLM CLI\u003c/a\u003e), but since I had to start somewhere, I downloaded \u003ca href=\"https://lmstudio.ai\" rel=\"noopener noreferrer\"\u003eLM Studio\u003c/a\u003e and \u003ca href=\"https://ollama.com\" rel=\"noopener noreferrer\"\u003eOllama\u003c/a\u003e, installed the \u003ca href=\"https://github.com/taketwo/llm-ollama\" rel=\"noopener noreferrer\"\u003ellm-ollama\u003c/a\u003e plugin, and began experimenting with open-weights models (served from \u003ca href=\"https://huggingface.co\" rel=\"noopener noreferrer\"\u003eHugging Face\u003c/a\u003e as well as the Ollama \u003ca href=\"https://ollama.com/library\" rel=\"noopener noreferrer\"\u003elibrary\u003c/a\u003e) both in the \u003ca href=\"https://huggingface.co/docs/hub/en/gguf\" rel=\"noopener noreferrer\"\u003eGGUF\u003c/a\u003e format and Apple’s own \u003ca href=\"https://huggingface.co/docs/hub/en/mlx\" rel=\"noopener noreferrer\"\u003eMLX\u003c/a\u003e framework.\u003c/p\u003e\n\n\u003cp id=\"p4\"\u003eI posted some of these early tests on \u003ca href=\"https://bsky.app/profile/viticci.macstories.net/post/3lpa6s2ov3s25\" rel=\"noopener noreferrer\"\u003eBluesky\u003c/a\u003e. I ran the massive \u003ca href=\"https://qwenlm.github.io/blog/qwen3/\" rel=\"noopener noreferrer\"\u003eQwen3-235B-A22B\u003c/a\u003e model (a \u003ca href=\"https://en.wikipedia.org/wiki/Mixture_of_experts\" rel=\"noopener noreferrer\"\u003eMixture-of-Experts\u003c/a\u003e model with 235 billion parameters, 22 billion of which activated at once) with both \u003ca href=\"https://huggingface.co/models?search=Qwen3-235B-A22B\" rel=\"noopener noreferrer\"\u003eGGUF and MLX\u003c/a\u003e using the beta version of the LM Studio app, and these were the results:\u003c/p\u003e\n\u003cul id=\"ul5\"\u003e\u003cli\u003eGGUF: 16 tokens/second, ~133 GB of RAM used\u003c/li\u003e\n\u003cli\u003eMLX: 24 tok/sec, ~124 GB RAM\u003c/li\u003e\n\u003c/ul\u003e\u003cp id=\"p6\"\u003eAs you can see from these first benchmarks (both based on the \u003ca href=\"https://huggingface.co/blog/4bit-transformers-bitsandbytes\" rel=\"noopener noreferrer\"\u003e4-bit quant\u003c/a\u003e of Qwen3-235B-A22B), the Apple Silicon-optimized version of the model resulted in better performance both for token generation and memory usage. Regardless of the version, the Mac Studio absolutely didn’t care and I could barely hear the fans going.\u003c/p\u003e\n\u003cp id=\"p7\"\u003eI also wanted to play around with the \u003ca href=\"https://huggingface.co/blog/vlms-2025\" rel=\"noopener noreferrer\"\u003enew generation of vision models\u003c/a\u003e (VLMs) to test modern OCR capabilities of these models. One of the tasks that has become kind of a personal AI \u003ca href=\"https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals\" rel=\"noopener noreferrer\"\u003eeval\u003c/a\u003e for me lately is taking a long screenshot of a shortcut from the Shortcuts app (using \u003ca href=\"https://cleanshot.com\" rel=\"noopener noreferrer\"\u003eCleanShot\u003c/a\u003e’s scrolling captures) and feed it either as a full-res PNG or PDF to an LLM. As I shared before, due to image compression, the vast majority of cloud LLMs either fail to accept the image as input or compresses the image \u003cem\u003eso much\u003c/em\u003e that graphical artifacts lead to severe hallucinations in the text analysis of the image. Only o4-mini-high – thanks to its more agentic capabilities and tool-calling – \u003ca href=\"https://bsky.app/profile/viticci.macstories.net/post/3lmxsf3t3cc2o\" rel=\"noopener noreferrer\"\u003ewas able to produce\u003c/a\u003e a decent output; even then, that was only possible because o4-mini-high decided to slice the image in multiple parts and iterate through each one with discrete \u003ca href=\"https://pypi.org/project/pytesseract/\" rel=\"noopener noreferrer\"\u003epytesseract\u003c/a\u003e calls. The task took almost seven minutes to run in ChatGPT.\u003c/p\u003e\n\u003cp id=\"p8\"\u003eThis morning, I installed the 72-billion parameter version of \u003ca href=\"https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5\" rel=\"noopener noreferrer\"\u003eQwen2.5-VL\u003c/a\u003e, gave it a full-resolution screenshot of a 40-action shortcut, and let it run with Ollama and llm-ollama. After 3.5 minutes and around 100 GB RAM usage, I got a really good, Markdown-formatted analysis of my shortcut back from the model.\u003c/p\u003e\n\n\u003cp id=\"p10\"\u003eTo make the experience nicer, I even \u003ca href=\"https://bsky.app/profile/viticci.macstories.net/post/3lpm4hhyqz224\" rel=\"noopener noreferrer\"\u003ebuilt\u003c/a\u003e a small local-scanning utility that lets me pick an image from Shortcuts and runs it through Qwen2.5-VL (72B) using the ‘Run Shell Script’ action on macOS. It worked beautifully on my first try. Amusingly, the smaller version of Qwen2.5-VL (32B) thought my photo of \u003ca href=\"https://www.protoarc.com/collections/mice/products/em11-nl-vertical-mouse?variant=42318201716825\" rel=\"noopener noreferrer\"\u003eergonomic mice\u003c/a\u003e was a “collection of seashells”. Fair enough: there’s a reason bigger models are heavier and costlier to run.\u003c/p\u003e\n\u003cp id=\"p11\"\u003eGiven my struggles with OCR and document analysis with cloud-hosted models, I’m \u003cem\u003every\u003c/em\u003e excited about the potential of local VLMs that bypass memory constraints thanks to the M3 Ultra and provide accurate results in just a few minutes without having to upload private images or PDFs anywhere. I’ve been \u003ca href=\"https://club.macstories.net/posts/automation-academy-how-i-built-a-smarter-web-clipper-for-obsidian-using-shortcuts-and-ai\" rel=\"noopener noreferrer\"\u003ewriting\u003c/a\u003e a lot about this idea of “hybrid automation” that combines traditional Mac scripting tools, Shortcuts, and LLMs to unlock workflows that just weren’t possible before; I feel like the power of this Mac Studio is going to be an amazing accelerator for that.\u003c/p\u003e\n\u003cp id=\"p12\"\u003eNext up on my list: understanding how to run MLX models with \u003ca href=\"https://github.com/ml-explore/mlx-lm\" rel=\"noopener noreferrer\"\u003emlx-lm\u003c/a\u003e, investigating long-context models with dual-chunk attention support (looking at you, \u003ca href=\"https://qwenlm.github.io/blog/qwen2.5-1m/\" rel=\"noopener noreferrer\"\u003eQwen 2.5\u003c/a\u003e), and experimenting with \u003ca href=\"https://blog.google/technology/developers/gemma-3/\" rel=\"noopener noreferrer\"\u003eGemma 3\u003c/a\u003e. Fun times ahead!\u003c/p\u003e\n            \u003c/div\u003e\u003cdiv\u003e\n            \u003cdiv\u003e\u003cp\u003e\u003cimg src=\"https://www.macstories.net/app/themes/macstories4/images/logo-shape-gold.svg\" alt=\"Club MacStories\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch3\u003eAccess Extra Content and Perks\u003c/h3\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003eFounded in 2015, \u003ca href=\"https://club.macstories.net/plans?utm_source=ms\u0026amp;utm_medium=web-inline\" rel=\"noopener noreferrer\"\u003eClub MacStories\u003c/a\u003e has delivered exclusive content every week for nearly a decade.\u003c/p\u003e\n\u003cp\u003eWhat started with weekly and monthly email newsletters has blossomed into \u003ca href=\"https://club.macstories.net/plans?utm_source=ms\u0026amp;utm_medium=web-inline\" rel=\"noopener noreferrer\"\u003ea family of memberships\u003c/a\u003e designed every MacStories fan.\u003c/p\u003e\n\u003cp\u003eLearn more \u003ca href=\"https://club.macstories.net/plans?utm_source=ms\u0026amp;utm_medium=web-inline\" rel=\"noopener noreferrer\"\u003ehere\u003c/a\u003e and from our \u003ca href=\"https://club.macstories.net/faq\" rel=\"noopener noreferrer\"\u003eClub FAQs\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e\u003cdiv\u003e\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://club.macstories.net/plans/club\" rel=\"noopener noreferrer\"\u003eClub MacStories\u003c/a\u003e\u003c/strong\u003e: Weekly and monthly newsletters via email and the web that are brimming with apps, tips, automation workflows, longform writing, early access to the \u003ca href=\"https://www.macstories.net/unwind/\" rel=\"noopener noreferrer\"\u003eMacStories Unwind podcast\u003c/a\u003e, periodic giveaways, and more;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://club.macstories.net/plans/plus\" rel=\"noopener noreferrer\"\u003eClub MacStories+\u003c/a\u003e\u003c/strong\u003e: Everything that Club MacStories offers, plus an active Discord community, advanced search and custom RSS features for exploring the Club’s entire back catalog, bonus columns, and dozens of app discounts;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://club.macstories.net/plans/premier\" rel=\"noopener noreferrer\"\u003eClub Premier\u003c/a\u003e\u003c/strong\u003e: All of the above \u003cem\u003eand\u003c/em\u003e AppStories+, an extended version of our flagship podcast that’s delivered early, ad-free, and in high-bitrate audio.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e        \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-05-20T18:14:37-04:00",
  "modifiedTime": null
}
