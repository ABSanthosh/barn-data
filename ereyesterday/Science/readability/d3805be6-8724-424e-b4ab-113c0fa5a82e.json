{
  "id": "d3805be6-8724-424e-b4ab-113c0fa5a82e",
  "title": "Brain implant translates thoughts to speech in an instant",
  "link": "https://www.nature.com/articles/d41586-025-01001-6",
  "description": "",
  "author": "Miryam Naddaf",
  "published": "2025-03-31",
  "source": "https://www.nature.com/nature.rss",
  "categories": null,
  "byline": "Naddaf, Miryam",
  "length": 4836,
  "excerpt": "Improvements to brain–computer interfaces are bringing the technology closer to natural conversation speed. Improvements to brain–computer interfaces are bringing the technology closer to natural conversation speed.",
  "siteName": "",
  "favicon": "https://www.nature.com/static/images/favicons/nature/apple-touch-icon-f39cb19454.png",
  "text": "NEWS 31 March 2025 Improvements to brain–computer interfaces are bringing the technology closer to natural conversation speed. You have full access to this article via your institution. This scan of the brain shows activity in the speech cortex — a part of the frontal lobe involved in speech production.Credit: Montreal Neurological Institute/Science Photo LibraryA brain-reading implant that translates neural signals into audible speech has allowed a woman with paralysis to hear what she intends to say nearly instantly.Researchers enhanced the device — known as a brain–computer interface (BCI) — with artificial intelligence (AI) algorithms that decoded sentences as the woman thought of them, and then spoke them out loud using a synthetic voice. Unlike previous efforts, which could produce sounds only after users finished an entire sentence, the current approach can simultaneously detect words and turn them into speech within three seconds.Brain-reading devices allow paralysed people to talk using their thoughtsThe findings, published in Nature Neuroscience on 31 March1, represent a big step towards BCIs that are of practical use.Older speech-generating BCIs are similar to “a WhatsApp conversation”, says Christian Herff, a computational neuroscientist at Maastricht University, the Netherlands, who was not involved with the work. “I write a sentence, you write a sentence and you need some time to write a sentence again… It just doesn’t flow like a normal conversation.”BCIs that stream speech in real time are “the next level” in research because they allow users to convey the tone and emphasis that are characteristic of natural speech, he adds.Brain-signal readerThe study participant, Ann, lost her ability to speak after a stroke in her brainstem in 2005. Some 18 years later, she underwent a surgery to place a paper-thin rectangle containing 253 electrodes on the surface on her brain cortex. The implant can record the combined activity of thousands of neurons at the same time.Researchers personalized the synthetic voice to sound like Ann’s own voice from before her injury, by training AI algorithms on recordings from her wedding video. doi: https://doi.org/10.1038/d41586-025-01001-6 ReferencesLittlejohn, K. T. et al. Nature Neurosci. https://doi.org/10.1038/s41593-025-01905-6 (2025).Article  Google Scholar  Metzger, S. L. et al. Nature 620, 1037–1046 (2023).Article  PubMed  Google Scholar  Stuart, A., Kalinowski, J., Rastatter, M. P. \u0026 Lynch, K. J. Acoust. Soc. Am. 111, 2237–2241 (2002).Article  Google Scholar  Download references Related Articles Brain-reading devices allow paralysed people to talk using their thoughts Brain-reading device is best yet at decoding ‘internal speech’ The brain-reading devices helping paralysed people to move, talk and touch Mind-reading devices are revealing the brain’s secrets First paralysed person to be 'reanimated' offers neuroscience insights Subjects Latest on:",
  "image": "https://media.nature.com/lw1200/magazine-assets/d41586-025-01001-6/d41586-025-01001-6_50811928.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-container-type=\"article\" data-component=\"article-container\" id=\"content\" role=\"main\" aria-label=\"main page content\"\u003e\n                    \u003carticle data-track-component=\"news\" lang=\"en\"\u003e\n                        \n\u003cdiv\u003e\n    \u003cheader\u003e\n        \u003cdiv\u003e\n            \u003cul data-test=\"article-identifier\"\u003e\n                \u003cli data-test=\"article-category\"\u003e\u003cspan\u003eNEWS\u003c/span\u003e\u003c/li\u003e\n                \u003cli\u003e\u003ctime datetime=\"2025-03-31\"\u003e31 March 2025\u003c/time\u003e\u003c/li\u003e\n                \n            \u003c/ul\u003e\n\n            \n\n            \u003cdiv\u003e\n                \n                \u003cp\u003e\n                    Improvements to brain–computer interfaces are bringing the technology closer to natural conversation speed.\n                \u003c/p\u003e\n            \u003c/div\u003e\n        \u003c/div\u003e\n        \n            \n        \n    \u003c/header\u003e\n    \n\u003c/div\u003e\n\n            \n            \n\n\n        \n            \n                \n                    \n                        \u003cdiv data-track-context=\"article body\"\u003e\n                                \n        \u003cp data-test=\"access-message\"\u003e\n                You have full access to this article via your institution.\u003c/p\u003e\n    \n\n                        \u003c/div\u003e\n                    \n                \n            \n\n            \n                \n                \u003cdiv data-test=\"access-teaser\"\u003e \u003cfigure\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://media.nature.com/lw767/magazine-assets/d41586-025-01001-6/d41586-025-01001-6_50811924.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-025-01001-6/d41586-025-01001-6_50811924.jpg?as=webp 319w\" sizes=\"(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px\"/\u003e\u003cimg alt=\"Coloured PET scan of a human brain during a speech exercise.\" loading=\"lazy\" src=\"https://media.nature.com/lw767/magazine-assets/d41586-025-01001-6/d41586-025-01001-6_50811924.jpg\"/\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eThis scan of the brain shows activity in the speech cortex — a part of the frontal lobe involved in speech production.\u003c/span\u003e\u003cspan\u003eCredit: Montreal Neurological Institute/Science Photo Library\u003c/span\u003e\u003c/p\u003e\u003c/figcaption\u003e\u003c/picture\u003e\u003c/figure\u003e\u003cp\u003eA brain-reading implant that translates neural signals into audible speech has allowed a woman with paralysis to hear what she intends to say nearly instantly.\u003c/p\u003e\u003cp\u003eResearchers enhanced the device — known as a brain–computer interface (BCI) — with artificial intelligence (AI) algorithms that decoded sentences as the woman thought of them, and then spoke them out loud using a synthetic voice. Unlike \u003ca href=\"https://www.nature.com/articles/d41586-023-02682-7\" data-track=\"click\" data-label=\"https://www.nature.com/articles/d41586-023-02682-7\" data-track-category=\"body text link\"\u003eprevious efforts\u003c/a\u003e, which could produce sounds only after users finished an entire sentence, the current approach can simultaneously detect words and turn them into speech within three seconds.\u003c/p\u003e\u003carticle data-label=\"Related\"\u003e\u003ca href=\"https://www.nature.com/articles/d41586-023-02682-7\" data-track=\"click\" data-track-label=\"recommended article\"\u003e\u003cimg alt=\"\" src=\"https://media.nature.com/w400/magazine-assets/d41586-025-01001-6/d41586-025-01001-6_26507088.jpg\"/\u003e\u003cp\u003eBrain-reading devices allow paralysed people to talk using their thoughts\u003c/p\u003e\u003c/a\u003e\u003c/article\u003e\u003cp\u003eThe findings, published in \u003ci\u003eNature\u003c/i\u003e \u003ci\u003eNeuroscience \u003c/i\u003eon 31 March\u003csup\u003e\u003ca href=\"#ref-CR1\" data-track=\"click\" data-action=\"anchor-link\" data-track-label=\"go to reference\" data-track-category=\"references\"\u003e1\u003c/a\u003e\u003c/sup\u003e, represent a big step towards BCIs that are of practical use.\u003c/p\u003e\u003cp\u003eOlder speech-generating BCIs are similar to “a WhatsApp conversation”, says Christian Herff, a computational neuroscientist at Maastricht University, the Netherlands, who was not involved with the work. “I write a sentence, you write a sentence and you need some time to write a sentence again… It just doesn’t flow like a normal conversation.”\u003c/p\u003e\u003cp\u003eBCIs that stream speech in real time are “the next level” in research because they allow users to convey the tone and emphasis that are characteristic of natural speech, he adds.\u003c/p\u003e\u003ch2\u003eBrain-signal reader\u003c/h2\u003e\u003cp\u003eThe study participant, Ann, lost her ability to speak after a stroke in her brainstem in 2005. Some 18 years later, she underwent a surgery to place a paper-thin rectangle containing 253 electrodes on the surface on her brain cortex. The implant can record the combined activity of thousands of neurons at the same time.\u003c/p\u003e\u003cp\u003eResearchers personalized the synthetic voice to sound like Ann’s own voice from before her injury, by training AI algorithms on recordings from her wedding video.\u003c/p\u003e\u003c/div\u003e\n            \n                \u003cp\u003e\u003cem\u003edoi: https://doi.org/10.1038/d41586-025-01001-6\u003c/em\u003e\u003c/p\u003e\n\n            \u003cdiv id=\"references\" aria-labelledby=\"Bib1\"\u003e\u003ch2 id=\"Bib1\"\u003eReferences\u003c/h2\u003e\u003cdiv data-container-section=\"references\" id=\"Bib1-content\"\u003e\u003col data-track-component=\"outbound reference\" data-track-context=\"references section\"\u003e\u003cli data-counter=\"1.\"\u003e\u003cp id=\"ref-CR1\"\u003eLittlejohn, K. T. \u003ci\u003eet al.\u003c/i\u003e \u003ci\u003eNature Neurosci.\u003c/i\u003e \u003ca href=\"https://doi.org/10.1038/s41593-025-01905-6\" data-track=\"click\" data-track-action=\"external reference\" data-track-label=\"https://doi.org/10.1038/s41593-025-01905-6\"\u003ehttps://doi.org/10.1038/s41593-025-01905-6\u003c/a\u003e (2025).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41593-025-01905-6\" data-track-item_id=\"10.1038/s41593-025-01905-6\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41593-025-01905-6\" aria-label=\"Article reference 1\" data-doi=\"10.1038/s41593-025-01905-6\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 1\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=\u0026amp;journal=Nature%20Neurosci.\u0026amp;doi=10.1038%2Fs41593-025-01905-6\u0026amp;publication_year=2025\u0026amp;author=Littlejohn%2CK.T.\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"2.\"\u003e\u003cp id=\"ref-CR2\"\u003eMetzger, S. L. \u003ci\u003eet al.\u003c/i\u003e \u003ci\u003eNature\u003c/i\u003e \u003cb\u003e620\u003c/b\u003e, 1037–1046 (2023).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41586-023-06443-4\" data-track-item_id=\"10.1038/s41586-023-06443-4\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41586-023-06443-4\" aria-label=\"Article reference 2\" data-doi=\"10.1038/s41586-023-06443-4\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=37612505\" aria-label=\"PubMed reference 2\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 2\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=\u0026amp;journal=Nature\u0026amp;doi=10.1038%2Fs41586-023-06443-4\u0026amp;volume=620\u0026amp;pages=1037-1046\u0026amp;publication_year=2023\u0026amp;author=Metzger%2CS.%20L.\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"3.\"\u003e\u003cp id=\"ref-CR3\"\u003eStuart, A., Kalinowski, J., Rastatter, M. P. \u0026amp; Lynch, K. J. \u003ci\u003eAcoust. Soc. Am.\u003c/i\u003e \u003cb\u003e111\u003c/b\u003e, 2237–2241 (2002).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1121/1.1466868\" data-track-item_id=\"10.1121/1.1466868\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1121%2F1.1466868\" aria-label=\"Article reference 3\" data-doi=\"10.1121/1.1466868\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 3\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=\u0026amp;journal=Acoust.%20Soc.%20Am.\u0026amp;doi=10.1121%2F1.1466868\u0026amp;volume=111\u0026amp;pages=2237-2241\u0026amp;publication_year=2002\u0026amp;author=Stuart%2CA.\u0026amp;author=Kalinowski%2CJ.\u0026amp;author=Rastatter%2CM.%20P.\u0026amp;author=Lynch%2CK.%20J.\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003ca data-track=\"click\" data-track-action=\"download citation references\" data-track-label=\"link\" rel=\"nofollow\" href=\"https://citation-needed.springer.com/v2/references/10.1038/d41586-025-01001-6?format=refman\u0026amp;flavour=references\"\u003eDownload references\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\n            \n                \n            \n\n            \n                \u003ch2 id=\"related-articles\"\u003eRelated Articles\u003c/h2\u003e\n                \n    \u003cul\u003e\n        \n            \u003cli\u003e\n                \u003cp\u003e\n                    \u003ca href=\"https://www.nature.com/articles/d41586-023-02682-7\" data-track=\"click\" data-track-label=\"related article (rank:0)\"\u003e\n                        \n                            \u003cimg loading=\"lazy\" src=\"https://media.nature.com/lw100/magazine-assets/d41586-025-01001-6/d41586-025-01001-6_26507088.jpg\" alt=\"\"/\u003e\n                        \n                        Brain-reading devices allow paralysed people to talk using their thoughts\n                    \u003c/a\u003e\n                \u003c/p\u003e\n            \u003c/li\u003e\n        \n            \u003cli\u003e\n                \u003cp\u003e\n                    \u003ca href=\"https://www.nature.com/articles/d41586-024-01424-7\" data-track=\"click\" data-track-label=\"related article (rank:1)\"\u003e\n                        \n                            \u003cimg loading=\"lazy\" src=\"https://media.nature.com/lw100/magazine-assets/d41586-025-01001-6/d41586-025-01001-6_27109432.jpg\" alt=\"\"/\u003e\n                        \n                        Brain-reading device is best yet at decoding ‘internal speech’\n                    \u003c/a\u003e\n                \u003c/p\u003e\n            \u003c/li\u003e\n        \n            \u003cli\u003e\n                \u003cp\u003e\n                    \u003ca href=\"https://www.nature.com/articles/d41586-022-01047-w\" data-track=\"click\" data-track-label=\"related article (rank:2)\"\u003e\n                        \n                            \u003cimg loading=\"lazy\" src=\"https://media.nature.com/lw100/magazine-assets/d41586-025-01001-6/d41586-025-01001-6_20343922.jpg\" alt=\"\"/\u003e\n                        \n                        The brain-reading devices helping paralysed people to move, talk and touch\n                    \u003c/a\u003e\n                \u003c/p\u003e\n            \u003c/li\u003e\n        \n            \u003cli\u003e\n                \u003cp\u003e\n                    \u003ca href=\"https://www.nature.com/articles/d41586-024-00481-2\" data-track=\"click\" data-track-label=\"related article (rank:3)\"\u003e\n                        \n                            \u003cimg loading=\"lazy\" src=\"https://media.nature.com/lw100/magazine-assets/d41586-025-01001-6/d41586-025-01001-6_26759616.jpg\" alt=\"\"/\u003e\n                        \n                        Mind-reading devices are revealing the brain’s secrets\n                    \u003c/a\u003e\n                \u003c/p\u003e\n            \u003c/li\u003e\n        \n            \u003cli\u003e\n                \u003cp\u003e\n                    \u003ca href=\"https://www.nature.com/news/first-paralysed-person-to-be-reanimated-offers-neuroscience-insights-1.19749\" data-track=\"click\" data-track-label=\"related article (rank:4)\"\u003e\n                        \n                        First paralysed person to be \u0026#39;reanimated\u0026#39; offers neuroscience insights\n                    \u003c/a\u003e\n                \u003c/p\u003e\n            \u003c/li\u003e\n        \n    \u003c/ul\u003e\n\n            \n\n            \n                \u003ch2 id=\"subjects\"\u003eSubjects\u003c/h2\u003e\n                \n    \n\n            \u003csection data-component-latest-content=\"true\" data-track=\"in-view\" data-track-action=\"in-view\" data-track-category=\"latest content\" data-track-label=\"visible\"\u003e\n    \u003ch2 id=\"tablist-heading\"\u003eLatest on:\u003c/h2\u003e\n    \n\u003c/section\u003e\n\n\n\n\n            \n\n        \n            \n                    \u003c/article\u003e\n                \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
