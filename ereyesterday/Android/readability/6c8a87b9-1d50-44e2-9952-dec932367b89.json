{
  "id": "6c8a87b9-1d50-44e2-9952-dec932367b89",
  "title": "Meta’s breakthrough wants to let you control AR glasses just by moving your fingers",
  "link": "https://www.androidcentral.com/apps-software/meta/metas-breakthrough-wants-to-let-you-control-ar-glasses-just-by-moving-your-fingers",
  "description": "Meta’s Reality Labs just published a paper showing how wrist-based sEMG tech could reshape how we interact with computers.",
  "author": "",
  "published": "Thu, 24 Jul 2025 19:27:45 +0000",
  "source": "http://feeds.androidcentral.com/androidcentral",
  "categories": [
    "Meta",
    "Apps \u0026amp; Software"
  ],
  "byline": "Jay Bonggolto",
  "length": 3169,
  "excerpt": "It's all about AR that reads your mind via a wristband that knows what your hands are up to.",
  "siteName": "Android Central",
  "favicon": "https://cdn.mos.cms.futurecdn.net/flexiimages/m3uyybbdos1632303243.png",
  "text": "(Image credit: Michael Hicks / Android Central) What you need to knowMeta is experimenting with a wristband that reads your muscle signals, so you can type or control stuff just by thinking about moving your fingers.The tech pairs with Meta’s Orion AR glasses, which look like regular specs but overlay digital stuff in your real-world view.It’s still a prototype (and pricey), but Meta says a more practical version is in the works.Imagine scrolling through your social media feed, sending a message, or skipping a song without tapping a button or resorting to voice commands, just a subtle twitch of your hand. That’s the wild promise of Meta’s latest research, and it’s not some distant sci-fi dream.Thanks to a new study published in Nature, we’re getting a sneak peek at how augmented reality (AR) glasses could soon read your intentions before you even fully act on them.Meta’s Reality Labs has been quietly working on a tech called surface electromyography (sEMG), which sounds complicated but boils down to one simple idea: detecting the tiny electrical signals your muscles send when you even think about moving.How it works: wristwear that reads intention(Image credit: Meta)Instead of waving your arms around or using a bulky controller, you slip on a lightweight armband. Trained on data from thousands of people, the system deciphers minute electrical patterns that the brain sends to your fingers.One test user typed at over 20 words per minute just thinking the stroke motions. The team smashed the key hurdles: generalization across users (so it doesn’t need per‑person calibration), consistent gesture recognition, and handwriting decoding, all embedded in a wristband-style interface.The big deal here is speed and subtlety. Current AR controls rely on voice, hand tracking (needs big gestures), or clunky remotes. In its Nature paper, Meta showed off Orion, a prototype wristband that predicted gestures with scary accuracy, even when people barely moved. It worked while typing on a keyboard or holding a coffee without needing to pause your life to interact with tech.Meta sees this fusion of AI, sEMG wristband, and AR glasses as the natural next step beyond smartphones.Get the latest news from Android Central, your trusted companion in the world of AndroidWhy this mattersThe tech is a big step forward for interacting with devices, especially for people with limited mobility since it doesn’t require any physical movement to trigger input. It also makes using tech feel more natural and less effortful.That said, it’s still early days. Orion glasses reportedly cost around $10,000 per unit and aren’t available to the public yet. The wristband, while extremely promising, is still in prototype phase with no clear timeline for a commercial launch.But Meta says it's working toward more affordable, consumer-ready versions of both, and the progress so far suggests it's serious. Jay Bonggolto always keeps a nose for news. He has been writing about consumer tech and apps for as long as he can remember, and he has used a variety of Android phones since falling in love with Jelly Bean. Send him a direct message via Twitter or LinkedIn.",
  "image": "https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-widget-type=\"contentparsed\" id=\"content\"\u003e\n\u003csection\u003e\n\u003cdiv\u003e\n\u003cdiv\u003e\n\u003cpicture data-new-v2-image=\"true\"\u003e\n\u003csource type=\"image/webp\" srcset=\"https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-1920-80.jpeg.webp 1920w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-1200-80.jpeg.webp 1200w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-1024-80.jpeg.webp 1024w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-970-80.jpeg.webp 970w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-650-80.jpeg.webp 650w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-480-80.jpeg.webp 480w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-320-80.jpeg.webp 320w\" sizes=\"(min-width: 1000px) 600px, calc(100vw - 40px)\"/\u003e\n\u003cimg src=\"https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a.jpeg\" alt=\"A close-up of the Meta company sign in Menlo Park.\" srcset=\"https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-1920-80.jpeg 1920w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-1200-80.jpeg 1200w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-1024-80.jpeg 1024w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-970-80.jpeg 970w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-650-80.jpeg 650w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-480-80.jpeg 480w, https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a-320-80.jpeg 320w\" sizes=\"(min-width: 1000px) 600px, calc(100vw - 40px)\" data-new-v2-image=\"true\" data-original-mos=\"https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a.jpeg\" data-pin-media=\"https://cdn.mos.cms.futurecdn.net/jiQTE9tce9jMbfrB7onX7a.jpeg\" data-pin-nopin=\"true\" fetchpriority=\"high\"/\u003e\n\u003c/picture\u003e\n\u003c/div\u003e\n\u003cfigcaption\u003e\n\u003cspan\u003e(Image credit: Michael Hicks / Android Central)\u003c/span\u003e\n\u003c/figcaption\u003e\n\u003c/div\u003e\n\n\u003cdiv id=\"article-body\"\u003e\n\u003ch2 id=\"what-you-need-to-know-3\"\u003eWhat you need to know\u003c/h2\u003e\u003cul\u003e\u003cli\u003eMeta is experimenting with a wristband that reads your muscle signals, so you can type or control stuff just by thinking about moving your fingers.\u003c/li\u003e\u003cli\u003eThe tech pairs with Meta’s Orion AR glasses, which look like regular specs but overlay digital stuff in your real-world view.\u003c/li\u003e\u003cli\u003eIt’s still a prototype (and pricey), but Meta says a more practical version is in the works.\u003c/li\u003e\u003c/ul\u003e\u003chr/\u003e\u003cp\u003eImagine scrolling through your social media feed, sending a message, or skipping a song without tapping a button or resorting to voice commands, just a subtle twitch of your hand. That’s the wild promise of Meta’s latest research, and it’s not some distant sci-fi dream.\u003c/p\u003e\u003cp\u003eThanks to a new study \u003ca data-analytics-id=\"inline-link\" href=\"https://go.redirectingat.com/?id=92X1690538\u0026amp;xcust=ac_us_1280437239258419735\u0026amp;xs=1\u0026amp;url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41586-025-09255-w\u0026amp;sref=https%3A%2F%2Fwww.androidcentral.com%2Fapps-software%2Fmeta%2Fmetas-breakthrough-wants-to-let-you-control-ar-glasses-just-by-moving-your-fingers\" target=\"_blank\" data-url=\"https://www.nature.com/articles/s41586-025-09255-w\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"sponsored noopener\" data-hl-processed=\"skimlinks\" data-placeholder-url=\"https://go.redirectingat.com/?id=92X1690538\u0026amp;xcust=hawk-custom-tracking\u0026amp;xs=1\u0026amp;url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41586-025-09255-w\u0026amp;sref=https%3A%2F%2Fwww.androidcentral.com%2Fapps-software%2Fmeta%2Fmetas-breakthrough-wants-to-let-you-control-ar-glasses-just-by-moving-your-fingers\" data-google-interstitial=\"false\" data-merchant-name=\"nature.com\" data-merchant-network=\"SkimLinks\"\u003epublished in Nature\u003c/a\u003e, we’re getting a sneak peek at how augmented reality (AR) glasses could soon read your intentions before you even fully act on them.\u003c/p\u003e\u003cp\u003eMeta’s Reality Labs has been \u003ca data-analytics-id=\"inline-link\" href=\"https://www.anrdoezrs.net/click-100048248-15736996?sid=ac-us-3906283234608610607\u0026amp;url=https://www.meta.com/blog/reality-labs-surface-emg-research-nature-publication-ar-glasses-orion/\" target=\"_blank\" data-url=\"https://www.meta.com/blog/reality-labs-surface-emg-research-nature-publication-ar-glasses-orion/\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"sponsored noopener\" data-hl-processed=\"hawklinks\" data-placeholder-url=\"https://www.anrdoezrs.net/click-100048248-15736996?sid=hawk-custom-tracking\u0026amp;url=https://www.meta.com/blog/reality-labs-surface-emg-research-nature-publication-ar-glasses-orion/\" data-google-interstitial=\"false\" data-merchant-name=\"Meta\" data-merchant-id=\"543744\" data-merchant-network=\"CJ\" data-merchant-url=\"meta.com\"\u003equietly working\u003c/a\u003e on a tech called surface electromyography (sEMG), which sounds complicated but boils down to one simple idea: detecting the tiny electrical signals your muscles send when you even think about moving.\u003c/p\u003e\u003ch2 id=\"how-it-works-wristwear-that-reads-intention-3\"\u003eHow it works: wristwear that reads intention\u003c/h2\u003e\u003cfigure data-bordeaux-image-check=\"\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-1200-80.jpg.webp 1200w\" sizes=\"(min-width: 1000px) 970px, calc(100vw - 40px)\"/\u003e\u003cimg src=\"https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23.jpg\" alt=\"Meta sEMG hand gestures\" srcset=\"https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23-1200-80.jpg 1200w\" sizes=\"(min-width: 1000px) 970px, calc(100vw - 40px)\" loading=\"lazy\" data-original-mos=\"https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23.jpg\" data-pin-media=\"https://cdn.mos.cms.futurecdn.net/xrK6Ed6uZB93yehYCzZC23.jpg\"/\u003e\u003c/picture\u003e\u003c/p\u003e\u003c/div\u003e\u003cfigcaption itemprop=\"caption description\"\u003e\u003cspan itemprop=\"copyrightHolder\"\u003e(Image credit: Meta)\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eInstead of waving your arms around or using a bulky controller, you slip on a lightweight armband. Trained on data from thousands of people, the system deciphers minute electrical patterns that the brain sends to your fingers.\u003c/p\u003e\u003cp\u003eOne test user typed at over 20 words per minute just thinking the stroke motions. The team smashed the key hurdles: generalization across users (so it doesn’t need per‑person calibration), consistent gesture recognition, and handwriting decoding, all embedded in a wristband-style interface.\u003c/p\u003e\u003cp\u003eThe big deal here is speed and subtlety. Current AR controls rely on voice, hand tracking (needs big gestures), or clunky remotes. In its Nature paper, Meta showed off \u003ca data-analytics-id=\"inline-link\" href=\"https://www.androidcentral.com/gaming/virtual-reality/meta-orion-hands-on\" data-before-rewrite-localise=\"https://www.androidcentral.com/gaming/virtual-reality/meta-orion-hands-on\"\u003eOrion\u003c/a\u003e, a prototype wristband that predicted gestures with scary accuracy, even when people barely moved. It worked while typing on a keyboard or holding a coffee without needing to pause your life to interact with tech.\u003c/p\u003e\u003cp\u003eMeta sees this fusion of AI, sEMG wristband, and \u003ca data-analytics-id=\"inline-link\" href=\"https://www.androidcentral.com/best-smart-glasses\" data-before-rewrite-localise=\"https://www.androidcentral.com/best-smart-glasses\"\u003eAR glasses\u003c/a\u003e as the natural next step beyond smartphones.\u003c/p\u003e\u003cdiv data-hydrate=\"true\" id=\"slice-container-newsletterForm-articleInbodyContent-oB5kPnL7WsthupYCmJFYHY\"\u003e\u003csection\u003e\u003cp\u003eGet the latest news from Android Central, your trusted companion in the world of Android\u003c/p\u003e\u003c/section\u003e\u003c/div\u003e\u003ch2 id=\"why-this-matters-3\"\u003eWhy this matters\u003c/h2\u003e\u003cp\u003eThe tech is a big step forward for interacting with devices, especially for people with limited mobility since it doesn’t require any physical movement to trigger input. It also makes using tech feel more natural and less effortful.\u003c/p\u003e\u003cp\u003eThat said, it’s still early days. Orion glasses reportedly cost around $10,000 per unit and aren’t available to the public yet. The wristband, while extremely promising, is still in prototype phase with no clear timeline for a commercial launch.\u003c/p\u003e\u003cp\u003eBut Meta says it\u0026#39;s working toward more affordable, consumer-ready versions of both, and the progress so far suggests it\u0026#39;s serious.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv id=\"slice-container-authorBio-oB5kPnL7WsthupYCmJFYHY\"\u003e\u003cp\u003eJay Bonggolto always keeps a nose for news. He has been writing about consumer tech and apps for as long as he can remember, and he has used a variety of Android phones since falling in love with Jelly Bean. Send him a direct message via Twitter or LinkedIn.\u003c/p\u003e\u003c/div\u003e\n\u003c/section\u003e\n\n\n\n\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-07-24T19:27:45Z",
  "modifiedTime": null
}
