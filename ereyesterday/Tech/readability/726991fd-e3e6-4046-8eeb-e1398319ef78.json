{
  "id": "726991fd-e3e6-4046-8eeb-e1398319ef78",
  "title": "Hidden AI instructions reveal how Anthropic controls Claude 4",
  "link": "https://arstechnica.com/ai/2025/05/hidden-ai-instructions-reveal-how-anthropic-controls-claude-4/",
  "description": "Expert analysis shows Anthropic's attempts to skip chatbot praise and avoid copyrighted content.",
  "author": "Benj Edwards",
  "published": "Tue, 27 May 2025 22:25:25 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Anthropic",
    "machine learning",
    "Simon Willison"
  ],
  "byline": "Benj Edwards",
  "length": 2146,
  "excerpt": "Expert analysis shows Anthropic’s attempts to skip chatbot praise and avoid copyrighted content.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Willison, who coined the term \"prompt injection\" in 2022, is always on the lookout for LLM vulnerabilities. In his post, he notes that reading system prompts reminds him of warning signs in the real world that hint at past problems. \"A system prompt can often be interpreted as a detailed list of all of the things the model used to do before it was told not to do them,\" he writes. Fighting the flattery problem Willison's analysis comes as AI companies grapple with sycophantic behavior in their models. As we reported in April, ChatGPT users have complained about GPT-4o's \"relentlessly positive tone\" and excessive flattery since OpenAI's March update. Users described feeling \"buttered up\" by responses like \"Good question! You're very astute to ask that,\" with software engineer Craig Weiss tweeting that \"ChatGPT is suddenly the biggest suckup I've ever met.\" The issue stems from how companies collect user feedback during training—people tend to prefer responses that make them feel good, creating a feedback loop where models learn that enthusiasm leads to higher ratings from humans. As a response to the feedback, OpenAI later rolled back ChatGPT's 4o model and altered the system prompt as well, something we reported on and Willison also analyzed at the time. One of Willison's most interesting findings about Claude 4 relates to how Anthropic has guided both Claude models to avoid sycophantic behavior. \"Claude never starts its response by saying a question or idea or observation was good, great, fascinating, profound, excellent, or any other positive adjective,\" Anthropic writes in the prompt. \"It skips the flattery and responds directly.\" Other system prompt highlights The Claude 4 system prompt also includes extensive instructions on when Claude should or shouldn't use bullet points and lists, with multiple paragraphs dedicated to discouraging frequent list-making in casual conversation. \"Claude should not use bullet points or numbered lists for reports, documents, explanations, or unless the user explicitly asks for a list or ranking,\" the prompt states.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/07/AI_voice_converstation_hero-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n          \n          \n\u003cp\u003eWillison, who \u003ca href=\"https://arstechnica.com/information-technology/2022/09/twitter-pranksters-derail-gpt-3-bot-with-newly-discovered-prompt-injection-hack/\"\u003ecoined\u003c/a\u003e the term \u0026#34;prompt injection\u0026#34; in 2022, is always on the lookout for LLM vulnerabilities. In his post, he notes that reading system prompts reminds him of warning signs in the real world that hint at past problems. \u0026#34;A system prompt can often be interpreted as a detailed list of all of the things the model used to do before it was told not to do them,\u0026#34; he writes.\u003c/p\u003e\n\u003ch2\u003eFighting the flattery problem\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"576\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_hearts-1024x576.jpg\" alt=\"An illustrated robot holds four red hearts with its four robotic arms.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_hearts-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_hearts-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_hearts-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_hearts-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_hearts-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_hearts-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_hearts.jpg 1200w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eWillison\u0026#39;s analysis comes as AI companies grapple with sycophantic behavior in their models. As we \u003ca href=\"https://arstechnica.com/ai/2025/04/annoyed-chatgpt-users-complain-about-bots-relentlessly-positive-tone/\"\u003ereported\u003c/a\u003e in April, ChatGPT users have complained about GPT-4o\u0026#39;s \u0026#34;relentlessly positive tone\u0026#34; and excessive flattery since OpenAI\u0026#39;s March update. Users described feeling \u0026#34;buttered up\u0026#34; by responses like \u0026#34;Good question! You\u0026#39;re very astute to ask that,\u0026#34; with software engineer Craig Weiss tweeting that \u0026#34;ChatGPT is suddenly the biggest suckup I\u0026#39;ve ever met.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThe issue stems from how companies collect user feedback during training—people tend to prefer responses that make them feel good, creating a feedback loop where models learn that enthusiasm leads to higher ratings from humans. As a response to the feedback, OpenAI later rolled back ChatGPT\u0026#39;s 4o model and altered the system prompt as well, something \u003ca href=\"https://arstechnica.com/ai/2025/04/openai-rolls-back-update-that-made-chatgpt-a-sycophantic-mess/\"\u003ewe reported on\u003c/a\u003e and Willison \u003ca href=\"https://simonwillison.net/2025/Apr/30/sycophancy-in-gpt-4o/\"\u003ealso analyzed\u003c/a\u003e at the time.\u003c/p\u003e\n\u003cp\u003eOne of Willison\u0026#39;s most interesting findings about Claude 4 relates to how Anthropic has guided both Claude models to avoid sycophantic behavior. \u0026#34;Claude never starts its response by saying a question or idea or observation was good, great, fascinating, profound, excellent, or any other positive adjective,\u0026#34; Anthropic writes in the prompt. \u0026#34;It skips the flattery and responds directly.\u0026#34;\u003c/p\u003e\n\u003ch2\u003eOther system prompt highlights\u003c/h2\u003e\n\u003cp\u003eThe Claude 4 system prompt also includes extensive instructions on when Claude should or shouldn\u0026#39;t use bullet points and lists, with multiple paragraphs dedicated to discouraging frequent list-making in casual conversation. \u0026#34;Claude should not use bullet points or numbered lists for reports, documents, explanations, or unless the user explicitly asks for a list or ranking,\u0026#34; the prompt states.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2025-05-27T22:25:25Z",
  "modifiedTime": "2025-05-27T22:25:25Z"
}
