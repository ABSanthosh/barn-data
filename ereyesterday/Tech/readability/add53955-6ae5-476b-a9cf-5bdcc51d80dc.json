{
  "id": "add53955-6ae5-476b-a9cf-5bdcc51d80dc",
  "title": "Open Source Maintainers Are Drowning in Junk Bug Reports Written By AI",
  "link": "https://developers.slashdot.org/story/24/12/10/2334221/open-source-maintainers-are-drowning-in-junk-bug-reports-written-by-ai?utm_source=rss1.0mainlinkanon\u0026utm_medium=feed",
  "description": "An anonymous reader shares a report: Software vulnerability submissions generated by AI models have ushered in a \"new era of slop security reports for open source\" -- and the devs maintaining these projects wish bug hunters would rely less on results produced by machine learning assistants. Seth Larson, security developer-in-residence at the Python Software Foundation, raised the issue in a blog post last week, urging those reporting bugs not to use AI systems for bug hunting. \"Recently I've noticed an uptick in extremely low-quality, spammy, and LLM-hallucinated security reports to open source projects,\" he wrote, pointing to similar findings from the Curl project in January. \"These reports appear at first glance to be potentially legitimate and thus require time to refute.\" Larson argued that low-quality reports should be treated as if they're malicious. As if to underscore the persistence of these concerns, a Curl project bug report posted on December 8 shows that nearly a year after maintainer Daniel Stenberg raised the issue, he's still confronted by \"AI slop\" -- and wasting his time arguing with a bug submitter who may be partially or entirely automated. Read more of this story at Slashdot.",
  "author": "msmash",
  "published": "2024-12-11T01:00:00+00:00",
  "source": "http://rss.slashdot.org/Slashdot/slashdotMain",
  "categories": [
    "programming"
  ],
  "byline": "",
  "length": 1182,
  "excerpt": "An anonymous reader shares a report: Software vulnerability submissions generated by AI models have ushered in a \"new era of slop security reports for open source\" -- and the devs maintaining these projects wish bug hunters would rely less on results produced by machine learning assistants. Seth Lar...",
  "siteName": "",
  "favicon": "",
  "text": "An anonymous reader shares a report: Software vulnerability submissions generated by AI models have ushered in a \"new era of slop security reports for open source\" -- and the devs maintaining these projects wish bug hunters would rely less on results produced by machine learning assistants. Seth Larson, security developer-in-residence at the Python Software Foundation, raised the issue in a blog post last week, urging those reporting bugs not to use AI systems for bug hunting. \"Recently I've noticed an uptick in extremely low-quality, spammy, and LLM-hallucinated security reports to open source projects,\" he wrote, pointing to similar findings from the Curl project in January. \"These reports appear at first glance to be potentially legitimate and thus require time to refute.\" Larson argued that low-quality reports should be treated as if they're malicious. As if to underscore the persistence of these concerns, a Curl project bug report posted on December 8 shows that nearly a year after maintainer Daniel Stenberg raised the issue, he's still confronted by \"AI slop\" -- and wasting his time arguing with a bug submitter who may be partially or entirely automated.",
  "image": "https://a.fsdn.com/sd/topics/programming_64.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"fhbody-175633271\"\u003e\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\u003cp\u003e\n\t\t\t\n\t\t \t\n\t\t\t\tAn anonymous reader \u003ca href=\"https://www.theregister.com/2024/12/10/ai_slop_bug_reports/?td=rt-3a\"\u003eshares a report\u003c/a\u003e:\u003ci\u003e Software vulnerability submissions generated by AI models have ushered in a \u0026#34;new era of slop security reports for open source\u0026#34; -- and the devs maintaining these projects wish bug hunters would rely less on results produced by machine learning assistants. Seth Larson, security developer-in-residence at the Python Software Foundation, raised the issue in a blog post last week, urging those reporting bugs not to use AI systems for bug hunting.\u003cp\u003e \n\n\u0026#34;Recently I\u0026#39;ve noticed an uptick in extremely low-quality, spammy, and LLM-hallucinated security reports to open source projects,\u0026#34; he wrote, pointing to similar findings from the Curl project in January. \u0026#34;These reports appear at first glance to be potentially legitimate and thus require time to refute.\u0026#34; Larson argued that low-quality reports should be treated as if they\u0026#39;re malicious.\u003c/p\u003e\u003cp\u003e \n\nAs if to underscore the persistence of these concerns, a Curl project bug report posted on December 8 shows that nearly a year after maintainer Daniel Stenberg raised the issue, he\u0026#39;s still confronted by \u0026#34;AI slop\u0026#34; -- and wasting his time arguing with a bug submitter who may be partially or entirely automated.\u003c/p\u003e\u003c/i\u003e\u003cbr/\u003e\n\t\t \t\n\t\t\u003c/p\u003e\n\n\t\t\n\n\t\t\n\n\t\t\n\t\t\t\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "2 min read",
  "publishedTime": null,
  "modifiedTime": null
}
