{
  "id": "3b781e05-9313-4945-86db-93c242c2af24",
  "title": "Apple study exposes deep cracks in LLMs’ “reasoning” capabilities",
  "link": "https://arstechnica.com/ai/2024/10/llms-cant-perform-genuine-logical-reasoning-apple-researchers-suggest/",
  "description": "Irrelevant red herrings lead to \"catastrophic\" failure of logical inference.",
  "author": "Kyle Orland",
  "published": "Mon, 14 Oct 2024 21:21:04 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI"
  ],
  "byline": "Kyle Orland",
  "length": 9917,
  "excerpt": "Irrelevant red herrings lead to “catastrophic” failure of logical inference.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Skip to content Irrelevant red herrings lead to \"catastrophic\" failure of logical inference. What is going on inside that anthropomorphized digital brain? Credit: Getty Images For a while now, companies like OpenAI and Google have been touting advanced \"reasoning\" capabilities as the next big step in their latest artificial intelligence models. Now, though, a new study from six Apple engineers shows that the mathematical \"reasoning\" displayed by advanced large language models can be extremely brittle and unreliable in the face of seemingly trivial changes to common benchmark problems. The fragility highlighted in these new results helps support previous research suggesting that LLMs use of probabilistic pattern matching is missing the formal understanding of underlying concepts needed for truly reliable mathematical reasoning capabilities. \"Current LLMs are not capable of genuine logical reasoning,\" the researchers hypothesize based on these results. \"Instead, they attempt to replicate the reasoning steps observed in their training data.\" Mix it up In \"GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\"—currently available as a pre-print paper—the six Apple researchers start with GSM8K's standardized set of over 8,000 grade-school level mathematical word problems, which is often used as a benchmark for modern LLMs' complex reasoning capabilities. They then take the novel approach of modifying a portion of that testing set to dynamically replace certain names and numbers with new values—so a question about Sophie getting 31 building blocks for her nephew in GSM8K could become a question about Bill getting 19 building blocks for his brother in the new GSM-Symbolic evaluation. This approach helps avoid any potential \"data contamination\" that can result from the static GSM8K questions being fed directly into an AI model's training data. At the same time, these incidental changes don't alter the actual difficulty of the inherent mathematical reasoning at all, meaning models should theoretically perform just as well when tested on GSM-Symbolic as GSM8K. Simply changing specific names and numbers found in GSM8K tests led to significant decreases in performance in many models. Simply changing specific names and numbers found in GSM8K tests led to significant decreases in performance in many models. Credit: Apple Research Instead, when the researchers tested more than 20 state-of-the-art LLMs on GSM-Symbolic, they found average accuracy reduced across the board compared to GSM8K, with performance drops between 0.3 percent and 9.2 percent, depending on the model. The results also showed high variance across 50 separate runs of GSM-Symbolic with different names and values. Gaps of up to 15 percent accuracy between the best and worst runs were common within a single model and, for some reason, changing the numbers tended to result in worse accuracy than changing the names. This kind of variance—both within different GSM-Symbolic runs and compared to GSM8K results—is more than a little surprising since, as the researchers point out, \"the overall reasoning steps needed to solve a question remain the same.\" The fact that such small changes lead to such variable results suggests to the researchers that these models are not doing any \"formal\" reasoning but are instead \"attempt[ing] to perform a kind of in-distribution pattern-matching, aligning given questions and solution steps with similar ones seen in the training data.\" Don’t get distracted Still, the overall variance shown for the GSM-Symbolic tests was often relatively small in the grand scheme of things. OpenAI's ChatGPT-4o, for instance, dropped from 95.2 percent accuracy on GSM8K to a still-impressive 94.9 percent on GSM-Symbolic. That's a pretty high success rate using either benchmark, regardless of whether or not the model itself is using \"formal\" reasoning behind the scenes (though total accuracy for many models dropped precipitously when the researchers added just one or two additional logical steps to the problems). An example showing how some models get mislead by irrelevant information added to the GSM8K benchmark suite. An example showing how some models get mislead by irrelevant information added to the GSM8K benchmark suite. Credit: Apple Research The tested LLMs fared much worse, though, when the Apple researchers modified the GSM-Symbolic benchmark by adding \"seemingly relevant but ultimately inconsequential statements\" to the questions. For this \"GSM-NoOp\" benchmark set (short for \"no operation\"), a question about how many kiwis someone picks across multiple days might be modified to include the incidental detail that \"five of them [the kiwis] were a bit smaller than average.\" Adding in these red herrings led to what the researchers termed \"catastrophic performance drops\" in accuracy compared to GSM8K, ranging from 17.5 percent to a whopping 65.7 percent, depending on the model tested. These massive drops in accuracy highlight the inherent limits in using simple \"pattern matching\" to \"convert statements to operations without truly understanding their meaning,\" the researchers write. Introducing irrelevant information to the prompts often led to \"catastrophic\" failure for most \"reasoning\" LLMs Introducing irrelevant information to the prompts often led to \"catastrophic\" failure for most \"reasoning\" LLMs Credit: Apple Research In the example with the smaller kiwis, for instance, most models try to subtract the smaller fruits from the final total because, the researchers surmise, \"their training datasets included similar examples that required conversion to subtraction operations.\" This is the kind of \"critical flaw\" that the researchers say \"suggests deeper issues in [the models'] reasoning processes\" that can't be helped with fine-tuning or other refinements. The illusion of understanding The results of this new GSM-Symbolic paper aren't completely new in the world of AI research. Other recent papers have similarly suggested that LLMs don't actually perform formal reasoning and instead mimic it with probabilistic pattern-matching of the closest similar data seen in their vast training sets. Still, the new research highlights just how fragile this kind of mimicry can be when the prompt in question pushes it in a direction that doesn't precisely match any training data. It also highlights the inherent limitations in trying to perform high-level reasoning without any underlying model of the logic or world behind it. As Ars' Benj Edwards put it in a July story about AI video generation: One of the reasons OpenAI's GPT-4 turned heads in text synthesis is that the model finally reached a size where it was large enough to have absorbed enough information (in training data) to give the impression that it might be able to genuinely understand and model the world when, in reality, a key aspect of its success is that it \"knows\" far more than most humans and can impress us by combining those existing concepts in novel ways. With enough training data and computation, the AI industry will likely reach what you might call \"the illusion of understanding\" with AI video synthesis eventually... We're likely seeing a similar \"illusion of understanding\" with AI's latest \"reasoning\" models, and seeing how that illusion can break when the model runs in to unexpected situations. AI expert Gary Marcus, in his analysis of the new GSM-Symbolic paper, argues that the next big leap in AI capability will only come when these neural networks can integrate true \"symbol manipulation, in which some knowledge is represented truly abstractly in terms of variables and operations over those variables, much as we see in algebra and traditional computer programming...\" Until then, we're going to get the kind of brittle \"reasoning\" that can lead AI models to fail mathematical tests in ways that calculators never do. Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once wrote a whole book about Minesweeper.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1622583937.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"app\"\u003e\n    \u003cp\u003e\u003ca href=\"#main\"\u003e\n  Skip to content\n\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cmain id=\"main\"\u003e\n            \u003carticle data-id=\"2056408\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n    \u003cdiv\u003e\n      \n\n      \n\n      \u003cp\u003e\n        Irrelevant red herrings lead to \u0026#34;catastrophic\u0026#34; failure of logical inference.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\n          \u003cdiv\u003e\n        \u003cp\u003e\u003cimg width=\"1000\" height=\"1000\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1622583937-1000x1000-1728939580.jpg\" alt=\"\" loading=\"eager\" decoding=\"async\" fetchpriority=\"high\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1622583937-1000x1000-1728939580.jpg 1000w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1622583937-150x150-1728939579.jpg 150w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1622583937-500x500-1728939580.jpg 500w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\n        \u003c/p\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      What is going on inside that anthropomorphized digital brain?\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Getty Images\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/div\u003e\n      \u003c/div\u003e\n\u003c/header\u003e\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \n\u003cp\u003eFor a while now, companies like OpenAI and Google have been \u003ca href=\"https://arstechnica.com/information-technology/2024/09/openais-new-reasoning-ai-models-are-here-o1-preview-and-o1-mini/\"\u003etouting advanced \u0026#34;reasoning\u0026#34; capabilities\u003c/a\u003e as \u003ca href=\"https://arstechnica.com/information-technology/2024/07/google-ai-earns-silver-medal-equivalent-at-international-mathematical-olympiad/\"\u003ethe next big step\u003c/a\u003e in their latest artificial intelligence models. Now, though, a new study from six Apple engineers shows that the mathematical \u0026#34;reasoning\u0026#34; displayed by advanced large language models can be extremely brittle and unreliable in the face of seemingly trivial changes to common benchmark problems.\u003c/p\u003e\n\u003cp\u003eThe fragility highlighted in these new results helps support previous research suggesting that LLMs use of probabilistic pattern matching is missing the formal understanding of underlying concepts needed for truly reliable mathematical reasoning capabilities. \u0026#34;Current LLMs are not capable of genuine logical reasoning,\u0026#34; the researchers hypothesize based on these results. \u0026#34;Instead, they attempt to replicate the reasoning steps observed in their training data.\u0026#34;\u003c/p\u003e\n\u003ch2\u003eMix it up\u003c/h2\u003e\n\u003cp\u003eIn \u0026#34;GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\u0026#34;—currently available \u003ca href=\"https://arxiv.org/pdf/2410.05229\"\u003eas a pre-print paper\u003c/a\u003e—the six Apple researchers start with \u003ca href=\"https://huggingface.co/datasets/openai/gsm8k\"\u003eGSM8K\u0026#39;s standardized set of over 8,000 grade-school level mathematical word problems\u003c/a\u003e, which is \u003ca href=\"https://klu.ai/glossary/GSM8K-eval\"\u003eoften used as a benchmark\u003c/a\u003e for modern LLMs\u0026#39; complex reasoning capabilities. They then take the novel approach of modifying a portion of that testing set to dynamically replace certain names and numbers with new values—so a question about Sophie getting 31 building blocks for her nephew in GSM8K could become a question about Bill getting 19 building blocks for his brother in the new GSM-Symbolic evaluation.\u003c/p\u003e\n\u003cp\u003eThis approach helps avoid any potential \u0026#34;data contamination\u0026#34; that can result from the static GSM8K questions being fed directly into an AI model\u0026#39;s training data. At the same time, these incidental changes don\u0026#39;t alter the actual difficulty of the inherent mathematical reasoning at all, meaning models should theoretically perform just as well when tested on GSM-Symbolic as GSM8K.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cdiv\u003e\n              \u003cp\u003e\u003ca data-pswp-width=\"1440\" data-pswp-height=\"629\" data-pswp-srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-300x131.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-640x279.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-768x335.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-1536x670.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-980x428.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-1440x629.png 1440w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1.png 1748w\" data-cropped=\"true\" href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-1440x629.png\" target=\"_blank\"\u003e\n                \u003cimg decoding=\"async\" width=\"1748\" height=\"763\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1.png\" alt=\"\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1.png 1748w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-300x131.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-640x279.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-768x335.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-1536x670.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-980x428.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-1440x629.png 1440w\" sizes=\"(max-width: 1748px) 100vw, 1748px\"/\u003e\n              \u003c/a\u003e\u003c/p\u003e\u003cdiv id=\"caption-2056424\"\u003e\u003cp\u003e\n                Simply changing specific names and numbers found in GSM8K tests led to significant decreases in performance in many models.\n                                  \u003c/p\u003e\n                              \u003c/div\u003e\n            \u003c/div\u003e\n                  \u003cfigcaption\u003e\n          \u003cdiv\u003e\n    \n    \u003cp\u003e\n      Simply changing specific names and numbers found in GSM8K tests led to significant decreases in performance in many models.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://arxiv.org/pdf/2410.05229\"\u003e\n          \n          Apple Research\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n        \u003c/figcaption\u003e\n            \u003c/figure\u003e\n\n\u003cp\u003eInstead, when the researchers tested more than 20 state-of-the-art LLMs on GSM-Symbolic, they found average accuracy reduced across the board compared to GSM8K, with performance drops between 0.3 percent and 9.2 percent, depending on the model. The results also showed high variance across 50 separate runs of GSM-Symbolic with different names and values. Gaps of up to 15 percent accuracy between the best and worst runs were common within a single model and, for some reason, changing the numbers tended to result in worse accuracy than changing the names.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eThis kind of variance—both within different GSM-Symbolic runs and compared to GSM8K results—is more than a little surprising since, as the researchers point out, \u0026#34;the overall reasoning steps needed to solve a question remain the same.\u0026#34; The fact that such small changes lead to such variable results suggests to the researchers that these models are not doing any \u0026#34;formal\u0026#34; reasoning but are instead \u0026#34;attempt[ing] to perform a kind of in-distribution pattern-matching, aligning given questions and solution steps with similar ones seen in the training data.\u0026#34;\u003c/p\u003e\n\n\u003ch2\u003eDon’t get distracted\u003c/h2\u003e\n\u003cp\u003eStill, the overall variance shown for the GSM-Symbolic tests was often relatively small in the grand scheme of things. OpenAI\u0026#39;s ChatGPT-4o, for instance, dropped from 95.2 percent accuracy on GSM8K to a still-impressive 94.9 percent on GSM-Symbolic. That\u0026#39;s a pretty high success rate using either benchmark, regardless of whether or not the model itself is using \u0026#34;formal\u0026#34; reasoning behind the scenes (though total accuracy for many models dropped precipitously when the researchers added just one or two additional logical steps to the problems).\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cdiv\u003e\n              \u003cp\u003e\u003ca data-pswp-width=\"1440\" data-pswp-height=\"745\" data-pswp-srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-300x155.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-640x331.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-768x398.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-1536x795.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-980x507.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-1440x745.png 1440w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3.png 1762w\" data-cropped=\"true\" href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-1440x745.png\" target=\"_blank\"\u003e\n                \u003cimg decoding=\"async\" width=\"1762\" height=\"912\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3.png\" alt=\"\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3.png 1762w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-300x155.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-640x331.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-768x398.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-1536x795.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-980x507.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-1440x745.png 1440w\" sizes=\"(max-width: 1762px) 100vw, 1762px\"/\u003e\n              \u003c/a\u003e\u003c/p\u003e\u003cdiv id=\"caption-2056422\"\u003e\u003cp\u003e\n                An example showing how some models get mislead by irrelevant information added to the GSM8K benchmark suite.\n                                  \u003c/p\u003e\n                              \u003c/div\u003e\n            \u003c/div\u003e\n                  \u003cfigcaption\u003e\n          \u003cdiv\u003e\n    \n    \u003cp\u003e\n      An example showing how some models get mislead by irrelevant information added to the GSM8K benchmark suite.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://arxiv.org/pdf/2410.05229\"\u003e\n          \n          Apple Research\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n        \u003c/figcaption\u003e\n            \u003c/figure\u003e\n\n\u003cp\u003eThe tested LLMs fared much worse, though, when the Apple researchers modified the GSM-Symbolic benchmark by adding \u0026#34;seemingly relevant but ultimately inconsequential statements\u0026#34; to the questions. For this \u0026#34;GSM-NoOp\u0026#34; benchmark set (short for \u0026#34;no operation\u0026#34;), a question about how many kiwis someone picks across multiple days might be modified to include the incidental detail that \u0026#34;five of them [the kiwis] were a bit smaller than average.\u0026#34;\u003c/p\u003e\n\u003cp\u003eAdding in these red herrings led to what the researchers termed \u0026#34;catastrophic performance drops\u0026#34; in accuracy compared to GSM8K, ranging from 17.5 percent to a whopping 65.7 percent, depending on the model tested. These massive drops in accuracy highlight the inherent limits in using simple \u0026#34;pattern matching\u0026#34; to \u0026#34;convert statements to operations without truly understanding their meaning,\u0026#34; the researchers write.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cdiv\u003e\n              \u003cp\u003e\u003ca data-pswp-width=\"696\" data-pswp-height=\"953\" data-pswp-srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2-300x411.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2-640x876.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2.png 696w\" data-cropped=\"true\" href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2.png\" target=\"_blank\"\u003e\n                \u003cimg decoding=\"async\" width=\"696\" height=\"953\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2.png\" alt=\"\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2.png 696w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2-300x411.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2-640x876.png 640w\" sizes=\"(max-width: 696px) 100vw, 696px\"/\u003e\n              \u003c/a\u003e\u003c/p\u003e\u003cdiv id=\"caption-2056423\"\u003e\u003cp\u003e\n                Introducing irrelevant information to the prompts often led to \u0026#34;catastrophic\u0026#34; failure for most \u0026#34;reasoning\u0026#34; LLMs\n                                  \u003c/p\u003e\n                              \u003c/div\u003e\n            \u003c/div\u003e\n                  \u003cfigcaption\u003e\n          \u003cdiv\u003e\n    \n    \u003cp\u003e\n      Introducing irrelevant information to the prompts often led to \u0026#34;catastrophic\u0026#34; failure for most \u0026#34;reasoning\u0026#34; LLMs\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://arxiv.org/pdf/2410.05229\"\u003e\n          \n          Apple Research\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n        \u003c/figcaption\u003e\n            \u003c/figure\u003e\n\n\u003cp\u003eIn the example with the smaller kiwis, for instance, most models try to subtract the smaller fruits from the final total because, the researchers surmise, \u0026#34;their training datasets included similar examples that required conversion to subtraction operations.\u0026#34; This is the kind of \u0026#34;critical flaw\u0026#34; that the researchers say \u0026#34;suggests deeper issues in [the models\u0026#39;] reasoning processes\u0026#34; that can\u0026#39;t be helped with fine-tuning or other refinements.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n        \n        \n        \n        \u003cdiv\u003e\n          \n          \n\u003ch2\u003eThe illusion of understanding\u003c/h2\u003e\n\u003cp\u003eThe results of this new GSM-Symbolic paper aren\u0026#39;t completely new in the world of AI research. \u003ca href=\"https://www.semanticscholar.org/paper/Can-large-language-models-reason-and-plan-Kambhampati/f531d1a681ed12fd582767133318d0728316a0ae\"\u003eOther\u003c/a\u003e recent \u003ca href=\"https://arxiv.org/abs/2206.10498\"\u003epapers\u003c/a\u003e have similarly suggested that LLMs don\u0026#39;t actually perform formal reasoning and instead mimic it with probabilistic pattern-matching of the closest similar data seen in their vast training sets.\u003c/p\u003e\n\u003cp\u003eStill, the new research highlights just how fragile this kind of mimicry can be when the prompt in question pushes it in a direction that doesn\u0026#39;t precisely match any training data. It also highlights the inherent limitations in trying to perform high-level reasoning without any underlying model of the logic or world behind it. As Ars\u0026#39; Benj Edwards put it \u003ca href=\"https://arstechnica.com/information-technology/2024/07/we-made-a-cat-drink-a-beer-with-runways-ai-video-generator-and-it-sprouted-hands/\"\u003ein a July story about AI video generation\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\u003cp\u003eOne of the reasons OpenAI\u0026#39;s GPT-4 turned heads in text synthesis is that the model finally reached a size where it was large enough to have absorbed enough information (in training data) to give the impression that it might be able to genuinely understand and model the world when, in reality, a key aspect of its success is that it \u0026#34;knows\u0026#34; far more than most humans and can impress us by combining those existing concepts in novel ways. With enough training data and computation, the AI industry will likely reach what you might call \u0026#34;the illusion of understanding\u0026#34; with AI video synthesis eventually...\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eWe\u0026#39;re likely seeing a similar \u0026#34;illusion of understanding\u0026#34; with AI\u0026#39;s latest \u0026#34;reasoning\u0026#34; models, and seeing how that illusion can break when the model runs in to unexpected situations.\u003c/p\u003e\n\u003cp\u003eAI expert Gary Marcus, in \u003ca href=\"https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and\"\u003ehis analysis\u003c/a\u003e of the new GSM-Symbolic paper, argues that the next big leap in AI capability will only come when these neural networks can integrate true \u0026#34;symbol manipulation, in which some knowledge is represented truly abstractly in terms of variables and operations over those variables, much as we see in algebra and traditional computer programming...\u0026#34; Until then, we\u0026#39;re going to get the kind of brittle \u0026#34;reasoning\u0026#34; that can lead AI models to fail mathematical tests in ways that calculators never do.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/kyle-orland/\"\u003e\u003cimg src=\"https://arstechnica.com/wp-content/uploads/2016/05/k.orland-13.jpg\" alt=\"Photo of Kyle Orland\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once \u003ca href=\"https://bossfightbooks.com/collections/books/products/minesweeper-by-kyle-orland\"\u003ewrote a whole book about \u003cem\u003eMinesweeper\u003c/em\u003e\u003c/a\u003e.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \n\n\n  \n\n\n\n  \n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n\u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/starship_flight5_catch1-768x432.jpg\" alt=\"Listing image for first story in Most Read: SpaceX catches returning rocket in mid-air, turning a fanciful idea into reality\" decoding=\"async\" loading=\"lazy\"/\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/main\u003e\n\n\n\n\n\n\n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2024-10-14T21:21:04Z",
  "modifiedTime": "2024-10-15T00:03:43Z"
}
