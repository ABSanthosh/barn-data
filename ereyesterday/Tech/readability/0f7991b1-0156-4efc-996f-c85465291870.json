{
  "id": "0f7991b1-0156-4efc-996f-c85465291870",
  "title": "New secret math benchmark stumps AI models and PhDs alike",
  "link": "https://arstechnica.com/ai/2024/11/new-secret-math-benchmark-stumps-ai-models-and-phds-alike/",
  "description": "FrontierMath's difficult questions remain unpublished so that AI companies can't train against it.",
  "author": "Benj Edwards",
  "published": "Tue, 12 Nov 2024 22:49:58 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "AI benchmarks",
    "Anthropic",
    "benchmarks",
    "Claude 3.5",
    "Epoch AI",
    "FrontierMath",
    "google",
    "GPT-4o",
    "large language models",
    "machine learning",
    "o1-preview",
    "openai",
    "research papers",
    "vibemarks"
  ],
  "byline": "Benj Edwards",
  "length": 2201,
  "excerpt": "FrontierMath’s difficult questions remain unpublished so that AI companies can’t train against it.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Epoch AI allowed Fields Medal winners Terence Tao and Timothy Gowers to review portions of the benchmark. \"These are extremely challenging,\" Tao said in feedback provided to Epoch. \"I think that in the near term basically the only way to solve them, short of having a real domain expert in the area, is by a combination of a semi-expert like a graduate student in a related field, maybe paired with some combination of a modern AI and lots of other algebra packages.\" A chart showing AI models' limited success on the FrontierMath problems, taken from Epoch AI's research paper. Credit: Epoch AI To aid in the verification of correct answers during testing, the FrontierMath problems must have answers that can be automatically checked through computation, either as exact integers or mathematical objects. The designers made problems \"guessproof\" by requiring large numerical answers or complex mathematical solutions, with less than a 1 percent chance of correct random guesses. Mathematician Evan Chen, writing on his blog, explained how he thinks that FrontierMath differs from traditional math competitions like the International Mathematical Olympiad (IMO). Problems in that competition typically require creative insight while avoiding complex implementation and specialized knowledge, he says. But for FrontierMath, \"they keep the first requirement, but outright invert the second and third requirement,\" Chen wrote. While IMO problems avoid specialized knowledge and complex calculations, FrontierMath embraces them. \"Because an AI system has vastly greater computational power, it's actually possible to design problems with easily verifiable solutions using the same idea that IOI or Project Euler does—basically, 'write a proof' is replaced by 'implement an algorithm in code,'\" Chen explained. The organization plans regular evaluations of AI models against the benchmark while expanding its problem set. They say they will release additional sample problems in the coming months to help the research community test their systems.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/11/madrobot_3-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n          \n          \n\u003cp\u003eEpoch AI allowed Fields Medal winners Terence Tao and Timothy Gowers to review portions of the benchmark. \u0026#34;These are extremely challenging,\u0026#34; Tao said in feedback provided to Epoch. \u0026#34;I think that in the near term basically the only way to solve them, short of having a real domain expert in the area, is by a combination of a semi-expert like a graduate student in a related field, maybe paired with some combination of a modern AI and lots of other algebra packages.\u0026#34;\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"567\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/frontiermath_chart-1024x567.jpg\" alt=\"A chart showing AI model success on the FrontierMath problems, taken from Epoch AI\u0026#39;s research paper.\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/frontiermath_chart-1024x567.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/frontiermath_chart-640x355.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/frontiermath_chart-768x425.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/frontiermath_chart-980x543.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/frontiermath_chart.jpg 1056w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A chart showing AI models\u0026#39; limited success on the FrontierMath problems, taken from Epoch AI\u0026#39;s research paper.\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Epoch AI\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eTo aid in the verification of correct answers during testing, the FrontierMath problems must have answers that can be automatically checked through computation, either as exact integers or mathematical objects. The designers made problems \u0026#34;guessproof\u0026#34; by requiring large numerical answers or complex mathematical solutions, with less than a 1 percent chance of correct random guesses.\u003c/p\u003e\n\u003cp\u003eMathematician Evan Chen, \u003ca href=\"https://blog.evanchen.cc/2024/11/10/frontiermath/\"\u003ewriting on his blog\u003c/a\u003e, explained how he thinks that FrontierMath differs from traditional math competitions like the \u003ca href=\"https://arstechnica.com/information-technology/2024/07/google-ai-earns-silver-medal-equivalent-at-international-mathematical-olympiad/\"\u003eInternational Mathematical Olympiad\u003c/a\u003e (IMO). Problems in that competition typically require creative insight while avoiding complex implementation and specialized knowledge, he says. But for FrontierMath, \u0026#34;they keep the first requirement, but outright invert the second and third requirement,\u0026#34; Chen wrote.\u003c/p\u003e\n\u003cp\u003eWhile IMO problems avoid specialized knowledge and complex calculations, FrontierMath embraces them. \u0026#34;Because an AI system has vastly greater computational power, it\u0026#39;s actually possible to design problems with easily verifiable solutions using the same idea that IOI or Project Euler does—basically, \u0026#39;write a proof\u0026#39; is replaced by \u0026#39;implement an algorithm in code,\u0026#39;\u0026#34; Chen explained.\u003c/p\u003e\n\u003cp\u003eThe organization plans regular evaluations of AI models against the benchmark while expanding its problem set. They say they will release additional sample problems in the coming months to help the research community test their systems.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2024-11-12T22:49:58Z",
  "modifiedTime": "2024-11-12T22:49:58Z"
}
