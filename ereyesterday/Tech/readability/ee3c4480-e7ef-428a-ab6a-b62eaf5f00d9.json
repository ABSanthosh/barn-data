{
  "id": "ee3c4480-e7ef-428a-ab6a-b62eaf5f00d9",
  "title": "After Meta Cheating Allegations, 'Unmodified' Llama 4 Maverick Model Tested - Ranks #32",
  "link": "https://tech.slashdot.org/story/25/04/13/2226203/after-meta-cheating-allegations-unmodified-llama-4-maverick-model-tested---ranks-32?utm_source=rss1.0mainlinkanon\u0026utm_medium=feed",
  "description": "Remember how last weekend Meta claimed its \"Maverick\" AI model (in the newly-released Llama-4 series) beat GPT-4o and Gemini Flash 2 \"on all benchmarks... This thing is a beast.\" And then how within a day several AI researchers pointed out that even Meta's own announcement admitted the Maverick tested on LM Arena was an \"experimental chat version,\" as TechCrunch pointed out. (\"As we've written about before, for various reasons, LM Arena has never been the most reliable measure of an AI model's performance. But AI companies generally haven't customized or otherwise fine-tuned their models to score better on LM Arena — or haven't admitted to doing so, at least.\") Friday TechCrunch on what happened when LMArena tested the unmodified release version of Maverick (Llama-4-Maverick-17B-128E-Instruct). It ranked 32nd. \"For the record, older models like Claude 3.5 Sonnet, released last June, and Gemini-1.5-Pro-002, released last September, rank higher,\" notes the tech site Neowin. Read more of this story at Slashdot.",
  "author": "EditorDavid",
  "published": "2025-04-13T22:28:00+00:00",
  "source": "http://rss.slashdot.org/Slashdot/slashdotMain",
  "categories": [
    "facebook"
  ],
  "byline": "",
  "length": 994,
  "excerpt": "Remember how last weekend Meta claimed its \"Maverick\" AI model (in the newly-released Llama-4 series) beat GPT-4o and Gemini Flash 2 \"on all benchmarks... This thing is a beast.\" And then how within a day several AI researchers pointed out that even Meta's own announcement admitted the Maverick te...",
  "siteName": "",
  "favicon": "",
  "text": "Remember how last weekend Meta claimed its \"Maverick\" AI model (in the newly-released Llama-4 series) beat GPT-4o and Gemini Flash 2 \"on all benchmarks... This thing is a beast.\" And then how within a day several AI researchers pointed out that even Meta's own announcement admitted the Maverick tested on LM Arena was an \"experimental chat version,\" as TechCrunch pointed out. (\"As we've written about before, for various reasons, LM Arena has never been the most reliable measure of an AI model's performance. But AI companies generally haven't customized or otherwise fine-tuned their models to score better on LM Arena — or haven't admitted to doing so, at least.\") Friday TechCrunch on what happened when LMArena tested the unmodified release version of Maverick (Llama-4-Maverick-17B-128E-Instruct). It ranked 32nd. \"For the record, older models like Claude 3.5 Sonnet, released last June, and Gemini-1.5-Pro-002, released last September, rank higher,\" notes the tech site Neowin.",
  "image": "https://a.fsdn.com/sd/topics/facebook_64.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"fhbody-177016965\"\u003e\u003cp\u003e\n\t\t\t\n\t\t \t\n\t\t\t\tRemember how last weekend Meta claimed its \u0026#34;Maverick\u0026#34; AI model (in the \u003ca href=\"https://news.slashdot.org/story/25/04/06/182233/in-milestone-for-open-source-meta-releases-new-benchmark-beating-llama-4-models\"\u003enewly-released Llama-4 series\u003c/a\u003e)  beat GPT-4o and Gemini Flash 2 \u0026#34;on all benchmarks... This thing is a beast.\u0026#34;\u003c/p\u003e\u003cp\u003e \nAnd then how within a day \u003ca href=\"https://x.com/natolambert/status/1908913635373842655\"\u003eseveral\u003c/a\u003e \u003ca href=\"https://x.com/suchenzang/status/1908938638869909724\"\u003eAI\u003c/a\u003e \u003ca href=\"https://x.com/ZainHasan6/status/1908943306936967597\"\u003eresearchers\u003c/a\u003e pointed out that even Meta\u0026#39;s own announcement admitted the Maverick tested on LM Arena was an \u0026#34;experimental chat version,\u0026#34; as \u003ca href=\"https://techcrunch.com/2025/04/06/metas-benchmarks-for-its-new-ai-models-are-a-bit-misleading/\"\u003eTechCrunch pointed out\u003c/a\u003e. (\u0026#34;As we\u0026#39;ve \u003ca href=\"https://techcrunch.com/2024/09/05/the-ai-industry-is-obsessed-with-chatbot-arena-but-it-might-not-be-the-best-benchmark/\"\u003ewritten about before\u003c/a\u003e, for various reasons, LM Arena has never been the most reliable measure of an AI model\u0026#39;s performance. But AI companies generally haven\u0026#39;t customized or otherwise fine-tuned their models to score better on LM Arena — or haven\u0026#39;t admitted to doing so, at least.\u0026#34;)\u003c/p\u003e\u003cp\u003e \n\nFriday TechCrunch on \u003ca href=\"https://techcrunch.com/2025/04/11/metas-vanilla-maverick-ai-model-ranks-below-rivals-on-a-popular-chat-benchmark/\"\u003ewhat happened when LMArena tested the \u003cem\u003eunmodified\u003c/em\u003e release version\u003c/a\u003e of Maverick (Llama-4-Maverick-17B-128E-Instruct).\u003c/p\u003e\u003cp\u003e \n\nIt ranked 32nd. \u003c/p\u003e\u003cp\u003e \n\u0026#34;For the record, older models like Claude 3.5 Sonnet, released last June, and Gemini-1.5-Pro-002, released last September, rank higher,\u0026#34; \u003ca href=\"https://www.neowin.net/news/unmodified-llama-4-maverick-ranks-below-rivals-following-meta-cheating-allegations/\"\u003enotes the tech site \u003cem\u003eNeowin\u003c/em\u003e\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "2 min read",
  "publishedTime": null,
  "modifiedTime": null
}
