{
  "id": "40012ac7-15e5-4d62-8927-b0ca1dcaafd6",
  "title": "The race to make AI as multilingual as Europe",
  "link": "https://thenextweb.com/news/making-multilingual-ai-in-europe",
  "description": "The European Union has 24 official languages and dozens more unofficial ones spoken across the continent. If you add in the European countries outside the union, then that brings at least a dozen more into the mix. Add dialects, endangered languages, and languages brought by migrants to Europe, and you end up with hundreds of languages. One thing many of us in technology could agree on is that the US dominates ‚Äî and that extends to online languages. There are many reasons for this, mostly due to American institutions, standards bodies, and companies defining how computers, their operating systems, and‚Ä¶This story continues at The Next Web",
  "author": "Chris Chinchilla",
  "published": "Mon, 30 Jun 2025 06:00:02 +0000",
  "source": "https://thenextweb.com/feed/",
  "categories": [
    "Insider",
    "Deep tech",
    "Startups and technology",
    "Next Featured",
    "Corporates and innovation"
  ],
  "byline": "Chris Chinchilla",
  "length": 14362,
  "excerpt": "Europe wants artificial intelligence to understand all its languages. Can it overcome English dominance to make AI truly multilingual?",
  "siteName": "TNW | Deep-Tech",
  "favicon": "https://next.tnwcdn.com/assets/img/favicon/favicon-194x194.png",
  "text": "The European Union has 24 official languages and dozens more unofficial ones spoken across the continent. If you add in the European countries outside the union, then that brings at least a dozen more into the mix. Add dialects, endangered languages, and languages brought by migrants to Europe, and you end up with hundreds of languages. One thing many of us in technology could agree on is that the US dominates ‚Äî and that extends to online languages. There are many reasons for this, mostly due to American institutions, standards bodies, and companies defining how computers, their operating systems, and the software they run work in their nascent days. This is changing, but for the short term at least, it remains the norm. This has also led to the majority of the web being in English. An astounding 50% of websites are in English, despite it being the native tongue of only about 6% of the world‚Äôs population, with Spanish, German, and Japanese next, but a long way behind, each only between 5-6% of the web. As we delve deeper into the new wave of AI-powered applications and services, many are driven by data in large language models (LLMs). As much of the data in these LLMs is scraped (controversially in many cases) from the web, LLMs predominantly understand and respond in English. As we find ourselves at the start of or in the midst of a shift in technological paradigm caused by the rapid growth of AI tools, this is a problem, and we‚Äôre bringing that problem into a new age. Europe already boasts several high-profile AI companies and projects, such as Mistral and Hugging Face. Google DeepMind also originated as a European company. The continent has research projects that develop language models to enhance how AI tools comprehend less commonly spoken languages. This article explores some of these initiatives, questions their effectiveness, and asks whether their efforts are worthwhile or if many users default to using English versions of tools. As Europe seeks to build its independence in AI and ML, does the continent have the companies and skills necessary to achieve its goals?The üíú of EU techThe latest rumblings from the EU tech scene, a story from our wise ol' founder Boris, and some questionable AI art. It's free, every week, in your inbox. Sign up now! Terminology and technology primer To make sense of what follows, you don‚Äôt need to understand how models are created, trained, or function. But it‚Äôs helpful to understand a couple of basics about models and their human language support. Unless model documentation explicitly mentions it is multilingual or cross-lingual, prompting it or requesting a response in an unsupported language may cause it to translate back and forth or respond in a language it does understand. Both strategies can produce unreliable and inconsistent results ‚Äî especially in low-resource languages. While high-resource languages, such as English, benefit from abundant training data. Low-resource languages, such as Gaelic or Galician, have far less, which often leads to inferior performance The harder concept to explain regarding models is ‚Äúopen,‚Äù which is unusual, as software in general has had a fairly clear definition of ‚Äúopen source‚Äù for a while. I don‚Äôt want to delve too deeply into this topic as the exact definition is still in flux and controversial. The summary is that even when a model might call itself ‚Äúopen‚Äù and is referenced as ‚Äúopen,‚Äù the meaning of ‚Äúopen‚Äù isn‚Äôt always the same. Here are two other useful terms to know: Training teaches a model to make predictions or decisions based on input data. Parameters are variables learned during model training that define how the model maps inputs to outputs. In other words, how it understands and responds to your questions. The larger the number of parameters, the more complex the model is. With that brief explanation done, how are European AI companies and projects working to enhance these processes to improve European language support? Hugging Face When someone wants to share code, they typically provide a link to their GitHub repository. When someone wants to share a model, they typically provide a Hugging Face link. Founded in 2016 by French entrepreneurs in New York City, the company is an active participant in creating communities and a strong proponent of open models. In 2024, it started an AI accelerator for European startups and partnered with Meta to develop translation tools based on Meta‚Äôs ‚ÄúNo Language Left Behind‚Äù model. They are also one of the driving forces behind the BLOOM model, a groundbreaking multilingual model that set new standards for international collaboration, openness, and training methodologies. Hugging Face is a useful tool for getting a rough idea of the language support in models. At the time of writing, Hugging Face lists 1,743,136 models and 298,927 datasets. Look at its leaderboard for monolingual models and datasets, and you see the following ranking for models and datasets that developers tag (add metadata) as supporting European languages at the time of writing: Language Language code Datasets Models English¬†English en 27,702 205,459 English eng 1,370 1,070 French fra 1,933 850 Spanish¬†Espa√±ol es 1,745 10,028 German¬†Deutsch de 1,442 9,714 English eng 1,370 1,070 You can already see some issues here. These aren‚Äôt tags set in stone. The community can add values freely. While you can see that they follow them for the most part, there is some duplication. As you can see, the models are dominated by English. A similar issue applies to the datasets on Hugging Face, which lack non-English data. What does this mean? Lucie-Aim√©e Kaffee, EU Policy Lead at Hugging Face, said that the tags indicate that a model has been trained to understand and process this language or that the dataset contains materials in that language. She added that the confusion between language support often comes during training.‚ÄúWhen training a large model, it‚Äôs common for other languages to accidentally get caught in training because there were some artefacts of it in that dataset,‚Äù she said. ‚ÄúThe language a model is tagged with is usually what the developers intended the model to understand.‚Äù As one of the main and busiest destinations for model developers and researchers, Hugging Face not only hosts much of their work, but also lets them create outward-facing communities to tell people how to use them. Thomas Wolf, co-founder of Hugging Face, described Bloom as ‚Äúthe world‚Äôs largest open multilingual language model.‚Äù Credit: Shauna Clinton/Web Summit via Sportsfile Mistral AI Perhaps the best-known Europe-based AI company is France‚Äôs Mistral AI, which unfortunately declined an interview. Its multilingual challenges partly inspired this article. At the FOSDEM developer conference in February 2024, linguistics researcher Julie Hunter asked one of Mistral‚Äôs models for a recipe in French ‚Äî but it responded in English. However, 16 months is an eternity in AI development, and neither the company‚Äôs ‚ÄúLe Chat‚Äù chat interface nor running its 7B model locally reproduced the same error in recent tests. But interestingly, 7B did produce a spelling error in the opening line: ‚Äúboueef‚Äù ‚Äî and more may follow. While Mistral sells several commercial models, tools, and services, its free-to-use models are popular, and I personally tend to use Mistral 7B for running tasks through local models. Until recently, the company wasn‚Äôt explicit about its models having multilingual support, but its announcement of the Magistral model at London Tech Week in June 2025 confirmed support for several European languages. EuroLLM EuroLLM was created as a partnership between Portuguese AI platform Unbabel and several European universities to understand and generate text in all official European Union languages. The model also includes non-European languages widely spoken by immigrant communities and major trading partners, such as Hindi, Chinese, and Turkish. Like some of the other open model projects in this article, its work was partly funded by the EU‚Äôs High Performance Computing Joint Undertaking program (EuroHPC JU). Many of them share similar names and aims, making it confusing to separate them all. EuroLLM was one of the first, and as Ricardo Rei, Senior Research Scientist at Unbabel, told me, the team has learned a lot from the projects that have come since. As Unbabel‚Äôs prime business is language translation, and translation is a key task for many multilingual models, the work on EuroLLM made sense to the Portuguese platform. Before EuroLLM, Unbabel had already been refining existing models to make its own and found them all too English-centric. One of the team‚Äôs biggest challenges was finding sufficient training data for low-resource languages. Ultimately, the availability of training material reflects the number of people who speak the language. One of the common data sources used to train European language models is Europarl, which contains transcripts of the European Parliament‚Äôs activities translated into all official EU languages. It‚Äôs also available as a Hugging Face dataset, thanks to ETH Z√ºrich. Currently, the project has a 1.7B parameter model and a 9B parameter model, and is working on a 22B parameter model. In all cases, the models can translate, but are also general-purpose, meaning you can chat with them in a similar way to ChatGPT, mixing and matching languages as you do. OpenLLM Europe OpenLLM Europe isn‚Äôt building anything directly, but it‚Äôs fostering a Europe-wide community of LLM projects, specifically medium and low-resource languages. Don‚Äôt let the one-page GitHub repository fool you: the Discord server is lively and active. OpenEuroLLM, Lumi, and Silo A joint project between several European universities and companies, OpenEuroLLM is one of the newer and larger entrants to the list of projects funded by EuroHPC. This means that it has no public models as of yet, but it involves many of the institutions and individuals behind the Lumi family of models that focus on Scandinavian and Nordic languages. It aims to create a multilingual model, provide more datasets for other models and conform to the EU AI Act. I spoke with Peter Sarlin of AMD Silo, one of the companies involved in the project and a key figure in Finnish and European AI development, about the plans. He explained that Finland, especially, has several institutes with significant AI research programs, including Lumi, one of the supercomputers part of EuroHPC. Silo, through its SiloGen product, offers open source models to customers, with a strong focus on supporting European languages. Sarlin pointed out that while sovereignty is an important motivation to him and Silo for creating and maintaining models that support European languages, the better reason is expanding the business and helping companies build solutions for small markets such as Estonia. ‚ÄúOpen models are great building blocks, but they aren‚Äôt as performant as closed ones, and many businesses in the Nordics and Scandinavia don‚Äôt have the resources to build tools based on open models,‚Äù he said. ‚ÄúSo Silo and our models can step in to fill the gaps.‚Äù Under Sarlin‚Äôs leadership, Silo AI built a Nordic LLM family to protect the region‚Äôs linguistic diversity. Credit: Silo AI The Lumi models use a ‚Äúcross-lingual training‚Äù technique in which the model shares its parameters between high-resource and low-resource languages. All this prior work led to the OpenEuroLLM project, which Sarlin describes as ‚ÄúEurope‚Äôs largest open source AI initiative ever, including pretty much all AI developers in Europe apart from Mistral.‚Äù While many efforts are underway and performing well, the training data issue for low-resource languages remains the biggest challenge, especially amid the move towards more nuanced reasoning models. Translations and cross-lingual training are options, but can create responses that sound unnatural to native speakers. As Sarlin said, ‚ÄúWe don‚Äôt want a model that sounds like an American speaking Finnish.‚Äù OpenLLM France France is one of the more active countries in AI development, with Mistral and Hugging Face leading the way. From a community perspective, the country also has OpenLLM France. The project (unsurprisingly) focuses on French language models, with several models of different parameters and datasets, which help other projects train and improve their models that support French. The datasets include a mix of political discourse, meeting recordings, theatre shows, and casual conversations. The project also maintains a leaderboard of French models on Hugging Face, one of the few (active) European language model benchmark pages. Do Europeans care about multilingual AI? Europe is full of people and projects working on multilingual language models. But do consumers care? Unfortunately, getting language usage rates for proprietary tools such as ChatGPT or Mistral is almost impossible. I created a poll on LinkedIn asking if people use AI tools in their native language, English, or a mixture of both. The results were a 50/50 split between English and a mixture of languages. This could indicate that the number of people using AI tools in a non-English language is higher than you think. Typically, people use AI tools in English for work and in their own language for personal tasks. Kaffee, a German and English speaker, said: ‚ÄúI use them mostly in English because I speak English at work and with my partner at home. But then, for personal tasks‚Ä¶, I use German.‚Äù Kaffee mentioned that Hugging Face was working on a soon-to-be-published research project that fully analysed the usage of multilingual models on the platform. She also noted anecdotally that their usage is on the rise.¬† ‚ÄúUsers have a conception that models are now more multilingual. And with the accessibility through large models like Llama, for example, being multilingual, I think that made a big impact on the research world regarding multilingual models and the number of people wanting to now use them in their own language.‚Äù The internet was always supposed to be global and for everyone, but the damning statistic that 50% of sites are in English shows it never really worked out that way. We‚Äôre entering a new phase in how we access information and who controls it. Maybe this time, the (AI) revolution will be international.",
  "image": "https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1\u0026fit=1280%2C640\u0026url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F06%2FUntitled-design-2-1.jpg\u0026signature=78bae8d540942519ca137acce026ae8f",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"article-main-content\"\u003e\n                            \u003cp\u003e\u003cspan\u003eThe European Union has \u003c/span\u003e\u003ca href=\"https://european-union.europa.eu/principles-countries-history/languages_en\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e24 official languages\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and dozens more unofficial ones spoken across the continent. If you add in the European countries outside the union, then that brings at least a dozen more into the mix. Add dialects, \u003c/span\u003e\u003ca href=\"https://www.ethnologue.com/insights/how-many-languages-endangered/\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eendangered languages\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, and languages brought by migrants to Europe, and you end up with hundreds of languages.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOne thing many of us in technology could agree on is that the US dominates ‚Äî and that extends to online languages. There are many reasons for this, mostly due to American institutions, standards bodies, and companies defining how computers, their operating systems, and the software they run work in their nascent days. This is changing, but for the short term at least, it remains the norm. This has also led to the majority of the web being in English. An astounding \u003c/span\u003e\u003ca href=\"https://w3techs.com/technologies/overview/content_language\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e50% of websites are in English\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, despite it being the native tongue of only about 6% of the world‚Äôs population, with Spanish, German, and Japanese next, but a long way behind, each only between 5-6% of the web.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs we delve deeper into the new wave of AI-powered applications and services, many are driven by data in large language models (LLMs). As much of the data in these LLMs is scraped (controversially in many cases) from the web, \u003c/span\u003e\u003ca href=\"https://www.sciencedirect.com/science/article/pii/S2666389924002903\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eLLMs predominantly understand and respond in English.\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e As we find ourselves at the start of or in the midst of a shift in technological paradigm caused by the rapid growth of AI tools, this is a problem, and we‚Äôre bringing that problem into a new age.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eEurope already boasts several high-profile AI companies and projects, such as \u003c/span\u003e\u003cspan\u003eMistral\u003c/span\u003e\u003cspan\u003e and \u003c/span\u003e\u003cspan\u003eHugging Face\u003c/span\u003e\u003cspan\u003e. \u003c/span\u003e\u003cspan\u003eGoogle DeepMind\u003c/span\u003e\u003cspan\u003e also originated as a European company. The continent has research projects that develop language models to enhance how AI tools comprehend less commonly spoken languages.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis article explores some of these initiatives, questions their effectiveness, and asks whether their efforts are worthwhile or if many users default to using English versions of tools. As Europe seeks to build its independence in AI and ML, does the continent have the companies and skills necessary to achieve its goals?\u003c/span\u003e\u003c/p\u003e\u003cdiv id=\"hs-embed-tnw\"\u003e\u003cp\u003e\u003cimg src=\"https://s3.eu-west-1.amazonaws.com/tnw.events/hardfork-2018/uploads/visuals/tnw-newsletter.png\"/\u003e\u003c/p\u003e\u003cdiv\u003e\u003cp\u003eThe üíú of EU tech\u003c/p\u003e\u003cp\u003eThe latest rumblings from the EU tech scene, a story from our wise ol\u0026#39; founder Boris, and some questionable AI art. It\u0026#39;s free, every week, in your inbox. Sign up now!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch2\u003eTerminology and technology primer\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eTo make sense of what follows, you don‚Äôt need to understand how models are created, trained, or function. But it‚Äôs helpful to understand a couple of basics about models and their human language support.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eUnless model documentation explicitly mentions it is \u003c/span\u003emultilingual or cross-lingual\u003cspan\u003e, prompting it or requesting a response in an unsupported language may cause it to translate back and forth or respond in a language it \u003c/span\u003e\u003ci\u003e\u003cspan\u003edoes\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e understand. Both strategies can produce unreliable and inconsistent results ‚Äî especially in low-resource languages.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhile \u003c/span\u003ehigh-resource\u003cspan\u003e languages, such as English, benefit from abundant training \u003ca href=\"https://thenextweb.com/topic/data\" target=\"_blank\" rel=\"noopener\"\u003edata\u003c/a\u003e. \u003c/span\u003eLow-resource\u003cspan\u003e languages, such as Gaelic or Galician, have far less, which often leads to inferior performance\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe harder concept to explain regarding models is ‚Äúopen,‚Äù which is unusual, as software in general has had a fairly clear definition of ‚Äúopen source‚Äù for a while. I don‚Äôt want to delve too deeply into this topic as the exact definition is still in flux and controversial. The summary is that even when a model might call itself ‚Äúopen‚Äù and is referenced as ‚Äúopen,‚Äù the meaning of ‚Äúopen‚Äù isn‚Äôt always the same.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHere are two other useful terms to know:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eTraining\u003c/b\u003e\u003cspan\u003e teaches a model to make predictions or decisions based on input data.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eParameters\u003c/b\u003e\u003cspan\u003e are variables learned during model training that define how the model maps inputs to outputs. In other words, how it understands and responds to your questions. The larger the number of parameters, the more complex the model is.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith that brief explanation done, how are European AI companies and projects working to enhance these processes to improve European language support?\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003eHugging Face\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWhen someone wants to share code, they typically provide a link to their GitHub repository. When someone wants to share a model, they typically provide a Hugging Face link. Founded in 2016 by French entrepreneurs in New York City, the company is an active participant in creating communities and a strong proponent of open models. In 2024, it started an AI accelerator for European startups and partnered with Meta to develop translation tools based on \u003c/span\u003e\u003cspan\u003eMeta‚Äôs \u003c/span\u003e\u003ca href=\"https://ai.meta.com/blog/nllb-200-high-quality-machine-translation/\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e‚ÄúNo Language Left Behind‚Äù model\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. They are also one of the driving forces behind the \u003c/span\u003e\u003ca href=\"https://huggingface.co/bigscience/bloom\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eBLOOM model\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, a groundbreaking multilingual model that set new standards for international collaboration, openness, and training methodologies.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHugging Face is a useful tool for getting a rough idea of the language support in models. At the time of writing, \u003c/span\u003e\u003cspan\u003eHugging Face lists \u003c/span\u003e\u003ca href=\"https://huggingface.co/models?sort=trending\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e1,743,136 models\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://huggingface.co/datasets?sort=trending\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e298,927 datasets\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. Look at its \u003c/span\u003e\u003ca href=\"https://huggingface.co/languages\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eleaderboard\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e for monolingual models and datasets\u003c/span\u003e\u003cspan\u003e, and you see the following ranking for models and datasets that developers tag (add metadata) as supporting European languages at the time of writing:\u003c/span\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003cstrong\u003eLanguage\u003c/strong\u003e\u003c/th\u003e\n\u003cth\u003e\u003cstrong\u003eLanguage code\u003c/strong\u003e\u003c/th\u003e\n\u003cth\u003e\u003cstrong\u003eDatasets\u003c/strong\u003e\u003c/th\u003e\n\u003cth\u003e\u003cstrong\u003eModels\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003cspan\u003eEnglish¬†English\u003c/span\u003e\u003c/th\u003e\n\u003cth\u003e\u003ca href=\"https://en.wikipedia.org/wiki/ISO_639:en\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003een\u003c/span\u003e\u003c/a\u003e\u003c/th\u003e\n\u003cth\u003e\u003ca href=\"https://huggingface.co/datasets?language=language:en\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e27,702\u003c/span\u003e\u003c/a\u003e\u003c/th\u003e\n\u003cth\u003e\u003ca href=\"https://huggingface.co/models?language=en\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e205,459\u003c/span\u003e\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eEnglish\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://en.wikipedia.org/wiki/ISO_639:eng\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eeng\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/datasets?language=language:eng\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e1,370\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/models?language=eng\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e1,070\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eFrench\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://en.wikipedia.org/wiki/ISO_639:fra\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003efra\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/datasets?language=language:fra\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e1,933\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/models?language=fra\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e850\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eSpanish¬†Espa√±ol\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://en.wikipedia.org/wiki/ISO_639:es\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003ees\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/datasets?language=language:es\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e1,745\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/models?language=es\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e10,028\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eGerman¬†Deutsch\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://en.wikipedia.org/wiki/ISO_639:de\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003ede\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/datasets?language=language:de\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e1,442\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/models?language=de\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e9,714\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eEnglish\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://en.wikipedia.org/wiki/ISO_639:eng\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eeng\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/datasets?language=language:eng\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e1,370\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/models?language=eng\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e1,070\u003c/span\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cspan\u003eYou can already see some issues here. These aren‚Äôt tags set in stone. The community can add values freely. While you can see that they follow them for the most part, there is some duplication.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs you can see, the models are dominated by English. A similar issue applies to the datasets on Hugging Face, which lack non-English data.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhat does this mean?\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eLucie-Aim√©e Kaffee, EU Policy Lead at Hugging Face, said that the tags indicate that a model has been trained to understand and process this language or that the dataset contains materials in that language. She added that the confusion between language support often comes during training.‚ÄúWhen training a large model, it‚Äôs common for other languages to accidentally get caught in training because there were some artefacts of it in that dataset,‚Äù she said. ‚ÄúThe language a model is tagged with is usually what the developers intended the model to understand.‚Äù\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs one of the main and busiest destinations for model developers and researchers, Hugging Face not only hosts much of their work, but also lets them create outward-facing communities to tell people how to use them.\u003c/span\u003e\u003c/p\u003e\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" alt=\"Thomas Wolf, Co-founder \u0026amp; Chief Science Officer, Hugging Face, on Centre Stage during day one of Web Summit 2024 at the MEO Arena in Lisbon, Portugal.\" width=\"1280\" height=\"720\" data-src=\"https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2025/06/Untitled-design-3.jpg\" src=\"https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2025/06/Untitled-design-3.jpg\"/\u003e\u003cfigcaption\u003eThomas Wolf, co-founder of Hugging Face, described Bloom as ‚Äúthe world‚Äôs largest open multilingual language model.‚Äù Credit: \u003ca href=\"https://flickr.com/photos/websummit/54134874860/in/photolist-2qtAY4c-2qtzQii-2qtzxfK-2qtzQuq-2qtAuX3-2qtB6md-2qtubyJ-2qtHogo-2qtGvoe-2qtF2Gc-2qtGLC3-2qtHina\" target=\"_blank\" rel=\"nofollow noopener\"\u003eShauna Clinton/Web Summit via Sportsfile\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch2\u003eMistral AI\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003ePerhaps the best-known Europe-based AI company is France‚Äôs \u003c/span\u003e\u003cspan\u003eMistral AI\u003c/span\u003e\u003cspan\u003e, which unfortunately declined an interview. Its multilingual challenges partly inspired this article. \u003c/span\u003e\u003cspan\u003eAt the \u003c/span\u003e\u003ca href=\"https://archive.fosdem.org/2024/schedule/event/fosdem-2024-2591-building-open-source-language-models/\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eFOSDEM developer conference\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e in February 2024,\u003c/span\u003e\u003cspan\u003e linguistics researcher Julie Hunter asked one of Mistral‚Äôs models for a recipe in French ‚Äî but it responded in English. However, 16 months is an eternity in AI development, and neither the company‚Äôs ‚ÄúLe Chat‚Äù chat interface nor running its 7B model locally reproduced the same error in recent tests. But interestingly, 7B did produce a spelling error in the opening line: ‚Äúboueef‚Äù ‚Äî and more may follow.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhile Mistral sells several commercial models, tools, and services, its\u003c/span\u003e\u003ca href=\"https://huggingface.co/mistralai\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e free-to-use models\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e are popular, and I personally tend to use \u003c/span\u003e\u003ca href=\"https://mistral.ai/news/announcing-mistral-7b\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eMistral 7B\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e for running tasks through local models.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eUntil recently, the company wasn‚Äôt explicit about its models having multilingual support, but its announcement of the \u003c/span\u003e\u003ca href=\"https://mistral.ai/news/magistral\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eMagistral model\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e at London Tech Week in June 2025 confirmed support for several European languages.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003eEuroLLM\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://eurollm.io/\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eEuroLLM\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e was created as a partnership between Portuguese AI platform \u003c/span\u003e\u003cspan\u003eUnbabel\u003c/span\u003e\u003cspan\u003e and several European universities to understand and generate text in all official European Union languages. The model also includes non-European languages widely spoken by immigrant communities and major trading partners, such as Hindi, Chinese, and Turkish.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eLike some of the other open model projects in this article, its work was partly funded by the \u003c/span\u003e\u003cspan\u003eEU‚Äôs \u003c/span\u003e\u003ca href=\"https://eurohpc-ju.europa.eu/index_en\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eHigh Performance Computing Joint Undertaking program\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e (EuroHPC JU). Many of them share similar names and aims, making it confusing to separate them all. EuroLLM was one of the first, and as Ricardo Rei, Senior Research Scientist at Unbabel, told me, the team has learned a lot from the projects that have come since.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs Unbabel‚Äôs prime business is language translation, and translation is a key task for many multilingual models, the work on EuroLLM made sense to the Portuguese platform. Before EuroLLM, Unbabel had already been refining existing models to make its own and found them all too English-centric.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOne of the team‚Äôs biggest challenges was finding sufficient training data for low-resource languages. Ultimately, the availability of training material reflects the number of people who speak the language. One of the common data sources used to train European language models is \u003c/span\u003e\u003ca href=\"https://www.europarl.europa.eu/portal/en\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eEuroparl\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, which contains transcripts of the European Parliament‚Äôs activities translated into all official EU languages. It‚Äôs also \u003c/span\u003e\u003ca href=\"https://huggingface.co/datasets/disco-eth/EuroSpeech\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eavailable as a Hugging Face dataset\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, thanks to \u003c/span\u003e\u003cspan\u003eETH Z√ºrich\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eCurrently, the project has a \u003c/span\u003e\u003ca href=\"https://huggingface.co/utter-project/EuroLLM-1.7B\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003e1.7B parameter model\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://huggingface.co/utter-project/EuroLLM-9B\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003ea 9B parameter model\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, and is working on a 22B parameter model. In all cases, the models can translate, but are also general-purpose, meaning you can chat with them in a similar way to ChatGPT, mixing and matching languages as you do.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003eOpenLLM Europe\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/OpenLLM-Europe/European-OpenLLM-Projects?tab=readme-ov-file\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eOpenLLM Europe\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e isn‚Äôt building anything directly, but it‚Äôs fostering a Europe-wide community of LLM projects, specifically medium and low-resource languages. Don‚Äôt let the one-page GitHub repository fool you: \u003c/span\u003e\u003ca href=\"https://discord.com/invite/b5UQTWQn\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003ethe Discord server\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e is lively and active\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003eOpenEuroLLM, Lumi, and Silo\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eA joint project between several European universities and companies, \u003c/span\u003e\u003ca href=\"https://openeurollm.eu/\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eOpenEuroLLM\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e is one of the newer and larger entrants to the list of projects funded by EuroHPC. This means that it has no public models as of yet, but it involves many of the institutions and individuals behind \u003c/span\u003e\u003ca href=\"https://huggingface.co/LumiOpen\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003ethe Lumi family of models\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e that focus on Scandinavian and Nordic languages. It aims to create a multilingual model, provide more datasets for other models and conform to the \u003c/span\u003e\u003cspan\u003eEU AI Act\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eI spoke with \u003c/span\u003e\u003cspan\u003ePeter Sarlin\u003c/span\u003e\u003cspan\u003e of \u003c/span\u003e\u003ca href=\"https://thenextweb.com/news/us-chipmaker-amd-buys-silo-ai\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eAMD Silo\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, one of the companies involved in the project and a key figure in Finnish and European AI development, about the plans. He explained that Finland, especially, has several institutes with significant AI research programs, including \u003c/span\u003e\u003cspan\u003eLumi\u003c/span\u003e\u003cspan\u003e, one of the supercomputers part of EuroHPC. Silo, through its SiloGen product, offers \u003ca href=\"https://thenextweb.com/topic/open-source\" target=\"_blank\" rel=\"noopener\"\u003eopen source\u003c/a\u003e models to customers, with a strong focus on supporting European languages. Sarlin pointed out that while sovereignty is an important motivation to him and Silo for creating and maintaining models that support European languages, the better reason is expanding the business and helping companies build solutions for small markets such as Estonia.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e‚ÄúOpen models are great building blocks, but they aren‚Äôt as performant as closed ones, and many businesses in the Nordics and Scandinavia don‚Äôt have the resources to build tools based on open models,‚Äù he said. ‚ÄúSo Silo and our models can step in to fill the gaps.‚Äù\u003c/span\u003e\u003c/p\u003e\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" alt=\"Silo AI CEO Peter Sarlin\" width=\"1280\" height=\"720\" data-src=\"https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2025/06/Untitled-design-3-2.jpg\" src=\"https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2025/06/Untitled-design-3-2.jpg\"/\u003e\u003cfigcaption\u003eUnder Sarlin‚Äôs leadership, Silo AI built a \u003ca href=\"https://thenextweb.com/news/silo-ai-viking-llms-nordic-languages\" target=\"_blank\" rel=\"noopener\"\u003eNordic LLM family\u003c/a\u003e to protect the region‚Äôs linguistic diversity. Credit: Silo AI\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cspan\u003eThe Lumi models use a ‚Äúcross-lingual training‚Äù technique in which the model shares its parameters between high-resource and low-resource languages.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAll this prior work led to the OpenEuroLLM project, which Sarlin describes as ‚ÄúEurope‚Äôs largest open source AI initiative ever, including pretty much all AI developers in Europe apart from Mistral.‚Äù\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhile many efforts are underway and performing well, the training data issue for low-resource languages remains the biggest challenge, especially amid the move towards more nuanced \u003c/span\u003e\u003ca href=\"https://arxiv.org/abs/2501.11223\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003ereasoning models\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. Translations and cross-lingual training are options, but can create responses that sound unnatural to native speakers. As Sarlin said, ‚ÄúWe don‚Äôt want a model that sounds like an American speaking Finnish.‚Äù\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003eOpenLLM France\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eFrance is one of the more active countries in AI development, with Mistral and Hugging Face leading the way. From a community perspective, the country also has \u003c/span\u003e\u003ca href=\"https://huggingface.co/OpenLLM-France\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eOpenLLM France\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. The project (unsurprisingly) focuses on French language models, with several models of different parameters and datasets, which help other projects train and improve their models that support French. \u003c/span\u003e\u003ca href=\"https://github.com/OpenLLM-France/Claire-datasets?tab=readme-ov-file#parliamentary-proceedings\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eThe datasets include\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e a mix of political discourse, meeting recordings, theatre shows, and casual conversations. The project also maintains \u003c/span\u003e\u003ca href=\"https://huggingface.co/spaces/le-leadboard/OpenLLMFrenchLeaderboard\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003ea leaderboard\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e of French models on Hugging Face\u003c/span\u003e\u003cspan\u003e, one of the few (active) European language model benchmark pages.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003eDo Europeans care about multilingual AI?\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eEurope is full of people and projects working on multilingual language models. But do consumers care? Unfortunately, getting language usage rates for proprietary tools such as ChatGPT or Mistral is almost impossible. I created a \u003c/span\u003e\u003ca href=\"https://www.linkedin.com/posts/chrischinchilla_i-am-working-on-a-piece-about-using-llms-activity-7328081633889169409-7FKM?utm_source=share\u0026amp;utm_medium=member_desktop\u0026amp;rcm=ACoAAAIljVUBH0xZMvfzrbeANZYOeLlaZ8y5g8E\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003epoll on LinkedIn\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e asking if people use AI tools in their native language, English, or a mixture of both. The results were a 50/50 split between English and a mixture of languages. This could indicate that the number of people using AI tools in a non-English language is higher than you think.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTypically, people use AI tools in English for work and in their own language for personal tasks.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eKaffee, a German and English speaker, said: ‚ÄúI use them mostly in English because I speak English at work and with my partner at home. But then, for personal tasks‚Ä¶, I use German.‚Äù\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eKaffee mentioned that Hugging Face was working on a soon-to-be-published research project that fully analysed the usage of multilingual models on the platform. She also noted anecdotally that their usage is on the rise.¬†\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e‚ÄúUsers have a conception that models are now more multilingual. And with the accessibility through large models like \u003c/span\u003e\u003ca href=\"https://www.llama.com\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eLlama\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, for example, being multilingual, I think that made a big impact on the research world regarding multilingual models and the number of people wanting to now use them in their own language.‚Äù\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe internet was always supposed to be global and for everyone, but the damning statistic that \u003c/span\u003e\u003cspan\u003e50% of sites are in \u003c/span\u003e\u003cspan\u003eEnglish shows it never really worked out that way. We‚Äôre entering a new phase in how we access information and who controls it. Maybe this time, the (AI) revolution will be international.\u003c/span\u003e\u003c/p\u003e\n                        \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2025-06-30T06:00:02Z",
  "modifiedTime": "2025-06-24T23:07:05Z"
}
