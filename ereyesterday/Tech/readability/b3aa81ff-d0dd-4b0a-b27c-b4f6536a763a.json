{
  "id": "b3aa81ff-d0dd-4b0a-b27c-b4f6536a763a",
  "title": "How close are we to an accurate AI fake news detector?",
  "link": "https://thenextweb.com/news/how-close-are-we-to-an-accurate-ai-fake-news-detector",
  "description": "In the ambitious pursuit to tackle the harms from false content on social media and news websites, data scientists are getting creative. While still in their training wheels, the large language models (LLMs) used to create chatbots like ChatGPT are being recruited to spot fake news. With better detection, AI fake news checking systems may be able to warn of, and ultimately counteract, serious harms from deepfakes, propaganda, conspiracy theories and misinformation. The next level AI tools will personalise detection of false content as well as protecting us against it. For this ultimate leap into user-centered AI, data science needs…This story continues at The Next Web",
  "author": "The Conversation",
  "published": "Mon, 11 Nov 2024 06:34:07 +0000",
  "source": "https://thenextweb.com/feed/",
  "categories": [
    "Insider",
    "Deep tech"
  ],
  "byline": "The Conversation",
  "length": 7123,
  "excerpt": "Data scientists are using AI to build a system that can detect fake news accurately. But just how close are we?",
  "siteName": "TNW | Deep-Tech",
  "favicon": "https://next.tnwcdn.com/assets/img/favicon/favicon-194x194.png",
  "text": "In the ambitious pursuit to tackle the harms from false content on social media and news websites, data scientists are getting creative. While still in their training wheels, the large language models (LLMs) used to create chatbots like ChatGPT are being recruited to spot fake news. With better detection, AI fake news checking systems may be able to warn of, and ultimately counteract, serious harms from deepfakes, propaganda, conspiracy theories and misinformation. The next level AI tools will personalise detection of false content as well as protecting us against it. For this ultimate leap into user-centered AI, data science needs to look to behavioural and neuroscience. Recent work suggests we might not always consciously know that we are encountering fake news. Neuroscience is helping to discover what is going on unconsciously. Biomarkers such as heart rate, eye movements and brain activity) appear to subtly change in response to fake and real content. In other words, these biomarkers may be “tells” that indicate if we have been taken in or not. For instance, when humans look at faces, eye-tracking data shows that we scan for rates of blinking and changes in skin colour caused by blood flow. If such elements seem unnatural, it can help us decide that we’re looking at a deepfake. This knowledge can give AI an edge – we can train it to mimic what humans look for, among other things. The personalisation of an AI fake news checker takes shape by using findings from human eye movement data and electrical brain activity that shows what types of false content has the greatest impact neurally, psychologically and emotionally, and for whom. Knowing our specific interests, personality and emotional reactions, an AI fact-checking system could detect and anticipate which content would trigger the most severe reaction in us. This could help establish when people are taken in and what sort of material fools people the easiest. Counteracting harms What comes next is customising the safeguards. Protecting us from the harms of fake news also requires building systems that could intervene – some sort of digital countermeasure to fake news. There are several ways to do this such as warning labels, links to expert-validated credible content and even asking people to try to consider different perspectives when they read something. Our own personalised AI fake news checker could be designed to give each of us one of these countermeasures to cancel out the harms from false content online. Such technology is already being trialled. Researchers in the US have studied how people interact with a personalised AI fake news checker of social media posts. It learned to reduce the number of posts in a news feed to those it deemed true. As a proof of concept, another study using social media posts tailored additional news content to each media post to encourage users to view alternative perspectives. Accurate detection of fake news But whether this all sounds impressive or dystopian, before we get carried away it might be worth asking some basic questions. Much, if not all, of the work on fake news, deepfakes, disinformation and misinformation highlights the same problem that any lie detector would face. There are many types of lie detectors, not just the polygraph test. Some exclusively depend on linguistic analysis. Others are systems designed to read people’s faces to detect if they are leaking micro-emotions that give away that they are lying. By the same token, there are AI systems that are designed to detect if a face is genuine or a deep fake. Before the detection begins, we all need to agree on what a lie looks like if we are to spot it. In fact, in deception research shows it can be easier because you can instruct people when to lie and when tell the truth. And so you have some way of knowing the ground truth before you train a human or a machine to tell the difference, because they are provided with examples on which to base their judgements. Knowing how good an expert lie detector is depends on how often they call out a lie when there was one (hit). But also, that they don’t frequently mistake someone as telling the truth when they were in fact lying (miss). This means they need to know what the truth is when they see it (correct rejection) and don’t accuse someone of lying when they were telling the truth (false alarm). What this refers to is signal detection, and the same logic applies to fake news detection which you can see in the diagram below. For an AI system detecting fake news, to be super accurate, the hits need to be really high (say 90%) and so the misses will be very low (say 10%), and the false alarms need to stay low (say 10%) which means real news isn’t called fake. If an AI fact-checking system, or a human one is recommended to us, based on signal detection, we can better understand how good it is. There are likely to be cases, as has been reported in a recent survey, where the news content may not be completely false or completely true, but partially accurate. We know this because the speed of news cycles means that what is considered accurate at one time, may later be found to be inaccurate, or vice versa. So, a fake news checking system has its work cut out. If we knew in advance what was faked and what was real news, how accurate are biomarkers at indicating unconsciously which is which? The answer is not very. Neural activity is most often the same when we come across real and fake news articles. When it comes to eye-tracking studies, it is worth knowing that there are different types of data collected from eye-tracking techniques (for example the length of time our eye fix on an object, the frequency that our eye moves across a visual scene). So depending on what is analysed, some studies show that we direct more attention when viewing false content, while others show the opposite. Are we there yet? AI fake news detection systems on the market are already using insights from behavioural science to help flag and warn us against fake news content. So it won’t be a stretch for the same AI systems to start appearing in our news feeds with customised protections for our unique user profile. The problem with all this is we still have a lot of basic ground to cover in knowing what is working, but also checking whether we want this. In the worst case scenario, we only see fake news as a problem online as an excuse to solve it using AI. But false and inaccurate content is everywhere, and gets discussed offline. Not only that, we don’t by default believe all fake news, some times we use it in discussions to illustrate bad ideas. In an imagined best case scenario, data science and behavioural science is confident about the scale of the various harms fake news might cause. But, even here, AI applications combined with scientific wizardry might still be very poor substitutes for less sophisticated but more effective solutions. Magda Osman, Professor of Policy Impact, University of Leeds This article is republished from The Conversation under a Creative Commons license. Read the original article.",
  "image": "https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1\u0026fit=1280%2C640\u0026url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FDesigner-1.jpeg\u0026signature=8863b54e2000e8f1f094cad168836e2b",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"article-main-content\"\u003e\n\u003cp\u003eIn the ambitious pursuit to tackle the harms from false content on \u003ca href=\"https://link.springer.com/content/pdf/10.1007/s13278-023-01028-5.pdf\" target=\"_blank\" rel=\"nofollow noopener\"\u003esocial media\u003c/a\u003e and \u003ca href=\"https://www.sciencedirect.com/science/article/pii/S266682702100013X\" target=\"_blank\" rel=\"nofollow noopener\"\u003enews websites\u003c/a\u003e, data scientists are getting creative.\u003c/p\u003e\n\u003cp\u003eWhile still in their training wheels, the \u003ca href=\"https://doi.org/10.1038/s42256-024-00881-z\" target=\"_blank\" rel=\"nofollow noopener\"\u003elarge language models (LLMs)\u003c/a\u003e used to create chatbots like ChatGPT are being recruited to spot \u003ca href=\"https://doi.org/10.3390/fi16080298\" target=\"_blank\" rel=\"nofollow noopener\"\u003efake news\u003c/a\u003e. With better detection, AI fake news checking systems may be able to warn of, and ultimately counteract, serious harms from \u003ca href=\"https://arxiv.org/pdf/2102.04458\" target=\"_blank\" rel=\"nofollow noopener\"\u003edeepfakes\u003c/a\u003e, \u003ca href=\"https://dl.acm.org/doi/full/10.1145/3613904.3642805\" target=\"_blank\" rel=\"nofollow noopener\"\u003epropaganda\u003c/a\u003e, \u003ca href=\"https://ieeexplore.ieee.org/abstract/document/9750122\" target=\"_blank\" rel=\"nofollow noopener\"\u003econspiracy theories\u003c/a\u003e and \u003ca href=\"https://doi.org/10.1007/s11042-023-17470-8\" target=\"_blank\" rel=\"nofollow noopener\"\u003emisinformation\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe next level \u003ca href=\"https://thenextweb.com/topic/artificial-intelligence\" target=\"_blank\" rel=\"noopener\"\u003eAI\u003c/a\u003e tools will personalise detection of false content as well as protecting us against it. For this ultimate leap into user-centered AI, \u003ca href=\"https://thenextweb.com/topic/data-science\" target=\"_blank\" rel=\"noopener\"\u003edata science\u003c/a\u003e needs to look to behavioural and neuroscience.\u003c/p\u003e\n\u003cp\u003eRecent work suggests we might \u003ca href=\"https://doi.org/10.1016/j.chb.2020.106633\" target=\"_blank\" rel=\"nofollow noopener\"\u003enot always consciously know\u003c/a\u003e that we are encountering fake news. Neuroscience is helping to discover what is going on unconsciously. Biomarkers such as \u003ca href=\"https://ieeexplore.ieee.org/abstract/document/9304909\" target=\"_blank\" rel=\"nofollow noopener\"\u003eheart rate\u003c/a\u003e, \u003ca href=\"https://dl.acm.org/doi/abs/10.1145/3382507.3418857\" target=\"_blank\" rel=\"nofollow noopener\"\u003eeye movements\u003c/a\u003e and \u003ca href=\"https://ieeexplore.ieee.org/abstract/document/9277701\" target=\"_blank\" rel=\"nofollow noopener\"\u003ebrain activity\u003c/a\u003e) appear to subtly change in response to fake and real content. In other words, these biomarkers may be “tells” that indicate if we have been taken in or not.\u003c/p\u003e\n\u003cp\u003eFor instance, when humans look at faces, eye-tracking data shows that we scan for rates of blinking and \u003ca href=\"https://doi.org/10.1016/j.jvcir.2024.104263\" target=\"_blank\" rel=\"nofollow noopener\"\u003echanges in skin colour\u003c/a\u003e caused by blood flow. If such elements seem unnatural, it can help us decide that we’re looking at a deepfake. This knowledge can give AI an edge – we can train it to mimic what humans look for, among other things.\u003c/p\u003e\n\u003cp\u003eThe personalisation of an AI fake news checker takes shape by using findings from \u003ca href=\"https://dl.acm.org/doi/abs/10.1145/3382507.3418857\" target=\"_blank\" rel=\"nofollow noopener\"\u003ehuman eye movement data\u003c/a\u003e and \u003ca href=\"https://ieeexplore.ieee.org/abstract/document/9277701\" target=\"_blank\" rel=\"nofollow noopener\"\u003eelectrical brain activity\u003c/a\u003e that shows what types of false content has the greatest impact neurally, psychologically and emotionally, \u003ca href=\"https://doi.org/10.1016/j.chb.2022.107307\" target=\"_blank\" rel=\"nofollow noopener\"\u003eand for whom\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eKnowing our specific interests, personality and \u003ca href=\"https://doi.org/10.1080/0960085X.2023.2224973\" target=\"_blank\" rel=\"nofollow noopener\"\u003eemotional reactions\u003c/a\u003e, an AI fact-checking system could detect and anticipate which content would trigger the most severe reaction in us. This could help establish when people are taken in and what sort of material fools people the easiest.\u003c/p\u003e\n\u003ch2\u003eCounteracting harms\u003c/h2\u003e\n\u003cp\u003eWhat comes next is customising the safeguards. Protecting us from the harms of fake news also requires building systems that could intervene – some sort of \u003ca href=\"https://doi.org/10.1027/1864-1105/a000407\" target=\"_blank\" rel=\"nofollow noopener\"\u003edigital countermeasure to fake news\u003c/a\u003e. There are several ways to do this such as warning labels, links to expert-validated credible content and even asking people to try to consider different perspectives when they read something.\u003c/p\u003e\n\u003cp\u003eOur own personalised AI fake news checker could be designed to give each of us one of these countermeasures \u003ca href=\"https://journals.sagepub.com/doi/full/10.1177/1529100620946707\" target=\"_blank\" rel=\"nofollow noopener\"\u003eto cancel out the harms from false content online\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSuch technology is already being trialled. Researchers in the US have studied how people interact with \u003ca href=\"https://dl.acm.org/doi/pdf/10.1145/3544548.3581219\" target=\"_blank\" rel=\"nofollow noopener\"\u003ea personalised AI fake news checker of social media posts\u003c/a\u003e. It learned to reduce the number of posts in a news feed to those it deemed true. \u003ca href=\"https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2019.00011/full\" target=\"_blank\" rel=\"nofollow noopener\"\u003eAs a proof of concept\u003c/a\u003e, another study using social media posts tailored additional news content to each media post to encourage users to view alternative perspectives.\u003c/p\u003e\n\u003ch2\u003eAccurate detection of fake news\u003c/h2\u003e\n\u003cp\u003eBut whether this all sounds impressive or dystopian, before we get carried away it might be worth asking some basic questions.\u003c/p\u003e\n\u003cp\u003eMuch, if not all, of the work on \u003ca href=\"https://journals.sagepub.com/doi/pdf/10.1177/20563051221150412\" target=\"_blank\" rel=\"nofollow noopener\"\u003efake news, deepfakes, disinformation\u003c/a\u003e and \u003ca href=\"https://journals.sagepub.com/doi/pdf/10.1177/17456916221141344\" target=\"_blank\" rel=\"nofollow noopener\"\u003emisinformation \u003c/a\u003e highlights the same problem that any lie detector would face.\u003c/p\u003e\n\u003cp\u003eThere are many types of lie detectors, not just the polygraph test. Some exclusively depend on linguistic analysis. Others are systems designed to read people’s faces to detect if they are leaking micro-emotions that give away that they are lying. By the same token, there are AI systems that are designed to detect if a face is genuine or a deep fake.\u003c/p\u003e\n\u003cp\u003eBefore the detection begins, we all need to agree on what a lie looks like if we are to spot it. In fact, in \u003ca href=\"https://doi.org/10.1177/09637214231173095\" target=\"_blank\" rel=\"nofollow noopener\"\u003edeception research\u003c/a\u003e shows it can be easier because you can instruct people when to lie and when tell the truth. And so you have some way of knowing the ground truth before you \u003ca href=\"https://doi.org/10.1080/00909880305377\" target=\"_blank\" rel=\"nofollow noopener\"\u003etrain a human\u003c/a\u003e or a \u003ca href=\"https://doi.org/10.1016/j.actpsy.2020.103250\" target=\"_blank\" rel=\"nofollow noopener\"\u003emachine\u003c/a\u003e to tell the difference, because they are provided with examples on which to base their judgements.\u003c/p\u003e\n\u003cp\u003eKnowing how good an expert lie detector is depends on how often they call out a lie when there was one (hit). But also, that they don’t frequently mistake someone as telling the truth when they were in fact lying (miss). This means they need to know what the truth is when they see it (correct rejection) and don’t accuse someone of lying when they were telling the truth (false alarm). What this refers to is signal detection, and the same logic applies to \u003ca href=\"https://doi.org/10.1177/1745691620986135\" target=\"_blank\" rel=\"nofollow noopener\"\u003efake news detection\u003c/a\u003e which you can see in the diagram below.\u003c/p\u003e\n\u003cp\u003eFor an AI system detecting fake news, to be super accurate, the hits need to be really high (say 90%) and so the misses will be very low (say 10%), and the false alarms need to stay low (say 10%) which means real news isn’t called fake. If an AI fact-checking system, or a human one is recommended to us, based on signal detection, we can better understand how good it is.\u003c/p\u003e\n\u003cp\u003eThere are likely to be cases, as has been reported in a recent \u003ca href=\"https://www.mdpi.com/2673-5172/5/2/50/pdf\" target=\"_blank\" rel=\"nofollow noopener\"\u003esurvey\u003c/a\u003e, where the news content may not be completely false or completely true, but partially accurate. We know this because the speed of news cycles means that what is considered accurate at one time, may later \u003ca href=\"https://doi.org/10.1080/13669877.2022.2049623\" target=\"_blank\" rel=\"nofollow noopener\"\u003ebe found to be inaccurate,\u003c/a\u003e or vice versa. So, a fake news checking system has its work cut out.\u003c/p\u003e\n\u003cp\u003eIf we knew in advance what was faked and what was real news, how accurate are biomarkers at indicating unconsciously which is which? The answer is not very. Neural activity \u003ca href=\"https://ieeexplore.ieee.org/iel7/9851848/9851959/09851990.pdf?casa_token=M5v1Y02PojMAAAAA:vcoUqhoCXi8F9R0cyq49HEAvMpWjFw6UND5vMTrR2TQ8NSgRobKeUT-7GvUZlVo4r_DHSFmYzA\" target=\"_blank\" rel=\"nofollow noopener\"\u003eis most often the same\u003c/a\u003e when we come across real and fake news articles.\u003c/p\u003e\n\u003cp\u003eWhen it comes to eye-tracking studies, it is worth knowing that there are different types of data collected from eye-tracking techniques (for example the length of time our eye fix on an object, the frequency that our eye moves across a visual scene).\u003c/p\u003e\n\u003cp\u003eSo depending on what is analysed, some studies show that \u003ca href=\"https://dl.acm.org/doi/pdf/10.1145/3517031.3529619?casa_token=H_djGz0jSMUAAAAA:qOJuvnWT1ER05kzEYreuK1YC2hDzsF0SdyHtDdeS3pRxOA4L5vReqXHpLBSfRO2_v1JYWpBIBnWUBw\" target=\"_blank\" rel=\"nofollow noopener\"\u003ewe direct more attention\u003c/a\u003e when viewing false content, while others show the \u003ca href=\"https://dl.acm.org/doi/pdf/10.1145/3397271.3401221?casa_token=yuYm20sEGgEAAAAA:LxvBqml_pS0hi8ojlM7vLdITFGJvSrOwsOm56_zyudAll89DKUGzmLA4y1lrQW7GD1yWOUF_7US5TQ\" target=\"_blank\" rel=\"nofollow noopener\"\u003eopposite\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eAre we there yet?\u003c/h2\u003e\n\u003cp\u003eAI fake news detection systems on the market are already using insights from behavioural science to help \u003ca href=\"https://doi.org/10.1111/jasp.12959\" target=\"_blank\" rel=\"nofollow noopener\"\u003eflag and warn us against fake news \u003c/a\u003e content. So it won’t be a stretch for the same AI systems to start appearing in our news feeds with customised protections for our unique user profile. The problem with all this is we still have a lot of basic ground to cover in knowing what is working, but also checking \u003ca href=\"https://doi.org/10.48550/arXiv.2308.10800\" target=\"_blank\" rel=\"nofollow noopener\"\u003ewhether we want this\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn the worst case scenario, we only see fake news as a problem online as an excuse to solve it using \u003ca href=\"https://books.google.co.uk/books/about/Smart_Until_It_s_Dumb.html?id=rfuizwEACAAJ\u0026amp;redir_esc=y\" target=\"_blank\" rel=\"nofollow noopener\"\u003eAI\u003c/a\u003e. But false and inaccurate content is everywhere, and gets discussed \u003ca href=\"https://www.csap.cam.ac.uk/media/uploads/files/1/offline-vs-online-sharing.pdf\" target=\"_blank\" rel=\"nofollow noopener\"\u003eoffline\u003c/a\u003e. Not only that, we don’t by default believe all fake news, some times we use it in discussions to \u003ca href=\"https://doi.org/10.3390/journalmedia5020050\" target=\"_blank\" rel=\"nofollow noopener\"\u003eillustrate bad ideas\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn an imagined best case scenario, data science and behavioural science is confident about the scale of the various harms fake news might cause. But, even here, AI applications combined with scientific wizardry might still be very poor substitutes for less sophisticated but more effective solutions.\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://counter.theconversation.com/content/242309/count.gif?distributor=republish-lightbox-basic\" alt=\"The Conversation\" width=\"1\" height=\"1\" srcset=\"\" data-old-src=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003ca href=\"https://theconversation.com/profiles/magda-osman-708478\" target=\"_blank\" rel=\"nofollow noopener\"\u003eMagda Osman\u003c/a\u003e, Professor of Policy Impact, \u003ca href=\"https://theconversation.com/institutions/university-of-leeds-1122\" target=\"_blank\" rel=\"nofollow noopener\"\u003eUniversity of Leeds\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThis article is republished from \u003ca href=\"https://theconversation.com\" target=\"_blank\" rel=\"nofollow noopener\"\u003eThe Conversation\u003c/a\u003e under a Creative Commons license. Read the \u003ca href=\"https://theconversation.com/how-close-are-we-to-an-accurate-ai-fake-news-detector-242309\" target=\"_blank\" rel=\"nofollow noopener\"\u003eoriginal article\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-11-11T06:34:07Z",
  "modifiedTime": "2024-11-11T06:34:08Z"
}
