{
  "id": "8307455d-d610-4b8e-b06d-5a287cbf7c65",
  "title": "Claude AI to process secret government data through new Palantir deal",
  "link": "https://arstechnica.com/ai/2024/11/safe-ai-champ-anthropic-teams-up-with-defense-giant-palantir-in-new-deal/",
  "description": "Critics worry Anthropic is endangering its \"ethical\" AI stance due to defense associations.",
  "author": "Benj Edwards",
  "published": "Fri, 08 Nov 2024 22:08:12 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "AI ethics",
    "Amazon",
    "Anthropic",
    "ChatGPT",
    "chatgtp",
    "Claude",
    "Claude 3",
    "Claude 3.5",
    "confabulation",
    "ethical ai",
    "futurism",
    "government contracts",
    "large language models",
    "LLaMA",
    "machine learning",
    "meta",
    "military AI",
    "openai",
    "palantir",
    "US Defense Department",
    "US intelligence agencies",
    "Victor Tangermann"
  ],
  "byline": "Benj Edwards",
  "length": 3034,
  "excerpt": "Critics worry Anthropic is endangering its “ethical” AI stance due to defense associations.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "An ethical minefield Since its founders started Anthropic in 2021, the company has marketed itself as one that takes an ethics- and safety-focused approach to AI development. The company differentiates itself from competitors like OpenAI by adopting what it calls responsible development practices and self-imposed ethical constraints on its models, such as its \"Constitutional AI\" system. As Futurism points out, this new defense partnership appears to conflict with Anthropic's public \"good guy\" persona, and pro-AI pundits on social media are noticing. Frequent AI commentator Nabeel S. Qureshi wrote on X, \"Imagine telling the safety-concerned, effective altruist founders of Anthropic in 2021 that a mere three years after founding the company, they'd be signing partnerships to deploy their ~AGI model straight to the military frontlines.\" Anthropic's \"Constitutional AI\" logo. Credit: Anthropic / Benj Edwards Anthropic's \"Constitutional AI\" logo. Credit: Anthropic / Benj Edwards Aside from the implications of working with defense and intelligence agencies, the deal connects Anthropic with Palantir, a controversial company which recently won a $480 million contract to develop an AI-powered target identification system called Maven Smart System for the US Army. Project Maven has sparked criticism within the tech sector over military applications of AI technology. It's worth noting that Anthropic's terms of service do outline specific rules and limitations for government use. These terms permit activities like foreign intelligence analysis and identifying covert influence campaigns, while prohibiting uses such as disinformation, weapons development, censorship, and domestic surveillance. Government agencies that maintain regular communication with Anthropic about their use of Claude may receive broader permissions to use the AI models. Even if Claude is never used to target a human or as part of a weapons system, other issues remain. While its Claude models are highly regarded in the AI community, they (like all LLMs) have the tendency to confabulate, potentially generating incorrect information in a way that is difficult to detect. That's a huge potential problem that could impact Claude's effectiveness with secret government data, and that fact, along with the other associations, has Futurism's Victor Tangermann worried. As he puts it, \"It's a disconcerting partnership that sets up the AI industry's growing ties with the US military-industrial complex, a worrying trend that should raise all kinds of alarm bells given the tech's many inherent flaws—and even more so when lives could be at stake.\"",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/11/ai_eyeball_spy-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n          \n          \n\u003ch2\u003eAn ethical minefield\u003c/h2\u003e\n\u003cp\u003eSince its founders started Anthropic in 2021, the company has \u003ca href=\"https://www.youtube.com/watch?v=UMF1nf3Iy3Q\"\u003emarketed itself\u003c/a\u003e as one that takes an ethics- and safety-focused approach to AI development. The company differentiates itself from competitors like OpenAI by adopting what it calls responsible development practices and self-imposed ethical constraints on its models, such as its \u0026#34;\u003ca href=\"https://arstechnica.com/information-technology/2023/05/ai-with-a-moral-compass-anthropic-outlines-constitutional-ai-in-its-claude-chatbot/\"\u003eConstitutional AI\u003c/a\u003e\u0026#34; system.\u003c/p\u003e\n\u003cp\u003eAs Futurism \u003ca href=\"https://futurism.com/the-byte/ethical-ai-anthropic-palantir\"\u003epoints out\u003c/a\u003e, this new defense partnership appears to conflict with Anthropic\u0026#39;s public \u0026#34;good guy\u0026#34; persona, and pro-AI pundits on social media are noticing. \u003cspan\u003eFrequent AI commentator Nabeel S. Qureshi \u003ca href=\"https://x.com/nabeelqu/status/1854574146283618521\"\u003ewrote\u003c/a\u003e on X, \u003c/span\u003e\u003cspan\u003e\u0026#34;Imagine telling the safety-concerned, effective altruist founders of Anthropic in 2021 that a mere three years after founding the company, they\u0026#39;d be signing partnerships to deploy their ~AGI model straight to the military frontlines.\u003c/span\u003e\u0026#34;\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cdiv\u003e\n              \u003cp\u003e\u003ca data-pswp-width=\"1200\" data-pswp-height=\"675\" data-pswp-srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red.jpg 1200w\" data-cropped=\"true\" href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red.jpg\" target=\"_blank\"\u003e\n                \u003cimg decoding=\"async\" width=\"1200\" height=\"675\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red.jpg\" alt=\"Anthropic\u0026#39;s \u0026#34;Constitutional AI\u0026#34; logo.\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red.jpg 1200w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/anthropic_constitution_red-980x551.jpg 980w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e\n              \u003c/a\u003e\u003c/p\u003e\u003cdiv id=\"caption-2061278\"\u003e\u003cp\u003e\n                Anthropic\u0026#39;s \u0026#34;Constitutional AI\u0026#34; logo.\n                                  \u003c/p\u003e\u003cp\u003e\n                    Credit:\n                                          Anthropic / Benj Edwards\n                                      \u003c/p\u003e\n                              \u003c/div\u003e\n            \u003c/div\u003e\n                  \u003cfigcaption\u003e\n          \u003cdiv\u003e\n    \n    \u003cp\u003e\n      Anthropic\u0026#39;s \u0026#34;Constitutional AI\u0026#34; logo.\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Anthropic / Benj Edwards\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n        \u003c/figcaption\u003e\n            \u003c/figure\u003e\n\n\u003cp\u003eAside from the implications of working with defense and intelligence agencies, the deal connects Anthropic with Palantir, a \u003ca href=\"https://amp.theguardian.com/commentisfree/2020/sep/04/palantir-ipo-ice-immigration-trump-administration\"\u003econtroversial company\u003c/a\u003e which \u003ca href=\"https://defensescoop.com/2024/05/29/palantir-480-million-army-contract-maven-smart-system-artificial-intelligence/\"\u003erecently won\u003c/a\u003e a $480 million contract to develop an AI-powered target identification system called Maven Smart System for the US Army. Project Maven has \u003ca href=\"https://www.reuters.com/article/business/media-telecom/google-to-scrub-us-military-deal-protested-by-employees-source-idUSL2N1T320P/\"\u003esparked criticism\u003c/a\u003e within the tech sector over military applications of AI technology.\u003c/p\u003e\n\u003cp\u003eIt\u0026#39;s worth noting that Anthropic\u0026#39;s terms of service \u003ca href=\"https://www.anthropic.com/news/expanding-access-to-claude-for-government\"\u003edo outline\u003c/a\u003e specific rules and limitations for government use. These terms permit activities like foreign intelligence analysis and identifying covert influence campaigns, while prohibiting uses such as disinformation, weapons development, censorship, and domestic surveillance. Government agencies that maintain regular communication with Anthropic about their use of Claude may receive broader permissions to use the AI models.\u003c/p\u003e\n\u003cp\u003eEven if Claude is never used to target a human or as part of a weapons system, other issues remain. While its Claude models are highly regarded in the AI community, they (like all LLMs) have the tendency to \u003ca href=\"https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\"\u003econfabulate\u003c/a\u003e, potentially generating incorrect information in a way that is difficult to detect.\u003c/p\u003e\n\u003cp\u003eThat\u0026#39;s a huge potential problem that could impact Claude\u0026#39;s effectiveness with secret government data, and that fact, along with the other associations, has Futurism\u0026#39;s Victor Tangermann worried. As he puts it, \u0026#34;It\u0026#39;s a disconcerting partnership that sets up the AI industry\u0026#39;s growing ties with the US military-industrial complex, a worrying trend that should raise all kinds of alarm bells given the tech\u0026#39;s many inherent flaws—and even more so when lives could be at stake.\u0026#34;\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-11-08T22:08:12Z",
  "modifiedTime": "2024-11-08T22:18:31Z"
}
