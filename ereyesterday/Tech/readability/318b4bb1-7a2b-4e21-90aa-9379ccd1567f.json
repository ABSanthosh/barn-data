{
  "id": "318b4bb1-7a2b-4e21-90aa-9379ccd1567f",
  "title": "FDA's New Drug Approval AI Is Generating Fake Studies",
  "link": "https://science.slashdot.org/story/25/07/23/2044251/fdas-new-drug-approval-ai-is-generating-fake-studies?utm_source=rss1.0mainlinkanon\u0026utm_medium=feed",
  "description": "An anonymous reader quotes a report from Gizmodo: Robert F. Kennedy Jr., the Secretary of Health and Human Services, has made a big push to get agencies like the Food and Drug Administration to use generative artificial intelligence tools. In fact, Kennedy recently told Tucker Carlson that AI will soon be used to approve new drugs \"very, very quickly.\" But a new report from CNN confirms all our worst fears. Elsa, the FDA's AI tool, is spitting out fake studies. CNN spoke with six current and former employees at the FDA, three of whom have used Elsa for work that they described as helpful, like creating meeting notes and summaries. But three of those FDA employees told CNN (paywalled) that Elsa just makes up nonexistent studies, something commonly referred to in AI as \"hallucinating.\" The AI will also misrepresent research, according to these employees. \"Anything that you don't have time to double-check is unreliable. It hallucinates confidently,\" one unnamed FDA employee told CNN. [...] Kennedy's Make America Healthy Again (MAHA) commission issued a report back in May that was later found to be filled with citations for fake studies. An analysis from the nonprofit news outlet NOTUS found that at least seven studies cited didn't even exist, with many more misrepresenting what was actually said in a given study. We still don't know if the commission used Elsa to generate that report. FDA Commissioner Marty Makary initially deployed Elsa across the agency on June 2, and an internal slide leaked to Gizmodo bragged that the system was \"cost-effective,\" only costing $12,000 in its first week. Makary said that Elsa was \"ahead of schedule and under budget\" when he first announced the AI rollout. But it seems like you get what you pay for. If you don't care about the accuracy of your work, Elsa sounds like a great tool for allowing you to get slop out the door faster, generating garbage studies that could potentially have real consequences for public health in the U.S. CNN notes that if an FDA employee asks Elsa to generate a one-paragraph summary of a 20-page paper on a new drug, there's no simple way to know if that summary is accurate. And even if the summary is more or less accurate, what if there's something within that 20-page report that would be a big red flag for any human with expertise? The only way to know for sure if something was missed or if the summary is accurate is to actually read the report. The FDA employees who spoke with CNN said they tested Elsa by asking basic questions like how many drugs of a certain class have been approved for children. Elsa confidently gave wrong answers, and while it apparently apologized when it was corrected, a robot being \"sorry\" doesn't really fix anything. Read more of this story at Slashdot.",
  "author": "BeauHD",
  "published": "2025-07-24T00:10:00+00:00",
  "source": "http://rss.slashdot.org/Slashdot/slashdotMain",
  "categories": [
    "medicine"
  ],
  "byline": "",
  "length": 2752,
  "excerpt": "An anonymous reader quotes a report from Gizmodo: Robert F. Kennedy Jr., the Secretary of Health and Human Services, has made a big push to get agencies like the Food and Drug Administration to use generative artificial intelligence tools. In fact, Kennedy recently told Tucker Carlson that AI will s...",
  "siteName": "",
  "favicon": "",
  "text": "An anonymous reader quotes a report from Gizmodo: Robert F. Kennedy Jr., the Secretary of Health and Human Services, has made a big push to get agencies like the Food and Drug Administration to use generative artificial intelligence tools. In fact, Kennedy recently told Tucker Carlson that AI will soon be used to approve new drugs \"very, very quickly.\" But a new report from CNN confirms all our worst fears. Elsa, the FDA's AI tool, is spitting out fake studies. CNN spoke with six current and former employees at the FDA, three of whom have used Elsa for work that they described as helpful, like creating meeting notes and summaries. But three of those FDA employees told CNN (paywalled) that Elsa just makes up nonexistent studies, something commonly referred to in AI as \"hallucinating.\" The AI will also misrepresent research, according to these employees. \"Anything that you don't have time to double-check is unreliable. It hallucinates confidently,\" one unnamed FDA employee told CNN. [...] Kennedy's Make America Healthy Again (MAHA) commission issued a report back in May that was later found to be filled with citations for fake studies. An analysis from the nonprofit news outlet NOTUS found that at least seven studies cited didn't even exist, with many more misrepresenting what was actually said in a given study. We still don't know if the commission used Elsa to generate that report. FDA Commissioner Marty Makary initially deployed Elsa across the agency on June 2, and an internal slide leaked to Gizmodo bragged that the system was \"cost-effective,\" only costing $12,000 in its first week. Makary said that Elsa was \"ahead of schedule and under budget\" when he first announced the AI rollout. But it seems like you get what you pay for. If you don't care about the accuracy of your work, Elsa sounds like a great tool for allowing you to get slop out the door faster, generating garbage studies that could potentially have real consequences for public health in the U.S. CNN notes that if an FDA employee asks Elsa to generate a one-paragraph summary of a 20-page paper on a new drug, there's no simple way to know if that summary is accurate. And even if the summary is more or less accurate, what if there's something within that 20-page report that would be a big red flag for any human with expertise? The only way to know for sure if something was missed or if the summary is accurate is to actually read the report. The FDA employees who spoke with CNN said they tested Elsa by asking basic questions like how many drugs of a certain class have been approved for children. Elsa confidently gave wrong answers, and while it apparently apologized when it was corrected, a robot being \"sorry\" doesn't really fix anything.",
  "image": "https://a.fsdn.com/sd/topics/medicine_64.png?refresh=now",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"fhbody-178449844\"\u003e\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\u003cp\u003e\n\t\t\t\n\t\t \t\n\t\t\t\tAn anonymous reader quotes a report from Gizmodo: \u003ci\u003eRobert F. Kennedy Jr., the Secretary of Health and Human Services, has made a big push to get agencies like the Food and Drug Administration to use generative artificial intelligence tools. In fact, Kennedy recently told Tucker Carlson that AI will soon be used to \u003ca href=\"https://science.slashdot.org/story/25/06/11/015216/fda-to-use-ai-in-drug-approvals-to-radically-increase-efficiency\"\u003eapprove new drugs\u003c/a\u003e \u0026#34;very, very quickly.\u0026#34; But a new report from CNN confirms all our worst fears. Elsa, the FDA\u0026#39;s AI tool, is \u003ca href=\"https://gizmodo.com/fdas-new-drug-approval-ai-is-generating-fake-studies-report-2000633153\"\u003espitting out fake studies\u003c/a\u003e.\n\u003cp\u003e \nCNN spoke with six current and former employees at the FDA, three of whom have used Elsa for work that they described as helpful, like creating meeting notes and summaries. But three of those FDA employees \u003ca href=\"https://www.cnn.com/2025/07/23/politics/fda-ai-elsa-drug-regulation-makary\"\u003etold CNN\u003c/a\u003e \u003cem\u003e(paywalled)\u003c/em\u003e that Elsa just makes up nonexistent studies, something commonly referred to in AI as \u0026#34;hallucinating.\u0026#34; The AI will also misrepresent research, according to these employees. \u0026#34;Anything that you don\u0026#39;t have time to double-check is unreliable. It hallucinates confidently,\u0026#34; one unnamed FDA employee told CNN. [...] Kennedy\u0026#39;s Make America Healthy Again (MAHA) commission issued a report \u003ca href=\"https://www.axios.com/2025/05/22/read-maha-commission-report-childrens-health\"\u003eback in May\u003c/a\u003e that was later found to be filled with citations for fake studies. An analysis from the nonprofit news outlet NOTUS found that at least seven studies cited \u003ca href=\"https://bsky.app/profile/ddiamond.bsky.social/post/3lqd3vgwlks27\"\u003edidn\u0026#39;t even exist\u003c/a\u003e, with many more misrepresenting what was actually said in a given study. We still don\u0026#39;t know if the commission used Elsa to generate that report.\n\u003c/p\u003e\u003cp\u003e \nFDA Commissioner Marty Makary initially deployed Elsa across the agency on June 2, and an internal slide leaked to Gizmodo bragged that the system was \u0026#34;cost-effective,\u0026#34; only costing $12,000 in its first week. Makary said that Elsa was \u0026#34;ahead of schedule and under budget\u0026#34; when he first announced the AI rollout. But it seems like you get what you pay for. If you don\u0026#39;t care about the accuracy of your work, Elsa sounds like a great tool for allowing you to get slop out the door faster, generating garbage studies that could potentially have real consequences for public health in the U.S. CNN notes that if an FDA employee asks Elsa to generate a one-paragraph summary of a 20-page paper on a new drug, there\u0026#39;s no simple way to know if that summary is accurate. And even if the summary is more or less accurate, what if there\u0026#39;s something within that 20-page report that would be a big red flag for any human with expertise? The only way to know for sure if something was missed or if the summary is accurate is to actually read the report. The FDA employees who spoke with CNN said they tested Elsa by asking basic questions like how many drugs of a certain class have been approved for children. Elsa confidently gave wrong answers, and while it apparently apologized when it was corrected, a robot being \u0026#34;sorry\u0026#34; doesn\u0026#39;t really fix anything.\u003c/p\u003e\u003c/i\u003e\u003cbr/\u003e\n\t\t \t\n\t\t\u003c/p\u003e\n\n\t\t\n\n\t\t\n\n\t\t\n\t\t\t\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": null,
  "modifiedTime": null
}
