{
  "id": "3a79a1fa-6b10-4f1e-a166-dcf95497716a",
  "title": "Conformance Checking at MongoDB: Testing That Our Code Matches Our TLA+ Specs",
  "link": "https://www.mongodb.com/blog/post/engineering/conformance-checking-at-mongodb-testing-our-code-matches-our-tla-specs",
  "description": "Comments",
  "author": "",
  "published": "Mon, 02 Jun 2025 21:50:15 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "A. Jesse Jiryu Davis",
  "length": 23807,
  "excerpt": "Testing code fidelity with TLA+ specs at MongoDB. Explore conformance checking experiences and agile modelling in distributed systems.",
  "siteName": "MongoDB",
  "favicon": "",
  "text": "Some features mentioned below have been sunset since this paper was originally written. Visit our docs to learn more. At MongoDB, we design a lot of distributed algorithms—algorithms with lots of concurrency and complexity, and dire consequences for mistakes. We formally specify some of the scariest algorithms in TLA+, to check that they behave correctly in every scenario. But how do we know that our implementations conform to our specs? And how do we keep them in sync as the implementation evolves? This problem is called conformance checking. In 2020, my colleagues and I experimented with two MongoDB products, to see if we could test their fidelity to our TLA+ specs. Here's a video of my presentation on this topic at the VLDB conference. (It'll be obvious to you that I recorded it from my New York apartment in deep Covid lockdown.) Below, I write about our experience with conformance checking from 2025's perspective. I'll tell you what worked for us in 2020 and what didn't, and what developments there have been in the field in the five years since our paper. Agile modelling Our conformance-checking project was born when I read a paper from 2011—\"Concurrent Development of Model and Implementation\"—which described a software methodology called eXtreme Modelling. The authors argued that there's a better way to use languages like TLA+, and I was convinced. They advocated a combination of agile development and rigorous formal specification: Multiple specifications model aspects of the system. Specifications are written just prior to the implementation. Specifications evolve with the implementation. Tests are generated from the model, and/or trace-checking verifies that test traces are legal in the specification. I was excited about this vision. Too often, an engineer tries to write one huge TLA+ spec for the whole system. It's too complex and detailed, so it's not much easier to understand than the implementation code, and state-space explosion dooms model checking. The author abandons the spec and concludes that TLA+ is impractical. In the eXtreme Modelling style, a big system is modeled by a collection of small specs, each focusing on an aspect of the whole. This was the direction MongoDB was already going, and it seemed right to me. In eXtreme Modelling, the conformance of the spec and implementation is continuously tested. The authors propose two conformance checking techniques. To understand these, let's consider what a TLA+ spec is: it's a description of an algorithm as a state machine. The state machine has a set of variables, and each state is an assignment of specific values to those variables. The state machine also has a set of allowed actions, which are transitions from one state to the next state. You can make a state graph by drawing states as nodes and allowed actions as edges. A behavior is any path through the graph. This diagram shows the whole state graph for some very simple imaginary spec. One of the spec's behaviors is highlighted in green. Figure 1. A formal spec's state graph, with one behavior highlighted. The spec has a set of behaviors Bspec, and the implementation has a set of behaviors Bimpl. An implementation refines a spec if Bimpl ⊂ Bspec. If the converse is also true, if Bspec ⊂ Bimpl, then this is called bisimulation, and it's a nice property to have, though not always necessary for a correctly implemented system. You can test each direction: Test-case generation: For every behavior in Bspec, generate a test case that forces the implementation to follow the same sequence of transitions. If there's a spec behavior the implementation can't follow, then Bspec ⊄ Bimpl, and the test fails. Trace-checking: For every behavior in Bimpl, generate a trace: a log file that records the implementation's state transitions, including all implementation variables that match spec variables. If the behavior recorded in the trace isn't allowed by the spec, then Bimpl ⊄ Bspec and the test fails. Figure 2. Two ways to test that the spec's behaviors are the same as the implementation's. Non-conforming behaviors are highlighted in red. Both techniques can be hard, of course. For test-case generation, you must somehow control every decision the implementation makes, squash all nondeterminism, and force it to follow a specific behavior. If the spec's state space is huge, you have to generate a huge number of tests, or choose an incomplete sample. Trace-checking, on the other hand, requires you to somehow map the implementation's state back to the spec's, and log a snapshot of the system state each time it changes—this is really hard with multithreaded programs and distributed systems. And you need to make the implementation explore a variety of behaviors, via fault-injection and stress-testing, and so on. Completeness is usually impossible. We found academic papers that demonstrated both techniques on little example applications, but we hadn’t seen them tried on production-scale systems like ours. I wanted to see how well they work, and what it would take to make them practical. I recruited my colleagues Judah Schvimer and Max Hirschhorn to try it with me. Judah and I tried trace-checking the MongoDB server (in the next section), and Max tried test-case generation with the MongoDB Mobile SDK (the remainder of this article). Figure 3. We tried two conformance checking techniques on two MongoDB products. Trace-checking the MongoDB server For the trace-checking experiment, the first step Judah and I took was to choose a TLA+ spec. MongoDB engineers had already written and model-checked a handful of specs that model different aspects of the MongoDB server (see this presentation and this one). We chose RaftMongo.tla, which focuses on how servers learn the commit point, which I'll explain now. MongoDB is typically deployed as a replica set of cooperating servers, usually three of them. They achieve consensus with a Raft-like protocol. First, they elect one server as the leader. Clients send all writes to the leader, which appends them to its log along with a monotonically increasing logical timestamp. Followers replicate the leader's log asynchronously, and they tell the leader how up-to-date they are. The leader keeps track of the commit point—the logical timestamp of the newest majority-replicated write. All writes up to and including the commit point are committed, all the writes after it are not. The commit point must be correctly tracked even when leaders and followers crash, messages are lost, a new leader is elected, uncommitted writes are rolled back, and so on. RaftMongo.tla models this protocol, and it checks two invariants: A safety property, which says that no committed write is ever lost, and a liveness property, which says that all servers eventually learn the newest commit point. Figure 4. MongoDB replica set servers and their logs. Judah and I wanted to test that MongoDB's C++ implementation matched our TLA+ spec, using trace-checking. Here are the steps: Run randomized tests of the implementation. Collect execution traces. Translate the execution traces into TLA+. Check the trace is permitted by the spec. Figure 5. The trace-checking workflow. The MongoDB server team has hundreds of integration tests handwritten in JavaScript, from which we chose about 300 for this experiment. We also have randomized tests; we chose one called the \"rollback fuzzer\" which does random CRUD operations while randomly creating and healing network partitions, causing uncommitted writes to be logged and rolled back. We added tracing code to the MongoDB server and ran each test with a three-node replica set. Since all server processes ran on one machine and communicated over localhost, we didn't worry about clock synchronization: we just merged the three logs, sorting by timestamp. We wrote a Python script to read the combined log and convert it into a giant TLA+ spec named Trace.tla with a sequence of states for the whole three-server system. Trace.tla asserted only one property: \"This behavior conforms to RaftMongo.tla.\" Here's some more detail about the Python script. At each moment during the test, the system has some state V, which is the values of the state variables for each node. The script tries to reconstruct all the changes to V and record them in Trace.tla. It begins by setting V to a hardcoded initial state V0, and outputs it as the first state of the sequence: \\* Each TLA+ tuple is \\* \u003c\u003caction, committedEntries, currentTerm, log, role, commitPoint, \\* serverLogLocation\u003e\u003e \\* We know the first state: all nodes are followers with empty logs. Trace == \u003c\u003c \u003c\u003c\"Init\", \\* action name \u003c\u003c\"Follower\",\"Follower\",\"Follower\"\u003e\u003e, \\* role per node \u003c\u003c1, 1, 1\u003e\u003e, \\* commitPoint per node \u003c\u003c\u003c\u003c...\u003e\u003e,\u003c\u003c...\u003e\u003e,\u003c\u003c...\u003e\u003e\u003e\u003e, \\* log per node \"\"\u003e\u003e, \\* trace log location (empty) \\* ... more states will follow ... The script reads events from the combined log and updates V. Here's an example where Node 1 was the leader in state Vi, then Node 2 logs that it became leader. The script combines these to produce Vi+1 where Node 2 is the leader and Node 1 is now a follower. Note, this is a lie. Node 1 didn't actually become a follower in the same instant Node 2 became leader. Foreshadowing! This will be a problem for Judah and me. Figure 6. Constructing the next state from a trace event. Anyway, the Python script appends a state to the sequence in Trace.tla: Trace == \u003c\u003c \\* ... thousands of events ... \u003c\u003c\"BecomePrimary\", \\* action name for debugging \u003c\u003c\"Follower\",\"Leader\",\"Follower\"\u003e\u003e, \\* role per node \u003c\u003c1, 1, 1\u003e\u003e, \\* commitPoint per node \u003c\u003c\u003c\u003c...\u003e\u003e,\u003c\u003c...\u003e\u003e,\u003c\u003c...\u003e\u003e\u003e\u003e, \\* log per node \\* trace log location, for debugging: \"/home/emptysquare/RollbackFuzzer/node2.log:12345\"\u003e\u003e, \\* ... thousands more events ... \u003e\u003e We used the Python script to generate a Trace.tla file for each of the hundreds of tests we'd selected: handwritten JavaScript tests and the randomized \"rollback fuzzer\" test. Now we wanted to use the model-checker to check that this state sequence was permitted by our TLA+ spec, so we know our C++ code behaved in a way that conforms to the spec. Following a technique published by Ron Pressler, we added these lines to each Trace.tla: VARIABLES log, role, commitPoint \\* Instantiate our hand-written spec, RaftMongo.tla. Model == INSTANCE RaftMongo VARIABLE i \\* the trace index \\* Load one trace event. Read == /\\ log = Trace[i][4] /\\ role = Trace[i][5] /\\ commitPoint = Trace[i][6] ReadNext == /\\ log' = Trace[i'][4] /\\ role' = Trace[i'][5] /\\ commitPoint' = Trace[i'][6] Init == i = 1 /\\ Read Next == \\/ i \u003c Len(Trace) /\\ i' = i + 1 /\\ ReadNext \\/ UNCHANGED \u003c\u003ci, vars\u003e\u003e \\* So that we don’t get a deadlock error in TLC TraceBehavior == Init /\\ [][Next]_\u003c\u003cvars, i\u003e\u003e \\* To verify, we check the spec TraceBehavior in TLC, with Model!SpecBehavior \\* as a temporal property. We run the standard TLA+ model-checker (\"TLC\"), which tells us if this trace is an allowed behavior in RaftMongo.tla. But this whole experiment failed. Our traces never matched our specification. We didn't reach our goal, but we learned three lessons that could help future engineers. What disappointment taught us Lesson one: It's hard to snapshot a multithreaded program's state. Each time a MongoDB node executes a state transition, it has to snapshot its state variables in order to log them. MongoDB is highly concurrent with fairly complex locking within each process—it was built to avoid global locking. It took us a month to figure out how to instrument MongoDB to get a consistent snapshot of all these values at one moment. We burned most of our budget for the experiment, and we worried we'd changed MongoDB too much (on a branch) to test it realistically. The 2024 paper \"Validating Traces of Distributed Programs Against TLA+ Specifications\" describes how to do trace-checking when you can only log some of the values (see my summary at the bottom of this page). We were aware of this option back in 2020, and we worried it would make trace-checking too permissive; it wouldn't catch every bug. Lesson two: The implementation must actually conform to the spec. This is obvious to me now. After all, conformance checking was the point of the project. In our real-life implementation, when an old leader votes for a new one, first the old leader steps down, then the new leader steps up. The spec we chose for trace-checking wasn't focused on the election protocol, though, so for simplicity, the spec assumed these two actions happened at once. (Remember I said a few paragraphs ago, \"This is a lie\"?) Judah and I knew about this discrepancy—we'd deliberately made this simplification in the spec. We tried to paper over the difference with some post-processing in our Python script, but it never worked. By the end of the project, we decided we should have backtracked, making our spec much more complex and realistic, but we'd run out of time. The eXtreme Modelling methodology says we should write the spec just before the implementation. But our spec was written long after most of the implementation, and it was highly abstract. I can imagine another world where we knew about eXtreme Modelling and TLA+ at the start, when we began coding MongoDB. In that world, we wrote our spec before the implementation, with trace-checking in mind. The spec and implementation would've been structured similarly, and this would all have been much easier. Lesson three: Trace-checking should extend easily to multiple specs. Judah and I put in 10 weeks of effort without successfully trace-checking one spec, and most of the work was specific to that spec, RaftMongo.tla. Sure, we learned general lessons (you're reading some of them) and wrote some general code, but even if we'd gotten trace-checking to work for one spec we'd be practically starting over with the next spec. Our original vision was to gather execution traces from all our tests, and trace-check them against all of our specifications, on every git commit. We estimated that the marginal cost of implementing trace-checking for more specs wasn't worth the marginal value, so we stopped the project. Practical trace-checking If we started again, we'd do it differently. We'd ensure the spec and implementation conform at the start, and we'd fix discrepancies by fixing the spec or the implementation right away. We'd model easily observed events like network messages, to avoid snapshotting the internal state of a multithreaded process. I still think trace-checking is worthwhile. I know it's worked for other projects. In fact MongoDB is sponsoring a grad student Finn Hackett, whom I'm mentoring, to continue trace-checking research. Let's move on to the second half of our project. Test-case generation for MongoDB Mobile SDK The MongoDB Mobile SDK is a database for mobile devices that syncs with a central server (since we wrote the paper, MongoDB has sunsetted the product). Mobile clients can make changes locally. These changes are periodically uploaded to the server and downloaded by other clients. The clients and the server all use the same algorithm to resolve write conflicts: Operational Transformation, or OT. Max wanted to test that the clients and server implement OT correctly, meaning they resolve conflicts the same way, eventually resulting in identical data everywhere. Originally, the clients and server shared one C++ implementation of OT, so we knew they implemented the same algorithm. But in 2020, we'd recently rewritten the server in Go, so testing their conformance became urgent. Figure 7. MongoDB mobile SDK. My colleague Max Hirschhorn used test-case generation to check conformance. This technique goes in the opposite direction from trace-checking: trace-checking starts with an implementation and checks that its behaviors are allowed by the spec, but test-case generation starts with a spec and checks that its behaviors are in the implementation. But first, we needed a TLA+ spec. Before this project, the mobile team had written out the OT algorithm in English and implemented it in C++. Max manually translated the algorithm from C++ to TLA+. In the mobile SDK, clients can do 19 kinds of operations on data; six of these can be performed on arrays, resulting in 21 array merge rules, which are implemented in about 1000 lines of C++. Those 21 rules are the most complex, and Max focused his specification there. He used the model-checker to verify that his TLA+ spec ensured all participants eventually had the same data. This translation was a gruelling job, but the model-checker caught Max's mistakes quickly, and he finished in two weeks. There was one kind of write conflict that crashed the model-checker: if one participant swapped two array elements, and another moved an element, then the model-checker crashed with a Java StackOverflowError. Surprisingly, this was an actual infinite-recursion bug in the algorithm. Max verified that the bug was in the C++ code. It had hidden there until he faithfully transcribed it into TLA+ and discovered it with the model-checker. He disabled the element-swap operation in his TLA+ spec, and the mobile team deprecated it in their implementation. To test conformance, Max used the model-checker to output the entire state graph for the spec. He constrained the algorithm to three participants, all editing a three-element array, each executing one (possibly conflicting) write operation. With these constraints, the state space is a DAG, with a finite number of behaviors (paths from an initial state to a final state). There are 30,184 states and 4913 behaviors. Max wrote a Go program to parse the model-checker's output and write out a C++ unit test for each behavior. Here’s an example unit test. (It's edited down from three participants to two.) At the start, there's an array containing {1, 2, 3}. One client sets the third element of an array to 4 and the second client removes the second element from the array. The test asserts that both clients agree the final array is {1, 4}. The bold parts are specific to this generated test. The rest of the code is the same for all tests. TEST(Transform_Array) { size_t num_clients = 2; TransformArrayFixture fixture{test_context, num_clients, {1, 2, 3}}; fixture.transaction(0, [](TableRef array) { array-\u003eset_int(0, 2, 4); }); fixture.transaction(1, [](TableRef array) { array-\u003eremove(1); }); fixture.sync_all_clients(); fixture.check_array({1, 4}); fixture.check_ops(0, {ArrayErase{1}}); fixture.check_ops(1, {ArraySet{1, 4}}); } These 4913 tests immediately achieved 100% branch coverage of the implementation, which we hadn't accomplished with our handwritten tests (21%) or millions of executions with the AFL fuzzer (92%). Retrospective Max's test-case generation worked quite well. He discovered a bug in the algorithm, and he thoroughly checked that the mobile SDK's Operational Transformation code conforms to the spec. Judah's and my trace-checking experiment didn't work: our spec and code were too far apart, and adding tracing to MongoDB took too long. Both techniques can work, given the right circumstances and strategy. Both techniques can fail, too! We published our results and lessons as a paper in VLDB 2020, titled \"eXtreme Modelling in Practice.\" In the subsequent five years, I've seen some progress in conformance checking techniques. Test-case generation: Model Checking Guided Testing for Distributed Systems. The \"Mocket\" system generates tests from a TLA+ spec, and instruments Java code (with a fair amount of human labor) to force it to deterministically follow each test, and check that its variables have the same values as the spec after each action. The authors tested the conformance of three Java distributed systems and found some new bugs. Their technique is Java-specific but could be adapted for other languages. Multi-Grained Specifications for Distributed System Model Checking and Verification. The authors wrote several new TLA+ specs of Zookeeper, at higher and lower levels of abstraction. They checked conformance between the most concrete specs and the implementation, with a technique similar to Mocket: a human programmer instruments some Java code to map Java variables to spec variables, and to make all interleavings deterministic. The model-checker randomly explores spec behaviors, while the test framework checks that the Java code can follow the same behaviors. SandTable: Scalable Distributed System Model Checking with Specification-Level State Exploration. This system is not language-specific: it overrides system calls to control nondeterminism and force the implementation to follow each behavior of the spec. It samples the spec's state space to maximize branch coverage and event diversity while minimizing the length of each behavior. As in the \"Multi-Grained\" paper, the SandTable authors wisely developed new TLA+ specs that closely matched the implementations they were testing, rather than trying to use existing, overly abstract specs like Judah and I did. Plus, my colleagues Will Schultz and Murat Demirbas are publishing a paper in VLDB 2025 that uses test-case generation with a new TLA+ spec of MongoDB's WiredTiger storage layer, the paper is titled \"Design and Modular Verification of Distributed Transactions in MongoDB.\" Trace-checking: Protocol Conformance with Choreographic PlusCal. The authors write new specs in an extremely high-level language that compiles to TLA+. From their specs they generate Go functions for trace-logging, which they manually add to existing Go programs. They check that the resulting traces are valid spec behaviors and find some bugs. Validating Traces of Distributed Programs Against TLA+ Specifications. Some veteran TLA+ experts demonstrate in detail how to trace-log from a Java program and validate the traces with TLC, the TLA+ model-checker. They've written small libraries and added TLC features for convenience. This paper focuses on validating incomplete traces: if you can only log some of the variables, TLC will infer the rest. Smart Casual Verification of the Confidential Consortium Framework. The authors started with an existing implementation of a secure consensus protocol. Their situation was like mine in 2020 (new specs of a big old C++ program) and so was their goal: to continuously check conformance and keep the spec and implementation in sync. Using the new TLC features announced in the \"Validating Traces\" paper above, they toiled for months, brought their specs and code into line, found some bugs, and realized the eXtreme Modelling vision. Finn Hackett is a PhD student I'm mentoring, he's developed a TLA+-to-Go compiler. He's now prototyping a trace-checker to verify that the Go code he produces really conforms to its source spec. We're doing a summer project together with Antithesis to thoroughly conformance-check the implementation's state space. I'm excited to see growing interest in conformance checking, because I think it's a serious problem that needs to be solved before TLA+ goes mainstream. The \"Validating Traces\" paper announced some new trace-checking features in TLC, and TLC's developers are discussing a better way to export a state graph for test-case generation. I hope these research prototypes lead to standard tools, so engineers can keep their code and specs in sync. Join our MongoDB Community to learn about upcoming events, hear stories from MongoDB users, and connect with community members from around the world.",
  "image": "https://webassets.mongodb.com/_com_assets/cms/Meta Image Template (9)-j83ciow8il.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cb\u003eSome features mentioned below have been sunset since this paper was originally written. Visit \u003ca href=\"https://www.mongodb.com/resources/resources/resource-library/docs-atlas-device-sync-getting-started\"\u003eour docs\u003c/a\u003e to learn more.\u003c/b\u003e\u003c/p\u003e\t\n\u003cp\u003eAt MongoDB, we design a lot of distributed algorithms—algorithms with lots of concurrency and complexity, and dire consequences for mistakes. We formally specify some of the scariest algorithms in TLA+, to check that they behave correctly in every scenario. But how do we know that our implementations conform to our specs? And how do we keep them in sync as the implementation evolves?\u003c/p\u003e\n\u003cp\u003eThis problem is called conformance checking. In 2020, my colleagues and I experimented with two MongoDB products, to see if we could test their fidelity to our TLA+ specs. Here\u0026#39;s a video of my presentation on this topic at the VLDB conference. (It\u0026#39;ll be obvious to you that I recorded it from my New York apartment in deep Covid lockdown.) Below, I write about our experience with conformance checking from 2025\u0026#39;s perspective. I\u0026#39;ll tell you what worked for us in 2020 and what didn\u0026#39;t, and what developments there have been in the field in the five years since our paper.\u003c/p\u003e\n\u003ciframe width=\"800\" height=\"425\" src=\"https://www.youtube.com/embed/IIGzXX72weQ?si=K1Q81wYICPgGfhK6\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003ch2\u003eAgile modelling\u003c/h2\u003e\n\u003cp\u003eOur conformance-checking project was born when I read a paper from 2011—\u0026#34;Concurrent Development of Model and Implementation\u0026#34;—which described a software methodology called eXtreme Modelling. The authors argued that there\u0026#39;s a better way to use languages like TLA+, and I was convinced. They advocated a combination of agile development and rigorous formal specification:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eMultiple specifications model aspects of the system.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSpecifications are written just prior to the implementation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSpecifications evolve with the implementation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTests are generated from the model, and/or trace-checking verifies that test traces are legal in the specification.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eI was excited about this vision. Too often, an engineer tries to write one huge TLA+ spec for the whole system. It\u0026#39;s too complex and detailed, so it\u0026#39;s not much easier to understand than the implementation code, and state-space explosion dooms model checking. The author abandons the spec and concludes that TLA+ is impractical. In the eXtreme Modelling style, a big system is modeled by a collection of small specs, each focusing on an aspect of the whole. This was the direction MongoDB was already going, and it seemed right to me.\u003c/p\u003e\n\u003cp\u003eIn eXtreme Modelling, the conformance of the spec and implementation is continuously tested. The authors propose two conformance checking techniques. To understand these, let\u0026#39;s consider what a TLA+ spec is: it\u0026#39;s a description of an algorithm as a state machine. The state machine has a set of variables, and each state is an assignment of specific values to those variables. The state machine also has a set of allowed actions, which are transitions from one state to the next state. You can make a state graph by drawing states as nodes and allowed actions as edges. A behavior is any path through the graph.\u003c/p\u003e\n\u003cp\u003eThis diagram shows the whole state graph for some very simple imaginary spec. One of the spec\u0026#39;s behaviors is highlighted in green.\u003c/p\u003e\n\u003ccenter\u003e\u003cb\u003eFigure 1.\u003c/b\u003e A formal spec\u0026#39;s state graph, with one behavior highlighted.\u003c/center\u003e\n\u003cfigure\u003e\n\u003cp\u003e\u003cimg src=\"https://webassets.mongodb.com/_com_assets/cms/Screenshot 2025-06-02 at 8.28.01 AM-8x0quly4v0.png\" alt=\"Diagram depicting a spec\u0026#39;s state graph, with part of the diagram highlighted in green and labeled initial state.\" title=\" \"/\u003e\n\u003c/p\u003e\n\u003cfigcaption\u003e \u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThe spec has a set of behaviors B\u003csub\u003espec\u003c/sub\u003e, and the implementation has a set of behaviors B\u003csub\u003eimpl\u003c/sub\u003e. An implementation refines a spec if B\u003csub\u003eimpl\u003c/sub\u003e ⊂ B\u003csub\u003espec\u003c/sub\u003e. If the converse is also true, if B\u003csub\u003espec\u003c/sub\u003e ⊂ B\u003csub\u003eimpl\u003c/sub\u003e, then this is called \u003ci\u003ebisimulation\u003c/i\u003e, and it\u0026#39;s a nice property to have, though not always necessary for a correctly implemented system. You can test each direction:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cb\u003eTest-case generation:\u003c/b\u003e For every behavior in B\u003csub\u003espec\u003c/sub\u003e, generate a test case that forces the implementation to follow the same sequence of transitions. If there\u0026#39;s a spec behavior the implementation can\u0026#39;t follow, then B\u003csub\u003espec\u003c/sub\u003e ⊄ B\u003csub\u003eimpl\u003c/sub\u003e, and the test fails.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cb\u003eTrace-checking:\u003c/b\u003e For every behavior in B\u003csub\u003eimpl\u003c/sub\u003e, generate a trace: a log file that records the implementation\u0026#39;s state transitions, including all implementation variables that match spec variables. If the behavior recorded in the trace isn\u0026#39;t allowed by the spec, then B\u003csub\u003eimpl\u003c/sub\u003e ⊄ B\u003csub\u003espec\u003c/sub\u003e and the test fails.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ccenter\u003e\u003cb\u003eFigure 2.\u003c/b\u003e Two ways to test that the spec\u0026#39;s behaviors are the same as the implementation\u0026#39;s. Non-conforming behaviors are highlighted in red.\u003c/center\u003e\n\u003cfigure\u003e\n\u003cp\u003e\u003cimg src=\"https://webassets.mongodb.com/_com_assets/cms/Screenshot 2025-06-02 at 8.32.32 AM-qeipk3ziqq.png\" alt=\"Diagram showing the test between the spec and implementation behaviors. On the left is the spec behavior, with a portion highlighted in red indicating that this behavior only appears in the spec. Between the two diagrams are arrows that highlight the test case generation and trace checking which analyze the two versions. On the right is the diagram for the implementation, which has a portion highlighted in red that indicates a behavior that only appears in the implementation version.\" title=\" \"/\u003e\n\u003c/p\u003e\n\u003cfigcaption\u003e \u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eBoth techniques can be hard, of course. For test-case generation, you must somehow control every decision the implementation makes, squash all nondeterminism, and force it to follow a specific behavior. If the spec\u0026#39;s state space is huge, you have to generate a huge number of tests, or choose an incomplete sample.\u003c/p\u003e\n\u003cp\u003eTrace-checking, on the other hand, requires you to somehow map the implementation\u0026#39;s state back to the spec\u0026#39;s, and log a snapshot of the system state each time it changes—this is really hard with multithreaded programs and distributed systems. And you need to make the implementation explore a variety of behaviors, via fault-injection and stress-testing, and so on. Completeness is usually impossible.\u003c/p\u003e\n\u003cp\u003eWe found academic papers that demonstrated both techniques on little example applications, but we hadn’t seen them tried on production-scale systems like ours. I wanted to see how well they work, and what it would take to make them practical. I recruited my colleagues \u003ca href=\"https://www.linkedin.com/in/judahschvimer/\" target=\"_blank\"\u003eJudah Schvimer\u003c/a\u003e and \u003ca href=\"https://www.linkedin.com/in/maxhirschhorn/\" target=\"_blank\"\u003eMax Hirschhorn\u003c/a\u003e to try it with me. Judah and I tried trace-checking the MongoDB server (in the next section), and Max tried test-case generation with the MongoDB Mobile SDK (the remainder of this article).\u003c/p\u003e\n\u003ccenter\u003e\u003cb\u003eFigure 3.\u003c/b\u003e We tried two conformance checking techniques on two MongoDB products.\u003c/center\u003e\n\u003cfigure\u003e\n\u003cp\u003e\u003cimg src=\"https://webassets.mongodb.com/_com_assets/cms/Screenshot 2025-06-02 at 8.36.05 AM-2yqcix233f.png\" alt=\"Diagram showing that for this project trace-checking was run on MongoDB server and test-case generation was run on MongoDB Mobile SDK.\" title=\" \"/\u003e\n\u003c/p\u003e\n\u003cfigcaption\u003e \u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2\u003eTrace-checking the MongoDB server\u003c/h2\u003e\n\u003cp\u003eFor the trace-checking experiment, the first step Judah and I took was to choose a TLA+ spec. MongoDB engineers had already written and model-checked a handful of specs that model different aspects of the MongoDB server (see \u003ca href=\"https://www.youtube.com/watch?v=-eAktIBUhHA\" target=\"_blank\"\u003ethis presentation\u003c/a\u003e and \u003ca href=\"https://www.youtube.com/watch?v=x9zSynTfLDE\" target=\"_blank\"\u003ethis one\u003c/a\u003e). We chose \u003ca href=\"https://github.com/mongodb/mongo/blob/master/src/mongo/tla_plus/Replication/RaftMongo/RaftMongo.tla\" target=\"_blank\"\u003eRaftMongo.tla\u003c/a\u003e, which focuses on how servers learn the commit point, which I\u0026#39;ll explain now.\u003c/p\u003e\n\u003cp\u003eMongoDB is typically deployed as a replica set of cooperating servers, usually three of them. They achieve consensus with a \u003ca href=\"https://www.usenix.org/conference/nsdi21/presentation/zhou\" target=\"_blank\"\u003eRaft-like protocol\u003c/a\u003e. First, they elect one server as the leader. Clients send all writes to the leader, which appends them to its log along with a monotonically increasing logical timestamp. Followers replicate the leader\u0026#39;s log asynchronously, and they tell the leader how up-to-date they are. The leader keeps track of the commit point—the logical timestamp of the newest majority-replicated write. All writes up to and including the commit point are committed, all the writes after it are not. The commit point must be correctly tracked even when leaders and followers crash, messages are lost, a new leader is elected, uncommitted writes are rolled back, and so on.\u003c/p\u003e\n\u003cp\u003eRaftMongo.tla models this protocol, and it checks two invariants: A safety property, which says that no committed write is ever lost, and a liveness property, which says that all servers eventually learn the newest commit point.\u003c/p\u003e\n\u003ccenter\u003e\u003cb\u003eFigure 4.\u003c/b\u003e MongoDB replica set servers and their logs.\u003c/center\u003e\n\u003cfigure\u003e\n\u003cp\u003e\u003cimg src=\"https://webassets.mongodb.com/_com_assets/cms/Screenshot 2025-06-02 at 8.38.56 AM-282t9s5p6s.png\" alt=\"Diagram showing the MongoDB replica set servers and their logs. At the bottom is a follower, with logs 1-5. In the middle is another follower, with logs 1-4. And at the top is a leader with logs 1-6 and log 5 labeled as the commit point. \" title=\" \"/\u003e\n\u003c/p\u003e\n\u003cfigcaption\u003e \u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eJudah and I wanted to test that MongoDB\u0026#39;s C++ implementation matched our TLA+ spec, using trace-checking. Here are the steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eRun randomized tests of the implementation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCollect execution traces.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTranslate the execution traces into TLA+.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCheck the trace is permitted by the spec.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ccenter\u003e\u003cb\u003eFigure 5.\u003c/b\u003e The trace-checking workflow.\u003c/center\u003e\n\u003cfigure\u003e\n\u003cp\u003e\u003cimg src=\"https://webassets.mongodb.com/_com_assets/cms/Screenshot 2025-06-02 at 8.41.08 AM-i6z6apjfwy.png\" alt=\"Diagram showing the trace-checking workflow. It starts top left with a JavaScript test which runs through the nodes. This then flows into the log files and then into the combined log. From there, through TLA+ generator, it flows to the trace.tla. This connects to the TLC model checking, which then provides a pass or fail result. \" title=\" \"/\u003e\n\u003c/p\u003e\n\u003cfigcaption\u003e \u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThe MongoDB server team has hundreds of integration tests handwritten in JavaScript, from which we chose about 300 for this experiment. We also have randomized tests; we chose one called the \u0026#34;rollback fuzzer\u0026#34; which does random CRUD operations while randomly creating and healing network partitions, causing uncommitted writes to be logged and rolled back.\u003c/p\u003e\n\u003cp\u003eWe added tracing code to the MongoDB server and ran each test with a three-node replica set. Since all server processes ran on one machine and communicated over localhost, we didn\u0026#39;t worry about clock synchronization: we just merged the three logs, sorting by timestamp. We wrote a Python script to read the combined log and convert it into a giant TLA+ spec named Trace.tla with a sequence of states for the whole three-server system. Trace.tla asserted only one property: \u0026#34;This behavior conforms to RaftMongo.tla.\u0026#34;\u003c/p\u003e\n\u003cp\u003eHere\u0026#39;s some more detail about the Python script. At each moment during the test, the system has some state V, which is the values of the state variables for each node. The script tries to reconstruct all the changes to V and record them in Trace.tla. It begins by setting V to a hardcoded initial state V0, and outputs it as the first state of the sequence:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\\* Each TLA+ tuple is\n\\* \u0026lt;\u0026lt;action, committedEntries, currentTerm, log, role, commitPoint,\n\\*   serverLogLocation\u0026gt;\u0026gt;\n\\* We know the first state: all nodes are followers with empty logs.\nTrace == \u0026lt;\u0026lt;\n  \u0026lt;\u0026lt;\u0026#34;Init\u0026#34;,                               \\* action name\n    \u0026lt;\u0026lt;\u0026#34;Follower\u0026#34;,\u0026#34;Follower\u0026#34;,\u0026#34;Follower\u0026#34;\u0026gt;\u0026gt;, \\* role per node\n    \u0026lt;\u0026lt;1, 1, 1\u0026gt;\u0026gt;,                          \\* commitPoint per node\n    \u0026lt;\u0026lt;\u0026lt;\u0026lt;...\u0026gt;\u0026gt;,\u0026lt;\u0026lt;...\u0026gt;\u0026gt;,\u0026lt;\u0026lt;...\u0026gt;\u0026gt;\u0026gt;\u0026gt;,          \\* log per node\n    \u0026#34;\u0026#34;\u0026gt;\u0026gt;,                                 \\* trace log location (empty)\n\\* ... more states will follow ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe script reads events from the combined log and updates V. Here\u0026#39;s an example where Node 1 was the leader in state Vi, then Node 2 logs that it became leader. The script combines these to produce Vi+1 where Node 2 is the leader and Node 1 is now a follower. Note, this is a lie. Node 1 didn\u0026#39;t actually become a follower in the same instant Node 2 became leader. Foreshadowing! This will be a problem for Judah and me.\u003c/p\u003e\n\u003ccenter\u003e\u003cb\u003eFigure 6.\u003c/b\u003e Constructing the next state from a trace event.\u003c/center\u003e\n\u003cfigure\u003e\n\u003cp\u003e\u003cimg src=\"https://webassets.mongodb.com/_com_assets/cms/Screenshot 2025-06-02 at 8.43.53 AM-yv5zqxyrok.png\" alt=\"Image of three tables that breakdown the construction of the next state from a trace event.\" title=\" \"/\u003e\n\u003c/p\u003e\n\u003cfigcaption\u003e \u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eAnyway, the Python script appends a state to the sequence in Trace.tla:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eTrace == \u0026lt;\u0026lt;\n  \\* ... thousands of events ...\n    \u0026lt;\u0026lt;\u0026#34;BecomePrimary\u0026#34;,                  \\* action name for debugging\n    \u0026lt;\u0026lt;\u0026#34;Follower\u0026#34;,\u0026#34;Leader\u0026#34;,\u0026#34;Follower\u0026#34;\u0026gt;\u0026gt;, \\* role per node\n    \u0026lt;\u0026lt;1, 1, 1\u0026gt;\u0026gt;,                        \\* commitPoint per node\n    \u0026lt;\u0026lt;\u0026lt;\u0026lt;...\u0026gt;\u0026gt;,\u0026lt;\u0026lt;...\u0026gt;\u0026gt;,\u0026lt;\u0026lt;...\u0026gt;\u0026gt;\u0026gt;\u0026gt;,        \\* log per node\n    \\* trace log location, for debugging:\n    \u0026#34;/home/emptysquare/RollbackFuzzer/node2.log:12345\u0026#34;\u0026gt;\u0026gt;,\n  \\* ... thousands more events ...\n\u0026gt;\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe used the Python script to generate a Trace.tla file for each of the hundreds of tests we\u0026#39;d selected: handwritten JavaScript tests and the randomized \u0026#34;rollback fuzzer\u0026#34; test. Now we wanted to use the model-checker to check that this state sequence was permitted by our TLA+ spec, so we know our C++ code behaved in a way that conforms to the spec. Following a \u003ca href=\"https://pron.github.io/files/Trace.pdf\" target=\"_blank\"\u003etechnique published by Ron Pressler\u003c/a\u003e, we added these lines to each Trace.tla:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eVARIABLES log, role, commitPoint\n\\* Instantiate our hand-written spec, RaftMongo.tla.\nModel == INSTANCE RaftMongo\nVARIABLE i \\* the trace index\n\n\\* Load one trace event.\nRead == /\\ log = Trace[i][4]\n        /\\ role = Trace[i][5]\n        /\\ commitPoint = Trace[i][6]\n\nReadNext == /\\ log\u0026#39; = Trace[i\u0026#39;][4]\n            /\\ role\u0026#39; = Trace[i\u0026#39;][5]\n            /\\ commitPoint\u0026#39; = Trace[i\u0026#39;][6]\n\nInit == i = 1 /\\ Read\nNext == \\/ i \u0026lt; Len(Trace) /\\ i\u0026#39; = i + 1 /\\ ReadNext\n        \\/ UNCHANGED \u0026lt;\u0026lt;i, vars\u0026gt;\u0026gt; \\* So that we don’t get a deadlock error in TLC\n\nTraceBehavior == Init /\\ [][Next]_\u0026lt;\u0026lt;vars, i\u0026gt;\u0026gt;\n\n\\* To verify, we check the spec TraceBehavior in TLC, with Model!SpecBehavior\n\\* as a temporal property.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe run the standard TLA+ model-checker (\u0026#34;TLC\u0026#34;), which tells us if this trace is an allowed behavior in RaftMongo.tla.\u003c/p\u003e\n\u003cp\u003eBut this whole experiment failed. Our traces never matched our specification. We didn\u0026#39;t reach our goal, but we learned three lessons that could help future engineers.\u003c/p\u003e\n\u003ch3\u003eWhat disappointment taught us\u003c/h3\u003e\n\u003cp\u003e\u003cb\u003eLesson one: It\u0026#39;s hard to snapshot a multithreaded program\u0026#39;s state.\u003c/b\u003e Each time a MongoDB node executes a state transition, it has to snapshot its state variables in order to log them. MongoDB is highly concurrent with fairly complex locking within each process—it was built to avoid global locking. It took us a month to figure out how to instrument MongoDB to get a consistent snapshot of all these values at one moment. We burned most of our budget for the experiment, and we worried we\u0026#39;d changed MongoDB too much (on a branch) to test it realistically.\u003c/p\u003e\n\u003cp\u003eThe 2024 paper \u0026#34;Validating Traces of Distributed Programs Against TLA+ Specifications\u0026#34; describes how to do trace-checking when you can only log some of the values (see my summary at the bottom of this page). We were aware of this option back in 2020, and we worried it would make trace-checking too permissive; it wouldn\u0026#39;t catch every bug.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eLesson two: The implementation must actually conform to the spec. This is obvious to me now.\u003c/b\u003e After all, conformance checking was the point of the project. In our real-life implementation, when an old leader votes for a new one, first the old leader steps down, then the new leader steps up. The spec we chose for trace-checking wasn\u0026#39;t focused on the election protocol, though, so for simplicity, the spec assumed these two actions happened at once. (Remember I said a few paragraphs ago, \u0026#34;This is a lie\u0026#34;?) Judah and I knew about this discrepancy—we\u0026#39;d deliberately made this simplification in the spec. We tried to paper over the difference with some post-processing in our Python script, but it never worked. By the end of the project, we decided we should have backtracked, making our spec much more complex and realistic, but we\u0026#39;d run out of time.\u003c/p\u003e\n\u003cp\u003eThe eXtreme Modelling methodology says we should write the spec just before the implementation. But our spec was written long after most of the implementation, and it was highly abstract. I can imagine another world where we knew about eXtreme Modelling and TLA+ at the start, when we began coding MongoDB. In that world, we wrote our spec before the implementation, with trace-checking in mind. The spec and implementation would\u0026#39;ve been structured similarly, and this would all have been much easier.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eLesson three: Trace-checking should extend easily to multiple specs.\u003c/b\u003e Judah and I put in 10 weeks of effort without successfully trace-checking one spec, and most of the work was specific to that spec, RaftMongo.tla. Sure, we learned general lessons (you\u0026#39;re reading some of them) and wrote some general code, but even if we\u0026#39;d gotten trace-checking to work for one spec we\u0026#39;d be practically starting over with the next spec. Our original vision was to gather execution traces from all our tests, and trace-check them against all of our specifications, on every git commit. We estimated that the marginal cost of implementing trace-checking for more specs wasn\u0026#39;t worth the marginal value, so we stopped the project.\u003c/p\u003e\n\u003ch3\u003ePractical trace-checking\u003c/h3\u003e\n\u003cp\u003eIf we started again, we\u0026#39;d do it differently. We\u0026#39;d ensure the spec and implementation conform at the start, and we\u0026#39;d fix discrepancies by fixing the spec or the implementation right away. We\u0026#39;d model easily observed events like network messages, to avoid snapshotting the internal state of a multithreaded process.\u003c/p\u003e\n\u003cp\u003eI still think trace-checking is worthwhile. I know it\u0026#39;s worked for other projects. In fact MongoDB is sponsoring a grad student \u003ca href=\"https://fhackett.com/\" target=\"_blank\"\u003eFinn Hackett\u003c/a\u003e, whom I\u0026#39;m mentoring, to continue trace-checking research.\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s move on to the second half of our project.\u003c/p\u003e\n\u003ch2\u003eTest-case generation for MongoDB Mobile SDK\u003c/h2\u003e\n\u003cp\u003eThe MongoDB Mobile SDK is a database for mobile devices that syncs with a central server (since we wrote the paper, MongoDB has \u003ca href=\"https://www.mongodb.com/resources/resources/resource-library/docs-atlas-device-sync-getting-started?xs=494444\" target=\"_blank\"\u003esunsetted the product\u003c/a\u003e). Mobile clients can make changes locally. These changes are periodically uploaded to the server and downloaded by other clients. The clients and the server all use the same algorithm to resolve write conflicts: \u003ca href=\"https://en.wikipedia.org/wiki/Operational_transformation\" target=\"_blank\"\u003eOperational Transformation\u003c/a\u003e, or OT. Max wanted to test that the clients and server implement OT correctly, meaning they resolve conflicts the same way, eventually resulting in identical data everywhere.\u003c/p\u003e\n\u003cp\u003eOriginally, the clients and server shared one C++ implementation of OT, so we knew they implemented the same algorithm. But in 2020, we\u0026#39;d recently rewritten the server in Go, so testing their conformance became urgent.\u003c/p\u003e\n\u003ccenter\u003e\u003cb\u003eFigure 7.\u003c/b\u003e MongoDB mobile SDK.\u003c/center\u003e\n\u003cfigure\u003e\n\u003cp\u003e\u003cimg src=\"https://webassets.mongodb.com/_com_assets/cms/Screenshot 2025-06-02 at 8.48.39 AM-uhpaka48xm.png\" alt=\"Diagram showing how mobile C++ flows into and from the server Go. \" title=\" \"/\u003e\n\u003c/p\u003e\n\u003cfigcaption\u003e \u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eMy colleague Max Hirschhorn used test-case generation to check conformance. This technique goes in the opposite direction from trace-checking: trace-checking starts with an implementation and checks that its behaviors are allowed by the spec, but test-case generation starts with a spec and checks that its behaviors are in the implementation.\u003c/p\u003e\n\u003cp\u003eBut first, we needed a TLA+ spec. Before this project, the mobile team had written out the OT algorithm in English and implemented it in C++. Max manually translated the algorithm from C++ to TLA+. In the mobile SDK, clients can do 19 kinds of operations on data; six of these can be performed on arrays, resulting in 21 array merge rules, which are implemented in about 1000 lines of C++. Those 21 rules are the most complex, and Max focused his specification there. He used the model-checker to verify that his TLA+ spec ensured all participants eventually had the same data. This translation was a gruelling job, but the model-checker caught Max\u0026#39;s mistakes quickly, and he finished in two weeks.\u003c/p\u003e\n\u003cp\u003eThere was one kind of write conflict that crashed the model-checker: if one participant swapped two array elements, and another moved an element, then the model-checker crashed with a Java StackOverflowError. Surprisingly, this was an actual infinite-recursion bug in the algorithm. Max verified that the bug was in the C++ code. It had hidden there until he faithfully transcribed it into TLA+ and discovered it with the model-checker. He disabled the element-swap operation in his TLA+ spec, and the mobile team deprecated it in their implementation.\u003c/p\u003e\n\u003cp\u003eTo test conformance, Max used the model-checker to output the entire state graph for the spec. He constrained the algorithm to three participants, all editing a three-element array, each executing one (possibly conflicting) write operation. With these constraints, the state space is a DAG, with a finite number of behaviors (paths from an initial state to a final state). There are 30,184 states and 4913 behaviors. Max wrote a Go program to parse the model-checker\u0026#39;s output and write out a C++ unit test for each behavior.\u003c/p\u003e\n\u003cp\u003eHere’s an example unit test. (It\u0026#39;s edited down from three participants to two.) At the start, there\u0026#39;s an array containing {1, 2, 3}. One client sets the third element of an array to 4 and the second client removes the second element from the array. The test asserts that both clients agree the final array is {1, 4}. The bold parts are specific to this generated test. The rest of the code is the same for all tests.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eTEST(Transform_Array)\n{\n  size_t num_clients = 2;\n  TransformArrayFixture fixture{test_context, num_clients, {1, 2, 3}};\n\n  fixture.transaction(0, [](TableRef array) {\n    array-\u0026gt;set_int(0, 2, 4);\n  });\n  fixture.transaction(1, [](TableRef array) {\n    array-\u0026gt;remove(1);\n  });\n\n  fixture.sync_all_clients();\n  fixture.check_array({1, 4});\n\n  fixture.check_ops(0, {ArrayErase{1}});\n  fixture.check_ops(1, {ArraySet{1, 4}});\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese 4913 tests immediately achieved 100% branch coverage of the implementation, which we hadn\u0026#39;t accomplished with our handwritten tests (21%) or millions of executions with the \u003ca href=\"https://lcamtuf.coredump.cx/afl/\" target=\"_blank\"\u003eAFL fuzzer\u003c/a\u003e (92%).\u003c/p\u003e\n\u003ch2\u003eRetrospective\u003c/h2\u003e\n\u003cp\u003eMax\u0026#39;s test-case generation worked quite well. He discovered a bug in the algorithm, and he thoroughly checked that the mobile SDK\u0026#39;s Operational Transformation code conforms to the spec. Judah\u0026#39;s and my trace-checking experiment didn\u0026#39;t work: our spec and code were too far apart, and adding tracing to MongoDB took too long. Both techniques can work, given the right circumstances and strategy. Both techniques can fail, too! We published our results and lessons as a paper in VLDB 2020, titled \u0026#34;\u003ca href=\"https://arxiv.org/abs/2006.00915\" target=\"_blank\"\u003eeXtreme Modelling in Practice\u003c/a\u003e.\u0026#34;\u003c/p\u003e\n\u003cp\u003eIn the subsequent five years, I\u0026#39;ve seen some progress in conformance checking techniques.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eTest-case generation:\u003c/b\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cb\u003e\u003ca href=\"https://dl.acm.org/doi/abs/10.1145/3552326.3587442\" target=\"_blank\"\u003eModel Checking Guided Testing for Distributed Systems\u003c/a\u003e\u003c/b\u003e. The \u0026#34;Mocket\u0026#34; system generates tests from a TLA+ spec, and instruments Java code (with a fair amount of human labor) to force it to deterministically follow each test, and check that its variables have the same values as the spec after each action. The authors tested the conformance of three Java distributed systems and found some new bugs. Their technique is Java-specific but could be adapted for other languages.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cb\u003e\u003ca href=\"https://muratbuffalo.blogspot.com/2025/04/multi-grained-specifications-for.html\" target=\"_blank\"\u003eMulti-Grained Specifications for Distributed System Model Checking and Verification\u003c/a\u003e\u003c/b\u003e. The authors wrote several new TLA+ specs of Zookeeper, at higher and lower levels of abstraction. They checked conformance between the most concrete specs and the implementation, with a technique similar to Mocket: a human programmer instruments some Java code to map Java variables to spec variables, and to make all interleavings deterministic. The model-checker randomly explores spec behaviors, while the test framework checks that the Java code can follow the same behaviors.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cb\u003e\u003ca href=\"https://github.com/tangruize/SandTable/blob/main/doc/SandTable-Paper.pdf\" target=\"_blank\"\u003eSandTable: Scalable Distributed System Model Checking with Specification-Level State Exploration\u003c/a\u003e\u003c/b\u003e. This system is not language-specific: it overrides system calls to control nondeterminism and force the implementation to follow each behavior of the spec. It samples the spec\u0026#39;s state space to maximize branch coverage and event diversity while minimizing the length of each behavior. As in the \u0026#34;Multi-Grained\u0026#34; paper, the SandTable authors wisely developed new TLA+ specs that closely matched the implementations they were testing, rather than trying to use existing, overly abstract specs like Judah and I did.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePlus, my colleagues \u003ca href=\"https://will62794.github.io/\" target=\"_blank\"\u003eWill Schultz\u003c/a\u003e and \u003ca href=\"http://muratbuffalo.blogspot.com/\" target=\"_blank\"\u003eMurat Demirbas\u003c/a\u003e are publishing a paper in \u003ca href=\"https://vldb.org/2025/\" target=\"_blank\"\u003eVLDB 2025\u003c/a\u003e that uses test-case generation with a new TLA+ spec of MongoDB\u0026#39;s WiredTiger storage layer, the paper is titled \u0026#34;Design and Modular Verification of  Distributed Transactions in MongoDB.\u0026#34;\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cb\u003eTrace-checking:\u003c/b\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cb\u003e\u003ca href=\"https://dariusf.github.io/cpluscal.pdf\" target=\"_blank\"\u003eProtocol Conformance with Choreographic PlusCal\u003c/a\u003e\u003c/b\u003e. The authors write new specs in an extremely high-level language that compiles to TLA+. From their specs they generate Go functions for trace-logging, which they manually add to existing Go programs. They check that the resulting traces are valid spec behaviors and find some bugs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cb\u003e\u003ca href=\"http://arxiv.org/pdf/2404.16075v2\" target=\"_blank\"\u003eValidating Traces of Distributed Programs Against TLA+ Specifications\u003c/a\u003e\u003c/b\u003e. Some veteran TLA+ experts demonstrate in detail how to trace-log from a Java program and validate the traces with TLC, the TLA+ model-checker. They\u0026#39;ve written small libraries and added TLC features for convenience. This paper focuses on validating incomplete traces: if you can only log some of the variables, TLC will infer the rest.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cb\u003e\u003ca href=\"https://muratbuffalo.blogspot.com/2025/02/smart-casual-verification-of.html\" target=\"_blank\"\u003eSmart Casual Verification of the Confidential Consortium Framework\u003c/a\u003e\u003c/b\u003e. The authors started with an existing implementation of a secure consensus protocol. Their situation was like mine in 2020 (new specs of a big old C++ program) and so was their goal: to continuously check conformance and keep the spec and implementation in sync. Using the \u003ca href=\"https://groups.google.com/d/msgid/tlaplus/2443fd1a-1c35-419a-95b8-72de361f28bdn%40googlegroups.com\" target=\"_blank\"\u003enew TLC features\u003c/a\u003e announced in the \u0026#34;Validating Traces\u0026#34; paper above, they toiled for months, brought their specs and code into line, found some bugs, and realized the eXtreme Modelling vision.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://fhackett.com/\" target=\"_blank\"\u003eFinn Hackett\u003c/a\u003e is a PhD student I\u0026#39;m mentoring, he\u0026#39;s developed a \u003ca href=\"https://www.cs.ubc.ca/~bestchai/papers/asplos23-pgo.pdf\" target=\"_blank\"\u003eTLA+-to-Go compiler\u003c/a\u003e. He\u0026#39;s now prototyping a trace-checker to verify that the Go code he produces really conforms to its source spec. We\u0026#39;re doing a summer project together with \u003ca href=\"https://antithesis.com/\" target=\"_blank\"\u003eAntithesis\u003c/a\u003e to thoroughly conformance-check the implementation\u0026#39;s state space.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI\u0026#39;m excited to see growing interest in conformance checking, because I think it\u0026#39;s a serious problem that needs to be solved before TLA+ goes mainstream. The \u0026#34;Validating Traces\u0026#34; paper announced some new trace-checking features in TLC, and TLC\u0026#39;s developers are \u003ca href=\"https://github.com/tlaplus/tlaplus/issues/1073\" target=\"_blank\"\u003ediscussing a better way to export a state graph for test-case generation\u003c/a\u003e. I hope these research prototypes lead to standard tools, so engineers can keep their code and specs in sync.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eJoin our \u003ca href=\"https://www.mongodb.com/community/?tck=extreme_modeling_blog_6_2\"\u003eMongoDB Community\u003c/a\u003e to learn about upcoming events, hear stories from MongoDB users, and connect with community members from around the world.\u003c/b\u003e\u003c/p\u003e\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "25 min read",
  "publishedTime": "2025-06-02T18:22:28.767Z",
  "modifiedTime": null
}
