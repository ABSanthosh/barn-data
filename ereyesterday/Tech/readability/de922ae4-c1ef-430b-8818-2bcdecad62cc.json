{
  "id": "de922ae4-c1ef-430b-8818-2bcdecad62cc",
  "title": "ETH Zurich and EPFL to release a LLM developed on public infrastructure",
  "link": "https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html",
  "description": "Comments",
  "author": "",
  "published": "Fri, 11 Jul 2025 18:45:10 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 4958,
  "excerpt": "ETH Zurich and EPFL will release a large language model (LLM) developed on public infrastructure. Trained on the “Alps” supercomputer at the Swiss National Supercomputing Centre (CSCS), the new LLM marks a milestone in open-source AI and multilingual excellence.",
  "siteName": "ETH Zurich",
  "favicon": "https://ethz.ch/etc/designs/ethz/img/icons/ETH-APP-Icons-Theme-white/192-xxxhpdi.png",
  "text": "Earlier this week in Geneva, around 50 leading global initiatives and organisations dedicated to open-source LLMs and trustworthy AI convened at the International Open-Source LLM Builders Summit. Hosted by the AI centres of EPFL and ETH Zurich, the event marked a significant step in building a vibrant and collaborative international ecosystem for open foundation models. Open LLMs are increasingly viewed as credible alternatives to commercial systems, most of which are developed behind closed doors in the United States or China. Participants of the summit previewed the forthcoming release of a fully open, publicly developed LLM — co-created by researchers at EPFL, ETH Zurich and other Swiss universities in close collaboration with engineers at CSCS. Currently in final testing, the model will be downloadable under an open license. The model focuses on transparency, multilingual performance, and broad accessibility. The model will be fully open: source code and weights will be publicly available, and the training data will be transparent and reproducible, supporting adoption across science, government, education, and the private sector. This approach is designed to foster both innovation and accountability. “Fully open models enable high-trust applications and are necessary for advancing research about the risks and opportunities of AI. Transparent processes also enable regulatory compliance,” says Imanol Schlag, research scientist at the ETH AI Center, who is leading the effort alongside EPFL AI Center faculty members and professors Antoine Bosselut and Martin Jaggi. Multilingual by design A defining characteristic of the LLM is its fluency in over 1000 languages. “We have emphasised making the models massively multilingual from the start,” says Antoine Bosselut. Training of the base model was done on a large text dataset in over 1500 languages — approximately 60% English and 40% non-English languages — as well as code and mathematics data. Given the representation of content from all languages and cultures, the resulting model maintains the highest global applicability. Designed for scale and inclusion The model will be released in two sizes — 8 billion and 70 billion parameters, meeting a broad range of users’ needs. The 70B version will rank among the most powerful fully open models worldwide. The number of parameters reflects a model’s capacity to learn and generate complex responses. High reliability is achieved through training on over 15 trillion high-quality training tokens (units representing a word or part of the word), enabling robust language understanding and versatile use cases. Responsible data practices The LLM is being developed with due consideration to Swiss data protection laws, Swiss copyright laws, and the transparency obligations under the EU AI Act. In a external page recent study, the project leaders demonstrated that for most everyday tasks and general knowledge acquisition, respecting web crawling opt-outs during data acquisition produces virtually no performance degradation. Supercomputer as an enabler of sovereign AI The model is trained on the “Alps” supercomputer at CSCS in Lugano, one of the world’s most advanced AI platforms, equipped with over 10,000 NVIDIA Grace Hopper Superchips. The system’s scale and architecture made it possible to train the model efficiently using 100% carbon-neutral electricity. The successful realisation of “Alps” was significantly facilitated by a long-standing collaboration spanning over 15 years with NVDIA and HPE/Cray. This partnership has been pivotal in shaping the capabilities of “Alps”, ensuring it meets the demanding requirements of large-scale AI workloads, including the pre-training of complex LLMs. “Training this model is only possible because of our strategic investment in ‘Alps’, a supercomputer purpose-built for AI,” says Thomas Schulthess, Director of CSCS and professor at ETH Zurich. “Our enduring collaboration with NVIDIA and HPE exemplifies how joint efforts between public research institutions and industry leaders can drive sovereign infrastructure, fostering open innovation — not just for Switzerland, but for science and society worldwide.” Public access and global reuse In late summer, the LLM will be released under the Apache 2.0 License. Accompanying documentation will detail the model architecture, training methods, and usage guidelines to enable transparent reuse and further development. “As scientists from public institutions, we aim to advance open models and enable organiations to build on them for their own applications”, says Antoine Bosselut. “By embracing full openness — unlike commercial models that are developed behind closed doors — we hope that our approach will drive innovation in Switzerland, across Europe, and through multinational collaborations. Furthermore, it is a key factor in attracting and nurturing top talent,” says EPFL professor Martin Jaggi.",
  "image": "https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good/_jcr_content/pageimages/imageCarousel.imageformat.lightbox.1905958724.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                \n                \n                \u003cp\u003eEarlier this week in Geneva, around 50 leading global initiatives and organisations dedicated to open-source LLMs and trustworthy AI convened at the \u003ci\u003eInternational Open-Source LLM Builders Summit\u003c/i\u003e. Hosted by the AI centres of EPFL and ETH Zurich, the event marked a significant step in building a vibrant and collaborative international ecosystem for open foundation models. Open LLMs are increasingly viewed as credible alternatives to commercial systems, most of which are developed behind closed doors in the United States or China.\u003c/p\u003e \n\u003cp\u003eParticipants of the summit previewed the forthcoming release of a fully open, publicly developed LLM — co-created by researchers at EPFL, ETH Zurich and other Swiss universities in close collaboration with engineers at CSCS. Currently in final testing, the model will be downloadable under an open license. The model focuses on transparency, multilingual performance, and broad accessibility.\u003c/p\u003e \n\u003cp\u003eThe model will be fully open: source code and weights will be publicly available, and the training data will be transparent and reproducible, supporting adoption across science, government, education, and the private sector. This approach is designed to foster both innovation and accountability.\u003c/p\u003e \n\u003cp\u003e“Fully open models enable high-trust applications and are necessary for advancing research about the risks and opportunities of AI. Transparent processes also enable regulatory compliance,” says Imanol Schlag, research scientist at the ETH AI Center, who is leading the effort alongside EPFL AI Center faculty members and professors Antoine Bosselut and Martin Jaggi.\u003c/p\u003e \n\u003ch2\u003e\u003cb\u003eMultilingual by design\u003c/b\u003e\u003c/h2\u003e \n\u003cp\u003eA defining characteristic of the LLM is its fluency in over 1000 languages. “We have emphasised making the models massively multilingual from the start,” says Antoine Bosselut.\u003c/p\u003e \n\u003cp\u003eTraining of the base model was done on a large text dataset in over 1500 languages — approximately 60% English and 40% non-English languages — as well as code and mathematics data. Given the representation of content from all languages and cultures, the resulting model maintains the highest global applicability.\u003c/p\u003e \n\u003ch2\u003e\u003cb\u003eDesigned for scale and inclusion\u003c/b\u003e\u003c/h2\u003e \n\u003cp\u003eThe model will be released in two sizes — 8 billion and 70 billion parameters, meeting a broad range of users’ needs. The 70B version will rank among the most powerful fully open models worldwide. The number of parameters reflects a model’s capacity to learn and generate complex responses.\u003c/p\u003e \n\u003cp\u003eHigh reliability is achieved through training on over 15 trillion high-quality training tokens (units representing a word or part of the word), enabling robust language understanding and versatile use cases.\u003c/p\u003e \n\u003ch2\u003e\u003cb\u003eResponsible data practices\u003c/b\u003e\u003c/h2\u003e \n\u003cp\u003eThe LLM is being developed with due consideration to Swiss data protection laws, Swiss copyright laws, and the transparency obligations under the EU AI Act. In a \u003ca href=\"https://arxiv.org/abs/2504.06219\"\u003e\u003cspan\u003eexternal page \u003c/span\u003erecent study\u003c/a\u003e, the project leaders demonstrated that for most everyday tasks and general knowledge acquisition, respecting web crawling opt-outs during data acquisition\u003cb\u003e\u003c/b\u003e produces virtually no performance degradation.\u003c/p\u003e \n\u003ch2\u003e\u003cb\u003eSupercomputer as an enabler of sovereign AI\u003c/b\u003e\u003c/h2\u003e \n\u003cp\u003eThe model is trained on the “Alps” supercomputer at CSCS in Lugano, one of the world’s most advanced AI platforms, equipped with over 10,000 NVIDIA Grace Hopper Superchips. The system’s scale and architecture made it possible to train the model efficiently using 100% carbon-neutral electricity.\u003c/p\u003e \n\u003cp\u003eThe successful realisation of “Alps” was significantly facilitated by a long-standing collaboration spanning over 15 years with NVDIA and HPE/Cray. This partnership has been pivotal in shaping the capabilities of “Alps”, ensuring it meets the demanding requirements of large-scale AI workloads, including the pre-training of complex LLMs.\u003c/p\u003e \n\u003cp\u003e“Training this model is only possible because of our strategic investment in ‘Alps’, a supercomputer purpose-built for AI,” says Thomas Schulthess, Director of CSCS and professor at ETH Zurich. “Our enduring collaboration with NVIDIA and HPE exemplifies how joint efforts between public research institutions and industry leaders can drive sovereign infrastructure, fostering open innovation — not just for Switzerland, but for science and society worldwide.”\u003c/p\u003e \n\u003ch2\u003e\u003cb\u003ePublic access and global reuse\u003c/b\u003e\u003c/h2\u003e \n\u003cp\u003eIn late summer, the LLM will be released under the Apache 2.0 License. Accompanying documentation will detail the model architecture, training methods, and usage guidelines to enable transparent reuse and further development.\u003c/p\u003e \n\u003cp\u003e“As scientists from public institutions, we aim to advance open models and enable organiations to build on them for their own applications”, says Antoine Bosselut.\u003c/p\u003e \n\u003cp\u003e“By embracing full openness — unlike commercial models that are developed behind closed doors — we hope that our approach will drive innovation in Switzerland, across Europe, and through multinational collaborations. Furthermore, it is a key factor in attracting and nurturing top talent,” says EPFL professor Martin Jaggi.\u003c/p\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
