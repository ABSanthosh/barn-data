{
  "id": "f9e8f443-77ed-468f-a55a-fb1c93695d14",
  "title": "12 days of OpenAI: The Ars Technica recap",
  "link": "https://arstechnica.com/information-technology/2024/12/12-days-of-openai-the-ars-technica-recap/",
  "description": "Did OpenAI's big holiday event live up to the billing?",
  "author": "Benj Edwards",
  "published": "Fri, 20 Dec 2024 22:01:34 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "12 days of OpenAI",
    "ChatGPT",
    "chatgtp",
    "machine learning",
    "openai"
  ],
  "byline": "Benj Edwards",
  "length": 13474,
  "excerpt": "Did OpenAI’s big holiday event live up to the billing?",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Did OpenAI's big holiday event live up to the billing? Over the past 12 business days, OpenAI has announced a new product or demoed an AI feature every weekday, calling the PR event \"12 days of OpenAI.\" We've covered some of the major announcements, but we thought a look at each announcement might be useful for people seeking a comprehensive look at each day's developments. The timing and rapid pace of these announcements—particularly in light of Google's competing releases—illustrates the intensifying competition in AI development. What might normally have been spread across months was compressed into just 12 business days, giving users and developers a lot to process as they head into 2025. Humorously, we asked ChatGPT what it thought about the whole series of announcements, and it was skeptical that the event even took place. \"The rapid-fire announcements over 12 days seem plausible,\" wrote ChatGPT-4o, \"But might strain credibility without a clearer explanation of how OpenAI managed such an intense release schedule, especially given the complexity of the features.\" But it did happen, and here's a chronicle of what went down on each day. Day 1: Thursday, December 5 On the first day of OpenAI, the company released its full o1 model, making it available to ChatGPT Plus and Team subscribers worldwide. The company reported that the model operates faster than its preview version and reduces major errors by 34 percent on complex real-world questions. The o1 model brings new capabilities for image analysis, allowing users to upload and receive detailed explanations of visual content. OpenAI said it plans to expand o1's features to include web browsing and file uploads in ChatGPT, with API access coming soon. The API version will support vision tasks, function calling, and structured outputs for system integration. OpenAI also launched ChatGPT Pro, a $200 subscription tier that provides \"unlimited\" access to o1, GPT-4o, and Advanced Voice features. Pro subscribers receive an exclusive version of o1 that uses additional computing power for complex problem-solving. Alongside this release, OpenAI announced a grant program that will provide ChatGPT Pro access to 10 medical researchers at established institutions, with plans to extend grants to other fields. Day 2: Friday, December 6 Day 2 wasn't as exciting. OpenAI unveiled Reinforcement Fine-Tuning (RFT), a model customization method that will let developers modify \"o-series\" models for specific tasks. The technique reportedly goes beyond traditional supervised fine-tuning by using reinforcement learning to help models improve their reasoning abilities through repeated iterations. In other words, OpenAI created a new way to train AI models that lets them learn from practice and feedback. OpenAI says that Berkeley Lab computational researcher Justin Reese tested RFT for researching rare genetic diseases, while Thomson Reuters has created a specialized o1-mini model for its CoCounsel AI legal assistant. The technique requires developers to provide a dataset and evaluation criteria, with OpenAI's platform managing the reinforcement learning process. OpenAI plans to release RFT to the public in early 2024 but currently offers limited access through its Reinforcement Fine-Tuning Research Program for researchers, universities, and companies. Day 3: Monday, December 9 On day 3, OpenAI released Sora, its text-to-video model, as a standalone product now accessible through sora.com for ChatGPT Plus and Pro subscribers. The company says the new version operates faster than the research preview shown in February 2024, when OpenAI first demonstrated the model's ability to create videos from text descriptions. The release moved Sora from research preview to a production service, marking OpenAI's official entry into the video synthesis market. The company published a blog post detailing the subscription tiers and deployment strategy for the service. Day 4: Tuesday, December 10 On day 4, OpenAI moved its Canvas feature out of beta testing, making it available to all ChatGPT users, including those on free tiers. Canvas provides a dedicated interface for extended writing and coding projects beyond the standard chat format, now with direct integration into the GPT-4o model. The updated canvas allows users to run Python code within the interface and includes a text-pasting feature for importing existing content. OpenAI added compatibility with custom GPTs and a \"show changes\" function that tracks modifications to writing and code. The company said Canvas is now on chatgpt.com for web users and also available through a Windows desktop application, with more features planned for future updates. Day 5: Wednesday, December 11 On day 5, OpenAI announced that ChatGPT would integrate with Apple Intelligence across iOS, iPadOS, and macOS devices. The integration works on iPhone 16 series phones, iPhone 15 Pro models, iPads with A17 Pro or M1 chips and later, and Macs with M1 processors or newer, running their respective latest operating systems. The integration lets users access ChatGPT's features (such as they are), including image and document analysis, directly through Apple's system-level intelligence features. The feature works with all ChatGPT subscription tiers and operates within Apple's privacy framework. Iffy message summaries remain unaffected by the additions. Enterprise and Team account users need administrator approval to access the integration. Day 6: Thursday, December 12 On the sixth day, OpenAI added two features to ChatGPT's voice capabilities: \"video calling\" with screen sharing support for ChatGPT Plus and Pro subscribers and a seasonal Santa Claus voice preset. The new visual Advanced Voice Mode features work through the mobile app, letting users show their surroundings or share their screen with the AI model during voice conversations. While the rollout covers most countries, users in several European nations, including EU member states, Switzerland, Iceland, Norway, and Liechtenstein, will get access at a later date. Enterprise and education users can expect these features in January. The Santa voice option appears as a snowflake icon in the ChatGPT interface across mobile devices, web browsers, and desktop apps, with conversations in this mode not affecting chat history or memory. Don't expect Santa to remember what you want for Christmas between sessions. Day 7: Friday, December 13 OpenAI introduced Projects, a new organizational feature in ChatGPT that lets users group related conversations and files, on day 7. The feature works with the company's GPT-4o model and provides a central location for managing resources related to specific tasks or topics—kinda like Anthropic's \"Projects\" feature. ChatGPT Plus, Pro, and Team subscribers can currently access Projects through chatgpt.com and the Windows desktop app, with view-only support on mobile devices and macOS. Users can create projects by clicking a plus icon in the sidebar, where they can add files and custom instructions that provide context for future conversations. OpenAI said it plans to expand Projects in 2024 with support for additional file types, cloud storage integration through Google Drive and Microsoft OneDrive, and compatibility with other models like o1. Enterprise and education users will receive access to Projects in January. Day 8: Monday, December 16 On day 8, OpenAI expanded its search features in ChatGPT, extending access to all users with free accounts while reportedly adding speed improvements and mobile optimizations. Basically, you can use ChatGPT like a web search engine, although in practice it doesn't seem to be as comprehensive as Google Search at the moment. The update includes a new maps interface and integration with Advanced Voice, allowing users to perform searches during voice conversations. The search capability, which previously required a paid subscription, now works across all platforms where ChatGPT operates. Day 9: Tuesday, December 17 On day 9, OpenAI released its o1 model through its API platform, adding support for function calling, developer messages, and vision processing capabilities. The company also reduced GPT-4o audio pricing by 60 percent and introduced a GPT-4o mini option that costs one-tenth of previous audio rates. OpenAI also simplified its WebRTC integration for real-time applications and unveiled Preference Fine-Tuning, which provides developers new ways to customize models. The company also launched beta versions of software development kits for the Go and Java programming languages, expanding its toolkit for developers. Day 10: Wednesday, December 18 On Wednesday, OpenAI did something a little fun and launched voice and messaging access to ChatGPT through a toll-free number (1-800-CHATGPT), as well as WhatsApp. US residents can make phone calls with a 15-minute monthly limit, while global users can message ChatGPT through WhatsApp at the same number. OpenAI said the release is a way to reach users who lack consistent high-speed Internet access or want to try AI through familiar communication channels, but it's also just a clever hack. As evidence, OpenAI notes that these new interfaces serve as experimental access points, with more \"limited functionality\" than the full ChatGPT service, and still recommends existing users continue using their regular ChatGPT accounts for complete features. Day 11: Thursday, December 19 On Thursday, OpenAI expanded ChatGPT's desktop app integration to include additional coding environments and productivity software. The update added support for Jetbrains IDEs like PyCharm and IntelliJ IDEA, VS Code variants including Cursor and VSCodium, and text editors such as BBEdit and TextMate. OpenAI also included integration with Apple Notes, Notion, and Quip while adding Advanced Voice Mode compatibility when working with desktop applications. These features require manual activation for each app and remain available to paid subscribers, including Plus, Pro, Team, Enterprise, and Education users, with Enterprise and Education customers needing administrator approval to enable the functionality. Day 12: Friday, December 20 On Friday, OpenAI concluded its twelve days of announcements by previewing two new simulated reasoning models, o3 and o3-mini, while opening applications for safety and security researchers to test them before public release. Early evaluations show o3 achieving a 2727 rating on Codeforces programming contests and scoring 96.7 percent on AIME 2024 mathematics problems. The company reports o3 set performance records on advanced benchmarks, solving 25.2 percent of problems on EpochAI's Frontier Math evaluations and scoring above 85 percent on the ARC-AGI test, which is comparable to human results. OpenAI also published research about \"deliberative alignment,\" a technique used in developing o1. The company has not announced firm release dates for either new o3 model, but CEO Sam Altman said o3-mini might ship in late January. So what did we learn? OpenAI's December campaign revealed that OpenAI had a lot of things sitting around that it needed to ship, and it picked a fun theme to unite the announcements. Google responded in kind, as we have covered. Several trends from the releases stand out. OpenAI is heavily investing in multimodal capabilities. The o1 model's release, Sora's evolution from research preview to product, and the expansion of voice features with video calling all point toward systems that can seamlessly handle text, images, voice, and video. The company is also focusing heavily on developer tools and customization, so it can continue to have a cloud service business and have its products integrated into other applications. Between the API releases, Reinforcement Fine-Tuning, and expanded IDE integrations, OpenAI is building out its ecosystem for developers and enterprises. And the introduction of o3 shows that OpenAI is still attempting to push technological boundaries, even in the face of diminishing returns in training LLM base models. OpenAI seems to be positioning itself for a 2025 where generative AI moves beyond text chatbots and simple image generators and finds its way into novel applications that we probably can't even predict yet. We'll have to wait and see what the company and developers come up with in the year ahead. Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC. 15 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/12_days_robot_sleigh-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n            \u003carticle data-id=\"2067177\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n      \n\n      \n\n      \u003cp\u003e\n        Did OpenAI\u0026#39;s big holiday event live up to the billing?\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \n\u003cp\u003eOver the past 12 business days, OpenAI has announced a new product or demoed an AI feature every weekday, calling the PR event \u0026#34;\u003ca href=\"https://arstechnica.com/ai/2024/12/openai-teases-12-days-of-mystery-product-launches-starting-tomorrow/\"\u003e12 days of OpenAI\u003c/a\u003e.\u0026#34; We\u0026#39;ve covered some of the major announcements, but we thought a look at each announcement might be useful for people seeking a comprehensive look at each day\u0026#39;s developments.\u003c/p\u003e\n\u003cp\u003eThe timing and rapid pace of these announcements—particularly in light of \u003ca href=\"https://arstechnica.com/information-technology/2024/12/google-and-openai-blitz-december-with-so-many-ai-releases-its-hard-to-keep-up/\"\u003eGoogle\u0026#39;s competing releases\u003c/a\u003e—illustrates the intensifying competition in AI development. What might normally have been spread across months was compressed into just 12 business days, giving users and developers a lot to process as they head into 2025.\u003c/p\u003e\n\u003cp\u003eHumorously, we asked ChatGPT what it thought about the whole series of announcements, and it was skeptical that the event even took place. \u0026#34;The rapid-fire announcements over 12 days seem plausible,\u0026#34; wrote ChatGPT-4o, \u0026#34;But might strain credibility without a clearer explanation of how OpenAI managed such an intense release schedule, especially given the complexity of the features.\u0026#34;\u003c/p\u003e\n\u003cp\u003eBut it did happen, and here\u0026#39;s a chronicle of what went down on each day.\u003c/p\u003e\n\u003ch2\u003eDay 1: Thursday, December 5\u003c/h2\u003e\n\u003cp\u003eOn the first day of OpenAI, the company \u003ca href=\"https://arstechnica.com/ai/2024/12/openais-new-200-mo-chatgpt-subscription-will-buy-you-more-compute-time/\"\u003ereleased its full o1 model\u003c/a\u003e, making it available to ChatGPT Plus and Team subscribers worldwide. The company reported that the model operates faster than its preview version and reduces major errors by 34 percent on complex real-world questions.\u003c/p\u003e\n\u003cp\u003eThe o1 model brings new capabilities for image analysis, allowing users to upload and receive detailed explanations of visual content. OpenAI said it plans to expand o1\u0026#39;s features to include web browsing and file uploads in ChatGPT, with API access coming soon. The API version will support vision tasks, function calling, and structured outputs for system integration.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eOpenAI also launched ChatGPT Pro, a $200 subscription tier that provides \u0026#34;unlimited\u0026#34; access to o1, GPT-4o, and Advanced Voice features. Pro subscribers receive an exclusive version of o1 that uses additional computing power for complex problem-solving. Alongside this release, OpenAI announced a grant program that will provide ChatGPT Pro access to 10 medical researchers at established institutions, with plans to extend grants to other fields.\u003c/p\u003e\n\u003ch2\u003eDay 2: Friday, December 6\u003c/h2\u003e\n\u003cp\u003eDay 2 wasn\u0026#39;t as exciting. OpenAI unveiled Reinforcement Fine-Tuning (RFT), a model customization method that will let developers modify \u0026#34;o-series\u0026#34; models for specific tasks. The technique reportedly goes beyond traditional supervised fine-tuning by using reinforcement learning to help models improve their reasoning abilities through repeated iterations. In other words, OpenAI created a new way to train AI models that lets them learn from practice and feedback.\u003c/p\u003e\n\u003cp\u003eOpenAI says that Berkeley Lab computational researcher Justin Reese tested RFT for researching rare genetic diseases, while Thomson Reuters has created a specialized o1-mini model for its CoCounsel AI legal assistant. The technique requires developers to provide a dataset and evaluation criteria, with OpenAI\u0026#39;s platform managing the reinforcement learning process.\u003c/p\u003e\n\u003cp\u003eOpenAI plans to release RFT to the public in early 2024 but currently offers limited access through its Reinforcement Fine-Tuning Research Program for researchers, universities, and companies.\u003c/p\u003e\n\u003ch2\u003eDay 3: Monday, December 9\u003c/h2\u003e\n\u003cp\u003eOn day 3, OpenAI \u003ca href=\"https://arstechnica.com/ai/2024/12/ten-months-after-first-tease-openai-launches-sora-video-generation-publicly/\"\u003ereleased Sora\u003c/a\u003e, its text-to-video model, as a standalone product now accessible through sora.com for ChatGPT Plus and Pro subscribers. The company says the new version operates faster than the research preview shown in February 2024, when OpenAI first demonstrated the model\u0026#39;s ability to create videos from text descriptions.\u003c/p\u003e\n\u003cp\u003eThe release moved Sora from research preview to a production service, marking OpenAI\u0026#39;s official entry into the video synthesis market. The company published \u003ca href=\"https://openai.com/sora/\"\u003ea blog post\u003c/a\u003e detailing the subscription tiers and deployment strategy for the service.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003ch2\u003eDay 4: Tuesday, December 10\u003c/h2\u003e\n\u003cp\u003eOn day 4, OpenAI moved its \u003ca href=\"https://arstechnica.com/ai/2024/10/openais-canvas-eases-collaborations-with-chatgpt/\"\u003eCanvas feature\u003c/a\u003e out of beta testing, making it available to all ChatGPT users, including those on free tiers. Canvas provides a dedicated interface for extended writing and coding projects beyond the standard chat format, now with direct integration into the GPT-4o model.\u003c/p\u003e\n\u003cp\u003eThe updated canvas allows users to run Python code within the interface and includes a text-pasting feature for importing existing content. OpenAI added compatibility with custom GPTs and a \u0026#34;show changes\u0026#34; function that tracks modifications to writing and code. The company said Canvas is now on chatgpt.com for web users and also available through a Windows desktop application, with more features planned for future updates.\u003c/p\u003e\n\u003ch2\u003eDay 5: Wednesday, December 11\u003c/h2\u003e\n\u003cp\u003eOn day 5, OpenAI announced that ChatGPT would \u003ca href=\"https://arstechnica.com/gadgets/2024/12/ios-18-2-macos-15-2-updates-arrive-today-with-image-and-emoji-generation/\"\u003eintegrate with Apple Intelligence\u003c/a\u003e across iOS, iPadOS, and macOS devices. The integration works on iPhone 16 series phones, iPhone 15 Pro models, iPads with A17 Pro or M1 chips and later, and Macs with M1 processors or newer, running their respective latest operating systems.\u003c/p\u003e\n\u003cp\u003eThe integration lets users access ChatGPT\u0026#39;s features (\u003ca href=\"https://arstechnica.com/ai/2024/11/despite-unforced-errors-the-future-of-apple-intelligence-could-be-bright/\"\u003esuch as they are\u003c/a\u003e), including image and document analysis, directly through Apple\u0026#39;s system-level intelligence features. The feature works with all ChatGPT subscription tiers and operates within Apple\u0026#39;s privacy framework. \u003ca href=\"https://arstechnica.com/apple/2024/11/apple-intelligence-notification-summaries-are-honestly-pretty-bad/\"\u003eIffy message summaries\u003c/a\u003e remain unaffected by the additions.\u003c/p\u003e\n\u003cp\u003eEnterprise and Team account users need administrator approval to access the integration.\u003c/p\u003e\n\u003ch2\u003eDay 6: Thursday, December 12\u003c/h2\u003e\n\u003cp\u003eOn the sixth day, OpenAI \u003ca href=\"https://arstechnica.com/information-technology/2024/12/openai-introduces-santa-mode-to-chatgpt-for-ho-ho-ho-voice-chats/\"\u003eadded two features\u003c/a\u003e to ChatGPT\u0026#39;s voice capabilities: \u0026#34;video calling\u0026#34; with screen sharing support for ChatGPT Plus and Pro subscribers and a seasonal Santa Claus voice preset.\u003c/p\u003e\n\u003cp\u003eThe new visual Advanced Voice Mode features work through the mobile app, letting users show their surroundings or share their screen with the AI model during voice conversations. While the rollout covers most countries, users in several European nations, including EU member states, Switzerland, Iceland, Norway, and Liechtenstein, will get access at a later date. Enterprise and education users can expect these features in January.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eThe Santa voice option appears as a snowflake icon in the ChatGPT interface across mobile devices, web browsers, and desktop apps, with conversations in this mode not affecting chat history or memory. Don\u0026#39;t expect Santa to remember what you want for Christmas between sessions.\u003c/p\u003e\n\n\u003ch2\u003eDay 7: Friday, December 13\u003c/h2\u003e\n\u003cp\u003eOpenAI introduced Projects, a new organizational feature in ChatGPT that lets users group related conversations and files, on day 7. The feature works with the company\u0026#39;s GPT-4o model and provides a central location for managing resources related to specific tasks or topics—kinda like Anthropic\u0026#39;s \u0026#34;Projects\u0026#34; feature.\u003c/p\u003e\n\u003cp\u003eChatGPT Plus, Pro, and Team subscribers can currently access Projects through chatgpt.com and the Windows desktop app, with view-only support on mobile devices and macOS. Users can create projects by clicking a plus icon in the sidebar, where they can add files and custom instructions that provide context for future conversations.\u003c/p\u003e\n\u003cp\u003eOpenAI said it plans to expand Projects in 2024 with support for additional file types, cloud storage integration through Google Drive and Microsoft OneDrive, and compatibility with other models like o1. Enterprise and education users will receive access to Projects in January.\u003c/p\u003e\n\u003ch2\u003eDay 8: Monday, December 16\u003c/h2\u003e\n\u003cp\u003eOn day 8, OpenAI expanded its \u003ca href=\"https://arstechnica.com/ai/2024/10/openai-launches-chatgpt-with-search-taking-google-head-on/\"\u003esearch features in ChatGPT\u003c/a\u003e, extending access to all users with free accounts while reportedly adding speed improvements and mobile optimizations. Basically, you can use ChatGPT like a web search engine, although in practice it doesn\u0026#39;t seem to be as comprehensive as Google Search at the moment.\u003c/p\u003e\n\u003cp\u003eThe update includes a new maps interface and integration with Advanced Voice, allowing users to perform searches during voice conversations. The search capability, which previously required a paid subscription, now works across all platforms where ChatGPT operates.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003ch2\u003eDay 9: Tuesday, December 17\u003c/h2\u003e\n\u003cp\u003eOn day 9, OpenAI released its o1 model\u003ca href=\"https://arstechnica.com/ai/2024/12/openais-api-users-get-full-access-to-the-new-o1-model/\"\u003e through its API platform\u003c/a\u003e, adding support for function calling, developer messages, and vision processing capabilities. The company also reduced GPT-4o audio pricing by 60 percent and introduced a GPT-4o mini option that costs one-tenth of previous audio rates.\u003c/p\u003e\n\u003cp\u003eOpenAI also simplified its WebRTC integration for real-time applications and unveiled Preference Fine-Tuning, which provides developers new ways to customize models. The company also launched beta versions of software development kits for the Go and Java programming languages, expanding its toolkit for developers.\u003c/p\u003e\n\u003ch2\u003eDay 10: Wednesday, December 18\u003c/h2\u003e\n\u003cp\u003eOn Wednesday, OpenAI did something a little fun and launched voice and messaging access to ChatGPT through a toll-free number (\u003ca href=\"https://arstechnica.com/information-technology/2024/12/openai-launches-free-phone-hotline-to-let-anyone-call-chatgpt/\"\u003e1-800-CHATGPT\u003c/a\u003e), as well as WhatsApp. US residents can make phone calls with a 15-minute monthly limit, while global users can message ChatGPT through WhatsApp at the same number.\u003c/p\u003e\n\u003cp\u003eOpenAI said the release is a way to reach users who lack consistent high-speed Internet access or want to try AI through familiar communication channels, but it\u0026#39;s also just a clever hack. As evidence, OpenAI notes that these new interfaces serve as experimental access points, with more \u0026#34;limited functionality\u0026#34; than the full ChatGPT service, and still recommends existing users continue using their regular ChatGPT accounts for complete features.\u003c/p\u003e\n\u003ch2\u003eDay 11: Thursday, December 19\u003c/h2\u003e\n\u003cp\u003eOn Thursday, OpenAI expanded ChatGPT\u0026#39;s desktop app integration to include additional coding environments and productivity software. The update added support for Jetbrains IDEs like PyCharm and IntelliJ IDEA, VS Code variants including Cursor and VSCodium, and text editors such as BBEdit and TextMate.\u003c/p\u003e\n\u003cp\u003eOpenAI also included integration with Apple Notes, Notion, and Quip while adding Advanced Voice Mode compatibility when working with desktop applications. These features require manual activation for each app and remain available to paid subscribers, including Plus, Pro, Team, Enterprise, and Education users, with Enterprise and Education customers needing administrator approval to enable the functionality.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003ch2\u003eDay 12: Friday, December 20\u003c/h2\u003e\n\u003cp\u003eOn Friday, OpenAI concluded its twelve days of announcements by \u003ca href=\"https://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/\"\u003epreviewing two new simulated reasoning models\u003c/a\u003e, o3 and o3-mini, while opening applications for safety and security researchers to test them before public release. Early evaluations show o3 achieving a 2727 rating on Codeforces programming contests and scoring 96.7 percent on AIME 2024 mathematics problems.\u003c/p\u003e\n\u003cp\u003eThe company reports o3 set performance records on advanced benchmarks, solving 25.2 percent of problems on EpochAI\u0026#39;s Frontier Math evaluations and scoring above 85 percent on the ARC-AGI test, which is comparable to human results. OpenAI also \u003ca href=\"https://openai.com/index/deliberative-alignment/\"\u003epublished research\u003c/a\u003e about \u0026#34;deliberative alignment,\u0026#34; a technique used in developing o1. The company has not announced firm release dates for either new o3 model, but CEO Sam Altman said o3-mini might ship in late January.\u003c/p\u003e\n\u003ch2\u003eSo what did we learn?\u003c/h2\u003e\n\u003cp\u003eOpenAI\u0026#39;s December campaign revealed that OpenAI had a lot of things sitting around that it needed to ship, and it picked a fun theme to unite the announcements. Google responded in kind, as \u003ca href=\"https://arstechnica.com/information-technology/2024/12/google-and-openai-blitz-december-with-so-many-ai-releases-its-hard-to-keep-up/\"\u003ewe have covered\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSeveral trends from the releases stand out. OpenAI is heavily investing in multimodal capabilities. The o1 model\u0026#39;s release, Sora\u0026#39;s evolution from research preview to product, and the expansion of voice features with video calling all point toward systems that can seamlessly handle text, images, voice, and video.\u003c/p\u003e\n\u003cp\u003eThe company is also focusing heavily on developer tools and customization, so it can continue to have a cloud service business and have its products integrated into other applications. Between the API releases, Reinforcement Fine-Tuning, and expanded IDE integrations, OpenAI is building out its ecosystem for developers and enterprises. And the introduction of o3 shows that OpenAI is still attempting to push technological boundaries, even in the face of diminishing returns in training LLM base models.\u003c/p\u003e\n\u003cp\u003eOpenAI seems to be positioning itself for a 2025 where generative AI moves beyond text chatbots and simple image generators and finds its way into novel applications that we probably can\u0026#39;t even predict yet. We\u0026#39;ll have to wait and see what the company and developers come up with in the year ahead.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/benjedwards/\"\u003e\u003cimg src=\"https://arstechnica.com/wp-content/uploads/2022/08/benj_ega.png\" alt=\"Photo of Benj Edwards\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Benj Edwards is Ars Technica\u0026#39;s Senior AI Reporter and founder of the site\u0026#39;s dedicated AI beat in 2022. He\u0026#39;s also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/information-technology/2024/12/12-days-of-openai-the-ars-technica-recap/#comments\" title=\"15 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    15 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \n\n\n  \n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/information-technology/2024/12/as-firms-abandon-vmware-broadcom-is-laughing-all-the-way-to-the-bank/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1503144949-768x432.jpg\" alt=\"Listing image for first story in Most Read: As firms abandon VMware, Broadcom is laughing all the way to the bank\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2024-12-20T22:01:34Z",
  "modifiedTime": "2024-12-20T23:13:46Z"
}
