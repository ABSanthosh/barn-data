{
  "id": "85aa663c-18d9-4db9-a663-6375a6260d94",
  "title": "OpenAI's AI Reasoning Model 'Thinks' In Chinese Sometimes, No One Really Knows Why",
  "link": "https://slashdot.org/story/25/01/14/239246/openais-ai-reasoning-model-thinks-in-chinese-sometimes-no-one-really-knows-why?utm_source=rss1.0mainlinkanon\u0026utm_medium=feed",
  "description": "OpenAI's \"reasoning\" AI model, o1, has exhibited a puzzling behavior of \"thinking\" in Chinese, Persian, or some other language -- \"even when asked a question in English,\" reports TechCrunch. While the exact cause remains unclear, as OpenAI has yet to provide an explanation, AI experts have proposed a few theories. From the report: Several on X, including Hugging Face CEO Clement Delangue, alluded to the fact that reasoning models like o1 are trained on datasets containing a lot of Chinese characters. Ted Xiao, a researcher at Google DeepMind, claimed that companies including OpenAI use third-party Chinese data labeling services, and that o1 switching to Chinese is an example of \"Chinese linguistic influence on reasoning.\" \"[Labs like] OpenAI and Anthropic utilize [third-party] data labeling services for PhD-level reasoning data for science, math, and coding,\" Xiao wrote in a post on X. \"[F]or expert labor availability and cost reasons, many of these data providers are based in China.\" [...] Other experts don't buy the o1 Chinese data labeling hypothesis, however. They point out that o1 is just as likely to switch to Hindi, Thai, or a language other than Chinese while teasing out a solution. Other experts don't buy the o1 Chinese data labeling hypothesis, however. They point out that o1 is just as likely to switch to Hindi, Thai, or a language other than Chinese while teasing out a solution. Rather, these experts say, o1 and other reasoning models might simply be using languages they find most efficient to achieve an objective (or hallucinating). \"The model doesn't know what language is, or that languages are different,\" Matthew Guzdial, an AI researcher and assistant professor at the University of Alberta, told TechCrunch. \"It's all just text to it.\" Tiezhen Wang, a software engineer at AI startup Hugging Face, agrees with Guzdial that reasoning models' language inconsistencies may be explained by associations the models made during training. \"By embracing every linguistic nuance, we expand the model's worldview and allow it to learn from the full spectrum of human knowledge,\" Wang wrote in a post on X. \"For example, I prefer doing math in Chinese because each digit is just one syllable, which makes calculations crisp and efficient. But when it comes to topics like unconscious bias, I automatically switch to English, mainly because that's where I first learned and absorbed those ideas.\" [...] Luca Soldaini, a research scientist at the nonprofit Allen Institute for AI, cautioned that we can't know for certain. \"This type of observation on a deployed AI system is impossible to back up due to how opaque these models are,\" they told TechCrunch. \"It's one of the many cases for why transparency in how AI systems are built is fundamental.\" Read more of this story at Slashdot.",
  "author": "BeauHD",
  "published": "2025-01-15T00:45:00+00:00",
  "source": "http://rss.slashdot.org/Slashdot/slashdotMain",
  "categories": [
    "ai"
  ],
  "byline": "",
  "length": 2791,
  "excerpt": "OpenAI's \"reasoning\" AI model, o1, has exhibited a puzzling behavior of \"thinking\" in Chinese, Persian, or some other language -- \"even when asked a question in English,\" reports TechCrunch. While the exact cause remains unclear, as OpenAI has yet to provide an explanation, AI experts have proposed ...",
  "siteName": "",
  "favicon": "",
  "text": "OpenAI's \"reasoning\" AI model, o1, has exhibited a puzzling behavior of \"thinking\" in Chinese, Persian, or some other language -- \"even when asked a question in English,\" reports TechCrunch. While the exact cause remains unclear, as OpenAI has yet to provide an explanation, AI experts have proposed a few theories. From the report: Several on X, including Hugging Face CEO Clement Delangue, alluded to the fact that reasoning models like o1 are trained on datasets containing a lot of Chinese characters. Ted Xiao, a researcher at Google DeepMind, claimed that companies including OpenAI use third-party Chinese data labeling services, and that o1 switching to Chinese is an example of \"Chinese linguistic influence on reasoning.\" \"[Labs like] OpenAI and Anthropic utilize [third-party] data labeling services for PhD-level reasoning data for science, math, and coding,\" Xiao wrote in a post on X. \"[F]or expert labor availability and cost reasons, many of these data providers are based in China.\" [...] Other experts don't buy the o1 Chinese data labeling hypothesis, however. They point out that o1 is just as likely to switch to Hindi, Thai, or a language other than Chinese while teasing out a solution. Other experts don't buy the o1 Chinese data labeling hypothesis, however. They point out that o1 is just as likely to switch to Hindi, Thai, or a language other than Chinese while teasing out a solution. Rather, these experts say, o1 and other reasoning models might simply be using languages they find most efficient to achieve an objective (or hallucinating). \"The model doesn't know what language is, or that languages are different,\" Matthew Guzdial, an AI researcher and assistant professor at the University of Alberta, told TechCrunch. \"It's all just text to it.\" Tiezhen Wang, a software engineer at AI startup Hugging Face, agrees with Guzdial that reasoning models' language inconsistencies may be explained by associations the models made during training. \"By embracing every linguistic nuance, we expand the model's worldview and allow it to learn from the full spectrum of human knowledge,\" Wang wrote in a post on X. \"For example, I prefer doing math in Chinese because each digit is just one syllable, which makes calculations crisp and efficient. But when it comes to topics like unconscious bias, I automatically switch to English, mainly because that's where I first learned and absorbed those ideas.\" [...] Luca Soldaini, a research scientist at the nonprofit Allen Institute for AI, cautioned that we can't know for certain. \"This type of observation on a deployed AI system is impossible to back up due to how opaque these models are,\" they told TechCrunch. \"It's one of the many cases for why transparency in how AI systems are built is fundamental.\"",
  "image": "https://a.fsdn.com/sd/topics/ai_64.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"fhbody-175910181\"\u003e\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\u003cp\u003e\n\t\t\t\n\t\t \t\n\t\t\t\tOpenAI\u0026#39;s \u0026#34;reasoning\u0026#34; AI model, o1, has exhibited a \u003ca href=\"https://techcrunch.com/2025/01/14/openais-ai-reasoning-model-thinks-in-chinese-sometimes-and-no-one-really-knows-why/\"\u003epuzzling behavior of \u0026#34;thinking\u0026#34; in Chinese, Persian, or some other language\u003c/a\u003e -- \u0026#34;even when asked a question in English,\u0026#34; reports TechCrunch. While the exact cause remains unclear, as OpenAI has yet to provide an explanation, AI experts have proposed a few theories. From the report: \u003ci\u003e Several on X, including Hugging Face CEO Clement Delangue, \u003ca href=\"https://x.com/ClementDelangue/status/1877767382120255792\"\u003ealluded\u003c/a\u003e to the fact that reasoning models like o1 are trained on datasets containing a lot of Chinese characters. Ted Xiao, a researcher at Google DeepMind, claimed that companies including OpenAI use third-party Chinese data labeling services, and that o1 switching to Chinese is an example of \u0026#34;Chinese linguistic influence on reasoning.\u0026#34;\n\u003cp\u003e \n\u0026#34;[Labs like] OpenAI and Anthropic utilize [third-party] data labeling services for PhD-level reasoning data for science, math, and coding,\u0026#34; Xiao wrote in \u003ca href=\"https://x.com/xiao_ted/status/1877503196811362504\"\u003ea post\u003c/a\u003e on X. \u0026#34;[F]or expert labor availability and cost reasons, many of these data providers are based in China.\u0026#34; [...] Other experts don\u0026#39;t buy the o1 Chinese data labeling hypothesis, however. They point out that o1 is just as likely to switch to Hindi, Thai, or a language other than Chinese while teasing out a solution.\n\u003c/p\u003e\u003cp\u003e \nOther experts don\u0026#39;t buy the o1 Chinese data labeling hypothesis, however. They point out that o1 is just as likely to switch to Hindi, Thai, or a language other than Chinese while teasing out a solution. Rather, these experts say, o1 and \u003ca href=\"https://x.com/julien_c/status/1862430852614914083\"\u003eother reasoning models\u003c/a\u003e might simply be using languages they find most efficient to achieve an objective (or hallucinating). \u0026#34;The model doesn\u0026#39;t know what language is, or that languages are different,\u0026#34; Matthew Guzdial, an AI researcher and assistant professor at the University of Alberta, told TechCrunch. \u0026#34;It\u0026#39;s all just text to it.\u0026#34;\n\u003c/p\u003e\u003cp\u003e \nTiezhen Wang, a software engineer at AI startup Hugging Face, agrees with Guzdial that reasoning models\u0026#39; language inconsistencies may be explained by associations the models made during training. \u0026#34;By embracing every linguistic nuance, we expand the model\u0026#39;s worldview and allow it to learn from the full spectrum of human knowledge,\u0026#34; Wang \u003ca href=\"https://x.com/Xianbao_QIAN/status/1878623350953857166\"\u003ewrote\u003c/a\u003e in a post on X. \u0026#34;For example, I prefer doing math in Chinese because each digit is just one syllable, which makes calculations crisp and efficient. But when it comes to topics like unconscious bias, I automatically switch to English, mainly because that\u0026#39;s where I first learned and absorbed those ideas.\u0026#34;\n\u003c/p\u003e\u003cp\u003e \n[...] Luca Soldaini, a research scientist at the nonprofit Allen Institute for AI, cautioned that we can\u0026#39;t know for certain. \u0026#34;This type of observation on a deployed AI system is impossible to back up due to how opaque these models are,\u0026#34; they told TechCrunch. \u0026#34;It\u0026#39;s one of the many cases for why transparency in how AI systems are built is fundamental.\u0026#34; \u003c/p\u003e\u003c/i\u003e\n\t\t \t\n\t\t\u003c/p\u003e\n\n\t\t\n\n\t\t\n\n\t\t\n\t\t\t\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": null,
  "modifiedTime": null
}
