{
  "id": "9988d73a-aeab-420d-bea9-f42d25cff560",
  "title": "How we built the new family of Gemini Robotics models",
  "link": "https://blog.google/products/gemini/how-we-built-gemini-robotics/",
  "description": "Robots powered by Gemini Robotics models can learn complex actions like preparing salads and even folding an origami fox.",
  "author": "Joel MearesContributor, The Keyword",
  "published": "Tue, 01 Apr 2025 21:00:00 +0000",
  "source": "https://www.blog.google/rss/",
  "categories": [
    "Gemini Models",
    "Gemini",
    "AI"
  ],
  "byline": "Joel Meares",
  "length": 5670,
  "excerpt": "Robots powered by Gemini Robotics models can learn complex actions like preparing salads and even folding an origami fox.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png?version=pr20250401-1731",
  "text": "Apr 01, 2025 [[read-time]] min read Powered by Gemini Robotics models, robots can learn complex actions like preparing salads, playing games like Tic-Tac-Toe and even folding an origami fox. As Google DeepMind prepared for its recent announcement of a new family of Gemini 2.0 models designed specifically for robots, its head of robotics, Carolina Parada, gathered her team for another check of the tech’s capabilities.They asked a bi-arm ALOHA robot — a duo of limber metal appendages with multiple joints and pincer-like hands used widely in research — to perform tasks it hadn’t done before, using objects it hadn’t seen. “We did random things like put my shoe on the table and ask it to put some pens inside,” Carolina says. “The robot took a moment to understand the task, then did it.”For the next request, they found a toy basketball hoop and ball and asked the robot to do a “slam dunk.” Carolina watched, proud and delighted, as it did just that. Carolina says witnessing the slam dunk was a “wow” moment. “We’d trained models to help robots with specific tasks and to understand natural language before, but this was a step change,” Carolina says. “The robot had never seen anything related to basketball, or this specific toy. Yet it understood something complex — ‘slam dunk the ball’ — and performed the action smoothly. On its first try.”This all-rounder robot was powered by a Gemini Robotics model that is part of a new family of multimodal models for robotics. The models build upon Gemini 2.0 through fine-tuning with robot-specific data, adding physical action to Gemini’s multimodal outputs like text, video and audio. \"This milestone lays the foundation for the next generation of robotics that can be helpful across a range of applications,\" said Google CEO Sundar Pichai when announcing the new models on X.The Gemini Robotics models are highly dextrous, interactive and general, meaning they can drive robots to react to new objects, environments and instructions without further training. Helpful, given the team’s ambitions.“Our mission is to build embodied AI to power robots that help you with everyday tasks in the real world,” says Carolina, whose fascination with robotics began with childhood sci-fi cartoons, fueled by dreams of automated chores. “Eventually, robots will be just another surface on which we interact with AI, like our phones or computers — agents in the physical world.” Like people, robots need two main functions to perform tasks effectively and safely: the ability to understand and make decisions, and the ability to take action. Gemini Robotics-ER, an \"embodied reasoning” model built on Gemini 2.0 Flash, focuses on the former, recognizing elements in front of it, defining their size and location, and predicting the trajectory and grip required to move them. It then can generate code to execute the action. We’re now making this model available to trusted testers and partners.Google DeepMind is also introducing Gemini Robotics, its most advanced vision-language-action model, which allows robots to reason about a scene, interact with the user and take action. Crucially, it makes significant advances in an area that has proved tricky for roboticists: dexterity. “What comes naturally to humans is difficult for robots,” Carolina explains. “Dexterity requires both spatial reasoning, and complex physical manipulation. Across testing, Gemini Robotics has set a new state-of-the-art for dexterity, solving complex multi-step tasks with smooth motions and great completion times.” Gemini Robotics-ER excels at embodied reasoning capabilities, including detecting objects and pointing at object parts, finding corresponding points and detecting objects in 3D. Powered by Gemini Robotics, machines have prepared salads, packed kids’ lunches, played games like Tic-Tac-Toe and even folded an origami fox.Preparing models that could do many different kinds of tasks was a challenge — largely because it went against the general industry practice of training models for a single task over and over until it can be solved. “Instead, we chose broad task learning, training models on a huge number of tasks,” Carolina says. “We expected to see generalization emerge after a certain amount of time, and we were right.”Both models can adapt to multiple embodiments, including academic-focused robots, like the bi-arm ALOHA machine, or humanoid robots like Apollo developed by our partner Apptronik. The models adapt to different embodiments, able to perform tasks like packing a lunchbox or wiping a whiteboard in different forms. This ability to adapt is key to a future where robots could take on a number of very different roles.“The possibilities for robots using highly general and capable models are broad and exciting,” Carolina says. “They could be more useful in industries where setups are complex, precision is important and the spaces aren’t human-friendly. And they could be helpful in human-centric spaces, like the home. That’s some years away, but these models are taking us several steps closer.”Sounds like someone will get some help with those chores — eventually.",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-SocialShare_1920x1080.width-1300.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\n\n    \n    \n\n\n\n\n\n    \n\n    \n      \n\n\u003cdiv data-analytics-module=\"{\n    \u0026#34;module_name\u0026#34;: \u0026#34;Hero Menu\u0026#34;,\n    \u0026#34;section_header\u0026#34;: \u0026#34;How we built the new family of Gemini Robotics models\u0026#34;\n  }\"\u003e\n      \u003cdiv\u003e\n          \n            \u003cp\u003eApr 01, 2025\u003c/p\u003e\n          \n          \n            \u003cp data-reading-time-render=\"\"\u003e[[read-time]] min read\u003c/p\u003e\n          \n        \u003c/div\u003e\n      \n        \u003cp\u003e\n          Powered by Gemini Robotics models, robots can learn complex actions like preparing salads, playing games like Tic-Tac-Toe and even folding an origami fox.\n        \u003c/p\u003e\n      \n    \u003c/div\u003e\n\n    \n\n    \n      \n\n\n\n\n\n\n\n\n\n\n\u003cdiv\u003e\n    \u003cfigure\u003e\n      \u003cdiv\u003e\n        \u003cp\u003e\u003cimg alt=\"A still of a white and black robot in a kitchen packing a blue lunchbox\" data-component=\"uni-progressive-image\" fetchpriority=\"high\" height=\"150px\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-HeroImage_2096x1182.width-200.format-webp.webp\" width=\"360px\" data-sizes=\"(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px\" data-srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-HeroImage_2096x1182.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-HeroImage_2096x1182.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-HeroImage_2096x1182.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-HeroImage_2096x1182.width-2200.format-webp.webp 2200w\"/\u003e\n        \u003c/p\u003e\n      \u003c/div\u003e\n      \n    \u003c/figure\u003e\n  \u003c/div\u003e\n\n\n\n\n\n\n    \n\n    \n    \u003cdiv data-reading-time=\"true\" data-component=\"uni-article-body\"\u003e\n\n            \n              \n\n\n\n\n\n\u003cuni-article-speakable page-title=\"How we built the new family of Gemini Robotics models\" listen-to-article=\"Listen to article\" data-date-modified=\"2025-04-01T21:25:21.117181+00:00\" data-tracking-ids=\"G-HGNBTNCHCQ,G-6NKTLKV14N\" data-voice-list=\"en.ioh-pngnat:Cyan,en.usb-pngnat:Lime\" data-script-src=\"https://www.gstatic.com/readaloud/player/web/api/js/api.js\"\u003e\u003c/uni-article-speakable\u003e\n\n            \n\n            \n            \n\n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;How we built the new family of Gemini Robotics models\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"0u55q\"\u003eAs Google DeepMind prepared for \u003ca href=\"https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/\"\u003eits recent announcement\u003c/a\u003e of a new family of Gemini 2.0 models designed specifically for robots, its head of robotics, Carolina Parada, gathered her team for another check of the tech’s capabilities.\u003c/p\u003e\u003cp data-block-key=\"752j6\"\u003eThey asked a bi-arm ALOHA robot — a duo of limber metal appendages with multiple joints and pincer-like hands used widely in research — to perform tasks it hadn’t done before, using objects it hadn’t seen. “We did random things like put my shoe on the table and ask it to put some pens inside,” Carolina says. “The robot took a moment to understand the task, then did it.”\u003c/p\u003e\u003cp data-block-key=\"a5acc\"\u003eFor the next request, they found a toy basketball hoop and ball and asked the robot to do a “slam dunk.” Carolina watched, proud and delighted, as it did just that.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"A GIF shows a black robot arm picking up a small orange ball and placing it into a miniature toy basketball hoop. A prompt, Pick up the basketball and slam dunk it, is written out at the bottom of the GIF.\" external-image=\"\" or-mp4-video-title=\"Robot dunk\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Robotics-Inline-RobotDunk.mp4\" section-header=\"How we built the new family of Gemini Robotics models\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"uv3a7\"\u003eCarolina says witnessing the slam dunk was a “wow” moment.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;How we built the new family of Gemini Robotics models\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"0u55q\"\u003e“We’d trained models to help robots with specific tasks and to understand natural language before, but this was a step change,” Carolina says. “The robot had never seen anything related to basketball, or this specific toy. Yet it understood something complex — ‘slam dunk the ball’ — and performed the action smoothly. \u003ci\u003eOn its first try.\u003c/i\u003e”\u003c/p\u003e\u003cp data-block-key=\"e1ol8\"\u003eThis all-rounder robot was powered by a \u003ca href=\"https://deepmind.google/technologies/gemini-robotics/\"\u003eGemini Robotics\u003c/a\u003e model that is part of a new family of multimodal models for robotics. The models build upon \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/\"\u003eGemini 2.0\u003c/a\u003e through fine-tuning with robot-specific data, adding physical action to Gemini’s multimodal outputs like text, video and audio. \u0026#34;This milestone lays the foundation for the next generation of robotics that can be helpful across a range of applications,\u0026#34; said Google CEO Sundar Pichai when \u003ca href=\"https://x.com/sundarpichai/status/1899838913054744679\"\u003eannouncing the new models on X\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"46l5k\"\u003eThe Gemini Robotics models are highly dextrous, interactive and general, meaning they can drive robots to react to new objects, environments and instructions without further training. Helpful, given the team’s ambitions.\u003c/p\u003e\u003cp data-block-key=\"ev3sg\"\u003e“Our mission is to build embodied AI to power robots that help you with everyday tasks in the real world,” says Carolina, whose fascination with robotics began with childhood sci-fi cartoons, fueled by dreams of automated chores. “Eventually, robots will be just another surface on which we interact with AI, like our phones or computers — agents in the physical world.”\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"4\" thumbnail-alt=\"A humanoid robot stands opposite a person looking at their laptop. In white text, the words Gemini 2.0 + Robotics are overlaid.\" video-id=\"4MvGnmmP3c0\" video-type=\"video\" image=\"Robotics-Inline1\" video-image-url-lazy=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline1.width-100.format-webp.webp\" video-image-url-mobile=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline1.width-700.format-webp.webp\" video-image-url-desktop=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline1.width-1000.format-webp.webp\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;How we built the new family of Gemini Robotics models\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"0u55q\"\u003eLike people, robots need two main functions to perform tasks effectively and safely: the ability to understand and make decisions, and the ability to take action. Gemini Robotics-ER, an \u0026#34;embodied reasoning” model built on Gemini 2.0 Flash, focuses on the former, recognizing elements in front of it, defining their size and location, and predicting the trajectory and grip required to move them. It then can generate code to execute the action. We’re now making this model available to trusted testers and partners.\u003c/p\u003e\u003cp data-block-key=\"e4isc\"\u003eGoogle DeepMind is also introducing Gemini Robotics, its most advanced vision-language-action model, which allows robots to reason about a scene, interact with the user and take action. Crucially, it makes significant advances in an area that has proved tricky for roboticists: dexterity. “What comes naturally to humans is difficult for robots,” Carolina explains. “Dexterity requires both spatial reasoning, and complex physical manipulation. Across testing, Gemini Robotics has set a new state-of-the-art for dexterity, solving complex multi-step tasks with smooth motions and great completion times.”\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"This is a collage of visualizations showcasing these capabilities. Top left: 2D object detection, top right: pointing, bottom left: multi-view correspondence, bottom right: 3d object detection.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"How we built the new family of Gemini Robotics models\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"uv3a7\"\u003eGemini Robotics-ER excels at embodied reasoning capabilities, including detecting objects and pointing at object parts, finding corresponding points and detecting objects in 3D.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"This is a collage of visualizations showcasing these capabilities. Top left: 2D object detection, top right: pointing, bottom left: multi-view correspondence, bottom right: 3d object detection.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline2.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline2.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline2.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;How we built the new family of Gemini Robotics models\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"0u55q\"\u003ePowered by Gemini Robotics, machines have prepared salads, packed kids’ lunches, played games like Tic-Tac-Toe and even folded an origami fox.\u003c/p\u003e\u003cp data-block-key=\"9067r\"\u003ePreparing models that could do many different kinds of tasks was a challenge — largely because it went against the general industry practice of training models for a \u003ci\u003esingle\u003c/i\u003e task over and over until it can be solved. “Instead, we chose broad task learning, training models on a huge number of tasks,” Carolina says. “We expected to see generalization emerge after a certain amount of time, and we were right.”\u003c/p\u003e\u003cp data-block-key=\"284af\"\u003eBoth models can adapt to multiple embodiments, including academic-focused robots, like the bi-arm ALOHA machine, or humanoid robots like Apollo developed by our partner Apptronik.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Four images of robots performing actions. In the top left, a humanoid robot is packing a lunch, in the top right a small arm can be seen picking up a snap pea from a tupperware container, in the bottom left two large white arms ready for a task on a bench, and in the bottom right a black pincer hand holds a whiteboard eraser atop a whiteboard.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"How we built the new family of Gemini Robotics models\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"uv3a7\"\u003eThe models adapt to different embodiments, able to perform tasks like packing a lunchbox or wiping a whiteboard in different forms.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Four images of robots performing actions. In the top left, a humanoid robot is packing a lunch, in the top right a small arm can be seen picking up a snap pea from a tupperware container, in the bottom left two large white arms ready for a task on a bench, and in the bottom right a black pincer hand holds a whiteboard eraser atop a whiteboard.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline3.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline3.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline3.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;How we built the new family of Gemini Robotics models\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"0u55q\"\u003eThis ability to adapt is key to a future where robots could take on a number of very different roles.\u003c/p\u003e\u003cp data-block-key=\"7tp33\"\u003e“The possibilities for robots using highly general and capable models are broad and exciting,” Carolina says. “They could be more useful in industries where setups are complex, precision is important and the spaces aren’t human-friendly. And they could be helpful in human-centric spaces, like the home. That’s some years away, but these models are taking us several steps closer.”\u003c/p\u003e\u003cp data-block-key=\"6ofc8\"\u003eSounds like someone will get some help with those chores — eventually.\u003c/p\u003e\u003c/div\u003e\n  \n\n\n            \n            \n\n            \n              \n\n\n\n\n            \n          \u003c/div\u003e\n  \u003c/article\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-04-01T21:00:00Z",
  "modifiedTime": null
}
