{
  "id": "bda93d11-7352-4790-a89c-295592c4026e",
  "title": "Zamba2-7B",
  "link": "https://www.zyphra.com/post/zamba2-7b",
  "description": "Comments",
  "author": "",
  "published": "Mon, 14 Oct 2024 22:45:51 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 58101,
  "excerpt": "",
  "siteName": "",
  "favicon": "https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66ad5e37dce4583adcb9235f_Favicon%20Small.png",
  "text": "October 14, 2024PALO ALTO, CALIFORNIAZyphra is excited to release Zamba2-7B, a state-of-the-art small language model. At the 7B scale, we outperform the leading models of Mistral, Google’s Gemma and Meta’s Llama3 series in both quality and performance. We believe Zamba2-7B is the leading model for running on-device and on consumer GPUs as well as for many enterprise applications which require a powerful but compact and efficient model for natural-language tasks.AuthorsZyphra TeamCollaboratorsDaniel A Roberts (Sequoia Capital \u0026 MIT), Andrey Gromov (Meta FAIR), Kushal Tirumala (Meta FAIR) and Hassan Shapourian (Cisco)Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available here, and a pure-pytorch implementation is available here.Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available here, and a pure-pytorch implementation is available here.Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available here, and a pure-pytorch implementation is available here.Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available here, and a pure-pytorch implementation is available here.Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available here, and a pure-pytorch implementation is available here.Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available here, and a pure-pytorch implementation is available here.Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.What is Annealing?Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available here, and a pure-pytorch implementation is available here.Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available here, and a pure-pytorch implementation is available here.Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available here, and a pure-pytorch implementation is available here.Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8BZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.Architectural improvements over Zamba1-7B:Mamba1 blocks have been replaced with Mamba2 blocksInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.We release the model weights open-source (Apache 2.0)Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u003c9B), we lead the pack in both quality and performance.Our model outperforms existing state-of-the-art models for the following reasons:Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.Our 3 trillion token pre-training dataset, which is composed of a combination of Zyda and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.We have a separate \"annealing\" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available here, and a pure-pytorch implementation is available here.Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.Analysis of Global DuplicatesWe present histograms depicting distribution of cluster sizes in all the datasets (see Fig. 7-11). Please, note that all the figures are in log-log scale. We see a significant drop in the number of clusters starting from the size of around 100. This drop is present both in DCLM and FineWeb-Edu2 (see Fig. 8 and 9 respectively), and most likely is explained by a combination of  the deduplication strategy and quality when creating both datasets: DCLM deduplication was done individually within 10 shards, while FineWeb-Edu2 was deduplicated within every Common Crawl snapshot. We find that large clusters usually contain low quality material (repeated advertisements, license agreements templates, etc), so it’s not surprising that such documents were removed. Notably, DCLM still contained one cluster with the size close to 1 million documents, containing low quality documents seemingly coming from the advertisements (see Appendix).We find both Zyda-1and Dolma-CC contain a small amount of duplicates, which is expected, since both datasets were deduplicated globally by their authors. Remaining duplicates are likely false negatives from the initial deduplication procedure. Note, that distribution of duplicates clusters sizes of these two datasets (Fig. 10 and 11) don’t contain any sharp drops, but rather hyper exponentially decreases with cluster size. Figure 7: Distribution of cluster sizes of duplicates in global dataset (log-log scale).Figure 8: Distribution of cluster sizes of duplicates in DCLM (log-log scale).Figure 9: Distribution of cluster sizes of duplicates in FineWeb-Edu2 (log-log scale).Figure 10: Distribution of cluster sizes of duplicates in Zyda-1 (log-log scale).Figure 11: Distribution of cluster sizes of duplicates in Dolma-CC (log-log scale).Largest cluster in DCLMBelow is an example of the document from the largest cluster (~1M documents) of duplicates in DCLM (quality score 0.482627):‍Is safe? Is scam?Is safe for your PC?Is safe or is it scam?Domain is SafeSafe score: 1‍‍The higher the number, the more dangerous the website.Any number higher than 1 means DANGER.‍‍Positive votes:Negative votes:Vote Up Vote Down review‍‍Have you had bad experience with Warn us, please!Examples of varying quality score in DCLM in a clusterBelow one will find a few documents with different quality scores from DCLM coming from the same duplicates cluster. Quality score varies from ~0.2 to ~0.04.",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-animation=\"default\" data-collapse=\"tiny\" data-duration=\"400\" data-easing=\"ease\" data-easing2=\"ease\" role=\"banner\"\u003e\u003cp\u003e\u003ca href=\"https://www.zyphra.com/\"\u003e\u003cimg width=\"123\" sizes=\"(max-width: 479px) 123px, (max-width: 767px) 22vw, 123px\" alt=\"\" src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch.png\" loading=\"lazy\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch.png 1505w\"/\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash.jpg\" loading=\"lazy\" width=\"726\" height=\"Auto\" alt=\"\" sizes=\"(max-width: 479px) 90vw, (max-width: 1279px) 80vw, 90vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-500.jpg 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-800.jpg 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-1080.jpg 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-1600.jpg 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-2000.jpg 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-2600.jpg 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-3200.jpg 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash.jpg 5760w\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b.png\" loading=\"lazy\" width=\"648\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b.png 1470w\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp\u003eOctober 14, 2024\u003c/p\u003e\u003cp\u003ePALO ALTO, CALIFORNIA\u003c/p\u003e\u003cp\u003eZyphra is excited to release Zamba2-7B, a state-of-the-art small language model. At the 7B scale, we outperform the leading models of Mistral, Google’s Gemma and Meta’s Llama3 series in both quality and performance. We believe Zamba2-7B is the leading model for running on-device and on consumer GPUs as well as for many enterprise applications which require a powerful but compact and efficient model for natural-language tasks.\u003c/p\u003e\u003cp\u003eAuthors\u003c/p\u003e\u003cp\u003eZyphra Team\u003c/p\u003e\u003cp\u003eCollaborators\u003c/p\u003e\u003cp\u003eDaniel A Roberts (Sequoia Capital \u0026amp; MIT), Andrey Gromov (Meta FAIR), Kushal Tirumala (Meta FAIR) and Hassan Shapourian (Cisco)\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"introduction\"\u003e\u003cdiv\u003e\u003cdiv id=\"zamba1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"zamba2\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" alt=\"Quality vs. Inference Speed\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"zamba3\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv id=\"zyda1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"zyda2\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"zyda3\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003csection id=\"zyda4\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003c/section\u003e\u003c/div\u003e\u003cdiv\u003e\u003csection id=\"small1\"\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003cdiv\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/section\u003e\u003csection id=\"small2\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"small3\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"small4\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w\"/\u003e\u003c/a\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003csection id=\"small5\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003cdiv\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv id=\"rag1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"rag2\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"rag3\"\u003e\u003cdiv\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"rag4\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003cdiv id=\"rag5\"\u003e\u003cp\u003eZamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.\u003c/p\u003e\u003cp\u003eZamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba\u0026#39;s unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available \u003ca href=\"https://github.com/Zyphra/transformers_zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e, and a pure-pytorch implementation is available \u003ca href=\"https://github.com/Zyphra/Zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eZyphra\u0026#39;s team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv id=\"tree1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"tree2\"\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003csection id=\"tree3\"\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" width=\"674\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" width=\"586\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eZamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.\u003c/p\u003e\u003cp\u003eZamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba\u0026#39;s unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available \u003ca href=\"https://github.com/Zyphra/transformers_zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e, and a pure-pytorch implementation is available \u003ca href=\"https://github.com/Zyphra/Zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eZyphra\u0026#39;s team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv id=\"layer1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"layer2\"\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" width=\"619\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" width=\"658\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" width=\"680\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/section\u003e\u003cdiv id=\"layer3\"\u003e\u003cp\u003eZamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.\u003c/p\u003e\u003cp\u003eZamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba\u0026#39;s unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available \u003ca href=\"https://github.com/Zyphra/transformers_zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e, and a pure-pytorch implementation is available \u003ca href=\"https://github.com/Zyphra/Zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eZyphra\u0026#39;s team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003csection id=\"edge1\"\u003e\u003cdiv\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" width=\"455\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003cdiv id=\"edge2\"\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003csection id=\"edge3\"\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003c/section\u003e\u003cdiv id=\"edge4\"\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"edge5\"\u003e\u003cp\u003eZamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.\u003c/p\u003e\u003cp\u003eZamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba\u0026#39;s unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available \u003ca href=\"https://github.com/Zyphra/transformers_zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e, and a pure-pytorch implementation is available \u003ca href=\"https://github.com/Zyphra/Zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eZyphra\u0026#39;s team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv id=\"hop1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"hop2\"\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"hop3\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" width=\"581\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003csection id=\"hop4\"\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003c/section\u003e\u003cdiv id=\"hop5\"\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003csection id=\"hop6\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" width=\"367\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eZamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.\u003c/p\u003e\u003cp\u003eZamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba\u0026#39;s unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available \u003ca href=\"https://github.com/Zyphra/transformers_zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e, and a pure-pytorch implementation is available \u003ca href=\"https://github.com/Zyphra/Zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eZyphra\u0026#39;s team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003csection id=\"hop7\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" width=\"479\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003csection id=\"hop8\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png\" loading=\"lazy\" width=\"458\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w\"/\u003e\u003c/a\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003csection id=\"hop9\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png\" loading=\"lazy\" width=\"360\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv target=\"_blank\" id=\"cook1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"cook2\"\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"cook3\"\u003e\u003cdiv\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003cdiv id=\"cook4\"\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"cook5\"\u003e\u003cp\u003eZamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.\u003c/p\u003e\u003cp\u003eZamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba\u0026#39;s unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available \u003ca href=\"https://github.com/Zyphra/transformers_zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e, and a pure-pytorch implementation is available \u003ca href=\"https://github.com/Zyphra/Zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eZyphra\u0026#39;s team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.\u003c/p\u003e\u003c/div\u003e\u003csection id=\"cook7\"\u003e\u003cp\u003eWhat is Annealing?\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003csection id=\"cook8\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003csection id=\"hop9\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv target=\"_blank\" id=\"mini1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"mini2\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"mini3\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003csection id=\"mini4\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w\"/\u003e\u003c/a\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003csection id=\"mini5\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png\" loading=\"lazy\" width=\"384\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003cdiv\u003e\u003csection id=\"noc1\"\u003e\u003cdiv target=\"_blank\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" width=\"429\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"noc2\"\u003e\u003cdiv\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" width=\"267\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png\" loading=\"lazy\" width=\"319\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w\"/\u003e\u003c/a\u003e\u003cdiv\u003e\u003cp\u003eZamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.\u003c/p\u003e\u003cp\u003eZamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba\u0026#39;s unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available \u003ca href=\"https://github.com/Zyphra/transformers_zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e, and a pure-pytorch implementation is available \u003ca href=\"https://github.com/Zyphra/Zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eZyphra\u0026#39;s team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003csection id=\"noc3\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png\" loading=\"lazy\" width=\"400\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv target=\"_blank\" id=\"longrag1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"longrag2\"\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003cdiv target=\"_blank\" id=\"longrag3\"\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003csection id=\"longrag4\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cdiv target=\"_blank\"\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"longrag5\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003cdiv target=\"_blank\"\u003e\u003cp\u003eZamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.\u003c/p\u003e\u003cp\u003eZamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba\u0026#39;s unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available \u003ca href=\"https://github.com/Zyphra/transformers_zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e, and a pure-pytorch implementation is available \u003ca href=\"https://github.com/Zyphra/Zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eZyphra\u0026#39;s team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv target=\"_blank\" id=\"zamba2_1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"zamba2_2\"\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" width=\"570\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" width=\"570\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cdiv target=\"_blank\"\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" width=\"570\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png\" loading=\"lazy\" width=\"319\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003csection id=\"zamba2_3\"\u003e\u003cdiv target=\"_blank\"\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png\" loading=\"lazy\" width=\"571\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w\"/\u003e\u003c/a\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png\" loading=\"lazy\" width=\"570\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w\"/\u003e\u003c/a\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png\" loading=\"lazy\" width=\"571\" alt=\"\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w\"/\u003e\u003c/a\u003e\u003cdiv target=\"_blank\"\u003e\u003cp\u003eZamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.\u003c/p\u003e\u003cp\u003eZamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba\u0026#39;s unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available \u003ca href=\"https://github.com/Zyphra/transformers_zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e, and a pure-pytorch implementation is available \u003ca href=\"https://github.com/Zyphra/Zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eZyphra\u0026#39;s team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv target=\"_blank\" id=\"zyda2_1\"\u003e\u003cul role=\"list\"\u003e\u003cli\u003eZamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B\u003c/li\u003e\u003cli\u003eZamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.\u003c/li\u003e\u003cli\u003eArchitectural improvements over \u003ca href=\"https://arxiv.org/abs/2405.16712\" target=\"_blank\"\u003eZamba1-7B\u003c/a\u003e:\u003cul role=\"list\"\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\"\u003eMamba1\u003c/a\u003e blocks have been replaced with \u003ca href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\"\u003eMamba2\u003c/a\u003e blocks\u003c/li\u003e\u003cli\u003eInstead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.\u003c/li\u003e\u003cli\u003eWe apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eWe release the model weights open-source (Apache 2.0)\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003csection id=\"zyda2_2\"\u003e\u003cp\u003eZamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (\u0026lt;9B), we lead the pack in both quality and performance.\u003c/p\u003e\u003c/section\u003e\u003csection id=\"zyda2_3\"\u003e\u003cdiv target=\"_blank\"\u003e\u003cp\u003eOur model outperforms existing state-of-the-art models for the following reasons:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eOur novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.\u003c/li\u003e\u003cli\u003eOur 3 trillion token pre-training dataset, which is composed of a combination of \u003ca href=\"https://arxiv.org/abs/2406.01981\" target=\"_blank\"\u003eZyda\u003c/a\u003e and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.\u003c/li\u003e\u003cli\u003eWe have a separate \u0026#34;annealing\u0026#34; pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eDue to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.\u003c/p\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w\"/\u003e\u003c/a\u003e\u003cp\u003eZamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.\u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png\" loading=\"lazy\" alt=\"\" sizes=\"(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w\"/\u003e\u003c/a\u003e\u003cdiv target=\"_blank\"\u003e\u003cp\u003eWe achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:\u003c/p\u003e\u003col start=\"1\" role=\"list\"\u003e\u003cli\u003eMamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.\u003c/li\u003e\u003cli\u003eMamba blocks only have small hidden states to store and don\u0026#39;t require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.\u003c/li\u003e\u003cli\u003eWe choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png\" loading=\"lazy\" alt=\"\" sizes=\"(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w\"/\u003e\u003c/a\u003e\u003cdiv target=\"_blank\"\u003e\u003cp\u003eZamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.\u003c/p\u003e\u003cp\u003eZamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba\u0026#39;s unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available \u003ca href=\"https://github.com/Zyphra/transformers_zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e, and a pure-pytorch implementation is available \u003ca href=\"https://github.com/Zyphra/Zamba2\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eZyphra\u0026#39;s team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.\u003c/p\u003e\u003c/div\u003e\u003c/section\u003e\u003csection id=\"zyda2_4\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png\" loading=\"lazy\" alt=\"\" sizes=\"(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003csection id=\"zyda2_5\"\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png\" loading=\"lazy\" alt=\"\" sizes=\"(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w\"/\u003e\u003c/a\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png\" loading=\"lazy\" alt=\"\" sizes=\"(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w\"/\u003e\u003c/a\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png\" loading=\"lazy\" alt=\"\" sizes=\"(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w\"/\u003e\u003c/a\u003e\u003c/section\u003e\u003csection id=\"zyda2_7\"\u003e\u003ch3\u003eAnalysis of Global Duplicates\u003c/h3\u003e\u003cp\u003eWe present histograms depicting distribution of cluster sizes in all the datasets (see Fig. 7-11). Please, note that all the figures are in log-log scale. We see a significant drop in the number of clusters starting from the size of around 100. This drop is present both in DCLM and FineWeb-Edu2 (see Fig. 8 and 9 respectively), and most likely is explained by a combination of  the deduplication strategy and quality when creating both datasets: DCLM deduplication was done individually within 10 shards, while FineWeb-Edu2 was deduplicated within every Common Crawl snapshot. We find that large clusters usually contain low quality material (repeated advertisements, license agreements templates, etc), so it’s not surprising that such documents were removed. Notably, DCLM still contained one cluster with the size close to 1 million documents, containing low quality documents seemingly coming from the advertisements (see Appendix).We find both Zyda-1and Dolma-CC contain a small amount of duplicates, which is expected, since both datasets were deduplicated globally by their authors. Remaining duplicates are likely false negatives from the initial deduplication procedure. Note, that distribution of duplicates clusters sizes of these two datasets (Fig. 10 and 11) don’t contain any sharp drops, but rather hyper exponentially decreases with cluster size. \u003c/p\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/img/placeholder-thumb.svg\" loading=\"lazy\" alt=\"\"/\u003e\u003c/a\u003e\u003ch5\u003e\u003cem\u003eFigure 7: Distribution of cluster sizes of duplicates in global dataset (log-log scale).\u003c/em\u003e\u003c/h5\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9.png\" loading=\"lazy\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9.png 1600w\" alt=\"\"/\u003e\u003c/a\u003e\u003ch5\u003e\u003cem\u003eFigure 8: Distribution of cluster sizes of duplicates in DCLM (log-log scale).\u003c/em\u003e\u003c/h5\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10.png\" loading=\"lazy\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10.png 1600w\" alt=\"\"/\u003e\u003c/a\u003e\u003ch5\u003e\u003cem\u003eFigure 9: Distribution of cluster sizes of duplicates in FineWeb-Edu2 (log-log scale).\u003c/em\u003e\u003c/h5\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11.png\" loading=\"lazy\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11.png 1600w\" alt=\"\"/\u003e\u003c/a\u003e\u003ch5\u003e\u003cem\u003eFigure 10: Distribution of cluster sizes of duplicates in Zyda-1 (log-log scale).\u003c/em\u003e\u003c/h5\u003e\u003ca href=\"#\"\u003e\u003cimg src=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12.png\" loading=\"lazy\" sizes=\"100vw\" srcset=\"https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12.png 1600w\" alt=\"\"/\u003e\u003c/a\u003e\u003ch5\u003e\u003cem\u003eFigure 11: Distribution of cluster sizes of duplicates in Dolma-CC (log-log scale).\u003c/em\u003e\u003c/h5\u003e\u003ch3\u003eLargest cluster in DCLM\u003c/h3\u003e\u003cp\u003eBelow is an example of the document from the largest cluster (~1M documents) of duplicates in DCLM (quality score 0.482627):\u003cbr/\u003e‍\u003cem\u003eIs safe? Is scam?\u003cbr/\u003eIs safe for your PC?\u003cbr/\u003eIs safe or is it scam?\u003cbr/\u003eDomain is SafeSafe score: 1\u003c/em\u003e‍\u003cbr/\u003e‍\u003cem\u003eThe higher the number, the more dangerous the website.Any number higher than 1 means DANGER.\u003c/em\u003e‍\u003cbr/\u003e‍\u003cem\u003ePositive votes:\u003cbr/\u003eNegative votes:\u003cbr/\u003eVote Up Vote Down review\u003c/em\u003e‍\u003cbr/\u003e‍\u003cem\u003eHave you had bad experience with Warn us, please!\u003c/em\u003e\u003c/p\u003e\u003ch3\u003eExamples of varying quality score in DCLM in a cluster\u003c/h3\u003e\u003cp\u003eBelow one will find a few documents with different quality scores from DCLM coming from the same duplicates cluster. Quality score varies from ~0.2 to ~0.04.\u003c/p\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\n\u003c/div\u003e",
  "readingTime": "60 min read",
  "publishedTime": null,
  "modifiedTime": null
}
