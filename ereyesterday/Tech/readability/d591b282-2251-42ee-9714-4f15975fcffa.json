{
  "id": "d591b282-2251-42ee-9714-4f15975fcffa",
  "title": "For algorithms, a little memory outweighs a lot of time",
  "link": "https://www.quantamagazine.org/for-algorithms-a-little-memory-outweighs-a-lot-of-time-20250521/",
  "description": "Comments",
  "author": "",
  "published": "Wed, 21 May 2025 19:34:20 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "By Ben Brubaker May 21, 2025",
  "length": 24097,
  "excerpt": "One computer scientist’s “stunning” proof is the first progress in 50 years on one of the most famous questions in computer science.",
  "siteName": "Quanta Magazine",
  "favicon": "https://www.quantamagazine.org/wp-content/themes/quanta2024/frontend/images/favicon.png",
  "text": "One computer scientist’s “stunning” proof is the first progress in 50 years on one of the most famous questions in computer science. Irene Pérez for Quanta Magazine One July afternoon in 2024, Ryan Williams set out to prove himself wrong. Two months had passed since he’d hit upon a startling discovery about the relationship between time and memory in computing. It was a rough sketch of a mathematical proof that memory was more powerful than computer scientists believed: A small amount would be as helpful as a lot of time in all conceivable computations. That sounded so improbable that he assumed something had to be wrong, and he promptly set the proof aside to focus on less crazy ideas. Now, he’d finally carved out time to find the error. That’s not what happened. After hours of poring over his argument, Williams couldn’t find a single flaw. “I just thought I was losing my mind,” said Williams, a theoretical computer scientist at the Massachusetts Institute of Technology. For the first time, he began to entertain the possibility that maybe, just maybe, memory really was as powerful as his work suggested. Over the months that followed, he fleshed out the details, scrutinized every step, and solicited feedback from a handful of other researchers. In February, he finally posted his proof online, to widespread acclaim. “It’s amazing. It’s beautiful,” said Avi Wigderson, a theoretical computer scientist at the Institute for Advanced Study in Princeton, New Jersey. As soon as he heard the news, Wigderson sent Williams a congratulatory email. Its subject line: “You blew my mind.” Time and memory (also called space) are the two most fundamental resources in computation: Every algorithm takes some time to run, and requires some space to store data while it’s running. Until now, the only known algorithms for accomplishing certain tasks required an amount of space roughly proportional to their runtime, and researchers had long assumed there’s no way to do better. Williams’ proof established a mathematical procedure for transforming any algorithm — no matter what it does — into a form that uses much less space. Ryan Williams stunned his colleagues with a milestone proof about the relationship between time and space in computation. Katherine Taylor for Quanta Magazine What’s more, this result — a statement about what you can compute given a certain amount of space — also implies a second result, about what you cannot compute in a certain amount of time. This second result isn’t surprising in itself: Researchers expected it to be true, but they had no idea how to prove it. Williams’ solution, based on his sweeping first result, feels almost cartoonishly excessive, akin to proving a suspected murderer guilty by establishing an ironclad alibi for everyone else on the planet. It could also offer a new way to attack one of the oldest open problems in computer science. “It’s a pretty stunning result, and a massive advance,” said Paul Beame, a computer scientist at the University of Washington. “It’s a little bit less of a surprise that it’s Ryan doing this.” Space to Roam Williams, 46, has an open, expressive face and a hint of gray in his hair. His office, which looks out over the colorful spires of MIT’s Stata Center, is another illustration of the creative use of space. A pair of yoga mats have transformed a window ledge into a makeshift reading nook, and the desk is tucked into an oddly shaped corner, freeing up room for a couch facing a large whiteboard brimming with mathematical scribblings. MIT is a long way from Williams’ childhood home in rural Alabama, where there was no shortage of space. He grew up on a 50-acre farm and first saw a computer at age 7, when his mother drove him across the county for a special academic enrichment class. He recalled being transfixed by a simple program for generating a digital fireworks display. “It was taking a random color and sending it in a random direction from the middle of the monitor,” Williams said. “You couldn’t have predicted what picture you’re going to get.” The world of computers seemed a wild and wonderful playground, full of infinite possibilities. Young Williams was hooked. “I was writing programs to myself, on paper, to be run on a future computer,” he said. “My parents didn’t really know what to do with me.” Williams’ office, like his new result, makes creative use of space. Katherine Taylor for Quanta Magazine As he grew older, Williams advanced from imaginary computers to real ones. For his last two years of high school, he transferred to the Alabama School of Math and Science, a prestigious public boarding school, where he first encountered the theoretical side of computer science. “I realized that there was a wider world of things out there, and that there was a way to think mathematically about computers,” he said. “That’s what I wanted to do.” Williams was especially drawn to a branch of theoretical computer science called computational complexity theory. It deals with the resources (such as time and space) that are needed to solve computational problems such as sorting lists or factoring numbers. Most problems can be solved by many different algorithms, each with its own demands on time and space. Complexity theorists sort problems into categories, called complexity classes, based on the resource demands of the best algorithms for solving them — that is, the algorithms that run fastest or use the least amount of space. But how do you make the study of computational resources mathematically rigorous? You won’t get far if you try to analyze time and space by comparing minutes to megabytes. To make any progress, you need to start with the right definitions. Getting Resourceful Those definitions emerged from the work of Juris Hartmanis, a pioneering computer scientist who had experience making do with limited resources. He was born in 1928 into a prominent Latvian family, but his childhood was disrupted by the outbreak of World War II. Occupying Soviet forces arrested and executed his father, and after the war Hartmanis finished high school in a refugee camp. He went on to university, where he excelled even though he couldn’t afford textbooks. “I compensated by taking very detailed notes in lectures,” he recalled in a 2009 interview. “There is a certain advantage to [having] to improvise and overcome difficulties.” Hartmanis immigrated to the United States in 1949, and worked a series of odd jobs — building agricultural machinery, manufacturing steel and even serving as a butler — while studying mathematics at the University of Kansas City. He went on to a spectacularly successful career in the young field of theoretical computer science. In 1960, while working at the General Electric research laboratory in Schenectady, New York, Hartmanis met Richard Stearns, a graduate student doing a summer internship. In a pair of groundbreaking papers they established precise mathematical definitions for time and space. These definitions gave researchers the language they needed to compare the two resources and sort problems into complexity classes. In the 1960s, Juris Hartmanis established the definitions that computer scientists use to analyze space and time. Division of Rare and Manuscript Collections, Cornell University Library One of the most important classes goes by the humble name “P.” Roughly speaking, it encompasses all problems that can be solved in a reasonable amount of time. An analogous complexity class for space is dubbed “PSPACE.” The relationship between these two classes is one of the central questions of complexity theory. Every problem in P is also in PSPACE, because fast algorithms just don’t have enough time to fill up much space in a computer’s memory. If the reverse statement were also true, the two classes would be equivalent: Space and time would have comparable computational power. But complexity theorists suspect that PSPACE is a much larger class, containing many problems that aren’t in P. In other words, they believe that space is a far more powerful computational resource than time. This belief stems from the fact that algorithms can use the same small chunk of memory over and over, while time isn’t as forgiving — once it passes, you can’t get it back. “The intuition is just so simple,” Williams said. “You can reuse space, but you can’t reuse time.” But intuition isn’t good enough for complexity theorists: They want rigorous proof. To prove that PSPACE is larger than P, researchers would have to show that for some problems in PSPACE, fast algorithms are categorically impossible. Where would they even start? A Space Odyssey As it happened, they started at Cornell University, where Hartmanis moved in 1965 to head the newly established computer science department. Under his leadership it quickly became a center of research in complexity theory, and in the early 1970s, a pair of researchers there, John Hopcroft and Wolfgang Paul, set out to establish a precise link between time and space. Hopcroft and Paul knew that to resolve the P versus PSPACE problem, they’d have to prove that you can’t do certain computations in a limited amount of time. But it’s hard to prove a negative. Instead, they decided to flip the problem on its head and explore what you can do with limited space. They hoped to prove that algorithms given a certain space budget can solve all the same problems as algorithms with a slightly larger time budget. That would indicate that space is at least slightly more powerful than time — a small but necessary step toward showing that PSPACE is larger than P. With that goal in mind, they turned to a method that complexity theorists call simulation, which involves transforming existing algorithms into new ones that solve the same problems, but with different amounts of space and time. To understand the basic idea, imagine you’re given a fast algorithm for alphabetizing your bookshelf, but it requires you to lay out your books in dozens of small piles. You might prefer an approach that takes up less space in your apartment, even if it takes longer. A simulation is a mathematical procedure you could use to get a more suitable algorithm: Feed it the original, and it’ll give you a new algorithm that saves space at the expense of time. Algorithm designers have long studied these space-time trade-offs for specific tasks like sorting. But to establish a general relationship between time and space, Hopcroft and Paul needed something more comprehensive: a space-saving simulation procedure that works for every algorithm, no matter what problem it solves. They expected this generality to come at a cost. A universal simulation can’t exploit the details of any specific problem, so it probably won’t save as much space as a specialized simulation. But when Hopcroft and Paul started their work, there were no known universal methods for saving space at all. Even saving a small amount would be progress. The breakthrough came in 1975, after Hopcroft and Paul teamed up with a young researcher named Leslie Valiant. The trio devised a universal simulation procedure that always saves a bit of space. No matter what algorithm you give it, you’ll get an equivalent one whose space budget is slightly smaller than the original algorithm’s time budget. “Anything you can do in so much time, you can also do in slightly less space,” Valiant said. It was the first major step in the quest to connect space and time. In 1975, Leslie Valiant helped prove that space is a slightly more powerful computational resource than time. Katherine Taylor for Quanta Magazine But then progress stalled, and complexity theorists began to suspect that they’d hit a fundamental barrier. The problem was precisely the universal character of Hopcroft, Paul and Valiant’s simulation. While many problems can be solved with much less space than time, some intuitively seemed like they’d need nearly as much space as time. If so, more space-efficient universal simulations would be impossible. Paul and two other researchers soon proved that they are indeed impossible, provided you make one seemingly obvious assumption: Different chunks of data can’t occupy the same space in memory at the same time. “Everybody took it for granted that you cannot do better,” Wigderson said. Paul’s result suggested that resolving the P versus PSPACE problem would require abandoning simulation altogether in favor of a different approach, but nobody had any good ideas. That was where the problem stood for 50 years — until Williams finally broke the logjam. First, he had to get through college. Complexity Classes In 1996, the time came for Williams to apply to colleges. He knew that pursuing complexity theory would take him far from home, but his parents made it clear that the West Coast and Canada were out of the question. Among his remaining options, Cornell stood out to him for its prominent place in the history of his favorite discipline. “This guy Hartmanis defined the time and space complexity classes,” he recalled thinking. “That was important for me.” Williams was admitted to Cornell with generous financial aid and arrived in the fall of 1997, planning to do whatever it took to become a complexity theorist himself. His single-mindedness stuck out to his fellow students. “He was just super-duper into complexity theory,” said Scott Aaronson, a computer scientist at the University of Texas, Austin, who overlapped with Williams at Cornell. Williams grew interested in the relationship between space and time as an undergraduate, but never found an opportunity to work on it until last year. Katherine Taylor for Quanta Magazine But by sophomore year, Williams was struggling to keep up in courses that emphasized mathematical rigor over intuition. After he got a middling grade in a class on the theory of computing, the teacher suggested he consider other careers. But Williams wouldn’t give up his dream. He doubled down and enrolled in a graduate theory course, hoping that a stellar grade in the harder class would look impressive on his grad school applications. The professor teaching that graduate course was Hartmanis, by then an elder statesman in the field. Williams began attending Hartmanis’ office hours every week, where he was almost always the only student who showed up. His tenacity paid off: he earned an A in the course, and Hartmanis agreed to advise him on an independent research project the following semester. Their weekly meetings continued throughout Williams’ time in college. Hartmanis encouraged him to cultivate an individual approach to complexity research and gently steered him away from dead ends. “I was deeply influenced by him then,” Williams said. “I guess I still am now.” But despite earning a coveted graduate research fellowship from the National Science Foundation, Williams was rejected by every doctoral program he applied to. On hearing the news, Hartmanis phoned a colleague, then turned around and congratulated Williams on getting accepted into a yearlong master’s program at Cornell. A year later Williams again applied to various doctoral programs, and with that extra research experience under his belt, he found success. Williams continued working in complexity theory in grad school and the years that followed. In 2010, four years after receiving his doctorate, he proved a milestone result — a small step, but the largest in decades, toward solving the most famous question in theoretical computer science, about the nature of hard problems. That result cemented Williams’ reputation, and he went on to write dozens of other papers on different topics in complexity theory. P versus PSPACE wasn’t one of them. Williams had been fascinated by the problem since he first encountered it in college. He’d even supplemented his computer science curriculum with courses in logic and philosophy, seeking inspiration from other perspectives on time and space, to no avail. “It’s always been in the back of my mind,” Williams said. “I just couldn’t think of anything interesting enough to say about it.” Last year, he finally found the opportunity he’d been waiting for. Squishy Pebbles The story of Williams’ new result started with progress on a different question about memory in computation: What problems can be solved with extremely limited space? In 2010, the pioneering complexity theorist Stephen Cook and his collaborators invented a task, called the tree evaluation problem, that they proved would be impossible for any algorithm with a space budget below a specific threshold. But there was a loophole. The proof relied on the same commonsense assumption that Paul and his colleagues had made decades earlier: Algorithms can’t store new data in space that’s already full. For over a decade, complexity theorists tried to close that loophole. Then, in 2023, Cook’s son James and a researcher named Ian Mertz blew it wide open, devising an algorithm that solved the tree evaluation problem using much less space than anyone thought possible. The elder Cook’s proof had assumed that bits of data were like pebbles that have to occupy separate places in an algorithm’s memory. But it turns out that’s not the only way to store data. “We can actually think about these pebbles as things that we can squish a little bit on top of each other,” Beame said. James Cook (left) and Ian Mertz recently devised a new algorithm that solved a specific problem using much less space than anyone thought possible. Colin Morris; Michal Koucký Cook and Mertz’s algorithm roused the curiosity of many researchers, but it wasn’t clear that it had any applications beyond the tree evaluation problem. “Nobody saw how central it is to time versus space itself,” Wigderson said. Ryan Williams was the exception. In spring 2024, a group of students gave a presentation about the Cook and Mertz paper as their final project in a class he was teaching. Their enthusiasm inspired him to take a closer look. As soon as he did, an idea jumped out at him. Cook and Mertz’s method, he realized, was really a general-purpose tool for reducing space usage. Why not use it to power a new universal simulation linking time and space — like the one designed by Hopcroft, Paul and Valiant, but better? That classic result was a way to transform any algorithm with a given time budget into a new algorithm with a slightly smaller space budget. Williams saw that a simulation based on squishy pebbles would make the new algorithm’s space usage much smaller — roughly equal to the square root of the original algorithm’s time budget. That new space-efficient algorithm would also be much slower, so the simulation was not likely to have practical applications. But from a theoretical point of view, it was nothing short of revolutionary. For 50 years, researchers had assumed it was impossible to improve Hopcroft, Paul and Valiant’s universal simulation. Williams’ idea — if it worked — wouldn’t just beat their record — it would demolish it. “I thought about it, and I was like, ‘Well, that just simply can’t be true,’” Williams said. He set it aside and didn’t come back to it until that fateful day in July, when he tried to find the flaw in the argument and failed. After he realized that there was no flaw, he spent months writing and rewriting the proof to make it as clear as possible. At the end of February, Williams finally put the finished paper online. Cook and Mertz were as surprised as everyone else. “I had to go take a long walk before doing anything else,” Mertz said. Valiant got a sneak preview of Williams’ improvement on his decades-old result during his morning commute. For years, he’s taught at Harvard University, just down the road from Williams’ office at MIT. They’d met before, but they didn’t know they lived in the same neighborhood until they bumped into each other on the bus on a snowy February day, a few weeks before the result was public. Williams described his proof to the startled Valiant and promised to send along his paper. “I was very, very impressed,” Valiant said. “If you get any mathematical result which is the best thing in 50 years, you must be doing something right.” PSPACE: The Final Frontier With his new simulation, Williams had proved a positive result about the computational power of space: Algorithms that use relatively little space can solve all problems that require a somewhat larger amount of time. Then, using just a few lines of math, he flipped that around and proved a negative result about the computational power of time: At least a few problems can’t be solved unless you use more time than space. That second, narrower result is in line with what researchers expected. The weird part is how Williams got there, by first proving a result that applies to all algorithms, no matter what problems they solve. “I still have a hard time believing it,” Williams said. “It just seems too good to be true.” Williams used Cook and Mertz’s technique to establish a stronger link between space and time — the first progress on that problem in 50 years. Katherine Taylor for Quanta Magazine Phrased in qualitative terms, Williams’ second result may sound like the long-sought solution to the P versus PSPACE problem. The difference is a matter of scale. P and PSPACE are very broad complexity classes, while Williams’ results work at a finer level. He established a quantitative gap between the power of space and the power of time, and to prove that PSPACE is larger than P, researchers will have to make that gap much, much wider. That’s a daunting challenge, akin to prying apart a sidewalk crack with a crowbar until it’s as wide as the Grand Canyon. But it might be possible to get there by using a modified version of Williams’ simulation procedure that repeats the key step many times, saving a bit of space each time. It’s like a way to repeatedly ratchet up the length of your crowbar — make it big enough, and you can pry open anything. That repeated improvement doesn’t work with the current version of the algorithm, but researchers don’t know whether that’s a fundamental limitation. “It could be an ultimate bottleneck, or it could be a 50-year bottleneck,” Valiant said. “Or it could be something which maybe someone can solve next week.” If the problem is solved next week, Williams will be kicking himself. Before he wrote the paper, he spent months trying and failing to extend his result. But even if such an extension is not possible, Williams is confident that more space exploration is bound to lead somewhere interesting — perhaps progress on an entirely different problem. “I can never prove precisely the things that I want to prove,” he said. “But often, the thing I prove is way better than what I wanted.” Editor’s note: Scott Aaronson is a member of Quanta Magazine’s advisory board.",
  "image": "https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Social.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"postBody\"\u003e\n                \u003cdiv\u003e\n        \u003cp\u003e\n            One computer scientist’s “stunning” proof is the first progress in 50 years on one of the most famous questions in computer science.        \u003c/p\u003e\n        \n    \u003c/div\u003e\n\u003cfigure\u003e\n    \n\u003c/figure\u003e\n    \u003cfigure\u003e\n        \u003cdiv\u003e\n                            \u003cp\u003e\u003cimg width=\"2560\" height=\"1440\" src=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede.webp\" alt=\"\" decoding=\"async\" fetchpriority=\"high\" srcset=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-1720x968.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-520x293.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-768x432.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-1536x864.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-2048x1152.webp 2048w\" sizes=\"(max-width: 2560px) 100vw, 2560px\"/\u003e                \u003c/p\u003e\n                        \u003c/div\u003e\n        \u003cfigcaption\u003e\n    \u003cdiv\u003e\n            \u003cp\u003eIrene Pérez for \u003cem\u003eQuanta Magazine\u003c/em\u003e\u003c/p\u003e\n        \u003c/div\u003e\n\u003c/figcaption\u003e\n    \u003c/figure\u003e\n\u003cdiv data-role=\"selectable\"\u003e\n    \u003cp\u003e\u003cspan\u003eO\u003c/span\u003ene July afternoon in 2024, \u003ca href=\"https://people.csail.mit.edu/rrw/\"\u003eRyan Williams\u003c/a\u003e set out to prove himself wrong. Two months had passed since he’d hit upon a startling discovery about the relationship between time and memory in computing. It was a rough sketch of a mathematical proof that memory was more powerful than computer scientists believed: A small amount would be as helpful as a lot of time in all conceivable computations. That sounded so improbable that he assumed something had to be wrong, and he promptly set the proof aside to focus on less crazy ideas. Now, he’d finally carved out time to find the error.\u003c/p\u003e\n\u003cp\u003eThat’s not what happened. After hours of poring over his argument, Williams couldn’t find a single flaw.\u003c/p\u003e\n\u003cp\u003e“I just thought I was losing my mind,” said Williams, a theoretical computer scientist at the Massachusetts Institute of Technology. For the first time, he began to entertain the possibility that maybe, just maybe, memory really was as powerful as his work suggested.\u003c/p\u003e\n\u003cp\u003eOver the months that followed, he fleshed out the details, scrutinized every step, and solicited feedback from a handful of other researchers. In February, he finally \u003ca href=\"https://arxiv.org/abs/2502.17779\"\u003eposted his proof online\u003c/a\u003e, to widespread acclaim.\u003c/p\u003e\n\u003cp\u003e“It’s amazing. It’s beautiful,” said \u003ca href=\"https://www.math.ias.edu/avi/home\"\u003eAvi Wigderson\u003c/a\u003e, a theoretical computer scientist at the Institute for Advanced Study in Princeton, New Jersey. As soon as he heard the news, Wigderson sent Williams a congratulatory email. Its subject line: “You blew my mind.”\u003c/p\u003e\n\u003cp\u003eTime and memory (also called space) are the two most fundamental resources in computation: Every algorithm takes some time to run, and requires some space to store data while it’s running. Until now, the only known algorithms for accomplishing certain tasks required an amount of space roughly proportional to their runtime, and researchers had long assumed there’s no way to do better. Williams’ proof established a mathematical procedure for transforming any algorithm — no matter what it does — into a form that uses much less space.\u003c/p\u003e\n\u003c/div\u003e\n    \u003cfigure\u003e\n        \u003cdiv\u003e\n                            \u003cp\u003e\u003cimg width=\"2301\" height=\"1577\" src=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp.webp\" alt=\"\" decoding=\"async\" srcset=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp.webp 2301w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-1720x1179.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-520x356.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-768x526.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-1536x1053.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-2048x1404.webp 2048w\" sizes=\"(max-width: 2301px) 100vw, 2301px\"/\u003e                \u003c/p\u003e\n                        \u003c/div\u003e\n        \u003cfigcaption\u003e\n    \u003cdiv\u003e\n                            \u003cp\u003eRyan Williams stunned his colleagues with a milestone proof about the relationship between time and space in computation.\u003c/p\u003e\n            \u003cp\u003eKatherine Taylor for \u003cem\u003eQuanta Magazine\u003c/em\u003e\u003c/p\u003e\n        \u003c/div\u003e\n\u003c/figcaption\u003e\n    \u003c/figure\u003e\n\u003cdiv data-role=\"selectable\"\u003e\n    \u003cp\u003eWhat’s more, this result — a statement about what you can compute given a certain amount of space — also implies a second result, about what you cannot compute in a certain amount of time. This second result isn’t surprising in itself: Researchers expected it to be true, but they had no idea how to prove it. Williams’ solution, based on his sweeping first result, feels almost cartoonishly excessive, akin to proving a suspected murderer guilty by establishing an ironclad alibi for everyone else on the planet. It could also offer a new way to attack one of the oldest open problems in computer science.\u003c/p\u003e\n\u003cp\u003e“It’s a pretty stunning result, and a massive advance,” said \u003ca href=\"https://www.cs.washington.edu/people/faculty/paul-beame/\"\u003ePaul Beame\u003c/a\u003e, a computer scientist at the University of Washington. “It’s a little bit less of a surprise that it’s Ryan doing this.”\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eSpace to Roam\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eWilliams, 46, has an open, expressive face and a hint of gray in his hair. His office, which looks out over the colorful spires of MIT’s Stata Center, is another illustration of the creative use of space. A pair of yoga mats have transformed a window ledge into a makeshift reading nook, and the desk is tucked into an oddly shaped corner, freeing up room for a couch facing a large whiteboard brimming with mathematical scribblings.\u003c/p\u003e\n\u003cp\u003eMIT is a long way from Williams’ childhood home in rural Alabama, where there was no shortage of space. He grew up on a 50-acre farm and first saw a computer at age 7, when his mother drove him across the county for a special academic enrichment class. He recalled being transfixed by a simple program for generating a digital fireworks display.\u003c/p\u003e\n\u003cp\u003e“It was taking a random color and sending it in a random direction from the middle of the monitor,” Williams said. “You couldn’t have predicted what picture you’re going to get.” The world of computers seemed a wild and wonderful playground, full of infinite possibilities. Young Williams was hooked.\u003c/p\u003e\n\u003cp\u003e“I was writing programs to myself, on paper, to be run on a future computer,” he said. “My parents didn’t really know what to do with me.”\u003c/p\u003e\n\u003c/div\u003e\n    \u003cfigure\u003e\n        \u003cdiv\u003e\n                            \u003cp\u003e\u003cimg width=\"1799\" height=\"2560\" src=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-scaled.webp\" alt=\"\" decoding=\"async\" srcset=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-scaled.webp 1799w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-1209x1720.webp 1209w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-365x520.webp 365w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-768x1093.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-1079x1536.webp 1079w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-1439x2048.webp 1439w\" sizes=\"(max-width: 1799px) 100vw, 1799px\"/\u003e                \u003c/p\u003e\n                        \u003c/div\u003e\n        \u003cfigcaption\u003e\n    \u003cdiv\u003e\n                            \u003cp\u003eWilliams’ office, like his new result, makes creative use of space.\u003c/p\u003e\n            \u003cp\u003eKatherine Taylor for \u003cem\u003eQuanta Magazine\u003c/em\u003e\u003c/p\u003e\n        \u003c/div\u003e\n\u003c/figcaption\u003e\n    \u003c/figure\u003e\n\u003cdiv data-role=\"selectable\"\u003e\n    \u003cp\u003eAs he grew older, Williams advanced from imaginary computers to real ones. For his last two years of high school, he transferred to the Alabama School of Math and Science, a prestigious public boarding school, where he first encountered the theoretical side of computer science.\u003c/p\u003e\n\u003cp\u003e“I realized that there was a wider world of things out there, and that there was a way to think mathematically about computers,” he said. “That’s what I wanted to do.”\u003c/p\u003e\n\u003cp\u003eWilliams was especially drawn to a branch of theoretical computer science called computational complexity theory. It deals with the resources (such as time and space) that are needed to solve computational problems such as sorting lists or factoring numbers. Most problems can be solved by many different algorithms, each with its own demands on time and space. Complexity theorists sort problems into categories, called complexity classes, based on the resource demands of the best algorithms for solving them — that is, the algorithms that run fastest or use the least amount of space.\u003c/p\u003e\n\u003cp\u003eBut how do you make the study of computational resources mathematically rigorous? You won’t get far if you try to analyze time and space by comparing minutes to megabytes. To make any progress, you need to start with the right definitions.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eGetting Resourceful\u003c/strong\u003e\u003c/h2\u003e\n\n\u003cp\u003eThose definitions emerged from the work of Juris Hartmanis, a pioneering computer scientist who had experience making do with limited resources. He was born in 1928 into a prominent Latvian family, but his childhood was disrupted by the outbreak of World War II. Occupying Soviet forces arrested and executed his father, and after the war Hartmanis finished high school in a refugee camp. He went on to university, where he excelled even though he couldn’t afford textbooks.\u003c/p\u003e\n\u003cp\u003e“I compensated by taking very detailed notes in lectures,” he recalled in a \u003ca href=\"https://dl.acm.org/doi/10.1145/1141880.1775727\"\u003e2009 interview\u003c/a\u003e. “There is a certain advantage to [having] to improvise and overcome difficulties.” Hartmanis immigrated to the United States in 1949, and worked a series of odd jobs — building agricultural machinery, manufacturing steel and even serving as a butler — while studying mathematics at the University of Kansas City. He went on to a spectacularly successful career in the young field of theoretical computer science.\u003c/p\u003e\n\u003cp\u003eIn 1960, while working at the General Electric research laboratory in Schenectady, New York, Hartmanis met Richard Stearns, a graduate student doing a summer internship. In a pair of \u003ca href=\"https://www.ams.org/journals/tran/1965-117-00/S0002-9947-1965-0170805-7/\"\u003egroundbreaking\u003c/a\u003e \u003ca href=\"https://ieeexplore.ieee.org/document/5397244/\"\u003epapers\u003c/a\u003e they established precise mathematical definitions for time and space. These definitions gave researchers the language they needed to compare the two resources and sort problems into complexity classes.\u003c/p\u003e\n\u003c/div\u003e\n    \u003cfigure\u003e\n        \u003cdiv\u003e\n                            \u003cp\u003e\u003cimg width=\"1800\" height=\"1033\" src=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis.webp\" alt=\"\" decoding=\"async\" srcset=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis.webp 1800w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-1720x987.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-520x298.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-768x441.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-1536x881.webp 1536w\" sizes=\"(max-width: 1800px) 100vw, 1800px\"/\u003e                \u003c/p\u003e\n                        \u003c/div\u003e\n        \u003cfigcaption\u003e\n    \u003cdiv\u003e\n                            \u003cp\u003eIn the 1960s, Juris Hartmanis established the definitions that computer scientists use to analyze space and time.\u003c/p\u003e\n            \u003cp\u003eDivision of Rare and Manuscript Collections, Cornell University Library\u003c/p\u003e\n        \u003c/div\u003e\n\u003c/figcaption\u003e\n    \u003c/figure\u003e\n\u003cdiv data-role=\"selectable\"\u003e\n    \u003cp\u003eOne of the most important classes goes by the humble name “P.” Roughly speaking, it encompasses all problems that can be solved in a reasonable amount of time. An analogous complexity class for space is dubbed “PSPACE.”\u003c/p\u003e\n\u003cp\u003eThe relationship between these two classes is one of the central questions of complexity theory. Every problem in P is also in PSPACE, because fast algorithms just don’t have enough time to fill up much space in a computer’s memory. If the reverse statement were also true, the two classes would be equivalent: Space and time would have comparable computational power. But complexity theorists suspect that PSPACE is a much larger class, containing many problems that aren’t in P. In other words, they believe that space is a far more powerful computational resource than time. This belief stems from the fact that algorithms can use the same small chunk of memory over and over, while time isn’t as forgiving — once it passes, you can’t get it back.\u003c/p\u003e\n\u003cp\u003e“The intuition is just so simple,” Williams said. “You can reuse space, but you can’t reuse time.”\u003c/p\u003e\n\u003cp\u003eBut intuition isn’t good enough for complexity theorists: They want rigorous proof. To prove that PSPACE is larger than P, researchers would have to show that for some problems in PSPACE, fast algorithms are categorically impossible. Where would they even start?\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eA Space Odyssey\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eAs it happened, they started at Cornell University, where Hartmanis moved in 1965 to head the newly established computer science department. Under his leadership it quickly became a center of research in complexity theory, and in the early 1970s, a pair of researchers there, John Hopcroft and Wolfgang Paul, set out to establish a precise link between time and space.\u003c/p\u003e\n\n\u003cp\u003eHopcroft and Paul knew that to resolve the P versus PSPACE problem, they’d have to prove that you can’t do certain computations in a limited amount of time. But it’s hard to prove a negative. Instead, they decided to flip the problem on its head and explore what you can do with limited space. They hoped to prove that algorithms given a certain space budget can solve all the same problems as algorithms with a slightly larger time budget. That would indicate that space is at least slightly more powerful than time — a small but necessary step toward showing that PSPACE is larger than P.\u003c/p\u003e\n\u003cp\u003eWith that goal in mind, they turned to a method that complexity theorists call simulation, which involves transforming existing algorithms into new ones that solve the same problems, but with different amounts of space and time. To understand the basic idea, imagine you’re given a fast algorithm for alphabetizing your bookshelf, but it requires you to lay out your books in dozens of small piles. You might prefer an approach that takes up less space in your apartment, even if it takes longer. A simulation is a mathematical procedure you could use to get a more suitable algorithm: Feed it the original, and it’ll give you a new algorithm that saves space at the expense of time.\u003c/p\u003e\n\u003cp\u003eAlgorithm designers have long studied these space-time trade-offs for specific tasks like sorting. But to establish a general relationship between time and space, Hopcroft and Paul needed something more comprehensive: a space-saving simulation procedure that works for every algorithm, no matter what problem it solves. They expected this generality to come at a cost. A universal simulation can’t exploit the details of any specific problem, so it probably won’t save as much space as a specialized simulation. But when Hopcroft and Paul started their work, there were no known universal methods for saving space at all. Even saving a small amount would be progress.\u003c/p\u003e\n\u003cp\u003eThe breakthrough came in 1975, after Hopcroft and Paul teamed up with a young researcher named \u003ca href=\"https://people.seas.harvard.edu/~valiant/\"\u003eLeslie Valiant\u003c/a\u003e. The trio devised a \u003ca href=\"https://dl.acm.org/doi/10.1145/322003.322015\"\u003euniversal simulation procedure\u003c/a\u003e that always saves a bit of space. No matter what algorithm you give it, you’ll get an equivalent one whose space budget is slightly smaller than the original algorithm’s time budget.\u003c/p\u003e\n\u003cp\u003e“Anything you can do in so much time, you can also do in slightly less space,” Valiant said. It was the first major step in the quest to connect space and time.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1600\" height=\"1556\" src=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor.webp\" alt=\"\" decoding=\"async\" srcset=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor.webp 1600w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor-520x506.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor-768x747.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor-1536x1494.webp 1536w\" sizes=\"(max-width: 1600px) 100vw, 1600px\"/\u003e    \u003c/p\u003e\n            \u003cfigcaption\u003e\n                            \u003cp\u003eIn 1975, Leslie Valiant helped prove that space is a slightly more powerful computational resource than time.\u003c/p\u003e\n            \u003cp\u003eKatherine Taylor for \u003cem\u003eQuanta Magazine\u003c/em\u003e\u003c/p\u003e\n        \u003c/figcaption\u003e\n    \u003c/figure\u003e\n\n\u003cp\u003eBut then progress stalled, and complexity theorists began to suspect that they’d hit a fundamental barrier. The problem was precisely the universal character of Hopcroft, Paul and Valiant’s simulation. While many problems can be solved with much less space than time, some intuitively seemed like they’d need nearly as much space as time. If so, more space-efficient universal simulations would be impossible. Paul and two other researchers soon proved that they are \u003ca href=\"https://dl.acm.org/doi/10.1145/800113.803643\"\u003eindeed impossible\u003c/a\u003e, provided you make one seemingly obvious assumption: Different chunks of data can’t occupy the same space in memory at the same time.\u003c/p\u003e\n\u003cp\u003e“Everybody took it for granted that you cannot do better,” Wigderson said.\u003c/p\u003e\n\u003cp\u003ePaul’s result suggested that resolving the P versus PSPACE problem would require abandoning simulation altogether in favor of a different approach, but nobody had any good ideas. That was where the problem stood for 50 years — until Williams finally broke the logjam.\u003c/p\u003e\n\u003cp\u003eFirst, he had to get through college.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eComplexity Classes\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eIn 1996, the time came for Williams to apply to colleges. He knew that pursuing complexity theory would take him far from home, but his parents made it clear that the West Coast and Canada were out of the question. Among his remaining options, Cornell stood out to him for its prominent place in the history of his favorite discipline.\u003c/p\u003e\n\u003cp\u003e“This guy Hartmanis defined the time and space complexity classes,” he recalled thinking. “That was important for me.”\u003c/p\u003e\n\u003cp\u003eWilliams was admitted to Cornell with generous financial aid and arrived in the fall of 1997, planning to do whatever it took to become a complexity theorist himself. His single-mindedness stuck out to his fellow students.\u003c/p\u003e\n\u003cp\u003e“He was just super-duper into complexity theory,” said \u003ca href=\"https://www.cs.utexas.edu/people/faculty-researchers/scott-aaronson\"\u003eScott Aaronson\u003c/a\u003e, a computer scientist at the University of Texas, Austin, who overlapped with Williams at Cornell.\u003c/p\u003e\n\u003c/div\u003e\n    \u003cfigure\u003e\n        \u003cdiv\u003e\n                            \u003cp\u003e\u003cimg width=\"2560\" height=\"1398\" src=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-scaled.webp\" alt=\"\" decoding=\"async\" srcset=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-1720x939.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-520x284.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-768x419.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-1536x839.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-2048x1118.webp 2048w\" sizes=\"(max-width: 2560px) 100vw, 2560px\"/\u003e                \u003c/p\u003e\n                        \u003c/div\u003e\n        \u003cfigcaption\u003e\n    \u003cdiv\u003e\n                            \u003cp\u003eWilliams grew interested in the relationship between space and time as an undergraduate, but never found an opportunity to work on it until last year.\u003c/p\u003e\n            \u003cp\u003eKatherine Taylor for \u003cem\u003eQuanta Magazine\u003c/em\u003e\u003c/p\u003e\n        \u003c/div\u003e\n\u003c/figcaption\u003e\n    \u003c/figure\u003e\n\u003cdiv data-role=\"selectable\"\u003e\n    \u003cp\u003eBut by sophomore year, Williams was struggling to keep up in courses that emphasized mathematical rigor over intuition. After he got a middling grade in a class on the theory of computing, the teacher suggested he consider other careers. But Williams wouldn’t give up his dream. He doubled down and enrolled in a graduate theory course, hoping that a stellar grade in the harder class would look impressive on his grad school applications. The professor teaching that graduate course was Hartmanis, by then an elder statesman in the field.\u003c/p\u003e\n\u003cp\u003eWilliams began attending Hartmanis’ office hours every week, where he was almost always the only student who showed up. His tenacity paid off: he earned an A in the course, and Hartmanis agreed to advise him on an independent research project the following semester. Their weekly meetings continued throughout Williams’ time in college. Hartmanis encouraged him to cultivate an individual approach to complexity research and gently steered him away from dead ends.\u003c/p\u003e\n\u003cp\u003e“I was deeply influenced by him then,” Williams said. “I guess I still am now.”\u003c/p\u003e\n\n\u003cp\u003eBut despite earning a coveted graduate research fellowship from the National Science Foundation, Williams was rejected by every doctoral program he applied to. On hearing the news, Hartmanis phoned a colleague, then turned around and congratulated Williams on getting accepted into a yearlong master’s program at Cornell. A year later Williams again applied to various doctoral programs, and with that extra research experience under his belt, he found success.\u003c/p\u003e\n\u003cp\u003eWilliams continued working in complexity theory in grad school and the years that followed. In 2010, four years after receiving his doctorate, he proved a \u003ca href=\"https://ieeexplore.ieee.org/document/5959801\"\u003emilestone result\u003c/a\u003e — a small step, but the largest in decades, toward solving the \u003ca href=\"https://www.quantamagazine.org/complexity-theorys-50-year-journey-to-the-limits-of-knowledge-20230817/\"\u003emost famous question in theoretical computer science\u003c/a\u003e, about the nature of hard problems. That result cemented Williams’ reputation, and he went on to write dozens of other papers on different topics in complexity theory.\u003c/p\u003e\n\u003cp\u003eP versus PSPACE wasn’t one of them. Williams had been fascinated by the problem since he first encountered it in college. He’d even supplemented his computer science curriculum with courses in logic and philosophy, seeking inspiration from other perspectives on time and space, to no avail.\u003c/p\u003e\n\u003cp\u003e“It’s always been in the back of my mind,” Williams said. “I just couldn’t think of anything interesting enough to say about it.”\u003c/p\u003e\n\u003cp\u003eLast year, he finally found the opportunity he’d been waiting for.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eSquishy Pebbles\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThe story of Williams’ new result started with progress on a different question about memory in computation: What problems can be solved with extremely limited space? In 2010, the pioneering complexity theorist Stephen Cook and his collaborators invented a task, called the \u003ca href=\"http://arxiv.org/abs/1005.2642\"\u003etree evaluation problem\u003c/a\u003e, that they proved would be impossible for any algorithm with a space budget below a specific threshold. But there was a loophole. The proof relied on the same commonsense assumption that Paul and his colleagues had made decades earlier: Algorithms can’t store new data in space that’s already full.\u003c/p\u003e\n\u003cp\u003eFor over a decade, complexity theorists tried to close that loophole. Then, in 2023, \u003ca href=\"https://www.falsifian.org/\"\u003eCook’s son James\u003c/a\u003e and a researcher named \u003ca href=\"https://iuuk.mff.cuni.cz/~iwmertz/\"\u003eIan Mertz\u003c/a\u003e blew it wide open, devising \u003ca href=\"https://dl.acm.org/doi/10.1145/3618260.3649664\"\u003ean algorithm\u003c/a\u003e that solved the tree evaluation problem using \u003ca href=\"https://www.quantamagazine.org/catalytic-computing-taps-the-full-power-of-a-full-hard-drive-20250218/\"\u003emuch less space\u003c/a\u003e than anyone thought possible. The elder Cook’s proof had assumed that bits of data were like pebbles that have to occupy separate places in an algorithm’s memory. But it turns out that’s not the only way to store data. “We can actually think about these pebbles as things that we can squish a little bit on top of each other,” Beame said.\u003c/p\u003e\n\u003c/div\u003e\n    \u003cfigure\u003e\n        \u003cdiv\u003e\n                            \u003cp\u003e\u003cimg width=\"2101\" height=\"2379\" src=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36.webp\" alt=\"\" decoding=\"async\" srcset=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36.webp 2101w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-1519x1720.webp 1519w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-459x520.webp 459w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-768x870.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-1357x1536.webp 1357w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-1809x2048.webp 1809w\" sizes=\"(max-width: 2101px) 100vw, 2101px\"/\u003e                \u003c/p\u003e\n                                \u003cp\u003e\u003cimg width=\"1716\" height=\"1817\" src=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky.webp\" alt=\"\" decoding=\"async\" srcset=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky.webp 1716w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-1624x1720.webp 1624w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-491x520.webp 491w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-768x813.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-1451x1536.webp 1451w\" sizes=\"(max-width: 1716px) 100vw, 1716px\"/\u003e                \u003c/p\u003e\n                        \u003c/div\u003e\n        \u003cfigcaption\u003e\n    \u003cdiv\u003e\n                            \u003cp\u003eJames Cook (left) and Ian Mertz recently devised a new algorithm that solved a specific problem using much less space than anyone thought possible.\u003c/p\u003e\n            \u003cp\u003eColin Morris; Michal Koucký\u003c/p\u003e\n        \u003c/div\u003e\n\u003c/figcaption\u003e\n    \u003c/figure\u003e\n\u003cdiv data-role=\"selectable\"\u003e\n    \u003cp\u003eCook and Mertz’s algorithm roused the curiosity of many researchers, but it wasn’t clear that it had any applications beyond the tree evaluation problem. “Nobody saw how central it is to time versus space itself,” Wigderson said.\u003c/p\u003e\n\u003cp\u003eRyan Williams was the exception. In spring 2024, a group of students gave a presentation about the Cook and Mertz paper as their final project in a class he was teaching. Their enthusiasm inspired him to take a closer look. As soon as he did, an idea jumped out at him. Cook and Mertz’s method, he realized, was really a general-purpose tool for reducing space usage. Why not use it to power a new universal simulation linking time and space — like the one designed by Hopcroft, Paul and Valiant, but better?\u003c/p\u003e\n\u003cp\u003eThat classic result was a way to transform any algorithm with a given time budget into a new algorithm with a slightly smaller space budget. Williams saw that a simulation based on squishy pebbles would make the new algorithm’s space usage much smaller — roughly equal to the square root of the original algorithm’s time budget. That new space-efficient algorithm would also be much slower, so the simulation was not likely to have practical applications. But from a theoretical point of view, it was nothing short of revolutionary.\u003c/p\u003e\n\n\u003cp\u003eFor 50 years, researchers had assumed it was impossible to improve Hopcroft, Paul and Valiant’s universal simulation. Williams’ idea — if it worked — wouldn’t just beat their record — it would demolish it.\u003c/p\u003e\n\u003cp\u003e“I thought about it, and I was like, ‘Well, that just simply can’t be true,’” Williams said. He set it aside and didn’t come back to it until that fateful day in July, when he tried to find the flaw in the argument and failed. After he realized that there was no flaw, he spent months writing and rewriting the proof to make it as clear as possible.\u003c/p\u003e\n\u003cp\u003eAt the end of February, Williams finally \u003ca href=\"https://arxiv.org/abs/2502.17779\"\u003eput the finished paper online\u003c/a\u003e. Cook and Mertz were as surprised as everyone else. “I had to go take a long walk before doing anything else,” Mertz said.\u003c/p\u003e\n\u003cp\u003eValiant got a sneak preview of Williams’ improvement on his decades-old result during his morning commute. For years, he’s taught at Harvard University, just down the road from Williams’ office at MIT. They’d met before, but they didn’t know they lived in the same neighborhood until they bumped into each other on the bus on a snowy February day, a few weeks before the result was public. Williams described his proof to the startled Valiant and promised to send along his paper.\u003c/p\u003e\n\u003cp\u003e“I was very, very impressed,” Valiant said. “If you get any mathematical result which is the best thing in 50 years, you must be doing something right.”\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003ePSPACE: The Final Frontier\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eWith his new simulation, Williams had proved a positive result about the computational power of space: Algorithms that use relatively little space can solve all problems that require a somewhat larger amount of time. Then, using just a few lines of math, he flipped that around and proved a negative result about the computational power of time: At least a few problems can’t be solved unless you use more time than space. That second, narrower result is in line with what researchers expected. The weird part is how Williams got there, by first proving a result that applies to all algorithms, no matter what problems they solve.\u003c/p\u003e\n\u003cp\u003e“I still have a hard time believing it,” Williams said. “It just seems too good to be true.”\u003c/p\u003e\n\u003c/div\u003e\n    \u003cfigure\u003e\n        \u003cdiv\u003e\n                            \u003cp\u003e\u003cimg width=\"1967\" height=\"2560\" src=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-scaled.webp\" alt=\"\" decoding=\"async\" srcset=\"https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-scaled.webp 1967w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-1322x1720.webp 1322w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-400x520.webp 400w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-768x999.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-1180x1536.webp 1180w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-1574x2048.webp 1574w\" sizes=\"(max-width: 1967px) 100vw, 1967px\"/\u003e                \u003c/p\u003e\n                        \u003c/div\u003e\n        \u003cfigcaption\u003e\n    \u003cdiv\u003e\n                            \u003cp\u003eWilliams used Cook and Mertz’s technique to establish a stronger link between space and time — the first progress on that problem in 50 years.\u003c/p\u003e\n            \u003cp\u003eKatherine Taylor for \u003cem\u003eQuanta Magazine\u003c/em\u003e\u003c/p\u003e\n        \u003c/div\u003e\n\u003c/figcaption\u003e\n    \u003c/figure\u003e\n\u003cdiv data-role=\"selectable\"\u003e\n    \u003cp\u003ePhrased in qualitative terms, Williams’ second result may sound like the long-sought solution to the P versus PSPACE problem. The difference is a matter of scale. P and PSPACE are very broad complexity classes, while Williams’ results work at a finer level. He established a quantitative gap between the power of space and the power of time, and to prove that PSPACE is larger than P, researchers will have to make that gap much, much wider.\u003c/p\u003e\n\u003cp\u003eThat’s a daunting challenge, akin to prying apart a sidewalk crack with a crowbar until it’s as wide as the Grand Canyon. But it might be possible to get there by using a modified version of Williams’ simulation procedure that repeats the key step many times, saving a bit of space each time. It’s like a way to repeatedly ratchet up the length of your crowbar — make it big enough, and you can pry open anything. That repeated improvement doesn’t work with the current version of the algorithm, but researchers don’t know whether that’s a fundamental limitation.\u003c/p\u003e\n        \n        \n\u003cp\u003e“It could be an ultimate bottleneck, or it could be a 50-year bottleneck,” Valiant said. “Or it could be something which maybe someone can solve next week.”\u003c/p\u003e\n\u003cp\u003eIf the problem is solved next week, Williams will be kicking himself. Before he wrote the paper, he spent months trying and failing to extend his result. But even if such an extension is not possible, Williams is confident that more space exploration is bound to lead somewhere interesting — perhaps progress on an entirely different problem.\u003c/p\u003e\n\u003cp\u003e“I can never prove precisely the things that I want to prove,” he said. “But often, the thing I prove is way better than what I wanted.”\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eEditor’s note: Scott Aaronson is a member of \u003c/em\u003eQuanta Magazine\u003cem\u003e’s \u003c/em\u003e\u003ca href=\"https://www.quantamagazine.org/about/\"\u003e\u003cem\u003eadvisory board\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n                \n                \n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "25 min read",
  "publishedTime": "2025-05-21T14:50:23Z",
  "modifiedTime": "2025-05-21T20:51:56Z"
}
