{
  "id": "bfe58ed5-9bcb-434e-bce5-c4c38a9f3719",
  "title": "Inception Emerges From Stealth With a New Type of AI Model",
  "link": "https://slashdot.org/story/25/02/26/2257224/inception-emerges-from-stealth-with-a-new-type-of-ai-model?utm_source=rss1.0mainlinkanon\u0026utm_medium=feed",
  "description": "Inception, a Palo Alto-based AI company founded by Stanford professor Stefano Ermon, claims to have developed a novel diffusion-based large language model (DLM) that significantly outperforms traditional LLMs in speed and efficiency. \"Inception's model offers the capabilities of traditional LLMs, including code generation and question-answering, but with significantly faster performance and reduced computing costs, according to the company,\" reports TechCrunch. From the report: Ermon hypothesized generating and modifying large blocks of text in parallel was possible with diffusion models. After years of trying, Ermon and a student of his achieved a major breakthrough, which they detailed in a research paper published last year. Recognizing the advancement's potential, Ermon founded Inception last summer, tapping two former students, UCLA professor Aditya Grover and Cornell professor Volodymyr Kuleshov, to co-lead the company. [...] \"What we found is that our models can leverage the GPUs much more efficiently,\" Ermon said, referring to the computer chips commonly used to run models in production. \"I think this is a big deal. This is going to change the way people build language models.\" Inception offers an API as well as on-premises and edge device deployment options, support for model fine-tuning, and a suite of out-of-the-box DLMs for various use cases. The company claims its DLMs can run up to 10x faster than traditional LLMs while costing 10x less. \"Our 'small' coding model is as good as [OpenAI's] GPT-4o mini while more than 10 times as fast,\" a company spokesperson told TechCrunch. \"Our 'mini' model outperforms small open-source models like [Meta's] Llama 3.1 8B and achieves more than 1,000 tokens per second.\" Read more of this story at Slashdot.",
  "author": "BeauHD",
  "published": "2025-02-27T00:20:00+00:00",
  "source": "http://rss.slashdot.org/Slashdot/slashdotMain",
  "categories": [
    "ai"
  ],
  "byline": "",
  "length": 1747,
  "excerpt": "Inception, a Palo Alto-based AI company founded by Stanford professor Stefano Ermon, claims to have developed a novel diffusion-based large language model (DLM) that significantly outperforms traditional LLMs in speed and efficiency. \"Inception's model offers the capabilities of traditional LLMs, in...",
  "siteName": "",
  "favicon": "",
  "text": "Inception, a Palo Alto-based AI company founded by Stanford professor Stefano Ermon, claims to have developed a novel diffusion-based large language model (DLM) that significantly outperforms traditional LLMs in speed and efficiency. \"Inception's model offers the capabilities of traditional LLMs, including code generation and question-answering, but with significantly faster performance and reduced computing costs, according to the company,\" reports TechCrunch. From the report: Ermon hypothesized generating and modifying large blocks of text in parallel was possible with diffusion models. After years of trying, Ermon and a student of his achieved a major breakthrough, which they detailed in a research paper published last year. Recognizing the advancement's potential, Ermon founded Inception last summer, tapping two former students, UCLA professor Aditya Grover and Cornell professor Volodymyr Kuleshov, to co-lead the company. [...] \"What we found is that our models can leverage the GPUs much more efficiently,\" Ermon said, referring to the computer chips commonly used to run models in production. \"I think this is a big deal. This is going to change the way people build language models.\" Inception offers an API as well as on-premises and edge device deployment options, support for model fine-tuning, and a suite of out-of-the-box DLMs for various use cases. The company claims its DLMs can run up to 10x faster than traditional LLMs while costing 10x less. \"Our 'small' coding model is as good as [OpenAI's] GPT-4o mini while more than 10 times as fast,\" a company spokesperson told TechCrunch. \"Our 'mini' model outperforms small open-source models like [Meta's] Llama 3.1 8B and achieves more than 1,000 tokens per second.\"",
  "image": "https://a.fsdn.com/sd/topics/ai_64.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"fhbody-176553963\"\u003e\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\u003cp\u003e\n\t\t\t\n\t\t \t\n\t\t\t\tInception, a Palo Alto-based AI company founded by Stanford professor Stefano Ermon, claims to have developed a novel diffusion-based large language model (DLM) that \u003ca href=\"https://techcrunch.com/2025/02/26/inception-emerges-from-stealth-with-a-new-type-of-ai-model/\"\u003esignificantly outperforms traditional LLMs in speed and efficiency\u003c/a\u003e. \u0026#34;Inception\u0026#39;s model offers the capabilities of traditional LLMs, including code generation and question-answering, but with significantly faster performance and reduced computing costs, according to the company,\u0026#34; reports TechCrunch. From the report: \u003ci\u003e Ermon hypothesized generating and modifying large blocks of text in parallel was possible with diffusion models. After years of trying, Ermon and a student of his achieved a major breakthrough, which they detailed in a \u003ca href=\"https://arxiv.org/pdf/2310.16834\"\u003eresearch paper\u003c/a\u003e published last year. Recognizing the advancement\u0026#39;s potential, Ermon founded Inception last summer, tapping two former students, UCLA professor Aditya Grover and Cornell professor Volodymyr Kuleshov, to co-lead the company. [...]\n\u003cp\u003e \n\u0026#34;What we found is that our models can leverage the GPUs much more efficiently,\u0026#34; Ermon said, referring to the computer chips commonly used to run models in production. \u0026#34;I think this is a big deal. This is going to change the way people build language models.\u0026#34; Inception offers an API as well as on-premises and edge device deployment options, support for model fine-tuning, and a suite of out-of-the-box DLMs for various use cases. The company claims its DLMs can run up to 10x faster than traditional LLMs while costing 10x less. \u0026#34;Our \u0026#39;small\u0026#39; coding model is as good as [OpenAI\u0026#39;s] GPT-4o mini while more than 10 times as fast,\u0026#34; a company spokesperson told TechCrunch. \u0026#34;Our \u0026#39;mini\u0026#39; model outperforms small open-source models like [Meta\u0026#39;s] Llama 3.1 8B and achieves more than 1,000 tokens per second.\u0026#34;\u003c/p\u003e\u003c/i\u003e\n\t\t \t\n\t\t\u003c/p\u003e\n\n\t\t\n\n\t\t\n\n\t\t\n\t\t\t\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": null,
  "modifiedTime": null
}
