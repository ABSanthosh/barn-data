{
  "id": "508b572c-f1cc-4696-a13b-b8f944bf75d0",
  "title": "Show HN: My LLM CLI tool can run tools now, from Python code or plugins",
  "link": "https://simonwillison.net/2025/May/27/llm-tools/",
  "description": "Comments",
  "author": "",
  "published": "Tue, 27 May 2025 20:53:03 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Simon Willison",
  "length": 14529,
  "excerpt": "LLM 0.26 is out with the biggest new feature since I started the project: support for tools. You can now use the LLM CLI tool—and Python library—to grant LLMs from …",
  "siteName": "Simon Willison’s Weblog",
  "favicon": "",
  "text": "27th May 2025 LLM 0.26 is out with the biggest new feature since I started the project: support for tools. You can now use the LLM CLI tool—and Python library—to grant LLMs from OpenAI, Anthropic, Gemini and local models from Ollama with access to any tool that you can represent as a Python function. LLM also now has tool plugins, so you can install a plugin that adds new capabilities to whatever model you are currently using. There’s a lot to cover here, but here are the highlights: LLM can run tools now! You can install tools from plugins and load them by name with --tool/-T name_of_tool. You can also pass in Python function code on the command-line with the --functions option. The Python API supports tools too: llm.get_model(\"gpt-4.1\").chain(\"show me the locals\", tools=[locals]).text() Tools work in both async and sync contexts. Here’s what’s covered in this post: Trying it out More interesting tools from plugins Ad-hoc command-line tools with --functions Tools in the LLM Python API Why did this take me so long? Is this agents then? What’s next for tools in LLM? Trying it out First, install the latest LLM. It may not be on Homebrew yet so I suggest using pip or pipx or uv: If you have it already, upgrade it. Tools work with other vendors, but let’s stick with OpenAI for the moment. Give LLM an OpenAI API key llm keys set openai # Paste key here Now let’s run our first tool: llm --tool llm_version \"What version?\" --td Here’s what I get: llm_version is a very simple demo tool that ships with LLM. Running --tool llm_version exposes that tool to the model—you can specify that multiple times to enable multiple tools, and it has a shorter version of -T to save on typing. The --td option stands for --tools-debug—it causes LLM to output information about tool calls and their responses so you can peek behind the scenes. This is using the default LLM model, which is usually gpt-4o-mini. I switched it to gpt-4.1-mini (better but fractionally more expensive) by running: llm models default gpt-4.1-mini You can try other models using the -m option. Here’s how to run a similar demo of the llm_time built-in tool using o4-mini: llm --tool llm_time \"What time is it?\" --td -m o4-mini Outputs: Tool call: llm_time({}) { \"utc_time\": \"2025-05-27 19:15:55 UTC\", \"utc_time_iso\": \"2025-05-27T19:15:55.288632+00:00\", \"local_timezone\": \"PDT\", \"local_time\": \"2025-05-27 12:15:55\", \"timezone_offset\": \"UTC-7:00\", \"is_dst\": true } The current time is 12:15 PM PDT (UTC−7:00) on May 27, 2025, which corresponds to 7:15 PM UTC. Models from (tool supporting) plugins work too. Anthropic’s Claude Sonnet 4: llm install llm-anthropic -U llm keys set anthropic # Paste Anthropic key here llm --tool llm_version \"What version?\" --td -m claude-4-sonnet Or Google’s Gemini 2.5 Flash: llm install llm-gemini -U llm keys set gemini # Paste Gemini key here llm --tool llm_version \"What version?\" --td -m gemini-2.5-flash-preview-05-20 You can even run simple tools with Qwen3:4b, a tiny (2.6GB) model that I run using Ollama: ollama pull qwen3:4b llm install 'llm-ollama\u003e=0.11a0' llm --tool llm_version \"What version?\" --td -m qwen3:4b Qwen 3 calls the tool, thinks about it a bit and then prints out a response: More interesting tools from plugins This demo has been pretty weak so far. Let’s do something a whole lot more interesting. LLMs are notoriously bad at mathematics. This is deeply surprising to many people: supposedly the most sophisticated computer systems we’ve ever built can’t multiply two large numbers together? We can fix that with tools. The llm-tools-simpleeval plugin exposes the simpleeval “Simple Safe Sandboxed Extensible Expression Evaluator for Python” library by Daniel Fairhead. This provides a robust-enough sandbox for executing simple Python expressions. Here’s how to run a calculation: llm install llm-tools-simpleeval llm -T simpleeval Trying that out: llm -T simple_eval 'Calculate 1234 * 4346 / 32414 and square root it' --td I got back this—it tried sqrt() first, then when that didn’t work switched to ** 0.5 instead: Tool call: simple_eval({'expression': '1234 * 4346 / 32414'}) 165.45208860368976 Tool call: simple_eval({'expression': 'sqrt(1234 * 4346 / 32414)'}) Error: Function 'sqrt' not defined, for expression 'sqrt(1234 * 4346 / 32414)'. Tool call: simple_eval({'expression': '(1234 * 4346 / 32414) ** 0.5'}) 12.862818066181678 The result of (1234 * 4346 / 32414) is approximately 165.45, and the square root of this value is approximately 12.86. I’ve released four tool plugins so far: llm-tools-simpleeval—as shown above, simple expression support for things like mathematics. llm-tools-quickjs—provides access to a sandboxed QuickJS JavaScript interpreter, allowing LLMs to run JavaScript code. The environment persists between calls so the model can set variables and build functions and reuse them later on. llm-tools-sqlite—read-only SQL query access to a local SQLite database. llm-tools-datasette—run SQL queries against a remote Datasette instance! Let’s try that Datasette one now: llm install llm-tools-datasette llm -T 'Datasette(\"https://datasette.io/content\")' --td \"What has the most stars?\" The syntax here is slightly different: the Datasette plugin is what I’m calling a “toolbox”—a plugin that has multiple tools inside it and can be configured with a constructor. Specifying --tool as Datasette(\"https://datasette.io/content\") provides the plugin with the URL to the Datasette instance it should use—in this case the content database that powers the Datasette website. Here’s the output, with the schema section truncated for brevity: This question triggered three calls. The model started by guessing the query! It tried SELECT name, stars FROM repos ORDER BY stars DESC LIMIT 1, which failed because the stars column doesn’t exist. The tool call returned an error, so the model had another go—this time calling the Datasette_schema() tool to get the schema of the database. Based on that schema it assembled and then executed the correct query, and output its interpretation of the result: The repository with the most stars is “datasette” with 10,020 stars. Getting to this point was a real Penny Arcade Minecraft moment for me. The possibilities here are limitless. If you can write a Python function for it, you can trigger it from an LLM. Ad-hoc command-line tools with --functions I’m looking forward to people building more plugins, but there’s also much less structured and more ad-hoc way to use tools with the LLM CLI tool: the --functions option. This was inspired by a similar feature I added to sqlite-utils a while ago. You can pass a block of literal Python code directly to the CLI tool using the --functions option, and any functions defined there will be made available to the model as tools. Here’s an example that adds the ability to search my blog: llm --functions ' import httpx def search_blog(q): \"Search Simon Willison blog\" return httpx.get(\"https://simonwillison.net/search/\", params={\"q\": q}).content ' --td 'Three features of sqlite-utils' -s 'use Simon search' This is such a hack of an implementation! I’m literally just hitting my search page and dumping the HTML straight back into tho model. It totally works though—it helps that the GPT-4.1 series all handle a million tokens now, so crufty HTML is no longer a problem for them. (I had to add “use Simon search” as the system prompt because without it the model would try to answer the question itself, rather than using the search tool I provided. System prompts for tools are clearly a big topic, Anthropic’s own web search tool has 6,471 tokens of instructions!) Here’s the output I got just now: Three features of sqlite-utils are: It is a combined CLI tool and Python library for manipulating SQLite databases. It can automatically add columns to a database table if you attempt to insert data that doesn’t quite fit (using the alter=True option). It supports plugins, allowing the extension of its functionality through third-party or custom plugins. A better search tool would have more detailed instructions and would return relevant snippets of the results, not just the headline and first paragraph for each result. This is pretty great for just four lines of Python though! Tools in the LLM Python API LLM is both a CLI tool and a Python library at the same time (similar to my other project sqlite-utils). The LLM Python library grew tool support in LLM 0.26 as well. Here’s a simple example solving one of the previously hardest problems in LLMs: counting the number of Rs in “strawberry”: import llm def count_char_in_text(char: str, text: str) -\u003e int: \"How many times does char appear in text?\" return text.count(char) model = llm.get_model(\"gpt-4.1-mini\") chain_response = model.chain( \"Rs in strawberry?\", tools=[count_char_in_text], after_call=print ) for chunk in chain_response: print(chunk, end=\"\", flush=True) The after_call=print argument is a way to peek at the tool calls, the Python equivalent of the --td option from earlier. The model.chain() method is new: it’s similar to model.prompt() but knows how to spot returned tool call requests, execute them and then prompt the model again with the results. A model.chain() could potentially execute dozens of responses on the way to giving you a final answer. You can iterate over the chain_response to output those tokens as they are returned by the model, even across multiple responses. I got back this: Tool(name='count_char_in_text', description='How many times does char appear in text?', input_schema={'properties': {'char': {'type': 'string'}, 'text': {'type': 'string'}}, 'required': ['char', 'text'], 'type': 'object'}, implementation=\u003cfunction count_char_in_text at 0x109dd4f40\u003e, plugin=None) ToolCall(name='count_char_in_text', arguments={'char': 'r', 'text': 'strawberry'}, tool_call_id='call_DGXcM8b2B26KsbdMyC1uhGUu') ToolResult(name='count_char_in_text', output='3', tool_call_id='call_DGXcM8b2B26KsbdMyC1uhGUu', instance=None, exception=None) There are 3 letter “r”s in the word “strawberry”. LLM’s Python library also supports asyncio, and tools can be async def functions as described here. If a model requests multiple async tools at once the library will run them concurrently with asyncio.gather(). The Toolbox form of tools is supported too: you can pass tools=[Datasette(\"https://datasette.io/content\")] to that chain() method to achieve the same effect as the --tool 'Datasette(...) option from earlier. Why did this take me so long? I’ve been tracking llm-tool-use for a while. I first saw the trick described in the ReAcT paper, first published in October 2022 (a month before the initial release of ChatGPT). I built a simple implementation of that in a few dozen lines of Python. It was clearly a very neat pattern! Over the past few years it has become very apparent that tool use is the single most effective way to extend the abilities of language models. It’s such a simple trick: you tell the model that there are tools it can use, and have it output special syntax (JSON or XML or tool_name(arguments), it doesn’t matter which) requesting a tool action, then stop. Your code parses that output, runs the requested tools and then starts a new prompt to the model with the results. This works with almost every model now. Most of them are specifically trained for tool usage, and there are leaderboards like the Berkeley Function-Calling Leaderboard dedicated to tracking which models do the best job of it. All of the big model vendors—OpenAI, Anthropic, Google, Mistral, Meta—have a version of this baked into their API, either called tool usage or function calling. It’s all the same underlying pattern. The models you can run locally are getting good at this too. Ollama added tool support last year, and it’s baked into the llama.cpp server as well. It’s been clear for a while that LLM absolutely needed to grow support for tools. I released LLM schema support back in February as a stepping stone towards this. I’m glad to finally have it over the line. As always with LLM, the challenge was designing an abstraction layer that could work across as many different models as possible. A year ago I didn’t feel that model tool support was mature enough to figure this out. Today there’s a very definite consensus among vendors about how this should work, which finally gave me the confidence to implement it. I also presented a workshop at PyCon US two weeks ago about Building software on top of Large Language Models, which was exactly the incentive I needed to finally get this working in an alpha! Here’s the tools section from that tutorial. Is this agents then? Sigh. I still don’t like using the term “agents”. I worry that developers will think tools in a loop, regular people will think virtual AI assistants voiced by Scarlett Johansson and academics will grumble about thermostats. But in the LLM world we appear to be converging on “tools in a loop”, and that’s absolutely what this. So yes, if you want to build “agents” then LLM 0.26 is a great way to do that. What’s next for tools in LLM? I already have a LLM tools v2 milestone with 13 issues in it, mainly around improvements to how tool execution logs are displayed but with quite a few minor issues I decided shouldn’t block this release. There’s a bunch more stuff in the tools label. I’m most excited about the potential for plugins. Writing tool plugins is really fun. I have an llm-plugin-tools cookiecutter template that I’ve been using for my own, and I plan to put together a tutorial around that soon. There’s more work to be done adding tool support to more model plugins. I added details of this to the advanced plugins documentation. This commit adding tool support for Gemini is a useful illustratino of what’s involved. And yes, Model Context Protocol support is clearly on the agenda as well. MCP is emerging as the standard way for models to access tools at a frankly bewildering speed. Two weeks ago it wasn’t directly supported by the APIs of any of the major vendors. In just the past eight days it’s been added by OpenAI, Anthropic and Mistral! It’s feeling like a lot less of a moving target today. I want LLM to be able to act as an MCP client, so that any of the MCP servers people are writing can be easily accessed as additional sources of tools for LLM. If you’re interested in talking more about what comes next for LLM, come and chat to us in our Discord.",
  "image": "https://static.simonwillison.net/static/2025/llm-tools-qwen.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-permalink-context=\"/2025/May/27/llm-tools/\"\u003e\n\n\u003cp\u003e27th May 2025\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://llm.datasette.io/en/stable/changelog.html#v0-26\"\u003eLLM 0.26\u003c/a\u003e\u003c/strong\u003e is out with the biggest new feature since I started the project: \u003ca href=\"https://llm.datasette.io/en/stable/tools.html\"\u003e\u003cstrong\u003esupport for tools\u003c/strong\u003e\u003c/a\u003e. You can now use the LLM \u003ca href=\"https://llm.datasette.io/en/stable/usage.html\"\u003eCLI tool\u003c/a\u003e—and \u003ca href=\"https://llm.datasette.io/en/stable/python-api.html\"\u003ePython library\u003c/a\u003e—to grant LLMs from OpenAI, Anthropic, Gemini and local models from Ollama with access to any tool that you can represent as a Python function.\u003c/p\u003e\n\u003cp\u003eLLM also now has \u003ca href=\"https://llm.datasette.io/en/stable/plugins/directory.html#tools\"\u003etool plugins\u003c/a\u003e, so you can install a plugin that adds new capabilities to whatever model you are currently using.\u003c/p\u003e\n\u003cp\u003eThere’s a lot to cover here, but here are the highlights:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eLLM can run tools now\u003c/strong\u003e! You can \u003cstrong\u003einstall tools from plugins\u003c/strong\u003e and load them by name with \u003ccode\u003e--tool/-T name_of_tool\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eYou can also \u003cstrong\u003epass in Python function code on the command-line\u003c/strong\u003e with the \u003ccode\u003e--functions\u003c/code\u003e option.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003ePython API supports tools too\u003c/strong\u003e: \u003ccode\u003ellm.get_model(\u0026#34;gpt-4.1\u0026#34;).chain(\u0026#34;show me the locals\u0026#34;, tools=[locals]).text()\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eTools work in \u003cstrong\u003eboth async and sync contexts\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere’s what’s covered in this post:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://simonwillison.net/2025/May/27/llm-tools/#trying-it-out\"\u003eTrying it out\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://simonwillison.net/2025/May/27/llm-tools/#more-interesting-tools-from-plugins\"\u003eMore interesting tools from plugins\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://simonwillison.net/2025/May/27/llm-tools/#ad-hoc-command-line-tools-with-functions\"\u003eAd-hoc command-line tools with --functions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://simonwillison.net/2025/May/27/llm-tools/#tools-in-the-llm-python-api\"\u003eTools in the LLM Python API\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://simonwillison.net/2025/May/27/llm-tools/#why-did-this-take-me-so-long-\"\u003eWhy did this take me so long?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://simonwillison.net/2025/May/27/llm-tools/#is-this-agents-then-\"\u003eIs this agents then?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://simonwillison.net/2025/May/27/llm-tools/#what-s-next-for-tools-in-llm-\"\u003eWhat’s next for tools in LLM?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ch4 id=\"trying-it-out\"\u003eTrying it out\u003c/h4\u003e\n\u003cp\u003eFirst, \u003ca href=\"https://llm.datasette.io/en/stable/setup.html\"\u003einstall the latest LLM\u003c/a\u003e. It may not be on Homebrew yet so I suggest using \u003ccode\u003epip\u003c/code\u003e or \u003ccode\u003epipx\u003c/code\u003e or \u003ccode\u003euv\u003c/code\u003e:\u003c/p\u003e\n\n\u003cp\u003eIf you have it already, \u003ca href=\"https://llm.datasette.io/en/stable/setup.html#upgrading-to-the-latest-version\"\u003eupgrade it\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eTools work with other vendors, but let’s stick with OpenAI for the moment. Give LLM an OpenAI API key\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003ellm keys \u003cspan\u003eset\u003c/span\u003e openai\n\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e Paste key here\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow let’s run our first tool:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003ellm --tool llm_version \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003eWhat version?\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e --td\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eHere’s what I get:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://static.simonwillison.net/static/2025/llm-tools.gif\" alt=\"Animated demo. I run that command, LLM shows Tool call: llm_version({}) in yellow, then 0.26a1 in green, then streams out the text The installed version is 0.26a1\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ellm_version\u003c/code\u003e is a very simple demo tool that ships with LLM. Running \u003ccode\u003e--tool llm_version\u003c/code\u003e exposes that tool to the model—you can specify that multiple times to enable multiple tools, and it has a shorter version of \u003ccode\u003e-T\u003c/code\u003e to save on typing.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003e--td\u003c/code\u003e option stands for \u003ccode\u003e--tools-debug\u003c/code\u003e—it causes LLM to output information about tool calls and their responses so you can peek behind the scenes.\u003c/p\u003e\n\u003cp\u003eThis is using the default LLM model, which is usually \u003ccode\u003egpt-4o-mini\u003c/code\u003e. I switched it to \u003ccode\u003egpt-4.1-mini\u003c/code\u003e (better but fractionally more expensive) by running:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003ellm models default gpt-4.1-mini\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can try other models using the \u003ccode\u003e-m\u003c/code\u003e option. Here’s how to run a similar demo of the \u003ccode\u003ellm_time\u003c/code\u003e built-in tool using \u003ccode\u003eo4-mini\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003ellm --tool llm_time \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003eWhat time is it?\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e --td -m o4-mini\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eOutputs:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ccode\u003eTool call: llm_time({})\u003c/code\u003e\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003e  {\n    \u003cspan\u003e\u0026#34;utc_time\u0026#34;\u003c/span\u003e: \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e2025-05-27 19:15:55 UTC\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003e\u0026#34;utc_time_iso\u0026#34;\u003c/span\u003e: \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e2025-05-27T19:15:55.288632+00:00\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003e\u0026#34;local_timezone\u0026#34;\u003c/span\u003e: \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003ePDT\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003e\u0026#34;local_time\u0026#34;\u003c/span\u003e: \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e2025-05-27 12:15:55\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003e\u0026#34;timezone_offset\u0026#34;\u003c/span\u003e: \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003eUTC-7:00\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003e\u0026#34;is_dst\u0026#34;\u003c/span\u003e: \u003cspan\u003etrue\u003c/span\u003e\n  }\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe current time is 12:15 PM PDT (UTC−7:00) on May 27, 2025, which corresponds to 7:15 PM UTC.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eModels from (tool supporting) plugins work too. Anthropic’s Claude Sonnet 4:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003ellm install llm-anthropic -U\nllm keys \u003cspan\u003eset\u003c/span\u003e anthropic\n\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e Paste Anthropic key here\u003c/span\u003e\nllm --tool llm_version \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003eWhat version?\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e --td -m claude-4-sonnet\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eOr Google’s Gemini 2.5 Flash:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003ellm install llm-gemini -U\nllm keys \u003cspan\u003eset\u003c/span\u003e gemini\n\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e Paste Gemini key here\u003c/span\u003e\nllm --tool llm_version \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003eWhat version?\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e --td -m gemini-2.5-flash-preview-05-20\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can even run simple tools with Qwen3:4b, a \u003cem\u003etiny\u003c/em\u003e (2.6GB) model that I run using \u003ca href=\"https://ollama.com/\"\u003eOllama\u003c/a\u003e:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003eollama pull qwen3:4b\nllm install \u003cspan\u003e\u003cspan\u003e\u0026#39;\u003c/span\u003ellm-ollama\u0026gt;=0.11a0\u003cspan\u003e\u0026#39;\u003c/span\u003e\u003c/span\u003e\nllm --tool llm_version \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003eWhat version?\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e --td -m qwen3:4b\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eQwen 3 calls the tool, thinks about it a bit and then prints out a response:\n\u003cimg src=\"https://static.simonwillison.net/static/2025/llm-tools-qwen.jpg\" alt=\"Tool call: llm_version({}) 0.26a1\u0026lt;think\u0026gt; Okay, the user asked, \u0026#34;What version?\u0026#34; I need to respond with the version of the LLM. The tool provided is llm_version, which returns the installed version. I called that function and got the response 0.26a1. Now I should present this information clearly. Let me check if there\u0026#39;s any additional context needed, but the user just asked for the version, so a straightforward answer should work. I\u0026#39;ll state the version number and maybe mention that it\u0026#39;s the installed version. Keep it simple and precise. \u0026lt;/think\u0026gt; The installed version of the LLM is 0.26a1.\"/\u003e\u003c/p\u003e\n\u003ch4 id=\"more-interesting-tools-from-plugins\"\u003eMore interesting tools from plugins\u003c/h4\u003e\n\u003cp\u003eThis demo has been pretty weak so far. Let’s do something a whole lot more interesting.\u003c/p\u003e\n\u003cp\u003eLLMs are notoriously bad at mathematics. This is deeply surprising to many people: supposedly the most sophisticated computer systems we’ve ever built can’t multiply two large numbers together?\u003c/p\u003e\n\u003cp\u003eWe can fix that with tools.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://github.com/simonw/llm-tools-simpleeval\"\u003ellm-tools-simpleeval\u003c/a\u003e plugin exposes the \u003ca href=\"https://github.com/danthedeckie/simpleeval\"\u003esimpleeval\u003c/a\u003e “Simple Safe Sandboxed Extensible Expression Evaluator for Python” library by Daniel Fairhead. This provides a robust-enough sandbox for executing simple Python expressions.\u003c/p\u003e\n\u003cp\u003eHere’s how to run a calculation:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003ellm install llm-tools-simpleeval\nllm -T simpleeval \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTrying that out:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003ellm -T simple_eval \u003cspan\u003e\u003cspan\u003e\u0026#39;\u003c/span\u003eCalculate 1234 * 4346 / 32414 and square root it\u003cspan\u003e\u0026#39;\u003c/span\u003e\u003c/span\u003e --td\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eI got back this—it tried \u003ccode\u003esqrt()\u003c/code\u003e first, then when that didn’t work switched to \u003ccode\u003e** 0.5\u003c/code\u003e instead:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eTool call: simple_eval({\u0026#39;expression\u0026#39;: \u0026#39;1234 * 4346 / 32414\u0026#39;})\n  165.45208860368976\n\n\nTool call: simple_eval({\u0026#39;expression\u0026#39;: \u0026#39;sqrt(1234 * 4346 / 32414)\u0026#39;})\n  Error: Function \u0026#39;sqrt\u0026#39; not defined, for expression \u0026#39;sqrt(1234 * 4346 / 32414)\u0026#39;.\n\n\nTool call: simple_eval({\u0026#39;expression\u0026#39;: \u0026#39;(1234 * 4346 / 32414) ** 0.5\u0026#39;})\n  12.862818066181678\n\nThe result of (1234 * 4346 / 32414) is approximately\n165.45, and the square root of this value is approximately 12.86.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI’ve released four tool plugins so far:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e\u003ca href=\"https://github.com/simonw/llm-tools-simpleeval\"\u003ellm-tools-simpleeval\u003c/a\u003e\u003c/strong\u003e—as shown above, simple expression support for things like mathematics.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e\u003ca href=\"https://github.com/simonw/llm-tools-quickjs\"\u003ellm-tools-quickjs\u003c/a\u003e\u003c/strong\u003e—provides access to a sandboxed QuickJS JavaScript interpreter, allowing LLMs to run JavaScript code. The environment persists between calls so the model can set variables and build functions and reuse them later on.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e\u003ca href=\"https://github.com/simonw/llm-tools-sqlite\"\u003ellm-tools-sqlite\u003c/a\u003e\u003c/strong\u003e—read-only SQL query access to a local SQLite database.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e\u003ca href=\"https://github.com/simonw/llm-tools-datasette\"\u003ellm-tools-datasette\u003c/a\u003e\u003c/strong\u003e—run SQL queries against a remote \u003ca href=\"https://datasette.io/\"\u003eDatasette\u003c/a\u003e instance!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet’s try that Datasette one now:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003ellm install llm-tools-datasette\nllm -T \u003cspan\u003e\u003cspan\u003e\u0026#39;\u003c/span\u003eDatasette(\u0026#34;https://datasette.io/content\u0026#34;)\u003cspan\u003e\u0026#39;\u003c/span\u003e\u003c/span\u003e --td \u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003eWhat has the most stars?\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe syntax here is slightly different: the Datasette plugin is what I’m calling a “toolbox”—a plugin that has multiple tools inside it and can be configured with a constructor.\u003c/p\u003e\n\u003cp\u003eSpecifying \u003ccode\u003e--tool\u003c/code\u003e as \u003ccode\u003eDatasette(\u0026#34;https://datasette.io/content\u0026#34;)\u003c/code\u003e provides the plugin with the URL to the Datasette instance it should use—in this case the \u003ca href=\"https://datasette.io/content\"\u003econtent database\u003c/a\u003e that powers the Datasette website.\u003c/p\u003e\n\u003cp\u003eHere’s the output, with the schema section truncated for brevity:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://static.simonwillison.net/static/2025/datasette-tool.jpg\" alt=\"I run that command. It first does a Tool call to Datasette_query with SELECT name, stars, FROM repos ORDER BY stars DESC LIMIT 1. This returns an error message because there is no such column stars. It calls the Datasette_schema() function which returns a whole load of CREATE TABLE statements. Then it executes Datasette_query again this time with SELECT name, stargazers_count FROM repos ORDER BY stargazers_count DESC LIMIT 1. This returns name=datasette a count of 10020, so the model replies and says The repository with the most stars is \u0026#34;datasette\u0026#34; with 10,020 stars.\"/\u003e\u003c/p\u003e\n\u003cp\u003eThis question triggered three calls. The model started by guessing the query! It tried \u003ccode\u003eSELECT name, stars FROM repos ORDER BY stars DESC LIMIT 1\u003c/code\u003e, which failed because the \u003ccode\u003estars\u003c/code\u003e column doesn’t exist.\u003c/p\u003e\n\u003cp\u003eThe tool call returned an error, so the model had another go—this time calling the \u003ccode\u003eDatasette_schema()\u003c/code\u003e tool to get the schema of the database.\u003c/p\u003e\n\u003cp\u003eBased on that schema it assembled and then executed the correct query, and output its interpretation of the result:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe repository with the most stars is “datasette” with 10,020 stars.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eGetting to this point was a real \u003ca href=\"https://www.penny-arcade.com/comic/2010/09/17/mine-all-mine-part-one\"\u003ePenny Arcade Minecraft moment\u003c/a\u003e for me. The possibilities here are \u003cem\u003elimitless\u003c/em\u003e. If you can write a Python function for it, you can trigger it from an LLM.\u003c/p\u003e\n\u003ch4 id=\"ad-hoc-command-line-tools-with-functions\"\u003eAd-hoc command-line tools with \u003ccode\u003e--functions\u003c/code\u003e\n\u003c/h4\u003e\n\u003cp\u003eI’m looking forward to people building more plugins, but there’s also much less structured and more ad-hoc way to use tools with the LLM CLI tool: the \u003ccode\u003e--functions\u003c/code\u003e option.\u003c/p\u003e\n\u003cp\u003eThis was inspired by a similar feature \u003ca href=\"https://sqlite-utils.datasette.io/en/stable/cli.html#defining-custom-sql-functions\"\u003eI added to sqlite-utils\u003c/a\u003e a while ago.\u003c/p\u003e\n\u003cp\u003eYou can pass a block of literal Python code directly to the CLI tool using the \u003ccode\u003e--functions\u003c/code\u003e option, and any functions defined there will be made available to the model as tools.\u003c/p\u003e\n\u003cp\u003eHere’s an example that adds the ability to search my blog:\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003ellm --functions \u003cspan\u003e\u003cspan\u003e\u0026#39;\u003c/span\u003e\u003c/span\u003e\n\u003cspan\u003eimport httpx\u003c/span\u003e\n\u003cspan\u003e\n\u003cspan\u003edef search_blog(q):\u003c/span\u003e\n\u003cspan\u003e    \u0026#34;Search Simon Willison blog\u0026#34;\u003c/span\u003e\n\u003cspan\u003e    return httpx.get(\u0026#34;https://simonwillison.net/search/\u0026#34;, params={\u0026#34;q\u0026#34;: q}).content\u003c/span\u003e\n\u003cspan\u003e\u003cspan\u003e\u0026#39;\u003c/span\u003e\u003c/span\u003e --td \u003cspan\u003e\u003cspan\u003e\u0026#39;\u003c/span\u003eThree features of sqlite-utils\u003cspan\u003e\u0026#39;\u003c/span\u003e\u003c/span\u003e -s \u003cspan\u003e\u003cspan\u003e\u0026#39;\u003c/span\u003euse Simon search\u003cspan\u003e\u0026#39;\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis is \u003cem\u003esuch a hack\u003c/em\u003e of an implementation! I’m literally just hitting \u003ca href=\"https://simonwillison.net/search/?q=pelicans\"\u003emy search page\u003c/a\u003e and dumping the HTML straight back into tho model.\u003c/p\u003e\n\u003cp\u003eIt totally works though—it helps that the GPT-4.1 series all handle a million tokens now, so crufty HTML is no longer a problem for them.\u003c/p\u003e\n\u003cp\u003e(I had to add “use Simon search” as the system prompt because without it the model would try to answer the question itself, rather than using the search tool I provided. System prompts for tools are clearly a \u003cem\u003ebig topic\u003c/em\u003e, Anthropic’s own web search tool has \u003ca href=\"https://simonwillison.net/2025/May/25/claude-4-system-prompt/#search-instructions\"\u003e6,471 tokens of instructions\u003c/a\u003e!)\u003c/p\u003e\n\u003cp\u003eHere’s the output I got just now:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThree features of sqlite-utils are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIt is a combined CLI tool and Python library for manipulating SQLite databases.\u003c/li\u003e\n\u003cli\u003eIt can automatically add columns to a database table if you attempt to insert data that doesn’t quite fit (using the alter=True option).\u003c/li\u003e\n\u003cli\u003eIt supports plugins, allowing the extension of its functionality through third-party or custom plugins.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eA better search tool would have more detailed instructions and would return relevant snippets of the results, not just the headline and first paragraph for each result. This is pretty great for just four lines of Python though!\u003c/p\u003e\n\u003ch4 id=\"tools-in-the-llm-python-api\"\u003eTools in the LLM Python API\u003c/h4\u003e\n\u003cp\u003eLLM is both a CLI tool and a Python library at the same time (similar to my other project \u003ca href=\"https://sqlite-utils.datasette.io/\"\u003esqlite-utils\u003c/a\u003e). The LLM Python library \u003ca href=\"https://llm.datasette.io/en/stable/python-api.html#tools\"\u003egrew tool support\u003c/a\u003e in LLM 0.26 as well.\u003c/p\u003e\n\u003cp\u003eHere’s a simple example solving one of the previously hardest problems in LLMs: counting the number of Rs in “strawberry”:\u003c/p\u003e\n\u003cpre\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ellm\u003c/span\u003e\n\n\u003cspan\u003edef\u003c/span\u003e \u003cspan\u003ecount_char_in_text\u003c/span\u003e(\u003cspan\u003echar\u003c/span\u003e: \u003cspan\u003estr\u003c/span\u003e, \u003cspan\u003etext\u003c/span\u003e: \u003cspan\u003estr\u003c/span\u003e) \u003cspan\u003e-\u0026gt;\u003c/span\u003e \u003cspan\u003eint\u003c/span\u003e:\n    \u003cspan\u003e\u0026#34;How many times does char appear in text?\u0026#34;\u003c/span\u003e\n    \u003cspan\u003ereturn\u003c/span\u003e \u003cspan\u003etext\u003c/span\u003e.\u003cspan\u003ecount\u003c/span\u003e(\u003cspan\u003echar\u003c/span\u003e)\n\n\u003cspan\u003emodel\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ellm\u003c/span\u003e.\u003cspan\u003eget_model\u003c/span\u003e(\u003cspan\u003e\u0026#34;gpt-4.1-mini\u0026#34;\u003c/span\u003e)\n\u003cspan\u003echain_response\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003emodel\u003c/span\u003e.\u003cspan\u003echain\u003c/span\u003e(\n    \u003cspan\u003e\u0026#34;Rs in strawberry?\u0026#34;\u003c/span\u003e,\n    \u003cspan\u003etools\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e[\u003cspan\u003ecount_char_in_text\u003c/span\u003e],\n    \u003cspan\u003eafter_call\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eprint\u003c/span\u003e\n)\n\u003cspan\u003efor\u003c/span\u003e \u003cspan\u003echunk\u003c/span\u003e \u003cspan\u003ein\u003c/span\u003e \u003cspan\u003echain_response\u003c/span\u003e:\n    \u003cspan\u003eprint\u003c/span\u003e(\u003cspan\u003echunk\u003c/span\u003e, \u003cspan\u003eend\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026#34;\u003c/span\u003e, \u003cspan\u003eflush\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e)\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003eafter_call=print\u003c/code\u003e argument is a way to peek at the tool calls, the Python equivalent of the \u003ccode\u003e--td\u003c/code\u003e option from earlier.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003emodel.chain()\u003c/code\u003e method is new: it’s similar to \u003ccode\u003emodel.prompt()\u003c/code\u003e but knows how to spot returned tool call requests, execute them and then prompt the model again with the results. A \u003ccode\u003emodel.chain()\u003c/code\u003e could potentially execute dozens of responses on the way to giving you a final answer.\u003c/p\u003e\n\u003cp\u003eYou can iterate over the \u003ccode\u003echain_response\u003c/code\u003e to output those tokens as they are returned by the model, even across multiple responses.\u003c/p\u003e\n\u003cp\u003eI got back this:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ccode\u003eTool(name=\u0026#39;count_char_in_text\u0026#39;, description=\u0026#39;How many times does char appear in text?\u0026#39;, input_schema={\u0026#39;properties\u0026#39;: {\u0026#39;char\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;string\u0026#39;}, \u0026#39;text\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;string\u0026#39;}}, \u0026#39;required\u0026#39;: [\u0026#39;char\u0026#39;, \u0026#39;text\u0026#39;], \u0026#39;type\u0026#39;: \u0026#39;object\u0026#39;}, implementation=\u0026lt;function count_char_in_text at 0x109dd4f40\u0026gt;, plugin=None) ToolCall(name=\u0026#39;count_char_in_text\u0026#39;, arguments={\u0026#39;char\u0026#39;: \u0026#39;r\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;strawberry\u0026#39;}, tool_call_id=\u0026#39;call_DGXcM8b2B26KsbdMyC1uhGUu\u0026#39;) ToolResult(name=\u0026#39;count_char_in_text\u0026#39;, output=\u0026#39;3\u0026#39;, tool_call_id=\u0026#39;call_DGXcM8b2B26KsbdMyC1uhGUu\u0026#39;, instance=None, exception=None)\u003c/code\u003e\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThere are 3 letter “r”s in the word “strawberry”.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLLM’s Python library also supports \u003ccode\u003easyncio\u003c/code\u003e, and tools can be \u003ccode\u003easync def\u003c/code\u003e functions \u003ca href=\"https://llm.datasette.io/en/latest/python-api.html#tool-functions-can-be-sync-or-async\"\u003eas described here\u003c/a\u003e. If a model requests multiple async tools at once the library will run them concurrently with \u003ccode\u003easyncio.gather()\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe Toolbox form of tools is supported too: you can pass \u003ccode\u003etools=[Datasette(\u0026#34;https://datasette.io/content\u0026#34;)]\u003c/code\u003e to that \u003ccode\u003echain()\u003c/code\u003e method to achieve the same effect as the \u003ccode\u003e--tool \u0026#39;Datasette(...)\u003c/code\u003e option from earlier.\u003c/p\u003e\n\u003ch4 id=\"why-did-this-take-me-so-long-\"\u003eWhy did this take me so long?\u003c/h4\u003e\n\u003cp\u003eI’ve been tracking \u003ca href=\"https://simonwillison.net/tags/llm-tool-use/\"\u003ellm-tool-use\u003c/a\u003e for a while. I first saw the trick described in \u003ca href=\"https://arxiv.org/abs/2210.03629\"\u003ethe ReAcT paper\u003c/a\u003e, first published in October 2022 (a month before the initial release of ChatGPT). I built \u003ca href=\"https://til.simonwillison.net/llms/python-react-pattern\"\u003ea simple implementation of that\u003c/a\u003e in a few dozen lines of Python. It was clearly a very neat pattern!\u003c/p\u003e\n\u003cp\u003eOver the past few years it has become \u003cem\u003every\u003c/em\u003e apparent that tool use is the single most effective way to extend the abilities of language models. It’s such a simple trick: you tell the model that there are tools it can use, and have it output special syntax (JSON or XML or \u003ccode\u003etool_name(arguments)\u003c/code\u003e, it doesn’t matter which) requesting a tool action, then stop.\u003c/p\u003e\n\u003cp\u003eYour code parses that output, runs the requested tools and then starts a new prompt to the model with the results.\u003c/p\u003e\n\u003cp\u003eThis works with almost \u003cstrong\u003eevery model\u003c/strong\u003e now. Most of them are specifically trained for tool usage, and there are leaderboards like the \u003ca href=\"https://gorilla.cs.berkeley.edu/leaderboard.html\"\u003eBerkeley Function-Calling Leaderboard\u003c/a\u003e dedicated to tracking which models do the best job of it.\u003c/p\u003e\n\u003cp\u003eAll of the big model vendors—OpenAI, Anthropic, Google, Mistral, Meta—have a version of this baked into their API, either called tool usage or function calling. It’s all the same underlying pattern.\u003c/p\u003e\n\u003cp\u003eThe models you can run locally are getting good at this too. Ollama \u003ca href=\"https://ollama.com/blog/tool-support\"\u003eadded tool support\u003c/a\u003e last year, and it’s baked into the \u003ca href=\"https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md\"\u003ellama.cpp\u003c/a\u003e server as well.\u003c/p\u003e\n\u003cp\u003eIt’s been clear for a while that LLM absolutely needed to grow support for tools. I released \u003ca href=\"https://simonwillison.net/2025/Feb/28/llm-schemas/\"\u003eLLM schema support\u003c/a\u003e back in February as a stepping stone towards this. I’m glad to finally have it over the line.\u003c/p\u003e\n\u003cp\u003eAs always with LLM, the challenge was designing an abstraction layer that could work across as many different models as possible. A year ago I didn’t feel that model tool support was mature enough to figure this out. Today there’s a very definite consensus among vendors about how this should work, which finally gave me the confidence to implement it.\u003c/p\u003e\n\u003cp\u003eI also presented a workshop at PyCon US two weeks ago about \u003ca href=\"https://simonwillison.net/2025/May/15/building-on-llms/\"\u003eBuilding software on top of Large Language Models\u003c/a\u003e, which was exactly the incentive I needed to finally get this working in an alpha! Here’s the \u003ca href=\"https://building-with-llms-pycon-2025.readthedocs.io/en/latest/tools.html\"\u003etools section\u003c/a\u003e from that tutorial.\u003c/p\u003e\n\u003ch4 id=\"is-this-agents-then-\"\u003eIs this agents then?\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eSigh\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eI still \u003ca href=\"https://simonwillison.net/2024/Dec/31/llms-in-2024/#-agents-still-haven-t-really-happened-yet\"\u003edon’t like\u003c/a\u003e using the term “agents”. I worry that developers will think \u003ca href=\"https://simonwillison.net/2025/May/22/tools-in-a-loop/\"\u003etools in a loop\u003c/a\u003e, regular people will think virtual AI assistants \u003ca href=\"https://en.m.wikipedia.org/wiki/Her_(2013_film)\"\u003evoiced by Scarlett Johansson\u003c/a\u003e and academics will \u003ca href=\"https://simonwillison.net/2025/Mar/19/worms-and-dogs-and-countries/\"\u003egrumble about thermostats\u003c/a\u003e. But in the LLM world we appear to be converging on “tools in a loop”, and that’s absolutely what this.\u003c/p\u003e\n\u003cp\u003eSo yes, if you want to build “agents” then LLM 0.26 is a great way to do that.\u003c/p\u003e\n\u003ch4 id=\"what-s-next-for-tools-in-llm-\"\u003eWhat’s next for tools in LLM?\u003c/h4\u003e\n\u003cp\u003eI already have a \u003ca href=\"https://github.com/simonw/llm/milestone/13\"\u003eLLM tools v2 milestone\u003c/a\u003e with 13 issues in it, mainly around improvements to how tool execution logs are displayed but with quite a few minor issues I decided shouldn’t block this release. There’s a bunch more stuff in the \u003ca href=\"https://github.com/simonw/llm/issues?q=is%3Aissue%20state%3Aopen%20label%3Atools\"\u003etools label\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI’m most excited about the potential for plugins.\u003c/p\u003e\n\u003cp\u003eWriting tool plugins is \u003cem\u003ereally fun\u003c/em\u003e. I have an \u003ca href=\"https://github.com/simonw/llm-plugin-tools\"\u003ellm-plugin-tools\u003c/a\u003e cookiecutter template that I’ve been using for my own, and I plan to put together a tutorial around that soon.\u003c/p\u003e\n\u003cp\u003eThere’s more work to be done adding tool support to more model plugins. I added \u003ca href=\"https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#supporting-tools\"\u003edetails of this\u003c/a\u003e to the advanced plugins documentation. This commit \u003ca href=\"https://github.com/simonw/llm-gemini/commit/a7f1096cfbb733018eb41c29028a8cc6160be298\"\u003eadding tool support for Gemini\u003c/a\u003e is a useful illustratino of what’s involved.\u003c/p\u003e\n\n\u003cp\u003eAnd yes, \u003cstrong\u003eModel Context Protocol\u003c/strong\u003e support is clearly on the agenda as well. MCP is emerging as the standard way for models to access tools at a frankly bewildering speed. Two weeks ago it wasn’t directly supported by the APIs of any of the major vendors. In just the past eight days \u003ca href=\"https://simonwillison.net/2025/May/27/mistral-agents-api/\"\u003eit’s been added\u003c/a\u003e by OpenAI, Anthropic \u003cem\u003eand\u003c/em\u003e Mistral! It’s feeling like a lot less of a moving target today.\u003c/p\u003e\n\u003cp\u003eI want LLM to be able to act as an MCP client, so that any of the MCP servers people are writing can be easily accessed as additional sources of tools for LLM.\u003c/p\u003e\n\u003cp\u003eIf you’re interested in talking more about what comes next for LLM, \u003ca href=\"https://datasette.io/discord-llm\"\u003ecome and chat to us in our Discord\u003c/a\u003e.\u003c/p\u003e\n\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "16 min read",
  "publishedTime": null,
  "modifiedTime": null
}
