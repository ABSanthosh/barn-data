{
  "id": "58996a3e-6247-4df4-846d-600484696d3c",
  "title": "Qwen3: Think deeper, act faster",
  "link": "https://qwenlm.github.io/blog/qwen3/",
  "description": "Comments",
  "author": "",
  "published": "Mon, 28 Apr 2025 20:44:25 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Qwen Team",
  "length": 15966,
  "excerpt": "QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD Introduction Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.",
  "siteName": "Qwen",
  "favicon": "https://qwenlm.github.io/favicon.png",
  "text": "QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORDIntroductionToday, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.We are open-weighting two MoE models: Qwen3-235B-A22B, a large model with 235 billion total parameters and 22 billion activated parameters, and Qwen3-30B-A3B, a smaller MoE model with 30 billion total parameters and 3 billion activated parameters. Additionally, six dense models are also open-weighted, including Qwen3-32B, Qwen3-14B, Qwen3-8B, Qwen3-4B, Qwen3-1.7B, and Qwen3-0.6B, under Apache 2.0 license.ModelsLayersHeads (Q / KV)Tie EmbeddingContext LengthQwen3-0.6B2816 / 8Yes32KQwen3-1.7B2816 / 8Yes32KQwen3-4B3632 / 8Yes32KQwen3-8B3632 / 8No128KQwen3-14B4040 / 8No128KQwen3-32B6464 / 8No128KModelsLayersHeads (Q / KV)# Experts (Total / Activated)Context LengthQwen3-30B-A3B4832 / 4128 / 8128KQwen3-235B-A22B9464 / 4128 / 8128KThe post-trained models, such as Qwen3-30B-A3B, along with their pre-trained counterparts (e.g., Qwen3-30B-A3B-Base), are now available on platforms like Hugging Face, ModelScope, and Kaggle. For deployment, we recommend using frameworks like SGLang and vLLM. For local usage, tools such as Ollama, LMStudio, MLX, llama.cpp, and KTransformers are highly recommended. These options ensure that users can easily integrate Qwen3 into their workflows, whether in research, development, or production environments.We believe that the release and open-sourcing of Qwen3 will significantly advance the research and development of large foundation models. Our goal is to empower researchers, developers, and organizations around the world to build innovative solutions using these cutting-edge models.Feel free to try Qwen3 out in Qwen Chat Web (chat.qwen.ai) and mobile APP!Key FeaturesHybrid Thinking ModesQwen3 models introduce a hybrid approach to problem-solving. They support two modes:Thinking Mode: In this mode, the model takes time to reason step by step before delivering the final answer. This is ideal for complex problems that require deeper thought.Non-Thinking Mode: Here, the model provides quick, near-instant responses, suitable for simpler questions where speed is more important than depth.This flexibility allows users to control how much “thinking” the model performs based on the task at hand. For example, harder problems can be tackled with extended reasoning, while easier ones can be answered directly without delay. Crucially, the integration of these two modes greatly enhances the model’s ability to implement stable and efficient thinking budget control. As demonstrated above, Qwen3 exhibits scalable and smooth performance improvements that are directly correlated with the computational reasoning budget allocated. This design enables users to configure task-specific budgets with greater ease, achieving a more optimal balance between cost efficiency and inference quality.Multilingual SupportQwen3 models are supporting 119 languages and dialects. This extensive multilingual capability opens up new possibilities for international applications, enabling users worldwide to benefit from the power of these models.Language FamilyLanguages \u0026 DialectsIndo-EuropeanEnglish, French, Portuguese, German, Romanian, Swedish, Danish, Bulgarian, Russian, Czech, Greek, Ukrainian, Spanish, Dutch, Slovak, Croatian, Polish, Lithuanian, Norwegian Bokmål, Norwegian Nynorsk, Persian, Slovenian, Gujarati, Latvian, Italian, Occitan, Nepali, Marathi, Belarusian, Serbian, Luxembourgish, Venetian, Assamese, Welsh, Silesian, Asturian, Chhattisgarhi, Awadhi, Maithili, Bhojpuri, Sindhi, Irish, Faroese, Hindi, Punjabi, Bengali, Oriya, Tajik, Eastern Yiddish, Lombard, Ligurian, Sicilian, Friulian, Sardinian, Galician, Catalan, Icelandic, Tosk Albanian, Limburgish, Dari, Afrikaans, Macedonian, Sinhala, Urdu, Magahi, Bosnian, ArmenianSino-TibetanChinese (Simplified Chinese, Traditional Chinese, Cantonese), BurmeseAfro-AsiaticArabic (Standard, Najdi, Levantine, Egyptian, Moroccan, Mesopotamian, Ta’izzi-Adeni, Tunisian), Hebrew, MalteseAustronesianIndonesian, Malay, Tagalog, Cebuano, Javanese, Sundanese, Minangkabau, Balinese, Banjar, Pangasinan, Iloko, Waray (Philippines)DravidianTamil, Telugu, Kannada, MalayalamTurkicTurkish, North Azerbaijani, Northern Uzbek, Kazakh, Bashkir, TatarTai-KadaiThai, LaoUralicFinnish, Estonian, HungarianAustroasiaticVietnamese, KhmerOtherJapanese, Korean, Georgian, Basque, Haitian, Papiamento, Kabuverdianu, Tok Pisin, SwahiliImproved Agentic CapabilitiesWe have optimized the Qwen3 models for coding and agentic capabilities, and also we have strengthened the support of MCP as well. Below we provide examples to show how Qwen3 thinks and interacts with the environment.Pre-trainingIn terms of pretraining, the dataset for Qwen3 has been significantly expanded compared to Qwen2.5. While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. This ensures the model can handle longer inputs effectively.Due to advancements in model architecture, increase in training data, and more effective training methods, the overall performance of Qwen3 dense base models matches that of Qwen2.5 base models with more parameters. For instance, Qwen3-1.7B/4B/8B/14B/32B-Base performs as well as Qwen2.5-3B/7B/14B/32B/72B-Base, respectively. Notably, in areas like STEM, coding, and reasoning, Qwen3 dense base models even outperform larger Qwen2.5 models. For Qwen3-MoE base models, they achieve similar performance to Qwen2.5 dense base models while using only 10% of the active parameters. This results in significant savings in both training and inference costs.Post-trainingTo develop the hybrid model capable of both step-by-step reasoning and rapid responses, we implemented a four-stage training pipeline. This pipeline includes: (1) long chain-of-thought (CoT) cold start, (2) reasoning-based reinforcement learning (RL), (3) thinking mode fusion, and (4) general RL.In the first stage, we fine-tuned the models using diverse long CoT data, covering various tasks and domains such as mathematics, coding, logical reasoning, and STEM problems. This process aimed to equip the model with fundamental reasoning abilities. The second stage focused on scaling up computational resources for RL, utilizing rule-based rewards to enhance the model’s exploration and exploitation capabilities.In the third stage, we integrated non-thinking capabilities into the thinking model by fine-tuning it on a combination of long CoT data and commonly used instruction-tuning data. This data was generated by the enhanced thinking model from the second stage, ensuring a seamless blend of reasoning and quick response capabilities. Finally, in the fourth stage, we applied RL across more than 20 general-domain tasks to further strengthen the model’s general capabilities and correct undesired behaviors. These tasks included instruction following, format following, and agent capabilities, etc.Develop with Qwen3Below is a simple guide for you to use Qwen3 on different frameworks. First of all, we provide an standard example of using Qwen3-30B-A3B in Hugging Face transformers:from modelscope import AutoModelForCausalLM, AutoTokenizer model_name = \"Qwen/Qwen3-30B-A3B\" # load the tokenizer and the model tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=\"auto\", device_map=\"auto\" ) # prepare the model input prompt = \"Give me a short introduction to large language model.\" messages = [ {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, enable_thinking=True # Switch between thinking and non-thinking modes. Default is True. ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) # conduct text completion generated_ids = model.generate( **model_inputs, max_new_tokens=32768 ) output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() # parsing thinking content try: # rindex finding 151668 (\u003c/think\u003e) index = len(output_ids) - output_ids[::-1].index(151668) except ValueError: index = 0 thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\") content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\") print(\"thinking content:\", thinking_content) print(\"content:\", content) To disable thinking, you just need to make changes to the argument enable_thinking like the following:text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, enable_thinking=False # True is the default value for enable_thinking. ) For deployment, you can use sglang\u003e=0.4.6.post1 or vllm\u003e=0.8.4 to create an OpenAI-compatible API endpoint:SGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --reasoning-parser qwen3 vLLM:vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1 If you use it for local development, you can use ollama by running a simple command ollama run qwen3:30b-a3b to play with the model, or you can use LMStudio or llama.cpp and ktransformers to build locally.Advanced UsagesWe provide a soft switch mechanism that allows users to dynamically control the model’s behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model’s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.Here is an example of a multi-turn conversation:from transformers import AutoModelForCausalLM, AutoTokenizer class QwenChatbot: def __init__(self, model_name=\"Qwen/Qwen3-30B-A3B\"): self.tokenizer = AutoTokenizer.from_pretrained(model_name) self.model = AutoModelForCausalLM.from_pretrained(model_name) self.history = [] def generate_response(self, user_input): messages = self.history + [{\"role\": \"user\", \"content\": user_input}] text = self.tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) inputs = self.tokenizer(text, return_tensors=\"pt\") response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist() response = self.tokenizer.decode(response_ids, skip_special_tokens=True) # Update history self.history.append({\"role\": \"user\", \"content\": user_input}) self.history.append({\"role\": \"assistant\", \"content\": response}) return response # Example Usage if __name__ == \"__main__\": chatbot = QwenChatbot() # First input (without /think or /no_think tags, thinking mode is enabled by default) user_input_1 = \"How many r's in strawberries?\" print(f\"User: {user_input_1}\") response_1 = chatbot.generate_response(user_input_1) print(f\"Bot: {response_1}\") print(\"----------------------\") # Second input with /no_think user_input_2 = \"Then, how many r's in blueberries? /no_think\" print(f\"User: {user_input_2}\") response_2 = chatbot.generate_response(user_input_2) print(f\"Bot: {response_2}\") print(\"----------------------\") # Third input with /think user_input_3 = \"Really? /think\" print(f\"User: {user_input_3}\") response_3 = chatbot.generate_response(user_input_3) print(f\"Bot: {response_3}\") Agentic UsagesQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.To define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.from qwen_agent.agents import Assistant # Define LLM llm_cfg = { 'model': 'Qwen3-30B-A3B', # Use the endpoint provided by Alibaba Model Studio: # 'model_type': 'qwen_dashscope', # 'api_key': os.getenv('DASHSCOPE_API_KEY'), # Use a custom endpoint compatible with OpenAI API: 'model_server': 'http://localhost:8000/v1', # api_base 'api_key': 'EMPTY', # Other parameters: # 'generate_cfg': { # # Add: When the response content is `\u003cthink\u003ethis is the thought\u003c/think\u003ethis is the answer; # # Do not add: When the response has been separated by reasoning_content and content. # 'thought_in_content': True, # }, } # Define Tools tools = [ {'mcpServers': { # You can specify the MCP configuration file 'time': { 'command': 'uvx', 'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai'] }, \"fetch\": { \"command\": \"uvx\", \"args\": [\"mcp-server-fetch\"] } } }, 'code_interpreter', # Built-in tools ] # Define Agent bot = Assistant(llm=llm_cfg, function_list=tools) # Streaming generation messages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}] for responses in bot.run(messages=messages): pass print(responses) Friends of QwenThanks to the support of so many friends. Qwen is nothing without its friends! We welcome more people or organizations to join our community and help us become better!Future WorkQwen3 represents a significant milestone in our journey toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI). By scaling up both pretraining and reinforcement learning (RL), we have achieved higher levels of intelligence. We have seamlessly integrated thinking and non-thinking modes, offering users the flexibility to control the thinking budget. Additionally, we have expanded support for a wide range of languages, enhancing global accessibility.Looking ahead, we aim to enhance our models across multiple dimensions. This includes refining model architectures and training methodologies to achieve several key objectives: scaling data, increasing model size, extending context length, broadening modalities, and advancing RL with environmental feedback for long-horizon reasoning. We believe we are transitioning from an era focused on training models to one centered on training agents. Our next iteration promises to bring meaningful advancements to everyone’s work and life.",
  "image": "https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cimg src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/qwen3-banner.png\" alt=\"Qwen3 Main Image\" width=\"100%\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003ca href=\"https://chat.qwen.ai\" target=\"_blank\"\u003eQWEN CHAT\u003c/a\u003e\n\u003ca href=\"https://github.com/QwenLM/Qwen3\" target=\"_blank\"\u003eGitHub\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f\" target=\"_blank\"\u003eHugging Face\u003c/a\u003e\n\u003ca href=\"https://modelscope.cn/collections/Qwen3-9743180bdc6b48\" target=\"_blank\"\u003eModelScope\u003c/a\u003e\n\u003ca href=\"https://www.kaggle.com/models/qwen-lm/qwen-3\" target=\"_blank\"\u003eKaggle\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/spaces/Qwen/Qwen3-Demo\" target=\"_blank\"\u003eDEMO\u003c/a\u003e\n\u003ca href=\"https://discord.gg/yPEP2vHTu4\" target=\"_blank\"\u003eDISCORD\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eToday, we are excited to announce the release of \u003cstrong\u003eQwen3\u003c/strong\u003e, the latest addition to the Qwen family of large language models. Our flagship model, \u003cstrong\u003eQwen3-235B-A22B\u003c/strong\u003e, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, \u003cstrong\u003eQwen3-30B-A3B\u003c/strong\u003e, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3/qwen3-235a22.jpg\" width=\"100%\"/\u003e\u003c/figure\u003e\u003cfigure\u003e\u003cimg src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3/qwen3-30a3.jpg\" width=\"100%\"/\u003e\u003c/figure\u003e\u003cp\u003eWe are open-weighting two MoE models: \u003cstrong\u003eQwen3-235B-A22B\u003c/strong\u003e, a large model with 235 billion total parameters and 22 billion activated parameters, and \u003cstrong\u003eQwen3-30B-A3B\u003c/strong\u003e, a smaller MoE model with 30 billion total parameters and 3 billion activated parameters. Additionally, six dense models are also open-weighted, including \u003cstrong\u003eQwen3-32B\u003c/strong\u003e, \u003cstrong\u003eQwen3-14B\u003c/strong\u003e, \u003cstrong\u003eQwen3-8B\u003c/strong\u003e, \u003cstrong\u003eQwen3-4B\u003c/strong\u003e, \u003cstrong\u003eQwen3-1.7B\u003c/strong\u003e, and \u003cstrong\u003eQwen3-0.6B\u003c/strong\u003e, under Apache 2.0 license.\u003c/p\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eModels\u003c/th\u003e\u003cth\u003eLayers\u003c/th\u003e\u003cth\u003eHeads (Q / KV)\u003c/th\u003e\u003cth\u003eTie Embedding\u003c/th\u003e\u003cth\u003eContext Length\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eQwen3-0.6B\u003c/td\u003e\u003ctd\u003e28\u003c/td\u003e\u003ctd\u003e16 / 8\u003c/td\u003e\u003ctd\u003eYes\u003c/td\u003e\u003ctd\u003e32K\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eQwen3-1.7B\u003c/td\u003e\u003ctd\u003e28\u003c/td\u003e\u003ctd\u003e16 / 8\u003c/td\u003e\u003ctd\u003eYes\u003c/td\u003e\u003ctd\u003e32K\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eQwen3-4B\u003c/td\u003e\u003ctd\u003e36\u003c/td\u003e\u003ctd\u003e32 / 8\u003c/td\u003e\u003ctd\u003eYes\u003c/td\u003e\u003ctd\u003e32K\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eQwen3-8B\u003c/td\u003e\u003ctd\u003e36\u003c/td\u003e\u003ctd\u003e32 / 8\u003c/td\u003e\u003ctd\u003eNo\u003c/td\u003e\u003ctd\u003e128K\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eQwen3-14B\u003c/td\u003e\u003ctd\u003e40\u003c/td\u003e\u003ctd\u003e40 / 8\u003c/td\u003e\u003ctd\u003eNo\u003c/td\u003e\u003ctd\u003e128K\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eQwen3-32B\u003c/td\u003e\u003ctd\u003e64\u003c/td\u003e\u003ctd\u003e64 / 8\u003c/td\u003e\u003ctd\u003eNo\u003c/td\u003e\u003ctd\u003e128K\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eModels\u003c/th\u003e\u003cth\u003eLayers\u003c/th\u003e\u003cth\u003eHeads (Q / KV)\u003c/th\u003e\u003cth\u003e# Experts (Total / Activated)\u003c/th\u003e\u003cth\u003eContext Length\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eQwen3-30B-A3B\u003c/td\u003e\u003ctd\u003e48\u003c/td\u003e\u003ctd\u003e32 / 4\u003c/td\u003e\u003ctd\u003e128 / 8\u003c/td\u003e\u003ctd\u003e128K\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eQwen3-235B-A22B\u003c/td\u003e\u003ctd\u003e94\u003c/td\u003e\u003ctd\u003e64 / 4\u003c/td\u003e\u003ctd\u003e128 / 8\u003c/td\u003e\u003ctd\u003e128K\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cp\u003eThe post-trained models, such as \u003cstrong\u003eQwen3-30B-A3B\u003c/strong\u003e, along with their pre-trained counterparts (e.g., \u003cstrong\u003eQwen3-30B-A3B-Base\u003c/strong\u003e), are now available on platforms like \u003cstrong\u003eHugging Face\u003c/strong\u003e, \u003cstrong\u003eModelScope\u003c/strong\u003e, and \u003cstrong\u003eKaggle\u003c/strong\u003e. For deployment, we recommend using frameworks like \u003cstrong\u003eSGLang\u003c/strong\u003e and \u003cstrong\u003evLLM\u003c/strong\u003e. For local usage, tools such as \u003cstrong\u003eOllama\u003c/strong\u003e, \u003cstrong\u003eLMStudio\u003c/strong\u003e, \u003cstrong\u003eMLX\u003c/strong\u003e, \u003cstrong\u003ellama.cpp\u003c/strong\u003e, and \u003cstrong\u003eKTransformers\u003c/strong\u003e are highly recommended. These options ensure that users can easily integrate Qwen3 into their workflows, whether in research, development, or production environments.\u003c/p\u003e\u003cp\u003eWe believe that the release and open-sourcing of Qwen3 will significantly advance the research and development of large foundation models. Our goal is to empower researchers, developers, and organizations around the world to build innovative solutions using these cutting-edge models.\u003c/p\u003e\u003cp\u003eFeel free to try Qwen3 out in Qwen Chat Web (\u003ca href=\"https://chat.qwen.ai\"\u003echat.qwen.ai\u003c/a\u003e) and mobile APP!\u003c/p\u003e\u003ch2 id=\"key-features\"\u003eKey Features\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHybrid Thinking Modes\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eQwen3 models introduce a hybrid approach to problem-solving. They support two modes:\u003c/p\u003e\u003col\u003e\u003cli\u003eThinking Mode: In this mode, the model takes time to reason step by step before delivering the final answer. This is ideal for complex problems that require deeper thought.\u003c/li\u003e\u003cli\u003eNon-Thinking Mode: Here, the model provides quick, near-instant responses, suitable for simpler questions where speed is more important than depth.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis flexibility allows users to control how much “thinking” the model performs based on the task at hand. For example, harder problems can be tackled with extended reasoning, while easier ones can be answered directly without delay. Crucially, the integration of these two modes greatly enhances the model’s ability to implement stable and efficient thinking budget control. As demonstrated above, Qwen3 exhibits scalable and smooth performance improvements that are directly correlated with the computational reasoning budget allocated. This design enables users to configure task-specific budgets with greater ease, achieving a more optimal balance between cost efficiency and inference quality.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://qianwen-res.oss-accelerate.aliyuncs.com/assets/blog/qwen3/thinking_budget.png\" width=\"100%\"/\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eMultilingual Support\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eQwen3 models are supporting \u003cstrong\u003e119 languages and dialects\u003c/strong\u003e. This extensive multilingual capability opens up new possibilities for international applications, enabling users worldwide to benefit from the power of these models.\u003c/p\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eLanguage Family\u003c/th\u003e\u003cth\u003eLanguages \u0026amp; Dialects\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eIndo-European\u003c/td\u003e\u003ctd\u003eEnglish, French, Portuguese, German, Romanian, Swedish, Danish, Bulgarian, Russian, Czech, Greek, Ukrainian, Spanish, Dutch, Slovak, Croatian, Polish, Lithuanian, Norwegian Bokmål, Norwegian Nynorsk, Persian, Slovenian, Gujarati, Latvian, Italian, Occitan, Nepali, Marathi, Belarusian, Serbian, Luxembourgish, Venetian, Assamese, Welsh, Silesian, Asturian, Chhattisgarhi, Awadhi, Maithili, Bhojpuri, Sindhi, Irish, Faroese, Hindi, Punjabi, Bengali, Oriya, Tajik, Eastern Yiddish, Lombard, Ligurian, Sicilian, Friulian, Sardinian, Galician, Catalan, Icelandic, Tosk Albanian, Limburgish, Dari, Afrikaans, Macedonian, Sinhala, Urdu, Magahi, Bosnian, Armenian\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eSino-Tibetan\u003c/td\u003e\u003ctd\u003eChinese (Simplified Chinese, Traditional Chinese, Cantonese), Burmese\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eAfro-Asiatic\u003c/td\u003e\u003ctd\u003eArabic (Standard, Najdi, Levantine, Egyptian, Moroccan, Mesopotamian, Ta’izzi-Adeni, Tunisian), Hebrew, Maltese\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eAustronesian\u003c/td\u003e\u003ctd\u003eIndonesian, Malay, Tagalog, Cebuano, Javanese, Sundanese, Minangkabau, Balinese, Banjar, Pangasinan, Iloko, Waray (Philippines)\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eDravidian\u003c/td\u003e\u003ctd\u003eTamil, Telugu, Kannada, Malayalam\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eTurkic\u003c/td\u003e\u003ctd\u003eTurkish, North Azerbaijani, Northern Uzbek, Kazakh, Bashkir, Tatar\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eTai-Kadai\u003c/td\u003e\u003ctd\u003eThai, Lao\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eUralic\u003c/td\u003e\u003ctd\u003eFinnish, Estonian, Hungarian\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eAustroasiatic\u003c/td\u003e\u003ctd\u003eVietnamese, Khmer\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eOther\u003c/td\u003e\u003ctd\u003eJapanese, Korean, Georgian, Basque, Haitian, Papiamento, Kabuverdianu, Tok Pisin, Swahili\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImproved Agentic Capabilities\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe have optimized the Qwen3 models for coding and agentic capabilities, and also we have strengthened the support of MCP as well. Below we provide examples to show how Qwen3 thinks and interacts with the environment.\u003c/p\u003e\u003ch2 id=\"pre-training\"\u003ePre-training\u003c/h2\u003e\u003cp\u003eIn terms of pretraining, the dataset for Qwen3 has been significantly expanded compared to Qwen2.5. While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.\u003c/p\u003e\u003cp\u003eThe pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. This ensures the model can handle longer inputs effectively.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/qwen3-base.jpg\" width=\"100%\"/\u003e\u003c/figure\u003e\u003cp\u003eDue to advancements in model architecture, increase in training data, and more effective training methods, the overall performance of Qwen3 dense base models matches that of Qwen2.5 base models with more parameters. For instance, Qwen3-1.7B/4B/8B/14B/32B-Base performs as well as Qwen2.5-3B/7B/14B/32B/72B-Base, respectively. Notably, in areas like STEM, coding, and reasoning, Qwen3 dense base models even outperform larger Qwen2.5 models. For Qwen3-MoE base models, they achieve similar performance to Qwen2.5 dense base models while using only 10% of the active parameters. This results in significant savings in both training and inference costs.\u003c/p\u003e\u003ch2 id=\"post-training\"\u003ePost-training\u003c/h2\u003e\u003cfigure\u003e\u003cimg src=\"https://qianwen-res.oss-accelerate.aliyuncs.com/assets/blog/qwen3/post-training.png\" width=\"100%\"/\u003e\u003c/figure\u003e\u003cp\u003eTo develop the hybrid model capable of both step-by-step reasoning and rapid responses, we implemented a four-stage training pipeline. This pipeline includes: (1) long chain-of-thought (CoT) cold start, (2) reasoning-based reinforcement learning (RL), (3) thinking mode fusion, and (4) general RL.\u003c/p\u003e\u003cp\u003eIn the first stage, we fine-tuned the models using diverse long CoT data, covering various tasks and domains such as mathematics, coding, logical reasoning, and STEM problems. This process aimed to equip the model with fundamental reasoning abilities. The second stage focused on scaling up computational resources for RL, utilizing rule-based rewards to enhance the model’s exploration and exploitation capabilities.\u003c/p\u003e\u003cp\u003eIn the third stage, we integrated non-thinking capabilities into the thinking model by fine-tuning it on a combination of long CoT data and commonly used instruction-tuning data. This data was generated by the enhanced thinking model from the second stage, ensuring a seamless blend of reasoning and quick response capabilities. Finally, in the fourth stage, we applied RL across more than 20 general-domain tasks to further strengthen the model’s general capabilities and correct undesired behaviors. These tasks included instruction following, format following, and agent capabilities, etc.\u003c/p\u003e\u003ch2 id=\"develop-with-qwen3\"\u003eDevelop with Qwen3\u003c/h2\u003e\u003cp\u003eBelow is a simple guide for you to use Qwen3 on different frameworks. First of all, we provide an standard example of using Qwen3-30B-A3B in Hugging Face transformers:\u003c/p\u003e\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"python\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003emodelscope\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eAutoModelForCausalLM\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eAutoTokenizer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emodel_name\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;Qwen/Qwen3-30B-A3B\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e# load the tokenizer and the model\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003etokenizer\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eAutoTokenizer\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_pretrained\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel_name\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emodel\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eAutoModelForCausalLM\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_pretrained\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003emodel_name\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003etorch_dtype\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003edevice_map\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e# prepare the model input\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eprompt\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;Give me a short introduction to large language model.\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emessages\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eprompt\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003etext\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003etokenizer\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eapply_chat_template\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003emessages\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003etokenize\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eadd_generation_prompt\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eenable_thinking\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e \u003cspan\u003e# Switch between thinking and non-thinking modes. Default is True.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emodel_inputs\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003etokenizer\u003c/span\u003e\u003cspan\u003e([\u003c/span\u003e\u003cspan\u003etext\u003c/span\u003e\u003cspan\u003e],\u003c/span\u003e \u003cspan\u003ereturn_tensors\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;pt\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eto\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edevice\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e# conduct text completion\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003egenerated_ids\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egenerate\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e**\u003c/span\u003e\u003cspan\u003emodel_inputs\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003emax_new_tokens\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e32768\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eoutput_ids\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003egenerated_ids\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e][\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel_inputs\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003einput_ids\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e]):]\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003etolist\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e# parsing thinking content\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003etry\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e# rindex finding 151668 (\u0026lt;/think\u0026gt;)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eindex\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eoutput_ids\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003e-\u003c/span\u003e \u003cspan\u003eoutput_ids\u003c/span\u003e\u003cspan\u003e[::\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eindex\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e151668\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eexcept\u003c/span\u003e \u003cspan\u003eValueError\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eindex\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003ethinking_content\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003etokenizer\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edecode\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eoutput_ids\u003c/span\u003e\u003cspan\u003e[:\u003c/span\u003e\u003cspan\u003eindex\u003c/span\u003e\u003cspan\u003e],\u003c/span\u003e \u003cspan\u003eskip_special_tokens\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003estrip\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e\\n\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003econtent\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003etokenizer\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edecode\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eoutput_ids\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eindex\u003c/span\u003e\u003cspan\u003e:],\u003c/span\u003e \u003cspan\u003eskip_special_tokens\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003estrip\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e\\n\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;thinking content:\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ethinking_content\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;content:\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003econtent\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo disable thinking, you just need to make changes to the argument \u003ccode\u003eenable_thinking\u003c/code\u003e like the following:\u003c/p\u003e\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"python\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003etext\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003etokenizer\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eapply_chat_template\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003emessages\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003etokenize\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eadd_generation_prompt\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eenable_thinking\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e  \u003cspan\u003e# True is the default value for enable_thinking.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFor deployment, you can use \u003ccode\u003esglang\u0026gt;=0.4.6.post1\u003c/code\u003e or \u003ccode\u003evllm\u0026gt;=0.8.4\u003c/code\u003e to create an OpenAI-compatible API endpoint:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSGLang:\u003c/p\u003e\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"shell\"\u003e\u003cspan\u003e\u003cspan\u003epython -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --reasoning-parser qwen3\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003evLLM:\u003c/p\u003e\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"shell\"\u003e\u003cspan\u003e\u003cspan\u003evllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you use it for local development, you can use ollama by running a simple command \u003ccode\u003eollama run qwen3:30b-a3b\u003c/code\u003e to play with the model, or you can use LMStudio or llama.cpp and ktransformers to build locally.\u003c/p\u003e\u003ch3 id=\"advanced-usages\"\u003eAdvanced Usages\u003c/h3\u003e\u003cp\u003eWe provide a soft switch mechanism that allows users to dynamically control the model’s behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model’s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\u003c/p\u003e\u003cp\u003eHere is an example of a multi-turn conversation:\u003c/p\u003e\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"python\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003etransformers\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eAutoModelForCausalLM\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eAutoTokenizer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eclass\u003c/span\u003e \u003cspan\u003eQwenChatbot\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003edef\u003c/span\u003e \u003cspan\u003e__init__\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003emodel_name\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;Qwen/Qwen3-30B-A3B\u0026#34;\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003etokenizer\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eAutoTokenizer\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_pretrained\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel_name\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodel\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eAutoModelForCausalLM\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_pretrained\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel_name\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ehistory\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003edef\u003c/span\u003e \u003cspan\u003egenerate_response\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003euser_input\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003emessages\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ehistory\u003c/span\u003e \u003cspan\u003e+\u003c/span\u003e \u003cspan\u003e[{\u003c/span\u003e\u003cspan\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003euser_input\u003c/span\u003e\u003cspan\u003e}]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003etext\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003etokenizer\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eapply_chat_template\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e            \u003cspan\u003emessages\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e            \u003cspan\u003etokenize\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e            \u003cspan\u003eadd_generation_prompt\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003einputs\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003etokenizer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003etext\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ereturn_tensors\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;pt\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003eresponse_ids\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egenerate\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e**\u003c/span\u003e\u003cspan\u003einputs\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003emax_new_tokens\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e32768\u003c/span\u003e\u003cspan\u003e)[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e][\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003einputs\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003einput_ids\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e]):]\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003etolist\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003eresponse\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003etokenizer\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edecode\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eresponse_ids\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eskip_special_tokens\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003e# Update history\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ehistory\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eappend\u003c/span\u003e\u003cspan\u003e({\u003c/span\u003e\u003cspan\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003euser_input\u003c/span\u003e\u003cspan\u003e})\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ehistory\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eappend\u003c/span\u003e\u003cspan\u003e({\u003c/span\u003e\u003cspan\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#34;assistant\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eresponse\u003c/span\u003e\u003cspan\u003e})\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003ereturn\u003c/span\u003e \u003cspan\u003eresponse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e# Example Usage\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eif\u003c/span\u003e \u003cspan\u003e__name__\u003c/span\u003e \u003cspan\u003e==\u003c/span\u003e \u003cspan\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003echatbot\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eQwenChatbot\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e# First input (without /think or /no_think tags, thinking mode is enabled by default)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003euser_input_1\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;How many r\u0026#39;s in strawberries?\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ef\u003c/span\u003e\u003cspan\u003e\u0026#34;User: \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003euser_input_1\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eresponse_1\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003echatbot\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egenerate_response\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003euser_input_1\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ef\u003c/span\u003e\u003cspan\u003e\u0026#34;Bot: \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003eresponse_1\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;----------------------\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e# Second input with /no_think\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003euser_input_2\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;Then, how many r\u0026#39;s in blueberries? /no_think\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ef\u003c/span\u003e\u003cspan\u003e\u0026#34;User: \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003euser_input_2\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eresponse_2\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003echatbot\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egenerate_response\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003euser_input_2\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ef\u003c/span\u003e\u003cspan\u003e\u0026#34;Bot: \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003eresponse_2\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;----------------------\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e# Third input with /think\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003euser_input_3\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;Really? /think\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ef\u003c/span\u003e\u003cspan\u003e\u0026#34;User: \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003euser_input_3\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eresponse_3\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003echatbot\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egenerate_response\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003euser_input_3\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ef\u003c/span\u003e\u003cspan\u003e\u0026#34;Bot: \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003eresponse_3\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"agentic-usages\"\u003eAgentic Usages\u003c/h3\u003e\u003cp\u003eQwen3 excels in tool calling capabilities. We recommend using \u003ca href=\"https://github.com/QwenLM/Qwen-Agent\"\u003eQwen-Agent\u003c/a\u003e to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\u003c/p\u003e\u003cp\u003eTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\u003c/p\u003e\u003cdiv\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"python\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003eqwen_agent.agents\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eAssistant\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e# Define LLM\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003ellm_cfg\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e\u0026#39;model\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#39;Qwen3-30B-A3B\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e# Use the endpoint provided by Alibaba Model Studio:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e# \u0026#39;model_type\u0026#39;: \u0026#39;qwen_dashscope\u0026#39;,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e# \u0026#39;api_key\u0026#39;: os.getenv(\u0026#39;DASHSCOPE_API_KEY\u0026#39;),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e# Use a custom endpoint compatible with OpenAI API:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e\u0026#39;model_server\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#39;http://localhost:8000/v1\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e  \u003cspan\u003e# api_base\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e\u0026#39;api_key\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#39;EMPTY\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e# Other parameters:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e# \u0026#39;generate_cfg\u0026#39;: {\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e#         # Add: When the response content is `\u0026lt;think\u0026gt;this is the thought\u0026lt;/think\u0026gt;this is the answer;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e#         # Do not add: When the response has been separated by reasoning_content and content.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e#         \u0026#39;thought_in_content\u0026#39;: True,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e#     },\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e# Define Tools\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003etools\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\u0026#39;mcpServers\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e  \u003cspan\u003e# You can specify the MCP configuration file\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e            \u003cspan\u003e\u0026#39;time\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e                \u003cspan\u003e\u0026#39;command\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#39;uvx\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e                \u003cspan\u003e\u0026#39;args\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;mcp-server-time\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#39;--local-timezone=Asia/Shanghai\u0026#39;\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e            \u003cspan\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e            \u003cspan\u003e\u0026#34;fetch\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e                \u003cspan\u003e\u0026#34;command\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#34;uvx\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e                \u003cspan\u003e\u0026#34;args\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#34;mcp-server-fetch\u0026#34;\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e            \u003cspan\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e        \u003cspan\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e  \u003cspan\u003e\u0026#39;code_interpreter\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e  \u003cspan\u003e# Built-in tools\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e# Define Agent\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003ebot\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eAssistant\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ellm\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003ellm_cfg\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003efunction_list\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003etools\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e# Streaming generation\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emessages\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e[{\u003c/span\u003e\u003cspan\u003e\u0026#39;role\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#39;user\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#39;content\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#39;https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen\u0026#39;\u003c/span\u003e\u003cspan\u003e}]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003efor\u003c/span\u003e \u003cspan\u003eresponses\u003c/span\u003e \u003cspan\u003ein\u003c/span\u003e \u003cspan\u003ebot\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003erun\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emessages\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003emessages\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e    \u003cspan\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eresponses\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"friends-of-qwen\"\u003eFriends of Qwen\u003c/h2\u003e\u003cp\u003eThanks to the support of so many friends. Qwen is nothing without its friends! We welcome more people or organizations to join our community and help us become better!\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/qwen3-logo.png\" width=\"100%\"/\u003e\u003c/figure\u003e\u003ch2 id=\"future-work\"\u003eFuture Work\u003c/h2\u003e\u003cp\u003eQwen3 represents a significant milestone in our journey toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI). By scaling up both pretraining and reinforcement learning (RL), we have achieved higher levels of intelligence. We have seamlessly integrated thinking and non-thinking modes, offering users the flexibility to control the thinking budget. Additionally, we have expanded support for a wide range of languages, enhancing global accessibility.\u003c/p\u003e\u003cp\u003eLooking ahead, we aim to enhance our models across multiple dimensions. This includes refining model architectures and training methodologies to achieve several key objectives: scaling data, increasing model size, extending context length, broadening modalities, and advancing RL with environmental feedback for long-horizon reasoning. We believe we are transitioning from an era focused on training models to one centered on training agents. Our next iteration promises to bring meaningful advancements to everyone’s work and life.\u003c/p\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "17 min read",
  "publishedTime": "2025-04-29T04:00:00+08:00",
  "modifiedTime": "2025-04-29T04:00:00+08:00"
}
