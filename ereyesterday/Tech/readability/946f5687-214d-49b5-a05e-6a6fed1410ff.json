{
  "id": "946f5687-214d-49b5-a05e-6a6fed1410ff",
  "title": "Snap says New Mexico intentionally friended alleged child predators, then blamed the company",
  "link": "https://www.theverge.com/2024/11/21/24302767/snap-new-mexico-attorney-general-motion-dismiss-child-exploitation",
  "description": "",
  "author": "Lauren Feiner",
  "published": "2024-11-21T19:08:08-05:00",
  "source": "https://www.theverge.com/rss/index.xml",
  "categories": null,
  "byline": "Lauren Feiner",
  "length": 3300,
  "excerpt": "Snap is accusing the New Mexico attorney general of misrepresenting its investigation into child sexual abuse material (CSAM) on the service in a motion to dismiss the suit.",
  "siteName": "The Verge",
  "favicon": "https://www.theverge.com/icons/android_chrome_512x512.png",
  "text": "Snap says the basis of a scathing lawsuit suggesting it systematically recommends teens’ accounts to child predators is backwards — the company is now accusing the New Mexico attorney general of intentionally seeking out such accounts before recommendations were made. The company says the AG’s case is based on “gross misrepresentations” and cherry picks from Snap’s internal documents. In a motion to dismiss filed Thursday, Snap says AG Raúl Torrez’s complaint makes “patently false” allegations, and particularly misrepresents its own undercover investigation, in which the AG’s office created a decoy 14-year-old account. Torrez alleges Snap violated the state’s unfair practices and public nuisance laws by misleading users’ about the safety and ephemerality of its “disappearing” messages, which he says have enabled abusers to collect and retain exploitative images of minors. But Snap claims that contrary to the way the state described it, investigators were the ones who sent friend requests from the decoy account “to obviously targeted usernames like ‘nudedude_22,’ ‘teenxxxxxxx06,’ ‘ineedasugardadx,’ and ‘xxx_tradehot.’” And Snap says it was actually the government’s decoy account that searched for and added an account called “Enzo (Nud15Ans)” — which allegedly went on to ask the decoy to send anonymous messages through an end-to-end encrypted service — rather than the reverse, as the state alleges. The state claims that after connecting with Enzo, “Snapchat suggested over 91 users, including numerous adult users whose accounts included or sought to exchange sexually explicit content.”Snap also says the state “repeatedly mischaracterizes” its internal documents, including blaming Snap for choosing “not to store child sex abuse images” and suggesting it failed to provide them to law enforcement. In reality, according to Snap, it’s not allowed to store child sexual abuse material (CSAM) on its servers under federal law, and says it “of course” turns any such content over to the National Center for Missing and Exploited Children as mandated.Lauren Rodriguez, director of communications for the New Mexico Department of Justice, says Snap wants to dismiss the case to “to avoid accountability for the serious harm its platform causes to children.” In a statement, she says, “The evidence we have presented—including internal documents and findings from our investigation—clearly demonstrates that Snap has long known about the dangers on its platform and has failed to act. Rather than addressing these critical issues with real change to their algorithms and design features, Snap continues to put profits over protecting children.”The company is seeking to dismiss the lawsuit on several grounds, including that the state is attempting to mandate age verification and parental controls that violate the First Amendment and that the legal liability shield Section 230 should block the suit. Snap also says that the AG’s claims of Snap’s alleged misrepresentation of its services is centered around “puffery-based ‘catchphrases’ (e.g., that Snapchat is a ‘worry-free’ platform) and aspirational statements regarding Snap’s commitment to safety, neither of which remotely guarantees that Snap would (much less could) extinguish all potential risks posed by third parties.”",
  "image": "https://cdn.vox-cdn.com/thumbor/BgReAXdHJwGNA68gB-pkYpmXCGU=/0x0:2040x1360/1200x628/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/23982579/acastro_STK466_01.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eSnap says the basis of a \u003ca href=\"https://www.theverge.com/2024/10/1/24259653/snap-new-mexico-ag-lawsuit-csam-kids-safety\"\u003escathing lawsuit\u003c/a\u003e suggesting it systematically recommends teens’ accounts to child predators is backwards — the company is now accusing the New Mexico attorney general of intentionally seeking out such accounts before recommendations were made. The company says the AG’s case is based on “gross misrepresentations” and cherry picks from Snap’s internal documents. \u003c/p\u003e\u003cp\u003eIn a motion to dismiss filed Thursday, Snap says AG Raúl Torrez’s complaint makes “patently false” allegations, and particularly misrepresents its own undercover investigation, in which the AG’s office created a decoy 14-year-old account. Torrez alleges Snap violated the state’s unfair practices and public nuisance laws by misleading users’ about the safety and ephemerality of its \u003ca href=\"https://www.theverge.com/2024/9/5/24236722/snapchat-new-mexico-attorney-general-kids-online-safety-csam\"\u003e“disappearing” messages\u003c/a\u003e, which he says have enabled abusers to collect and retain exploitative images of minors. \u003c/p\u003e\u003cp\u003eBut Snap claims that contrary to the way the state described it, investigators were the ones who sent friend requests from the decoy account “to obviously targeted usernames like ‘nudedude_22,’ ‘teenxxxxxxx06,’ ‘ineedasugardadx,’ and ‘xxx_tradehot.’” \u003c/p\u003e\u003cp\u003eAnd Snap says it was actually the government’s decoy account that searched for and added an account called “Enzo (Nud15Ans)” — which allegedly went on to ask the decoy to send anonymous messages through an end-to-end encrypted service — rather than the reverse, as the state alleges. The state claims that after connecting with Enzo, “Snapchat suggested over 91 users, including numerous adult users whose accounts included or sought to exchange sexually explicit content.”\u003c/p\u003e\u003cp\u003eSnap also says the state “repeatedly mischaracterizes” its internal documents, including blaming Snap for choosing “not to store child sex abuse images” and suggesting it failed to provide them to law enforcement. In reality, according to Snap, it’s not allowed to store child sexual abuse material (CSAM) on its servers under federal law, and says it “of course” turns any such content over to the National Center for Missing and Exploited Children as mandated.\u003c/p\u003e\u003cp\u003eLauren Rodriguez, director of communications for the New Mexico Department of Justice, says Snap wants to dismiss the case to “to avoid accountability for the serious harm its platform causes to children.” In a statement, she says, “The evidence we have presented—including internal documents and findings from our investigation—clearly demonstrates that Snap has long known about the dangers on its platform and has failed to act. Rather than addressing these critical issues with real change to their algorithms and design features, Snap continues to put profits over protecting children.”\u003c/p\u003e\u003cp\u003eThe company is seeking to dismiss the lawsuit on several grounds, including that the state is attempting to mandate age verification and parental controls that violate the First Amendment and that the legal liability shield Section 230 should block the suit. \u003c/p\u003e\u003cp\u003eSnap also says that the AG’s claims of Snap’s alleged misrepresentation of its services is centered around “puffery-based ‘catchphrases’ (e.g., that Snapchat is a ‘worry-free’ platform) and aspirational statements regarding Snap’s commitment to safety, neither of which remotely guarantees that Snap would (much less could) extinguish all potential risks posed by third parties.”\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-11-22T00:08:07.968Z",
  "modifiedTime": "2024-11-22T00:08:07.968Z"
}
