{
  "id": "782dc67d-d234-4e38-8819-95ed35d556b3",
  "title": "Eerily realistic AI voice demo sparks amazement and discomfort online",
  "link": "https://arstechnica.com/ai/2025/03/users-report-emotional-bonds-with-startlingly-realistic-ai-voice-demo/",
  "description": "Sesame's new AI voice model features uncanny imperfections, and it's willing to act like an angry boss.",
  "author": "Benj Edwards",
  "published": "Tue, 04 Mar 2025 23:35:01 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "Tech",
    "AI assistants",
    "chatbots",
    "ChatGPT",
    "chatgtp",
    "emotional AI",
    "machine learning",
    "Sesame",
    "voice synthesis"
  ],
  "byline": "Benj Edwards",
  "length": 9068,
  "excerpt": "Sesame’s new AI voice model features uncanny imperfections, and it’s willing to act like an angry boss.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Sesame's new AI voice model features uncanny imperfections, and it's willing to act like an angry boss. In late 2013, the Spike Jonze film Her imagined a future where people would form emotional connections with AI voice assistants. Nearly 12 years later, that fictional premise has veered closer to reality with the release of a new conversational voice model from AI startup Sesame that has left many users both fascinated and unnerved. \"I tried the demo, and it was genuinely startling how human it felt,\" wrote one Hacker News user who tested the system. \"I'm almost a bit worried I will start feeling emotionally attached to a voice assistant with this level of human-like sound.\" In late February, Sesame released a demo for the company's new Conversational Speech Model (CSM) that appears to cross over what many consider the \"uncanny valley\" of AI-generated speech, with some testers reporting emotional connections to the male or female voice assistant (\"Miles\" and \"Maya\"). In our own evaluation, we spoke with the male voice for about 28 minutes, talking about life in general and how it decides what is \"right\" or \"wrong\" based on its training data. The synthesized voice was expressive and dynamic, imitating breath sounds, chuckles, interruptions, and even sometimes stumbling over words and correcting itself. These imperfections are intentional. \"At Sesame, our goal is to achieve 'voice presence'—the magical quality that makes spoken interactions feel real, understood, and valued,\" writes the company in a blog post. \"We are creating conversational partners that do not just process requests; they engage in genuine dialogue that builds confidence and trust over time. In doing so, we hope to realize the untapped potential of voice as the ultimate interface for instruction and understanding.\" Sometimes the model tries too hard to sound like a real human. In one demo posted online by a Reddit user called MetaKnowing, the AI model talks about craving \"peanut butter and pickle sandwiches.\" An example of Sesame's female voice model craving peanut butter and pickle sandwiches, captured by Reddit user MetaKnowing. An example of Sesame's female voice model craving peanut butter and pickle sandwiches, captured by Reddit user MetaKnowing. Founded by Brendan Iribe, Ankit Kumar, and Ryan Brown, Sesame AI has attracted significant backing from prominent venture capital firms. The company has secured investments from Andreessen Horowitz, led by Anjney Midha and Marc Andreessen, along with Spark Capital, Matrix Partners, and various founders and individual investors. Browsing reactions to Sesame found online, we found many users expressing astonishment at its realism. \"I've been into AI since I was a child, but this is the first time I've experienced something that made me definitively feel like we had arrived,\" wrote one Reddit user. \"I'm sure it's not beating any benchmarks, or meeting any common definition of AGI, but this is the first time I've had a real genuine conversation with something I felt was real.\" Many other Reddit threads express similar feelings of surprise, with commenters saying it's \"jaw-dropping\" or \"mind-blowing.\" While that sounds like a bunch of hyperbole at first glance, not everyone finds the Sesame experience pleasant. Mark Hachman, a senior editor at PCWorld, wrote about being deeply unsettled by his interaction with the Sesame voice AI. \"Fifteen minutes after 'hanging up' with Sesame's new 'lifelike' AI, and I'm still freaked out,\" Hachman reported. He described how the AI's voice and conversational style eerily resembled an old friend he had dated in high school. Others have compared Sesame's voice model to OpenAI's Advanced Voice Mode for ChatGPT, saying that Sesame's CSM features more realistic voices, and others are pleased that the model in the demo will roleplay angry characters, which ChatGPT refuses to do. An example argument with Sesame's CSM created by Gavin Purcell. An example argument with Sesame's CSM created by Gavin Purcell. Gavin Purcell, co-host of the AI for Humans podcast, posted an example video on Reddit where the human pretends to be an embezzler and argues with a boss. It's so dynamic that it's difficult to tell who the human is and which one is the AI model. Judging by our own demo, it's entirely capable of what you see in the video. “Near-human quality” Under the hood, Sesame's CSM achieves its realism by using two AI models working together (a backbone and a decoder) based on Meta's Llama architecture that processes interleaved text and audio. Sesame trained three AI model sizes, with the largest using 8.3 billion parameters (an 8 billion backbone model plus a 300 million parameter decoder) on approximately 1 million hours of primarily English audio. Sesame's CSM doesn't follow the traditional two-stage approach used by many earlier text-to-speech systems. Instead of generating semantic tokens (high-level speech representations) and acoustic details (fine-grained audio features) in two separate stages, Sesame's CSM integrates into a single-stage, multimodal transformer-based model, jointly processing interleaved text and audio tokens to produce speech. OpenAI's voice model uses a similar multimodal approach. In blind tests without conversational context, human evaluators showed no clear preference between CSM-generated speech and real human recordings, suggesting the model achieves near-human quality for isolated speech samples. However, when provided with conversational context, evaluators still consistently preferred real human speech, indicating a gap remains in fully contextual speech generation. Sesame co-founder Brendan Iribe acknowledged current limitations in a comment on Hacker News, noting that the system is \"still too eager and often inappropriate in its tone, prosody and pacing\" and has issues with interruptions, timing, and conversation flow. \"Today, we're firmly in the valley, but we're optimistic we can climb out,\" he wrote. Too close for comfort? Despite CSM's technological impressiveness, advancements in conversational voice AI carry significant risks for deception and fraud. The ability to generate highly convincing human-like speech has already supercharged voice phishing scams, allowing criminals to impersonate family members, colleagues, or authority figures with unprecedented realism. But adding realistic interactivity to those scams may take them to another level of potency. Unlike current robocalls that often contain tell-tale signs of artificiality, next-generation voice AI could eliminate these red flags entirely. As synthetic voices become increasingly indistinguishable from human speech, you may never know who you're talking to on the other end of the line. It's inspired some people to share a secret word or phrase with their family for identity verification. Although Sesame's demo does not clone a person's voice, future open source releases of similar technology could allow malicious actors to potentially adapt these tools for social engineering attacks. OpenAI itself held back its own voice technology from wider deployment over fears of misuse. Sesame sparked a lively discussion on Hacker News about its potential uses and dangers. Some users reported having extended conversations with the two demo voices, with conversations lasting up to the 30-minute limit. In one case, a parent recounted how their 4-year-old daughter developed an emotional connection with the AI model, crying after not being allowed to talk to it again. The company says it plans to open-source \"key components\" of its research under an Apache 2.0 license, enabling other developers to build upon their work. Their roadmap includes scaling up model size, increasing dataset volume, expanding language support to over 20 languages, and developing \"fully duplex\" models that better handle the complex dynamics of real conversations. You can try the Sesame demo on the company's website, assuming that it isn't too overloaded with people who want to simulate a rousing argument. Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC. 13 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/bluedigitalface1-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n            \u003carticle data-id=\"2079608\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n      \n\n      \n\n      \u003cp\u003e\n        Sesame\u0026#39;s new AI voice model features uncanny imperfections, and it\u0026#39;s willing to act like an angry boss.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \n\u003cp\u003eIn late 2013, the Spike Jonze film \u003ca href=\"https://en.wikipedia.org/wiki/Her_(2013_film)\"\u003e\u003cem\u003eHer\u003c/em\u003e\u003c/a\u003e imagined a future where people would form \u003ca href=\"https://arstechnica.com/information-technology/2023/10/people-are-speaking-with-chatgpt-for-hours-bringing-2013s-her-closer-to-reality/\"\u003eemotional connections\u003c/a\u003e with AI voice assistants. Nearly 12 years later, that fictional premise has veered closer to reality with the release of a new conversational voice model from AI startup Sesame that has left many users both fascinated and unnerved.\u003c/p\u003e\n\u003cp\u003e\u0026#34;I tried the demo, and it was genuinely startling how human it felt,\u0026#34; \u003ca href=\"https://news.ycombinator.com/item?id=43227957\"\u003ewrote\u003c/a\u003e one Hacker News user who tested the system. \u0026#34;I\u0026#39;m almost a bit worried I will start feeling emotionally attached to a voice assistant with this level of human-like sound.\u0026#34;\u003c/p\u003e\n\u003cp\u003eIn late February, Sesame \u003ca href=\"https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo\"\u003ereleased a demo\u003c/a\u003e for the company\u0026#39;s new Conversational Speech Model (CSM) that appears to cross over what many consider the \u0026#34;uncanny valley\u0026#34; of AI-generated speech, with some testers reporting emotional connections to the male or female voice assistant (\u0026#34;Miles\u0026#34; and \u0026#34;Maya\u0026#34;).\u003c/p\u003e\n\u003cp\u003eIn our own evaluation, we spoke with the male voice for about 28 minutes, talking about life in general and how it decides what is \u0026#34;right\u0026#34; or \u0026#34;wrong\u0026#34; based on its training data. The synthesized voice was expressive and dynamic, imitating breath sounds, chuckles, interruptions, and even sometimes stumbling over words and correcting itself. These imperfections are intentional.\u003c/p\u003e\n\u003cp\u003e\u0026#34;At Sesame, our goal is to achieve \u0026#39;voice presence\u0026#39;—the magical quality that makes spoken interactions feel real, understood, and valued,\u0026#34; writes the company in a \u003ca href=\"https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice\"\u003eblog post\u003c/a\u003e. \u0026#34;We are creating conversational partners that do not just process requests; they engage in genuine dialogue that builds confidence and trust over time. In doing so, we hope to realize the untapped potential of voice as the ultimate interface for instruction and understanding.\u0026#34;\u003c/p\u003e\n\u003cp\u003eSometimes the model tries too hard to sound like a real human. In one demo posted online by a Reddit user called MetaKnowing, the AI model talks about craving \u0026#34;peanut butter and pickle sandwiches.\u0026#34;\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cfigure\u003e\n  \u003cp\u003e\n    \u003cvideo id=\"video-2079608-1\" width=\"222\" height=\"480\" preload=\"metadata\" controls=\"controls\"\u003e\u003csource type=\"video/mp4\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/03/m2-res_480p.mp4?_=1\"/\u003eAn example of Sesame\u0026#39;s female voice model craving peanut butter and pickle sandwiches, captured by Reddit user MetaKnowing.\u003c/video\u003e\n  \u003c/p\u003e\n\n  \u003cfigcaption\u003e\n    \u003cspan\u003e\u003c/span\u003e\n    \u003cdiv\u003e\n    \n    \u003cp\u003e\n      An example of Sesame\u0026#39;s female voice model craving peanut butter and pickle sandwiches, captured by Reddit user MetaKnowing.\n\n          \u003c/p\u003e\n  \u003c/div\u003e\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eFounded by Brendan Iribe, Ankit Kumar, and Ryan Brown, Sesame AI has attracted significant backing from prominent venture capital firms. The company has secured investments from Andreessen Horowitz, led by Anjney Midha and Marc Andreessen, along with Spark Capital, Matrix Partners, and various founders and individual investors.\u003c/p\u003e\n\u003cp\u003eBrowsing reactions to Sesame found online, we found many users expressing astonishment at its realism. \u0026#34;I\u0026#39;ve been into AI since I was a child, but this is the first time I\u0026#39;ve experienced something that made me definitively feel like we had arrived,\u0026#34; \u003ca href=\"https://www.reddit.com/r/singularity/comments/1j12j93/the_sesame_voice_model_has_been_the_moment_for_me/\"\u003ewrote\u003c/a\u003e one Reddit user. \u0026#34;I\u0026#39;m sure it\u0026#39;s not beating any benchmarks, or meeting any common definition of AGI, but this is the first time I\u0026#39;ve had a real genuine conversation with something I felt was real.\u0026#34; Many other \u003ca href=\"https://www.reddit.com/r/SesameAI/\"\u003eReddit threads\u003c/a\u003e express similar feelings of surprise, with commenters saying it\u0026#39;s \u0026#34;\u003ca href=\"https://www.reddit.com/r/singularity/comments/1j0t2fn/comment/mfe6396/\"\u003ejaw-dropping\u003c/a\u003e\u0026#34; or \u0026#34;\u003ca href=\"https://www.reddit.com/r/singularity/comments/1j0t2fn/comment/mfev579/\"\u003emind-blowing\u003c/a\u003e.\u0026#34;\u003c/p\u003e\n\u003cp\u003eWhile that sounds like a bunch of hyperbole at first glance, not everyone finds the Sesame experience pleasant. Mark Hachman, a senior editor at PCWorld, \u003ca href=\"https://www.pcworld.com/article/2623695/i-was-so-freaked-out-by-talking-to-this-ai-that-i-had-to-leave.html\"\u003ewrote about\u003c/a\u003e being deeply unsettled by his interaction with the Sesame voice AI. \u0026#34;Fifteen minutes after \u0026#39;hanging up\u0026#39; with Sesame\u0026#39;s new \u0026#39;lifelike\u0026#39; AI, and I\u0026#39;m still freaked out,\u0026#34; Hachman reported. He described how the AI\u0026#39;s voice and conversational style eerily resembled an old friend he had dated in high school.\u003c/p\u003e\n\u003cp\u003eOthers have compared Sesame\u0026#39;s voice model to OpenAI\u0026#39;s \u003ca href=\"https://arstechnica.com/information-technology/2024/07/when-counting-quickly-openais-new-voice-mode-stops-to-catch-its-breath/\"\u003eAdvanced Voice Mode\u003c/a\u003e for ChatGPT, saying that Sesame\u0026#39;s CSM features more realistic voices, and others are pleased that the model in the demo will roleplay angry characters, which ChatGPT refuses to do.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cfigure\u003e\n  \u003cp\u003e\n    \u003cvideo id=\"video-2079608-2\" width=\"1920\" height=\"1080\" preload=\"metadata\" controls=\"controls\"\u003e\u003csource type=\"video/mp4\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/03/roleplay_with_sesames_new_voice_ai_feels_like_the-5vfk12m3qbme1.mp4?_=2\"/\u003eAn example argument with Sesame\u0026#39;s CSM created by Gavin Purcell.\u003c/video\u003e\n  \u003c/p\u003e\n\n  \u003cfigcaption\u003e\n    \u003cspan\u003e\u003c/span\u003e\n    \u003cdiv\u003e\n    \n    \u003cp\u003e\n      An example argument with Sesame\u0026#39;s CSM created by Gavin Purcell.\n\n          \u003c/p\u003e\n  \u003c/div\u003e\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eGavin Purcell, co-host of the \u003ca href=\"https://www.aiforhumans.show/\"\u003eAI for Humans podcast\u003c/a\u003e, posted an \u003ca href=\"https://www.reddit.com/r/singularity/comments/1j1yern/roleplay_with_sesames_new_voice_ai_feels_like_the/\"\u003eexample video on Reddit\u003c/a\u003e where the human pretends to be an embezzler and argues with a boss. It\u0026#39;s so dynamic that it\u0026#39;s difficult to tell who the human is and which one is the AI model. Judging by our own demo, it\u0026#39;s entirely capable of what you see in the video.\u003c/p\u003e\n\n\u003ch2\u003e“Near-human quality”\u003c/h2\u003e\n\u003cp\u003eUnder the hood, Sesame\u0026#39;s CSM achieves its realism by using two AI models working together (a backbone and a decoder) based on \u003ca href=\"https://arstechnica.com/information-technology/2024/04/meta-releases-chatgpt-like-ai-site-and-open-weights-llama-3-model/\"\u003eMeta\u0026#39;s Llama\u003c/a\u003e architecture that processes interleaved text and audio. Sesame trained three AI model sizes, with the largest using 8.3 billion parameters (an 8 billion backbone model plus a 300 million parameter decoder) on approximately 1 million hours of primarily English audio.\u003c/p\u003e\n\u003cp\u003eSesame\u0026#39;s CSM doesn\u0026#39;t follow the traditional two-stage approach used by many earlier text-to-speech systems. Instead of generating semantic tokens (high-level speech representations) and acoustic details (fine-grained audio features) in two separate stages, Sesame\u0026#39;s CSM integrates into a single-stage, multimodal transformer-based model, jointly processing interleaved text and audio tokens to produce speech. OpenAI\u0026#39;s voice model uses a similar multimodal approach.\u003c/p\u003e\n\u003cp\u003eIn blind tests without conversational context, human evaluators showed no clear preference between CSM-generated speech and real human recordings, suggesting the model achieves near-human quality for isolated speech samples. However, when provided with conversational context, evaluators still consistently preferred real human speech, indicating a gap remains in fully contextual speech generation.\u003c/p\u003e\n\u003cp\u003eSesame co-founder Brendan Iribe \u003ca href=\"https://news.ycombinator.com/item?id=43230447\"\u003eacknowledged\u003c/a\u003e current limitations in a comment on Hacker News, noting that the system is \u0026#34;still too eager and often inappropriate in its tone, prosody and pacing\u0026#34; and has issues with interruptions, timing, and conversation flow. \u0026#34;Today, we\u0026#39;re firmly in the valley, but we\u0026#39;re optimistic we can climb out,\u0026#34; he wrote.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003ch2\u003eToo close for comfort?\u003c/h2\u003e\n\u003cp\u003eDespite CSM\u0026#39;s technological impressiveness, advancements in conversational voice AI carry significant risks for deception and fraud. The ability to generate highly convincing human-like speech has already \u003ca href=\"https://arstechnica.com/tech-policy/2023/03/rising-scams-use-ai-to-mimic-voices-of-loved-ones-in-financial-distress/\"\u003esupercharged voice phishing scams\u003c/a\u003e, allowing criminals to impersonate family members, colleagues, or authority figures with unprecedented realism. But adding realistic interactivity to those scams may take them to another level of potency.\u003c/p\u003e\n\u003cp\u003eUnlike current robocalls that often contain tell-tale signs of artificiality, next-generation voice AI could eliminate these red flags entirely. As synthetic voices become increasingly indistinguishable from human speech, you may never know who you\u0026#39;re talking to on the other end of the line. It\u0026#39;s inspired some people to \u003ca href=\"https://arstechnica.com/ai/2024/12/your-ai-clone-could-target-your-family-but-theres-a-simple-defense/\"\u003eshare a secret word or phrase\u003c/a\u003e with their family for identity verification.\u003c/p\u003e\n\u003cp\u003eAlthough Sesame\u0026#39;s demo does not clone a person\u0026#39;s voice, future open source releases of similar technology could allow malicious actors to potentially adapt these tools for social engineering attacks. OpenAI itself held back its own \u003ca href=\"https://arstechnica.com/information-technology/2024/03/openai-holds-back-wide-release-of-voice-cloning-tech-due-to-misuse-concerns/\"\u003evoice technology\u003c/a\u003e from wider deployment over fears of misuse.\u003c/p\u003e\n\u003cp\u003eSesame sparked a \u003ca href=\"https://news.ycombinator.com/item?id=43227881\"\u003elively discussion\u003c/a\u003e on Hacker News about its potential uses and dangers. Some users reported having extended conversations with the two demo voices, with conversations lasting up to the 30-minute limit. In one case, a parent \u003ca href=\"https://news.ycombinator.com/item?id=43229168\"\u003erecounted\u003c/a\u003e how their 4-year-old daughter developed an emotional connection with the AI model, crying after not being allowed to talk to it again.\u003c/p\u003e\n\u003cp\u003eThe company says it plans to open-source \u0026#34;key components\u0026#34; of its research under an Apache 2.0 license, enabling other developers to build upon their work. Their roadmap includes scaling up model size, increasing dataset volume, expanding language support to over 20 languages, and developing \u0026#34;fully duplex\u0026#34; models that better handle the complex dynamics of real conversations.\u003c/p\u003e\n\u003cp\u003eYou can \u003ca href=\"https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo\"\u003etry the Sesame demo\u003c/a\u003e on the company\u0026#39;s website, assuming that it isn\u0026#39;t too overloaded with people who want to simulate a rousing argument.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n  \n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/benjedwards/\"\u003e\u003cimg src=\"https://arstechnica.com/wp-content/uploads/2022/08/benj_ega.png\" alt=\"Photo of Benj Edwards\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Benj Edwards is Ars Technica\u0026#39;s Senior AI Reporter and founder of the site\u0026#39;s dedicated AI beat in 2022. He\u0026#39;s also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/ai/2025/03/users-report-emotional-bonds-with-startlingly-realistic-ai-voice-demo/#comments\" title=\"13 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    13 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \n\n\n  \n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/cars/2025/03/protests-broken-windows-even-arson-teslas-massive-elon-problem/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/03/GettyImages-2199656670-768x432.jpg\" alt=\"Listing image for first story in Most Read: Protests, broken windows, even arson: Tesla’s massive Elon problem\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-03-04T23:35:01Z",
  "modifiedTime": "2025-03-04T23:45:31Z"
}
