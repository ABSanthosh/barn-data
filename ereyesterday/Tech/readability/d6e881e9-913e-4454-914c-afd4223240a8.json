{
  "id": "d6e881e9-913e-4454-914c-afd4223240a8",
  "title": "Static search trees: 40x faster than binary search",
  "link": "https://curiouscoding.nl/posts/static-search-tree/",
  "description": "Comments",
  "author": "",
  "published": "Wed, 01 Jan 2025 00:08:03 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Ragnar {Groot Koerkamp}",
  "length": 76249,
  "excerpt": "Table of Contents 1 Introduction 1.1 Problem statement 1.2 Recommended reading 1.3 Binary search and Eytzinger layout 1.4 Hugepages 1.5 A note on benchmarking 1.6 Cache lines 1.7 S-trees and B-trees 2 Optimizing find 2.1 Linear 2.2 Auto-vectorization 2.3 Trailing zeros 2.4 Popcount 2.5 Manual SIMD 3 Optimizing the search 3.1 Batching 3.2 Prefetching 3.3 Pointer arithmetic 3.3.1 Up-front splat 3.3.2 Byte-based pointers 3.3.3 The final version 3.4 Skip prefetch 3.5 Interleave 4 Optimizing the tree layout 4.1 Left-tree 4.2 Memory layouts 4.3 Node size \\(B=15\\) 4.3.1 Data structure size 4.4 Summary 5 Prefix partitioning 5.1 Full layout 5.2 Compact subtrees 5.3 The best of both: compact first level 5.4 Overlapping trees 5.5 Human data 5.6 Prefix map 5.7 Summary 6 Multi-threaded comparison 7 Conclusion 7.1 Future work 7.1.1 Branchy search 7.1.2 Interpolation search 7.1.3 Packing data smaller 7.1.4 Returning indices in original data 7.1.5 Range queries In this post, we will implement a static search tree (S+ tree) for high-throughput searching of sorted data, as introduced on Algorithmica. We‚Äôll mostly take the code presented there as a starting point, and optimize it to its limits. For a large part, I‚Äôm simply taking the ‚Äòfuture work‚Äô ideas of that post and implementing them. And then there will be a bunch of looking at assembly code to shave off all the instructions we can. Lastly, there will be one big addition to optimize throughput: batching.",
  "siteName": "CuriousCoding",
  "favicon": "https://curiouscoding.nl/images/bike-64.png",
  "text": "December 2024 60-minute readTable of Contents1 Introduction1.1 Problem statement1.2 Recommended reading1.3 Binary search and Eytzinger layout1.4 Hugepages1.5 A note on benchmarking1.6 Cache lines1.7 S-trees and B-trees2 Optimizing find2.1 Linear2.2 Auto-vectorization2.3 Trailing zeros2.4 Popcount2.5 Manual SIMD3 Optimizing the search3.1 Batching3.2 Prefetching3.3 Pointer arithmetic3.3.1 Up-front splat3.3.2 Byte-based pointers3.3.3 The final version3.4 Skip prefetch3.5 Interleave4 Optimizing the tree layout4.1 Left-tree4.2 Memory layouts4.3 Node size \\(B=15\\)4.3.1 Data structure size4.4 Summary5 Prefix partitioning5.1 Full layout5.2 Compact subtrees5.3 The best of both: compact first level5.4 Overlapping trees5.5 Human data5.6 Prefix map5.7 Summary6 Multi-threaded comparison7 Conclusion7.1 Future work7.1.1 Branchy search7.1.2 Interpolation search7.1.3 Packing data smaller7.1.4 Returning indices in original data7.1.5 Range queriesIn this post, we will implement a static search tree (S+ tree) for high-throughput searching of sorted data, as introduced on Algorithmica. We‚Äôll mostly take the code presented there as a starting point, and optimize it to its limits. For a large part, I‚Äôm simply taking the ‚Äòfuture work‚Äô ideas of that post and implementing them. And then there will be a bunch of looking at assembly code to shave off all the instructions we can. Lastly, there will be one big addition to optimize throughput: batching.All source code, including benchmarks and plotting code, is at github:RagnarGrootKoerkamp/suffix-array-searching.1 Introduction 1.1 Problem statement Input. A sorted list of \\(n\\) 32bit unsigned integers vals: Vec\u003cu32\u003e.Output. A data structure that supports queries \\(q\\), returning the smallest element of vals that is at least \\(q\\), or u32::MAX if no such element exists. Optionally, the index of this element may also be returned.Metric. We optimize throughput. That is, the number of (independent) queries that can be answered per second. The typical case is where we have a sufficiently long queries: \u0026[u32] as input, and return a corresponding answers: Vec\u003cu32\u003e.Note that we‚Äôll usually report reciprocal throughput as ns/query (or just ns), instead of queries/s. You can think of this as amortized (not average) time spent per query.Benchmarking setup. For now, we will assume that both the input and queries are simply uniform random sampled 31bit integers1.Code. In code, this can be modelled by the trait shown in Code Snippet 1.1 2 3 4 5 6 7 8 9 trait SearchIndex { /// Two functions with default implementations in terms of each other. fn query_one(\u0026self, query: u32) -\u003e u32 { Self::query(\u0026vec![query])[0] } fn query(\u0026self, queries: \u0026[u32]) -\u003e Vec\u003cu32\u003e { queries.iter().map(|\u0026q| Self::query_one(q)).collect() } } Code Snippet 1: Trait that our solution should implement.1.2 Recommended reading The classical solution to this problem is binary search, which we will briefly visit in the next section. A great paper on this and other search layouts is ‚ÄúArray Layouts for Comparison-Based Searching‚Äù by Khuong and Morin (2017). Algorithmica also has a case study based on that paper.This post will focus on S+ trees, as introduced on Algorithmica in the followup post, static B-trees. In the interest of my time, I will mostly assume that you are familiar with that post.I also recommend reading my work-in-progress introduction to CPU performance, which contains some benchmarks pushing the CPU to its limits. We will use the metrics obtained there as baseline to understand our optimization attempts.Also helpful is the Intel Intrinsics Guide when looking into SIMD instructions. Note that we‚Äôll only be using AVX2 instructions here, as in, we‚Äôre assuming intel. And we‚Äôre not assuming less available AVX512 instructions (in particular, since my laptop doesn‚Äôt have them).1.3 Binary search and Eytzinger layout As a baseline, we will use the Rust standard library binary search implementation. 1 2 3 4 5 6 7 8 9 10 pub struct SortedVec { vals: Vec\u003cu32\u003e, } impl SortedVec { pub fn binary_search_std(\u0026self, q: u32) -\u003e u32 { let idx = self.vals.binary_search(\u0026q).unwrap_or_else(|i| i); self.vals[idx] } } Code Snippet 2: The binary search in the Rust standard library.The main conclusion of the array layouts paper (Khuong and Morin 2017) is that the Eytzinger layout is one of the best in practice. This layout reorders the values in memory: the binary search effectively is a binary search tree on the data, the root the middle node, then the nodes at positions \\(\\frac 14 n\\) and \\(\\frac 34 n\\), then \\(\\frac 18n, \\frac 38n, \\frac 58n, \\frac 78n\\), and so on. The main benefit of this layout is that all values needed for the first steps of the binary search are close together, so they can be cached efficiently. If we put the root at index \\(1\\), the two children of the node at index \\(i\\) are at \\(2i\\) and \\(2i+1\\). This means that we can effectively prefetch the next cache line, before knowing whether we need index \\(2i\\) or \\(2i+1\\). This can be taken a step further and we can prefetch the cache line containing indices \\(16i\\) to \\(16i+15\\), which are exactly the values needed 4 iterations from now. For a large part, this can quite effectively hide the latency associated with the traversal of the tree. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 pub struct Eytzinger { /// The root of the tree is at index 1. vals: Vec\u003cu32\u003e, } impl Eytzinger { /// L: number of levels ahead to prefetch. pub fn search_prefetch\u003cconst L: usize\u003e(\u0026self, q: u32) -\u003e u32 { let mut idx = 1; while (1 \u003c\u003c L) * idx \u003c self.vals.len() { idx = 2 * idx + (q \u003e self.get(idx)) as usize; prefetch_index(\u0026self.vals, (1 \u003c\u003c L) * idx); } // The last few iterations don't need prefetching anymore. while idx \u003c self.vals.len() { idx = 2 * idx + (q \u003e self.get(idx)) as usize; } let zeros = idx.trailing_ones() + 1; let idx = idx \u003e\u003e zeros; self.get(idx) } } Code Snippet 3: Implementation of searching the Eytzinger layout, with \\(L=4\\) levels of prefetching.If we plot these two, we see that Eytzinger layout performs as good as binary search when the array fits in L2 cache (256kB for me, the middle red line), but starts to be much better than binary search as the array grows to be much larger than the L3 cache (12MB). In the end, Eytzinger search is around 4 times faster, which nicely corresponds to being able to prefetch 4 iterations of cache lines from memory at a time.Figure 1: Query throughput of binary search and Eytzinger layout as the size of the input increases. At 1GB input, binary search needs around 1150ns/query, while Eytzinger is 6x faster at 200ns/query.1.4 Hugepages For all experiments, we‚Äôll make sure to allocate the tree using 2MB hugepages by default, instead of the usual 4kB pages. This reduces pressure on the translation lookaside buffer (TLB) that translates virtual memory addresses to hardware memory addresses, since its internal table of pages is much smaller when using hugepages, and hence can be cached better.With transparent hugepages enabled, they are automatically given out whenever allocating an exact multiple of 2MB, and so we always round up the allocation for the tree to the next multiple of 2MB. However, it turns out that small allocations below 32MB still go on the program‚Äôs heap, rather than asking the kernel for new memory pages, causing them to not actually be hugepages. Thus, all allocations we do are actually rounded up to the next multiple of 32MB instead.All together, hugepages sometimes makes a small difference when the dataset is indeed between 1MB and 32MB in size. Smaller data structures don‚Äôt really need hugepages anyway. Enabling them for the Eytzinger layout as in the plot above also gives a significant speedup for larger sizes.1.5 A note on benchmarking The plots have the size of the input data on the logarithmic (bottom) x-axis. On the top, they show the corresponding number of elements in the vector, which is 4 times less, since each element is a u32 spanning 4 bytes. Measurements are taken at values \\(2^i\\), \\(1.25 \\cdot 2^i\\), \\(1.5\\cdot 2^i\\), and \\(1.75\\cdot 2^i\\).The y-axis shows measured time per query. In the plot above, it says latency, since it is benchmarked as for q in queries { index.query(q); }. Even then, the pipelining and out-of-order execution of the CPU will make it execute multiple iterations in parallel. Specifically, while it is waiting for the last cache lines of iteration \\(i\\), it can already start executing the first instructions of the next query. To measure the true latency, we would have to introduce a loop carried dependency by making query \\(i+1\\) dependent on the result of query \\(i\\). However, the main goal of this post is to optimize for throughput, so we won‚Äôt bother with that.Thus, all plots will show the throughput of doing index.query(all_queries).For the benchmarks, I‚Äôm using my laptop‚Äôs i7-10750H CPU, with the frequency fixed to 2.6GHz using Code Snippet 4.21 sudo cpupower frequency-set -g powersave -d 2.6GHz -u 2.6GHz Code Snippet 4: Pinning the CPU frequency to 2.6GHz.Also relevant are the sizes of the caches: 32KiB L1 cache per core, 256KiB L2 cache per core, and 12MiB L3 cache shared between the physical 6 cores. Furthermore, hyper-threading is disabled.All measurements are done 5 times. The line follows the median, and we show the spread of the 2nd to 4th value (i.e., after discarding the minimum and maximum). Observe that in most of the plot above, the spread is barely visible! Thus, while especially the graph for binary search looks very noisy, that ‚Äônoise‚Äô is in fact completely reproducible. Indeed, it‚Äôs caused by effects of cache associativity, as explained in the array layouts paper (Khuong and Morin (2017); this post is long enough already).1.6 Cache lines Main memory and the caches work at the level of cache lines consisting of 64 bytes (at least on my machine), or 16 u32 values. Thus, even if you only read a single byte, if the cache line containing that byte is not yet in the L1 cache, the entire thing will be fetched from RAM or L3 or L2 into L1.Plain binary search typically only uses a single value of each cache line, until it gets to the end of the search where the last 16 values span just 1 or 2 cache lines.They Eytzinger layout suffers the same problem: even though the next cache line can be prefetched, it still only uses a single value in each. This fundamentally means that both these search schemes are using the available memory bandwidth quite inefficiently, and since most of what they are doing is waiting for memory to come through, that‚Äôs not great. Also, while that‚Äôs not relevant yet, when doing this with many threads in parallel, or with batching, single-core RAM throughput and the throughput of the main memory itself become a bottleneck.It would be much better if somehow, we could use the information in each cache line much more efficiently ;)We can do that by storing our data in a different way. Instead of storing it layer by layer, so that each iteration goes into a new layer, we can store 4 layers of the tree at a time (Code Snippet 5). That takes 15 values, and could nicely be padded into a full cache line. Then when we fetch a cache line, we can use it for 4 iterations at once ‚Äì much better! On the other hand, now we can‚Äôt prefetch upcoming cache lines in advance anymore, so that overall the latency will be the same. But we fetch up to 4 times fewer cache lines overall, which should help throughput.Unfortunately, I don‚Äôt have code and plots here, because what I really want to focus on is the next bit.Figure 2: The first two rows show how we could pack four layers of the Eytzinger search into a single cache line. The first follows a classic binary search layout, while the second applies the Eytzinger layout recursively. The third row shows an S-tree node instead. For simplicity and clarity, I‚Äôm using consecutive values, but in practice, this would be any list of sorted numbers.1.7 S-trees and B-trees We just ended with a node of 15 values that represent a height-4 search tree in which we can binary search. From there, it‚Äôs just a small step to S-trees.B-trees. But first I have to briefly mention B-trees though (wikipedia). Those are the more classic dynamic variant, where nodes are linked together via pointers. As wikipedia writes, they are typically used with much larger block sizes, for example 4kB, since files read from disk usually come in 4kB chunks. Thus, they also have much larger branching factors.S-trees. But we will instead use S-trees, as named so by Algorithmica. They are a nice middle ground between the high branching factor of B-trees, and the compactness of the Eytzinger layout. Instead of interpreting the 15 values as a search tree, we can also store them in a sorted way, and consider them as a 16-ary search tree: the 15 values simply split the data in the subtree into 16 parts, and we can do a linear scan to find which part to recurse into. But if we store 15 values and one padding in a cache line, we might as well make it 16 values and have a branching factor of 17 instead.S+ trees. B-trees and S-trees only store each value once, either in a leaf node or in an internal node. This turns out to be somewhat annoying, since we must track in which layer the result was found. To simplify this, we can store all values as a leaf, and duplicate them in the internal nodes. This is then called a B+ tree or S+ tree. However, I will be lazy and just use S-tree to include this modification.Figure 3: An example of a ‚Äòfull‚Äô S+ tree (that I will from now just call S-tree) on 18 values with nodes of size (B=2) and branching factor (B+1=3). Each internal node stores the smallest value in the subtree on its right. In memory, the layers are simply packed together behind each other.A full S-tree can be navigated in a way similar to the Eytzinger layout: The node (note: not3 value) at index \\(i\\) has its \\(B+1\\) child-nodes at indices \\((B+1)\\cdot i + 1 + \\{0, \\dots, B\\}\\).When the tree is only partially filled, the full layout can waste a lot of space (Figure 4). Instead, we can pack the layers together, by storing the offset \\(o_\\ell\\) of each layer.The children of node \\(o_\\ell + i\\) are then at \\(o_{\\ell+1} + (B+1)\\cdot i + \\{0, \\dots, B\\}\\).Figure 4: The full representation can be inefficient. The packed representation removes the empty space, and explicitly stores the offset (o_ell) where each layer starts.At last, let‚Äôs have a look at some code. Each node in the tree is simply represented as a list of \\(N=16\\) u32 values. We explicitly ask that nodes are aligned to 64byte cache line boundaries.1 2 3 4 #[repr(align(64))] pub struct TreeNode\u003cconst N: usize\u003e { data: [u32; N], } Code Snippet 5: Search tree node, aligned to a 64 byte cache line. For now, N is always 16. The values in a node must always be sorted.The S-tree itself is simply a list of nodes, and the offsets where each layer starts.1 2 3 4 5 6 7 8 9 /// N: #elements in a node, always 16. /// B: branching factor \u003c= N+1. Typically 17. pub struct STree\u003cconst B: usize, const N: usize\u003e { /// The list of tree nodes. tree: Vec\u003cTreeNode\u003cN\u003e\u003e, /// The root is at index tree[offsets[0]]. /// It's children start at tree[offsets[1]], and so on. offsets: Vec\u003cusize\u003e, } Code Snippet 6: The S-tree data structure. It depends on the number of values per node \\(B\\) (usually 16 but sometimes 15) and the size of each node \\(N\\) (always 16).To save some space, and focus on the interesting part (to me, at least), I will not show any code for constructing S-trees. It‚Äôs a whole bunch of uninteresting fiddling with indices, and takes a lot of time to get right. Also, construction is not optimized at all currently. Anyway, find the code here.TODO: Reverse offsets.What we will look at, is code for searching S-trees. 1 2 3 4 5 6 7 8 9 10 11 12 13 fn search(\u0026self, q: u32, find: impl Fn(\u0026TreeNode\u003cN\u003e, u32) -\u003e usize) -\u003e u32 { let mut k = 0; for o in self.offsets[0..self.offsets.len()-1] { let jump_to = find(self.node(o + k), q); k = k * (B + 1) + jump_to; } let o = self.offsets.last().unwrap(); // node(i) returns tree[i] using unchecked indexing. let mut idx = find(self.node(o + k), q); // get(i, j) returns tree[i].data[j] using unchecked indexing. self.get(o + k + idx / N, idx % N) } Our first step will be optimizing the find function.2 Optimizing find 2.1 Linear Let‚Äôs first precisely define what we want find to do: it‚Äôs input is a node with 16 sorted values and a query value \\(q\\), and it should return the index of the first element that is at least \\(q\\).Some simple code for this is Code Snippet 8.1 2 3 4 5 6 7 8 pub fn find_linear(\u0026self, q: u32) -\u003e usize { for i in 0..N { if self.data[i] \u003e= q { return i; } } N } Code Snippet 8: A linear scan for the first element \\(\\geq q\\), that breaks as soon as it is found.The results are not very impressive yet.Figure 5: The initial version of our S-tree search is quite a bit slower than the Eytzinger layout. In this and following plots, ‚Äòold‚Äô lines will be dimmed, and the best previous and best new line slightly highlighted. Colours will be consistent from one plot to the next.2.2 Auto-vectorization As it turns out, the break; in Code Snippet 8 is really bad for performance, since the branch predictor can‚Äôt do a good job on it.Instead, we can count the number of values less than \\(q\\), and return that as the index of the first value \\(\\geq q\\). (Example: all values \\(\\geq q\\) index gives index 0.)1 2 3 4 5 6 7 8 9 pub fn find_linear_count(\u0026self, q: u32) -\u003e usize { let mut count = 0; for i in 0..N { if self.data[i] \u003c q { count += 1; } } count } Code Snippet 9: Counting values \\(\u003c q\\) instead of an early break. The if self.data[i] \u003c q can be optimized into branchless code.In fact, the code is not just branchless, but actually it‚Äôs auto-vectorized into SIMD instructions! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 vmovdqu (%rax,%rcx), %ymm1 ; load data[..8] vmovdqu 32(%rax,%rcx), %ymm2 ; load data[8..] vpbroadcastd %xmm0, %ymm0 ; 'splat' the query value vpmaxud %ymm0, %ymm2, %ymm3 ; v vpcmpeqd %ymm3, %ymm2, %ymm2 ; v vpmaxud %ymm0, %ymm1, %ymm0 ; v vpcmpeqd %ymm0, %ymm1, %ymm0 ; 4x compare query with values vpackssdw %ymm2, %ymm0, %ymm0 ; vpcmpeqd %ymm1, %ymm1, %ymm1 ; v vpxor %ymm1, %ymm0, %ymm0 ; 2x negate result vextracti128 $1, %ymm0, %xmm1 ; v vpacksswb %xmm1, %xmm0, %xmm0 ; v vpshufd $216, %xmm0, %xmm0 ; v vpmovmskb %xmm0, %ecx ; 4x extract mask popcntl %ecx, %ecx ; popcount the 16bit mask Code Snippet 10: Code Snippet 9 is auto-vectorized!To save some space: you can find this and further results for this section in Figure 34 at the end of the section.This auto-vectorized version is over two times faster than the linear find, and now clearly beats Eytzinger layout!2.3 Trailing zeros We can also roll our own SIMD. The SIMD version of the original linear scan idea does 16 comparisons in parallel, converts that to a bitmask, and then counts the number of trailing zeros. Using #[feature(portable_simd)], that looks like this:1 2 3 4 5 6 pub fn find_ctz(\u0026self, q: u32) -\u003e usize { let data: Simd\u003cu32, N\u003e = Simd::from_slice(\u0026self.data[0..N]); let q = Simd::splat(q); let mask = q.simd_le(data); mask.first_set().unwrap_or(N) } Code Snippet 11: A find implementation using the count-trailing-zeros instruction. 1 2 3 4 5 6 7 8 9 10 11 vpminud 32(%rsi,%r8), %ymm0, %ymm1 ; take min of data[8..] and query vpcmpeqd %ymm1, %ymm0, %ymm1 ; does the min equal query? vpminud (%rsi,%r8), %ymm0, %ymm2 ; take min of data[..8] and query vpcmpeqd %ymm2, %ymm0, %ymm2 ; does the min equal query? vpackssdw %ymm1, %ymm2, %ymm1 ; pack the two results together, interleaved as 16bit words vextracti128 $1, %ymm1, %xmm2 ; extract half (both halves are equal) vpacksswb %xmm2, %xmm1, %xmm1 ; go down to 8bit values, but weirdly shuffled vpshufd $216, %xmm1, %xmm1 ; unshuffle vpmovmskb %xmm1, %r8d ; extract the high bit of each 8bit value. orl $65536,%r8d ; set bit 16, to cover the unwrap_or(N) tzcntl %r8d,%r15d ; count trailing zeros Code Snippet 12: Assembly code for Code Snippet 11. Instead of ending with popcntl, this ends with tzcntl.Now, let‚Äôs look at this generated code in a bit more detail.First up: why does simd_le translate into min and cmpeq?From checking the Intel Intrinsics Guide, we find out that there are only signed comparisons, while our data is unsigned. For now, let‚Äôs just assume that all values fit in 31 bits and are at most i32::MAX. Then, we can transmute our input to Simd\u003ci32, 8\u003e without changing its meaning.AssumptionBoth input values and queries are between 0 and i32::MAX.Eventually we can fix this by either taking i32 input directly, or by shifting u32 values to fit in the i32 range. 1 2 3 4 5 6 7 8 9 10 11 pub fn find_ctz_signed(\u0026self, q: u32) -\u003e usize where LaneCount\u003cN\u003e: SupportedLaneCount, { - let data: Simd\u003cu32, N\u003e = Simd::from_slice( \u0026self.data[0..N] ); + let data: Simd\u003ci32, N\u003e = Simd::from_slice(unsafe { transmute(\u0026self.data[0..N]) }); - let q = Simd::splat(q ); + let q = Simd::splat(q as i32); let mask = q.simd_le(data); mask.first_set().unwrap_or(N) } Code Snippet 13: Same as before, but now using i32 values instead of u32. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 -vpminud 32(%rsi,%r8), %ymm0, %ymm1 -vpcmpeqd %ymm1, %ymm0, %ymm1 +vpcmpgtd 32(%rsi,%rdi), %ymm1, %ymm2 ; is query(%ymm1) \u003e data[8..]? -vpminud (%rsi,%r8), %ymm0, %ymm2 -vpcmpeqd %ymm2, %ymm0, %ymm2 +vpcmpgtd (%rsi,%rdi), %ymm1, %ymm1 ; is query(%ymm1) \u003e data[..8]? vpackssdw %ymm2, %ymm1, %ymm1 ; pack results +vpxor %ymm0, %ymm1, %ymm1 ; negate results (ymm0 is all-ones) vextracti128 $1, %ymm1, %xmm2 ; extract u16x16 vpacksswb %xmm2, %xmm1, %xmm1 ; shuffle vpshufd $216, %xmm1, %xmm1 ; extract u8x16 vpmovmskb %xmm1, %edi ; extract u16 mask orl $65536,%edi ; add bit to get 16 when none set tzcntl %edi,%edi ; count trailing zeros Code Snippet 14: The two vpminud and vpcmpeqd instructions are gone now and merged into vpcmpgtd, but instead we got a vpxor back :/ (Ignore the different registers being used in the old versus the new version.)It turns out there is only a \u003e instruction in SIMD, and not \u003e=, and so there is no way to avoid inverting the result.We also see a vpshufd instruction that feels very out of place. What‚Äôs happening is that while packing the result of the 16 u32 comparisons down to a single 16bit value, data is interleaved in an unfortunate way, and we need to fix that. Here, Algorithmica takes the approach of ‚Äòpre-shuffling‚Äô the values in each node to counter for the unshuffle instruction. They also suggest using popcount instead, which is indeed what we‚Äôll do next.2.4 Popcount As we saw, the drawback of the trailing zero count approach is that the order of the lanes must be preserved. Instead, we‚Äôll now simply count the number of lanes with a value less than the query, similar to the auto-vectorized SIMD before, so that the order of lanes doesn‚Äôt matter. 1 2 3 4 5 6 7 8 9 10 11 pub fn find_popcnt_portable(\u0026self, q: u32) -\u003e usize where LaneCount\u003cN\u003e: SupportedLaneCount, { let data: Simd\u003ci32, N\u003e = Simd::from_slice(unsafe { transmute(\u0026self.data[0..N]) }); let q = Simd::splat(q as i32); - let mask = q.simd_le(data); + let mask = q.simd_gt(data); - mask.first_set().unwrap_or(N) + mask.to_bitmask().count_ones() as usize } Code Snippet 15: Using popcount instead of trailing zeros. 1 2 3 4 5 6 7 8 9 10 vpcmpgtd 32(%rsi,%rdi), %ymm0, %ymm1 vpcmpgtd (%rsi,%rdi), %ymm0, %ymm0 vpackssdw %ymm1, %ymm0, %ymm0 ; 1 -vpxor %ymm0, %ymm1, %ymm1 vextracti128 $1, %ymm0, %xmm1 ; 2 vpacksswb %xmm1, %xmm0, %xmm0 ; 3 vpshufd $216, %xmm0, %xmm0 ; 4 vpmovmskb %xmm0, %edi ; 5 -orl $65536,%edi +popcntl %edi, %edi Code Snippet 16: the xor and or instructions are gone, but we are still stuck with the sequence of 5 instructions to go from the comparison results to an integer bitmask.Ideally we would like to movmsk directly on the u16x16 output of the first pack instruction, vpackssdw, to get the highest bit of each of the 16 16-bit values. Unfortunately, we are again let down by AVX2: there are movemask instructions for u8, u32, and u64, but not for u16.Also, the vpshufd instruction is now provably useless, so it‚Äôs slightly disappointing the compiler didn‚Äôt elide it. Time to write the SIMD by hand instead.2.5 Manual SIMD As it turns out, we can get away without most of the packing! Instead of using vpmovmskb (_mm256_movemask_epi8) on 8bit data, we can actually just use it directly on the 16bit output of vpackssdw! Since the comparison sets each lane to all-zeros or all-ones, we can safely read the most significant and middle bit, and divide the count by two at the end.4 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 pub fn find_popcnt(\u0026self, q: u32) -\u003e usize { // We explicitly require that N is 16. let low: Simd\u003cu32, 8\u003e = Simd::from_slice(\u0026self.data[0..N / 2]); let high: Simd\u003cu32, 8\u003e = Simd::from_slice(\u0026self.data[N / 2..N]); let q_simd = Simd::\u003c_, 8\u003e::splat(q as i32); unsafe { use std::mem::transmute as t; // Transmute from u32 to i32. let mask_low = q_simd.simd_gt(t(low)); let mask_high = q_simd.simd_gt(t(high)); // Transmute from portable_simd to __m256i intrinsic types. let merged = _mm256_packs_epi32(t(mask_low), t(mask_high)); // 32 bits is sufficient to hold a count of 2 per lane. let mask: i32 = _mm256_movemask_epi8(t(merged)); mask.count_ones() as usize / 2 } } Code Snippet 17: Manual version of the SIMD code, by explicitly using the intrinsics. This is kinda ugly now, and there's a lot of transmuting (casting) going on between [u32; 8], Simd\u003cu32, 8\u003e and the native __m256i type, but we'll have to live with it.1 2 3 4 5 6 7 8 9 vpcmpgtd (%rsi,%rdi), %ymm0, %ymm1 vpcmpgtd 32(%rsi,%rdi), %ymm0, %ymm0 vpackssdw %ymm0, %ymm1, %ymm0 -vextracti128 $1, %ymm0, %xmm1 -vpacksswb %xmm1, %xmm0, %xmm0 -vpshufd $216, %xmm0, %xmm0 -vpmovmskb %xmm0, %edi +vpmovmskb %ymm0, %edi popcntl %edi, %edi Code Snippet 18: Only 5 instructions total are left now. Note that there is no explicit division by 2, since this is absorbed into the pointer arithmetic in the remainder, after the function is inlined.Now let‚Äôs have a look at the results of all this work.Figure 6: Using the S-tree with an optimized find function improves throughput from 240ns/query for Eytzinger to 140ns/query for the auto-vectorized one, and down to 115ns/query for the final hand-optimized version, which is over 2x speedup!As can be seen very nicely in this plot, each single instruction that we remove gives a small but consistent improvement in throughput. The biggest improvement comes from the last step, where we indeed shaved off 3 instructions.In fact, we can analyse this plot a bit more:For input up to \\(2^6=64\\) bytes, the performance is constant, since in this case the ‚Äòsearch tree‚Äô only consists of the root node.Up to input of size \\(2^{10}\\), the thee has two layers, and the performance is constant.Similarly, we see the latency jumping up at size \\(2^{14}\\), \\(2^{18}\\), \\(2^{22}\\) and \\(2^{26}\\), each time because a new layer is added to the tree. (Or rather, the jumps are at powers of the branching factor \\(B+1=17\\) instead of \\(2^4=16\\), but you get the idea.)In a way, we can also (handwaivily) interpret the x-axis as time: each time the graph jumps up, the height of the jump is pretty much the time spent on processing that one extra layer of the tree.Once we exceed the size of L3 cache, things slow down quickly. At that point, each extra layer of the tree adds a significant amount of time, since waiting for RAM is inherently slow.On the other hand, once we hit RAM, the slowdown is more smooth rather than stepwise. This is because L3 is still able to cache a fraction of the data structure, and that fraction only decreases slowly.Again handwavily, we can also interpret the x-axis as a snapshot of space usage at a fixed moment in time: the first three layers of the tree fit in L1. The 4th and 5th layers fit in L2 and L3. Once the three is 6 layers deep, the reads of that layer will mostly hit RAM, and any additional layers for sure are going to RAM.From now on, this last version, find_popcnt, is the one we will be using.3 Optimizing the search 3.1 Batching As promised, the first improvement we‚Äôll make is batching. Instead of processing one query at a time, we can process multiple (many) queries at once. This allows the CPU to work on multiple queries at the same time, and in particular, it can have multiple (up to 10-12) in-progress requests to RAM at a time. That way, instead of waiting for a latency of 80ns per read, we effectively wait for 10 reads at the same time, lowering the amortized wait time to around 8ns.Batching very much benefits from the fact that we use an S+ tree instead of S-tree, since each element is find in the last layer (at the same depth), and hence the number of seach steps through the tree is the same for every element in the batch. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 fn batch\u003cconst P: usize\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let mut k = [0; P]; for [o, _o2] in self.offsets.array_windows() { for i in 0..P { let jump_to = self.node(o + k[i]).find(qb[i]); k[i] = k[i] * (B + 1) + jump_to; } } let o = self.offsets.last().unwrap(); from_fn(|i| { let idx = self.node(o + k[i]).find(qb[i]); self.get(o + k[i] + idx / N, idx % N) }) } Code Snippet 19: The batching code is very similar to processing one query at a time. We just insert an additional loop over the batch of \\(P\\) items.Figure 7: Batch size 1 (red) performs very similar to our non-batched version (blue), around 115ns/query. Increasing the batch size to 2, 4, and 8 each time significantly improves performance, until it saturates at 45ns/query (2.5x faster) around 16.One interesting observation is that going from batch size 1 to 2 does not double the performance. I suspect this is because the CPU‚Äôs out-of-order execution was already deep enough to effectively execute (almost) 2 queries in parallel anyway. Going to a batch size of 4 and then 8 does provide a significant speedup. Again going to 4 the speedup is relatively a bit less than when going to 8, so probably even with batch size 4 the CPU is somewhat looking ahead into the next batch of 4 already ü§Ø.Throughput saturates at batch size 16 (or really, around 12 already), which corresponds to the CPU having 12 line fill buffers and thus being able to read up to 12 cache lines in parallel.Nevertheless, we will settle on a batch size of 128, mostly because it leads to slightly cleaner plots in the remainder. It is also every so slightly faster, probably because the constant overhead of initializing a batch is smaller when batches are larger.3.2 Prefetching The CPU is already fetching multiple reads in parallel using out-of-order execution, but we can also help out a bit by doing this explicitly using prefetching. After processing a node, we determine the child node k that we need to visit next, so we can directly request that node to be read from memory before continuing with the rest of the batch. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fn batch\u003cconst P: usize\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let mut k = [0; P]; for [o, o2] in self.offsets.array_windows() { for i in 0..P { let jump_to = self.node(o + k[i]).find(qb[i]); k[i] = k[i] * (B + 1) + jump_to; + prefetch_index(\u0026self.tree, o2 + k[i]); } } let o = self.offsets.last().unwrap(); from_fn(|i| { let idx = self.node(o + k[i]).find(qb[i]); self.get(o + k[i] + idx / N, idx % N) }) } Code Snippet 20: Prefetching the cache line/node for the next iteration ahead.Figure 8: Prefetching helps speeding things up once the data does not fit in L2 cache anymore, and gets us down from 45ns/query to 30ns/query for 1GB input.We observe a few things: first prefetching slightly slow things down while data fits in L1 already, since in that case the instruction just doesn‚Äôt do anything anyway. In L2, it makes the graph slightly more flat, indicating that already there, the latency is already a little bit of a bottleneck. In L3 this effect gets larger, and we get a nice smooth/horizontal graph, until we hit RAM size. There, prefetching provides the biggest gains.3.3 Pointer arithmetic Again, it‚Äôs time to look at some assembly code, now to optimize the search function itself. Results are down below in Figure 9.3.3.1 Up-front splat First, we can note that the find function splat‚Äôs the query from a u32 to a Simd\u003cu32, 8\u003e on each call. It‚Äôs slightly nicer (but not really faster, actually) to splat all the queries up-front, and then reuse those. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pub fn batch_splat\u003cconst P: usize\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let mut k = [0; P]; + let q_simd = qb.map(|q| Simd::\u003cu32, 8\u003e::splat(q)); for [o, o2] in self.offsets.array_windows() { for i in 0..P { - let jump_to = self.node(o + k[i]).find (qb[i] ); + let jump_to = self.node(o + k[i]).find_splat(q_simd[i]); k[i] = k[i] * (B + 1) + jump_to; prefetch_index(\u0026self.tree, o2 + k[i]); } } let o = self.offsets.last().unwrap(); from_fn(|i| { - let idx = self.node(o + k[i]).find (qb[i] ); + let idx = self.node(o + k[i]).find_splat(q_simd[i]); self.get(o + k[i] + idx / N, idx % N) }) } Code Snippet 21: Hoisting the splat out of the loop is slightly nicer, but not faster.The assembly code for each iteration of the first loop now looks like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 movq (%rsp,%r11),%r15 leaq (%r9,%r15),%r12 shlq $6, %r12 vmovdqa 1536(%rsp,%r11,4),%ymm0 vpcmpgtd (%rsi,%r12), %ymm0, %ymm1 vpcmpgtd 32(%rsi,%r12), %ymm0, %ymm0 vpackssdw %ymm0, %ymm1, %ymm0 vpmovmskb %ymm0, %r12d popcntl %r12d, %r12d shrl %r12d movq %r15,%r13 shlq $4, %r13 addq %r15,%r13 addq %r12,%r13 movq %r13,(%rsp,%r11) shlq $6, %r13 prefetcht0 (%r10,%r13) Code Snippet 22: Assembly code for each iteration of Code Snippet 21. (Actually it's unrolled into two copied of this, but they're identical.)3.3.2 Byte-based pointers Looking at the code above, we see two shlq $6 instructions that multiply the given value by \\(64\\). That‚Äôs because our tree nodes are 64 bytes large, and hence, to get the \\(i\\)‚Äôth element of the array, we need to read at byte \\(64\\cdot i\\). For smaller element sizes, there are dedicated read instructions that inline, say, an index multiplication by 8. But for a stride of 64, the compiler has to generate ‚Äòmanual‚Äô multiplications in the form of a shift.Additionally, direct pointer-based lookups can be slightly more efficient here than array-indexing: when doing self.tree[o + k[i]], we can effectively pre-compute the pointer to self.tree[o], so that only k[i] still has to be added. Let‚Äôs first look at that diff: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 pub fn batch_ptr\u003cconst P: usize\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let mut k = [0; P]; let q_simd = qb.map(|q| Simd::\u003cu32, 8\u003e::splat(q)); + // offsets[l] is a pointer to self.tree[self.offsets[l]] + let offsets = self.offsets.iter() + .map(|o| unsafe { self.tree.as_ptr().add(*o) }) + .collect_vec(); for [o, o2] in offsets.array_windows() { for i in 0..P { - let jump_to = self.node(o + k[i]) .find_splat(q_simd[i]); + let jump_to = unsafe { *o.add(k[i]) }.find_splat(q_simd[i]); k[i] = k[i] * (B + 1) + jump_to; - prefetch_index(\u0026self.tree, o2 + k[i]); + prefetch_ptr(unsafe { o2.add(k[i]) }); } } let o = offsets.last().unwrap(); from_fn(|i| { - let idx = self.node(o + k[i]) .find_splat(q_simd[i]); + let idx = unsafe { *o.add(k[i]) }.find_splat(q_simd[i]); - self.get(o + k[i] + idx / N, idx % N) + unsafe { *(*o.add(k[i] + idx / N)).data.get_unchecked(idx % N) } }) } Code Snippet 23: Using pointer-based indexing instead of array indexing.Now, we can avoid all the multiplications by 64, by just multiplying all k[i] by 64 to start with: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 pub fn batch_byte_ptr\u003cconst P: usize\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let mut k = [0; P]; let q_simd = qb.map(|q| Simd::\u003cu32, 8\u003e::splat(q)); let offsets = self .offsets .iter() .map(|o| unsafe { self.tree.as_ptr().add(*o) }) .collect_vec(); for [o, o2] in offsets.array_windows() { for i in 0..P { - let jump_to = unsafe { *o. add(k[i]) }.find_splat(q_simd[i]); + let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]); - k[i] = k[i] * (B + 1) + jump_to ; + k[i] = k[i] * (B + 1) + jump_to * 64; - prefetch_ptr(unsafe { o2. add(k[i]) }); + prefetch_ptr(unsafe { o2.byte_add(k[i]) }); } } let o = offsets.last().unwrap(); from_fn(|i| { - let idx = unsafe { *o. add(k[i]) }.find_splat(q_simd[i]); + let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]); - unsafe { *(*o.add(k[i] + idx / N)).data.get_unchecked(idx % N) } + unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() } }) } Code Snippet 24: We multiply k[i] by 64 up-front, and then call byte_add instead of the usual add.Indeed, the generated code now goes down from 17 to 15 instructions, and we can see in Figure 9 that this gives a significant speedup! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 movq 32(%rsp,%rdi),%r8 vmovdqa 1568(%rsp,%rdi,4),%ymm0 vpcmpgtd (%rsi,%r8), %ymm0, %ymm1 vpcmpgtd 32(%rsi,%r8), %ymm0, %ymm0 vpackssdw %ymm0, %ymm1, %ymm0 vpmovmskb %ymm0, %r9d popcntl %r9d, %r9d movq %r8,%r10 shlq $4, %r10 addq %r8,%r10 shll $5, %r9d andl $-64,%r9d addq %r10,%r9 movq %r9,32(%rsp,%rdi) prefetcht0 (%rcx,%r9) Code Snippet 25: When using byte-based pointers, we avoid some multiplications by 64.3.3.3 The final version One particularity about the code above is the andl $-64,%r9d. In line 6, the bitmask gets written there. Then in line 7, it‚Äôs popcounted. Life 11 does a shll $5, i.e., a multiplication by 32, which is a combination of the /2 to compensate for the double-popcount and the * 64. Then, it does the and $-64, where the mask of -64 is 111..11000000 which ends in 6 zeros. But we just multiplied by 32, so all this does is zeroing out a single bit, in case the popcount was odd. But we know for a fact that that can never be, so we don‚Äôt actually need this and instruction.To avoid it, we do this /2*64 =\u003e *32 optimization manually. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 pub fn find_splat64(\u0026self, q_simd: Simd\u003cu32, 8\u003e) -\u003e usize { let low: Simd\u003cu32, 8\u003e = Simd::from_slice(\u0026self.data[0..N / 2]); let high: Simd\u003cu32, 8\u003e = Simd::from_slice(\u0026self.data[N / 2..N]); unsafe { let q_simd: Simd\u003ci32, 8\u003e = t(q_simd); let mask_low = q_simd.simd_gt(t(low)); let mask_high = q_simd.simd_gt(t(high)); use std::mem::transmute as t; let merged = _mm256_packs_epi32(t(mask_low), t(mask_high)); let mask = _mm256_movemask_epi8(merged); - mask.count_ones() as usize / 2 + mask.count_ones() as usize * 32 } } pub fn batch_byte_ptr\u003cconst P: usize\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let mut k = [0; P]; let q_simd = qb.map(|q| Simd::\u003cu32, 8\u003e::splat(q)); let offsets = self .offsets .iter() .map(|o| unsafe { self.tree.as_ptr().add(*o) }) .collect_vec(); for [o, o2] in offsets.array_windows() { for i in 0..P { - let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat (q_simd[i]); + let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]); - k[i] = k[i] * (B + 1) + jump_to * 64; + k[i] = k[i] * (B + 1) + jump_to ; prefetch_ptr(unsafe { o2.byte_add(k[i]) }); } } let o = offsets.last().unwrap(); from_fn(|i| { let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]); unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() } }) } Code Snippet 26: Manually merging /2 and *64 into *32.Again, this gives a small speedup.Figure 9: Results of improving the search function bit by bit. Like before, the improvements are small but consistent. Throughput on 1GB input improves from 31ns/query to 28ns/query.3.4 Skip prefetch Now we know that the first three levels of the graph fit in L1 cache, so probably we can simply skip prefetching for those levels.Figure 10: Skipping the prefetch for the first layers is slightly slower.As it turns out, skipping the prefetch does not help. Probably because the prefetch is cheap if the data is already available, and there is a small chance that the data we need was evicted to make room for other things, in which case the prefetch is useful.3.5 Interleave One other observation is that the first few layers are CPU bound, while the last few layers are memory throughput bound. By merging the two domains, we should be able to get a higher total throughput. (Somewhat similar to how for a piece wise linear convex function \\(f\\), \\(f((x+y)/2) \u003c (f(x)+f(y))/2\\) when \\(x\\) and \\(y\\) are on different pieces.) Thus, maybe we could process two batches of queries at the same time by processing layer \\(i\\) of one batch at the same time as layer \\(i+L/2\\) of the other batch (where \\(L\\) is the height of the tree). I implemented this, but unfortunately the result is not faster than what we had.Or maybe we can split the work as: interleave the last level of one half with all but the last level of the other half? Since the last-level memory read takes most of the time. Also that turns out slower in practice.What does give a small speedup: process the first two levels of the next batch interleaved with the last prefetch of the current batch. Still the result is only around 2ns speedup, while code the (not shown ;\") gets significantly more messy.What does work great, is interleaving all layers of the search: when the tree has \\(L\\) layers, we can interleave \\(L\\) batches at a time, and then process layer \\(i\\) of the \\(i\\)‚Äôth in-progress batch. Then we ‚Äòshift out‚Äô the completed batch and store the answers to those queries, and ‚Äòshift in‚Äô a new batch. This we, completely average the different workloads of all the layers, and should achieve near-optimal performance given the CPU‚Äôs memory bandwidth to L3 and RAM (at least, that‚Äôs what I assume is the bottleneck now).Click to show code for interleaving. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 pub fn batch_interleave_full_128(\u0026self, qs: \u0026[u32]) -\u003e Vec\u003cu32\u003e { match self.offsets.len() { // 1 batch of size 128 1 =\u003e self.batch_interleave_full::\u003c128, 1, 128\u003e(qs), // 2 batches of size 64 in parallel, with product 128 2 =\u003e self.batch_interleave_full::\u003c64, 2, 128\u003e(qs), // 3 batches of size 32 in parallel with product 96 3 =\u003e self.batch_interleave_full::\u003c32, 3, 96\u003e(qs), 4 =\u003e self.batch_interleave_full::\u003c32, 4, 128\u003e(qs), 5 =\u003e self.batch_interleave_full::\u003c16, 5, 80\u003e(qs), 6 =\u003e self.batch_interleave_full::\u003c16, 6, 96\u003e(qs), 7 =\u003e self.batch_interleave_full::\u003c16, 7, 112\u003e(qs), 8 =\u003e self.batch_interleave_full::\u003c16, 8, 128\u003e(qs), _ =\u003e panic!(\"Unsupported tree height {}\", self.offsets.len()), } } pub fn batch_interleave_full\u003cconst P: usize, const L: usize, const PL: usize\u003e( \u0026self, qs: \u0026[u32], ) -\u003e Vec\u003cu32\u003e { assert_eq!(self.offsets.len(), L); let mut out = Vec::with_capacity(qs.len()); let mut ans = [0; P]; // Iterate over chunks of size P of queries. // Omitted: initialize let first_i = L-1; for chunk in qs.array_chunks::\u003cP\u003e() { let i = first_i; // Decrement first_i, modulo L. if first_i == 0 { first_i = L; } first_i -= 1; // Process 1 element per chunk, starting at element first_i. // (Omitted: process first up-to L elements.) // Write output and read new queries from index j. let mut j = 0; loop { // First L-1 levels: do the usual thing. // The compiler will unroll this loop. for l in 0..L - 1 { let jump_to = unsafe { *offsets[l].byte_add(k[i]) }.find_splat64(q_simd[i]); k[i] = k[i] * (B + 1) + jump_to; prefetch_ptr(unsafe { offsets[l + 1].byte_add(k[i]) }); i += 1; } // Last level: read answer. ans[j] = { let idx = unsafe { *ol.byte_add(k[i]) }.find_splat(q_simd[i]); unsafe { (ol.byte_add(k[i]) as *const u32).add(idx).read() } }; // Last level: reset index, and read new query. k[i] = 0; q_simd[i] = Simd::splat(chunk[j]); i += 1; j += 1; if i \u003e PL - L { break; } } // (Omitted: process last up-to L elements.) out.extend_from_slice(\u0026ans); } out } Code Snippet 27: In code, we interleave all layers by compiling a separate function for each height of the tree. Then the compiler can unroll the loop over the layers. There is a bunch of overhead in the code for the first and last iterations that's omitted.Figure 11: Interleaving all layers of the search binary search improves throughput from 29ns/query to 24ns/query.4 Optimizing the tree layout 4.1 Left-tree So far, every internal node of the tree stores the minimum of the subtree on it‚Äôs right (Figure 3, reproduced below).Figure 12: Usually in B+ trees, each node stores the minimum of it‚Äôs right subtree. Let‚Äôs call this a right (S+/B+) tree.This turns out somewhat inefficient when searching values that are exactly in between two subtrees (as also already suggested by Algorithmica), such as \\(5.5\\). In that case, the search descends into the leftmost (green) subtree with node \\([2, 4]\\). Then, it goes to the rightmost (red) node \\([4,5]\\). There, we realize \\(5.5 \u003e 5\\), and thus we need the next value in the red layer (which is stored as a single array), which is \\(6\\). The problem now is that the red tree nodes exactly correspond to cache lines, and thus, the \\(6\\) will be in a new cache line that needs to be fetched from memory.Now consider the left-max tree below:Figure 13: In the left-max S+ tree, each internal node contains the maximum of its left subtree.Now if we search for \\(5.5\\), we descend into the middle subtree rooted at \\([7,9]\\). Then we go left to the \\([6,7]\\) node, and end up reading \\(6\\) as the first value \\(\\geq 5.5\\). Now, the search directly steers toward the node that actually contains the answer, instead of the one just before.Figure 14: The left-S tree brings runtime down from 24ns/query for the interleaved version to 22ns/query now.4.2 Memory layouts Let‚Äôs now consider some alternative memory layouts. So far, we were packing all layers in forward order, but the Algorithmica post actually stores them in reverse, so we‚Äôll try that too. The query code is exactly the same, since the order of the layers is already encoded into the offsets.Another potential improvement is to always store a full array. This may seem very inefficient, but is actually not that bad when we make sure to use uninitialized memory. In that case, untouched memory pages will simply never be mapped, so that we waste on average only about 2MB per layer when hugepages are enabled, and 14MB when there are 7 layers and the entire array takes 1GB.Figure 15: So far we have been using the packed layout. We now also try the reversed layout as used by Algorithmica, and the full layout that allows simple arithmetic for indexing.A benefit of storing the full array is that instead of using the offsets, we can simply compute the index in the next layer directly, as we did for the Eytzinger search. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 pub fn batch_ptr3_full\u003cconst P: usize\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let mut k = [0; P]; let q_simd = qb.map(|q| Simd::\u003cu32, 8\u003e::splat(q)); + let o = self.tree.as_ptr(); - for [o, o2] in offsets.array_windows() { + for _l in 0..self.offsets.len() - 1 { for i in 0..P { let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]); - k[i] = k[i] * (B + 1) + jump_to ; + k[i] = k[i] * (B + 1) + jump_to + 64; prefetch_ptr(unsafe { o.byte_add(k[i]) }); } } from_fn(|i| { let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]); unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() } }) } Code Snippet 28: When storing the array in full, we can drop the per-layer offsets and instead compute indices directly.Figure 16: Comparison with reverse and full memory layout, and full memory layout with using a dedicated _full search that computes indices directly.As it turns out, neither of those layouts improves performance, and so we will not use them going forward.4.3 Node size \\(B=15\\) We can also try storing only 15 values per node, so that the branching factor is 16. This has the benefit of making the multiplication by \\(B+1\\) (17 so far) slightly simpler, since it replaces x = (x\u003c\u003c4)+x by x = x\u003c\u003c4.Figure 17: Storing 15 values per node. The lines in the bottom part of the plot show the overhead that each data structure has relative to the size of the input, capped at 1 (which corresponds to take double the size).When the tree has up to 5 layers and the data fits in L3 cache, using \\(B=15\\) is indeed slightly faster when the number of layers in the tree is the same. On the other hand, the lower branching factor of \\(16\\) requires an additional layer for smaller sizes than when using branching factor \\(17\\). When the input is much larger than L3 cache the speedup disappears, because RAM throughput becomes a common bottleneck.4.3.1 Data structure size Plain binary search and the Eytzinger layout have pretty much no overhead. Our S+ tree so far has around \\(1/16=6.25\\%\\) overhead: \\(1/17\\) of the values in the final layer is duplicated in the layer above, and \\(1/17\\) of those is duplicated again, and so on, for a total of \\(1/17 + 1/17^2 + \\cdots = 1/16\\).Using node size \\(15\\) instead, increases the overhead: Each node now only stores \\(15\\) instead of \\(16\\) elements, so that we already have an overhead of \\(1/15\\). Furthermore the reduced branching factor increases the duplication overhead fro \\(1/16\\) to \\(1/15\\) as well, for a total overhead of \\(2/15 = 13.3\\%\\), which matches the dashed blue line in Figure 17.4.4 Summary Figure 18: A summary of all the improvements we made so far.Of all the improvements so far, only the interleaving is maybe a bit too much: it is the only method that does not work batch-by-batch, but really benefits from having the full input at once. And also its code is three times longer than the plain batched query methods because the first and last few iterations of each loop are handled separately.5 Prefix partitioning So far, we‚Äôve been doing a purely comparison-based search. Now, it is time for something new: partitioning the input values.The simplest form of the idea is to simply partition values by their top \\(b\\) bits, into \\(2^b\\) parts. Then we can build \\(2^b\\) independent search trees and search each query in one of them. If \\(b=12\\), this saves the first two levels of the search (or slightly less, actually, since \\(2^{12} = 16^3 \u003c 17^3\\)).5.1 Full layout In memory, we can store these trees very similar to the full layout we had before, with the main differences that the first few layers are skipped and that now there will be padding at the end of each part, rather than once at the end.Figure 19: The full partitioned layout concatenates the full trees for all parts ‚Äòhorizontally‚Äô. As a new detail, when a part is not full, the smallest value of the next part is appended in the leaf layer.For some choices of \\(b\\), it could happen that up to \\(15/16\\) of each tree is padding. To reduce this overhead, we attempt to shrink \\(b\\) while keeping the height of all trees the same: as long as all pairs of adjacent trees would fit together in the same space, we decrease \\(b\\) by one. This way, all parts will be filled for at least \\(50\\%\\) when the elements are evenly distributed.Once construction is done, the code for querying is very similar to before: we only have to start the search for each query at the index of its part, given by q \u003e\u003e shift for some value of shift, rather than at index \\(0\\). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 pub fn search_prefix\u003cconst P: usize\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let offsets = self .offsets .iter() .map(|o| unsafe { self.tree.as_ptr().add(*o) }) .collect_vec(); // Initial parts, and prefetch them. let o0 = offsets[0]; - let mut k = [0; P]; + let mut k = qb.map(|q| { + (q as usize \u003e\u003e self.shift) * 64 + }); let q_simd = qb.map(|q| Simd::\u003cu32, 8\u003e::splat(q)); for [o, o2] in offsets.array_windows() { for i in 0..P { let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]); k[i] = k[i] * (B + 1) + jump_to; prefetch_ptr(unsafe { o2.byte_add(k[i]) }); } } let o = offsets.last().unwrap(); from_fn(|i| { let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]); unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() } }) } Code Snippet 29: Searching the full layout of the partitioned tree starts in the partition in which each query belongs.Figure 20: The ‚Äòsimple‚Äô partitioned tree, for (b_{textrm{max}}in {4,8,12,16,20}), shown as dotted lines.We see that indeed, the partitioned tree has a space overhead varying between \\(0\\) and \\(1\\), making this not yet useful in practice. Larger \\(b\\) reduce the height of the remaining trees, and indeed we see that queries are faster for larger \\(b\\). Especially for small trees there is a significant speedup over interleaving. Somewhat surprisingly, none of the partition sizes has faster queries than interleaving for large inputs. Also important to note is that while partitioning is very fast for sizes up to L1 cache, this is only possible because they have \\(\\gg 1\\) space overhead.5.2 Compact subtrees Just like we used the packed layout before, we can also do that now, by simply concatenating the representation of all packed subtrees. We ensure that all subtrees are still padded into the same total size, but now we only add as much padding as needed for the largest part, rather than padding to full trees. Then, we give each tree the same layout in memory.We‚Äôll have offsets \\(o_\\ell\\) of where each layer starts in the first tree, and we store the constant size of the trees. That way, we can easily index each layer of each part.Figure 21: Compared to before, Figure 19, the lowest level of each subtree now only takes 2 instead of 3 nodes.The code for querying does become slightly more complicated. Now, we must explicitly track the part that each query belongs to, and compute all indices based on the layer offset, the in-layer offset k[i], and the part offset. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 pub fn search\u003cconst P: usize\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let offsets = self .offsets .iter() .map(|o| unsafe { self.tree.as_ptr().add(*o) }) .collect_vec(); // Initial parts, and prefetch them. let o0 = offsets[0]; + let mut k: [usize; P] = [0; P]; + let parts: [usize; P] = qb.map(|q| { + // byte offset of the part. + (q as usize \u003e\u003e self.shift) * self.bpp * 64 + }); let q_simd = qb.map(|q| Simd::\u003cu32, 8\u003e::splat(q)); for [o, o2] in offsets.array_windows() { for i in 0..P { - let jump_to = unsafe { *o.byte_add( k[i]) }.find_splat64(q_simd[i]); + let jump_to = unsafe { *o.byte_add(parts[i] + k[i]) }.find_splat64(q_simd[i]); k[i] = k[i] * (B + 1) + jump_to; - prefetch_ptr(unsafe { o2.byte_add( k[i]) }); + prefetch_ptr(unsafe { o2.byte_add(parts[i] + k[i]) }); } } let o = offsets.last().unwrap(); from_fn(|i| { - let idx = unsafe { *o.byte_add( k[i]) }.find_splat(q_simd[i]); + let idx = unsafe { *o.byte_add(parts[i] + k[i]) }.find_splat(q_simd[i]); - unsafe { (o.byte_add( k[i]) as *const u32).add(idx).read() } + unsafe { (o.byte_add(parts[i] + k[i]) as *const u32).add(idx).read() } }) } Code Snippet 30: The indexing for the packed subtrees requires explicitly tracking the part of each query. This slows things down a bit.Figure 22: Compared to the the simple/full layout before (dark blue dots for (b=16)), the compact layout (e.g. red dots for (b=16)) consistently uses less memory, but is slightly slower.For fixed \\(b_{\\textrm{max}}\\), memory overhead of the compact layout is small as long as the input is sufficiently large and the trees have sufficiently many layers. Thus, this tree could be practical. Unfortunately though, querying them is slightly slower than before, because we must explicitly track the part of each query.5.3 The best of both: compact first level As we just saw, storing the trees one by one slows queries down, so we would like to avoid that. But on the other hand, the full layout can waste space.Here, we combine the two ideas. We would like to store the horizontal concatenation of the packed trees (each packed to the same size), but this is complicated, because then levels would have a non-constant branching factor. Instead, we can fully omit the last few (level 2) subtrees from each tree, and pad those subtrees that are present to full subtrees. This way, only the first level has a configurable branching factor \\(B_1\\), which we can simply store after construction is done.This layout takes slightly more space than before because the subtrees must be full, but the overhead should typically be on the order of \\(1/16\\), since (for uniform data) each tree will have \\(\\geq 9\\) subtrees, of which only the last is not full.Figure 23: We can also store the horizontal concatenation of all trees. Here, the number of subtrees can be fixed to be less than (B+1), and is (2) instead of (B+1=3). Although not shown, deeper layers must always be full and have a (B+1) branching factor. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 pub fn search_b1\u003cconst P: usize\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let offsets = self .offsets .iter() .map(|o| unsafe { self.tree.as_ptr().add(*o) }) .collect_vec(); let o0 = offsets[0]; let mut k: [usize; P] = qb.map(|q| { (q as usize \u003e\u003e self.shift) * 64 }); let q_simd = qb.map(|q| Simd::\u003cu32, 8\u003e::splat(q)); - for [o, o2] in offsets.array_windows() { + if let Some([o1, o2]) = offsets.array_windows().next() { for i in 0..P { let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]); - k[i] = k[i] * (B + 1) + jump_to; + k[i] = k[i] * self.b1 + jump_to; prefetch_ptr(unsafe { o2.byte_add(k[i]) }); } } - for [o, o2] in offsets .array_windows() { + for [o, o2] in offsets[1..].array_windows() { for i in 0..P { let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]); k[i] = k[i] * (B + 1) + jump_to; prefetch_ptr(unsafe { o2.byte_add(k[i]) }); } } let o = offsets.last().unwrap(); from_fn(|i| { let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]); unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() } }) } Code Snippet 31: Now, the code is simple again, in that we don't need to explicitly track part indices. All that changes is that we handle the first iteration of the for loop separately, and use branching factor self.b1 instead of B+1 there.Figure 24: When compressing the first level, space usage is very similar to the compact layout before, and query speed is as fast as the full layout before.5.4 Overlapping trees A drawback of all the above methods is that memory usage is heavily influenced by the largest part, since all parts must be at least as large. This is especially a problem when the distribution of part sizes is very skewed. We can avoid this by sharing storage between adjacent trees. Let \\(S_p\\) be the number of subtrees for each part \\(p\\), and \\(S_{max} = \\max_p S_p\\). Then, we can define the overlap \\(0\\leq v\\leq B\\), and append only \\(B_1 = S_{max}-v\\) new subtrees for each new part, rather than \\(S_{max}\\) as we did before. The values for each part are then simply appended where the previous part left off, unless that subtree is ‚Äòout-of-reach‚Äô for the current part, in which case first some padding is added. This way, consecutive parts can overlap and exchange memory, and we can somewhat ‚Äòbuffer‚Äô the effect of large parts.Figure 25: In this example, the third tree has (6) values in ([8, 12)) and requires (S_{max}=3) subtrees. We have an overlap of (v=1), so that for each additional tree, only (2) subtrees are added. We add padding elements in grey to ensure all elements are reachable from their own tree.When the overlap is \\(1\\), as in the example above, the nodes in the first layer each contain the maximum value of \\(B\\) subtrees. When the overlap is larger than \\(1\\), the nodes in the first layer would contain overlapping values. Instead, we store a single list of values, in which we can do unaligned reads to get the right slice of \\(B\\) values that we need. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 pub fn search\u003cconst P: usize, const PF: bool\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let offsets = self .offsets .iter() .map(|o| unsafe { self.tree.as_ptr().add(*o) }) .collect_vec(); let o0 = offsets[0]; let mut k: [usize; P] = qb.map(|q| { - (q as usize \u003e\u003e self.shift) * 4 * 16 + (q as usize \u003e\u003e self.shift) * 4 * (16 - self.overlap) }); let q_simd = qb.map(|q| Simd::\u003cu32, 8\u003e::splat(q)); if let Some([o1, o2]) = offsets.array_windows().next() { for i in 0..P { + // First level read may be unaligned. - let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]); + let jump_to = unsafe { o.byte_add(k[i]).read_unaligned() }.find_splat64(q_simd[i]); k[i] = k[i] * self.l1 + jump_to; prefetch_ptr(unsafe { o2.byte_add(k[i]) }); } } for [o, o2] in offsets[1..].array_windows() { for i in 0..P { let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]); k[i] = k[i] * (B + 1) + jump_to; prefetch_ptr(unsafe { o2.byte_add(k[i]) }); } } let o = offsets.last().unwrap(); from_fn(|i| { - let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]); + let idx = unsafe { o.byte_add(k[i]).read_unaligned() }.find_splat(q_simd[i]); unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() } }) } Code Snippet 32: Each part now contains \\(16-v\\) values, instead of the original 16. We use read_unaligned since we do not always read at 16-value boundaries anymore.Figure 26: Overlapping trees usually use less memory than the equivalent version with first-level compression, while being about as fast.5.5 Human data So far we‚Äôve been testing with uniform random data, where the largest part deviates form the mean size by around \\(\\sqrt n\\). Now, let‚Äôs look at some real data: k-mers of a human genome. DNA consists of ACGT characters that can be encoded as 2 bits, so each string of \\(k=16\\) characters defines a 32 bit integer5. We then look at the first \\(n\\) k-mers of the human genome, starting at chromosome 1.To give an idea, the plot below show for each k-mer of length \\(k=12\\) how often it occurs in the full human genome. In total, there are around 3G k-mers, and so the expected count for each k-mer is around 200. But instead, we see k-mers that occur over 2 million times! So if we were to partition on the first 24 bits, the size of the largest part is only around \\(2^{-10}\\) of the input, rather than \\(2^{-24}\\).The accumulated counts are shown in orange, where we also see a number of flat regions caused by underrepresented k-mers.Figure 27: A plot showing k-mer counts for all (4^{12} = 16M) $k=12$-mers of the human genome. On random data each k-mer would occur around 200 times, but here we see some k-mers occurring over 2 million times.Figure 28: Building the overlapping trees for k-mers of the human genome takes much more space, and even using only 16 parts regularly requires up to 50% overhead, making this data structure not quite practical.5.6 Prefix map We need a way to handle unbalanced partition sizes, instead of mapping everything linearly. We can do this by simply storing the full tree compactly as we did before, preceded by an array (in blue below) that points to the index of the first subtree containing elements of the part. Like for the overlapping trees before, the first layer is simply a list of the largest elements of all subtrees that can be indexed anywhere (potentially unaligned).Figure 29: The prefix map, in blue, stores (2^b) elements, that for each $b$-bit prefix stores the index of the first subtree that contains an element of that prefix.To answer a query, we first find its part, then read the block (16 elements) starting at the pointed-to element, and then proceed as usual from the sub-tree onward. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 pub fn search\u003cconst P: usize, const PF: bool\u003e(\u0026self, qb: \u0026[u32; P]) -\u003e [u32; P] { let offsets = self .offsets .iter() .map(|o| unsafe { self.tree.as_ptr().add(*o) }) .collect_vec(); let o0 = offsets[0]; let mut k: [usize; P] = qb.map(|q| { - 4 * (16 - self.overlap) * (q as usize \u003e\u003e self.shift) + unsafe { 4 * *self.prefix_map.get_unchecked(q as usize \u003e\u003e self.shift) } }); let q_simd = qb.map(|q| Simd::\u003cu32, 8\u003e::splat(q)); if let Some([o1, o2]) = offsets.array_windows().next() { for i in 0..P { let jump_to = unsafe { o.byte_add(k[i]).read_unaligned() }.find_splat64(q_simd[i]); k[i] = k[i] * self.l1 + jump_to; prefetch_ptr(unsafe { o2.byte_add(k[i]) }); } } for [o, o2] in offsets[1..].array_windows() { for i in 0..P { let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]); k[i] = k[i] * (B + 1) + jump_to; prefetch_ptr(unsafe { o2.byte_add(k[i]) }); } } let o = offsets.last().unwrap(); from_fn(|i| { let idx = unsafe { o.byte_add(k[i]).read_unaligned() }.find_splat(q_simd[i]); unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() } }) } Code Snippet 33: In code, the only thing that changes compared to the previous overlapping version is that instead of computing the start index linearly (and adapting the element layout accordingly), we use the prefix_map to jump directly to the right place in the packed tree representation.Figure 30: As long as there are more elements than parts and the tree has at least two layers, the space overhead of this representation is close to (1/16) again.Although memory usage is now similar to the unpartitioned version, queries for large inputs are slightly slower than those previous layouts due to the additional index required.We can also again do the interleaving queries. These are slightly faster for small inputs, and around as fast as interleaving was without the partitioning.Figure 31: Prefix-map index with interleaving queries on random data.On human data, we see that the partitioned index is a bit faster in L1 and L2, and consistently saves the time of roughly one layer in L3. For larger indices, performance is still very similar to not using partitioning at all.Figure 32: Prefix-map with interleaving on human data.5.7 Summary Figure 33: Summary of partitioning results. Overall, it seems that partitioning does not provide when we already interleave queries.6 Multi-threaded comparison Figure 34: When using 6 threads, runtime goes down from 27ns to 7ns. Given that the speedup is less than 4x, we are now bottlenecked by total RAM throughput, and indeed methods that are slower for a single thread also reach near-optimal throughput now.7 Conclusion All together, we went from 1150ns/query for binary search on 4GB input to 27ns for the optimized S-tree with interleaved queries, over 40x speedup! A large part of this improvement is due to batching queries and prefetching upcoming nodes. To get even higher throughput, interleaving queries at different levels helps to balance the CPU-bound part of the computation with the memory-bound part, so that we get a higher overall throughput. Using a 15 elements per node instead of 16 also improves throughput somewhat, but doubles the overhead of the data structure from 6.25% to 13.3%. For inputs that fit in L3 cache that‚Äôs fine and the speedup is worthwhile, while for larger inputs the speed is memory-bound anyway, so that there is no speedup while the additional memory requirements are somewhat large.We also looked into partitioning the data by prefix. While this does give some speedup, it turns out that on skewed input data, the benefits quickly diminish since the tree either requires a lot of buffer space, or else requires an additional lookup to map each part to its location in the first level of the tree. In the end, I‚Äôd say the additional complexity and dependency on the shape of the input data of partitioning is not worth the speedup compared to simply using interleaved queries directly.7.1 Future work 7.1.1 Branchy search All methods we considered are branchless and use the exact same number of iterations for each query. Especially in combination with partitioning, it may be possible to handle the few large parts independently from the usual smaller parts. That way we could answer most queries with slightly fewer iterations.On the other hand, the layers saved would mostly be the quick lookups near the root of the tree, and introducing branches to the code could possibly cause quite a bit of delay due to mispredictions.7.1.2 Interpolation search As we saw in the last plot above, total RAM throughput (rather than per-core throughput) becomes a bottleneck once we‚Äôre using multiple threads. Thus, the only way to improve total query throughput is to use strictly fewer RAM accesses per query. Prefix lookups won‚Äôt help, since they only replace the layers of the tree that would otherwise fit in the cache. Instead, we could use interpolation search (wikipedia), where the estimated position of a query \\(q\\) is linearly interpolated between known positions of surrounding elements. On random data, this only takes \\(O(\\lg \\lg n)\\) iterations, rather than \\(O(\\lg n)\\) for binary search, and could save some RAM accesses. On the other hand, when data is not random its worst case performance is \\(O(n)\\) rather than the statically bounded \\(O(\\lg n)\\).The PLA-index (Abrar and Medvedev 2024) also uses a single interpolation step in a precisely constructed piece wise linear approximation. The error after the approximation is determined by some global upper bound, so that the number of remaining search steps can be bounded as well.7.1.3 Packing data smaller Another option to use the RAM lookups more efficiently would be to pack values into 16 bits rather than the 32 bits we‚Äôve been using so far. Especially if we first do a 16 bit prefix lookup, we already know those bits anyway, so it would suffice to only compare the last 16 bits of the query and values. This increases the branching factor from 17 to 33, which reduces the number of layers of the tree by around 1.5 for inputs of 1GB.7.1.4 Returning indices in original data For various applications, it may be helpful to not only return the smallest value \\(\\geq q\\), but also the index in the original list of sorted values, for example when storing an array with additional data for each item.Since we use the S+ tree that stores all data in the bottom layer, this is mostly straightforward. The prefix map partitioned tree also natively supports this, while the other partitioned variants do not: they include buffer/padding elements in their bottom layer, and hence we would need to store and look up the position offset of each part separately.7.1.5 Range queries We could extend the current query methods to a version that return both the first value \\(\\geq q\\) and the first value \\(\u003eq\\), so that the range of positions corresponding to value \\(q\\) can be determined. In practice, the easiest way to do this is by simply doubling the queries into \\(q\\) and \\(q+1\\). This will cause some CPU overhead in the initial layers, but the query execution will remain branch-free. When \\(q\\) is not found or only occurs a few times, they will mostly fetch the same cache lines, so that memory is efficiently reused and the bandwidth can be used for other queries.In practice though, this seems only around 20% faster per individual query for 4GB input, so around 60% slower for a range than for a single query. For small inputs, the speedup is less, and sometimes querying ranges is even more than twice slower than individual random queries.References Abrar, Md. Hasin, and Paul Medvedev. 2024. ‚ÄúPla-Index: A K-Mer Index Exploiting Rank Curve Linearity.‚Äù Schloss Dagstuhl ‚Äì Leibniz-Zentrum f√ºr Informatik. https://doi.org/10.4230/LIPICS.WABI.2024.13.Khuong, Paul-Virak, and Pat Morin. 2017. ‚ÄúArray Layouts for Comparison-Based Searching.‚Äù Acm Journal of Experimental Algorithmics 22 (May): 1‚Äì39. https://doi.org/10.1145/3053370.",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\u003carticle\u003e\u003cheader\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan title=\"created\"\u003e\n\u003ctime datetime=\"2024-12-18T00:00:00+01:00\"\u003eDecember 2024\n\u003c/time\u003e\u003c/span\u003e\u003cspan\u003e\n60-minute read\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/header\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003eTable of Contents\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cspan\u003e1\u003c/span\u003e \u003ca href=\"#introduction\"\u003eIntroduction\u003c/a\u003e\u003cul\u003e\u003cli\u003e\u003cspan\u003e1.1\u003c/span\u003e \u003ca href=\"#problem-statement\"\u003eProblem statement\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e1.2\u003c/span\u003e \u003ca href=\"#recommended-reading\"\u003eRecommended reading\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e1.3\u003c/span\u003e \u003ca href=\"#binary-search-and-eytzinger-layout\"\u003eBinary search and Eytzinger layout\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e1.4\u003c/span\u003e \u003ca href=\"#hugepages\"\u003eHugepages\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e1.5\u003c/span\u003e \u003ca href=\"#a-note-on-benchmarking\"\u003eA note on benchmarking\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e1.6\u003c/span\u003e \u003ca href=\"#cache-lines\"\u003eCache lines\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e1.7\u003c/span\u003e \u003ca href=\"#s-trees-and-b-trees\"\u003eS-trees and B-trees\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e2\u003c/span\u003e \u003ca href=\"#optimizing-find\"\u003eOptimizing \u003ccode\u003efind\u003c/code\u003e\u003c/a\u003e\u003cul\u003e\u003cli\u003e\u003cspan\u003e2.1\u003c/span\u003e \u003ca href=\"#linear\"\u003eLinear\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e2.2\u003c/span\u003e \u003ca href=\"#auto-vectorization\"\u003eAuto-vectorization\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e2.3\u003c/span\u003e \u003ca href=\"#trailing-zeros\"\u003eTrailing zeros\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e2.4\u003c/span\u003e \u003ca href=\"#popcount\"\u003ePopcount\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e2.5\u003c/span\u003e \u003ca href=\"#manual-simd\"\u003eManual SIMD\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e3\u003c/span\u003e \u003ca href=\"#optimizing-the-search\"\u003eOptimizing the search\u003c/a\u003e\u003cul\u003e\u003cli\u003e\u003cspan\u003e3.1\u003c/span\u003e \u003ca href=\"#batching\"\u003eBatching\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e3.2\u003c/span\u003e \u003ca href=\"#prefetching\"\u003ePrefetching\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e3.3\u003c/span\u003e \u003ca href=\"#pointer-arithmetic\"\u003ePointer arithmetic\u003c/a\u003e\u003cul\u003e\u003cli\u003e\u003cspan\u003e3.3.1\u003c/span\u003e \u003ca href=\"#up-front-splat\"\u003eUp-front splat\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e3.3.2\u003c/span\u003e \u003ca href=\"#byte-based-pointers\"\u003eByte-based pointers\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e3.3.3\u003c/span\u003e \u003ca href=\"#the-final-version\"\u003eThe final version\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e3.4\u003c/span\u003e \u003ca href=\"#skip-prefetch\"\u003eSkip prefetch\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e3.5\u003c/span\u003e \u003ca href=\"#interleave\"\u003eInterleave\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e4\u003c/span\u003e \u003ca href=\"#optimizing-the-tree-layout\"\u003eOptimizing the tree layout\u003c/a\u003e\u003cul\u003e\u003cli\u003e\u003cspan\u003e4.1\u003c/span\u003e \u003ca href=\"#left-tree\"\u003eLeft-tree\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e4.2\u003c/span\u003e \u003ca href=\"#memory-layouts\"\u003eMemory layouts\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e4.3\u003c/span\u003e \u003ca href=\"#node-size-b-15\"\u003eNode size \\(B=15\\)\u003c/a\u003e\u003cul\u003e\u003cli\u003e\u003cspan\u003e4.3.1\u003c/span\u003e \u003ca href=\"#data-structure-size\"\u003eData structure size\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e4.4\u003c/span\u003e \u003ca href=\"#summary\"\u003eSummary\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e5\u003c/span\u003e \u003ca href=\"#prefix-partitioning\"\u003ePrefix partitioning\u003c/a\u003e\u003cul\u003e\u003cli\u003e\u003cspan\u003e5.1\u003c/span\u003e \u003ca href=\"#full-layout\"\u003eFull layout\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e5.2\u003c/span\u003e \u003ca href=\"#compact-subtrees\"\u003eCompact subtrees\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e5.3\u003c/span\u003e \u003ca href=\"#the-best-of-both-compact-first-level\"\u003eThe best of both: compact first level\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e5.4\u003c/span\u003e \u003ca href=\"#overlapping-trees\"\u003eOverlapping trees\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e5.5\u003c/span\u003e \u003ca href=\"#human-data\"\u003eHuman data\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e5.6\u003c/span\u003e \u003ca href=\"#prefix-map\"\u003ePrefix map\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e5.7\u003c/span\u003e \u003ca href=\"#prefix-summary\"\u003eSummary\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e6\u003c/span\u003e \u003ca href=\"#multi-threaded-comparison\"\u003eMulti-threaded comparison\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e7\u003c/span\u003e \u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003cul\u003e\u003cli\u003e\u003cspan\u003e7.1\u003c/span\u003e \u003ca href=\"#future-work\"\u003eFuture work\u003c/a\u003e\u003cul\u003e\u003cli\u003e\u003cspan\u003e7.1.1\u003c/span\u003e \u003ca href=\"#branchy-search\"\u003eBranchy search\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e7.1.2\u003c/span\u003e \u003ca href=\"#interpolation-search\"\u003eInterpolation search\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e7.1.3\u003c/span\u003e \u003ca href=\"#packing-data-smaller\"\u003ePacking data smaller\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e7.1.4\u003c/span\u003e \u003ca href=\"#returning-indices-in-original-data\"\u003eReturning indices in original data\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e7.1.5\u003c/span\u003e \u003ca href=\"#range-queries\"\u003eRange queries\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cp\u003eIn this post, we will implement a static search tree (S+ tree) for\nhigh-throughput searching of sorted data, as \u003ca href=\"https://en.algorithmica.org/hpc/data-structures/s-tree/\"\u003eintroduced\u003c/a\u003e on Algorithmica.\nWe‚Äôll mostly take the code presented there as a starting point, and optimize it\nto its limits. For a large part, I‚Äôm simply taking the ‚Äòfuture work‚Äô ideas of that post\nand implementing them. And then there will be a bunch of looking at assembly\ncode to shave off all the instructions we can.\nLastly, there will be one big addition to optimize throughput: \u003cem\u003ebatching\u003c/em\u003e.\u003c/p\u003e\u003cp\u003eAll \u003cstrong\u003esource code\u003c/strong\u003e, including benchmarks and plotting code, is at \u003ca href=\"https://github.com/RagnarGrootKoerkamp/suffix-array-searching/tree/master/static-search-tree\"\u003egithub:RagnarGrootKoerkamp/suffix-array-searching\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"introduction\"\u003e\u003cspan\u003e1\u003c/span\u003e Introduction\n\u003ca href=\"#introduction\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003ch2 id=\"problem-statement\"\u003e\u003cspan\u003e1.1\u003c/span\u003e Problem statement\n\u003ca href=\"#problem-statement\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003e\u003cstrong\u003eInput.\u003c/strong\u003e A sorted list of \\(n\\) 32bit unsigned integers \u003ccode\u003evals: Vec\u0026lt;u32\u0026gt;\u003c/code\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOutput.\u003c/strong\u003e A data structure that supports queries \\(q\\), returning the smallest\nelement of \u003ccode\u003evals\u003c/code\u003e that is at least \\(q\\), or \u003ccode\u003eu32::MAX\u003c/code\u003e if no such element exists.\nOptionally, the index of this element may also be returned.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eMetric.\u003c/strong\u003e We optimize \u003cem\u003ethroughput\u003c/em\u003e. That is, the number of (independent) queries\nthat can be answered per second. The typical case is where we have a\nsufficiently long \u003ccode\u003equeries: \u0026amp;[u32]\u003c/code\u003e as input, and return a corresponding \u003ccode\u003eanswers: Vec\u0026lt;u32\u0026gt;\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eNote that we‚Äôll usually report reciprocal throughput as \u003ccode\u003ens/query\u003c/code\u003e (or just\n\u003ccode\u003ens\u003c/code\u003e), instead of \u003ccode\u003equeries/s\u003c/code\u003e. You can think of this as amortized (not \u003cem\u003eaverage\u003c/em\u003e) time spent per query.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBenchmarking setup.\u003c/strong\u003e For now, we will assume that both the input and queries\nare simply uniform random sampled 31bit integers\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCode.\u003c/strong\u003e\nIn code, this can be modelled by the trait shown in \u003ca href=\"#code-snippet--trait\"\u003eCode Snippet 1\u003c/a\u003e.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e1\n\u003c/span\u003e\u003cspan\u003e2\n\u003c/span\u003e\u003cspan\u003e3\n\u003c/span\u003e\u003cspan\u003e4\n\u003c/span\u003e\u003cspan\u003e5\n\u003c/span\u003e\u003cspan\u003e6\n\u003c/span\u003e\u003cspan\u003e7\n\u003c/span\u003e\u003cspan\u003e8\n\u003c/span\u003e\u003cspan\u003e9\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003etrait\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eSearchIndex\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e/// Two functions with default implementations in terms of each other.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003equery_one\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003equery\u003c/span\u003e: \u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eu32\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003eSelf\u003c/span\u003e::\u003cspan\u003equery\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003evec!\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003equery\u003c/span\u003e\u003cspan\u003e])[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003equery\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003equeries\u003c/span\u003e: \u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e])\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eVec\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003equeries\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eiter\u003c/span\u003e\u003cspan\u003e().\u003c/span\u003e\u003cspan\u003emap\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e|\u0026amp;\u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e|\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eSelf\u003c/span\u003e::\u003cspan\u003equery_one\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e)).\u003c/span\u003e\u003cspan\u003ecollect\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--trait\"\u003eCode Snippet 1\u003c/a\u003e:\u003c/span\u003e\nTrait that our solution should implement.\u003c/p\u003e\u003ch2 id=\"recommended-reading\"\u003e\u003cspan\u003e1.2\u003c/span\u003e Recommended reading\n\u003ca href=\"#recommended-reading\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eThe classical solution to this problem is \u003cstrong\u003ebinary search\u003c/strong\u003e, which we will briefly\nvisit in the next section. A great paper on this and other search layouts is\n\u003ca href=\"#citeproc_bib_item_2\"\u003e‚ÄúArray Layouts for Comparison-Based Searching‚Äù\u003c/a\u003e by Khuong and Morin (\u003ca href=\"#citeproc_bib_item_2\"\u003e2017\u003c/a\u003e).\nAlgorithmica also has a \u003ca href=\"https://en.algorithmica.org/hpc/data-structures/binary-search/\"\u003ecase study\u003c/a\u003e based on that paper.\u003c/p\u003e\u003cp\u003eThis post will focus on \u003cstrong\u003eS+ trees\u003c/strong\u003e, as introduced on Algorithmica in the\nfollowup post, \u003ca href=\"https://en.algorithmica.org/hpc/data-structures/s-tree/\"\u003estatic B-trees\u003c/a\u003e. In the interest of my time, I will mostly assume\nthat you are familiar with that post.\u003c/p\u003e\u003cp\u003eI also recommend reading my work-in-progress \u003ca href=\"https://curiouscoding.nl/posts/cpu-benchmarks\"\u003eintroduction to CPU performance\u003c/a\u003e,\nwhich contains some benchmarks pushing the CPU to its limits. We will use the\nmetrics obtained there as baseline to understand our optimization attempts.\u003c/p\u003e\u003cp\u003eAlso helpful is the \u003ca href=\"https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#=undefined\u0026amp;techs=AVX_ALL\"\u003eIntel Intrinsics Guide\u003c/a\u003e when looking into SIMD instructions.\nNote that we‚Äôll only be using \u003ccode\u003eAVX2\u003c/code\u003e instructions here, as in, we‚Äôre assuming\nintel. And we‚Äôre not assuming less available \u003ccode\u003eAVX512\u003c/code\u003e instructions (in\nparticular, since my laptop doesn‚Äôt have them).\u003c/p\u003e\u003ch2 id=\"binary-search-and-eytzinger-layout\"\u003e\u003cspan\u003e1.3\u003c/span\u003e Binary search and Eytzinger layout\n\u003ca href=\"#binary-search-and-eytzinger-layout\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eAs a baseline, we will use the Rust standard library binary search implementation.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003estruct\u003c/span\u003e \u003cspan\u003eSortedVec\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003evals\u003c/span\u003e: \u003cspan\u003eVec\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eimpl\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eSortedVec\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003ebinary_search_std\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e: \u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eu32\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003evals\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebinary_search\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e).\u003c/span\u003e\u003cspan\u003eunwrap_or_else\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e|\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e|\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003evals\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--binary-search\"\u003eCode Snippet 2\u003c/a\u003e:\u003c/span\u003e\nThe binary search in the Rust standard library.\u003c/p\u003e\u003cp\u003eThe main conclusion of the array layouts paper (\u003ca href=\"#citeproc_bib_item_2\"\u003eKhuong and Morin 2017\u003c/a\u003e) is\nthat the Eytzinger layout is one of the best in practice.\nThis layout reorders the values in memory: the binary search effectively is a\nbinary search tree on the data, the root the middle node, then the nodes at\npositions \\(\\frac 14 n\\) and \\(\\frac 34 n\\), then \\(\\frac 18n, \\frac 38n, \\frac 58n,\n\\frac 78n\\), and so on. The main benefit of this layout is that all values needed\nfor the first steps of the binary search are close together, so they can be\ncached efficiently. If we put the root at index \\(1\\), the two children of the\nnode at index \\(i\\) are at \\(2i\\) and \\(2i+1\\). This means that we can effectively\nprefetch the next cache line, before knowing whether we need index \\(2i\\) or\n\\(2i+1\\). This can be taken a step further and we can prefetch the cache line\ncontaining indices \\(16i\\) to \\(16i+15\\), which are exactly the values needed 4\niterations from now.\nFor a large part, this can quite effectively hide the latency associated with\nthe traversal of the tree.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003cspan\u003e22\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003estruct\u003c/span\u003e \u003cspan\u003eEytzinger\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e/// The root of the tree is at index 1.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003evals\u003c/span\u003e: \u003cspan\u003eVec\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eimpl\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eEytzinger\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e/// L: number of levels ahead to prefetch.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003esearch_prefetch\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eL\u003c/span\u003e: \u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e: \u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eu32\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emut\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003ewhile\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026lt;\u0026lt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eL\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003evals\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eget\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eas\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003eprefetch_index\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003evals\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026lt;\u0026lt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eL\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// The last few iterations don\u0026#39;t need prefetching anymore.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003ewhile\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003evals\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eget\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eas\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ezeros\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003etrailing_ones\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ezeros\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eget\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--eytzinger\"\u003eCode Snippet 3\u003c/a\u003e:\u003c/span\u003e\nImplementation of searching the Eytzinger layout, with \\(L=4\\) levels of prefetching.\u003c/p\u003e\u003cp\u003eIf we plot these two, we see that Eytzinger layout performs as good as binary\nsearch when the array fits in L2 cache (\u003ccode\u003e256kB\u003c/code\u003e for me, the middle red line), but starts to be much\nbetter than binary search as the array grows to be much larger than the L3 cache (\u003ccode\u003e12MB\u003c/code\u003e).\nIn the end, Eytzinger search is around 4 times faster, which nicely corresponds\nto being able to prefetch 4 iterations of cache lines from memory at a time.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/1-binary-search.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/1-binary-search.svg\" alt=\"Figure 1: Query throughput of binary search and Eytzinger layout as the size of the input increases. At 1GB input, binary search needs around 1150ns/query, while Eytzinger is 6x faster at 200ns/query.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 1: \u003c/span\u003eQuery throughput of binary search and Eytzinger layout as the size of the input increases. At \u003ccode\u003e1GB\u003c/code\u003e input, binary search needs around \u003ccode\u003e1150ns/query\u003c/code\u003e, while Eytzinger is 6x faster at \u003ccode\u003e200ns/query\u003c/code\u003e.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"hugepages\"\u003e\u003cspan\u003e1.4\u003c/span\u003e Hugepages\n\u003ca href=\"#hugepages\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eFor all experiments, we‚Äôll make sure to allocate the tree using \u003ccode\u003e2MB\u003c/code\u003e \u003cem\u003ehugepages\u003c/em\u003e\nby default, instead of the usual \u003ccode\u003e4kB\u003c/code\u003e pages.\nThis reduces pressure on the \u003cem\u003etranslation lookaside buffer\u003c/em\u003e (TLB) that\ntranslates virtual memory addresses to hardware memory addresses, since its\ninternal table of pages is much smaller when using hugepages, and hence can be\ncached better.\u003c/p\u003e\u003cp\u003eWith \u003cem\u003etransparent hugepages\u003c/em\u003e enabled, they are automatically given out whenever\nallocating an exact multiple of \u003ccode\u003e2MB\u003c/code\u003e, and so we always round up the allocation\nfor the tree to the next multiple of \u003ccode\u003e2MB\u003c/code\u003e. However, it turns out that small\nallocations below \u003ccode\u003e32MB\u003c/code\u003e still go on the program‚Äôs \u003cem\u003eheap\u003c/em\u003e, rather than asking\nthe kernel for new memory pages, causing them to not actually be hugepages.\nThus, all allocations we do are actually rounded up to the next multiple of\n\u003ccode\u003e32MB\u003c/code\u003e instead.\u003c/p\u003e\u003cp\u003eAll together, hugepages sometimes makes a small difference when the dataset is\nindeed between \u003ccode\u003e1MB\u003c/code\u003e and \u003ccode\u003e32MB\u003c/code\u003e in size. Smaller data structures don‚Äôt really need\nhugepages anyway. Enabling them for the Eytzinger layout as in the plot above\nalso gives a significant speedup for larger sizes.\u003c/p\u003e\u003ch2 id=\"a-note-on-benchmarking\"\u003e\u003cspan\u003e1.5\u003c/span\u003e A note on benchmarking\n\u003ca href=\"#a-note-on-benchmarking\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eThe plots have the size of the input data on the logarithmic (bottom) x-axis. On the top,\nthey show the corresponding number of elements in the vector, which is 4 times\nless, since each element is a \u003ccode\u003eu32\u003c/code\u003e spanning 4 bytes.\nMeasurements are taken at values \\(2^i\\), \\(1.25 \\cdot 2^i\\), \\(1.5\\cdot 2^i\\), and\n\\(1.75\\cdot 2^i\\).\u003c/p\u003e\u003cp\u003eThe y-axis shows measured time per query. In the plot above, it says\n\u003cem\u003elatency\u003c/em\u003e, since it is benchmarked as \u003ccode\u003efor q in queries { index.query(q); }\u003c/code\u003e.\nEven then, the pipelining and out-of-order execution of the CPU will make it\nexecute multiple iterations in parallel. Specifically, while it is waiting for\nthe last cache lines of iteration \\(i\\), it can already start executing the first\ninstructions of the next query. To measure the true latency, we would have to\nintroduce a \u003cem\u003eloop carried dependency\u003c/em\u003e by making query \\(i+1\\) dependent on the\nresult of query \\(i\\).\nHowever, the main goal of this post is to optimize for \u003cem\u003ethroughput\u003c/em\u003e, so we won‚Äôt\nbother with that.\u003c/p\u003e\u003cp\u003eThus, all plots will show the throughput of doing \u003ccode\u003eindex.query(all_queries)\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eFor the benchmarks, I‚Äôm using my laptop‚Äôs \u003ccode\u003ei7-10750H\u003c/code\u003e CPU, with the frequency\nfixed to \u003ccode\u003e2.6GHz\u003c/code\u003e using \u003ca href=\"#code-snippet--pin\"\u003eCode Snippet 4\u003c/a\u003e.\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"sh\"\u003e\u003cspan\u003e\u003cspan\u003esudo cpupower frequency-set -g powersave -d 2.6GHz -u 2.6GHz\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--pin\"\u003eCode Snippet 4\u003c/a\u003e:\u003c/span\u003e\nPinning the CPU frequency to \u003ccode\u003e2.6GHz\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eAlso relevant are the sizes of the caches: \u003ccode\u003e32KiB\u003c/code\u003e L1 cache per core, \u003ccode\u003e256KiB\u003c/code\u003e\nL2 cache per core, and \u003ccode\u003e12MiB\u003c/code\u003e L3 cache shared between the physical 6 cores.\nFurthermore, hyper-threading is disabled.\u003c/p\u003e\u003cp\u003eAll measurements are done 5 times. The line follows the median, and we show the\nspread of the 2nd to 4th value (i.e., after discarding the minimum and maximum).\nObserve that in most of the plot above, the spread is barely visible! Thus,\nwhile especially the graph for binary search looks very noisy, that ‚Äônoise‚Äô is\nin fact completely reproducible. Indeed, it‚Äôs caused by effects of \u003cem\u003ecache\nassociativity\u003c/em\u003e, as explained in the array layouts paper\n(Khuong and Morin (\u003ca href=\"#citeproc_bib_item_2\"\u003e2017\u003c/a\u003e); this post is long enough already).\u003c/p\u003e\u003ch2 id=\"cache-lines\"\u003e\u003cspan\u003e1.6\u003c/span\u003e Cache lines\n\u003ca href=\"#cache-lines\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eMain memory and the caches work at the level of \u003cem\u003ecache lines\u003c/em\u003e consisting of 64\nbytes (at least on my machine), or 16 \u003ccode\u003eu32\u003c/code\u003e values. Thus, even if you only read a single byte, if\nthe cache line containing that byte is not yet in the L1 cache, the entire thing\nwill be fetched from RAM or L3 or L2 into L1.\u003c/p\u003e\u003cp\u003ePlain binary search typically only uses a single value of each cache line,\nuntil it gets to the end of the search where the last 16 values span just 1 or 2\ncache lines.\u003c/p\u003e\u003cp\u003eThey Eytzinger layout suffers the same problem: even though the next cache line\ncan be prefetched, it still only uses a single value in each.\nThis fundamentally means that both these search schemes are using the available\nmemory bandwidth quite inefficiently, and since most of what they are doing is\nwaiting for memory to come through, that‚Äôs not great.\nAlso, while that‚Äôs not relevant \u003cem\u003eyet\u003c/em\u003e, when doing this with many threads in\nparallel, or with batching, single-core RAM throughput and the throughput of the\nmain memory itself become a bottleneck.\u003c/p\u003e\u003cp\u003eIt would be much better if \u003cem\u003esomehow\u003c/em\u003e, we could use the information in each cache\nline much more efficiently ;)\u003c/p\u003e\u003cp\u003eWe can do that by storing our data in a different way. Instead of storing it\nlayer by layer, so that each iteration goes into a new layer,\nwe can store 4 layers of the tree at a time (\u003ca href=\"#code-snippet--node\"\u003eCode Snippet 5\u003c/a\u003e). That takes 15 values, and could\nnicely be padded into a full cache line. Then when we fetch a cache line, we can\nuse it for 4 iterations at once ‚Äì much better!\nOn the other hand, now we can‚Äôt prefetch upcoming cache lines in advance\nanymore, so that overall the latency will be the same. But we fetch up to 4\ntimes fewer cache lines overall, which should help throughput.\u003c/p\u003e\u003cp\u003eUnfortunately, I don‚Äôt have code and plots here, because what I really want to\nfocus on is the next bit.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/packed-eytzinger.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/packed-eytzinger.svg\" alt=\"Figure 2: The first two rows show how we could pack four layers of the Eytzinger search into a single cache line. The first follows a classic binary search layout, while the second applies the Eytzinger layout recursively. The third row shows an S-tree node instead. For simplicity and clarity, I‚Äôm using consecutive values, but in practice, this would be any list of sorted numbers.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 2: \u003c/span\u003eThe first two rows show how we could pack four layers of the Eytzinger search into a single cache line. The first follows a classic binary search layout, while the second applies the Eytzinger layout recursively. The third row shows an S-tree node instead. For simplicity and clarity, I‚Äôm using consecutive values, but in practice, this would be any list of sorted numbers.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"s-trees-and-b-trees\"\u003e\u003cspan\u003e1.7\u003c/span\u003e S-trees and B-trees\n\u003ca href=\"#s-trees-and-b-trees\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eWe just ended with a \u003cem\u003enode\u003c/em\u003e of 15 values that represent a height-4 search tree\nin which we can binary search. From there, it‚Äôs just a small step to S-trees.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eB-trees.\u003c/strong\u003e But first I have to briefly mention B-trees though (\u003ca href=\"https://en.wikipedia.org/wiki/B-tree\"\u003ewikipedia\u003c/a\u003e). Those are\nthe more classic dynamic variant, where nodes are linked together via pointers.\nAs wikipedia writes, they are typically used with much larger block sizes, for\nexample 4kB, since files read from disk usually come in 4kB chunks. Thus, they\nalso have much larger branching factors.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eS-trees.\u003c/strong\u003e But we will instead use S-trees, as named so by Algorithmica. They\nare a nice middle ground between the high branching factor of B-trees, and the\ncompactness of the Eytzinger layout.\nInstead of interpreting the 15 values as a search tree, we can also store them\nin a sorted way, and consider them as a 16-ary search tree: the 15 values simply\nsplit the data in the subtree into 16 parts, and we can do a linear scan to find\nwhich part to recurse into.\nBut if we store 15 values and one padding in a cache line, we might as well make\nit 16 values and have a branching factor of 17 instead.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eS+ trees.\u003c/strong\u003e B-trees and S-trees only store each value once, either in a leaf node or\nin an internal node. This turns out to be somewhat annoying, since we must track\nin which layer the result was found. To simplify this, we can store \u003cem\u003eall\u003c/em\u003e values\nas a leaf, and \u003cem\u003eduplicate\u003c/em\u003e them in the internal nodes. This is then called a B+\ntree or S+ tree. However, I will be lazy and just use S-tree to include this modification.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/full.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/full.svg\" alt=\"Figure 3: An example of a ‚Äòfull‚Äô S+ tree (that I will from now just call S-tree) on 18 values with nodes of size (B=2) and branching factor (B+1=3). Each internal node stores the smallest value in the subtree on its right. In memory, the layers are simply packed together behind each other.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 3: \u003c/span\u003eAn example of a ‚Äòfull‚Äô S+ tree (that I will from now just call S-tree) on 18 values with nodes of size (B=2) and branching factor (B+1=3). Each internal node stores the smallest value in the subtree on its right. In memory, the layers are simply packed together behind each other.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eA full S-tree can be navigated in a way similar to the Eytzinger layout: The\nnode (note: not\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e value) at index \\(i\\) has its \\(B+1\\) child-nodes at indices \\((B+1)\\cdot i + 1 + \\{0, \\dots, B\\}\\).\u003c/p\u003e\u003cp\u003eWhen the tree is only partially filled, the full layout can waste a lot of space\n(\u003ca href=\"#figure--stree-partial\"\u003eFigure 4\u003c/a\u003e). Instead, we can \u003cem\u003epack\u003c/em\u003e the layers together, by storing the\noffset \\(o_\\ell\\) of each layer.\u003c/p\u003e\u003cp\u003eThe children of node \\(o_\\ell + i\\) are then at \\(o_{\\ell+1} + (B+1)\\cdot i + \\{0, \\dots, B\\}\\).\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/partial.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/partial.svg\" alt=\"Figure 4: The full representation can be inefficient. The packed representation removes the empty space, and explicitly stores the offset (o_ell) where each layer starts.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 4: \u003c/span\u003eThe \u003cem\u003efull\u003c/em\u003e representation can be inefficient. The \u003cem\u003epacked\u003c/em\u003e representation removes the empty space, and explicitly stores the offset (o_ell) where each layer starts.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAt last, let‚Äôs have a look at some code. Each node in the tree is simply\nrepresented as a list of \\(N=16\\) \u003ccode\u003eu32\u003c/code\u003e values. We explicitly ask that nodes are\naligned to 64byte cache line boundaries.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e1\n\u003c/span\u003e\u003cspan\u003e2\n\u003c/span\u003e\u003cspan\u003e3\n\u003c/span\u003e\u003cspan\u003e4\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e#[repr(align(64))]\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003estruct\u003c/span\u003e \u003cspan\u003eTreeNode\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e: \u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003edata\u003c/span\u003e: \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e],\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--node\"\u003eCode Snippet 5\u003c/a\u003e:\u003c/span\u003e\nSearch tree node, aligned to a 64 byte cache line. For now, N is always 16. The values in a node must always be sorted.\u003c/p\u003e\u003cp\u003eThe S-tree itself is simply a list of nodes, and the offsets where each layer starts.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e1\n\u003c/span\u003e\u003cspan\u003e2\n\u003c/span\u003e\u003cspan\u003e3\n\u003c/span\u003e\u003cspan\u003e4\n\u003c/span\u003e\u003cspan\u003e5\n\u003c/span\u003e\u003cspan\u003e6\n\u003c/span\u003e\u003cspan\u003e7\n\u003c/span\u003e\u003cspan\u003e8\n\u003c/span\u003e\u003cspan\u003e9\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e/// N: #elements in a node, always 16.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e/// B: branching factor \u0026lt;= N+1. Typically 17.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003estruct\u003c/span\u003e \u003cspan\u003eSTree\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eB\u003c/span\u003e: \u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e: \u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e/// The list of tree nodes.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003etree\u003c/span\u003e: \u003cspan\u003eVec\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eTreeNode\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e/// The root is at index tree[offsets[0]].\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e/// It\u0026#39;s children start at tree[offsets[1]], and so on.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e: \u003cspan\u003eVec\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--stree\"\u003eCode Snippet 6\u003c/a\u003e:\u003c/span\u003e\nThe S-tree data structure. It depends on the number of values per node \\(B\\) (usually 16 but sometimes 15) and the size of each node \\(N\\) (always 16).\u003c/p\u003e\u003cp\u003eTo save some space, and focus on the interesting part (to me, at least), I will\nnot show any code for constructing S-trees. It‚Äôs a whole bunch of uninteresting\nfiddling with indices, and takes a lot of time to get right. Also, construction\nis not optimized at all currently. Anyway, find the code \u003ca href=\"https://github.com/RagnarGrootKoerkamp/suffix-array-searching/tree/master/static-search-tree/src\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eTODO: Reverse offsets.\u003c/p\u003e\u003cp\u003eWhat we \u003cem\u003ewill\u003c/em\u003e look at, is code for searching S-trees.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003esearch\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e: \u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efind\u003c/span\u003e: \u003cspan\u003eimpl\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eFn\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eTreeNode\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eu32\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emut\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003efor\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eo\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ein\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e..\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ejump_to\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efind\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003enode\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eB\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ejump_to\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eo\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elast\u003c/span\u003e\u003cspan\u003e().\u003c/span\u003e\u003cspan\u003eunwrap\u003c/span\u003e\u003cspan\u003e();\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e// node(i) returns tree[i] using unchecked indexing.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emut\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efind\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003enode\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e// get(i, j) returns tree[i].data[j] using unchecked indexing.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eget\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e/\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e%\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003eOur first step will be optimizing the \u003ccode\u003efind\u003c/code\u003e function.\u003c/p\u003e\u003ch2 id=\"optimizing-find\"\u003e\u003cspan\u003e2\u003c/span\u003e Optimizing \u003ccode\u003efind\u003c/code\u003e\n\u003ca href=\"#optimizing-find\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003ch2 id=\"linear\"\u003e\u003cspan\u003e2.1\u003c/span\u003e Linear\n\u003ca href=\"#linear\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eLet‚Äôs first precisely define what we want \u003ccode\u003efind\u003c/code\u003e to do:\nit‚Äôs input is a node with 16 sorted values and a query value \\(q\\), and it should return\nthe index of the first element that is at least \\(q\\).\u003c/p\u003e\u003cp\u003eSome simple code for this is \u003ca href=\"#code-snippet--find-linear\"\u003eCode Snippet 8\u003c/a\u003e.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e1\n\u003c/span\u003e\u003cspan\u003e2\n\u003c/span\u003e\u003cspan\u003e3\n\u003c/span\u003e\u003cspan\u003e4\n\u003c/span\u003e\u003cspan\u003e5\n\u003c/span\u003e\u003cspan\u003e6\n\u003c/span\u003e\u003cspan\u003e7\n\u003c/span\u003e\u003cspan\u003e8\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003efind_linear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e: \u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eusize\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003efor\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ein\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e..\u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003eif\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edata\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026gt;=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003ereturn\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--find-linear\"\u003eCode Snippet 8\u003c/a\u003e:\u003c/span\u003e\nA linear scan for the first element \\(\\geq q\\), that breaks as soon as it is found.\u003c/p\u003e\u003cp\u003eThe results are not very impressive yet.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/2-find-linear.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/2-find-linear.svg\" alt=\"Figure 5: The initial version of our S-tree search is quite a bit slower than the Eytzinger layout. In this and following plots, ‚Äòold‚Äô lines will be dimmed, and the best previous and best new line slightly highlighted. Colours will be consistent from one plot to the next.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 5: \u003c/span\u003eThe initial version of our S-tree search is quite a bit slower than the Eytzinger layout. In this and following plots, ‚Äòold‚Äô lines will be dimmed, and the best previous and best new line slightly highlighted. Colours will be consistent from one plot to the next.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"auto-vectorization\"\u003e\u003cspan\u003e2.2\u003c/span\u003e Auto-vectorization\n\u003ca href=\"#auto-vectorization\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eAs it turns out, the \u003ccode\u003ebreak;\u003c/code\u003e in \u003ca href=\"#code-snippet--find-linear\"\u003eCode Snippet 8\u003c/a\u003e is really bad for performance,\nsince the branch predictor can‚Äôt do a good job on it.\u003c/p\u003e\u003cp\u003eInstead, we can \u003cem\u003ecount\u003c/em\u003e the number of values less than \\(q\\), and return that as\nthe index of the first value \\(\\geq q\\). (Example: all values \\(\\geq q\\) index\ngives index 0.)\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e1\n\u003c/span\u003e\u003cspan\u003e2\n\u003c/span\u003e\u003cspan\u003e3\n\u003c/span\u003e\u003cspan\u003e4\n\u003c/span\u003e\u003cspan\u003e5\n\u003c/span\u003e\u003cspan\u003e6\n\u003c/span\u003e\u003cspan\u003e7\n\u003c/span\u003e\u003cspan\u003e8\n\u003c/span\u003e\u003cspan\u003e9\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003efind_linear_count\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e: \u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eusize\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emut\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ecount\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003efor\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ein\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e..\u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003eif\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edata\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003ecount\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003ecount\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--linear-count\"\u003eCode Snippet 9\u003c/a\u003e:\u003c/span\u003e\nCounting values \\(\u0026lt; q\\) instead of an early break. The \u003ccode\u003eif self.data[i] \u0026lt; q\u003c/code\u003e can be optimized into branchless code.\u003c/p\u003e\u003cp\u003eIn fact, the code is not just branchless, but actually it‚Äôs auto-vectorized into\nSIMD instructions!\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"asm\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evmovdqu\u003c/span\u003e      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rax\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%rcx\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e     \u003cspan\u003e; load data[..8]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evmovdqu\u003c/span\u003e      \u003cspan\u003e32\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rax\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%rcx\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003e%ymm2\u003c/span\u003e   \u003cspan\u003e; load data[8..]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpbroadcastd\u003c/span\u003e \u003cspan\u003e%xmm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e           \u003cspan\u003e; \u0026#39;splat\u0026#39; the query value\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpmaxud\u003c/span\u003e      \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm3\u003c/span\u003e    \u003cspan\u003e; v\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpcmpeqd\u003c/span\u003e     \u003cspan\u003e%ymm3\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm2\u003c/span\u003e    \u003cspan\u003e; v\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpmaxud\u003c/span\u003e      \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e    \u003cspan\u003e; v\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpcmpeqd\u003c/span\u003e     \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e    \u003cspan\u003e; 4x compare query with values\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpackssdw\u003c/span\u003e    \u003cspan\u003e%ymm2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e    \u003cspan\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpcmpeqd\u003c/span\u003e     \u003cspan\u003e%ymm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e    \u003cspan\u003e; v\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpxor\u003c/span\u003e        \u003cspan\u003e%ymm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e    \u003cspan\u003e; 2x negate result\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evextracti128\u003c/span\u003e \u003cspan\u003e$1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%xmm1\u003c/span\u003e       \u003cspan\u003e; v\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpacksswb\u003c/span\u003e    \u003cspan\u003e%xmm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%xmm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%xmm0\u003c/span\u003e    \u003cspan\u003e; v\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpshufd\u003c/span\u003e      \u003cspan\u003e$216\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%xmm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%xmm0\u003c/span\u003e     \u003cspan\u003e; v\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpmovmskb\u003c/span\u003e    \u003cspan\u003e%xmm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ecx\u003c/span\u003e            \u003cspan\u003e; 4x extract mask\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003epopcntl\u003c/span\u003e      \u003cspan\u003e%ecx\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ecx\u003c/span\u003e             \u003cspan\u003e; popcount the 16bit mask\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--linear-count-asm\"\u003eCode Snippet 10\u003c/a\u003e:\u003c/span\u003e\nCode Snippet \u003ca href=\"#org580fb2e\"\u003e9\u003c/a\u003e is auto-vectorized!\u003c/p\u003e\u003cp\u003eTo save some space: you can find this and further results for this section in\n\u003ca href=\"#figure--find-results\"\u003eFigure 34\u003c/a\u003e at the end of the section.\u003c/p\u003e\u003cp\u003eThis auto-vectorized version is over two times faster than the linear find,\nand now clearly beats Eytzinger layout!\u003c/p\u003e\u003ch2 id=\"trailing-zeros\"\u003e\u003cspan\u003e2.3\u003c/span\u003e Trailing zeros\n\u003ca href=\"#trailing-zeros\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eWe can also roll our own SIMD. The SIMD version of the original linear scan idea\ndoes 16 comparisons in parallel, converts that to a bitmask, and then counts the\nnumber of trailing zeros. Using \u003ccode\u003e#[feature(portable_simd)]\u003c/code\u003e, that looks like this:\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e1\n\u003c/span\u003e\u003cspan\u003e2\n\u003c/span\u003e\u003cspan\u003e3\n\u003c/span\u003e\u003cspan\u003e4\n\u003c/span\u003e\u003cspan\u003e5\n\u003c/span\u003e\u003cspan\u003e6\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003efind_ctz\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e: \u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eusize\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003edata\u003c/span\u003e: \u003cspan\u003eSimd\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eSimd\u003c/span\u003e::\u003cspan\u003efrom_slice\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edata\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e..\u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e]);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eSimd\u003c/span\u003e::\u003cspan\u003esplat\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emask\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003esimd_le\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edata\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003emask\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efirst_set\u003c/span\u003e\u003cspan\u003e().\u003c/span\u003e\u003cspan\u003eunwrap_or\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--find-ctz\"\u003eCode Snippet 11\u003c/a\u003e:\u003c/span\u003e\nA \u003ccode\u003efind\u003c/code\u003e implementation using the \u003ci\u003ecount-trailing-zeros\u003c/i\u003e instruction.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"asm\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evpminud\u003c/span\u003e      \u003cspan\u003e32\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsi\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r8\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e  \u003cspan\u003e; take min of data[8..] and query\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpcmpeqd\u003c/span\u003e     \u003cspan\u003e%ymm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e         \u003cspan\u003e; does the min equal query?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpminud\u003c/span\u003e      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsi\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r8\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm2\u003c/span\u003e    \u003cspan\u003e; take min of data[..8] and query\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpcmpeqd\u003c/span\u003e     \u003cspan\u003e%ymm2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm2\u003c/span\u003e         \u003cspan\u003e; does the min equal query?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpackssdw\u003c/span\u003e    \u003cspan\u003e%ymm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e         \u003cspan\u003e; pack the two results together, interleaved as 16bit words\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evextracti128\u003c/span\u003e \u003cspan\u003e$1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%xmm2\u003c/span\u003e            \u003cspan\u003e; extract half (both halves are equal)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpacksswb\u003c/span\u003e    \u003cspan\u003e%xmm2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%xmm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%xmm1\u003c/span\u003e         \u003cspan\u003e; go down to 8bit values, but weirdly shuffled\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpshufd\u003c/span\u003e      \u003cspan\u003e$216\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%xmm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%xmm1\u003c/span\u003e          \u003cspan\u003e; unshuffle\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evpmovmskb\u003c/span\u003e    \u003cspan\u003e%xmm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%r8d\u003c/span\u003e                 \u003cspan\u003e; extract the high bit of each 8bit value.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eorl\u003c/span\u003e          \u003cspan\u003e$65536\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r8d\u003c/span\u003e                 \u003cspan\u003e; set bit 16, to cover the unwrap_or(N)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003etzcntl\u003c/span\u003e       \u003cspan\u003e%r8d\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r15d\u003c/span\u003e                  \u003cspan\u003e; count trailing zeros\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eCode Snippet 12:\u003c/span\u003e\nAssembly code for Code Snippet \u003ca href=\"#orge6452ef\"\u003e11\u003c/a\u003e. Instead of ending with \u003ccode\u003epopcntl\u003c/code\u003e, this ends with \u003ccode\u003etzcntl\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eNow, let‚Äôs look at this generated code in a bit more detail.\u003c/p\u003e\u003cp\u003eFirst up: why does \u003ccode\u003esimd_le\u003c/code\u003e translate into \u003ccode\u003emin\u003c/code\u003e and \u003ccode\u003ecmpeq\u003c/code\u003e?\u003c/p\u003e\u003cp\u003eFrom checking the \u003ca href=\"https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#=undefined\u0026amp;techs=AVX_ALL\u0026amp;text=_mm256_cmp\"\u003eIntel Intrinsics Guide\u003c/a\u003e, we find out that there are only signed\ncomparisons, while our data is unsigned. For now, let‚Äôs just assume that all\nvalues fit in 31 bits and are at most \u003ccode\u003ei32::MAX\u003c/code\u003e. Then, we can transmute our input\nto \u003ccode\u003eSimd\u0026lt;i32, 8\u0026gt;\u003c/code\u003e without changing its meaning.\u003c/p\u003e\u003cdiv\u003e\u003cp\u003eAssumption\u003c/p\u003e\u003cdiv\u003e\u003cp\u003eBoth input values and queries are between \u003ccode\u003e0\u003c/code\u003e and \u003ccode\u003ei32::MAX\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eEventually we can fix this by either taking \u003ccode\u003ei32\u003c/code\u003e input directly, or by shifting\n\u003ccode\u003eu32\u003c/code\u003e values to fit in the \u003ccode\u003ei32\u003c/code\u003e range.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn find_ctz_signed(\u0026amp;self, q: u32) -\u0026gt; usize\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e where\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     LaneCount\u0026lt;N\u0026gt;: SupportedLaneCount,\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-    let data: Simd\u0026lt;u32, N\u0026gt; = Simd::from_slice(                   \u0026amp;self.data[0..N]   );\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+    let data: Simd\u0026lt;i32, N\u0026gt; = Simd::from_slice(unsafe { transmute(\u0026amp;self.data[0..N]) });\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e-    let q = Simd::splat(q       );\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+    let q = Simd::splat(q as i32);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e     let mask = q.simd_le(data);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     mask.first_set().unwrap_or(N)\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--ctz-signed\"\u003eCode Snippet 13\u003c/a\u003e:\u003c/span\u003e\nSame as before, but now using \u003ccode\u003ei32\u003c/code\u003e values instead of \u003ccode\u003eu32\u003c/code\u003e.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-vpminud      32(%rsi,%r8), %ymm0, %ymm1\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-vpcmpeqd     %ymm1, %ymm0, %ymm1\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+vpcmpgtd     32(%rsi,%rdi), %ymm1, %ymm2 ; is query(%ymm1) \u0026gt; data[8..]?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e-vpminud      (%rsi,%r8), %ymm0, %ymm2\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-vpcmpeqd     %ymm2, %ymm0, %ymm2\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+vpcmpgtd     (%rsi,%rdi), %ymm1, %ymm1   ; is query(%ymm1) \u0026gt; data[..8]?\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e vpackssdw    %ymm2, %ymm1, %ymm1         ; pack results\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+vpxor        %ymm0, %ymm1, %ymm1         ; negate results (ymm0 is all-ones)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e vextracti128 $1, %ymm1, %xmm2            ; extract u16x16\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e vpacksswb    %xmm2, %xmm1, %xmm1         ; shuffle\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e vpshufd      $216, %xmm1, %xmm1          ; extract u8x16\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e vpmovmskb    %xmm1, %edi                 ; extract u16 mask\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e orl          $65536,%edi                 ; add bit to get 16 when none set\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e tzcntl       %edi,%edi                   ; count trailing zeros\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--ctz-signed-asm\"\u003eCode Snippet 14\u003c/a\u003e:\u003c/span\u003e\nThe two \u003ccode\u003evpminud\u003c/code\u003e and \u003ccode\u003evpcmpeqd\u003c/code\u003e instructions are gone now and merged into \u003ccode\u003evpcmpgtd\u003c/code\u003e, but instead we got a \u003ccode\u003evpxor\u003c/code\u003e back :/ (Ignore the different registers being used in the old versus the new version.)\u003c/p\u003e\u003cp\u003eIt turns out there is only a \u003ccode\u003e\u0026gt;\u003c/code\u003e instruction in SIMD, and not \u003ccode\u003e\u0026gt;=\u003c/code\u003e, and so there\nis no way to avoid inverting the result.\u003c/p\u003e\u003cp\u003eWe also see a \u003ccode\u003evpshufd\u003c/code\u003e instruction that feels \u003cem\u003every\u003c/em\u003e out of place. What‚Äôs\nhappening is that while packing the result of the 16 \u003ccode\u003eu32\u003c/code\u003e comparisons down to a\nsingle 16bit value, data is interleaved in an unfortunate way, and we need to\nfix that.\nHere, Algorithmica takes the approach of ‚Äòpre-shuffling‚Äô the values in each\nnode to counter for the unshuffle instruction.\nThey also suggest using \u003ccode\u003epopcount\u003c/code\u003e instead, which is indeed what we‚Äôll do next.\u003c/p\u003e\u003ch2 id=\"popcount\"\u003e\u003cspan\u003e2.4\u003c/span\u003e Popcount\n\u003ca href=\"#popcount\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eAs we saw, the drawback of the trailing zero count approach is that the order of\nthe lanes must be preserved. Instead, we‚Äôll now simply count the number of lanes\nwith a value less than the query, similar to the auto-vectorized SIMD before,\nso that the order of lanes doesn‚Äôt matter.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn find_popcnt_portable(\u0026amp;self, q: u32) -\u0026gt; usize\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e where\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     LaneCount\u0026lt;N\u0026gt;: SupportedLaneCount,\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let data: Simd\u0026lt;i32, N\u0026gt; = Simd::from_slice(unsafe { transmute(\u0026amp;self.data[0..N]) });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let q = Simd::splat(q as i32);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-    let mask = q.simd_le(data);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+    let mask = q.simd_gt(data);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e-    mask.first_set().unwrap_or(N)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+    mask.to_bitmask().count_ones() as usize\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--popcount-1\"\u003eCode Snippet 15\u003c/a\u003e:\u003c/span\u003e\nUsing popcount instead of trailing zeros.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e vpcmpgtd     32(%rsi,%rdi), %ymm0, %ymm1\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e vpcmpgtd     (%rsi,%rdi), %ymm0, %ymm0\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e vpackssdw    %ymm1, %ymm0, %ymm0     ; 1\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-vpxor        %ymm0, %ymm1, %ymm1\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e vextracti128 $1, %ymm0, %xmm1        ; 2\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e vpacksswb    %xmm1, %xmm0, %xmm0     ; 3\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e vpshufd      $216, %xmm0, %xmm0      ; 4\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e vpmovmskb    %xmm0, %edi             ; 5\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-orl          $65536,%edi\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+popcntl      %edi, %edi\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--popcount-1-asm\"\u003eCode Snippet 16\u003c/a\u003e:\u003c/span\u003e\nthe \u003ccode\u003exor\u003c/code\u003e and \u003ccode\u003eor\u003c/code\u003e instructions are gone, but we are still stuck with the sequence of 5 instructions to go from the comparison results to an integer bitmask.\u003c/p\u003e\u003cp\u003eIdeally we would like to \u003ccode\u003emovmsk\u003c/code\u003e directly on the \u003ccode\u003eu16x16\u003c/code\u003e output of the first\npack instruction, \u003ccode\u003evpackssdw\u003c/code\u003e, to get the highest bit of each of the 16 16-bit values.\nUnfortunately, we are again let down by AVX2: there are \u003ccode\u003emovemask\u003c/code\u003e \u003ca href=\"https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#=undefined\u0026amp;techs=AVX_ALL\u0026amp;text=movms\"\u003einstructions\u003c/a\u003e\nfor \u003ccode\u003eu8\u003c/code\u003e, \u003ccode\u003eu32\u003c/code\u003e, and \u003ccode\u003eu64\u003c/code\u003e, but not for \u003ccode\u003eu16\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eAlso, the \u003ccode\u003evpshufd\u003c/code\u003e instruction is now provably useless, so it‚Äôs slightly\ndisappointing the compiler didn‚Äôt elide it. Time to write the SIMD by hand instead.\u003c/p\u003e\u003ch2 id=\"manual-simd\"\u003e\u003cspan\u003e2.5\u003c/span\u003e Manual SIMD\n\u003ca href=\"#manual-simd\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eAs it turns out, we can get away without most of the packing!\nInstead of using \u003ccode\u003evpmovmskb\u003c/code\u003e (\u003ccode\u003e_mm256_movemask_epi8\u003c/code\u003e) on 8bit data, we can\nactually just use it directly on the 16bit output of \u003ccode\u003evpackssdw\u003c/code\u003e!\nSince the comparison sets each lane to all-zeros or all-ones, we can safely read\nthe most significant \u003cem\u003eand\u003c/em\u003e middle bit, and divide the count by two at the\nend.\u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003efind_popcnt\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e: \u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eusize\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e// We explicitly require that N is 16.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003elow\u003c/span\u003e: \u003cspan\u003eSimd\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e8\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eSimd\u003c/span\u003e::\u003cspan\u003efrom_slice\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edata\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e..\u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e/\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e]);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ehigh\u003c/span\u003e: \u003cspan\u003eSimd\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e8\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eSimd\u003c/span\u003e::\u003cspan\u003efrom_slice\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edata\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e/\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e..\u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e]);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq_simd\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eSimd\u003c/span\u003e::\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e_\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e8\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e::\u003cspan\u003esplat\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eas\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ei32\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003eunsafe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003euse\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003estd\u003c/span\u003e::\u003cspan\u003emem\u003c/span\u003e::\u003cspan\u003etransmute\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eas\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003et\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// Transmute from u32 to i32.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emask_low\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq_simd\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003esimd_gt\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003et\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elow\u003c/span\u003e\u003cspan\u003e));\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emask_high\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eq_simd\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003esimd_gt\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003et\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ehigh\u003c/span\u003e\u003cspan\u003e));\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// Transmute from portable_simd to __m256i intrinsic types.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emerged\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e_mm256_packs_epi32\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003et\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emask_low\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003et\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emask_high\u003c/span\u003e\u003cspan\u003e));\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// 32 bits is sufficient to hold a count of 2 per lane.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emask\u003c/span\u003e: \u003cspan\u003ei32\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e_mm256_movemask_epi8\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003et\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emerged\u003c/span\u003e\u003cspan\u003e));\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003emask\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ecount_ones\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eas\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e/\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--popcount\"\u003eCode Snippet 17\u003c/a\u003e:\u003c/span\u003e\nManual version of the SIMD code, by explicitly using the intrinsics. This is kinda ugly now, and there\u0026#39;s a lot of transmuting (casting) going on between \u003ccode\u003e[u32; 8]\u003c/code\u003e, \u003ccode\u003eSimd\u0026lt;u32, 8\u0026gt;\u003c/code\u003e and the native \u003ccode\u003e__m256i\u003c/code\u003e type, but we\u0026#39;ll have to live with it.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e1\n\u003c/span\u003e\u003cspan\u003e2\n\u003c/span\u003e\u003cspan\u003e3\n\u003c/span\u003e\u003cspan\u003e4\n\u003c/span\u003e\u003cspan\u003e5\n\u003c/span\u003e\u003cspan\u003e6\n\u003c/span\u003e\u003cspan\u003e7\n\u003c/span\u003e\u003cspan\u003e8\n\u003c/span\u003e\u003cspan\u003e9\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e vpcmpgtd     (%rsi,%rdi), %ymm0, %ymm1\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e vpcmpgtd     32(%rsi,%rdi), %ymm0, %ymm0\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e vpackssdw    %ymm0, %ymm1, %ymm0\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-vextracti128 $1, %ymm0, %xmm1\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-vpacksswb    %xmm1, %xmm0, %xmm0\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-vpshufd      $216, %xmm0, %xmm0\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-vpmovmskb    %xmm0, %edi\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+vpmovmskb    %ymm0, %edi\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e popcntl      %edi, %edi\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--popcount-asm\"\u003eCode Snippet 18\u003c/a\u003e:\u003c/span\u003e\nOnly 5 instructions total are left now. Note that there is no explicit division by 2, since this is absorbed into the pointer arithmetic in the remainder, after the function is inlined.\u003c/p\u003e\u003cp\u003eNow let‚Äôs have a look at the results of all this work.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/3-find.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/3-find.svg\" alt=\"Figure 6: Using the S-tree with an optimized find function improves throughput from 240ns/query for Eytzinger to 140ns/query for the auto-vectorized one, and down to 115ns/query for the final hand-optimized version, which is over 2x speedup!\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 6: \u003c/span\u003eUsing the S-tree with an optimized \u003ccode\u003efind\u003c/code\u003e function improves throughput from \u003ccode\u003e240ns/query\u003c/code\u003e for Eytzinger to \u003ccode\u003e140ns/query\u003c/code\u003e for the auto-vectorized one, and down to \u003ccode\u003e115ns/query\u003c/code\u003e for the final hand-optimized version, which is over 2x speedup!\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAs can be seen very nicely in this plot, each single instruction that we remove\ngives a small but consistent improvement in throughput. The biggest improvement\ncomes from the last step, where we indeed shaved off 3 instructions.\u003c/p\u003e\u003cp\u003eIn fact, we can analyse this plot a bit more:\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor input up to \\(2^6=64\\) bytes, the performance is constant, since in this\ncase the ‚Äòsearch tree‚Äô only consists of the root node.\u003c/li\u003e\u003cli\u003eUp to input of size \\(2^{10}\\), the thee has two layers, and the performance is constant.\u003c/li\u003e\u003cli\u003eSimilarly, we see the latency jumping up at size \\(2^{14}\\), \\(2^{18}\\), \\(2^{22}\\)\nand \\(2^{26}\\), each time because a new layer is added to the tree. (Or rather,\nthe jumps are at powers of the branching factor \\(B+1=17\\) instead of \\(2^4=16\\), but you get the idea.)\u003c/li\u003e\u003cli\u003eIn a way, we can also (handwaivily) interpret the x-axis as time: each time\nthe graph jumps up, the height of the jump is pretty much the time spent on\nprocessing that one extra layer of the tree.\u003c/li\u003e\u003cli\u003eOnce we exceed the size of L3 cache, things slow down quickly. At that\npoint, each extra layer of the tree adds a significant amount of time, since\nwaiting for RAM is inherently slow.\u003c/li\u003e\u003cli\u003eOn the other hand, once we hit RAM, the slowdown is more smooth rather than\nstepwise. This is because L3 is still able to cache a fraction of the\ndata structure, and that fraction only decreases slowly.\u003c/li\u003e\u003cli\u003eAgain handwavily, we can also interpret the x-axis as a snapshot of space\nusage at a fixed moment in time: the first three layers of the tree fit in L1.\nThe 4th and 5th layers fit in L2 and L3. Once the three is 6 layers deep, the\nreads of that layer will mostly hit RAM, and any additional layers for sure\nare going to RAM.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFrom now on, this last version, \u003ccode\u003efind_popcnt\u003c/code\u003e, is the one we will be using.\u003c/p\u003e\u003ch2 id=\"optimizing-the-search\"\u003e\u003cspan\u003e3\u003c/span\u003e Optimizing the search\n\u003ca href=\"#optimizing-the-search\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003ch2 id=\"batching\"\u003e\u003cspan\u003e3.1\u003c/span\u003e Batching\n\u003ca href=\"#batching\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eAs promised, the first improvement we‚Äôll make is \u003cem\u003ebatching\u003c/em\u003e.\nInstead of processing one query at a time, we can process multiple (many) queries\nat once. This allows the CPU to work on multiple queries at the same time, and\nin particular, it can have multiple (up to 10-12) in-progress requests to RAM at\na time. That way, instead of waiting for a latency of 80ns per read, we\neffectively wait for 10 reads at the same time, lowering the amortized wait time\nto around 8ns.\u003c/p\u003e\u003cp\u003eBatching very much benefits from the fact that we use an S+ tree instead of\nS-tree, since each element is find in the last layer (at the same depth), and\nhence the number of seach steps through the tree is the same for every element\nin the batch.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003ebatch\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eP\u003c/span\u003e: \u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eqb\u003c/span\u003e: \u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eP\u003c/span\u003e\u003cspan\u003e])\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eP\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emut\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eP\u003c/span\u003e\u003cspan\u003e];\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003efor\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eo\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e_o2\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ein\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003earray_windows\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003efor\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ein\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e..\u003c/span\u003e\u003cspan\u003eP\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ejump_to\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003enode\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]).\u003c/span\u003e\u003cspan\u003efind\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqb\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eB\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ejump_to\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eo\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elast\u003c/span\u003e\u003cspan\u003e().\u003c/span\u003e\u003cspan\u003eunwrap\u003c/span\u003e\u003cspan\u003e();\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003efrom_fn\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e|\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e|\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003enode\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]).\u003c/span\u003e\u003cspan\u003efind\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqb\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eget\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e/\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e%\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eN\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e})\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--batch\"\u003eCode Snippet 19\u003c/a\u003e:\u003c/span\u003e\nThe batching code is very similar to processing one query at a time. We just insert an additional loop over the batch of \\(P\\) items.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/4-batching.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/4-batching.svg\" alt=\"Figure 7: Batch size 1 (red) performs very similar to our non-batched version (blue), around 115ns/query. Increasing the batch size to 2, 4, and 8 each time significantly improves performance, until it saturates at 45ns/query (2.5x faster) around 16.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 7: \u003c/span\u003eBatch size 1 (red) performs very similar to our non-batched version (blue), around \u003ccode\u003e115ns/query\u003c/code\u003e. Increasing the batch size to 2, 4, and 8 each time significantly improves performance, until it saturates at \u003ccode\u003e45ns/query\u003c/code\u003e (2.5x faster) around 16.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eOne interesting observation is that going from batch size 1 to 2 does \u003cem\u003enot\u003c/em\u003e\ndouble the performance. I suspect this is because the CPU‚Äôs out-of-order\nexecution was already deep enough to effectively execute (almost) 2 queries in\nparallel anyway. Going to a batch size of 4 and then 8 does provide a\nsignificant speedup. Again going to 4 the speedup is relatively a bit less than\nwhen going to 8, so probably even with batch size 4 the CPU is somewhat looking\nahead into the next batch of 4 already ü§Ø.\u003c/p\u003e\u003cp\u003eThroughput saturates at batch size 16 (or really, around 12 already), which\ncorresponds to the CPU having 12 \u003cem\u003eline fill buffers\u003c/em\u003e and thus being able to\nread up to 12 cache lines in parallel.\u003c/p\u003e\u003cp\u003eNevertheless, we will settle on a batch size of 128, mostly because it leads to\nslightly cleaner plots in the remainder. It is also every so slightly faster,\nprobably because the constant overhead of initializing a batch is smaller when\nbatches are larger.\u003c/p\u003e\u003ch2 id=\"prefetching\"\u003e\u003cspan\u003e3.2\u003c/span\u003e Prefetching\n\u003ca href=\"#prefetching\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eThe CPU is already fetching multiple reads in parallel using out-of-order\nexecution, but we can also help out a bit by doing this explicitly using \u003cem\u003eprefetching\u003c/em\u003e.\nAfter processing a node, we determine the child node \u003ccode\u003ek\u003c/code\u003e that we need to visit\nnext, so we can directly request that node to be read from memory before\ncontinuing with the rest of the batch.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e fn batch\u0026lt;const P: usize\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let mut k = [0; P];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     for [o, o2] in self.offsets.array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             let jump_to = self.node(o + k[i]).find(qb[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             k[i] = k[i] * (B + 1) + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+            prefetch_index(\u0026amp;self.tree, o2 + k[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o = self.offsets.last().unwrap();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let idx = self.node(o + k[i]).find(qb[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         self.get(o + k[i] + idx / N, idx % N)\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--prefetch\"\u003eCode Snippet 20\u003c/a\u003e:\u003c/span\u003e\nPrefetching the cache line/node for the next iteration ahead.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/5-prefetch.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/5-prefetch.svg\" alt=\"Figure 8: Prefetching helps speeding things up once the data does not fit in L2 cache anymore, and gets us down from 45ns/query to 30ns/query for 1GB input.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 8: \u003c/span\u003ePrefetching helps speeding things up once the data does not fit in L2 cache anymore, and gets us down from \u003ccode\u003e45ns/query\u003c/code\u003e to \u003ccode\u003e30ns/query\u003c/code\u003e for \u003ccode\u003e1GB\u003c/code\u003e input.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe observe a few things: first prefetching slightly slow things down while data\nfits in L1 already, since in that case the instruction just doesn‚Äôt do anything anyway.\nIn L2, it makes the graph slightly more flat, indicating that already there, the\nlatency is already a little bit of a bottleneck.\nIn L3 this effect gets larger, and we get a nice smooth/horizontal graph, until\nwe hit RAM size. There, prefetching provides the biggest gains.\u003c/p\u003e\u003ch2 id=\"pointer-arithmetic\"\u003e\u003cspan\u003e3.3\u003c/span\u003e Pointer arithmetic\n\u003ca href=\"#pointer-arithmetic\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eAgain, it‚Äôs time to look at some assembly code, now to optimize the search\nfunction itself. Results are down below in \u003ca href=\"#figure--pointer-arithmetic\"\u003eFigure 9\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"up-front-splat\"\u003e\u003cspan\u003e3.3.1\u003c/span\u003e Up-front splat\n\u003ca href=\"#up-front-splat\"\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eFirst, we can note that the \u003ccode\u003efind\u003c/code\u003e function \u003ccode\u003esplat\u003c/code\u003e‚Äôs the query from a \u003ccode\u003eu32\u003c/code\u003e to\na \u003ccode\u003eSimd\u0026lt;u32, 8\u0026gt;\u003c/code\u003e on each call. It‚Äôs slightly nicer (but not really faster,\nactually) to splat all the queries\nup-front, and then reuse those.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn batch_splat\u0026lt;const P: usize\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let mut k = [0; P];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+    let q_simd = qb.map(|q| Simd::\u0026lt;u32, 8\u0026gt;::splat(q));\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     for [o, o2] in self.offsets.array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-            let jump_to = self.node(o + k[i]).find      (qb[i]    );\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            let jump_to = self.node(o + k[i]).find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e             k[i] = k[i] * (B + 1) + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             prefetch_index(\u0026amp;self.tree, o2 + k[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o = self.offsets.last().unwrap();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-        let idx = self.node(o + k[i]).find      (qb[i]    );\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        let idx = self.node(o + k[i]).find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e         self.get(o + k[i] + idx / N, idx % N)\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--splat\"\u003eCode Snippet 21\u003c/a\u003e:\u003c/span\u003e\n\u003ci\u003eHoisting\u003c/i\u003e the \u003ccode\u003esplat\u003c/code\u003e out of the \u003ci\u003eloop\u003c/i\u003e is slightly nicer, but not faster.\u003c/p\u003e\u003cp\u003eThe assembly code for each iteration of the first loop now looks like this:\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"asm\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emovq\u003c/span\u003e         \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsp\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r11\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e%r15\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eleaq\u003c/span\u003e         \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%r9\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r15\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e%r12\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eshlq\u003c/span\u003e         \u003cspan\u003e$6\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%r12\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evmovdqa\u003c/span\u003e      \u003cspan\u003e1536\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsp\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r11\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e4\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e%ymm0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evpcmpgtd\u003c/span\u003e     \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsi\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r12\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evpcmpgtd\u003c/span\u003e     \u003cspan\u003e32\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsi\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r12\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evpackssdw\u003c/span\u003e    \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evpmovmskb\u003c/span\u003e    \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%r12d\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003epopcntl\u003c/span\u003e      \u003cspan\u003e%r12d\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%r12d\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eshrl\u003c/span\u003e         \u003cspan\u003e%r12d\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emovq\u003c/span\u003e         \u003cspan\u003e%r15\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r13\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eshlq\u003c/span\u003e         \u003cspan\u003e$4\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%r13\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eaddq\u003c/span\u003e         \u003cspan\u003e%r15\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r13\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eaddq\u003c/span\u003e         \u003cspan\u003e%r12\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r13\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emovq\u003c/span\u003e         \u003cspan\u003e%r13\u003c/span\u003e\u003cspan\u003e,(\u003c/span\u003e\u003cspan\u003e%rsp\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r11\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eshlq\u003c/span\u003e         \u003cspan\u003e$6\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%r13\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eprefetcht0\u003c/span\u003e   \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%r10\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r13\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eCode Snippet 22:\u003c/span\u003e\nAssembly code for each iteration of Code Snippet \u003ca href=\"#orgaceea1d\"\u003e21\u003c/a\u003e. (Actually it\u0026#39;s unrolled into two copied of this, but they\u0026#39;re identical.)\u003c/p\u003e\u003ch3 id=\"byte-based-pointers\"\u003e\u003cspan\u003e3.3.2\u003c/span\u003e Byte-based pointers\n\u003ca href=\"#byte-based-pointers\"\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eLooking at the code above, we see two \u003ccode\u003eshlq $6\u003c/code\u003e instructions that multiply the\ngiven value by \\(64\\). That‚Äôs because our tree nodes are 64 bytes large, and\nhence, to get the \\(i\\)‚Äôth element of the array, we need to read at byte \\(64\\cdot\ni\\). For smaller element sizes, there are dedicated read instructions that\ninline, say, an index multiplication by 8. But for a stride of 64, the compiler\nhas to generate ‚Äòmanual‚Äô multiplications in the form of a shift.\u003c/p\u003e\u003cp\u003eAdditionally, direct pointer-based lookups can be slightly more efficient here than\narray-indexing: when doing \u003ccode\u003eself.tree[o + k[i]]\u003c/code\u003e, we can effectively pre-compute\nthe pointer to \u003ccode\u003eself.tree[o]\u003c/code\u003e, so that only \u003ccode\u003ek[i]\u003c/code\u003e still has to be added. Let‚Äôs\nfirst look at that diff:\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003cspan\u003e22\n\u003c/span\u003e\u003cspan\u003e23\n\u003c/span\u003e\u003cspan\u003e24\n\u003c/span\u003e\u003cspan\u003e25\n\u003c/span\u003e\u003cspan\u003e26\n\u003c/span\u003e\u003cspan\u003e27\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn batch_ptr\u0026lt;const P: usize\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let mut k = [0; P];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let q_simd = qb.map(|q| Simd::\u0026lt;u32, 8\u0026gt;::splat(q));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+    // offsets[l] is a pointer to self.tree[self.offsets[l]]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+    let offsets = self.offsets.iter()\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+        .map(|o| unsafe { self.tree.as_ptr().add(*o) })\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+        .collect_vec();\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     for [o, o2] in offsets.array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-            let jump_to = self.node(o  +  k[i])  .find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            let jump_to = unsafe { *o.add(k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e             k[i] = k[i] * (B + 1) + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-            prefetch_index(\u0026amp;self.tree, o2 + k[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            prefetch_ptr(unsafe { o2.add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o = offsets.last().unwrap();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-        let idx = self.node(o  +  k[i])  .find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        let idx = unsafe { *o.add(k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e-        self.get(o + k[i] + idx / N, idx % N)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        unsafe { *(*o.add(k[i] + idx / N)).data.get_unchecked(idx % N) }\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--ptr\"\u003eCode Snippet 23\u003c/a\u003e:\u003c/span\u003e\nUsing pointer-based indexing instead of array indexing.\u003c/p\u003e\u003cp\u003eNow, we can avoid all the multiplications by 64, by just multiplying all \u003ccode\u003ek[i]\u003c/code\u003e\nby 64 to start with:\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003cspan\u003e22\n\u003c/span\u003e\u003cspan\u003e23\n\u003c/span\u003e\u003cspan\u003e24\n\u003c/span\u003e\u003cspan\u003e25\n\u003c/span\u003e\u003cspan\u003e26\n\u003c/span\u003e\u003cspan\u003e27\n\u003c/span\u003e\u003cspan\u003e28\n\u003c/span\u003e\u003cspan\u003e29\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn batch_byte_ptr\u0026lt;const P: usize\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let mut k = [0; P];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let q_simd = qb.map(|q| Simd::\u0026lt;u32, 8\u0026gt;::splat(q));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let offsets = self\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .offsets\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .iter()\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .map(|o| unsafe { self.tree.as_ptr().add(*o) })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .collect_vec();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     for [o, o2] in offsets.array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-            let jump_to = unsafe { *o.     add(k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e-            k[i] = k[i] * (B + 1) + jump_to     ;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            k[i] = k[i] * (B + 1) + jump_to * 64;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e-            prefetch_ptr(unsafe { o2.     add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            prefetch_ptr(unsafe { o2.byte_add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o = offsets.last().unwrap();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-        let idx = unsafe { *o.     add(k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e-        unsafe { *(*o.add(k[i] + idx / N)).data.get_unchecked(idx % N) }\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--ptr64\"\u003eCode Snippet 24\u003c/a\u003e:\u003c/span\u003e\nWe multiply \u003ccode\u003ek[i]\u003c/code\u003e by 64 up-front, and then call \u003ccode\u003ebyte_add\u003c/code\u003e instead of the usual \u003ccode\u003eadd\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eIndeed, the generated code now goes down from 17 to 15 instructions, and we can\nsee in \u003ca href=\"#figure--pointer-arithmetic\"\u003eFigure 9\u003c/a\u003e that this gives a significant speedup!\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"asm\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emovq\u003c/span\u003e         \u003cspan\u003e32\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsp\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%rdi\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e%r8\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evmovdqa\u003c/span\u003e      \u003cspan\u003e1568\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsp\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%rdi\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e4\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e%ymm0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evpcmpgtd\u003c/span\u003e     \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsi\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r8\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evpcmpgtd\u003c/span\u003e     \u003cspan\u003e32\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsi\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r8\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evpackssdw\u003c/span\u003e    \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%ymm0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003evpmovmskb\u003c/span\u003e    \u003cspan\u003e%ymm0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%r9d\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003epopcntl\u003c/span\u003e      \u003cspan\u003e%r9d\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%r9d\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emovq\u003c/span\u003e         \u003cspan\u003e%r8\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eshlq\u003c/span\u003e         \u003cspan\u003e$4\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%r10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eaddq\u003c/span\u003e         \u003cspan\u003e%r8\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eshll\u003c/span\u003e         \u003cspan\u003e$5\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e%r9d\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eandl\u003c/span\u003e         \u003cspan\u003e$-64\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r9d\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eaddq\u003c/span\u003e         \u003cspan\u003e%r10\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r9\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003emovq\u003c/span\u003e         \u003cspan\u003e%r9\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e32\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rsp\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%rdi\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003eprefetcht0\u003c/span\u003e   \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e%rcx\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e%r9\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e\u003ca href=\"#code-snippet--byte-ptr\"\u003eCode Snippet 25\u003c/a\u003e:\u003c/span\u003e\nWhen using byte-based pointers, we avoid some multiplications by 64.\u003c/p\u003e\u003ch3 id=\"the-final-version\"\u003e\u003cspan\u003e3.3.3\u003c/span\u003e The final version\n\u003ca href=\"#the-final-version\"\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eOne particularity about the code above is the \u003ccode\u003eandl $-64,%r9d\u003c/code\u003e.\nIn line 6, the bitmask gets written there. Then in line 7, it‚Äôs popcounted.\nLife 11 does a \u003ccode\u003eshll $5\u003c/code\u003e, i.e., a multiplication by 32, which is a combination\nof the \u003ccode\u003e/2\u003c/code\u003e to compensate for the double-popcount and the \u003ccode\u003e* 64\u003c/code\u003e. Then, it does\nthe \u003ccode\u003eand $-64\u003c/code\u003e, where the mask of -64 is \u003ccode\u003e111..11000000\u003c/code\u003e which ends in 6 zeros.\nBut we just multiplied by 32, so all this does is zeroing out a single bit, in\ncase the popcount was odd. But we know for a fact that that can never be, so we\ndon‚Äôt actually need this \u003ccode\u003eand\u003c/code\u003e instruction.\u003c/p\u003e\u003cp\u003eTo avoid it, we do this \u003ccode\u003e/2*64 =\u0026gt; *32\u003c/code\u003e optimization manually.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003cspan\u003e22\n\u003c/span\u003e\u003cspan\u003e23\n\u003c/span\u003e\u003cspan\u003e24\n\u003c/span\u003e\u003cspan\u003e25\n\u003c/span\u003e\u003cspan\u003e26\n\u003c/span\u003e\u003cspan\u003e27\n\u003c/span\u003e\u003cspan\u003e28\n\u003c/span\u003e\u003cspan\u003e29\n\u003c/span\u003e\u003cspan\u003e30\n\u003c/span\u003e\u003cspan\u003e31\n\u003c/span\u003e\u003cspan\u003e32\n\u003c/span\u003e\u003cspan\u003e33\n\u003c/span\u003e\u003cspan\u003e34\n\u003c/span\u003e\u003cspan\u003e35\n\u003c/span\u003e\u003cspan\u003e36\n\u003c/span\u003e\u003cspan\u003e37\n\u003c/span\u003e\u003cspan\u003e38\n\u003c/span\u003e\u003cspan\u003e39\n\u003c/span\u003e\u003cspan\u003e40\n\u003c/span\u003e\u003cspan\u003e41\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn find_splat64(\u0026amp;self, q_simd: Simd\u0026lt;u32, 8\u0026gt;) -\u0026gt; usize {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let low: Simd\u0026lt;u32, 8\u0026gt; = Simd::from_slice(\u0026amp;self.data[0..N / 2]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let high: Simd\u0026lt;u32, 8\u0026gt; = Simd::from_slice(\u0026amp;self.data[N / 2..N]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     unsafe {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let q_simd: Simd\u0026lt;i32, 8\u0026gt; = t(q_simd);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let mask_low = q_simd.simd_gt(t(low));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let mask_high = q_simd.simd_gt(t(high));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         use std::mem::transmute as t;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let merged = _mm256_packs_epi32(t(mask_low), t(mask_high));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let mask = _mm256_movemask_epi8(merged);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-        mask.count_ones() as usize / 2\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        mask.count_ones() as usize * 32\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e pub fn batch_byte_ptr\u0026lt;const P: usize\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let mut k = [0; P];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let q_simd = qb.map(|q| Simd::\u0026lt;u32, 8\u0026gt;::splat(q));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let offsets = self\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .offsets\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .iter()\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .map(|o| unsafe { self.tree.as_ptr().add(*o) })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .collect_vec();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     for [o, o2] in offsets.array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-            let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat  (q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e-            k[i] = k[i] * (B + 1) + jump_to * 64;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            k[i] = k[i] * (B + 1) + jump_to     ;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e             prefetch_ptr(unsafe { o2.byte_add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o = offsets.last().unwrap();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eCode Snippet 26:\u003c/span\u003e\nManually merging \u003ccode\u003e/2\u003c/code\u003e and \u003ccode\u003e*64\u003c/code\u003e into \u003ccode\u003e*32\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eAgain, this gives a small speedup.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/6-improvements.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/6-improvements.svg\" alt=\"Figure 9: Results of improving the search function bit by bit. Like before, the improvements are small but consistent. Throughput on 1GB input improves from 31ns/query to 28ns/query.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 9: \u003c/span\u003eResults of improving the search function bit by bit. Like before, the improvements are small but consistent. Throughput on \u003ccode\u003e1GB\u003c/code\u003e input improves from \u003ccode\u003e31ns/query\u003c/code\u003e to \u003ccode\u003e28ns/query\u003c/code\u003e.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"skip-prefetch\"\u003e\u003cspan\u003e3.4\u003c/span\u003e Skip prefetch\n\u003ca href=\"#skip-prefetch\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eNow we know that the first three levels of the graph fit in L1 cache, so\nprobably we can simply skip prefetching for those levels.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/7-skip-prefetch.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/7-skip-prefetch.svg\" alt=\"Figure 10: Skipping the prefetch for the first layers is slightly slower.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 10: \u003c/span\u003eSkipping the prefetch for the first layers is slightly slower.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAs it turns out, skipping the prefetch does not help. Probably because the\nprefetch is cheap if the data is already available, and there is a small chance\nthat the data we need was evicted to make room for other things, in which case\nthe prefetch \u003cem\u003eis\u003c/em\u003e useful.\u003c/p\u003e\u003ch2 id=\"interleave\"\u003e\u003cspan\u003e3.5\u003c/span\u003e Interleave\n\u003ca href=\"#interleave\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eOne other observation is that the first few layers are CPU bound, while the last\nfew layers are memory throughput bound.\nBy merging the two domains, we should be able to get a higher total throughput.\n(Somewhat similar to how for a piece wise linear convex function \\(f\\), \\(f((x+y)/2) \u0026lt;\n(f(x)+f(y))/2\\) when \\(x\\) and \\(y\\) are on different pieces.)\nThus, maybe we could process two batches\nof queries at the same time by processing layer \\(i\\) of one batch at the same\ntime as layer \\(i+L/2\\) of the other batch (where \\(L\\) is the height of the tree).\nI implemented this, but unfortunately the result is not faster than what we had.\u003c/p\u003e\u003cp\u003eOr maybe we can split the work as: interleave the last level of one half\nwith \u003cem\u003eall but the last\u003c/em\u003e level of the other half? Since the last-level memory\nread takes most of the time. Also that turns out slower in practice.\u003c/p\u003e\u003cp\u003eWhat does give a small speedup: process the first \u003cem\u003etwo\u003c/em\u003e levels of the next batch\ninterleaved with the last prefetch of the current batch. Still the result is\nonly around \u003ccode\u003e2ns\u003c/code\u003e speedup, while code the (not shown ;\u0026#34;) gets significantly more\nmessy.\u003c/p\u003e\u003cp\u003eWhat \u003cem\u003edoes\u003c/em\u003e work great, is interleaving \u003cem\u003eall\u003c/em\u003e layers of the search: when the\ntree has \\(L\\) layers, we can interleave \\(L\\) batches at a time, and then process\nlayer \\(i\\) of the \\(i\\)‚Äôth in-progress batch. Then we ‚Äòshift out‚Äô the completed\nbatch and store the answers to those queries, and ‚Äòshift in‚Äô a new batch.\nThis we, completely average the different workloads of all the layers, and\nshould achieve near-optimal performance given the CPU‚Äôs memory bandwidth to L3\nand RAM (at least, that‚Äôs what I assume is the bottleneck now).\u003c/p\u003e\u003cdetails\u003e\u003csummary\u003eClick to show code for interleaving.\u003c/summary\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003cspan\u003e22\n\u003c/span\u003e\u003cspan\u003e23\n\u003c/span\u003e\u003cspan\u003e24\n\u003c/span\u003e\u003cspan\u003e25\n\u003c/span\u003e\u003cspan\u003e26\n\u003c/span\u003e\u003cspan\u003e27\n\u003c/span\u003e\u003cspan\u003e28\n\u003c/span\u003e\u003cspan\u003e29\n\u003c/span\u003e\u003cspan\u003e30\n\u003c/span\u003e\u003cspan\u003e31\n\u003c/span\u003e\u003cspan\u003e32\n\u003c/span\u003e\u003cspan\u003e33\n\u003c/span\u003e\u003cspan\u003e34\n\u003c/span\u003e\u003cspan\u003e35\n\u003c/span\u003e\u003cspan\u003e36\n\u003c/span\u003e\u003cspan\u003e37\n\u003c/span\u003e\u003cspan\u003e38\n\u003c/span\u003e\u003cspan\u003e39\n\u003c/span\u003e\u003cspan\u003e40\n\u003c/span\u003e\u003cspan\u003e41\n\u003c/span\u003e\u003cspan\u003e42\n\u003c/span\u003e\u003cspan\u003e43\n\u003c/span\u003e\u003cspan\u003e44\n\u003c/span\u003e\u003cspan\u003e45\n\u003c/span\u003e\u003cspan\u003e46\n\u003c/span\u003e\u003cspan\u003e47\n\u003c/span\u003e\u003cspan\u003e48\n\u003c/span\u003e\u003cspan\u003e49\n\u003c/span\u003e\u003cspan\u003e50\n\u003c/span\u003e\u003cspan\u003e51\n\u003c/span\u003e\u003cspan\u003e52\n\u003c/span\u003e\u003cspan\u003e53\n\u003c/span\u003e\u003cspan\u003e54\n\u003c/span\u003e\u003cspan\u003e55\n\u003c/span\u003e\u003cspan\u003e56\n\u003c/span\u003e\u003cspan\u003e57\n\u003c/span\u003e\u003cspan\u003e58\n\u003c/span\u003e\u003cspan\u003e59\n\u003c/span\u003e\u003cspan\u003e60\n\u003c/span\u003e\u003cspan\u003e61\n\u003c/span\u003e\u003cspan\u003e62\n\u003c/span\u003e\u003cspan\u003e63\n\u003c/span\u003e\u003cspan\u003e64\n\u003c/span\u003e\u003cspan\u003e65\n\u003c/span\u003e\u003cspan\u003e66\n\u003c/span\u003e\u003cspan\u003e67\n\u003c/span\u003e\u003cspan\u003e68\n\u003c/span\u003e\u003cspan\u003e69\n\u003c/span\u003e\u003cspan\u003e70\n\u003c/span\u003e\u003cspan\u003e71\n\u003c/span\u003e\u003cspan\u003e72\n\u003c/span\u003e\u003cspan\u003e73\n\u003c/span\u003e\u003cspan\u003e74\n\u003c/span\u003e\u003cspan\u003e75\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"rust\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003ebatch_interleave_full_128\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e: \u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e])\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eVec\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003ematch\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// 1 batch of size 128\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebatch_interleave_full\u003c/span\u003e::\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e128\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e128\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// 2 batches of size 64 in parallel, with product 128\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebatch_interleave_full\u003c/span\u003e::\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e64\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e128\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// 3 batches of size 32 in parallel with product 96\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e3\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebatch_interleave_full\u003c/span\u003e::\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e32\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e3\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e96\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e4\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebatch_interleave_full\u003c/span\u003e::\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e32\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e4\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e128\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e5\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebatch_interleave_full\u003c/span\u003e::\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e16\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e5\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e80\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e6\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebatch_interleave_full\u003c/span\u003e::\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e16\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e6\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e96\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e7\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebatch_interleave_full\u003c/span\u003e::\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e16\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e7\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e112\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e8\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebatch_interleave_full\u003c/span\u003e::\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e16\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e8\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e128\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e_\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003epanic!\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;Unsupported tree height \u003c/span\u003e\u003cspan\u003e{}\u003c/span\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e()),\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003epub\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003ebatch_interleave_full\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eP\u003c/span\u003e: \u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eL\u003c/span\u003e: \u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ePL\u003c/span\u003e: \u003cspan\u003eusize\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e: \u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e],\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e-\u0026gt; \u003cspan\u003eVec\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003eassert_eq!\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e(),\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eL\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emut\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eout\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eVec\u003c/span\u003e::\u003cspan\u003ewith_capacity\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e());\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emut\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eans\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eP\u003c/span\u003e\u003cspan\u003e];\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e// Iterate over chunks of size P of queries.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e// Omitted: initialize\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efirst_i\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eL\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003efor\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003echunk\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ein\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eqs\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003earray_chunks\u003c/span\u003e::\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eP\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efirst_i\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// Decrement first_i, modulo L.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003eif\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efirst_i\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e==\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003efirst_i\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eL\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003efirst_i\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e-=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// Process 1 element per chunk, starting at element first_i.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// (Omitted: process first up-to L elements.)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// Write output and read new queries from index j.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emut\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ej\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003eloop\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003e// First L-1 levels: do the usual thing.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003e// The compiler will unroll this loop.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003efor\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003el\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ein\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e..\u003c/span\u003e\u003cspan\u003eL\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e                \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ejump_to\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eunsafe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003el\u003c/span\u003e\u003cspan\u003e].\u003c/span\u003e\u003cspan\u003ebyte_add\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e])\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e}.\u003c/span\u003e\u003cspan\u003efind_splat64\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_simd\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e                \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eB\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ejump_to\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e                \u003c/span\u003e\u003cspan\u003eprefetch_ptr\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eunsafe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eoffsets\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003el\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e].\u003c/span\u003e\u003cspan\u003ebyte_add\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e])\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e});\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e                \u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003e// Last level: read answer.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003eans\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ej\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e                \u003c/span\u003e\u003cspan\u003elet\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eunsafe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003eol\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebyte_add\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e])\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e}.\u003c/span\u003e\u003cspan\u003efind_splat\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_simd\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e                \u003c/span\u003e\u003cspan\u003eunsafe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eol\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebyte_add\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e])\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eas\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eu32\u003c/span\u003e\u003cspan\u003e).\u003c/span\u003e\u003cspan\u003eadd\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eidx\u003c/span\u003e\u003cspan\u003e).\u003c/span\u003e\u003cspan\u003eread\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003e};\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003e// Last level: reset index, and read new query.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003ek\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003eq_simd\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eSimd\u003c/span\u003e::\u003cspan\u003esplat\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003echunk\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003ej\u003c/span\u003e\u003cspan\u003e]);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003ej\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e+=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003eif\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ei\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ePL\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eL\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e                \u003c/span\u003e\u003cspan\u003ebreak\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e            \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003e// (Omitted: process last up-to L elements.)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e        \u003c/span\u003e\u003cspan\u003eout\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eextend_from_slice\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eans\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e    \u003c/span\u003e\u003cspan\u003eout\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eCode Snippet 27:\u003c/span\u003e\nIn code, we interleave all layers by compiling a separate function for each height of the tree. Then the compiler can unroll the loop over the layers. There is a bunch of overhead in the code for the first and last iterations that\u0026#39;s omitted.\u003c/p\u003e\u003c/details\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/8-interleave.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/8-interleave.svg\" alt=\"Figure 11: Interleaving all layers of the search binary search improves throughput from 29ns/query to 24ns/query.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 11: \u003c/span\u003eInterleaving all layers of the search binary search improves throughput from \u003ccode\u003e29ns/query\u003c/code\u003e to \u003ccode\u003e24ns/query\u003c/code\u003e.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"optimizing-the-tree-layout\"\u003e\u003cspan\u003e4\u003c/span\u003e Optimizing the tree layout\n\u003ca href=\"#optimizing-the-tree-layout\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003ch2 id=\"left-tree\"\u003e\u003cspan\u003e4.1\u003c/span\u003e Left-tree\n\u003ca href=\"#left-tree\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eSo far, every internal node of the tree stores the minimum of the subtree on\nit‚Äôs right (\u003ca href=\"#figure--stree-full\"\u003eFigure 3\u003c/a\u003e, reproduced below).\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/full.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/full.svg\" alt=\"Figure 12: Usually in B+ trees, each node stores the minimum of it‚Äôs right subtree. Let‚Äôs call this a right (S+/B+) tree.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 12: \u003c/span\u003eUsually in B+ trees, each node stores the minimum of it‚Äôs right subtree. Let‚Äôs call this a \u003cem\u003eright\u003c/em\u003e (S+/B+) tree.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThis turns out somewhat inefficient when searching values that are exactly in\nbetween two subtrees (as \u003cem\u003ealso\u003c/em\u003e already suggested by Algorithmica), such as\n\\(5.5\\). In that case, the search descends into the\nleftmost (green) subtree with node \\([2, 4]\\). Then, it goes to the rightmost\n(red) node \\([4,5]\\). There, we realize \\(5.5 \u0026gt; 5\\), and thus we need the next value\nin the red layer (which is stored as a single array), which is \\(6\\). The problem\nnow is that the red tree nodes exactly correspond to cache lines, and thus, the\n\\(6\\) will be in a new cache line that needs to be fetched from memory.\u003c/p\u003e\u003cp\u003eNow consider the \u003cem\u003eleft-max\u003c/em\u003e tree below:\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/flipped.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/flipped.svg\" alt=\"Figure 13: In the left-max S+ tree, each internal node contains the maximum of its left subtree.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 13: \u003c/span\u003eIn the \u003cem\u003eleft-max\u003c/em\u003e S+ tree, each internal node contains the maximum of its \u003cem\u003eleft\u003c/em\u003e subtree.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eNow if we search for \\(5.5\\), we descend into the middle subtree rooted at\n\\([7,9]\\). Then we go left to the \\([6,7]\\) node, and end up reading \\(6\\) as the\nfirst value \\(\\geq 5.5\\). Now, the search directly steers toward the node\nthat actually contains the answer, instead of the one just before.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/9-left-max-tree.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/9-left-max-tree.svg\" alt=\"Figure 14: The left-S tree brings runtime down from 24ns/query for the interleaved version to 22ns/query now.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 14: \u003c/span\u003eThe left-S tree brings runtime down from \u003ccode\u003e24ns/query\u003c/code\u003e for the interleaved version to \u003ccode\u003e22ns/query\u003c/code\u003e now.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"memory-layouts\"\u003e\u003cspan\u003e4.2\u003c/span\u003e Memory layouts\n\u003ca href=\"#memory-layouts\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eLet‚Äôs now consider some alternative memory layouts.\nSo far, we were packing all layers in forward order, but the Algorithmica post\nactually stores them in reverse, so we‚Äôll try that too. The query code is\nexactly the same, since the order of the layers is already encoded into the offsets.\u003c/p\u003e\u003cp\u003eAnother potential improvement is to always store a \u003cem\u003efull\u003c/em\u003e array. This may seem\nvery inefficient, but is actually not that bad when we make sure to use\nuninitialized memory. In that case, untouched memory pages will simply never be\nmapped, so that we waste on average only about 2MB\nper layer when hugepages are enabled, and 14MB when there are 7 layers and the\nentire array takes 1GB.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/layouts.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/layouts.svg\" alt=\"Figure 15: So far we have been using the packed layout. We now also try the reversed layout as used by Algorithmica, and the full layout that allows simple arithmetic for indexing.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 15: \u003c/span\u003eSo far we have been using the packed layout. We now also try the \u003cem\u003ereversed\u003c/em\u003e layout as used by Algorithmica, and the \u003cem\u003efull\u003c/em\u003e layout that allows simple arithmetic for indexing.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eA benefit of storing the full array is that instead of using the offsets, we can\nsimply compute the index in the next layer directly, as we did for the\nEytzinger search.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn batch_ptr3_full\u0026lt;const P: usize\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let mut k = [0; P];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let q_simd = qb.map(|q| Simd::\u0026lt;u32, 8\u0026gt;::splat(q));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+    let o = self.tree.as_ptr();\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-    for [o, o2] in offsets.array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+    for _l      in 0..self.offsets.len() - 1 {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-            k[i] = k[i] * (B + 1) + jump_to     ;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            k[i] = k[i] * (B + 1) + jump_to + 64;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e             prefetch_ptr(unsafe { o.byte_add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eCode Snippet 28:\u003c/span\u003e\nWhen storing the array in full, we can drop the per-layer offsets and instead compute indices directly.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/9-params.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/9-params.svg\" alt=\"Figure 16: Comparison with reverse and full memory layout, and full memory layout with using a dedicated _full search that computes indices directly.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 16: \u003c/span\u003eComparison with reverse and full memory layout, and full memory layout with using a dedicated \u003ccode\u003e_full\u003c/code\u003e search that computes indices directly.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAs it turns out, neither of those layouts improves performance, and so we will\nnot use them going forward.\u003c/p\u003e\u003ch2 id=\"node-size-b-15\"\u003e\u003cspan\u003e4.3\u003c/span\u003e Node size \\(B=15\\)\n\u003ca href=\"#node-size-b-15\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eWe can also try storing only 15 values per node, so that the branching factor\nis 16. This has the benefit of making the multiplication by \\(B+1\\) (17 so far)\nslightly simpler, since it replaces \u003ccode\u003ex = (x\u0026lt;\u0026lt;4)+x\u003c/code\u003e by \u003ccode\u003ex = x\u0026lt;\u0026lt;4\u003c/code\u003e.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/10-base15.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/10-base15.svg\" alt=\"Figure 17: Storing 15 values per node. The lines in the bottom part of the plot show the overhead that each data structure has relative to the size of the input, capped at 1 (which corresponds to take double the size).\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 17: \u003c/span\u003eStoring 15 values per node. The lines in the bottom part of the plot show the overhead that each data structure has relative to the size of the input, capped at 1 (which corresponds to take double the size).\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWhen the tree has up to 5 layers and the data fits in L3 cache, using \\(B=15\\) is\nindeed slightly faster when the number of layers in the tree is the same. On the\nother hand, the lower branching factor of \\(16\\) requires an additional layer for smaller sizes than\nwhen using branching factor \\(17\\). When the input is much larger than L3 cache\nthe speedup disappears, because RAM throughput becomes a common bottleneck.\u003c/p\u003e\u003ch3 id=\"data-structure-size\"\u003e\u003cspan\u003e4.3.1\u003c/span\u003e Data structure size\n\u003ca href=\"#data-structure-size\"\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp\u003ePlain binary search and the Eytzinger layout have pretty much no overhead.\nOur S+ tree so far has around \\(1/16=6.25\\%\\) overhead: \\(1/17\\) of the\nvalues in the final layer is duplicated in the layer above, and \\(1/17\\) of\n\u003cem\u003ethose\u003c/em\u003e is duplicated again, and so on, for a total of \\(1/17 + 1/17^2 + \\cdots =\n1/16\\).\u003c/p\u003e\u003cp\u003eUsing node size \\(15\\) instead, increases the overhead:\nEach node now only stores \\(15\\) instead of \\(16\\) elements, so that we already have\nan overhead of \\(1/15\\). Furthermore the reduced branching factor increases the\nduplication overhead fro \\(1/16\\) to \\(1/15\\) as well, for a total overhead of \\(2/15\n= 13.3\\%\\), which matches the dashed blue line in \u003ca href=\"#figure--b15\"\u003eFigure 17\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"summary\"\u003e\u003cspan\u003e4.4\u003c/span\u003e Summary\n\u003ca href=\"#summary\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/11-summary.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/11-summary.svg\" alt=\"Figure 18: A summary of all the improvements we made so far.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 18: \u003c/span\u003eA summary of all the improvements we made so far.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eOf all the improvements so far, only the interleaving is maybe a bit too much:\nit is the only method that does not work batch-by-batch, but really benefits\nfrom having the full input at once. And also its code is three times longer\nthan the plain batched query methods because the first and last few\niterations of each loop are handled separately.\u003c/p\u003e\u003ch2 id=\"prefix-partitioning\"\u003e\u003cspan\u003e5\u003c/span\u003e Prefix partitioning\n\u003ca href=\"#prefix-partitioning\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eSo far, we‚Äôve been doing a purely \u003cem\u003ecomparison-based search\u003c/em\u003e.\nNow, it is time for something new: \u003cem\u003epartitioning\u003c/em\u003e the input values.\u003c/p\u003e\u003cp\u003eThe simplest form of the idea is to simply partition values by their top \\(b\\)\nbits, into \\(2^b\\) parts. Then we can build \\(2^b\\) independent search trees and\nsearch each query in one of them. If \\(b=12\\), this saves the first two levels of\nthe search (or slightly less, actually, since \\(2^{12} = 16^3 \u0026lt; 17^3\\)).\u003c/p\u003e\u003ch2 id=\"full-layout\"\u003e\u003cspan\u003e5.1\u003c/span\u003e Full layout\n\u003ca href=\"#full-layout\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eIn memory, we can store these trees very similar to the \u003cem\u003efull\u003c/em\u003e layout we had\nbefore, with the main differences that the first few layers are skipped and that\nnow there will be padding at the end of each part, rather than once at the end.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/prefix.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/prefix.svg\" alt=\"Figure 19: The full partitioned layout concatenates the full trees for all parts ‚Äòhorizontally‚Äô. As a new detail, when a part is not full, the smallest value of the next part is appended in the leaf layer.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 19: \u003c/span\u003eThe \u003cem\u003efull\u003c/em\u003e partitioned layout concatenates the full trees for all parts ‚Äòhorizontally‚Äô. As a new detail, when a part is not full, the smallest value of the next part is appended in the leaf layer.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eFor some choices of \\(b\\), it could happen that up to \\(15/16\\) of each tree is\npadding. To reduce this overhead, we attempt to shrink \\(b\\) while keeping the\nheight of all trees the same: as long as all pairs of adjacent trees would\nfit together in the same space, we decrease \\(b\\) by one. This way, all parts will\nbe filled for at least \\(50\\%\\) when the elements are evenly distributed.\u003c/p\u003e\u003cp\u003eOnce construction is done, the code for querying is very similar to before: we\nonly have to start the search for each query at the index of its part, given by\n\u003ccode\u003eq \u0026gt;\u0026gt; shift\u003c/code\u003e for some value of \u003ccode\u003eshift\u003c/code\u003e, rather than at index \\(0\\).\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003cspan\u003e22\n\u003c/span\u003e\u003cspan\u003e23\n\u003c/span\u003e\u003cspan\u003e24\n\u003c/span\u003e\u003cspan\u003e25\n\u003c/span\u003e\u003cspan\u003e26\n\u003c/span\u003e\u003cspan\u003e27\n\u003c/span\u003e\u003cspan\u003e28\n\u003c/span\u003e\u003cspan\u003e29\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn search_prefix\u0026lt;const P: usize\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let offsets = self\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .offsets\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .iter()\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .map(|o| unsafe { self.tree.as_ptr().add(*o) })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .collect_vec();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     // Initial parts, and prefetch them.\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o0 = offsets[0];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-    let mut k = [0; P];\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+    let mut k = qb.map(|q| {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+        (q as usize \u0026gt;\u0026gt; self.shift) * 64\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+    });\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e     let q_simd = qb.map(|q| Simd::\u0026lt;u32, 8\u0026gt;::splat(q));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     for [o, o2] in offsets.array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             k[i] = k[i] * (B + 1) + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             prefetch_ptr(unsafe { o2.byte_add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o = offsets.last().unwrap();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eCode Snippet 29:\u003c/span\u003e\nSearching the full layout of the partitioned tree starts in the partition in which each query belongs.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/20-prefix.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/20-prefix.svg\" alt=\"Figure 20: The ‚Äòsimple‚Äô partitioned tree, for (b_{textrm{max}}in {4,8,12,16,20}), shown as dotted lines.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 20: \u003c/span\u003eThe ‚Äòsimple‚Äô partitioned tree, for (b_{textrm{max}}in {4,8,12,16,20}), shown as dotted lines.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe see that indeed, the partitioned tree has a space overhead varying between\n\\(0\\) and \\(1\\), making this not yet useful in practice.\nLarger \\(b\\) reduce the height of the remaining trees, and indeed we\nsee that queries are faster for larger \\(b\\). Especially for small trees there is\na significant speedup over interleaving. Somewhat surprisingly, none of the\npartition sizes has faster queries than interleaving for large inputs. Also\nimportant to note is that while partitioning is very fast for sizes up to L1\ncache, this is only possible because they have \\(\\gg 1\\) space overhead.\u003c/p\u003e\u003ch2 id=\"compact-subtrees\"\u003e\u003cspan\u003e5.2\u003c/span\u003e Compact subtrees\n\u003ca href=\"#compact-subtrees\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eJust like we used the \u003cem\u003epacked\u003c/em\u003e layout before, we can also do that now, by simply\nconcatenating the representation of all packed subtrees.\nWe ensure that all subtrees are still padded into the same total size, but now\nwe only add as much padding as needed for the largest part, rather than padding\nto \u003cem\u003efull\u003c/em\u003e trees. Then, we give each tree the same layout in memory.\u003c/p\u003e\u003cp\u003eWe‚Äôll have offsets \\(o_\\ell\\) of where each layer starts in the first tree, and we\nstore the constant size of the trees. That way, we can easily index each layer\nof each part.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/prefix-compact.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/prefix-compact.svg\" alt=\"Figure 21: Compared to before, Figure 19, the lowest level of each subtree now only takes 2 instead of 3 nodes.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 21: \u003c/span\u003eCompared to before, \u003ca href=\"#figure--prefix\"\u003eFigure 19\u003c/a\u003e, the lowest level of each subtree now only takes 2 instead of 3 nodes.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe code for querying does become slightly more complicated. Now, we must\nexplicitly track the part that each query belongs to, and compute all indices\nbased on the layer offset, the in-layer offset \u003ccode\u003ek[i]\u003c/code\u003e, \u003cem\u003eand\u003c/em\u003e the part offset.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003cspan\u003e22\n\u003c/span\u003e\u003cspan\u003e23\n\u003c/span\u003e\u003cspan\u003e24\n\u003c/span\u003e\u003cspan\u003e25\n\u003c/span\u003e\u003cspan\u003e26\n\u003c/span\u003e\u003cspan\u003e27\n\u003c/span\u003e\u003cspan\u003e28\n\u003c/span\u003e\u003cspan\u003e29\n\u003c/span\u003e\u003cspan\u003e30\n\u003c/span\u003e\u003cspan\u003e31\n\u003c/span\u003e\u003cspan\u003e32\n\u003c/span\u003e\u003cspan\u003e33\n\u003c/span\u003e\u003cspan\u003e34\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn search\u0026lt;const P: usize\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let offsets = self\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .offsets\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .iter()\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .map(|o| unsafe { self.tree.as_ptr().add(*o) })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .collect_vec();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     // Initial parts, and prefetch them.\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o0 = offsets[0];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+    let mut k: [usize; P] = [0; P];\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+    let parts: [usize; P] = qb.map(|q| {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+        // byte offset of the part.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+        (q as usize \u0026gt;\u0026gt; self.shift) * self.bpp * 64\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+    });\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e     let q_simd = qb.map(|q| Simd::\u0026lt;u32, 8\u0026gt;::splat(q));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     for [o, o2] in offsets.array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-            let jump_to = unsafe { *o.byte_add(           k[i]) }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            let jump_to = unsafe { *o.byte_add(parts[i] + k[i]) }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e             k[i] = k[i] * (B + 1) + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-            prefetch_ptr(unsafe { o2.byte_add(           k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            prefetch_ptr(unsafe { o2.byte_add(parts[i] + k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o = offsets.last().unwrap();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-        let idx = unsafe { *o.byte_add(           k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        let idx = unsafe { *o.byte_add(parts[i] + k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e-        unsafe { (o.byte_add(           k[i]) as *const u32).add(idx).read() }\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        unsafe { (o.byte_add(parts[i] + k[i]) as *const u32).add(idx).read() }\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eCode Snippet 30:\u003c/span\u003e\nThe indexing for the packed subtrees requires explicitly tracking the part of each query. This slows things down a bit.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/21-compact.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/21-compact.svg\" alt=\"Figure 22: Compared to the the simple/full layout before (dark blue dots for (b=16)), the compact layout (e.g. red dots for (b=16)) consistently uses less memory, but is slightly slower.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 22: \u003c/span\u003eCompared to the the simple/full layout before (dark blue dots for (b=16)), the compact layout (e.g. red dots for (b=16)) consistently uses less memory, but is slightly slower.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eFor fixed \\(b_{\\textrm{max}}\\), memory overhead of the compact layout is small as\nlong as the input is sufficiently large and the trees have sufficiently many\nlayers. Thus, this tree could be practical.\nUnfortunately though, querying them is slightly slower than before,\nbecause we must explicitly track the part of each query.\u003c/p\u003e\u003ch2 id=\"the-best-of-both-compact-first-level\"\u003e\u003cspan\u003e5.3\u003c/span\u003e The best of both: compact first level\n\u003ca href=\"#the-best-of-both-compact-first-level\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eAs we just saw, storing the trees one by one slows queries down, so we would\nlike to avoid that. But on the other hand, the full layout can waste space.\u003c/p\u003e\u003cp\u003eHere, we combine the two ideas. We would like to store the \u003cem\u003ehorizontal\u003c/em\u003e\nconcatenation of the packed trees (each packed to the same size), but this is\ncomplicated, because then levels would have a non-constant branching factor.\nInstead, we can fully omit the last few (level 2) subtrees from each\ntree, and pad those subtrees that \u003cem\u003eare\u003c/em\u003e present to full subtrees.\nThis way, only the first level has a configurable branching factor \\(B_1\\), which we can\nsimply store after construction is done.\u003c/p\u003e\u003cp\u003eThis layout takes slightly more space than before because the subtrees must\nbe full, but the overhead should typically be on the order of \\(1/16\\),\nsince (for uniform data) each tree will have \\(\\geq 9\\) subtrees, of which only\nthe last is not full.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/prefix-l1.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/prefix-l1.svg\" alt=\"Figure 23: We can also store the horizontal concatenation of all trees. Here, the number of subtrees can be fixed to be less than (B+1), and is (2) instead of (B+1=3). Although not shown, deeper layers must always be full and have a (B+1) branching factor.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 23: \u003c/span\u003eWe can also store the horizontal concatenation of all trees. Here, the number of subtrees can be fixed to be less than (B+1), and is (2) instead of (B+1=3). Although not shown, deeper layers must always be full and have a (B+1) branching factor.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003cspan\u003e22\n\u003c/span\u003e\u003cspan\u003e23\n\u003c/span\u003e\u003cspan\u003e24\n\u003c/span\u003e\u003cspan\u003e25\n\u003c/span\u003e\u003cspan\u003e26\n\u003c/span\u003e\u003cspan\u003e27\n\u003c/span\u003e\u003cspan\u003e28\n\u003c/span\u003e\u003cspan\u003e29\n\u003c/span\u003e\u003cspan\u003e30\n\u003c/span\u003e\u003cspan\u003e31\n\u003c/span\u003e\u003cspan\u003e32\n\u003c/span\u003e\u003cspan\u003e33\n\u003c/span\u003e\u003cspan\u003e34\n\u003c/span\u003e\u003cspan\u003e35\n\u003c/span\u003e\u003cspan\u003e36\n\u003c/span\u003e\u003cspan\u003e37\n\u003c/span\u003e\u003cspan\u003e38\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn search_b1\u0026lt;const P: usize\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let offsets = self\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .offsets\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .iter()\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .map(|o| unsafe { self.tree.as_ptr().add(*o) })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .collect_vec();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o0 = offsets[0];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let mut k: [usize; P] = qb.map(|q| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e          (q as usize \u0026gt;\u0026gt; self.shift) * 64\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let q_simd = qb.map(|q| Simd::\u0026lt;u32, 8\u0026gt;::splat(q));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-    for         [o, o2]  in offsets.array_windows()        {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+    if let Some([o1, o2]) = offsets.array_windows().next() {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-            k[i] = k[i] * (B + 1) + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            k[i] = k[i] * self.b1 + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e             prefetch_ptr(unsafe { o2.byte_add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-    for [o, o2] in offsets     .array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+    for [o, o2] in offsets[1..].array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             k[i] = k[i] * (B + 1) + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             prefetch_ptr(unsafe { o2.byte_add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o = offsets.last().unwrap();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eCode Snippet 31:\u003c/span\u003e\nNow, the code is simple again, in that we don\u0026#39;t need to explicitly track part indices. All that changes is that we handle the first iteration of the for loop separately, and use branching factor \u003ccode\u003eself.b1\u003c/code\u003e instead of \u003ccode\u003eB+1\u003c/code\u003e there.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/22-l1.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/22-l1.svg\" alt=\"Figure 24: When compressing the first level, space usage is very similar to the compact layout before, and query speed is as fast as the full layout before.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 24: \u003c/span\u003eWhen compressing the first level, space usage is very similar to the compact layout before, and query speed is as fast as the full layout before.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"overlapping-trees\"\u003e\u003cspan\u003e5.4\u003c/span\u003e Overlapping trees\n\u003ca href=\"#overlapping-trees\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eA drawback of all the above methods is that memory usage is heavily influenced by the\nlargest part, since all parts must be at least as large. This is especially a\nproblem when the distribution of part sizes is very skewed.\nWe can avoid this by sharing storage between adjacent trees.\nLet \\(S_p\\) be the number of subtrees for each part \\(p\\), and \\(S_{max} = \\max_p S_p\\).\nThen, we can define the \u003cem\u003eoverlap\u003c/em\u003e \\(0\\leq v\\leq B\\), and append only\n\\(B_1 = S_{max}-v\\) new subtrees for each new part, rather than \\(S_{max}\\) as we\ndid before.\nThe values for each part are then simply appended where the previous part left\noff, unless that subtree is ‚Äòout-of-reach‚Äô for the current part, in which\ncase first some padding is added.\nThis way, consecutive\nparts can overlap and exchange memory, and we can somewhat ‚Äòbuffer‚Äô the effect\nof large parts.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/prefix-overlapping.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/prefix-overlapping.svg\" alt=\"Figure 25: In this example, the third tree has (6) values in ([8, 12)) and requires (S_{max}=3) subtrees. We have an overlap of (v=1), so that for each additional tree, only (2) subtrees are added. We add padding elements in grey to ensure all elements are reachable from their own tree.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 25: \u003c/span\u003eIn this example, the third tree has (6) values in ([8, 12)) and requires (S_{max}=3) subtrees. We have an overlap of (v=1), so that for each additional tree, only (2) subtrees are added. We add padding elements in grey to ensure all elements are reachable from their own tree.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWhen the overlap is \\(1\\), as in the example above, the nodes in the first layer\neach contain the maximum value of \\(B\\) subtrees. When the overlap is larger than\n\\(1\\), the nodes in the first layer would contain overlapping values. Instead, we\nstore a single list of values, in which we can do \u003cem\u003eunaligned\u003c/em\u003e reads to get the\nright slice of \\(B\\) values that we need.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003cspan\u003e22\n\u003c/span\u003e\u003cspan\u003e23\n\u003c/span\u003e\u003cspan\u003e24\n\u003c/span\u003e\u003cspan\u003e25\n\u003c/span\u003e\u003cspan\u003e26\n\u003c/span\u003e\u003cspan\u003e27\n\u003c/span\u003e\u003cspan\u003e28\n\u003c/span\u003e\u003cspan\u003e29\n\u003c/span\u003e\u003cspan\u003e30\n\u003c/span\u003e\u003cspan\u003e31\n\u003c/span\u003e\u003cspan\u003e32\n\u003c/span\u003e\u003cspan\u003e33\n\u003c/span\u003e\u003cspan\u003e34\n\u003c/span\u003e\u003cspan\u003e35\n\u003c/span\u003e\u003cspan\u003e36\n\u003c/span\u003e\u003cspan\u003e37\n\u003c/span\u003e\u003cspan\u003e38\n\u003c/span\u003e\u003cspan\u003e39\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn search\u0026lt;const P: usize, const PF: bool\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let offsets = self\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .offsets\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .iter()\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .map(|o| unsafe { self.tree.as_ptr().add(*o) })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .collect_vec();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o0 = offsets[0];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let mut k: [usize; P] = qb.map(|q| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-        (q as usize \u0026gt;\u0026gt; self.shift) * 4 *  16\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        (q as usize \u0026gt;\u0026gt; self.shift) * 4 * (16 - self.overlap)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e     });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let q_simd = qb.map(|q| Simd::\u0026lt;u32, 8\u0026gt;::splat(q));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     if let Some([o1, o2]) = offsets.array_windows().next() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e+            // First level read may be unaligned.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e-            let jump_to = unsafe { *o.byte_add(k[i])                  }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+            let jump_to = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e             k[i] = k[i] * self.l1 + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             prefetch_ptr(unsafe { o2.byte_add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     for [o, o2] in offsets[1..].array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             k[i] = k[i] * (B + 1) + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             prefetch_ptr(unsafe { o2.byte_add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o = offsets.last().unwrap();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-        let idx = unsafe { *o.byte_add(k[i])                  }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        let idx = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eCode Snippet 32:\u003c/span\u003e\nEach part now contains \\(16-v\\) values, instead of the original 16. We use \u003ccode\u003eread_unaligned\u003c/code\u003e since we do not always read at 16-value boundaries anymore.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/23-overlap.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/23-overlap.svg\" alt=\"Figure 26: Overlapping trees usually use less memory than the equivalent version with first-level compression, while being about as fast.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 26: \u003c/span\u003eOverlapping trees usually use less memory than the equivalent version with first-level compression, while being about as fast.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"human-data\"\u003e\u003cspan\u003e5.5\u003c/span\u003e Human data\n\u003ca href=\"#human-data\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eSo far we‚Äôve been testing with uniform random data, where the largest part\ndeviates form the mean size by around \\(\\sqrt n\\). Now, let‚Äôs look at some real\ndata: k-mers of a human genome. DNA consists of \u003ccode\u003eACGT\u003c/code\u003e characters that can be\nencoded as 2 bits, so each string of \\(k=16\\) characters defines a 32 bit\ninteger\u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e.\nWe then look at the first \\(n\\) k-mers of the human genome, starting at chromosome 1.\u003c/p\u003e\u003cp\u003eTo give an idea, the plot below show for each k-mer of length \\(k=12\\) how often\nit occurs in the full human genome. In total, there are around 3G\nk-mers, and so the expected count for each k-mer is around 200. But instead,\nwe see k-mers that occur over 2 million times! So if we were to partition on the\nfirst 24 bits, the size of the largest part is only around \\(2^{-10}\\) of the input,\nrather than \\(2^{-24}\\).\u003c/p\u003e\u003cp\u003eThe accumulated counts are shown in orange, where we also see a number of flat\nregions caused by underrepresented k-mers.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/rank-curve.png\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/rank-curve.png\" alt=\"Figure 27: A plot showing k-mer counts for all (4^{12} = 16M) $k=12$-mers of the human genome. On random data each k-mer would occur around 200 times, but here we see some k-mers occurring over 2 million times.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 27: \u003c/span\u003eA plot showing k-mer counts for all (4^{12} = 16M) $k=12$-mers of the human genome. On random data each k-mer would occur around 200 times, but here we see some k-mers occurring over 2 million times.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/23-overlap-human.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/23-overlap-human.svg\" alt=\"Figure 28: Building the overlapping trees for k-mers of the human genome takes much more space, and even using only 16 parts regularly requires up to 50% overhead, making this data structure not quite practical.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 28: \u003c/span\u003eBuilding the overlapping trees for k-mers of the human genome takes much more space, and even using only 16 parts regularly requires up to 50% overhead, making this data structure not quite practical.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"prefix-map\"\u003e\u003cspan\u003e5.6\u003c/span\u003e Prefix map\n\u003ca href=\"#prefix-map\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eWe need a way to handle unbalanced partition sizes, instead of mapping\neverything linearly.\nWe can do this by simply storing the full tree compactly as we did before,\npreceded by an array (in blue below) that points to the index of the first\nsubtree containing elements of the part. Like for the overlapping trees before,\nthe first layer is simply a list of the largest elements of all subtrees that\ncan be indexed anywhere (potentially unaligned).\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/prefix-map.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/prefix-map.svg\" alt=\"Figure 29: The prefix map, in blue, stores (2^b) elements, that for each $b$-bit prefix stores the index of the first subtree that contains an element of that prefix.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 29: \u003c/span\u003eThe prefix map, in blue, stores (2^b) elements, that for each $b$-bit prefix stores the index of the first subtree that contains an element of that prefix.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eTo answer a query, we first find its part, then read the block (16 elements)\nstarting at the pointed-to element, and then proceed as usual from the sub-tree onward.\u003c/p\u003e\u003cdiv\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003cspan\u003e 1\n\u003c/span\u003e\u003cspan\u003e 2\n\u003c/span\u003e\u003cspan\u003e 3\n\u003c/span\u003e\u003cspan\u003e 4\n\u003c/span\u003e\u003cspan\u003e 5\n\u003c/span\u003e\u003cspan\u003e 6\n\u003c/span\u003e\u003cspan\u003e 7\n\u003c/span\u003e\u003cspan\u003e 8\n\u003c/span\u003e\u003cspan\u003e 9\n\u003c/span\u003e\u003cspan\u003e10\n\u003c/span\u003e\u003cspan\u003e11\n\u003c/span\u003e\u003cspan\u003e12\n\u003c/span\u003e\u003cspan\u003e13\n\u003c/span\u003e\u003cspan\u003e14\n\u003c/span\u003e\u003cspan\u003e15\n\u003c/span\u003e\u003cspan\u003e16\n\u003c/span\u003e\u003cspan\u003e17\n\u003c/span\u003e\u003cspan\u003e18\n\u003c/span\u003e\u003cspan\u003e19\n\u003c/span\u003e\u003cspan\u003e20\n\u003c/span\u003e\u003cspan\u003e21\n\u003c/span\u003e\u003cspan\u003e22\n\u003c/span\u003e\u003cspan\u003e23\n\u003c/span\u003e\u003cspan\u003e24\n\u003c/span\u003e\u003cspan\u003e25\n\u003c/span\u003e\u003cspan\u003e26\n\u003c/span\u003e\u003cspan\u003e27\n\u003c/span\u003e\u003cspan\u003e28\n\u003c/span\u003e\u003cspan\u003e29\n\u003c/span\u003e\u003cspan\u003e30\n\u003c/span\u003e\u003cspan\u003e31\n\u003c/span\u003e\u003cspan\u003e32\n\u003c/span\u003e\u003cspan\u003e33\n\u003c/span\u003e\u003cspan\u003e34\n\u003c/span\u003e\u003cspan\u003e35\n\u003c/span\u003e\u003cspan\u003e36\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode data-lang=\"diff\"\u003e\u003cspan\u003e\u003cspan\u003e pub fn search\u0026lt;const P: usize, const PF: bool\u0026gt;(\u0026amp;self, qb: \u0026amp;[u32; P]) -\u0026gt; [u32; P] {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let offsets = self\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .offsets\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .iter()\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .map(|o| unsafe { self.tree.as_ptr().add(*o) })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         .collect_vec();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o0 = offsets[0];\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let mut k: [usize; P] = qb.map(|q| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e-                 4 * (16 - self.overlap)         * (q as usize \u0026gt;\u0026gt; self.shift)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e+        unsafe { 4 * *self.prefix_map.get_unchecked(q as usize \u0026gt;\u0026gt; self.shift) }\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003c/span\u003e     });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let q_simd = qb.map(|q| Simd::\u0026lt;u32, 8\u0026gt;::splat(q));\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     if let Some([o1, o2]) = offsets.array_windows().next() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             let jump_to = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             k[i] = k[i] * self.l1 + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             prefetch_ptr(unsafe { o2.byte_add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     for [o, o2] in offsets[1..].array_windows() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         for i in 0..P {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             k[i] = k[i] * (B + 1) + jump_to;\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e             prefetch_ptr(unsafe { o2.byte_add(k[i]) });\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     let o = offsets.last().unwrap();\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     from_fn(|i| {\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         let idx = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat(q_simd[i]);\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e     })\n\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eCode Snippet 33:\u003c/span\u003e\nIn code, the only thing that changes compared to the previous overlapping version is that instead of computing the start index linearly (and adapting the element layout accordingly), we use the \u003ccode\u003eprefix_map\u003c/code\u003e to jump directly to the right place in the packed tree representation.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/24-map.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/24-map.svg\" alt=\"Figure 30: As long as there are more elements than parts and the tree has at least two layers, the space overhead of this representation is close to (1/16) again.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 30: \u003c/span\u003eAs long as there are more elements than parts and the tree has at least two layers, the space overhead of this representation is close to (1/16) again.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eAlthough memory usage is now similar to the unpartitioned version, queries for\nlarge inputs are slightly slower than those previous layouts due to the\nadditional index required.\u003c/p\u003e\u003cp\u003eWe can also again do the interleaving queries. These are slightly faster for\nsmall inputs, and around as fast as interleaving was without the partitioning.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/25-map-interleave.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/25-map-interleave.svg\" alt=\"Figure 31: Prefix-map index with interleaving queries on random data.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 31: \u003c/span\u003ePrefix-map index with interleaving queries on random data.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eOn human data, we see that the partitioned index is a bit faster in L1 and L2,\nand consistently saves the time of roughly one layer in L3. For larger indices,\nperformance is still very similar to not using partitioning at all.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/25-map-interleave-human.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/25-map-interleave-human.svg\" alt=\"Figure 32: Prefix-map with interleaving on human data.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 32: \u003c/span\u003ePrefix-map with interleaving on human data.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"prefix-summary\"\u003e\u003cspan\u003e5.7\u003c/span\u003e Summary\n\u003ca href=\"#prefix-summary\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/27-summary.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/27-summary.svg\" alt=\"Figure 33: Summary of partitioning results. Overall, it seems that partitioning does not provide when we already interleave queries.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 33: \u003c/span\u003eSummary of partitioning results. Overall, it seems that partitioning does not provide when we already interleave queries.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"multi-threaded-comparison\"\u003e\u003cspan\u003e6\u003c/span\u003e Multi-threaded comparison\n\u003ca href=\"#multi-threaded-comparison\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cfigure\u003e\u003ca href=\"https://curiouscoding.nl/ox-hugo/28-threads.svg\"\u003e\u003cimg src=\"https://curiouscoding.nl/ox-hugo/28-threads.svg\" alt=\"Figure 34: When using 6 threads, runtime goes down from 27ns to 7ns. Given that the speedup is less than 4x, we are now bottlenecked by total RAM throughput, and indeed methods that are slower for a single thread also reach near-optimal throughput now.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cp\u003e\u003cspan\u003eFigure 34: \u003c/span\u003eWhen using 6 threads, runtime goes down from \u003ccode\u003e27ns\u003c/code\u003e to \u003ccode\u003e7ns\u003c/code\u003e. Given that the speedup is less than 4x, we are now bottlenecked by total RAM throughput, and indeed methods that are slower for a single thread also reach near-optimal throughput now.\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"conclusion\"\u003e\u003cspan\u003e7\u003c/span\u003e Conclusion\n\u003ca href=\"#conclusion\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eAll together, we went from \u003ccode\u003e1150ns/query\u003c/code\u003e for binary search on 4GB input to\n\u003ccode\u003e27ns\u003c/code\u003e for the optimized S-tree with interleaved queries, over \u003ccode\u003e40x\u003c/code\u003e speedup!\nA large part of this improvement is due to \u003cstrong\u003ebatching\u003c/strong\u003e queries and \u003cstrong\u003eprefetching\u003c/strong\u003e\nupcoming nodes. To get even higher throughput, \u003cstrong\u003einterleaving\u003c/strong\u003e queries at different\nlevels helps to balance the CPU-bound part of the computation with the\nmemory-bound part, so that we get a higher overall throughput. Using a \u003cstrong\u003e15\nelements per node\u003c/strong\u003e instead of 16 also improves throughput somewhat, but doubles\nthe overhead of the data structure from 6.25% to 13.3%. For inputs that fit in\nL3 cache that‚Äôs fine and the speedup is worthwhile, while for larger inputs the\nspeed is memory-bound anyway, so that there is no speedup while the additional\nmemory requirements are somewhat large.\u003c/p\u003e\u003cp\u003eWe also looked into \u003cstrong\u003epartitioning\u003c/strong\u003e the data by prefix. While this does give some speedup,\nit turns out that on skewed input data, the benefits quickly\ndiminish since the tree either requires a lot of buffer space, or else requires\nan additional lookup to map each part to its location in the first level of the tree.\nIn the end, I‚Äôd say the additional complexity and dependency on the shape of\nthe input data of partitioning is not worth the speedup compared to simply using interleaved\nqueries directly.\u003c/p\u003e\u003ch2 id=\"future-work\"\u003e\u003cspan\u003e7.1\u003c/span\u003e Future work\n\u003ca href=\"#future-work\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003ch3 id=\"branchy-search\"\u003e\u003cspan\u003e7.1.1\u003c/span\u003e Branchy search\n\u003ca href=\"#branchy-search\"\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eAll methods we considered are \u003cem\u003ebranchless\u003c/em\u003e and use the exact same number of\niterations for each query. Especially in combination with partitioning, it may\nbe possible to handle the few large parts independently from the usual\nsmaller parts. That way we could answer most queries with slightly fewer\niterations.\u003c/p\u003e\u003cp\u003eOn the other hand, the layers saved would mostly be the quick lookups near the\nroot of the tree, and introducing branches to the code could possibly cause\nquite a bit of delay due to mispredictions.\u003c/p\u003e\u003ch3 id=\"interpolation-search\"\u003e\u003cspan\u003e7.1.2\u003c/span\u003e Interpolation search\n\u003ca href=\"#interpolation-search\"\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eAs we saw in the last plot above, total RAM throughput (rather than per-core\nthroughput) becomes a bottleneck once we‚Äôre using multiple threads.\nThus, the only way to improve total query throughput is to use strictly fewer RAM\naccesses per query.\nPrefix lookups won‚Äôt help, since they only replace the layers of the tree\nthat would otherwise fit in the cache. Instead, we could use \u003cem\u003einterpolation\nsearch\u003c/em\u003e (\u003ca href=\"https://en.wikipedia.org/wiki/Interpolation_search\"\u003ewikipedia\u003c/a\u003e), where the estimated position of a query \\(q\\) is linearly\ninterpolated between known positions of surrounding elements. On random data, this only takes\n\\(O(\\lg \\lg n)\\) iterations, rather than \\(O(\\lg n)\\) for binary search, and could\nsave some RAM accesses. On the\nother hand, when data is not random its worst case performance is \\(O(n)\\) rather\nthan the statically bounded \\(O(\\lg n)\\).\u003c/p\u003e\u003cp\u003eThe PLA-index (\u003ca href=\"#citeproc_bib_item_1\"\u003eAbrar and Medvedev 2024\u003c/a\u003e) also uses a single interpolation step in a\nprecisely constructed piece wise linear approximation. The error after the\napproximation is determined by some global upper bound, so that the number of remaining\nsearch steps can be bounded as well.\u003c/p\u003e\u003ch3 id=\"packing-data-smaller\"\u003e\u003cspan\u003e7.1.3\u003c/span\u003e Packing data smaller\n\u003ca href=\"#packing-data-smaller\"\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eAnother option to use the RAM lookups more efficiently would be to pack values\ninto 16 bits rather than the 32 bits we‚Äôve been using so far. Especially if we\nfirst do a 16 bit prefix lookup, we already know those bits anyway, so it would\nsuffice to only compare the last 16 bits of the query and values. This increases\nthe branching factor from 17 to 33, which reduces the number of layers of the\ntree by around 1.5 for inputs of 1GB.\u003c/p\u003e\u003ch3 id=\"returning-indices-in-original-data\"\u003e\u003cspan\u003e7.1.4\u003c/span\u003e Returning indices in original data\n\u003ca href=\"#returning-indices-in-original-data\"\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eFor various applications, it may be helpful to not only return the smallest\nvalue \\(\\geq q\\), but also the index in the original list of sorted values, for\nexample when storing an array with additional data for each item.\u003c/p\u003e\u003cp\u003eSince we use the S+ tree that stores all data in the bottom layer, this is\nmostly straightforward. The \u003cem\u003eprefix map\u003c/em\u003e partitioned tree also natively supports\nthis, while the other partitioned variants do not: they include buffer/padding\nelements in their bottom layer, and hence we would need to store and look up the position\noffset of each part separately.\u003c/p\u003e\u003ch3 id=\"range-queries\"\u003e\u003cspan\u003e7.1.5\u003c/span\u003e Range queries\n\u003ca href=\"#range-queries\"\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eWe could extend the current query methods to a version that return both the\nfirst value \\(\\geq q\\) and the first value \\(\u0026gt;q\\), so that the range of positions\ncorresponding to value \\(q\\) can be determined. In practice, the easiest way to do\nthis is by simply doubling the queries into \\(q\\) and \\(q+1\\). This will cause some\nCPU overhead in the initial layers, but the query execution will remain\nbranch-free. When \\(q\\) is not found or only occurs a few times, they will mostly\nfetch the same cache lines, so that memory is efficiently reused and the\nbandwidth can be used for other queries.\u003c/p\u003e\u003cp\u003eIn practice though, this seems only around 20% faster per individual query for 4GB input, so\naround 60% slower for a range than for a single query. For small inputs, the\nspeedup is less, and sometimes querying ranges is even more than twice slower\nthan individual random queries.\u003c/p\u003e\u003ch2 id=\"references\"\u003eReferences\n\u003ca href=\"#references\"\u003e\u003c/a\u003e\u003c/h2\u003e\u003cdiv\u003e\u003cp\u003e\u003ca id=\"citeproc_bib_item_1\"\u003e\u003c/a\u003eAbrar, Md. Hasin, and Paul Medvedev. 2024. ‚ÄúPla-Index: A K-Mer Index Exploiting Rank Curve Linearity.‚Äù Schloss Dagstuhl ‚Äì Leibniz-Zentrum f√ºr Informatik. \u003ca href=\"https://doi.org/10.4230/LIPICS.WABI.2024.13\"\u003ehttps://doi.org/10.4230/LIPICS.WABI.2024.13\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003ca id=\"citeproc_bib_item_2\"\u003e\u003c/a\u003eKhuong, Paul-Virak, and Pat Morin. 2017. ‚ÄúArray Layouts for Comparison-Based Searching.‚Äù \u003ci\u003eAcm Journal of Experimental Algorithmics\u003c/i\u003e 22 (May): 1‚Äì39. \u003ca href=\"https://doi.org/10.1145/3053370\"\u003ehttps://doi.org/10.1145/3053370\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/section\u003e\u003c/div\u003e",
  "readingTime": "79 min read",
  "publishedTime": "2024-12-18T00:00:00+01:00",
  "modifiedTime": "2024-12-18T00:00:00+01:00"
}
