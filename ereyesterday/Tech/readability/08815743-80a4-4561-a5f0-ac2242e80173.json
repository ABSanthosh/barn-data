{
  "id": "08815743-80a4-4561-a5f0-ac2242e80173",
  "title": "Apple hit with $1.2B lawsuit after killing controversial CSAM-detecting tool",
  "link": "https://arstechnica.com/tech-policy/2024/12/thousands-of-child-sex-abuse-victims-sue-apple-for-lax-csam-reporting/",
  "description": "Apple knowingly ignoring child porn is a \"never-ending nightmare,\" lawsuit says.",
  "author": "Ashley Belanger",
  "published": "Mon, 09 Dec 2024 20:14:10 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "Apple",
    "Policy",
    "apple",
    "child porn",
    "child pornography",
    "child sex abuse materials",
    "csam",
    "end to end encryption",
    "icloud",
    "iphone"
  ],
  "byline": "Ashley Belanger",
  "length": 1916,
  "excerpt": "Apple knowingly ignoring child porn is a “never-ending nightmare,” lawsuit says.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "When Apple devices are used to spread CSAM, it's a huge problem for survivors, who allegedly face a range of harms, including \"exposure to predators, sexual exploitation, dissociative behavior, withdrawal symptoms, social isolation, damage to body image and self-worth, increased risky behavior, and profound mental health issues, including but not limited to depression, anxiety, suicidal ideation, self-harm, insomnia, eating disorders, death, and other harmful effects.\" One survivor told The Times she \"lives in constant fear that someone might track her down and recognize her.\" Survivors suing have also incurred medical and other expenses due to Apple's inaction, the lawsuit alleged. And those expenses will keep piling up if the court battle drags on for years and Apple's practices remain unchanged. Apple could win, a lawyer and policy fellow at the Stanford Institute for Human-Centered Artificial Intelligence, Riana Pfefferkorn, told The Times, as survivors face \"significant hurdles\" seeking liability for mishandling content that Apple says Section 230 shields. And a win for survivors could \"backfire,\" Pfefferkorn suggested, if Apple proves that forced scanning of devices and services violates the Fourth Amendment. Survivors, some of whom own iPhones, think that Apple has a responsibility to protect them. In a press release, Margaret E. Mabie, a lawyer representing survivors, praised survivors for raising \"a call for justice and a demand for Apple to finally take responsibility and protect these victims.\" “Thousands of brave survivors are coming forward to demand accountability from one of the most successful technology companies on the planet,\" Mabie said. \"Apple has not only rejected helping these victims, it has advertised the fact that it does not detect child sex abuse material on its platform or devices thereby exponentially increasing the ongoing harm caused to these victims.\"",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-961402090-1024x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n          \n          \n\u003cp\u003eWhen Apple devices are used to spread CSAM, it\u0026#39;s a huge problem for survivors, who allegedly face a range of harms, including \u0026#34;exposure to predators, sexual exploitation, dissociative behavior, withdrawal symptoms, social isolation, damage to body image and self-worth, increased risky behavior, and profound mental health issues, including but not limited to depression, anxiety, suicidal ideation, self-harm, insomnia, eating disorders, death, and other harmful effects.\u0026#34; One survivor told The Times she \u0026#34;lives in constant fear that someone might track her down and recognize her.\u0026#34;\u003c/p\u003e\n\u003cp\u003eSurvivors suing have also incurred medical and other expenses due to Apple\u0026#39;s inaction, the lawsuit alleged. And those expenses will keep piling up if the court battle drags on for years and Apple\u0026#39;s practices remain unchanged.\u003c/p\u003e\n\u003cp\u003eApple could win, a lawyer and policy fellow at the Stanford Institute for Human-Centered Artificial Intelligence, Riana Pfefferkorn, told The Times, as survivors face \u0026#34;significant hurdles\u0026#34; seeking liability for mishandling content that Apple says Section 230 shields. And a win for survivors could \u0026#34;backfire,\u0026#34; Pfefferkorn suggested, if Apple proves that forced scanning of devices and services violates the Fourth Amendment.\u003c/p\u003e\n\u003cp\u003eSurvivors, some of whom own iPhones, think that Apple has a responsibility to protect them. In a press release, Margaret E. Mabie, a lawyer representing survivors, praised survivors for raising \u0026#34;a call for justice and a demand for Apple to finally take responsibility and protect these victims.\u0026#34;\u003c/p\u003e\n\u003cp\u003e“Thousands of brave survivors are coming forward to demand accountability from one of the most successful technology companies on the planet,\u0026#34; Mabie said. \u0026#34;Apple has not only rejected helping these victims, it has advertised the fact that it does not detect child sex abuse material on its platform or devices thereby exponentially increasing the ongoing harm caused to these victims.\u0026#34;\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2024-12-09T20:14:10Z",
  "modifiedTime": "2024-12-09T20:25:31Z"
}
