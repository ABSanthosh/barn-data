{
  "id": "3a65cbf3-72cf-4691-a568-ce9fff872203",
  "title": "ChatGPT advises women to ask for lower salaries, finds new study",
  "link": "https://thenextweb.com/news/chatgpt-advises-women-to-ask-for-lower-salaries-finds-new-study",
  "description": "A new study has found that large language models (LLMs) such as ChatGPT consistently advise women to ask for lower salaries than men, even when both have identical qualifications. The research was led by Ivan Yamshchikov, a professor of AI and robotics at the Technical University of Würzburg-Schweinfurt (THWS) in Germany. Yamshchikov, who also founded Pleias — a French–German startup building ethically trained language models for regulated industries — worked with his team to test five popular LLMs, including ChatGPT. They prompted each model with user profiles that differed only by gender but included the same education, experience, and job…This story continues at The Next Web",
  "author": "Siôn Geschwindt",
  "published": "Fri, 11 Jul 2025 12:33:25 +0000",
  "source": "https://thenextweb.com/feed/",
  "categories": [
    "Deep tech",
    "Artificial Intelligence",
    "Corporates and innovation"
  ],
  "byline": "Siôn Geschwindt",
  "length": 2721,
  "excerpt": "A new study has found that large language models (LLMs) like ChatGPT consistently advise women to ask for lower salaries than men.",
  "siteName": "TNW | Deep-Tech",
  "favicon": "https://next.tnwcdn.com/assets/img/favicon/favicon-194x194.png",
  "text": "A new study has found that large language models (LLMs) such as ChatGPT consistently advise women to ask for lower salaries than men, even when both have identical qualifications. The research was led by Ivan Yamshchikov, a professor of AI and robotics at the Technical University of Würzburg-Schweinfurt (THWS) in Germany. Yamshchikov, who also founded Pleias — a French–German startup building ethically trained language models for regulated industries — worked with his team to test five popular LLMs, including ChatGPT. They prompted each model with user profiles that differed only by gender but included the same education, experience, and job role. Then they asked the models to suggest a target salary for an upcoming negotiation. In one example, OpenAI’s ChatGPT O3 model was prompted to give advice to a female job applicant: Credit: Ivan Yamshchikov. In another, the researchers made the same prompt but for a male applicant: Credit: Ivan Yamshchikov. “The difference in the prompts is two letters, the difference in the ‘advice’ is $120K a year,” said Yamshchikov. The pay gap was most pronounced in law and medicine, followed by business administration and engineering. Only in the social sciences did the models offer near-identical advice for men and women. The researchers also tested how the models advised users on career choices, goal-setting, and even behavioural tips. Across the board, the LLMs responded differently based on the user’s gender, despite identical qualifications and prompts. Crucially, the models don’t disclaim their bias.   A recurring problem  This is far from the first time AI has been caught reflecting and reinforcing systemic bias. In 2018, Amazon scrapped an internal hiring tool after discovering that it systematically downgraded female candidates. Last year, a clinical machine learning model used to diagnose women’s health conditions was shown to underdiagnose women and Black patients, because it was trained on skewed datasets dominated by white men.  The researchers behind the THWS study argue that technical fixes alone won’t solve the problem. What’s needed, they say, are clear ethical standards, independent review processes, and greater transparency in how these models are developed and deployed. As generative AI becomes a go-to source for everything from mental health advice to career planning, the stakes are only growing. If unchecked, the illusion of objectivity could become one of AI’s most dangerous traits. Get the TNW newsletter Get the most important tech news in your inbox each week.",
  "image": "https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1\u0026fit=1280%2C640\u0026url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2023%2F09%2FUntitled-design-15-5.jpg\u0026signature=8585e3414299d131a523e588f691d33b",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                \u003carticle id=\"articleOutput\"\u003e\n                                                                        \u003cdiv\u003e\n                                \u003cfigure\u003e\n                                    \u003cimg alt=\"ChatGPT advises women to ask for lower salaries, finds new study\" src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2023%2F09%2FUntitled-design-15-5.jpg\u0026amp;signature=12059e187c242b29437c38eecb870e58\" sizes=\"(max-width: 1023px) 100vw\n                                                   868px\" srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2023%2F09%2FUntitled-design-15-5.jpg\u0026amp;signature=7bb63a685f844f5dc449e72d794eb75f 576w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2023%2F09%2FUntitled-design-15-5.jpg\u0026amp;signature=4442d114f285415bc6238765d26ab29d 1152w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2023%2F09%2FUntitled-design-15-5.jpg\u0026amp;signature=12059e187c242b29437c38eecb870e58 1280w\" data-src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2023%2F09%2FUntitled-design-15-5.jpg\u0026amp;signature=12059e187c242b29437c38eecb870e58\" data-srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2023%2F09%2FUntitled-design-15-5.jpg\u0026amp;signature=7bb63a685f844f5dc449e72d794eb75f 576w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2023%2F09%2FUntitled-design-15-5.jpg\u0026amp;signature=4442d114f285415bc6238765d26ab29d 1152w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2023%2F09%2FUntitled-design-15-5.jpg\u0026amp;signature=12059e187c242b29437c38eecb870e58 1280w\"/\u003e\n\n                                    \n\n                                                                    \u003c/figure\u003e\n                            \u003c/div\u003e\n                        \n                                                    \n                            \n                                            \n                    \n                    \n\n                    \n                    \u003cdiv\u003e\n                        \u003cdiv id=\"article-main-content\"\u003e\n                            \u003cp data-start=\"152\" data-end=\"327\"\u003eA new study has found that large language models (LLMs) such as ChatGPT consistently advise women to ask for lower salaries than men, even when both have identical qualifications.\u003c/p\u003e\n\u003cp data-start=\"329\" data-end=\"668\"\u003eThe research was led by Ivan Yamshchikov, a professor of AI and robotics at the Technical University of Würzburg-Schweinfurt (THWS) in Germany. Yamshchikov, who also founded Pleias — a French–German startup building ethically trained language models for regulated industries — worked with his team to test five popular LLMs, including ChatGPT.\u003c/p\u003e\n\u003cp data-start=\"670\" data-end=\"884\"\u003eThey prompted each model with user profiles that differed only by gender but included the same education, experience, and job role. Then they asked the models to suggest a target salary for an upcoming negotiation.\u003c/p\u003e\n\u003cp data-start=\"886\" data-end=\"1058\"\u003eIn one example, OpenAI’s ChatGPT O3 model was prompted to give advice to a female job applicant:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" alt=\"\" width=\"1640\" height=\"1133\" data-src=\"https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2025/07/screenshot_2025-07-11_at_08.19.47.png\" src=\"https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2025/07/screenshot_2025-07-11_at_08.19.47.png\"/\u003e\u003cfigcaption\u003eCredit: Ivan Yamshchikov.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eIn another, the researchers made the same prompt but for a male applicant:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" alt=\"\" width=\"1630\" height=\"1138\" data-src=\"https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2025/07/screenshot_2025-07-11_at_08.20.00.png\" src=\"https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2025/07/screenshot_2025-07-11_at_08.20.00.png\"/\u003e\u003cfigcaption\u003eCredit: Ivan Yamshchikov.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cspan\u003e“The difference in the prompts is two letters, the difference in the ‘advice’ is $120K a year,” said \u003c/span\u003e\u003cspan\u003eYamshchikov.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe pay gap was most pronounced in law and medicine, followed by business administration and engineering. Only in the social sciences did the models offer near-identical advice for men and women.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe researchers also tested how the models advised users on career choices, goal-setting, and even behavioural tips. Across the board, the LLMs responded differently based on the user’s gender, despite identical qualifications and prompts. \u003c/span\u003e\u003cspan\u003eCrucially, the models don’t disclaim their bias.  \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cb\u003eA recurring problem \u003c/b\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eThis is far from the first time AI has been caught reflecting and reinforcing systemic bias. In 2018, Amazon scrapped an internal hiring tool after discovering that it systematically \u003c/span\u003e\u003ca href=\"https://www.bbc.com/news/technology-45809919\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003edowngraded\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e female candidates. Last year, a clinical machine learning model used to diagnose women’s health conditions was shown to \u003c/span\u003e\u003ca href=\"https://ojs.aaai.org/index.php/AIES/article/view/31748/33915\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cspan\u003eunderdiagnose women and Black patients\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, because it was trained on skewed datasets dominated by white men. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe researchers behind the THWS study argue that technical fixes alone won’t solve the problem. What’s needed, they say, are clear ethical standards, independent review processes, and greater transparency in how these models are developed and deployed.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs generative AI becomes a go-to source for everything from mental health advice to career planning, the stakes are only growing. If unchecked, the illusion of objectivity could become one of AI’s most dangerous traits.\u003c/span\u003e\u003c/p\u003e\n\n                        \u003c/div\u003e\n\n                        \n\n                        \u003cdiv id=\"nl-container\"\u003e\n                                                        \u003ch2\u003eGet the TNW newsletter\u003c/h2\u003e\n                            \u003cp\u003eGet the most important tech news in your inbox each week.\u003c/p\u003e\n                            \n                        \u003c/div\u003e\n\n                        \n                        \n                        \n\n                        \n                    \u003c/div\u003e\n                    \n\n                    \n                \u003c/article\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-07-11T12:33:25Z",
  "modifiedTime": "2025-07-11T13:32:34Z"
}
