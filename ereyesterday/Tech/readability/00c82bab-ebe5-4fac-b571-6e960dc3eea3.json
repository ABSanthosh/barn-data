{
  "id": "00c82bab-ebe5-4fac-b571-6e960dc3eea3",
  "title": "OpenAI rolls back update that made ChatGPT a sycophantic mess",
  "link": "https://arstechnica.com/ai/2025/04/openai-rolls-back-update-that-made-chatgpt-a-sycophantic-mess/",
  "description": "OpenAI CEO Sam Altman says the super-positive update to GPT-4o is being pulled.",
  "author": "Ryan Whitwam",
  "published": "Tue, 29 Apr 2025 21:15:37 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Artificial Intelligence",
    "ChatGPT",
    "openai"
  ],
  "byline": "Ryan Whitwam",
  "length": 2300,
  "excerpt": "OpenAI CEO Sam Altman says the super-positive update to GPT-4o is being pulled.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "In search of good vibes OpenAI, along with competitors like Google and Anthropic, is trying to build chatbots that people want to chat with. So, designing the model's apparent personality to be positive and supportive makes sense—people are less likely to use an AI that comes off as harsh or dismissive. For lack of a better word, it's increasingly about vibemarking. When Google revealed Gemini 2.5, the team crowed about how the model topped the LM Arena leaderboard, which lets people choose between two different model outputs in a blinded test. The models people like more end up at the top of the list, suggesting they are more pleasant to use. Of course, people can like outputs for different reasons—maybe one is more technically accurate, or the layout is easier to read. But overall, people like models that make them feel good. The same is true of OpenAI's internal model tuning work, it would seem. An example of ChatGPT's overzealous praise. Credit: /u/Talvy An example of ChatGPT's overzealous praise. Credit: /u/Talvy It's possible this pursuit of good vibes is pushing models to display more sycophantic behaviors, which is a problem. Anthropic's Alex Albert has cited this as a \"toxic feedback loop.\" An AI chatbot telling you that you're a world-class genius who sees the unseen might not be damaging if you're just brainstorming. However, the model's unending praise can lead people who are using AI to plan business ventures or, heaven forbid, enact sweeping tariffs, to be fooled into thinking they've stumbled onto something important. In reality, the model has just become so sycophantic that it loves everything. The unending pursuit of engagement has been a detriment to numerous products in the Internet era, and it seems generative AI is not immune. OpenAI's GPT-4o update is a testament to that, but hopefully, this can serve as a reminder for the developers of generative AI that good vibes are not all that matters.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift_3-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n          \n          \n\u003ch2\u003eIn search of good vibes\u003c/h2\u003e\n\u003cp\u003eOpenAI, along with competitors like Google and Anthropic, is trying to build chatbots that people want to chat with. So, designing the model\u0026#39;s apparent personality to be positive and supportive makes sense—people are less likely to use an AI that comes off as harsh or dismissive. For lack of a better word, it\u0026#39;s increasingly about \u003cem\u003e\u003ca href=\"https://arstechnica.com/information-technology/2024/07/the-first-gpt-4-class-ai-model-anyone-can-download-has-arrived-llama-405b/\"\u003evibemarking\u003c/a\u003e\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eWhen Google \u003ca href=\"https://arstechnica.com/ai/2025/03/google-says-the-new-gemini-2-5-pro-model-is-its-smartest-ai-yet/\"\u003erevealed Gemini 2.5\u003c/a\u003e, the team crowed about how the model topped the LM Arena leaderboard, which lets people choose between two different model outputs in a blinded test. The models people like more end up at the top of the list, suggesting they are more pleasant to use. Of course, people can like outputs for different reasons—maybe one is more technically accurate, or the layout is easier to read. But overall, people like models that make them feel good. The same is true of OpenAI\u0026#39;s internal model tuning work, it would seem.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cdiv\u003e\n            \u003cp\u003e\u003ca data-pswp-width=\"1124\" data-pswp-height=\"682\" data-pswp-srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg.jpg 1124w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg-640x388.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg-1024x621.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg-768x466.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg-980x595.jpg 980w\" data-cropped=\"false\" href=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg.jpg\" target=\"_blank\"\u003e\n              \u003cimg width=\"1124\" height=\"682\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg.jpg\" alt=\"\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg.jpg 1124w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg-640x388.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg-1024x621.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg-768x466.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/04/openAI-omg-980x595.jpg 980w\" sizes=\"auto, (max-width: 1124px) 100vw, 1124px\"/\u003e\n            \u003c/a\u003e\u003c/p\u003e\u003cdiv id=\"caption-2092129\"\u003e\u003cp\u003e\n              An example of ChatGPT\u0026#39;s overzealous praise.\n                              \u003c/p\u003e\u003cp\u003e\n                  Credit:\n                                      /u/Talvy\n                                  \u003c/p\u003e\n                          \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      An example of ChatGPT\u0026#39;s overzealous praise.\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          /u/Talvy\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eIt\u0026#39;s possible this pursuit of good vibes is pushing models to display more sycophantic behaviors, which is a problem. Anthropic\u0026#39;s Alex Albert has cited this as a \u0026#34;\u003ca href=\"https://x.com/alexalbert__/status/1916878483390869612\"\u003etoxic feedback loop\u003c/a\u003e.\u0026#34; An AI chatbot telling you that you\u0026#39;re a world-class genius who sees the unseen might not be damaging if you\u0026#39;re just brainstorming. However, the model\u0026#39;s unending praise can lead people who are using AI to plan business ventures or, heaven forbid, \u003ca href=\"https://arstechnica.com/tech-policy/2025/04/critics-suspect-trumps-weird-tariff-math-came-from-chatbots/\"\u003eenact sweeping tariffs\u003c/a\u003e, to be fooled into thinking they\u0026#39;ve stumbled onto something important. In reality, the model has just become so sycophantic that it loves everything.\u003c/p\u003e\n\u003cp\u003eThe unending pursuit of engagement has been a detriment to numerous products in the Internet era, and it seems generative AI is not immune. OpenAI\u0026#39;s GPT-4o update is a testament to that, but hopefully, this can serve as a reminder for the developers of generative AI that good vibes are not all that matters.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2025-04-29T21:15:37Z",
  "modifiedTime": "2025-04-29T21:48:37Z"
}
