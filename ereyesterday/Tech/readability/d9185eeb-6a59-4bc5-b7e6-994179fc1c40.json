{
  "id": "d9185eeb-6a59-4bc5-b7e6-994179fc1c40",
  "title": "Asking Chatbots For Short Answers Can Increase Hallucinations, Study Finds",
  "link": "https://slashdot.org/story/25/05/12/2114214/asking-chatbots-for-short-answers-can-increase-hallucinations-study-finds?utm_source=rss1.0mainlinkanon\u0026utm_medium=feed",
  "description": "Requesting concise answers from AI chatbots significantly increases their tendency to hallucinate, according to new research from Paris-based AI testing company Giskard. The study found that leading models -- including OpenAI's GPT-4o, Mistral Large, and Anthropic's Claude 3.7 Sonnet -- sacrifice factual accuracy when instructed to keep responses short. \"When forced to keep it short, models consistently choose brevity over accuracy,\" Giskard researchers noted, explaining that models lack sufficient \"space\" to acknowledge false premises and offer proper rebuttals. Even seemingly innocuous prompts like \"be concise\" can undermine a model's ability to debunk misinformation. Read more of this story at Slashdot.",
  "author": "msmash",
  "published": "2025-05-13T00:42:00+00:00",
  "source": "http://rss.slashdot.org/Slashdot/slashdotMain",
  "categories": [
    "ai"
  ],
  "byline": "",
  "length": 680,
  "excerpt": "Requesting concise answers from AI chatbots significantly increases their tendency to hallucinate, according to new research from Paris-based AI testing company Giskard. The study found that leading models -- including OpenAI's GPT-4o, Mistral Large, and Anthropic's Claude 3.7 Sonnet -- sacrifice fa...",
  "siteName": "",
  "favicon": "",
  "text": "Requesting concise answers from AI chatbots significantly increases their tendency to hallucinate, according to new research from Paris-based AI testing company Giskard. The study found that leading models -- including OpenAI's GPT-4o, Mistral Large, and Anthropic's Claude 3.7 Sonnet -- sacrifice factual accuracy when instructed to keep responses short. \"When forced to keep it short, models consistently choose brevity over accuracy,\" Giskard researchers noted, explaining that models lack sufficient \"space\" to acknowledge false premises and offer proper rebuttals. Even seemingly innocuous prompts like \"be concise\" can undermine a model's ability to debunk misinformation.",
  "image": "https://a.fsdn.com/sd/topics/ai_64.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"fhbody-177471609\"\u003e\u003cp\u003e\n\t\t\t\n\t\t \t\n\t\t\t\tRequesting concise answers from AI chatbots \u003ca href=\"https://techcrunch.com/2025/05/08/asking-chatbots-for-short-answers-can-increase-hallucinations-study-finds/\"\u003esignificantly increases their tendency to hallucinate\u003c/a\u003e, according to new research from Paris-based AI testing company Giskard. The study found that leading models -- including OpenAI\u0026#39;s GPT-4o, Mistral Large, and Anthropic\u0026#39;s Claude 3.7 Sonnet -- sacrifice factual accuracy when instructed to keep responses short.\u003c/p\u003e\u003cp\u003e \n\n\u0026#34;When forced to keep it short, models consistently choose brevity over accuracy,\u0026#34; Giskard researchers noted, explaining that models lack sufficient \u0026#34;space\u0026#34; to acknowledge false premises and offer proper rebuttals. Even seemingly innocuous prompts like \u0026#34;be concise\u0026#34; can undermine a model\u0026#39;s ability to debunk misinformation.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "Less than 1 min",
  "publishedTime": null,
  "modifiedTime": null
}
