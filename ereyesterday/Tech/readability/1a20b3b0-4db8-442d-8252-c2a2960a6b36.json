{
  "id": "1a20b3b0-4db8-442d-8252-c2a2960a6b36",
  "title": "Hunyuan3D 2.0 ‚Äì High-Resolution 3D Assets Generation",
  "link": "https://github.com/Tencent/Hunyuan3D-2",
  "description": "Comments",
  "author": "",
  "published": "Tue, 21 Jan 2025 22:42:12 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Tencent",
  "length": 5065,
  "excerpt": "High-Resolution 3D Assets Generation with Large Scale Hunyuan3D Diffusion Models. - Tencent/Hunyuan3D-2",
  "siteName": "GitHub",
  "favicon": "https://github.com/fluidicon.png",
  "text": "‰∏≠ÊñáÈòÖËØª ‚Äú Living out everyone‚Äôs imagination on creating and manipulating 3D assets.‚Äù üî• News Jan 21, 2025: üí¨ Enjoy exciting 3D generation on our website Hunyuan3D Studio! Jan 21, 2025: üí¨ Release inference code and pretrained models of Hunyuan3D 2.0. Jan 21, 2025: üí¨ Release Hunyuan3D 2.0. Please give it a try via huggingface space our official site! Abstract We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets. This system includes two foundation components: a large-scale shape generation model - Hunyuan3D-DiT, and a large-scale texture synthesis model - Hunyuan3D-Paint. The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly aligns with a given condition image, laying a solid foundation for downstream applications. The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant texture maps for either generated or hand-crafted meshes. Furthermore, we build Hunyuan3D-Studio - a versatile, user-friendly production platform that simplifies the re-creation process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models, including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and e.t.c. ‚òØÔ∏è Hunyuan3D 2.0 Architecture Hunyuan3D 2.0 features a two-stage generation pipeline, starting with the creation of a bare mesh, followed by the synthesis of a texture map for that mesh. This strategy is effective for decoupling the difficulties of shape and texture generation and also provides flexibility for texturing either generated or handcrafted meshes. Performance We have evaluated Hunyuan3D 2.0 with other open-source as well as close-source 3d-generation methods. The numerical results indicate that Hunyuan3D 2.0 surpasses all baselines in the quality of generated textured 3D assets and the condition following ability. Model CMMD(‚¨á) FID_CLIP(‚¨á) FID(‚¨á) CLIP-score(‚¨Ü) Top Open-source Model1 3.591 54.639 289.287 0.787 Top Close-source Model1 3.600 55.866 305.922 0.779 Top Close-source Model2 3.368 49.744 294.628 0.806 Top Close-source Model3 3.218 51.574 295.691 0.799 Hunyuan3D 2.0 3.193 49.165 282.429 0.809 Generation results of Hunyuan3D 2.0: Pretrained Models Model Date Huggingface Hunyuan3D-DiT-v2-0 2025-01-21 Download Hunyuan3D-Paint-v2-0 2025-01-21 Download ü§ó Get Started with Hunyuan3D 2.0 You may follow the next steps to use Hunyuan3D 2.0 via code or the Gradio App. Install Requirements Please install Pytorch via the official site. Then install the other requirements via pip install -r requirements.txt # for texture cd hy3dgen/texgen/custom_rasterizer python3 setup.py install cd hy3dgen/texgen/differentiable_renderer bash compile_mesh_painter.sh API Usage We designed a diffusers-like API to use our shape generation model - Hunyuan3D-DiT and texture synthesis model - Hunyuan3D-Paint. You could assess Hunyuan3D-DiT via: from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2') mesh = pipeline(image='assets/demo.png')[0] The output mesh is a trimesh object, which you could save to glb/obj (or other format) file. For Hunyuan3D-Paint, do the following: from hy3dgen.texgen import Hunyuan3DPaintPipeline from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline # let's generate a mesh first pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2') mesh = pipeline(image='assets/demo.png')[0] pipeline = Hunyuan3DPaintPipeline.from_pretrained('tencent/Hunyuan3D-2') mesh = pipeline(mesh, image='assets/demo.png') Please visit minimal_demo.py for more advanced usage, such as text to 3D and texture generation for handcrafted mesh. Gradio App You could also host a Gradio App in your own computer via: Don't forget to visit Hunyuan3D for quick use, if you don't want to host yourself. üìë Open-Source Plan Inference Code Model Checkpoints Technical Report ComfyUI TensorRT Version üîó BibTeX If you found this repository helpful, please cite our reports: @misc{hunyuan3d22025tencent, title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation}, author={Tencent Hunyuan3D Team}, year={2025}, } @misc{yang2024tencent, title={Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation}, year={2024}, author={Tencent Hunyuan3D Team}, eprint={2411.02293}, archivePrefix={arXiv}, primaryClass={cs.CV} } Acknowledgements We would like to thank the contributors to the DINOv2, Stable Diffusion, FLUX, diffusers, HuggingFace, CraftsMan3D, and Michelangelo repositories, for their open research and exploration. Star History",
  "image": "https://opengraph.githubassets.com/0d390f2c337148f65264ee03a63182d64ffd3cd32ad11400d3d9fbe9f951e734/Tencent/Hunyuan3D-2",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-hpc=\"true\"\u003e\u003carticle itemprop=\"text\"\u003e\u003cp dir=\"auto\"\u003e\u003ca href=\"https://github.com/Tencent/Hunyuan3D-2/blob/main/README_zh_cn.md\"\u003e‰∏≠ÊñáÈòÖËØª\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e \n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/images/teaser.jpg\"\u003e\u003cimg src=\"https://github.com/Tencent/Hunyuan3D-2/raw/main/assets/images/teaser.jpg\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://3d.hunyuan.tencent.com\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/febf402ad9aa71393c9530c20f10eb5ff99f8cc9762cec7d49a8cb675dd0433c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4f6666696369616c253230536974652d626c61636b2e7376673f6c6f676f3d686f6d6570616765\" height=\"22px\" data-canonical-src=\"https://img.shields.io/badge/Official%20Site-black.svg?logo=homepage\"/\u003e\u003c/a\u003e\n  \u003ca href=\"https://huggingface.co/spaces/tencent/Hunyuan3D-2\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b01df6333acfa3279bfe495dce3acab0be0c1027bb6dfe321a2b2d4eaf9dc6c2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323044656d6f2d3237366362342e737667\" height=\"22px\" data-canonical-src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg\"/\u003e\u003c/a\u003e\n  \u003ca href=\"https://huggingface.co/tencent/Hunyuan3D-2\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/dbbc817c0d92c742d5b07f0d54d890f96e5cea25b2124c12b9acd58ecc7b0cb7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372532304d6f64656c732d6439363930322e737667\" height=\"22px\" data-canonical-src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg\"/\u003e\u003c/a\u003e\n  \u003ca href=\"https://3d-models.hunyuan.tencent.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/30e5ac4827a0cef8589c5402c9020003d93136b3fffd99d6d91b57bf5cdbea15/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f506167652d6262386132652e7376673f6c6f676f3d676974687562\" height=\"22px\" data-canonical-src=\"https://img.shields.io/badge/Page-bb8a2e.svg?logo=github\"/\u003e\u003c/a\u003e\n  \u003ca href=\"https://discord.gg/GuaWYwzKbX\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d25736b4edde958cb4922599c6ba056a053637b482d31c24f41393eefdc73a4b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d77686974652e7376673f6c6f676f3d646973636f7264\" height=\"22px\" data-canonical-src=\"https://img.shields.io/badge/Discord-white.svg?logo=discord\"/\u003e\u003c/a\u003e\n  \u003ca href=\"https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/report/Tencent_Hunyuan3D_2_0.pdf\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/cf668b2c3aaa6c4d256eddd435c21642bba70e38b4baae0719946859f6efe677/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5265706f72742d6235323132662e7376673f6c6f676f3d6172786976\" height=\"22px\" data-canonical-src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\n\u003cp dir=\"auto\"\u003e\n‚Äú Living out everyone‚Äôs imagination on creating and manipulating 3D assets.‚Äù\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eüî• News\u003c/h2\u003e\u003ca id=\"user-content--news\" aria-label=\"Permalink: üî• News\" href=\"#-news\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eJan 21, 2025: üí¨ Enjoy exciting 3D generation on our website \u003ca href=\"https://3d.hunyuan.tencent.com\" rel=\"nofollow\"\u003eHunyuan3D Studio\u003c/a\u003e!\u003c/li\u003e\n\u003cli\u003eJan 21, 2025: üí¨ Release inference code and pretrained models of \u003ca href=\"https://huggingface.co/tencent/Hunyuan3D-2\" rel=\"nofollow\"\u003eHunyuan3D 2.0\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eJan 21, 2025: üí¨ Release Hunyuan3D 2.0. Please give it a try via \u003ca href=\"https://huggingface.co/spaces/tencent/Hunyuan3D-2\" rel=\"nofollow\"\u003ehuggingface space\u003c/a\u003e our \u003ca href=\"https://3d.hunyuan.tencent.com\" rel=\"nofollow\"\u003eofficial site\u003c/a\u003e!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003e\u003cstrong\u003eAbstract\u003c/strong\u003e\u003c/h2\u003e\u003ca id=\"user-content-abstract\" aria-label=\"Permalink: Abstract\" href=\"#abstract\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWe present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets.\nThis system includes two foundation components: a large-scale shape generation model - Hunyuan3D-DiT, and a large-scale\ntexture synthesis model - Hunyuan3D-Paint.\nThe shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly\naligns with a given condition image, laying a solid foundation for downstream applications.\nThe texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant\ntexture maps for either generated or hand-crafted meshes.\nFurthermore, we build Hunyuan3D-Studio - a versatile, user-friendly production platform that simplifies the re-creation\nprocess of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes\nefficiently.\nWe systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models,\nincluding the open-source models and closed-source models in geometry details, condition alignment, texture quality, and\ne.t.c.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/images/system.jpg\"\u003e\u003cimg src=\"https://github.com/Tencent/Hunyuan3D-2/raw/main/assets/images/system.jpg\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003e‚òØÔ∏è \u003cstrong\u003eHunyuan3D 2.0\u003c/strong\u003e\u003c/h2\u003e\u003ca id=\"user-content-Ô∏è-hunyuan3d-20\" aria-label=\"Permalink: ‚òØÔ∏è Hunyuan3D 2.0\" href=\"#Ô∏è-hunyuan3d-20\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eArchitecture\u003c/h3\u003e\u003ca id=\"user-content-architecture\" aria-label=\"Permalink: Architecture\" href=\"#architecture\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eHunyuan3D 2.0 features a two-stage generation pipeline, starting with the creation of a bare mesh, followed by the\nsynthesis of a texture map for that mesh. This strategy is effective for decoupling the difficulties of shape and\ntexture generation and also provides flexibility for texturing either generated or handcrafted meshes.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/images/arch.jpg\"\u003e\u003cimg src=\"https://github.com/Tencent/Hunyuan3D-2/raw/main/assets/images/arch.jpg\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003ePerformance\u003c/h3\u003e\u003ca id=\"user-content-performance\" aria-label=\"Permalink: Performance\" href=\"#performance\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWe have evaluated Hunyuan3D 2.0 with other open-source as well as close-source 3d-generation methods.\nThe numerical results indicate that Hunyuan3D 2.0 surpasses all baselines in the quality of generated textured 3D assets\nand the condition following ability.\u003c/p\u003e\n\u003cmarkdown-accessiblity-table\u003e\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eModel\u003c/th\u003e\n\u003cth\u003eCMMD(‚¨á)\u003c/th\u003e\n\u003cth\u003eFID_CLIP(‚¨á)\u003c/th\u003e\n\u003cth\u003eFID(‚¨á)\u003c/th\u003e\n\u003cth\u003eCLIP-score(‚¨Ü)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eTop Open-source Model1\u003c/td\u003e\n\u003ctd\u003e3.591\u003c/td\u003e\n\u003ctd\u003e54.639\u003c/td\u003e\n\u003ctd\u003e289.287\u003c/td\u003e\n\u003ctd\u003e0.787\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTop Close-source Model1\u003c/td\u003e\n\u003ctd\u003e3.600\u003c/td\u003e\n\u003ctd\u003e55.866\u003c/td\u003e\n\u003ctd\u003e305.922\u003c/td\u003e\n\u003ctd\u003e0.779\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTop Close-source Model2\u003c/td\u003e\n\u003ctd\u003e3.368\u003c/td\u003e\n\u003ctd\u003e49.744\u003c/td\u003e\n\u003ctd\u003e294.628\u003c/td\u003e\n\u003ctd\u003e0.806\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTop Close-source Model3\u003c/td\u003e\n\u003ctd\u003e3.218\u003c/td\u003e\n\u003ctd\u003e51.574\u003c/td\u003e\n\u003ctd\u003e295.691\u003c/td\u003e\n\u003ctd\u003e0.799\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eHunyuan3D 2.0\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e3.193\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e49.165\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e282.429\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e0.809\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\u003c/markdown-accessiblity-table\u003e\n\u003cp dir=\"auto\"\u003eGeneration results of Hunyuan3D 2.0:\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/images/e2e-1.gif\"\u003e\u003cimg src=\"https://github.com/Tencent/Hunyuan3D-2/raw/main/assets/images/e2e-1.gif\" height=\"250\" data-animated-image=\"\"/\u003e\u003c/a\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/images/e2e-2.gif\"\u003e\u003cimg src=\"https://github.com/Tencent/Hunyuan3D-2/raw/main/assets/images/e2e-2.gif\" height=\"250\" data-animated-image=\"\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003ePretrained Models\u003c/h3\u003e\u003ca id=\"user-content-pretrained-models\" aria-label=\"Permalink: Pretrained Models\" href=\"#pretrained-models\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cmarkdown-accessiblity-table\u003e\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eModel\u003c/th\u003e\n\u003cth\u003eDate\u003c/th\u003e\n\u003cth\u003eHuggingface\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eHunyuan3D-DiT-v2-0\u003c/td\u003e\n\u003ctd\u003e2025-01-21\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/tencent/Hunyuan3D-2\" rel=\"nofollow\"\u003eDownload\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eHunyuan3D-Paint-v2-0\u003c/td\u003e\n\u003ctd\u003e2025-01-21\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/tencent/Hunyuan3D-2\" rel=\"nofollow\"\u003eDownload\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\u003c/markdown-accessiblity-table\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eü§ó Get Started with Hunyuan3D 2.0\u003c/h2\u003e\u003ca id=\"user-content--get-started-with-hunyuan3d-20\" aria-label=\"Permalink: ü§ó Get Started with Hunyuan3D 2.0\" href=\"#-get-started-with-hunyuan3d-20\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eYou may follow the next steps to use Hunyuan3D 2.0 via code or the Gradio App.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eInstall Requirements\u003c/h3\u003e\u003ca id=\"user-content-install-requirements\" aria-label=\"Permalink: Install Requirements\" href=\"#install-requirements\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003ePlease install Pytorch via the \u003ca href=\"https://pytorch.org/\" rel=\"nofollow\"\u003eofficial\u003c/a\u003e site. Then install the other requirements via\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"pip install -r requirements.txt\n# for texture\ncd hy3dgen/texgen/custom_rasterizer\npython3 setup.py install\ncd hy3dgen/texgen/differentiable_renderer\nbash compile_mesh_painter.sh\"\u003e\u003cpre\u003epip install -r requirements.txt\n\u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e for texture\u003c/span\u003e\n\u003cspan\u003ecd\u003c/span\u003e hy3dgen/texgen/custom_rasterizer\npython3 setup.py install\n\u003cspan\u003ecd\u003c/span\u003e hy3dgen/texgen/differentiable_renderer\nbash compile_mesh_painter.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eAPI Usage\u003c/h3\u003e\u003ca id=\"user-content-api-usage\" aria-label=\"Permalink: API Usage\" href=\"#api-usage\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWe designed a diffusers-like API to use our shape generation model - Hunyuan3D-DiT and texture synthesis model -\nHunyuan3D-Paint.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eYou could assess \u003cstrong\u003eHunyuan3D-DiT\u003c/strong\u003e via:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\n\npipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(\u0026#39;tencent/Hunyuan3D-2\u0026#39;)\nmesh = pipeline(image=\u0026#39;assets/demo.png\u0026#39;)[0]\"\u003e\u003cpre\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ehy3dgen\u003c/span\u003e.\u003cspan\u003eshapegen\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eHunyuan3DDiTFlowMatchingPipeline\u003c/span\u003e\n\n\u003cspan\u003epipeline\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eHunyuan3DDiTFlowMatchingPipeline\u003c/span\u003e.\u003cspan\u003efrom_pretrained\u003c/span\u003e(\u003cspan\u003e\u0026#39;tencent/Hunyuan3D-2\u0026#39;\u003c/span\u003e)\n\u003cspan\u003emesh\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003epipeline\u003c/span\u003e(\u003cspan\u003eimage\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#39;assets/demo.png\u0026#39;\u003c/span\u003e)[\u003cspan\u003e0\u003c/span\u003e]\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eThe output mesh is a \u003ca href=\"https://trimesh.org/trimesh.html\" rel=\"nofollow\"\u003etrimesh object\u003c/a\u003e, which you could save to glb/obj (or other\nformat) file.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eFor \u003cstrong\u003eHunyuan3D-Paint\u003c/strong\u003e, do the following:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"from hy3dgen.texgen import Hunyuan3DPaintPipeline\nfrom hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\n\n# let\u0026#39;s generate a mesh first\npipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(\u0026#39;tencent/Hunyuan3D-2\u0026#39;)\nmesh = pipeline(image=\u0026#39;assets/demo.png\u0026#39;)[0]\n\npipeline = Hunyuan3DPaintPipeline.from_pretrained(\u0026#39;tencent/Hunyuan3D-2\u0026#39;)\nmesh = pipeline(mesh, image=\u0026#39;assets/demo.png\u0026#39;)\"\u003e\u003cpre\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ehy3dgen\u003c/span\u003e.\u003cspan\u003etexgen\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eHunyuan3DPaintPipeline\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ehy3dgen\u003c/span\u003e.\u003cspan\u003eshapegen\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eHunyuan3DDiTFlowMatchingPipeline\u003c/span\u003e\n\n\u003cspan\u003e# let\u0026#39;s generate a mesh first\u003c/span\u003e\n\u003cspan\u003epipeline\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eHunyuan3DDiTFlowMatchingPipeline\u003c/span\u003e.\u003cspan\u003efrom_pretrained\u003c/span\u003e(\u003cspan\u003e\u0026#39;tencent/Hunyuan3D-2\u0026#39;\u003c/span\u003e)\n\u003cspan\u003emesh\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003epipeline\u003c/span\u003e(\u003cspan\u003eimage\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#39;assets/demo.png\u0026#39;\u003c/span\u003e)[\u003cspan\u003e0\u003c/span\u003e]\n\n\u003cspan\u003epipeline\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eHunyuan3DPaintPipeline\u003c/span\u003e.\u003cspan\u003efrom_pretrained\u003c/span\u003e(\u003cspan\u003e\u0026#39;tencent/Hunyuan3D-2\u0026#39;\u003c/span\u003e)\n\u003cspan\u003emesh\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003epipeline\u003c/span\u003e(\u003cspan\u003emesh\u003c/span\u003e, \u003cspan\u003eimage\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#39;assets/demo.png\u0026#39;\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003ePlease visit \u003ca href=\"https://github.com/Tencent/Hunyuan3D-2/blob/main/minimal_demo.py\"\u003eminimal_demo.py\u003c/a\u003e for more advanced usage, such as \u003cstrong\u003etext to 3D\u003c/strong\u003e and \u003cstrong\u003etexture generation\nfor handcrafted mesh\u003c/strong\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eGradio App\u003c/h3\u003e\u003ca id=\"user-content-gradio-app\" aria-label=\"Permalink: Gradio App\" href=\"#gradio-app\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eYou could also host a \u003ca href=\"https://www.gradio.app/\" rel=\"nofollow\"\u003eGradio\u003c/a\u003e App in your own computer via:\u003c/p\u003e\n\n\u003cp dir=\"auto\"\u003eDon\u0026#39;t forget to visit \u003ca href=\"https://3d.hunyuan.tencent.com\" rel=\"nofollow\"\u003eHunyuan3D\u003c/a\u003e for quick use, if you don\u0026#39;t want to host yourself.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eüìë Open-Source Plan\u003c/h2\u003e\u003ca id=\"user-content--open-source-plan\" aria-label=\"Permalink: üìë Open-Source Plan\" href=\"#-open-source-plan\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e Inference Code\u003c/li\u003e\n\u003cli\u003e Model Checkpoints\u003c/li\u003e\n\u003cli\u003e Technical Report\u003c/li\u003e\n\u003cli\u003e ComfyUI\u003c/li\u003e\n\u003cli\u003e TensorRT Version\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eüîó BibTeX\u003c/h2\u003e\u003ca id=\"user-content--bibtex\" aria-label=\"Permalink: üîó BibTeX\" href=\"#-bibtex\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eIf you found this repository helpful, please cite our reports:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{hunyuan3d22025tencent,\n    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2025},\n}\n\n@misc{yang2024tencent,\n    title={Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},\n    year={2024},\n    author={Tencent Hunyuan3D Team},\n    eprint={2411.02293},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\"\u003e\u003cpre\u003e\u003cspan\u003e@misc\u003c/span\u003e{\u003cspan\u003ehunyuan3d22025tencent\u003c/span\u003e,\n    \u003cspan\u003etitle\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003eHunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003eauthor\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003eTencent Hunyuan3D Team\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003eyear\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003e2025\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n}\n\n\u003cspan\u003e@misc\u003c/span\u003e{\u003cspan\u003eyang2024tencent\u003c/span\u003e,\n    \u003cspan\u003etitle\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003eTencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003eyear\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003e2024\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003eauthor\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003eTencent Hunyuan3D Team\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003eeprint\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003e2411.02293\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003earchivePrefix\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003earXiv\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n    \u003cspan\u003eprimaryClass\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003ecs.CV\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eAcknowledgements\u003c/h2\u003e\u003ca id=\"user-content-acknowledgements\" aria-label=\"Permalink: Acknowledgements\" href=\"#acknowledgements\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWe would like to thank the contributors to\nthe \u003ca href=\"https://github.com/facebookresearch/dinov2\"\u003eDINOv2\u003c/a\u003e, \u003ca href=\"https://github.com/Stability-AI/stablediffusion\"\u003eStable Diffusion\u003c/a\u003e, \u003ca href=\"https://github.com/black-forest-labs/flux\"\u003eFLUX\u003c/a\u003e, \u003ca href=\"https://github.com/huggingface/diffusers\"\u003ediffusers\u003c/a\u003e, \u003ca href=\"https://huggingface.co\" rel=\"nofollow\"\u003eHuggingFace\u003c/a\u003e, \u003ca href=\"https://github.com/wyysf-98/CraftsMan3D\"\u003eCraftsMan3D\u003c/a\u003e, and \u003ca href=\"https://github.com/NeuralCarver/Michelangelo/tree/main\"\u003eMichelangelo\u003c/a\u003e repositories, for their open research and exploration.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eStar History\u003c/h2\u003e\u003ca id=\"user-content-star-history\" aria-label=\"Permalink: Star History\" href=\"#star-history\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ca href=\"https://star-history.com/#Tencent/Hunyuan3D-2\u0026amp;Date\" rel=\"nofollow\"\u003e\n \u003cthemed-picture data-catalyst-inline=\"true\"\u003e\u003cpicture\u003e\n   \u003csource media=\"(prefers-color-scheme: dark)\" srcset=\"https://camo.githubusercontent.com/e768710d4e8b6d1209558f1887f847f85fb4d0ec7c5359bf4622a8aebafd9dfe/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d54656e63656e742f48756e7975616e33442d3226747970653d44617465267468656d653d6461726b\" data-canonical-src=\"https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2\u0026amp;type=Date\u0026amp;theme=dark\"/\u003e\n   \u003csource media=\"(prefers-color-scheme: light)\" srcset=\"https://camo.githubusercontent.com/087278d49d3be0181fd0bf1eb124b5d5199b5e28450487147b5a7837b8391476/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d54656e63656e742f48756e7975616e33442d3226747970653d44617465\" data-canonical-src=\"https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2\u0026amp;type=Date\"/\u003e\n   \u003cimg alt=\"Star History Chart\" src=\"https://camo.githubusercontent.com/087278d49d3be0181fd0bf1eb124b5d5199b5e28450487147b5a7837b8391476/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d54656e63656e742f48756e7975616e33442d3226747970653d44617465\" data-canonical-src=\"https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2\u0026amp;type=Date\"/\u003e\n \u003c/picture\u003e\u003c/themed-picture\u003e\n\u003c/a\u003e\n\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
