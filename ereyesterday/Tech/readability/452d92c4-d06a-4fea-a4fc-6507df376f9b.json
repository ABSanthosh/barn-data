{
  "id": "452d92c4-d06a-4fea-a4fc-6507df376f9b",
  "title": "AI PCs Aren't Good at AI: The CPU Beats the NPU",
  "link": "https://github.com/usefulsensors/qc_npu_benchmark",
  "description": "Comments",
  "author": "",
  "published": "Wed, 16 Oct 2024 19:44:02 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "usefulsensors",
  "length": 9603,
  "excerpt": "Code sample showing how to run and benchmark models on Qualcomm's Window PCs - usefulsensors/qc_npu_benchmark",
  "siteName": "GitHub",
  "favicon": "https://github.com/fluidicon.png",
  "text": "Benchmarking Qualcomm's NPU on the Microsoft Surface Tablet TL;DR - We see 1.3% of Qualcomm's NPU 45 Teraops/s claim when benchmarking Windows AI PCs Introduction Installation Python Cmake Visual Studio Pip Packages Benchmark Running Understanding the Output What the Benchmark Measures Possible Confounding Factors Compute Bound Power Settings Model Topology Configuration Errors Onnx Framework Interpreting the Results Introduction Microsoft now offers Surface tablets that run Windows on a Qualcomm Arm-based SoC. These are marketed as AI PCs, due to their ability to run machine learning models faster and more efficiently than other systems. We are fans of Qualcomm's hardware, and its NPU in particular, so we've invested a lot of time and resources into porting our third-party app to this plaform. Unfortunately there aren't many code examples or benchmarks available to demonstrate how to achieve fast results as an external developer, so we've put together a small standalone project to show the performance we're seeing. It's significantly below what we'd hoped for, so we're publishing this benchmark to see if we can get ideas on how to achieve lower latency. I'm hopeful there will be software changes, either at the application, framework, or driver level, that will improve these results in the future, since I've seen the underlying hardware perform very effectively on other platforms like Android. Installation Python We're using Python to run our test scripts, and on Windows there are several ways to install the language. As of October 2nd, 2024, the Python available on the Microsoft Store doesn't support the Arm architecture, and so it's not suitable for running the packages we need to access Qualcomm's NPU. Instead, you should use the official Python dot org installer. For the results reported here I used version 3.11.9. Cmake We'll also need the cmake build tool to compile Onnx (since prebuilt packages aren't yet available for Windows on Arm). To do this I ran the following command from a Powershell: Visual Studio The build process also requires Visual Studio for the compiler. Download Visual Studio Community Edition (not Code!) from visualstudio.microsoft.com/downloads/. During the installation you will be prompted to select Workload from several options: select Desktop C++ Development checkbox then press install. Pip Packages You can install all the required Python packages by running the following from within this folder: py -m pip install -r requirements.txt This includes a couple of custom packages. The first is my branch of Onnx, which has a fix for compiling using the official py launcher backported to Onnx version 1.16, since the Qualcomm Onnx Runtime doesn't work with newer Onnx versions (giving an Unsupported model IR version error). I also grab a nightly build of Qualcomm's Onnx Runtime package. If you want to install a more recent version, there's a list here. Benchmark Running To execute the benchmark, run: Understanding the Output The Onnx runtime initially generates a lot of log spam, including: Error in cpuinfo: Unknown chip model name 'Snapdragon(R) X 12-core X1E80100 @ 3.40 GHz'. Please add new Windows on Arm SoC/chip support to arm/windows/init.c! unknown Qualcomm CPU part 0x1 ignored and Starting stage: Finalizing Graph Sequence Completed stage: Finalizing Graph Sequence (115919 us) Starting stage: Completion Completed stage: Completion (1025 us) After all those messages, you should see the actual benchmark results at the end, something like this: ************ Benchmark Results ************ NPU quantized compute, float I/O accuracy difference is 0.0100 NPU quantized compute and I/O accuracy difference is 0.0060 CPU took 8.42ms, 821,141,860,688 ops per second NPU (quantized compute, float I/O) took 30.63ms, 225,667,671,183 ops per second NPU (quantized compute and I/O) took 12.05ms, 573,475,650,364 ops per second The first two lines confirm that the numerical results of the operations match between the CPU and the NPU. The final three show the latency of the three approaches to running a simple model. The latency is the wall time it took to execute the model from start to finish, and the ops per second is calculated from that latency to indicate the equivalent computational throughput. In this example, we see the CPU is capable of running 821 billion ops/second (821 Gigaops), the first NPU approach gives us 225 Gigaops, and the second 573 Gigaops. What the Benchmark Measures This benchmark is designed to resemble some real world models we depend on, running 6 large matrix multiplications that are similar to the most time-consuming layers in transformer models like OpenAI's Whisper. The shapes are (6, 1500, 256) X (6, 256, 1500), producing a (6, 1500, 1500) result. The model we running consists of a single MatMul node with two inputs and one output. The models are created on the fly using the Onnx model framework, and then fed into the Onnx runtime. The control model is a pure float version that runs entirely on the CPU. The NPU mostly requires quantized models to run effectively (though it has limited support for float16). The first approach we took to quantization used the official ORT quantize_static() method. For convenience this leaves the input and output tensors in 32-bit float and performs runtime conversions at the start and end of the graph so that the rest of the computation happens in eight-bit. Unfortunately we discovered that the conversion operations as implemented on the NPU were extremely slow, much slower than the main matrix multiplication in fact. You can see the results in the npu_quant_profile.csv file in this repository, with conversions taking over 75% of the time. To work around this, we constructed an equivalent model graph programmatically with eight-bit inputs and outputs This is the second \"quantized compute and I/O\" approach mentioned in the results. This is usually around three times faster than the float I/O version, and profiling shows most of the time is going on the matrix multiplication, as we'd hope. Possible Confounding Factors There are a lot of variables involved in measuring performance. Here are some of the assumptions we've made: Compute Bound Modern transformer models are based around large matrix multiplications, unlike older convolutional models. One potential issue is that accelerators could become memory bound if the layers start to resemble matrix times vectors, since that doesn't allow reuse of many of the weights, and performance becomes bottle necked on fetching values from DRAM. We've tried to avoid that by making both the input matrices more square, so that tiling and reuse should be possible. The original matrices from the tiny Whisper model had a k dimension of only 64, so in case that was too small we bumped it up to 256 in this benchmark to give as much room for SIMD optimizations as possible. Power Settings Windows has a lot of different configuration options around energy usage, so we tried to ensure that all of the settings were on \"Best Performance\" and that we ran the benchmark with the tablet connected to mains power. There's also a session option on the Qualcomm Onnx Runtime, htp_performance_mode, that we set to sustained_high_performance, since that seemed to give the lowest overall latency in our experiments. Model Topology We wanted to create a graph of operations that reflected modern AI models, but was simple enough to easily interpret. We could have added multiple layers, or used convolutions, or static weights, but settled for a single matrix multiplication operation with dynamic inputs, since that reflected the transformer architectures that are widely used for LLMs and other modern models. Configuration Errors It's possible that the way we build and run our models causes them to fall off the fast path of the drivers or accelerator implementation. For example, we're using unsigned eight-bit quantization, with qdq elements in the graph. We've attempted to follow best practice from the documentation, but we'd welcome ways to improve performance, especially since these would improve the performance of our actual applications. Onnx Framework There are multiple different ways to access AI acceleration on Windows. We looked at DirectML, but it only seems to support GPU access. OpenVino doesn't run on our Arm hardware, as far as we can tell. We've seen similar performance results to those shown here using the Qualcomm QNN SDK directly. TensorFlow Lite isn't supported on Windows for Arm. From this research and our experiments, Onnx is supported by both Microsoft and Qualcomm, and seems to be the best framework to use to get accelerated performance from the NPU, but we're interested in learning if other APIs would be more appropriate. Interpreting the Results The results shown here are current as of October 2nd, 2024, when running on a Microsoft Surface Pro 11th Edition, with a Snapdragon(R) X 12-core X1E80100 clocked at 3.40 GHz. The first obvious thing is that the NPU results, even without float conversion, are slower than the CPU. This is not ideal for an accelerator, even though it could still potentially offer energy or sustained performance advantages that make it worth using. The second conclusion is that the measured performance of 573 billion operations per second is only 1.3% of the 45 trillion ops/s that the marketing material promises. By contrast, running the same model on an Nvidia Geforce RTX 4080 Laptop GPU runs in 3.2ms, an equivalent of 2,160 billion operations per second, almost four times the throughput.",
  "image": "https://opengraph.githubassets.com/a44b158fc2dc57f895442a4055ea9bdc7a95940f98f1b5e6bc345b76c3b43ce2/usefulsensors/qc_npu_benchmark",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-hpc=\"true\"\u003e\u003carticle itemprop=\"text\"\u003e\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eBenchmarking Qualcomm\u0026#39;s NPU on the Microsoft Surface Tablet\u003c/h2\u003e\u003ca id=\"user-content-benchmarking-qualcomms-npu-on-the-microsoft-surface-tablet\" aria-label=\"Permalink: Benchmarking Qualcomm\u0026#39;s NPU on the Microsoft Surface Tablet\" href=\"#benchmarking-qualcomms-npu-on-the-microsoft-surface-tablet\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eTL;DR - We see 1.3% of Qualcomm\u0026#39;s NPU 45 Teraops/s claim when benchmarking Windows AI PCs\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#introduction\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#installation\"\u003eInstallation\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#python\"\u003ePython\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#cmake\"\u003eCmake\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#visual-studio\"\u003eVisual Studio\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#pip-packages\"\u003ePip Packages\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#benchmark\"\u003eBenchmark\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#running\"\u003eRunning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#understanding-the-output\"\u003eUnderstanding the Output\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#what-the-benchmark-measures\"\u003eWhat the Benchmark Measures\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#possible-confounding-factors\"\u003ePossible Confounding Factors\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#compute-bound\"\u003eCompute Bound\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#power-settings\"\u003ePower Settings\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#model-topology\"\u003eModel Topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#configuration-errors\"\u003eConfiguration Errors\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#onnx-framework\"\u003eOnnx Framework\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#interpreting-the-results\"\u003eInterpreting the Results\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eIntroduction\u003c/h2\u003e\u003ca id=\"user-content-introduction\" aria-label=\"Permalink: Introduction\" href=\"#introduction\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eMicrosoft now offers Surface tablets that run Windows on a Qualcomm Arm-based\nSoC. These are marketed as AI PCs, due to their ability to run machine learning\nmodels faster and more efficiently than other systems. We are fans of\nQualcomm\u0026#39;s hardware, and its NPU in particular, so we\u0026#39;ve invested a lot of time\nand resources into porting our third-party app to this plaform.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eUnfortunately there  aren\u0026#39;t many code examples or benchmarks available to\ndemonstrate how to achieve fast results as an external developer, so we\u0026#39;ve put\ntogether a small standalone project to show the performance we\u0026#39;re seeing. It\u0026#39;s\nsignificantly below what we\u0026#39;d hoped for, so we\u0026#39;re publishing this benchmark to\nsee if we can get ideas on how to achieve lower latency. I\u0026#39;m hopeful there will\nbe software changes, either at the application, framework, or driver level,\nthat will improve these results in the future, since I\u0026#39;ve seen the underlying\nhardware perform very effectively on other platforms like Android.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eInstallation\u003c/h2\u003e\u003ca id=\"user-content-installation\" aria-label=\"Permalink: Installation\" href=\"#installation\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003ePython\u003c/h3\u003e\u003ca id=\"user-content-python\" aria-label=\"Permalink: Python\" href=\"#python\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWe\u0026#39;re using Python to run our test scripts, and on Windows \u003ca href=\"https://docs.python.org/3/using/windows.html\" rel=\"nofollow\"\u003ethere are several ways to install the language\u003c/a\u003e.\nAs of October 2nd, 2024, the Python available on the Microsoft Store doesn\u0026#39;t\nsupport the Arm architecture, and so it\u0026#39;s not suitable for running the packages\nwe need to access Qualcomm\u0026#39;s NPU. Instead, you should use \u003ca href=\"https://www.python.org/downloads/\" rel=\"nofollow\"\u003ethe official Python dot org installer\u003c/a\u003e.\nFor the results reported here I used \u003ca href=\"https://www.python.org/ftp/python/3.11.9/python-3.11.9-arm64.exe\" rel=\"nofollow\"\u003eversion 3.11.9\u003c/a\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eCmake\u003c/h3\u003e\u003ca id=\"user-content-cmake\" aria-label=\"Permalink: Cmake\" href=\"#cmake\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWe\u0026#39;ll also need the cmake build tool to compile Onnx (since prebuilt packages\naren\u0026#39;t yet available for Windows on Arm). To do this I ran the following\ncommand from a Powershell:\u003c/p\u003e\n\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eVisual Studio\u003c/h3\u003e\u003ca id=\"user-content-visual-studio\" aria-label=\"Permalink: Visual Studio\" href=\"#visual-studio\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe build process also requires Visual Studio for the compiler. Download Visual\nStudio Community Edition (not Code!) from \u003ca href=\"https://visualstudio.microsoft.com/downloads/\" rel=\"nofollow\"\u003evisualstudio.microsoft.com/downloads/\u003c/a\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eDuring the installation you will be prompted to select \u003ccode\u003eWorkload\u003c/code\u003e from several options: select \u003ccode\u003eDesktop C++ Development\u003c/code\u003e checkbox then press install.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003ePip Packages\u003c/h3\u003e\u003ca id=\"user-content-pip-packages\" aria-label=\"Permalink: Pip Packages\" href=\"#pip-packages\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eYou can install all the required Python packages by running the following\nfrom within this folder:\u003c/p\u003e\n\u003cdiv data-snippet-clipboard-copy-content=\"py -m pip install -r requirements.txt\"\u003e\u003cpre\u003e\u003ccode\u003epy -m pip install -r requirements.txt\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eThis includes a couple of custom packages. The first is \u003ca href=\"https://github.com/petewarden/onnx/tree/rel-1.16.2\"\u003emy branch of Onnx\u003c/a\u003e,\nwhich has \u003ca href=\"https://github.com/onnx/onnx/pull/6407\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/onnx/onnx/pull/6407/hovercard\"\u003ea fix for compiling using the official \u003ccode\u003epy\u003c/code\u003e launcher\u003c/a\u003e\nbackported to Onnx version 1.16, since the Qualcomm Onnx Runtime doesn\u0026#39;t work\nwith newer Onnx versions (giving an \u003ccode\u003eUnsupported model IR version\u003c/code\u003e error).\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eI also grab \u003ca href=\"https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/7982ae20-ed19-4a35-a362-a96ac99897b7/pypi/download/ort-nightly-qnn/1.20.dev20240928001/ort_nightly_qnn-1.20.0.dev20240928001-cp311-cp311-win_arm64.whl#sha256=3b12e3882d1afadf66c2349b2a167dfcbb9ae7a332dc98e0fd51c101d34ddf6e\" rel=\"nofollow\"\u003ea nightly build\u003c/a\u003e\nof \u003ca href=\"https://onnxruntime.ai/docs/execution-providers/QNN-ExecutionProvider.html\" rel=\"nofollow\"\u003eQualcomm\u0026#39;s Onnx Runtime package\u003c/a\u003e.\nIf you want to install a more recent version, there\u0026#39;s \u003ca href=\"https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ort-nightly-qnn/\" rel=\"nofollow\"\u003ea list here\u003c/a\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eBenchmark\u003c/h2\u003e\u003ca id=\"user-content-benchmark\" aria-label=\"Permalink: Benchmark\" href=\"#benchmark\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eRunning\u003c/h3\u003e\u003ca id=\"user-content-running\" aria-label=\"Permalink: Running\" href=\"#running\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eTo execute the benchmark, run:\u003c/p\u003e\n\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eUnderstanding the Output\u003c/h3\u003e\u003ca id=\"user-content-understanding-the-output\" aria-label=\"Permalink: Understanding the Output\" href=\"#understanding-the-output\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe Onnx runtime initially generates a lot of log spam, including:\u003c/p\u003e\n\u003cdiv data-snippet-clipboard-copy-content=\"Error in cpuinfo: Unknown chip model name \u0026#39;Snapdragon(R) X 12-core X1E80100 @ 3.40 GHz\u0026#39;.\nPlease add new Windows on Arm SoC/chip support to arm/windows/init.c!\nunknown Qualcomm CPU part 0x1 ignored\"\u003e\u003cpre\u003e\u003ccode\u003eError in cpuinfo: Unknown chip model name \u0026#39;Snapdragon(R) X 12-core X1E80100 @ 3.40 GHz\u0026#39;.\nPlease add new Windows on Arm SoC/chip support to arm/windows/init.c!\nunknown Qualcomm CPU part 0x1 ignored\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eand\u003c/p\u003e\n\u003cdiv data-snippet-clipboard-copy-content=\"Starting stage: Finalizing Graph Sequence\nCompleted stage: Finalizing Graph Sequence (115919 us)\nStarting stage: Completion\nCompleted stage: Completion (1025 us)\"\u003e\u003cpre\u003e\u003ccode\u003eStarting stage: Finalizing Graph Sequence\nCompleted stage: Finalizing Graph Sequence (115919 us)\nStarting stage: Completion\nCompleted stage: Completion (1025 us)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eAfter all those messages, you should see the actual benchmark\nresults at the end, something like this:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"************ Benchmark Results ************\nNPU quantized compute, float I/O accuracy difference is 0.0100\nNPU quantized compute and I/O accuracy difference is 0.0060\nCPU took 8.42ms, 821,141,860,688 ops per second\nNPU (quantized compute, float I/O) took 30.63ms, 225,667,671,183 ops per second\nNPU (quantized compute and I/O) took 12.05ms, 573,475,650,364 ops per second\"\u003e\u003cpre\u003e\u003cspan\u003e************\u003c/span\u003e Benchmark Results \u003cspan\u003e************\u003c/span\u003e\nNPU quantized compute, float I/O accuracy difference is 0.0100\nNPU quantized compute and I/O accuracy difference is 0.0060\nCPU took 8.42ms, 821,141,860,688 ops per second\nNPU (quantized compute, float I/O) took 30.63ms, 225,667,671,183 ops per second\nNPU (quantized compute and I/O) took 12.05ms, 573,475,650,364 ops per second\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eThe first two lines confirm that the numerical results of the operations match\nbetween the CPU and the NPU. The final three show the latency of the three\napproaches to running a simple model. The latency is the wall time it took to\nexecute the model from start to finish, and the ops per second is calculated\nfrom that latency to indicate the equivalent computational throughput.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eIn this example, we see the CPU is capable of running 821 billion ops/second\n(821 Gigaops), the first NPU approach gives us 225 Gigaops, and the second 573\nGigaops.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eWhat the Benchmark Measures\u003c/h3\u003e\u003ca id=\"user-content-what-the-benchmark-measures\" aria-label=\"Permalink: What the Benchmark Measures\" href=\"#what-the-benchmark-measures\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThis benchmark is designed to resemble some real world models we depend on,\nrunning 6 large matrix multiplications that are similar to the most\ntime-consuming layers in transformer models like OpenAI\u0026#39;s Whisper. The shapes\nare (6, 1500, 256) X (6, 256, 1500), producing a (6, 1500, 1500) result. The\nmodel we running consists of a single MatMul node with two inputs and one\noutput.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe models are created on the fly using the Onnx model framework, and then fed\ninto the Onnx runtime. The control model is a pure float version that runs\nentirely on the CPU.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe NPU mostly requires quantized models to run effectively (though it has\nlimited support for float16). The first approach we took to quantization used\n\u003ca href=\"https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html#static-quantization\" rel=\"nofollow\"\u003ethe official ORT \u003ccode\u003equantize_static()\u003c/code\u003e method\u003c/a\u003e.\nFor convenience this leaves the input and output tensors in 32-bit float and\nperforms runtime conversions at the start and end of the graph so that the rest\nof the computation happens in eight-bit.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eUnfortunately we discovered that the conversion operations as implemented on\nthe NPU were extremely slow, much slower than the main matrix multiplication\nin fact. You can see the results in the \u003ccode\u003enpu_quant_profile.csv\u003c/code\u003e file in this\nrepository, with conversions taking over 75% of the time.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eTo work around this, we constructed an equivalent model graph programmatically\nwith eight-bit inputs and outputs This is the second \u0026#34;quantized compute and\nI/O\u0026#34; approach mentioned in the results. This is usually around three times\nfaster than the float I/O version, and profiling shows most of the time is\ngoing on the matrix multiplication, as we\u0026#39;d hope.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003ePossible Confounding Factors\u003c/h3\u003e\u003ca id=\"user-content-possible-confounding-factors\" aria-label=\"Permalink: Possible Confounding Factors\" href=\"#possible-confounding-factors\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThere are a lot of variables involved in measuring performance. Here are some\nof the assumptions we\u0026#39;ve made:\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" dir=\"auto\"\u003eCompute Bound\u003c/h4\u003e\u003ca id=\"user-content-compute-bound\" aria-label=\"Permalink: Compute Bound\" href=\"#compute-bound\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eModern transformer models are based around large matrix multiplications, unlike\nolder convolutional models. One potential issue is that accelerators could\nbecome memory bound if the layers start to resemble matrix times vectors, since\nthat doesn\u0026#39;t allow reuse of many of the weights, and performance becomes bottle\nnecked on fetching values from DRAM. We\u0026#39;ve tried to avoid that by making both\nthe input matrices more square, so that tiling and reuse should be possible.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe original matrices from the tiny Whisper model had a k dimension of only 64,\nso in case that was too small we bumped it up to 256 in this benchmark to give\nas much room for SIMD optimizations as possible.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" dir=\"auto\"\u003ePower Settings\u003c/h4\u003e\u003ca id=\"user-content-power-settings\" aria-label=\"Permalink: Power Settings\" href=\"#power-settings\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWindows has a lot of different configuration options around energy usage, so we\ntried to ensure that all of the settings were on \u0026#34;Best Performance\u0026#34; and that we\nran the benchmark with the tablet connected to mains power. There\u0026#39;s also a\nsession option on the Qualcomm Onnx Runtime, \u003ccode\u003ehtp_performance_mode\u003c/code\u003e, that we\nset to \u003ccode\u003esustained_high_performance\u003c/code\u003e, since that seemed to give the lowest\noverall latency in our experiments.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" dir=\"auto\"\u003eModel Topology\u003c/h4\u003e\u003ca id=\"user-content-model-topology\" aria-label=\"Permalink: Model Topology\" href=\"#model-topology\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWe wanted to create a graph of operations that reflected modern AI models, but\nwas simple enough to easily interpret. We could have added multiple layers, or\nused convolutions, or static weights, but settled for a single matrix\nmultiplication operation with dynamic inputs, since that reflected the\ntransformer architectures that are widely used for LLMs and other modern\nmodels.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" dir=\"auto\"\u003eConfiguration Errors\u003c/h4\u003e\u003ca id=\"user-content-configuration-errors\" aria-label=\"Permalink: Configuration Errors\" href=\"#configuration-errors\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eIt\u0026#39;s possible that the way we build and run our models causes them to fall off\nthe fast path of the drivers or accelerator implementation. For example, we\u0026#39;re\nusing unsigned eight-bit quantization, with qdq elements in the graph. We\u0026#39;ve\nattempted to follow best practice from the documentation, but we\u0026#39;d welcome ways\nto improve performance, especially since these would improve the performance of\nour actual applications.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" dir=\"auto\"\u003eOnnx Framework\u003c/h4\u003e\u003ca id=\"user-content-onnx-framework\" aria-label=\"Permalink: Onnx Framework\" href=\"#onnx-framework\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThere are multiple different ways to access AI acceleration on Windows. We\nlooked at DirectML, but it only seems to support GPU access. OpenVino doesn\u0026#39;t\nrun on our Arm hardware, as far as we can tell. We\u0026#39;ve seen similar performance\nresults to those shown here using the \u003ca href=\"https://www.qualcomm.com/developer/software/neural-processing-sdk-for-ai\" rel=\"nofollow\"\u003eQualcomm QNN SDK\u003c/a\u003e\ndirectly. TensorFlow Lite isn\u0026#39;t supported on Windows for Arm. From this\nresearch and our experiments, Onnx is supported by both Microsoft and Qualcomm,\nand seems to be the best framework to use to get accelerated performance from\nthe NPU, but we\u0026#39;re interested in learning if other APIs would be more\nappropriate.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eInterpreting the Results\u003c/h2\u003e\u003ca id=\"user-content-interpreting-the-results\" aria-label=\"Permalink: Interpreting the Results\" href=\"#interpreting-the-results\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe results shown here are current as of October 2nd, 2024, when running on a\nMicrosoft Surface Pro 11th Edition, with a Snapdragon(R) X 12-core X1E80100\nclocked at 3.40 GHz. The first obvious thing is that the NPU results, even\nwithout float conversion, are slower than the CPU. This is not ideal for an\naccelerator, even though it could still potentially offer energy or sustained\nperformance advantages that make it worth using.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe second conclusion is that the measured performance of 573 billion\noperations per second is only 1.3% of the 45 trillion ops/s that \u003ca href=\"https://www.microsoft.com/en-us/surface/devices/surface-pro-11th-edition\" rel=\"nofollow\"\u003ethe marketing material\u003c/a\u003e\npromises.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eBy contrast, running the same model on an Nvidia Geforce RTX 4080 Laptop GPU\nruns in 3.2ms, an equivalent of 2,160 billion operations per second, almost\nfour times the throughput.\u003c/p\u003e\n\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": null,
  "modifiedTime": null
}
