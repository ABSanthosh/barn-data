{
  "id": "4e76447e-84bb-48c9-98a1-d49f9481e3a8",
  "title": "Are LLMs capable of non-verbal reasoning?",
  "link": "https://arstechnica.com/ai/2024/12/are-llms-capable-of-non-verbal-reasoning/",
  "description": "Processing in the \"latent space\" could help AI with tricky logical questions.",
  "author": "Kyle Orland",
  "published": "Thu, 12 Dec 2024 21:55:20 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI"
  ],
  "byline": "Kyle Orland",
  "length": 3032,
  "excerpt": "Processing in the “latent space” could help AI with tricky logical questions.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "In the researchers' COCONUT model (for Chain Of CONtinUous Thought), those kinds of hidden states are encoded as \"latent thoughts\" that replace the individual written steps in a logical sequence both during training and when processing a query. This avoids the need to convert to and from natural language for each step and \"frees the reasoning from being within the language space,\" the researchers write, leading to an optimized reasoning path that they term a \"continuous thought.\" Being more breadth-minded While doing logical processing in the latent space has some benefits for model efficiency, the more important finding is that this kind of model can \"encode multiple potential next steps simultaneously.\" Rather than having to pursue individual logical options fully and one by one (in a \"greedy\" sort of process), staying in the \"latent space\" allows for a kind of instant backtracking that the researchers compare to a breadth-first-search through a graph. This emergent, simultaneous processing property comes through in testing even though the model isn't explicitly trained to do so, the researchers write. \"While the model may not initially make the correct decision, it can maintain many possible options within the continuous thoughts and progressively eliminate incorrect paths through reasoning, guided by some implicit value functions,\" they write. A figure highlighting some of the ways different models can fail at certain types of logical inference. Credit: Training Large Language Models to Reason in a Continuous Latent Space That kind of multi-path reasoning didn't really improve COCONUT's accuracy over traditional chain-of-thought models on relatively straightforward tests of math reasoning (GSM8K) or general reasoning (ProntoQA). But the researchers found the model did comparatively well on a randomly generated set of ProntoQA-style queries involving complex and winding sets of logical conditions (e.g., \"every apple is a fruit, every fruit is food, etc.\") For these tasks, standard chain-of-thought reasoning models would often get stuck down dead-end paths of inference or even hallucinate completely made-up rules when trying to resolve the logical chain. Previous research has also shown that the \"verbalized\" logical steps output by these chain-of-thought models \"may actually utilize a different latent reasoning process\" than the one being shared. This new research joins a growing body of research looking to understand and exploit the way large language models work at the level of their underlying neural networks. And while that kind of research hasn't led to a huge breakthrough just yet, the researchers conclude that models pre-trained with these kinds of \"continuous thoughts\" from the get-go could \"enable models to generalize more effectively across a wider range of reasoning scenarios.\"",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n          \n          \n\u003cp\u003eIn the researchers\u0026#39; COCONUT model (for Chain Of CONtinUous Thought), those kinds of hidden states are encoded as \u0026#34;latent thoughts\u0026#34; that replace the individual written steps in a logical sequence both during training and when processing a query. This avoids the need to convert to and from natural language for each step and \u0026#34;frees the reasoning from being within the language space,\u0026#34; the researchers write, leading to an optimized reasoning path that they term a \u0026#34;continuous thought.\u0026#34;\u003c/p\u003e\n\u003ch2\u003eBeing more breadth-minded\u003c/h2\u003e\n\u003cp\u003eWhile doing logical processing in the latent space has some benefits for model efficiency, the more important finding is that this kind of model can \u0026#34;encode multiple potential next steps simultaneously.\u0026#34; Rather than having to pursue individual logical options fully and one by one (in \u003ca href=\"https://arxiv.org/abs/2210.01240\"\u003ea \u0026#34;greedy\u0026#34; sort of process\u003c/a\u003e), staying in the \u0026#34;latent space\u0026#34; allows for a kind of instant backtracking that the researchers compare to \u003ca href=\"https://www.geeksforgeeks.org/breadth-first-search-or-bfs-for-a-graph/\"\u003ea breadth-first-search through a graph\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis emergent, simultaneous processing property comes through in testing even though the model isn\u0026#39;t explicitly trained to do so, the researchers write. \u0026#34;While the model may not initially make the correct decision, it can maintain many possible options within the continuous thoughts and progressively eliminate incorrect paths through reasoning, guided by some implicit value functions,\u0026#34; they write.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"479\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/llm2-1024x479.png\" alt=\"\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/12/llm2-1024x479.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/llm2-640x300.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/llm2-768x359.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/llm2-980x459.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/llm2.png 1034w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\u003cp\u003e\n      A figure highlighting some of the ways different models can fail at certain types of logical inference.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://arxiv.org/abs/2412.06769\" target=\"_blank\"\u003e\n          \n          Training Large Language Models to Reason in a Continuous Latent Space\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eThat kind of multi-path reasoning didn\u0026#39;t really improve COCONUT\u0026#39;s accuracy over traditional chain-of-thought models on relatively straightforward tests of math reasoning (\u003ca href=\"https://paperswithcode.com/dataset/gsm8k\"\u003eGSM8K\u003c/a\u003e) or general reasoning (\u003ca href=\"https://github.com/asaparov/prontoqa\"\u003eProntoQA\u003c/a\u003e). But the researchers found the model did comparatively well on a randomly generated set of ProntoQA-style queries involving complex and winding sets of logical conditions (e.g., \u0026#34;every apple is a fruit, every fruit is food, etc.\u0026#34;)\u003c/p\u003e\n\u003cp\u003eFor these tasks, standard chain-of-thought reasoning models would often get stuck down dead-end paths of inference or even hallucinate completely made-up rules when trying to resolve the logical chain. Previous research has also shown that the \u0026#34;verbalized\u0026#34; logical steps output by these chain-of-thought models \u0026#34;may actually utilize a different latent reasoning process\u0026#34; than the one being shared.\u003c/p\u003e\n\u003cp\u003eThis new research joins a \u003ca href=\"https://arxiv.org/html/2409.14199v3\"\u003egrowing body\u003c/a\u003e of research looking to understand and exploit the way large language models work at the level of their underlying neural networks. And while that kind of research hasn\u0026#39;t led to a huge breakthrough just yet, the researchers conclude that models pre-trained with these kinds of \u0026#34;continuous thoughts\u0026#34; from the get-go could \u0026#34;enable models to generalize more effectively across a wider range of reasoning scenarios.\u0026#34;\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-12-12T21:55:20Z",
  "modifiedTime": "2024-12-12T21:55:20Z"
}
