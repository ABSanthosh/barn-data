{
  "id": "63a430af-f3c7-49a4-89eb-662099770d0e",
  "title": "FBI warns of ongoing scam that uses deepfake audio to impersonate government officials",
  "link": "https://arstechnica.com/security/2025/05/fbi-warns-of-ongoing-scam-that-uses-deepfake-audio-to-impersonate-government-officials/",
  "description": "Warning comes as the use of deepfakes in the wild are rising.",
  "author": "Dan Goodin",
  "published": "Thu, 15 May 2025 21:06:17 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Security",
    "deepfake",
    "fbi",
    "Federal Bureau of Investigation"
  ],
  "byline": "Dan Goodin",
  "length": 2338,
  "excerpt": "Warning comes as the use of deepfakes in the wild are rising.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "The FBI is warning people to be vigilant of an ongoing malicious messaging campaign that uses AI-generated voice audio to impersonate government officials in an attempt to trick recipients into clicking on links that can infect their computers. “Since April 2025, malicious actors have impersonated senior US officials to target individuals, many of whom are current or former senior US federal or state government officials and their contacts,” Thursday’s advisory from the bureau’s Internet Crime Complaint Center said. “If you receive a message claiming to be from a senior US official, do not assume it is authentic.” Think you can’t be fooled? Think again. The campaign's creators are sending AI-generated voice messages—better known as deepfakes—along with text messages “in an effort to establish rapport before gaining access to personal accounts,” FBI officials said. Deepfakes use AI to mimic the voice and speaking characteristics of a specific individual. The differences between the authentic and simulated speakers are often indistinguishable without trained analysis. Deepfake videos work similarly. One way to gain access to targets' devices is for the attacker to ask if the conversation can be continued on a separate messaging platform and then successfully convince the target to click on a malicious link under the guise that it will enable the alternate platform. The advisory provided no additional details about the campaign. The advisory comes amid a rise in reports of deepfaked audio and sometimes video used in fraud and espionage campaigns. Last year, password manager LastPass warned that it had been targeted in a sophisticated phishing campaign that used a combination of email, text messages, and voice calls to trick targets into divulging their master passwords. One part of the campaign included targeting a LastPass employee with a deepfake audio call that impersonated company CEO Karim Toubba. In a separate incident last year, a robocall campaign that encouraged New Hampshire Democrats to sit out the coming election used a deepfake of then-President Joe Biden’s voice. A Democratic consultant was later indicted in connection with the calls. The telco that transmitted the spoofed robocalls also agreed to pay a $1 million civil penalty for not authenticating the caller as required by FCC rules.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/deepfake.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                      \n                      \n          \u003cp\u003eThe FBI is warning people to be vigilant of an ongoing malicious messaging campaign that uses AI-generated voice audio to impersonate government officials in an attempt to trick recipients into clicking on links that can infect their computers.\u003c/p\u003e\n\u003cp\u003e“Since April 2025, malicious actors have impersonated senior US officials to target individuals, many of whom are current or former senior US federal or state government officials and their contacts,” \u003ca href=\"https://www.ic3.gov/PSA/2025/PSA250515\"\u003eThursday’s advisory\u003c/a\u003e from the bureau’s Internet Crime Complaint Center said. “If you receive a message claiming to be from a senior US official, do not assume it is authentic.”\u003c/p\u003e\n\u003ch2\u003eThink you can’t be fooled? Think again.\u003c/h2\u003e\n\u003cp\u003eThe campaign\u0026#39;s creators are sending AI-generated voice messages—better known as deepfakes—along with text messages “in an effort to establish rapport before gaining access to personal accounts,” FBI officials said. Deepfakes use AI to mimic the voice and speaking characteristics of a specific individual. The differences between the authentic and simulated speakers are often indistinguishable without trained analysis. Deepfake videos work similarly.\u003c/p\u003e\n\u003cp\u003eOne way to gain access to targets\u0026#39; devices is for the attacker to ask if the conversation can be continued on a separate messaging platform and then successfully convince the target to click on a malicious link under the guise that it will enable the alternate platform. The advisory provided no additional details about the campaign.\u003c/p\u003e\n\u003cp\u003eThe advisory comes amid a rise in reports of deepfaked audio and sometimes video used in fraud and espionage campaigns. Last year, password manager LastPass warned that it had been targeted in a sophisticated phishing campaign that used a combination of email, text messages, and voice calls to trick targets into divulging their master passwords. One part of the campaign included targeting a LastPass employee with a \u003ca href=\"https://arstechnica.com/security/2024/04/lastpass-users-targeted-in-phishing-attacks-good-enough-to-trick-even-the-savvy/\"\u003edeepfake audio call\u003c/a\u003e that impersonated company CEO Karim Toubba.\u003c/p\u003e\n\u003cp\u003eIn a separate incident last year, a robocall campaign that encouraged New Hampshire Democrats to sit out the coming election used a deepfake of \u003cspan\u003e\u003ca href=\"https://arstechnica.com/tech-policy/2024/01/robocall-with-artificial-joe-biden-voice-tells-democrats-not-to-vote/\" target=\"_blank\" rel=\"noopener\"\u003ethen-President\u003c/a\u003e\u003c/span\u003e\u003ca href=\"https://arstechnica.com/tech-policy/2024/01/robocall-with-artificial-joe-biden-voice-tells-democrats-not-to-vote/\"\u003e Joe Biden’s voice\u003c/a\u003e. A Democratic consultant was later \u003ca href=\"https://arstechnica.com/tech-policy/2024/05/democratic-consultant-indicted-for-biden-deepfake-that-told-people-not-to-vote/\"\u003eindicted\u003c/a\u003e in connection with the calls. The telco that transmitted the spoofed robocalls also agreed to pay a \u003ca href=\"https://arstechnica.com/tech-policy/2024/08/telco-to-pay-1m-fine-for-fake-biden-robocalls-that-told-people-not-to-vote/\"\u003e$1 million civil penalty\u003c/a\u003e for not authenticating the caller as required by FCC rules.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2025-05-15T21:06:17Z",
  "modifiedTime": "2025-05-15T21:06:17Z"
}
