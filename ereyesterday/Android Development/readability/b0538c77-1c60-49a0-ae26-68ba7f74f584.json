{
  "id": "b0538c77-1c60-49a0-ae26-68ba7f74f584",
  "title": "Top 3 things to know for AI on Android at Google I/O ‘25",
  "link": "http://android-developers.googleblog.com/2025/06/top-3-updates-for-ai-on-android-google-io.html",
  "description": "",
  "author": "Android Developers",
  "published": "2025-06-16T09:00:00.000-07:00",
  "source": "http://feeds.feedburner.com/blogspot/hsDu",
  "categories": [
    "#GenerativeAI",
    "AI Edge",
    "Androidify",
    "Firebase AI Logic",
    "Gemini Nano",
    "Google I/O 2025",
    "machine learning",
    "ML Kit",
    "on-device AI"
  ],
  "byline": "",
  "length": 4940,
  "excerpt": "Explore the top 3 Google I/O '25 Android AI announcements, making AI development easier and smarter with Gemini Nano, AI Edge, and Firebase AI Logic.",
  "siteName": "Android Developers Blog",
  "favicon": "",
  "text": "Posted by Kateryna Semenova – Sr. Developer Relations Engineer AI is reshaping how users interact with their favorite apps, opening new avenues for developers to create intelligent experiences. At Google I/O, we showcased how Android is making it easier than ever for you to build smart, personalized and creative apps. And we’re committed to providing you with the tools needed to innovate across the full development stack in this evolving landscape. This year, we focused on making AI accessible across the spectrum, from on-device processing to cloud-powered capabilities. Here are the top 3 announcements you need to know for building with AI on Android from Google I/O ‘25: #1 Leverage the efficiency of Gemini Nano for on-device AI experiences For on-device AI, we announced a new set of ML Kit GenAI APIs powered by Gemini Nano, our most efficient and compact model designed and optimized for running directly on mobile devices. These APIs provide high-level, easy integration for common tasks including text summarization, proofreading, rewriting content in different styles, and generating image description. Building on-device offers significant benefits such as local data processing and offline availability at no additional cost for inference. To start integrating these solutions explore the ML Kit GenAI documentation, the sample on GitHub and watch the \"Gemini Nano on Android: Building with on-device GenAI\" talk. #2 Seamlessly integrate on-device ML/AI with your own custom models The Google AI Edge platform enables building and deploying a wide range of pretrained and custom models on edge devices and supports various frameworks like TensorFlow, PyTorch, Keras, and Jax, allowing for more customization in apps. The platform now also offers improved support of on-device hardware accelerators and a new AI Edge Portal service for broad coverage of on-device benchmarking and evals. If you are looking for GenAI language models on devices where Gemini Nano is not available, you can use other open models via the MediaPipe LLM Inference API. Serving your own custom models on-device can pose challenges related to handling large model downloads and updates, impacting the user experience. To improve this, we’ve launched Play for On-Device AI in beta. This service is designed to help developers manage custom model downloads efficiently, ensuring the right model size and speed are delivered to each Android device precisely when needed. For more information watch “Small language models with Google AI Edge” talk. #3 Power your Android apps with Gemini Flash, Pro and Imagen using Firebase AI Logic For more advanced generative AI use cases, such as complex reasoning tasks, analyzing large amounts of data, processing audio or video, or generating images, you can use larger models from the Gemini Flash and Gemini Pro families, and Imagen running in the cloud. These models are well suited for scenarios requiring advanced capabilities or multimodal inputs and outputs. And since the AI inference runs in the cloud any Android device with an internet connection is supported. They are easy to integrate into your Android app by using Firebase AI Logic, which provides a simplified, secure way to access these capabilities without managing your own backend. Its SDK also includes support for conversational AI experiences using the Gemini Live API or generating custom contextual visual assets with Imagen. To learn more, check out our sample on GitHub and watch \"Enhance your Android app with Gemini Pro and Flash, and Imagen\" session. These powerful AI capabilities can also be brought to life in immersive Android XR experiences. You can find corresponding documentation, samples and the technical session: \"The future is now, with Compose and AI on Android XR\". Figure 1: Firebase AI Logic integration architecture Get inspired and start building with AI on Android today We released a new open source app, Androidify, to help developers build AI-driven Android experiences using Gemini APIs, ML Kit, Jetpack Compose, CameraX, Navigation 3, and adaptive design. Users can create personalized Android bot with Gemini and Imagen via the Firebase AI Logic SDK. Additionally, it incorporates ML Kit pose detection to detect a person in the camera viewfinder. The full code sample is available on GitHub for exploration and inspiration. Discover additional AI examples in our Android AI Sample Catalog. The original image and Androidifi-ed image Choosing the right Gemini model depends on understanding your specific needs and the model's capabilities, including modality, complexity, context window, offline capability, cost, and device reach. To explore these considerations further and see all our announcements in action, check out the AI on Android at I/O ‘25 playlist on YouTube and check out our documentation. We are excited to see what you will build with the power of Gemini!",
  "image": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGrbvjPNwEv5vrSrbuXf6hzYltmdc-6vYGCwiILsy8AB8SZQBiLimrvJaGham-g7COGhcJLle_PkZY6cvIOU4lTqSqZbNRiGNv56eQwTHcR9g5QZUUYuogdCAEk5-5aXlMDaRSH2irUF76DehM0CQ0mJ0zkUfpzNePmViElp4SivlJGqi03CrIYbXTUAU/w1200-h630-p-k-no-nu/AI-on-Android.gif",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cmeta content=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGrbvjPNwEv5vrSrbuXf6hzYltmdc-6vYGCwiILsy8AB8SZQBiLimrvJaGham-g7COGhcJLle_PkZY6cvIOU4lTqSqZbNRiGNv56eQwTHcR9g5QZUUYuogdCAEk5-5aXlMDaRSH2irUF76DehM0CQ0mJ0zkUfpzNePmViElp4SivlJGqi03CrIYbXTUAU/s1600/AI-on-Android.gif\" name=\"twitter:image\"/\u003e\n\u003cp\u003e\n\n\u003cem\u003ePosted by Kateryna Semenova – Sr. Developer Relations Engineer\u003c/em\u003e\n\n\u003ca href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOX2BHr_Vzaux-H3QG3ocu_7B6V6uT2JsGWJv7B_2V2I-1DBa6V6vTD5DwsI-2p0gI3d8oo7CvisFNSWtqSj6InVlxsGiPAP9eJyaveQGKgoWgGyzTWbdALv0sWXL2qzqN6RDkpG4BcIISye0Iu0aemgtJp8YhxYZ4HtG0TFOLYKrbS1A6L0qtLkcylxQ/s1600/android-spotlight-ai-on-android-banner.png\"\u003e\u003cimg data-original-height=\"800\" data-original-width=\"100%\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOX2BHr_Vzaux-H3QG3ocu_7B6V6uT2JsGWJv7B_2V2I-1DBa6V6vTD5DwsI-2p0gI3d8oo7CvisFNSWtqSj6InVlxsGiPAP9eJyaveQGKgoWgGyzTWbdALv0sWXL2qzqN6RDkpG4BcIISye0Iu0aemgtJp8YhxYZ4HtG0TFOLYKrbS1A6L0qtLkcylxQ/s1600/android-spotlight-ai-on-android-banner.png\"/\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003eAI is reshaping how users interact with their favorite apps, opening new avenues for developers to create intelligent experiences. At Google I/O, we showcased how Android is making it easier than ever for you to build smart, personalized and creative apps. And we’re committed to providing you with the tools needed to innovate across the full development stack in this evolving landscape.\u003c/p\u003e\n\n\u003cp\u003eThis year, we focused on making AI accessible across the spectrum, from on-device processing to cloud-powered capabilities. Here are the top 3 announcements you need to know for building with AI on Android from Google I/O ‘25:\u003c/p\u003e\n\n\u003ch2\u003e\u003cspan\u003e#1 Leverage the efficiency of Gemini Nano for on-device AI experiences\u003c/span\u003e\u003c/h2\u003e\n\n\u003ciframe allowfullscreen=\"\" height=\"210\" src=\"https://www.youtube.com/embed/mP9QESmEDls\" width=\"280\" youtube-src-id=\"mP9QESmEDls\"\u003e\u003c/iframe\u003e\n\n\u003cp\u003eFor on-device AI, we \u003ca href=\"https://android-developers.googleblog.com/2025/05/on-device-gen-ai-apis-ml-kit-gemini-nano.html\" target=\"_blank\"\u003eannounced a new set of ML Kit GenAI APIs\u003c/a\u003e powered by Gemini Nano, our most efficient and compact model designed and optimized for running directly on mobile devices. These APIs provide high-level, easy integration for common tasks including text summarization, proofreading, rewriting content in different styles, and generating image description. Building on-device offers significant benefits such as local data processing and offline availability at no additional cost for inference. To start integrating these solutions explore the \u003ca href=\"http://goo.gle/mlkit-genai\" target=\"_blank\"\u003eML Kit GenAI documentation\u003c/a\u003e, the \u003ca href=\"https://github.com/android/ai-samples/tree/main/ai-catalog\" target=\"_blank\"\u003esample on GitHub\u003c/a\u003e and watch the \u0026#34;\u003ca href=\"https://youtu.be/mP9QESmEDls?feature=shared\" target=\"_blank\"\u003eGemini Nano on Android: Building with on-device GenAI\u003c/a\u003e\u0026#34; talk.\u003c/p\u003e\n\n\n\u003ch2\u003e\u003cspan\u003e#2 Seamlessly integrate on-device ML/AI with your own custom models\u003c/span\u003e\u003c/h2\u003e\n\n\u003ciframe allowfullscreen=\"\" height=\"210\" src=\"https://www.youtube.com/embed/xLmJJk1gbuE\" width=\"280\" youtube-src-id=\"xLmJJk1gbuE\"\u003e\u003c/iframe\u003e\n\n\u003cp\u003e\u003ca href=\"https://ai.google.dev/edge\" target=\"_blank\"\u003eThe Google AI Edge\u003c/a\u003e platform enables building and deploying a wide range of pretrained and custom models on edge devices and supports various frameworks like TensorFlow, PyTorch, Keras, and Jax, allowing for more customization in apps. The platform now also offers \u003ca href=\"https://developers.googleblog.com/en/litert-maximum-performance-simplified\" target=\"_blank\"\u003eimproved support\u003c/a\u003e of on-device hardware accelerators and a new \u003ca href=\"https://cloud.google.com/blog/products/ai-machine-learning/ai-edge-portal-brings-on-device-ml-testing-at-scale\" target=\"_blank\"\u003eAI Edge Portal\u003c/a\u003e service for broad coverage of on-device benchmarking and evals. If you are looking for GenAI language models on devices where Gemini Nano is not available, you can use other open models via the \u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/android\" target=\"_blank\"\u003eMediaPipe LLM Inference API\u003c/a\u003e.\u003c/p\u003e  \n\n\u003cp\u003eServing your own custom models on-device can pose challenges related to handling large model downloads and updates, impacting the user experience. To improve this, we’ve launched \u003ca href=\"https://developer.android.com/google/play/on-device-ai\" target=\"_blank\"\u003ePlay for On-Device AI\u003c/a\u003e in beta. This service is designed to help developers manage custom model downloads efficiently, ensuring the right model size and speed are delivered to each Android device precisely when needed.\u003c/p\u003e \n\n\u003cp\u003eFor more information watch “\u003ca href=\"https://www.youtube.com/watch?v=xLmJJk1gbuE\" target=\"_blank\"\u003eSmall language models with Google AI Edge\u003c/a\u003e” talk.\u003c/p\u003e\n\n\u003ch2\u003e\u003cspan\u003e#3 Power your Android apps with Gemini Flash, Pro and Imagen using Firebase AI Logic \u003c/span\u003e\u003c/h2\u003e\n\n\u003ciframe allowfullscreen=\"\" height=\"210\" src=\"https://www.youtube.com/embed/U8Nb68XsVY4\" width=\"280\" youtube-src-id=\"U8Nb68XsVY4\"\u003e\u003c/iframe\u003e\n\n\u003cp\u003eFor more advanced generative AI use cases, such as complex reasoning tasks, analyzing large amounts of data, processing audio or video, or generating images, you can use larger models from the Gemini Flash and Gemini Pro families, and Imagen running in the cloud. These models are well suited for scenarios requiring advanced capabilities or multimodal inputs and outputs. And since the AI inference runs in the cloud any Android device with an internet connection is supported. They are easy to integrate into your Android app by using \u003ca href=\"http://developer.android.com/ai/gemini\" target=\"_blank\"\u003eFirebase AI Logic\u003c/a\u003e, which provides a simplified, secure way to access these capabilities without managing your own backend. Its SDK also includes support for conversational AI experiences using the \u003ca href=\"https://firebase.google.com/docs/vertex-ai/live-api\" target=\"_blank\"\u003eGemini Live API\u003c/a\u003e or generating custom contextual visual assets with \u003ca href=\"http://d.android.com/ai/imagen\" target=\"_blank\"\u003eImagen\u003c/a\u003e. To learn more, check out our \u003ca href=\"https://github.com/android/ai-samples/tree/main/ai-catalog\" target=\"_blank\"\u003esample on GitHub\u003c/a\u003e and watch \u0026#34;\u003ca href=\"https://www.youtube.com/watch?v=U8Nb68XsVY4\" target=\"_blank\"\u003eEnhance your Android app with Gemini Pro and Flash, and Imagen\u003c/a\u003e\u0026#34; session.\u003c/p\u003e\n\n\u003cp\u003eThese powerful AI capabilities can also be brought to life in immersive Android XR experiences. You can find corresponding \u003ca href=\"http://d.android.com/develop/xr\" target=\"_blank\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"http://goo.gle/android-xr-samples\" target=\"_blank\"\u003esamples\u003c/a\u003e and the technical session: \u0026#34;\u003ca href=\"https://www.youtube.com/watch?v=YYMt7Ddzz6Y\u0026amp;list=PLWz5rJ2EKKc_HBZR5747Ux5mp8M27xgb0\" target=\"_blank\"\u003eThe future is now, with Compose and AI on Android XR\u003c/a\u003e\u0026#34;.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"Flow cahrt demonstrating Firebase AI Logic integration architecture\" height=\"174\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlxyIIF3CslZqqFJmWV3bRwrvarojbJBLpu3dZJhgwVHbVtkZP0RwhaasWoakGVkVkzTNU522cbjuOopVVP7WiWa2GdGNoV9gBNiZ0dPpClFULesarMqpyqFgqAU8CM7tsPNdCIg5SJLuytl2_RPlM8LZafBz5oFKK5GZCSUjtBQQKQMLkXRIuQ-r-TiI/w640-h399/Firebase-AI-Logic-integration-architecture.jpg\" width=\"280\"/\u003e\u003c/p\u003e\u003cimgcaption\u003e\u003ccenter\u003e\u003cem\u003e\u003cb\u003eFigure 1:\u003c/b\u003e Firebase AI Logic integration architecture\u003c/em\u003e\u003c/center\u003e\u003c/imgcaption\u003e\u003cbr/\u003e\n\n\u003ch2\u003e\u003cspan\u003eGet inspired and start building with AI on Android today\u003c/span\u003e\u003c/h2\u003e \n\n\u003cp\u003eWe released a new \u003ca href=\"https://android-developers.googleblog.com/2025/05/androidify-building-ai-driven-experiences-jetpack-compose-gemini-camerax.html\" target=\"_blank\"\u003eopen source app, Androidify\u003c/a\u003e, to help developers build AI-driven Android experiences using Gemini APIs, ML Kit, Jetpack Compose, CameraX, Navigation 3, and adaptive design. Users can create personalized Android bot with Gemini and Imagen via the Firebase AI Logic SDK. Additionally, it incorporates ML Kit pose detection to detect a person in the camera viewfinder. The full code sample is \u003ca href=\"http://github.com/android/androidify\" target=\"_blank\"\u003eavailable on GitHub\u003c/a\u003e for exploration and inspiration. Discover additional AI examples in our \u003ca href=\"https://github.com/android/ai-samples/tree/main/ai-catalog\" target=\"_blank\"\u003eAndroid AI Sample Catalog\u003c/a\u003e.\u003c/p\u003e\n\n\n\u003cp\u003e\u003cimg alt=\"moving image of the Androidify app on a mobile device, showing a fair-skinned woman with blond hair wearing a red jacket with black shirt and pants and a pair of sunglasses converting into a 3D image of a droid with matching skin tone and blond hair wearing a red jacket with black shirt and pants and a pair of sunglasses\" height=\"622\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZXikUZH5SUQtydBcaXw3nraWM-YyMl-X6wRFbfwmFuMeIyttXSzVEU4Gzyvx5jtXp1AYcnFu_sr6OzGYmhUDpnVTIdPW8ZBzYM-alwIeGu0BILeZ4_VV0oirOGqSS7C0rng1qzSSK1MLnsdBqOlhj4eUiSnYz3qn3YWFn5BKx5xiHbVYlr43zDsh_0p4/w288-h640/Androidify-photo-to-droid-bot-demo.gif\" width=\"280\"/\u003e\u003c/p\u003e\u003cimgcaption\u003e\u003ccenter\u003e\u003cem\u003eThe original image and \u003ci\u003eAndroidifi-ed\u003c/i\u003e image\u003c/em\u003e\u003c/center\u003e\u003c/imgcaption\u003e\n\n\n\u003cp\u003eChoosing the right Gemini model depends on understanding your specific needs and the model\u0026#39;s capabilities, including modality, complexity, context window, offline capability, cost, and device reach. To explore these considerations further and see all our announcements in action, check out the \u003ca href=\"https://goo.gle/io25-build-with-ai\" target=\"_blank\"\u003eAI on Android at I/O ‘25 playlist on YouTube\u003c/a\u003e and check out our \u003ca href=\"https://developer.android.com/ai\" target=\"_blank\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eWe are excited to see what you will build with the power of Gemini!\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
