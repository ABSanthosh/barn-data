{
  "id": "bcc4abf7-de9b-4705-9e68-57616484c558",
  "title": "Automation Platform v2: Improving Conversational AI at Airbnb",
  "link": "https://medium.com/airbnb-engineering/automation-platform-v2-improving-conversational-ai-at-airbnb-d86c9386e0cb?source=rss----53c7c27702d5---4",
  "description": "",
  "author": "Chutian Wang",
  "published": "Mon, 28 Oct 2024 17:02:25 GMT",
  "source": "https://medium.com/feed/airbnb-engineering",
  "categories": [
    "engineering",
    "automation",
    "machine-learning-ai",
    "llm",
    "ai"
  ],
  "byline": "Chutian Wang",
  "length": 10452,
  "excerpt": "By Chutian Wang, Zhiheng Xu, Paul Lou, Ziyi Wang, Jiayu Lou, Liuming Zhang, Jingwen Qiang, Clint Kelly, Lei Shi, Dan Zhao, Xu Hu, Jianqi Liao, Zecheng Xu, Tong Chen Artificial intelligence and large…",
  "siteName": "The Airbnb Tech Blog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "How Airbnb’s conversational AI platform powers LLM application development.By Chutian Wang, Zhiheng Xu, Paul Lou, Ziyi Wang, Jiayu Lou, Liuming Zhang, Jingwen Qiang, Clint Kelly, Lei Shi, Dan Zhao, Xu Hu, Jianqi Liao, Zecheng Xu, Tong ChenIntroductionArtificial intelligence and large language models (LLMs) are a rapidly evolving sector at the forefront of technological innovation. AI’s capacity for logical reasoning and task completion is changing the way we interact with technology.In this blog post, we will showcase how we advanced Automation Platform, Airbnb’s conversational AI platform, from version 1, which supported conversational systems driven by static workflows, to version 2, which is designed specifically for emerging LLM applications. Now, developers can build LLM applications that help customer support agents work more efficiently, provide better resolutions, and quicker responses. LLM application architecture is a rapidly evolving domain and this blog post provides an overview of our efforts to adopt state-of-the-art LLM architecture to keep enhancing our platform based on the latest developments in the field.Overview of Automation PlatformIn a previous blog post, we introduced Automation Platform v1, an enterprise-level platform developed by Airbnb to support a suite of conversational AI products.Automation Platform v1 modeled traditional conversational AI products (e.g., chatbots) into predefined step-by-step workflows that could be designed and managed by product engineering and business teams.Figure 1. Automation Platform v1 architecture.Challenges of Traditional Conversational AI SystemsFigure 2. Typical workflow that is supported by v1 of Automation Platform.We saw several challenges when implementing Automation Platform v1, which may also be broadly applicable to typical conversational products:Not flexible enough: the AI products are following a predefined (and usually rigid) process.Hard to scale: product creators need to manually create workflows and tasks for every scenario, and repeat the process for any new use case later, which is time-consuming and error prone.Opportunities of Conversational AI Driven by LLMOur early experiments showed that LLM-powered conversation can provide a more natural and intelligent conversational experience than our current human-designed workflows. For example, with a LLM-powered chatbot, customers can engage in a natural dialogue experience asking open-ended questions and explaining their issues in detail. LLM can more accurately interpret customer queries, even capturing nuanced information from the ongoing conversation.However, LLM-powered applications are still relatively new, and the community is improving some of its aspects to meet production level requirements, like latency or hallucination.So it is too early to fully rely on them for large scale and diverse experience for millions of customers at Airbnb. For instance, it’s more suitable to use a transition workflow instead of LLM to process a claim related product that requires sensitive data and numbers of strict validations.We believe that at this moment, the best strategy is to combine them with traditional workflows and leverage the benefits of both approaches.Figure 3. Comparison of traditional workflows and AI driven workflowsArchitecture of LLM Application on Automation Platform v2Figure 4 shows a high level overview of how Automation Platform v2 powers LLM applications.Here is an example of a customer asking our LLM chatbot “where is my next reservation?”Firstly, user inquiry arrives at our platform. Based on the inquiry, our platform collects relevant contextual information, such as previous chat history, user id, user role, etc.After that, our platform loads and assembles the prompt using inquiry and context, then sends it to LLM.In this example, the first LLM response will be requesting a tool execution that makes a service call to fetch the most recent reservation of the current user. Our platform follows this order and does the actual service call then saves call responses into the current context.Next, our platform sends the updated context to LLM and the second LLM response will be a complete sentence describing the location of the user’s next reservation.Lastly, our platform returns LLM response and records this round of conversion for future reference.Figure 4. Overview of how Automation Platform v2 powers LLM applicationAnother important area we support is developers of LLM applications. There are several integrations between our system and developer tools to make the development process seamless. Also, we offer a number of tools like context management, guardrails, playground and insights.Figure 5. Overview of how Automation Platform v2 powers LLM developersIn the following subsections, we will deep dive into a few key areas on supporting LLM applications including: LLM workflows, context management and guardrails.While we won’t cover all aspects in detail in this post, we have also built other components to facilitate LLM practice at Airbnb including:Playground feature to bridge the gap between development and production tech stacks by allowing prompt writers to freely iterate on their prompts.LLM-oriented observability with detailed insights into each LLM interaction, like latency and token usage.Enhancement to Tool management that is responsible for tools registration, the publishing process, execution and observability.Chain of Thought WorkflowChain of Thought is one of AI agent frameworks that enables LLMs to reason about issues.We implemented the concept of Chain of Thought in the form of a workflow on Automation Platform v2 as shown below. The core idea of Chain of Thought is to use an LLM as the reasoning engine to determine which tools to use and in which order. Tools are the way an LLM interacts with the world to solve real problems, for example checking a reservation’s status or checking listing availability.Tools are essentially actions and workflows, the basic building blocks of traditional products in Automation Platform v1. Actions and workflows work well as tools in Chain of Thought because of their unified interface and managed execution environment.Figure 6. Overview of Chain of Thought workflowFigure 6 contains the main steps of the Chain of Thought workflow. It starts with preparing context for the LLM, including prompt, contextual data, and historical conversations. Then it triggers the logic reasoning loop: asking the LLM for reasoning, executing the LLM-requested tool and processing the tool’s outcome. Chain of Thought will stay in the reasoning loop until a result is generated.Figure 7. High level components powering Chain of Thought in Automation PlatformFigure 7 shows all high-level components powering Chain of Thought:CoT (Chain of Thought) IO handler: assemble the prompt, prepare contextual data, collect user input and general data processing before sending it to the LLM.Tool Manager: prepare tool payload with LLM input \u0026 output, manage tool execution and offer quality of life features like retry or rate limiting.LLM Adapter: allow developers to add customized logic facilitating integration with different types of LLMs.Context ManagementTo ensure the LLM makes the best decision, we need to provide all necessary and relevant information to the LLM such as historical interactions with the LLM, the intent of the customer support inquiry, current trip information and more. For use cases like offline evaluation, point-in-time data retrieval is also supported by our system via configuration.Given the large amount of available contextual information, developers are allowed to either statically declare the needed context (e.g. customer name) or name a dynamic context retriever (e.g. relevant help articles of customer’s questions ).Figure 8. Overall architecture of context management in Automation Platform v2Context Management is the key component ensuring the LLM has the access to all necessary contextual information. Figure 8 shows major Context Management components:Context Loader: connect to different sources and fetch relevant context based on developers’ customizable fetching logic.Runtime Context Manager: maintain runtime context, process context for each LLM call and interact with context storage.Guardrails FrameworkLLMs are powerful text generation tools, but they also can come with issues like hallucinations and jailbreaks. This is where our Guardrails Framework comes in, a safe-guarding mechanism that monitors communications with the LLM, ensuring it is helpful, relevant and ethical.Figure 9. Guardrails Framework architectureFigure 9 shows the architecture of Guardrails Framework where engineers from different teams create reusable guardrails. During runtime, guardrails can be executed in parallel and leverage different downstream tech stacks. For example, the content moderation guardrail calls various LLMs to detect violations in communication content, and tool guardrails use rules to prevent bad execution, for example updating listings with invalid setup.What’s NextIn this blog, we presented the most recent evolution of Automation Platform, the conversational AI platform at Airbnb, to power emerging LLM applications.LLM application is a rapidly developing domain, and we will continue to evolve with these transformative technologies, explore other AI agent frameworks, expand Chain of Thought tool capabilities and investigate LLM application simulation. We anticipate further efficiency and productivity gains for all AI practitioners at Airbnb with these innovations.We’re hiring! If work like this interests you check out our careers site.AcknowledgementsThanks to Mia Zhao, Zay Guan, Michael Lubavin, Wei Wu, Yashar Mehdad, Julian Warszawski, Ting Luo, Junlan Li, Wayne Zhang, Zhenyu Zhao, Yuanpei Cao, Yisha Wu, Peng Wang, Heng Ji, Tiantian Zhang, Cindy Chen, Hanchen Su, Wei Han, Mingzhi Xu, Ying Lyu, Elaine Liu, Hengyu Zhou, Teng Wang, Shawn Yan, Zecheng Xu, Haiyu Zhang, Gary Pan, Tong Chen, Pei-Fen Tu, Ying Tan, Fengyang Chen, Haoran Zhu, Xirui Liu, Tony Jiang, Xiao Zeng, Wei Wu, Tongyun Lv, Zixuan Yang, Keyao Yang, Danny Deng, Xiang Lan and Wei Ji for the product collaborations.Thanks to Joy Zhang, Raj Rajagopal, Tina Su, Peter Frank, Shuohao Zhang, Jack Song, Navjot Sidhu, Weiping Peng, Kelvin Xiong, Andy Yasutake and Hanlin Fang’s leadership support for the Intelligent Automation Platform.",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*36lUKfHUjs_YMo8DMj0huQ.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"3e82\"\u003e\u003cstrong\u003eHow Airbnb’s conversational AI platform powers LLM application development.\u003c/strong\u003e\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca rel=\"noopener follow\" href=\"https://medium.com/@wawjchinawct?source=post_page---byline--d86c9386e0cb--------------------------------\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Chutian Wang\" src=\"https://miro.medium.com/v2/da:true/resize:fill:88:88/0*PsaMvqhFFp36LuNF\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/airbnb-engineering?source=post_page---byline--d86c9386e0cb--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"The Airbnb Tech Blog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*MlNQKg-sieBGW5prWoe9HQ.jpeg\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"34d4\"\u003eBy \u003ca href=\"https://www.linkedin.com/in/chutianwang/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eChutian Wang\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/zhiheng-xu-50249b31/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eZhiheng Xu\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/paullou-sea/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePaul Lou\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/ziyi-wang-6651b5b1/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eZiyi Wang\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/jiayu-lou-337ba785/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJiayu Lou\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/liuming-zhang-4b120894/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLiuming Zhang\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/jingwen-qiang-76aba382/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJingwen Qiang\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/clintonkelly/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eClint Kelly\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/mleoshi/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLei Shi\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/dan-zhao-560460143/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDan Zhao\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/huxiaoxu/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eXu Hu\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/jianqi-liao-84b32510a/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJianqi Liao\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/zecheng-xu-11bb778a/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eZecheng Xu\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/tong-chen-3a5b1519/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTong Chen\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"62e3\"\u003eIntroduction\u003c/h2\u003e\u003cp id=\"be9c\"\u003eArtificial intelligence and large language models (LLMs) are a rapidly evolving sector at the forefront of technological innovation. AI’s capacity for logical reasoning and task completion is changing the way we interact with technology.\u003c/p\u003e\u003cp id=\"fa72\"\u003eIn this blog post, we will showcase how we advanced Automation Platform, Airbnb’s conversational AI platform, from version 1, which supported conversational systems driven by static workflows, to version 2, which is designed specifically for emerging LLM applications. Now, developers can build LLM applications that help customer support agents work more efficiently, provide better resolutions, and quicker responses. LLM application architecture is a rapidly evolving domain and this blog post provides an overview of our efforts to adopt state-of-the-art LLM architecture to keep enhancing our platform based on the latest developments in the field.\u003c/p\u003e\u003ch2 id=\"039d\"\u003eOverview of Automation Platform\u003c/h2\u003e\u003cp id=\"e438\"\u003eIn a previous \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/intelligent-automation-platform-empowering-conversational-ai-and-beyond-at-airbnb-869c44833ff2\"\u003eblog post\u003c/a\u003e, we introduced Automation Platform v1, an enterprise-level platform developed by Airbnb to support a suite of conversational AI products.\u003c/p\u003e\u003cp id=\"c5e8\"\u003eAutomation Platform v1 modeled traditional conversational AI products (e.g., chatbots) into predefined step-by-step workflows that could be designed and managed by product engineering and business teams.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 1. Automation Platform v1 architecture.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"9000\"\u003eChallenges of Traditional Conversational AI Systems\u003c/h2\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 2. Typical workflow that is supported by v1 of Automation Platform.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"bfc4\"\u003eWe saw several challenges when implementing Automation Platform v1, which may also be broadly applicable to typical conversational products:\u003c/p\u003e\u003col\u003e\u003cli id=\"e0da\"\u003eNot flexible enough: the AI products are following a predefined (and usually rigid) process.\u003c/li\u003e\u003cli id=\"b01a\"\u003eHard to scale: product creators need to manually create workflows and tasks for every scenario, and repeat the process for any new use case later, which is time-consuming and error prone.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"cc6e\"\u003eOpportunities of Conversational AI Driven by LLM\u003c/h2\u003e\u003cp id=\"0ef8\"\u003eOur early experiments showed that LLM-powered conversation can provide a more natural and intelligent conversational experience than our current human-designed workflows. For example, with a LLM-powered chatbot, customers can engage in a natural dialogue experience asking open-ended questions and explaining their issues in detail. LLM can more accurately interpret customer queries, even capturing nuanced information from the ongoing conversation.\u003c/p\u003e\u003cp id=\"a037\"\u003eHowever, LLM-powered applications are still relatively new, and the community is improving some of its aspects to meet production level requirements, like latency or hallucination.So it is too early to fully rely on them for large scale and diverse experience for millions of customers at Airbnb. For instance, it’s more suitable to use a transition workflow instead of LLM to process a claim related product that requires sensitive data and numbers of strict validations.\u003c/p\u003e\u003cp id=\"52b6\"\u003eWe believe that at this moment, the best strategy is to combine them with traditional workflows and leverage the benefits of both approaches.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 3. Comparison of traditional workflows and AI driven workflows\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"f1c2\"\u003eArchitecture of LLM Application on Automation Platform v2\u003c/h2\u003e\u003cp id=\"3570\"\u003eFigure 4 shows a high level overview of how Automation Platform v2 powers LLM applications.\u003c/p\u003e\u003cp id=\"0534\"\u003eHere is an example of a customer asking our LLM chatbot “where is my next reservation?”\u003c/p\u003e\u003cul\u003e\u003cli id=\"9b50\"\u003eFirstly, user inquiry arrives at our platform. Based on the inquiry, our platform collects relevant contextual information, such as previous chat history, user id, user role, etc.\u003c/li\u003e\u003cli id=\"edd9\"\u003eAfter that, our platform loads and assembles the prompt using inquiry and context, then sends it to LLM.\u003c/li\u003e\u003cli id=\"a926\"\u003eIn this example, the first LLM response will be requesting a tool execution that makes a service call to fetch the most recent reservation of the current user. Our platform follows this order and does the actual service call then saves call responses into the current context.\u003c/li\u003e\u003cli id=\"71d8\"\u003eNext, our platform sends the updated context to LLM and the second LLM response will be a complete sentence describing the location of the user’s next reservation.\u003c/li\u003e\u003cli id=\"55e4\"\u003eLastly, our platform returns LLM response and records this round of conversion for future reference.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 4. Overview of how Automation Platform v2 powers LLM application\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"7d79\"\u003eAnother important area we support is developers of LLM applications. There are several integrations between our system and developer tools to make the development process seamless. Also, we offer a number of tools like context management, guardrails, playground and insights.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 5. Overview of how Automation Platform v2 powers LLM developers\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"4d85\"\u003eIn the following subsections, we will deep dive into a few key areas on supporting LLM applications including: LLM workflows, context management and guardrails.\u003c/p\u003e\u003cp id=\"1744\"\u003eWhile we won’t cover all aspects in detail in this post, we have also built other components to facilitate LLM practice at Airbnb including:\u003c/p\u003e\u003cul\u003e\u003cli id=\"6419\"\u003ePlayground feature to bridge the gap between development and production tech stacks by allowing prompt writers to freely iterate on their prompts.\u003c/li\u003e\u003cli id=\"2c70\"\u003eLLM-oriented observability with detailed insights into each LLM interaction, like latency and token usage.\u003c/li\u003e\u003cli id=\"c575\"\u003eEnhancement to Tool management that is responsible for tools registration, the publishing process, execution and observability.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"c677\"\u003eChain of Thought Workflow\u003c/h2\u003e\u003cp id=\"a9db\"\u003e\u003ca href=\"https://arxiv.org/pdf/2201.11903.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eChain of Thought\u003c/a\u003e is one of AI agent frameworks that enables LLMs to reason about issues.\u003c/p\u003e\u003cp id=\"d3ec\"\u003eWe implemented the concept of Chain of Thought in the form of a workflow on Automation Platform v2 as shown below. The core idea of Chain of Thought is to use an LLM as the reasoning engine to determine which tools to use and in which order. Tools are the way an LLM interacts with the world to solve real problems, for example checking a reservation’s status or checking listing availability.\u003c/p\u003e\u003cp id=\"f441\"\u003eTools are essentially actions and workflows, the basic building blocks of traditional products in Automation Platform v1. Actions and workflows work well as tools in Chain of Thought because of their unified interface and managed execution environment.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 6. Overview of Chain of Thought workflow\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"ec2a\"\u003eFigure 6 contains the main steps of the Chain of Thought workflow. It starts with preparing context for the LLM, including prompt, contextual data, and historical conversations. Then it triggers the logic reasoning loop: asking the LLM for reasoning, executing the LLM-requested tool and processing the tool’s outcome. Chain of Thought will stay in the reasoning loop until a result is generated.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 7. High level components powering Chain of Thought in Automation Platform\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"b4c5\"\u003eFigure 7 shows all high-level components powering Chain of Thought:\u003c/p\u003e\u003col\u003e\u003cli id=\"30e0\"\u003eCoT (Chain of Thought) IO handler: assemble the prompt, prepare contextual data, collect user input and general data processing before sending it to the LLM.\u003c/li\u003e\u003cli id=\"87cd\"\u003eTool Manager: prepare tool payload with LLM input \u0026amp; output, manage tool execution and offer quality of life features like retry or rate limiting.\u003c/li\u003e\u003cli id=\"e13b\"\u003eLLM Adapter: allow developers to add customized logic facilitating integration with different types of LLMs.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"3e39\"\u003eContext Management\u003c/h2\u003e\u003cp id=\"f4d8\"\u003eTo ensure the LLM makes the best decision, we need to provide all necessary and relevant information to the LLM such as historical interactions with the LLM, the intent of the customer support inquiry, current trip information and more. For use cases like offline evaluation, point-in-time data retrieval is also supported by our system via configuration.\u003c/p\u003e\u003cp id=\"8dae\"\u003eGiven the large amount of available contextual information, developers are allowed to either statically declare the needed context (e.g. customer name) or name a dynamic context retriever (e.g. relevant help articles of customer’s questions ).\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 8. Overall architecture of context management in Automation Platform v2\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"70ff\"\u003eContext Management is the key component ensuring the LLM has the access to all necessary contextual information. Figure 8 shows major Context Management components:\u003c/p\u003e\u003col\u003e\u003cli id=\"7de3\"\u003eContext Loader: connect to different sources and fetch relevant context based on developers’ customizable fetching logic.\u003c/li\u003e\u003cli id=\"4ead\"\u003eRuntime Context Manager: maintain runtime context, process context for each LLM call and interact with context storage.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"a0bf\"\u003eGuardrails Framework\u003c/h2\u003e\u003cp id=\"01df\"\u003eLLMs are powerful text generation tools, but they also can come with issues like \u003ca href=\"https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehallucinations\u003c/a\u003e and \u003ca href=\"https://en.wikipedia.org/wiki/Prompt_injection#Types\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ejailbreaks\u003c/a\u003e. This is where our Guardrails Framework comes in, a safe-guarding mechanism that monitors communications with the LLM, ensuring it is helpful, relevant and ethical.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 9. Guardrails Framework architecture\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"4fe7\"\u003eFigure 9 shows the architecture of Guardrails Framework where engineers from different teams create reusable guardrails. During runtime, guardrails can be executed in parallel and leverage different downstream tech stacks. For example, the content moderation guardrail calls various LLMs to detect violations in communication content, and tool guardrails use rules to prevent bad execution, for example updating listings with invalid setup.\u003c/p\u003e\u003ch2 id=\"cdec\"\u003eWhat’s Next\u003c/h2\u003e\u003cp id=\"18ca\"\u003eIn this blog, we presented the most recent evolution of Automation Platform, the conversational AI platform at Airbnb, to power emerging LLM applications.\u003c/p\u003e\u003cp id=\"1871\"\u003eLLM application is a rapidly developing domain, and we will continue to evolve with these transformative technologies, explore other \u003ca href=\"https://arxiv.org/abs/2305.10601\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAI agent frameworks\u003c/a\u003e, expand Chain of Thought tool capabilities and investigate LLM application simulation. We anticipate further efficiency and productivity gains for all AI practitioners at Airbnb with these innovations.\u003c/p\u003e\u003cp id=\"5a2d\"\u003eWe’re hiring! If work like this interests you check out our \u003ca href=\"https://careers.airbnb.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecareers site\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"5dc1\"\u003eAcknowledgements\u003c/h2\u003e\u003cp id=\"790d\"\u003eThanks to Mia Zhao, Zay Guan, Michael Lubavin, Wei Wu, Yashar Mehdad, Julian Warszawski, Ting Luo, Junlan Li, Wayne Zhang, Zhenyu Zhao, Yuanpei Cao, Yisha Wu, Peng Wang, Heng Ji, Tiantian Zhang, Cindy Chen, Hanchen Su, Wei Han, Mingzhi Xu, Ying Lyu, Elaine Liu, Hengyu Zhou, Teng Wang, Shawn Yan, Zecheng Xu, Haiyu Zhang, Gary Pan, Tong Chen, Pei-Fen Tu, Ying Tan, Fengyang Chen, Haoran Zhu, Xirui Liu, Tony Jiang, Xiao Zeng, Wei Wu, Tongyun Lv, Zixuan Yang, Keyao Yang, Danny Deng, Xiang Lan and Wei Ji for the product collaborations.\u003c/p\u003e\u003cp id=\"83a5\"\u003eThanks to Joy Zhang, Raj Rajagopal, Tina Su, Peter Frank, Shuohao Zhang, Jack Song, Navjot Sidhu, Weiping Peng, Kelvin Xiong, Andy Yasutake and Hanlin Fang’s leadership support for the Intelligent Automation Platform.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2024-10-28T17:02:24.709Z",
  "modifiedTime": null
}
