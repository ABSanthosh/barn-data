{
  "id": "cae7fc32-bf5f-48b2-9b53-422fc03a0d1f",
  "title": "Stanford’s Marin foundation model: The first fully open model developed using JAX",
  "link": "https://developers.googleblog.com/en/stanfords-marin-foundation-model-first-fully-open-model-developed-using-jax/",
  "description": "The Marin project aims to expand the definition of 'open' in AI to include the entire scientific process, not just the model itself, by making the complete development journey accessible and reproducible. This effort, powered by the JAX framework and its Levanter tool, allows for deep scrutiny, trust in, and building upon foundation models, fostering a more transparent future for AI research.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Srikanth Kilaru, David Hall",
  "length": 10595,
  "excerpt": "The Marin project aims to expand the definition of 'open' in AI to include the entire scientific process, not just the model itself, by making the complete development journey accessible and reproducible. This effort, powered by the JAX framework and its Levanter tool, allows for deep scrutiny, trust in, and building upon foundation models, fostering a more transparent future for AI research.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "David Hall Research Engineering Lead Stanford HAI An exciting element of the current AI era is how powerful foundation models are being shared in the open and helping to accelerate innovation for everyone. This progress inspires us to ask, 'What's next for openness?' The Marin project sees an opportunity to expand the definition of 'open' to encompass the entire scientific process behind a model.Stanford CRFM’s (Center for Research on Foundation Models) Marin project is designed as an 'open lab,' where the goal is not only to share the model but to make the complete journey accessible — including the code, data set, data methodologies, experiments, hyperparameters and training logs. This level of transparency complements the existing ecosystem by providing a unique, fully reproducible resource that empowers researchers to scrutinize, build upon, and trust the models being developed. Stanford’s Marin project seeks to foster a more transparent and accessible future for foundation model research.The spectrum of AI model openness The first releases from this open lab are the Marin-8B-Base and Marin-8B-Instruct models. In keeping with the project's principles, the models, data, code, and tokenizer are all released under the permissive Apache 2.0 license. This commitment to complete reproducibility is a formidable engineering problem, requiring control over every source of variance in a massively distributed system. The project's success hinges on a technology stack that can deliver this guarantee of reproducibility at scale, and maximize efficiency to train a foundation model with leading price / performance.Core challenges of building open foundation modelsFor the Marin project to succeed in creating truly open, scalable, and reproducible foundation models, the CRFM team had to solve several engineering challenges. The team chose JAX as the foundation because its design principles provided direct solutions to these problems, and built a new framework, Levanter (see below), to harness JAX's power. Here are a few examples of challenges and their solutionsAchieving maximum speed on a single acceleratorProblem: The core training loop is executed billions of times, so the overhead from an interpreted language like Python creates a massive performance bottleneck. If operations are dispatched step by step, the loop can also incur excessive memory traffic and overhead—especially on hardware like TPUs, where throughput depends on executing fused operations efficiently.Our solution:To eliminate interpreter overhead, Levanter encapsulates the entire multi-stage training step (forward pass, loss, backpropagation, and update) into a single function and uses the @jax.jit decorator. JAX's XLA compiler transforms this entire process into a single, highly-optimized machine code kernel, fusing operations to maximize hardware utilization at scale.To avoid redundant computation, we use jax.value_and_grad to compute both the loss and its gradients in a single pass. JAX also makes it easy to use advanced techniques like gradient checkpointing, saving memory and enabling us to use larger batch sizes with almost no overhead.Levanter also uses JAX’s powerful Pallas-based Splash Attention kernel, a highly optimized implementation of Dot Product Attention, one of the most critical operations at the heart of nearly all large language models.Managing the complexity of large-scale parallelismProblem: Training state-of-the-art models requires scaling out to thousands of accelerator chips. Manually managing how the model and data are partitioned and how the devices communicate is immensely complex, and the code quickly becomes difficult to read, debug, and adapt.Our solution:JAX’s @jax.jit decorator also seamlessly supports Single-Program, Multiple-Data (SPMD) parallelization that automates the underlying data sharding and communication. The XLA compiler automatically schedules communication between accelerators to minimize time spent waiting on the network and maximizing time spent on computation.To make jit’s power even easier and safer to use, Levanter developed Haliax, a library for named tensors. By referring to tensor axes with human-readable names (like \"embed\" or \"batch\") instead of positional indices, the code becomes self-documenting and robust.This abstraction allows us to define and modify sophisticated sharding strategies like Fully Sharded Data Parallelism (FSDP) and Tensor Parallelism simply by changing a few lines in a configuration file, without ever touching the model code.Building and managing resilient, cost-effective compute clustersProblem: Large-scale training requires flexible access to massive compute clusters. We rely heavily on preemptible TPU instances to manage costs, which means we need a way to easily combine many smaller, disparate TPU slices into one logical cluster and be resilient to frequent interruptions.Our solution:We leverage Google Cloud TPU Multislice, a technology that allows a training job to use multiple TPU slices as if they were one large system. This makes it easy to stitch many small, preemptible TPU slices together into a single, powerful compute cluster for training.Levanter uses Ray to orchestrate this process, seamlessly scaling the number of TPU slices up or down during a training job and, crucially, ensuring the job remains resilient if any single slice is preempted.Thanks to JAX and XLA, Levanter and Marin were able to get similar high performance results on GPUs as well.Fostering scientific trust with perfect reproducibilityProblem: A core goal of the Marin project is to enable verifiable science. This requires achieving reproducible results, even when training is paused, restarted, or moved between different machine configurations—a significant technical hurdle.Our solution:This was a fundamental requirement that drove Levanter's design. We chose JAX specifically for its strong reproducibility guarantees, such as its default use of deterministic pseudo-random number generators (PRNGs).This choice was validated during the training of Marin-8B, which involved migrating between different TPU slices and hardware types while successfully maintaining bit-for-bit reproducibility across preemptions.Levanter also includes a robust data loading system built on Google’s Tensorstore library. Levanter’s data store offers deterministic, random access to any batch of training data, regardless of job restarts or data source changes—critical for supporting advanced training strategies like mid-training. JAX’s determinism and Levanter’s data store also make it easy for interpretability researchers to understand how specific data impacts the model during training.Creating a cohesive frameworkProblem: While JAX provides a powerful engine, no existing high-level framework met our stringent, combined requirements for legibility, massive scalability, and bitwise determinism. We needed a complete, opinionated system to orchestrate the entire training process.Our solution:We built Levanter, a JAX-native framework, from the ground up to be the system we needed: bitwise deterministic, scalable with advanced distribution strategies, and resilient.We could do this because JAX is more than just a library; it's a \"meta-framework\" for building new tools. We built upon its mature, high-performance support for TPUs and its seamless integration of high-level abstractions (jit) with low-level control (Pallas).This approach is common in the JAX community, which has produced a vibrant ecosystem of libraries like Flax, Equinox, Orbax and Optax that work together, allowing teams like ours to build powerful solutions.A look under the hood: The voyage of Marin-8BThe principles, tools and libraries discussed above were implemented and put to work during the Marin-8B training run. The model architecture is a Llama-style transformer.Marin-8B-Base: Model architecture at a glance Rather than a static, monolithic run, the training of Marin-8B was an adaptive journey, internally dubbed the \"Tootsie\" process. This honest portrayal of a real-world research workflow is detailed in the public. The process spanned over 12 trillion tokens and involved multiple phases that adapted to new data, techniques, and even different hardware configurations — migrating between large-scale, multi-slice TPU configurations (2x v5e-256 to 1x v4-2048 pods) mid-stream. The team continuously refined the data mixture, incorporating higher-quality sources, and adjusted hyperparameters like learning rate and batch size to optimize performance. This \"messy\" reality is a powerful educational tool, and the ability of the JAX and Levanter stack to handle these significant shifts while maintaining bit-for-bit reproducibility is a powerful demonstration of its robustness.Join the Marin communityThe Marin project is an open invitation to participate in the future of foundation model development and contribute to the JAX ecosystem. The journey of Marin represents the answer to our question, \"What's next for openness?\" This effort to create an 'open lab' is made possible by the technical capabilities of the JAX ecosystem. Its performance, portability, and foundational design for reproducibility are the key ingredients that allow us to make the 'complete journey' of research accessible.By sharing everything from data methodologies to training logs, we aim to provide a fully reproducible resource—one that empowers researchers to deeply scrutinize, build upon, and trust the work. We believe this is a collaborative step toward a more transparent future for AI. We invite you to join us in this 'open lab'—to use Marin, to contribute to the research, and to help build the next wave of innovative and trustworthy foundation models.The central resource for the project is the official website, marin.community. From there, you can find the released models on Hugging Face, explore the \"open lab\" on GitHub, read Marin documentation, and dive into the Levanter training framework. You can also test drive Marin in a colab with a simple inference example.And active discussions are taking place in the Discord channel where you can interact directly with other developers. For those new to the ecosystem, the official JAX documentation provides excellent resources, including a Quickstart guide.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/stanfiord-Marin-project-in-JAX.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \u003csection\u003e\n      \n        \n          \n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=David+Hall\"\u003eDavid Hall\u003c/a\u003e\n            \n              \u003cspan\u003eResearch Engineering Lead\u003c/span\u003e\n            \n            \n              \u003cspan\u003eStanford HAI\u003c/span\u003e\n            \n          \u003c/p\u003e\n        \n\n      \n      \u003c/section\u003e\n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cdiv data-block-key=\"5q1ht\"\u003e\u003cp\u003eAn exciting element of the current AI era is how powerful foundation models are being shared in the open and helping to accelerate innovation for everyone. This progress inspires us to ask, \u0026#39;What\u0026#39;s next for openness?\u0026#39; The \u003ca href=\"http://marin.community/\"\u003eMarin project\u003c/a\u003e sees an opportunity to expand the definition of \u0026#39;open\u0026#39; to encompass the entire scientific process behind a model.\u003c/p\u003e\u003cp\u003eStanford \u003ca href=\"https://crfm.stanford.edu/\"\u003eCRFM\u003c/a\u003e’s (Center for Research on Foundation Models) \u003ca href=\"http://marin.community/\"\u003eMarin\u003c/a\u003e project is designed as an \u0026#39;\u003ca href=\"https://marin.community/blog/2025/05/19/announcement/\"\u003eopen lab\u003c/a\u003e,\u0026#39; where the goal is not only to share the model but to make the complete journey accessible — including the code, data set, data methodologies, experiments, hyperparameters and training logs. This level of transparency complements the existing ecosystem by providing a unique, fully reproducible resource that empowers researchers to scrutinize, build upon, and trust the models being developed. Stanford’s Marin project seeks to foster a more transparent and accessible future for foundation model research.\u003c/p\u003e\u003c/div\u003e\u003ch2 data-block-key=\"9ec97\" id=\"the-spectrum-of-ai-model-openness\"\u003e\u003cbr/\u003eThe spectrum of AI model openness\u003c/h2\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ai-model-openness-spectrum-stanford-marin-open-.original_4oMY0az.png\" alt=\"The Spectrum of AI Model Openness\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"5q1ht\"\u003eThe first releases from this open lab are the \u003ca href=\"https://marin.readthedocs.io/en/latest/reports/marin-8b-retro/#base-model-results\"\u003eMarin-8B-Base and Marin-8B-Instruct models\u003c/a\u003e. In keeping with the project\u0026#39;s principles, the models, data, code, and tokenizer are all released under the permissive Apache 2.0 license. This commitment to complete reproducibility is a formidable engineering problem, requiring control over every source of variance in a \u003ca href=\"https://github.com/stanford-crfm/levanter\"\u003emassively distributed system\u003c/a\u003e. The project\u0026#39;s success hinges on a technology stack that can deliver this guarantee of reproducibility at scale, and maximize efficiency to train a foundation model with leading price / performance.\u003c/p\u003e\u003ch2 data-block-key=\"q0c3d\" id=\"core-challenges-of-building-open-foundation-models\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eCore challenges of building open foundation models\u003c/h2\u003e\u003cp data-block-key=\"cja3f\"\u003eFor the Marin project to succeed in creating truly open, scalable, and reproducible foundation models, the CRFM team had to solve several engineering challenges. The team chose \u003ca href=\"http://jax.dev/\"\u003eJAX\u003c/a\u003e as the foundation because its design principles provided direct solutions to these problems, and built a new framework, \u003ca href=\"https://github.com/stanford-crfm/levanter\"\u003eLevanter\u003c/a\u003e (see below), to harness JAX\u0026#39;s power. Here are a few examples of challenges and their solutions\u003c/p\u003e\u003ch3 data-block-key=\"0a3qs\" id=\"achieving-maximum-speed-on-a-single-accelerator\"\u003e\u003cb\u003e\u003cbr/\u003eAchieving maximum speed on a single accelerator\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"eb8ro\"\u003e\u003cb\u003eProblem:\u003c/b\u003e The core training loop is executed billions of times, so the overhead from an interpreted language like Python creates a massive performance bottleneck. If operations are dispatched step by step, the loop can also incur excessive memory traffic and overhead—especially on hardware like TPUs, where throughput depends on executing fused operations efficiently.\u003c/p\u003e\u003cp data-block-key=\"9mrtt\"\u003e\u003cb\u003eOur solution:\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"blf55\"\u003eTo eliminate interpreter overhead, Levanter encapsulates the entire multi-stage training step (forward pass, loss, backpropagation, and update) into a single function and uses the \u003ca href=\"https://docs.jax.dev/en/latest/_autosummary/jax.jit.html\"\u003e\u003ccode\u003e@jax.jit\u003c/code\u003e\u003c/a\u003e decorator. JAX\u0026#39;s XLA compiler transforms this entire process into a single, highly-optimized machine code kernel, fusing operations to maximize hardware utilization at scale.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"4heko\"\u003eTo avoid redundant computation, we use \u003ca href=\"https://docs.jax.dev/en/latest/_autosummary/jax.value_and_grad.html\"\u003e\u003ccode\u003ejax.value_and_grad\u003c/code\u003e\u003c/a\u003e to compute both the loss and its gradients in a single pass. JAX also makes it easy to use advanced techniques like \u003ca href=\"https://docs.jax.dev/en/latest/gradient-checkpointing.html\"\u003egradient checkpointing\u003c/a\u003e, saving memory and enabling us to use larger batch sizes with almost no overhead.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"744sj\"\u003eLevanter also uses JAX’s powerful \u003ca href=\"https://docs.jax.dev/en/latest/pallas/index.html\"\u003e\u003ccode\u003ePallas\u003c/code\u003e\u003c/a\u003e-based Splash Attention kernel, a highly optimized implementation of Dot Product Attention, one of the most critical operations at the heart of nearly all large language models.\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"jr8cn\" id=\"\"\u003e\u003cb\u003e\u003cbr/\u003eManaging the complexity of large-scale parallelism\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"7g9t5\"\u003e\u003cb\u003eProblem:\u003c/b\u003e Training state-of-the-art models requires scaling out to thousands of accelerator chips. Manually managing how the model and data are partitioned and how the devices communicate is immensely complex, and the code quickly becomes difficult to read, debug, and adapt.\u003c/p\u003e\u003cp data-block-key=\"5hrao\"\u003e\u003cb\u003eOur solution:\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"1veak\"\u003eJAX’s \u003ca href=\"https://docs.jax.dev/en/latest/_autosummary/jax.jit.html\"\u003e\u003ccode\u003e@jax.jit\u003c/code\u003e\u003c/a\u003e decorator also seamlessly supports Single-Program, Multiple-Data (SPMD) parallelization that automates the underlying data sharding and communication. The XLA compiler automatically schedules communication between accelerators to minimize time spent waiting on the network and maximizing time spent on computation.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"34k3\"\u003eTo make \u003ca href=\"https://docs.jax.dev/en/latest/_autosummary/jax.jit.html\"\u003e\u003ccode\u003ejit\u003c/code\u003e\u003c/a\u003e’s power even easier and safer to use, Levanter developed \u003ca href=\"https://github.com/stanford-crfm/haliax\"\u003eHaliax\u003c/a\u003e, a library for named tensors. By referring to tensor axes with human-readable names (like \u0026#34;embed\u0026#34; or \u0026#34;batch\u0026#34;) instead of positional indices, the code becomes self-documenting and robust.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"2dae\"\u003eThis abstraction allows us to define and modify sophisticated sharding strategies like Fully Sharded Data Parallelism (FSDP) and Tensor Parallelism simply by changing a few lines in a configuration file, without ever touching the model code.\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"jpb20\" id=\"building-and-managing-resilient-cost-effective-compute-clusters\"\u003e\u003cb\u003e\u003cbr/\u003eBuilding and managing resilient, cost-effective compute clusters\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"djqkt\"\u003e\u003cb\u003eProblem:\u003c/b\u003e Large-scale training requires flexible access to massive compute clusters. We rely heavily on preemptible TPU instances to manage costs, which means we need a way to easily combine many smaller, disparate TPU slices into one logical cluster and be resilient to frequent interruptions.\u003c/p\u003e\u003cp data-block-key=\"52sr4\"\u003e\u003cb\u003eOur solution:\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"30ckb\"\u003eWe leverage \u003ca href=\"https://cloud.google.com/tpu/docs/multislice-introduction\"\u003eGoogle Cloud TPU Multislice\u003c/a\u003e, a technology that allows a training job to use multiple TPU slices as if they were one large system. This makes it easy to stitch many small, preemptible TPU slices together into a single, powerful compute cluster for training.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"98nro\"\u003eLevanter uses \u003ca href=\"https://github.com/ray-project/ray\"\u003eRay\u003c/a\u003e to orchestrate this process, seamlessly scaling the number of TPU slices up or down during a training job and, crucially, ensuring the job remains resilient if any single slice is preempted.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ehn3c\"\u003eThanks to JAX and XLA, Levanter and Marin were able to get similar high performance results on GPUs as well.\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"4k7bs\" id=\"fostering-scientific-trust-with-perfect-reproducibility\"\u003e\u003cb\u003e\u003cbr/\u003eFostering scientific trust with perfect reproducibility\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"eg7ni\"\u003e\u003cb\u003eProblem:\u003c/b\u003e A core goal of the Marin project is to enable verifiable science. This requires achieving reproducible results, even when training is paused, restarted, or moved between different machine configurations—a significant technical hurdle.\u003c/p\u003e\u003cp data-block-key=\"d73t\"\u003e\u003cb\u003eOur solution:\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"7v8i1\"\u003eThis was a fundamental requirement that drove Levanter\u0026#39;s design. We chose JAX specifically for its strong reproducibility guarantees, such as its default use of deterministic \u003ca href=\"https://docs.jax.dev/en/latest/_autosummary/jax.random.PRNGKey.html\"\u003epseudo-random number generators (PRNGs)\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"fc9e7\"\u003eThis choice was validated during the training of Marin-8B, which involved migrating between different TPU slices and hardware types while successfully maintaining bit-for-bit reproducibility across preemptions.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"b6knk\"\u003eLevanter also includes a robust data loading system built on Google’s \u003ca href=\"https://google.github.io/tensorstore/\"\u003eTensorstore\u003c/a\u003e library. Levanter’s data store offers deterministic, random access to any batch of training data, regardless of job restarts or data source changes—critical for supporting advanced training strategies like \u003ca href=\"https://vintagedata.org/blog/posts/what-is-mid-training\"\u003emid-training\u003c/a\u003e. JAX’s determinism and Levanter’s data store also make it easy for interpretability researchers to understand how specific data impacts the model during training.\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"jlpfy\" id=\"creating-a-cohesive-framework\"\u003e\u003cb\u003e\u003cbr/\u003eCreating a cohesive framework\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"5qcle\"\u003e\u003cb\u003eProblem:\u003c/b\u003e While JAX provides a powerful engine, no existing high-level framework met our stringent, combined requirements for legibility, massive scalability, \u003ci\u003eand\u003c/i\u003e bitwise determinism. We needed a complete, opinionated system to orchestrate the entire training process.\u003c/p\u003e\u003cp data-block-key=\"1l1gp\"\u003e\u003cb\u003eOur solution:\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"eete\"\u003eWe built Levanter, a JAX-native framework, from the ground up to be the system we needed: bitwise deterministic, scalable with advanced distribution strategies, and resilient.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"1qf4q\"\u003eWe could do this because JAX is more than just a library; it\u0026#39;s a \u0026#34;meta-framework\u0026#34; for building new tools. We built upon its mature, high-performance support for TPUs and its seamless integration of high-level abstractions (\u003ca href=\"https://docs.jax.dev/en/latest/_autosummary/jax.jit.html\"\u003e\u003ccode\u003ejit\u003c/code\u003e\u003c/a\u003e) with low-level control (\u003ca href=\"https://docs.jax.dev/en/latest/pallas/index.html\"\u003e\u003ccode\u003ePallas\u003c/code\u003e\u003c/a\u003e).\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"fn8i\"\u003eThis approach is common in the JAX community, which has produced a vibrant ecosystem of libraries like \u003ca href=\"https://flax.readthedocs.io/en/latest/\"\u003eFlax\u003c/a\u003e, \u003ca href=\"https://docs.kidger.site/equinox/\"\u003eEquinox\u003c/a\u003e, \u003ca href=\"https://orbax.readthedocs.io/en/latest/\"\u003eOrbax\u003c/a\u003e and \u003ca href=\"https://optax.readthedocs.io/\"\u003eOptax\u003c/a\u003e that work together, allowing teams like ours to build powerful solutions.\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"bdgqx\" id=\"a-look-under-the-hood:-the-voyage-of-marin-8b\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eA look under the hood: The voyage of Marin-8B\u003c/h2\u003e\u003cp data-block-key=\"1bq5p\"\u003eThe principles, tools and libraries discussed above were implemented and put to work during the Marin-8B training run. The model architecture is a Llama-style transformer.\u003c/p\u003e\u003ch3 data-block-key=\"1qy87\" id=\"marin-8b-base:-model-architecture-at-a-glance\"\u003e\u003cbr/\u003e\u003cb\u003eMarin-8B-Base: Model architecture at a glance\u003c/b\u003e\u003c/h3\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Marin-8B-Base-model-architecture_2.original.png\" alt=\"Marin 8B-Base model architecture at a glance\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"5q1ht\"\u003eRather than a static, monolithic run, the training of Marin-8B was an adaptive journey, internally dubbed the \u003ca href=\"https://marin.readthedocs.io/en/latest/reports/marin-8b-retro/\"\u003e\u0026#34;Tootsie\u0026#34; process\u003c/a\u003e. This honest portrayal of a real-world research workflow is detailed in the public. The process spanned over 12 trillion tokens and involved multiple phases that adapted to new data, techniques, and even different hardware configurations — migrating between large-scale, multi-slice TPU configurations (\u003ca href=\"https://cloud.google.com/tpu/docs/v5e\"\u003e2x v5e-256\u003c/a\u003e to \u003ca href=\"https://cloud.google.com/tpu/docs/v4\"\u003e1x v4-2048\u003c/a\u003e pods) mid-stream. The team continuously refined the data mixture, incorporating higher-quality sources, and adjusted hyperparameters like learning rate and batch size to optimize performance. This \u0026#34;messy\u0026#34; reality is a powerful educational tool, and the ability of the JAX and Levanter stack to handle these significant shifts while maintaining bit-for-bit reproducibility is a powerful demonstration of its robustness.\u003c/p\u003e\u003ch2 data-block-key=\"lpx79\" id=\"\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eJoin the Marin community\u003c/h2\u003e\u003cp data-block-key=\"37hie\"\u003eThe Marin project is an open invitation to participate in the future of foundation model development and contribute to the JAX ecosystem. The journey of Marin represents the answer to our question, \u0026#34;What\u0026#39;s next for openness?\u0026#34; This effort to create an \u003cb\u003e\u0026#39;open lab\u0026#39;\u003c/b\u003e is made possible by the technical capabilities of the JAX ecosystem. Its performance, portability, and foundational design for reproducibility are the key ingredients that allow us to make the \u0026#39;complete journey\u0026#39; of research accessible.\u003c/p\u003e\u003cp data-block-key=\"54e9p\"\u003eBy sharing everything from data methodologies to training logs, we aim to provide a fully reproducible resource—one that empowers researchers to deeply scrutinize, build upon, and trust the work. We believe this is a collaborative step toward a more transparent future for AI. We invite you to join us in this \u0026#39;open lab\u0026#39;—to use Marin, to contribute to the research, and to help build the next wave of innovative and trustworthy foundation models.\u003c/p\u003e\u003cp data-block-key=\"6sk8s\"\u003eThe central resource for the project is the official website, \u003ca href=\"http://marin.community/\"\u003emarin.community\u003c/a\u003e. From there, you can find the released models on \u003ca href=\"https://huggingface.co/marin-community\"\u003eHugging Face\u003c/a\u003e, explore the \u003ca href=\"https://github.com/stanford-crfm/marin\"\u003e\u0026#34;open lab\u0026#34; on GitHub\u003c/a\u003e, read \u003ca href=\"https://marin.readthedocs.io/en/latest/\"\u003eMarin documentation\u003c/a\u003e, and dive into the \u003ca href=\"https://github.com/stanford-crfm/levanter\"\u003eLevanter training framework\u003c/a\u003e. You can also test drive \u003ca href=\"https://colab.research.google.com/drive/1qlSMKGqpsg2TiaUCJPIz4xPSX5npQrsK?usp=sharing\"\u003eMarin in a colab\u003c/a\u003e with a simple inference example.\u003c/p\u003e\u003cp data-block-key=\"bh3l3\"\u003eAnd active discussions are taking place in the \u003ca href=\"https://discord.gg/J9CTk7pqcM\"\u003eDiscord channel\u003c/a\u003e where you can interact directly with other developers. For those new to the ecosystem, the official \u003ca href=\"http://jax.dev/\"\u003eJAX documentation\u003c/a\u003e provides excellent resources, including a \u003ca href=\"https://docs.jax.dev/en/latest/quickstart.html\"\u003eQuickstart guide\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2025-07-16T00:00:00Z",
  "modifiedTime": null
}
