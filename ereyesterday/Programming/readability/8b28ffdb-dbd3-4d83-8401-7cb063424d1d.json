{
  "id": "8b28ffdb-dbd3-4d83-8401-7cb063424d1d",
  "title": "DeepMind Researchers Propose Defense Against LLM Prompt Injection",
  "link": "https://www.infoq.com/news/2025/04/deepmind-camel-promt-injection/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "To prevent prompt injection attacks when working with untrusted sources, Google DeepMind researchers have proposed CaMeL, a defense layer around LLMs that blocks malicious inputs by extracting the control and data flows from the query. According to their results, CaMeL can neutralize 67% of attacks in the AgentDojo security benchmark. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Sat, 26 Apr 2025 15:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Large language models",
    "Prompt Engineering",
    "Data Privacy",
    "Security",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 4329,
  "excerpt": "To prevent prompt injection attacks when working with untrusted sources, Google DeepMind researchers have proposed CaMeL, a defense layer around LLMs that blocks malicious inputs by extracting the con",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250422123044_u1/apple-touch-icon.png",
  "text": "To prevent prompt injection attacks when working with untrusted sources, Google DeepMind researchers have proposed CaMeL, a defense layer around LLMs that blocks malicious inputs by extracting the control and data flows from the query. According to their results, CaMeL can neutralize 67% of attacks in the AgentDojo security benchmark. It is well known that adversaries can inject malicious data or instructions into an LLM's context to exfiltrate data or direct it to use tools in a harmful way. For instance, an attacker might attempt to discover a chatbot's system prompt to gain control or steal sensitive information– such as accessing data on private Slack channels. Even more concerning is when LLMs have access to tools to carry through actions with a real-world impact, such send sending an email or placing an order. Even when LLMs implement specific strategies to protect themselves from prompt injection, attackers continue to find ways to bypass these defenses. One recent example is a phishing-style attack demonstrated by AI security Johann Rehberger, who successfully circumvented Gemini's safeguard against delayed tool execution. CaMel is a new proposal to address this kinds of risks. Rather than relying on more AI to defend AI systems, such as an AI-based prompt injection detector, CaMeL applies traditional software security principles such as control flow integrity, access control, and information flow control. CaMeL associates, to every value, some metadata (commonly called capabilities in the software security literature) to restrict data and control flows, giving the possibility to express what can and cannot be done with each individual value by using fine-grained security policies. CaMel uses a custom Python interpreter to track the origin of data and instructions, enforcing capability-based security guarantees which do not require to modify the LLM itself. To this end, it leverages the Dual LLM pattern described by Simon Willison, who originally coined the term \"prompt injection\", and extends it in a clever way. Willison's original proposal features a privileged LLM that processes the user's prompt directly, and a quarantined LLM exposed to untrusted data with no access to tools. The privileged LLM is manages the workflow and may ask the quarantined LLM to extract specific information, such as en email address, from untrusted data. This ensures that the privileged LLM is never exposed to untrusted tokens, but only to the filtered results returned by the quarantined model. The weak point in this scheme, Google researchers say, is that an attacker could still manipulate the quarantined LLM into producing misleading output, e.g., the email address of a recipient not authorized to access sensitive information. In their new approach, the privileged LLM generates a program written in a restricted subset of Python, responsible for carrying through all required steps. When this program receives data from the quarantined LLM or other tools, it constructs a data flow graph tracking each data element's origin, access rights, and relevant metadata. This metadata is then used to ensure that any operation on the data complies with privilege restrictions. As Willison notes in his reaction to CaMeL proposal, the importance of this approach lies in not relying on more AI to solve AI problems: AI techniques use probabilities: you can train a model on a collection of previous prompt injection examples and get to a 99% score in detecting new ones... and that’s useless, because in application security 99% is a failing grade. To test CaMeL's effectiveness, DeepMind researchers integrated it into AgentDojo, a security benchmark featuring a set of realistic utility and security tasks for autonomous agents. CaMeL is not a perfect solution to LLM security, DeepMind researchers admit, with its most notable limitation being its reliance on users to define security policies. Moreover, as CaMeL may require users to manually approve privacy-sensitive tasks, there is a risk of user fatigue, which may lead to automatic, careless approvals. For a more details discussion, don’t miss the original paper. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/04/deepmind-camel-promt-injection/en/headerimage/deepmind-camel-prompt-injection-1745677636042.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eTo prevent prompt injection attacks when working with untrusted sources, \u003ca href=\"https://arxiv.org/abs/2503.18813\"\u003eGoogle DeepMind researchers have proposed CaMeL\u003c/a\u003e, a defense layer around LLMs that blocks malicious inputs by extracting the control and data flows from the query. According to their results, CaMeL can neutralize 67% of attacks in the AgentDojo security benchmark.\u003c/p\u003e\n\n\u003cp\u003eIt is well known that adversaries can inject malicious data or instructions into an LLM\u0026#39;s context to exfiltrate data or direct it to use tools in a harmful way. For instance, an attacker might attempt to discover a chatbot\u0026#39;s system prompt to gain control or steal sensitive information– such as \u003ca href=\"https://www.infoq.com/articles/large-language-models-prompt-injection-stealing/\"\u003eaccessing data on private Slack channels\u003c/a\u003e. Even more concerning is when LLMs have access to tools to carry through actions with a real-world impact, such send sending an email or placing an order.\u003c/p\u003e\n\n\u003cp\u003eEven when LLMs implement specific strategies to protect themselves from prompt injection, attackers continue to find ways to bypass these defenses. One recent example is a \u003ca href=\"https://www.infoq.com/news/2025/02/gemini-long-term-memory-attack/\"\u003ephishing-style attack demonstrated by AI security Johann Rehberger, who successfully circumvented Gemini\u0026#39;s safeguard against delayed tool execution\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eCaMel is a new proposal to address this kinds of risks. Rather than relying on more AI to defend AI systems, such as an AI-based prompt injection detector, CaMeL applies traditional software security principles such as control flow integrity, access control, and information flow control.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eCaMeL associates, to every value, some metadata (commonly called capabilities in the software security literature) to restrict data and control flows, giving the possibility to express what can and cannot be done with each individual value by using fine-grained security policies.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eCaMel uses a custom Python interpreter to track the origin of data and instructions, enforcing capability-based security guarantees which do not require to modify the LLM itself. To this end, it leverages the Dual LLM pattern described by Simon Willison, \u003ca href=\"https://simonwillison.net/2022/Sep/12/prompt-injection/\"\u003ewho originally coined the term \u0026#34;prompt injection\u0026#34;\u003c/a\u003e, and extends it in a clever way.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://simonwillison.net/2023/Apr/25/dual-llm-pattern/\"\u003eWillison\u0026#39;s original proposal\u003c/a\u003e features a privileged LLM that processes the user\u0026#39;s prompt directly, and a quarantined LLM exposed to untrusted data with no access to tools. The privileged LLM is manages the workflow and may ask the quarantined LLM to extract specific information, such as en email address, from untrusted data. This ensures that the privileged LLM is never exposed to untrusted tokens, but only to the filtered results returned by the quarantined model.\u003c/p\u003e\n\n\u003cp\u003eThe weak point in this scheme, Google researchers say, is that an attacker could still manipulate the quarantined LLM into producing misleading output, e.g., the email address of a recipient not authorized to access sensitive information.\u003c/p\u003e\n\n\u003cp\u003eIn their new approach, the privileged LLM generates a program written in a restricted subset of Python, responsible for carrying through all required steps. When this program receives data from the quarantined LLM or other tools, it constructs a data flow graph tracking each data element\u0026#39;s origin, access rights, and relevant metadata. This metadata is then used to ensure that any operation on the data complies with privilege restrictions.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/04/deepmind-camel-promt-injection/en/resources/1google-deepmind-camel-1745677635057.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/04/deepmind-camel-promt-injection/en/resources/1google-deepmind-camel-1745677635057.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eAs Willison notes in his reaction to CaMeL proposal, \u003ca href=\"https://simonwillison.net/2025/Apr/11/camel/\"\u003ethe importance of this approach lies in not relying on more AI to solve AI problems\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAI techniques use probabilities: you can train a model on a collection of previous prompt injection examples and get to a 99% score in detecting new ones... and that’s useless, because in application security 99% is a failing grade.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eTo test CaMeL\u0026#39;s effectiveness, DeepMind researchers integrated it into AgentDojo, a security benchmark featuring a set of realistic utility and security tasks for autonomous agents.\u003c/p\u003e\n\n\u003cp\u003eCaMeL is not a perfect solution to LLM security, DeepMind researchers admit, with its most notable limitation being its reliance on users to define security policies. Moreover, as CaMeL may require users to manually approve privacy-sensitive tasks, there is a risk of user fatigue, which may lead to automatic, careless approvals.\u003c/p\u003e\n\n\u003cp\u003eFor a more details discussion, don’t miss the original paper.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-04-26T00:00:00Z",
  "modifiedTime": null
}
