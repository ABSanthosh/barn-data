{
  "id": "d71d8f05-ea17-40a8-bcc1-50f555af77e5",
  "title": "Gemma 3n Available for On-Device Inference Alongside RAG and Function Calling Libraries",
  "link": "https://www.infoq.com/news/2025/05/gemma-3n-on-device-inference/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Google has announced that Gemma 3n is now available in preview on the new LiteRT Hugging Face community, alongside many previously released models. Gemma 3n is a multimodal small language model that supports text, image, video, and audio inputs. It also supports finetuning, customization through retrieval-augmented generation (RAG), and function calling using new AI Edge SDKs. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Thu, 29 May 2025 16:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Gemma",
    "Edge Computing",
    "Android",
    "Open Source",
    "Mobile",
    "iOS",
    "Google",
    "Development",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3729,
  "excerpt": "Google has announced that Gemma 3n is now available in preview on the new LiteRT Hugging Face community, alongside many previously released models. Gemma 3n is a multimodal small language model that s",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250527074915/apple-touch-icon.png",
  "text": "Google has announced that Gemma 3n is now available in preview on the new LiteRT Hugging Face community, alongside many previously released models. Gemma 3n is a multimodal small language model that supports text, image, video, and audio inputs. It also supports finetuning, customization through retrieval-augmented generation (RAG), and function calling using new AI Edge SDKs. Gemma 3n is available in two parameter variants, Gemma 3n 2B and Gemma 3n 4B, both supporting text and image inputs, with audio support coming soon, according to Google. This marks a notable increase in size compared to the earlier non-multimodal Gemma 3 1B, which launched earlier this year and required just 529MB to process up to 2,585 tokens per second on a mobile GPU. Gemma 3n is great for enterprise use cases where developers have the full resources of the device available to them, allowing for larger models on mobile. Field technicians with no service could snap a photo of a part and ask a question. Workers in a warehouse or a kitchen could update inventory via voice while their hands were full. According to Google, Gemma 3n use selective parameter activation, a technique for efficient parameter management. This means the two models contain more parameters than the 2B or 4B that are active during inference. Google highlights the ability for developers to fine-tune the base model and then convert and quantize it using new quantization tools available through Google AI Edge. With the latest release of our quantization tools, we have new quantization schemes that allow for much higher quality int4 post training quantization. Compared to bf16, the default data type for many models, int4 quantization can reduce the size of language models by a factor of 2.5-4X while significantly decreasing latency and peak memory consumption. As an alternative to fine-tuning, the models can be used for on-device Retrieval Augmented Generation (RAG), which enhances a language model with application-specific data. This capability is powered by the AI Edge RAG library, currently available only on Android and coming in future on other platforms. The RAG library uses a simple pipeline including several steps: data import, chunking and indexing, embeddings generation, information retrieval, and response generation using an LLM. It allows full customization of the RAG pipeline, including support for custom databases, chunking strategies, and retrieval functions. Alongside Gemma 3n, Google also announced the AI Edge On-device Function Calling SDK, also currently available only on Android, which enables models to call specific functions to execute real-world actions. Rather than just generating text, an LLM using the FC SDK can generate a structured call to a function that executes an action, such as searching for up-to-date information, setting alarms, or making reservations. To integrate an LLM with an external function, you describe the function by specifying its name, a description to guide the LLM on when to use it, and the parameters it requires. This metadata is placed into a Tool object that is passed to the large language model via the GenerativeModel constructor. The function calling SDK includes support for receiving function calls from the LLM based on the description you provided, and sending execution results back to the LLM. If you want to take a closer look at these new tools, the best place to start is the Google AI Edge Gallery, an experimental app that showcases various models and supports text, image, and audio processing. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/05/gemma-3n-on-device-inference/en/headerimage/gemma-3n-on-device-inference-1748531614779.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eGoogle has announced that \u003ca href=\"https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling/\"\u003eGemma 3n is now available in preview\u003c/a\u003e on the new \u003ca href=\"https://huggingface.co/litert-community\"\u003eLiteRT Hugging Face community\u003c/a\u003e, alongside many previously released models. Gemma 3n is a multimodal small language model that supports text, image, video, and audio inputs. It also supports finetuning, customization through retrieval-augmented generation (RAG), and function calling using new AI Edge SDKs.\u003c/p\u003e\n\n\u003cp\u003eGemma 3n is available in two parameter variants, \u003ca href=\"https://huggingface.co/google/gemma-3n-E2B-it-litert-preview\"\u003eGemma 3n 2B\u003c/a\u003e and \u003ca href=\"https://huggingface.co/google/gemma-3n-E4B-it-litert-preview\"\u003eGemma 3n 4B\u003c/a\u003e, both supporting text and image inputs, with audio support coming soon, according to Google. This marks a notable increase in size compared to the earlier non-multimodal Gemma 3 1B, which launched earlier this year and required just 529MB to process up to 2,585 tokens per second on a mobile GPU.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eGemma 3n is great for enterprise use cases where developers have the full resources of the device available to them, allowing for larger models on mobile. Field technicians with no service could snap a photo of a part and ask a question. Workers in a warehouse or a kitchen could update inventory via voice while their hands were full.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAccording to Google, \u003ca href=\"https://ai.google.dev/gemma/docs/gemma-3n#parameters\"\u003eGemma 3n use selective parameter activation\u003c/a\u003e, a technique for efficient parameter management. This means the two models contain more parameters than the 2B or 4B that are active during inference.\u003c/p\u003e\n\n\u003cp\u003eGoogle highlights the ability for developers to fine-tune the base model and then \u003ca href=\"https://colab.sandbox.google.com/github/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb\"\u003econvert and quantize it\u003c/a\u003e using new quantization tools available through Google AI Edge.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWith the latest release of our quantization tools, we have new quantization schemes that allow for much higher quality int4 post training quantization. Compared to bf16, the default data type for many models, int4 quantization can reduce the size of language models by a factor of 2.5-4X while significantly decreasing latency and peak memory consumption.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAs an alternative to fine-tuning, the models can be used for on-device Retrieval Augmented Generation (RAG), which enhances a language model with application-specific data. This capability is powered by the \u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/genai/rag\"\u003eAI Edge RAG library\u003c/a\u003e, currently available only on Android and coming in future on other platforms.\u003c/p\u003e\n\n\u003cp\u003eThe RAG library uses a simple pipeline including several steps: data import, chunking and indexing, embeddings generation, information retrieval, and response generation using an LLM. It allows full customization of the RAG pipeline, including support for custom databases, chunking strategies, and retrieval functions.\u003c/p\u003e\n\n\u003cp\u003eAlongside Gemma 3n, Google also announced the \u003ca href=\"https://github.com/google-ai-edge/ai-edge-apis/tree/main/local_agents/function_calling\"\u003eAI Edge On-device Function Calling SDK\u003c/a\u003e, also currently available only on Android, which enables models to call specific functions to execute real-world actions.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eRather than just generating text, an LLM using the FC SDK can generate a structured call to a function that executes an action, such as searching for up-to-date information, setting alarms, or making reservations.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eTo integrate an LLM with an external function, you describe the function by specifying its name, a description to guide the LLM on when to use it, and the parameters it requires. This metadata is placed into a \u003ccode\u003eTool\u003c/code\u003e object that is passed to the large language model via the \u003ccode\u003eGenerativeModel\u003c/code\u003e constructor. The function calling SDK includes support for receiving function calls from the LLM based on the description you provided, and sending execution results back to the LLM.\u003c/p\u003e\n\n\u003cp\u003eIf you want to take a closer look at these new tools, the best place to start is the \u003ca href=\"https://github.com/google-ai-edge/gallery\"\u003eGoogle AI Edge Gallery\u003c/a\u003e, an experimental app that showcases various models and supports text, image, and audio processing.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-05-29T00:00:00Z",
  "modifiedTime": null
}
