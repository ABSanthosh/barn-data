{
  "id": "ec75ef3e-3094-46f8-88f0-264295e3ac92",
  "title": "Microsoft Phi-4 is a Small Language Model Specialized for Complex Math Reasoning",
  "link": "https://www.infoq.com/news/2025/01/microsoft-phi-4/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Phi-4 is 14B parameter model from Microsoft Research that aims to improve the state of the art for math reasoning. Previously available on Azure AI Foundry, Phi-4 has recently become available on Hugging Face under the MIT license. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Fri, 24 Jan 2025 18:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Hugging Face",
    "Microsoft",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3370,
  "excerpt": "Phi-4 is 14B parameter model from Microsoft Research that aims to improve the state of the art for math reasoning. Previously available on Azure AI Foundry, Phi-4 has recently become available on Hugg",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250124075613/apple-touch-icon.png",
  "text": "Phi-4 is 14B parameter model from Microsoft Research that aims to improve the state of the art for math reasoning. Previously available on Azure AI Foundry, Phi-4 has recently become available on Hugging Face under the MIT license. According to Microsoft, Phi-4 outperforms comparable and larger models on math reasoning thanks to a number of innovations throughout the training process, including the use of synthetic data for pre-training and mid-training, curation and filtering of organic data, and a new post-training scheme. This approach, Microsoft says, produced a significant improvement over previous models in the Phi family: While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. The use of synthetic data is not new for LLMs or Phi models in particular. Microsoft says that using synthetic data is not a cheap substitute for organic data but offers distinct advantages over the latter by providing a more gradual learning path and better alignment with inference contexts. For example, organic data from the Web could include the statement of a mathematical problem followed by the final solution, with the reasoning steps coming afterward. This makes it harder for an LLM to learn to generate the solution from the problem statement. In contrast, a synthetic description of the problem would lead the LLM step by step from the initial problem statement to the final solution. Along with synthetic data, Microsoft also used curated organic data, including tens of millions of high-quality organic problems and solutions from public websites and external datasets. In cases where accurate solutions were not provided, they were generated synthetically using majority voting to increase accuracy. Academic papers, educational forums, and programming tutorials were also collected. We found clean and correct natural data to be absolutely crucial for seeding synthetic data: minor errors can result in severe quality degradations for derived synthetic documents. We therefore invested heavily in the perfectionistic curation of our web data. The post-training phase for Phi-4 was aimed at transforming the pretrained model into a reliable AI assistant. In the first step, Microsoft fine-tuned the model with data generated from high-quality data across diverse domains, including math, coding, reasoning, conversation, model identity, and safety. Then, they ran two direct preference optimization (DPO) steps to better align the model with human preferences and exclude undesired behavior. In the first step, Microsoft used a new technique, called Pivotal Token Search, to generate pairs of desired/undesired results; in the second, they relied on GPT-4o as a judge to label positive or negative each given pair. Phi-4 was evaluated on a set of benchmarks using OpenAI's SIMPLE-EVALS framework and outperformed Llama-3.1-405B on several of them as well as its teacher model GPT-4o on the GPQA (graduate-level STEM Q\u0026A) and MATH (math competition) benchmarks. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/01/microsoft-phi-4/en/headerimage/microsoft-phi-4-1737739784899.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090\"\u003ePhi-4 is 14B parameter model from Microsoft Research that aims to improve the state of the art for math reasoning\u003c/a\u003e. Previously available on Azure AI Foundry, Phi-4 has recently become available on \u003ca href=\"https://huggingface.co/microsoft/phi-4\"\u003eHugging Face\u003c/a\u003e under the MIT license.\u003c/p\u003e\n\n\u003cp\u003eAccording to Microsoft, Phi-4 outperforms comparable and larger models on math reasoning thanks to a number of innovations throughout the training process, including the use of synthetic data for pre-training and mid-training, curation and filtering of organic data, and a new post-training scheme. This approach, Microsoft says, produced a significant improvement over \u003ca href=\"https://www.infoq.com/news/2024/08/microsoft-phi-3-5/\"\u003eprevious models in the Phi family\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhile previous models in the Phi family largely \u003cem\u003edistill\u003c/em\u003e the capabilities of a teacher model (specifically GPT-4), phi-4 substantially \u003cem\u003esurpasses\u003c/em\u003e its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2412.08905\"\u003eThe use of synthetic data is not new for LLMs or Phi models in particular\u003c/a\u003e. Microsoft says that using synthetic data is not a cheap substitute for organic data but offers distinct advantages over the latter by providing a more gradual learning path and better alignment with inference contexts. For example, organic data from the Web could include the statement of a mathematical problem followed by the final solution, with the reasoning steps coming afterward. This makes it harder for an LLM to learn to generate the solution from the problem statement. In contrast, a synthetic description of the problem would lead the LLM step by step from the initial problem statement to the final solution.\u003c/p\u003e\n\n\u003cp\u003eAlong with synthetic data, Microsoft also used curated organic data, including tens of millions of high-quality organic problems and solutions from public websites and external datasets. In cases where accurate solutions were not provided, they were generated synthetically using majority voting to increase accuracy. Academic papers, educational forums, and programming tutorials were also collected.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWe found clean and correct natural data to be absolutely crucial for seeding synthetic data: minor errors can result in severe quality degradations for derived synthetic documents. We therefore invested heavily in the perfectionistic curation of our web data.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe post-training phase for Phi-4 was aimed at transforming the pretrained model into a reliable AI assistant. In the first step, Microsoft fine-tuned the model with data generated from high-quality data across diverse domains, including math, coding, reasoning, conversation, model identity, and safety. Then, they ran two direct preference optimization (DPO) steps to better align the model with human preferences and exclude undesired behavior. In the first step, Microsoft used a new technique, called Pivotal Token Search, to generate pairs of desired/undesired results; in the second, they relied on GPT-4o as a judge to label positive or negative each given pair.\u003c/p\u003e\n\n\u003cp\u003ePhi-4 was evaluated on a set of benchmarks using OpenAI\u0026#39;s SIMPLE-EVALS framework and outperformed Llama-3.1-405B on several of them as well as its teacher model GPT-4o on the GPQA (graduate-level STEM Q\u0026amp;A) and MATH (math competition) benchmarks.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-01-24T00:00:00Z",
  "modifiedTime": null
}
