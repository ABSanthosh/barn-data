{
  "id": "13943bb5-03ea-4a18-8ea1-1e99edf68ac0",
  "title": "Simulating a neural operating system with Gemini 2.5 Flash-Lite",
  "link": "https://developers.googleblog.com/en/simulating-a-neural-operating-system-with-gemini-2-5-flash-lite/",
  "description": "A research prototype simulating a neural operating system generates UI in real-time adapting to user interactions with Gemini 2.5 Flash-Lite, using interaction tracing for contextual awareness, streaming the UI for responsiveness, and achieving statefulness with an in-memory UI graph.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "D Shin, Ali Eslami, Madhavi Sewak",
  "length": 6049,
  "excerpt": "A research prototype simulating a neural operating system generates UI in real-time adapting to user interactions with Gemini 2.5 Flash-Lite, using interaction tracing for contextual awareness, streaming the UI for responsiveness, and achieving statefulness with an in-memory UI graph.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "In traditional computing, user interfaces are pre-defined. Every button, menu, and window is meticulously coded by developers. But what if an interface could be generated in real time, adapting to a user's context with each interaction? We explored this question by building a research prototype (view demo app in Google AI Studio) for a generative, infinite computer experience.Our prototype simulates an operating system where each screen is generated on the fly by a large language model. It uses Gemini 2.5 Flash-Lite, a model whose low latency is critical for creating a responsive interaction that feels instantaneous. Instead of navigating a static file system, the user interacts with an environment that the model builds and rebuilds with every click. This post outlines the core technical concepts behind this prototype. Sorry, your browser doesn't support playback for this video Conditioning the model for on-the-fly UI generationTo generate a UI on-the-fly, we need to provide the model with a clear structure and context for each request. We engineered our prompt by dividing the model's input into two parts: a \"UI constitution\" and a \"UI interaction\".The UI constitution is a system prompt that contains a fixed set of rules for UI generation. These rules define consistent elements like the OS-level styling, the home screen format, and logic for embedding elements like maps.The UI interaction is a JSON object that captures the user's most recent action, such as a mouse click on an icon. This object serves as the specific query that prompts the model to generate the next screen. For example, clicking on a “Save Note” icon within the Notepad app may generate an object as the following: { // `id`: The unique ID from the button's `data-interaction-id` attribute. id: 'save_note_action', // `type`: The interaction type from `data-interaction-type`. type: 'button_press', // `value`: Because the button has a `data-value-from` attribute, the system // retrieves the content from the textarea with the ID 'notepad_main_textarea'. value: 'Meeting notes\\n- Discuss Q3 roadmap\\n- Finalize budget', // `elementType`: The HTML tag of the element that was clicked. elementType: 'button', // `elementText`: The visible text inside the button. elementText: 'Save Note', // `appContext`: The ID of the application the user is currently in. // This comes from the `activeApp` state in `App.tsx`. appContext: 'notepad_app' } JSON Copied This two-part, context-setting approach allows the model to maintain a consistent look- and- feel while generating novel screens based on specific, real-time user inputs.Using interaction tracing for contextual awarenessA single interaction provides immediate context, but a sequence of interactions tells a richer story. Our prototype can use a trace of the past N interactions to generate a more contextually relevant screen. For example, the content generated within a calculator app could differ depending on whether the user previously visited a shopping cart or a travel booking app. By adjusting the length of this interaction trace, we can tune the balance between contextual accuracy and UI variability.Streaming the UI for a responsive experienceTo make the system feel fast, we can't wait for the model to generate the entire UI screen before rendering. Our prototype leverages model streaming and the browser's native parser to implement progressive rendering. As the model generates HTML code in chunks, we continuously append it to our component's state. React then re-renders the content, allowing the browser to display valid HTML elements as soon as they are received. For the user, this creates the experience of an interface materializing on screen almost instantly.Achieving statefulness with a generative UI graphBy default, our model generates a new screen from scratch with each user input. This means visiting the same folder twice could produce entirely different contents. Such non-deterministic, stateless experience may not always be preferred given that the GUI we are used to is static. To introduce statefulness to our prototype, our demo system has an option to build an in-memory cache for modeling a session-specific UI graph. When a user navigates to a screen that has already been generated, the system serves the stored version from the graph, without querying Gemini again. When the user requests a new screen not in cache, the UI graph grows incrementally. This method provides state without compromising the quality of the generative output, which can be a side effect of simply lowering the model's sampling temperature.Potential applications for just-in-time generative UIWhile this is a conceptual prototype, the underlying framework could be applied to more practical use cases.Contextual shortcuts: A system could observe a user's interaction patterns and generate an ephemeral UI panel to accelerate their task. For instance, as the user is comparing flights across multiple websites, a floating widget could just-in-time appear with dynamically generated buttons for comparing prices or booking a flight directly, saving the user several steps.“Generative mode” in existing apps: Developers could add a \"generative mode\" to their applications. In Google Calendar, for example, a user could activate this mode to see just-in-time UIs. When moving a calendar invite, instead of a standard dialog, the system could generate a screen presenting the best alternative times as a series of directly selectable buttons based on attendees' schedules. This would create a hybrid experience where generative and static UI elements coexist seamlessly in one application.Exploring novel concepts like this helps us understand how new paradigms for human-computer interaction are evolving. As models continue to get faster and more capable, we believe generative interfaces represent a promising area for future research and development.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini-computer_1.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n         \u003cdiv\u003e\n    \u003cp data-block-key=\"ws82h\"\u003eIn traditional computing, user interfaces are pre-defined. Every button, menu, and window is meticulously coded by developers. But what if an interface could be generated in real time, adapting to a user\u0026#39;s context with each interaction? We explored this question by building a \u003ca href=\"https://x.com/OriolVinyalsML/status/1935005985070084197\"\u003eresearch prototype\u003c/a\u003e (view \u003ca href=\"https://aistudio.google.com/apps/bundled/gemini_os\"\u003edemo app\u003c/a\u003e in Google AI Studio) for a generative, infinite computer experience.\u003c/p\u003e\u003cp data-block-key=\"1oa6n\"\u003eOur prototype simulates an operating system where each screen is generated on the fly by a large language model. It uses \u003ca href=\"https://deepmind.google/models/gemini/flash-lite/\"\u003eGemini 2.5 Flash-Lite\u003c/a\u003e, a model whose low latency is critical for creating a responsive interaction that feels instantaneous. Instead of navigating a static file system, the user interacts with an environment that the model builds and rebuilds with every click. This post outlines the core technical concepts behind this prototype.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-wqr_hxsh_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/GDM-25042_Flash_Lite_Gemini_Computer_DEMO_MAP_EG1.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"61eaj\" id=\"conditioning-the-model-for-on-the-fly-ui-generation\"\u003eConditioning the model for on-the-fly UI generation\u003c/h2\u003e\u003cp data-block-key=\"ffj56\"\u003eTo generate a UI on-the-fly, we need to provide the model with a clear structure and context for each request. We engineered our prompt by dividing the model\u0026#39;s input into two parts: a \u0026#34;UI constitution\u0026#34; and a \u0026#34;UI interaction\u0026#34;.\u003c/p\u003e\u003cp data-block-key=\"dtna2\"\u003eThe \u003cb\u003eUI constitution\u003c/b\u003e is a system prompt that contains a fixed set of rules for UI generation. These rules define consistent elements like the OS-level styling, the home screen format, and logic for embedding elements like maps.\u003c/p\u003e\u003cp data-block-key=\"l7o4\"\u003eThe \u003cb\u003eUI interaction\u003c/b\u003e is a JSON object that captures the user\u0026#39;s most recent action, such as a mouse click on an icon. This object serves as the specific query that prompts the model to generate the next screen. For example, clicking on a “Save Note” icon within the Notepad app may generate an object as the following:\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003e{\n  // `id`: The unique ID from the button\u0026#39;s `data-interaction-id` attribute.\n  id: \u0026#39;save_note_action\u0026#39;,\n\n  // `type`: The interaction type from `data-interaction-type`.\n  type: \u0026#39;button_press\u0026#39;,\n\n  // `value`: Because the button has a `data-value-from` attribute, the system\n  // retrieves the content from the textarea with the ID \u0026#39;notepad_main_textarea\u0026#39;.\n  value: \u0026#39;Meeting notes\\n- Discuss Q3 roadmap\\n- Finalize budget\u0026#39;,\n\n  // `elementType`: The HTML tag of the element that was clicked.\n  elementType: \u0026#39;button\u0026#39;,\n\n  // `elementText`: The visible text inside the button.\n  elementText: \u0026#39;Save Note\u0026#39;,\n\n  // `appContext`: The ID of the application the user is currently in.\n  // This comes from the `activeApp` state in `App.tsx`.\n  appContext: \u0026#39;notepad_app\u0026#39;\n}\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        JSON\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"ws82h\"\u003eThis two-part, context-setting approach allows the model to maintain a consistent look- and- feel while generating novel screens based on specific, real-time user inputs.\u003c/p\u003e\u003ch2 data-block-key=\"iv79v\" id=\"using-interaction-tracing-for-contextual-awareness\"\u003e\u003cbr/\u003eUsing interaction tracing for contextual awareness\u003c/h2\u003e\u003cp data-block-key=\"fi0if\"\u003eA single interaction provides immediate context, but a sequence of interactions tells a richer story. Our prototype can use a trace of the past \u003ci\u003eN\u003c/i\u003e interactions to generate a more contextually relevant screen. For example, the content generated within a calculator app could differ depending on whether the user previously visited a shopping cart or a travel booking app. By adjusting the length of this interaction trace, we can tune the balance between contextual accuracy and UI variability.\u003c/p\u003e\u003ch2 data-block-key=\"cksb4\" id=\"streaming-the-ui-for-a-responsive-experience\"\u003e\u003cbr/\u003eStreaming the UI for a responsive experience\u003c/h2\u003e\u003cp data-block-key=\"a0e20\"\u003eTo make the system feel fast, we can\u0026#39;t wait for the model to generate the entire UI screen before rendering. Our prototype leverages model streaming and the browser\u0026#39;s native parser to implement progressive rendering. As the model generates HTML code in chunks, we continuously append it to our component\u0026#39;s state. React then re-renders the content, allowing the browser to display valid HTML elements as soon as they are received. For the user, this creates the experience of an interface materializing on screen almost instantly.\u003c/p\u003e\u003ch2 data-block-key=\"ubreu\" id=\"achieving-statefulness-with-a-generative-ui-graph\"\u003e\u003cbr/\u003eAchieving statefulness with a generative UI graph\u003c/h2\u003e\u003cp data-block-key=\"3opj0\"\u003eBy default, our model generates a new screen from scratch with each user input. This means visiting the same folder twice could produce entirely different contents. Such non-deterministic, stateless experience may not always be preferred given that the GUI we are used to is static. To introduce statefulness to our prototype, our demo system has an option to build an in-memory cache for modeling a session-specific UI graph. When a user navigates to a screen that has already been generated, the system serves the stored version from the graph, without querying Gemini again. When the user requests a new screen not in cache, the UI graph grows incrementally. This method provides state without compromising the quality of the generative output, which can be a side effect of simply lowering the model\u0026#39;s sampling temperature.\u003c/p\u003e\u003ch2 data-block-key=\"n5h96\" id=\"potential-applications-for-just-in-time-generative-ui\"\u003e\u003cbr/\u003ePotential applications for just-in-time generative UI\u003c/h2\u003e\u003cp data-block-key=\"avjrm\"\u003eWhile this is a conceptual prototype, the underlying framework could be applied to more practical use cases.\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"9j3vf\"\u003e\u003cb\u003eContextual shortcuts:\u003c/b\u003e A system could observe a user\u0026#39;s interaction patterns and generate an ephemeral UI panel to accelerate their task. For instance, as the user is comparing flights across multiple websites, a floating widget could just-in-time appear with dynamically generated buttons for comparing prices or booking a flight directly, saving the user several steps.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"d49oa\"\u003e\u003cb\u003e“Generative mode” in existing apps:\u003c/b\u003e Developers could add a \u0026#34;generative mode\u0026#34; to their applications. In Google Calendar, for example, a user could activate this mode to see just-in-time UIs. When moving a calendar invite, instead of a standard dialog, the system could generate a screen presenting the best alternative times as a series of directly selectable buttons based on attendees\u0026#39; schedules. This would create a hybrid experience where generative and static UI elements coexist seamlessly in one application.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"4gbpn\"\u003e\u003cbr/\u003eExploring novel concepts like this helps us understand how new paradigms for human-computer interaction are evolving. As models continue to get faster and more capable, we believe generative interfaces represent a promising area for future research and development.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-06-25T00:00:00Z",
  "modifiedTime": null
}
