{
  "id": "33cbaff3-4ad4-42ee-92a8-9f8d39caa6a7",
  "title": "A guide to deciding what AI model to use in GitHub Copilot",
  "link": "https://github.blog/ai-and-ml/github-copilot/a-guide-to-deciding-what-ai-model-to-use-in-github-copilot/",
  "description": "What to look for with each model and how to test them in your workflows—with tips, tricks, and pointers. The post A guide to deciding what AI model to use in GitHub Copilot appeared first on The GitHub Blog.",
  "author": "Klint Finley",
  "published": "Thu, 24 Apr 2025 16:00:51 +0000",
  "source": "https://github.blog/feed/",
  "categories": [
    "AI \u0026 ML",
    "Generative AI",
    "GitHub Copilot",
    "LLMs",
    "AI models",
    "generative AI",
    "reasoning models"
  ],
  "byline": "Klint Finley",
  "length": 8365,
  "excerpt": "Explore a framework—including a few strategies—for evaluating whether any given AI model is a good fit for your use.",
  "siteName": "The GitHub Blog",
  "favicon": "https://github.blog/wp-content/uploads/2019/01/cropped-github-favicon-512.png?fit=192%2C192",
  "text": "To ensure that you have access to the best technology available, we’re continuously adding support for new models to GitHub Copilot. That being said, we know it can be hard to keep up with so many new models being released all the time. All of this raises an obvious question: Which model should you use? You can read our recent blog post for an overview of the models currently available in Copilot and their strengths, or check out our documentation for a deep dive comparing different models and tasks. But the AI landscape moves quickly. In this article we’ll explore a framework—including a few strategies—for evaluating whether any given AI model is a good fit for your use, even as new models continue to appear at a rapid pace. It’s hard to go wrong with our base model, which has been fine-tuned specifically for programming-related tasks. But depending on what you’re working on, you likely have varying needs and preferences. There’s no single “best” model. Some may favor a more verbose model for chat, while others prefer a terse one, for example. We spoke with several developers about their model selection process. Keep reading to discover how to apply their strategies to your own needs. 💡 Watch the video below for tips on prompt engineering to get the best results. Why use multiple models? There’s no reason you have to pick one model and stick with it. Since you can easily switch between models for both chat and code completion with GitHub Copilot, you can use different models for different use cases. It's kind of like dogfooding your own stack: You won’t know if it really fits your workflow until you've shipped some real code with it. - Anand Chowdhary, FirstQuadrant CTO and co-founder Chat vs. code completion Using one model for chat and another for autocomplete is one of the most common patterns we see among developers. Generally, developers prefer autocompletion models because they’re fast and responsive, which they need if they’re looking for suggestions as they think and type. Developers are more tolerant of latency in chat, when they’re in more of an exploratory state of mind (like considering a complex refactoring job, for instance). Reasoning models for certain programming tasks Reasoning models like OpenAI o1 often respond slower than traditional LLMs such as GPT-4o or Claude Sonnet 3.5. That’s in large part because these models break a prompt down into parts and consider multiple approaches to a problem. That introduces latency in their response times, but makes them more effective at completing complex tasks. Many developers prefer these more deliberative models for particular tasks. For instance, Fatih Kadir Akın, a developer relations manager, uses o1 when starting new projects from scratch. “Reasoning models better ‘understand’ my vision and create more structured projects than non-reasoning models,” he explains. FirstQuadrant CTO and co-founder Anand Chowdhary favors reasoning models for large-scale code refactoring jobs. “A model that rewrites complex backend code without careful reasoning is rarely accurate the first time,” he says. “Seeing the thought process also helps me understand the changes.” When creating technical interview questions for her newsletter, GitHub Senior Director of Developer Advocacy, Cassidy Williams mixes models for certain tasks. When she writes a question, she uses GPT-4o to refine the prose, and then Claude 3.7 Sonnet Thinking to verify code accuracy. “Reasoning models help ensure technical correctness because of their multi-step process,” she says. “If they initially get something wrong, they often correct themselves in later steps so the final answer is more accurate.” There’s some subjectivity, but I compare model output based on the code structure, patterns, comments, and adherence to best practices. - Portilla Edo, cloud infrastructure engineering lead What to look for in a new AI model Let’s say a new model just dropped and you’re ready to try it out. Here are a few things to consider before making it your new go-to. Recentness Different models use different training data. That means one model might have more recent data than another, and therefore might be trained on new versions of the programming languages, frameworks, and libraries you use. “When I’m trying out a new model, one of the first things I do is check how up to date it is,” says Xavier Portilla Edo, a cloud infrastructure engineering lead. He typically does this by creating a project manifest file for the project to see what version numbers Copilot autocomplete suggests. “If the versions are quite old, I’ll move on,” he says. Speed and responsiveness As mentioned, developers tend to tolerate more latency in a chat than in autocomplete. But responsiveness is still important in chat. “I enjoy bouncing ideas off a model and getting feedback,” says Rishab Kumar, a staff developer evangelist at Twilio. “For that type of interaction, I need fast responses so I can stay in the flow.” Accuracy Naturally, you need to evaluate which models produce the best code. “There’s some subjectivity, but I compare model output based on the code structure, patterns, comments, and adherence to best practices,” Portilla Edo says. “I also look at how readable and maintainable the code is—does it follow naming conventions? Is it modular? Are the comments helpful or just restating what the code does? These are all signals of quality that go beyond whether the code simply runs.” How to test an AI model in your workflow OK, so now you know what to look for in a model. But how do you actually evaluate it for responsiveness and correctness? You use it, of course. Start with a simple app Akın will generally start with a simple todo app written in vanilla JavaScript. “I just check the code, and how well it’s structured,” he says. Similarly, Kumar will start with a websocket server in Python. The idea is to start with something that you understand well enough to evaluate, and then layer on more complexity. “Eventually I’ll see if it can build something in 3D using 3js,” Akın says. Portilla Edo starts by prompting a new model he wants to evaluate in Copilot Chat. “I usually ask it for simple things, like a function in Go, or a simple HTML file,” he says. Then he moves on to autocompletion to see how the model performs there. Use it as a “daily driver” for a while Chowdhary prefers to just jump in and start using a model. “When a new model drops, I swap it into my workflow as my daily driver and just live with it for a bit,” he says. “Available benchmarks and tests only tell you part of the story. I think the real test is seeing if it actually improves your day to day.” For example, he checks to see if it actually speeds up his debugging jobs or produces cleaner refactors. “It’s kind of like dogfooding your own stack: You won’t know if it really fits your workflow until you’ve shipped some real code with it,” he says. “After evaluating it for a bit, I decide whether to stick with the new model or revert to my previous choice.” Take this with you What just about everyone agrees on is that the best way to evaluate a model is to use it. The important thing is to keep learning. “You don’t need to be switching models all the time, but it’s important to know what’s going on,” Chowdhary says. “The state of the art is moving quickly. It’s easy to get left behind.” Additional resources Choosing the right AI model for your task Examples for AI model comparison Which AI models should I use with GitHub Copilot? Tags: AI models generative AI GitHub Copilot reasoning models Written by Explore more from GitHub Docs Everything you need to master GitHub, all in one place. Go to Docs GitHub Build what’s next on GitHub, the place for anyone from anywhere to build anything. Start building Customer stories Meet the companies and engineering teams that build with GitHub. Learn more Enterprise content Executive insights, curated just for you Get started",
  "image": "https://github.blog/wp-content/uploads/2025/04/wallpaper-copilot-mona-logo-generic.png?fit=1920%2C1080",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\n\t\n\u003cp\u003eTo ensure that you have access to the best technology available, we’re continuously adding support for new models to \u003ca href=\"https://github.com/features/copilot\"\u003eGitHub Copilot\u003c/a\u003e. That being said, we know it can be hard to keep up with so many new models being released all the time.\u003c/p\u003e\n\u003cp\u003eAll of this raises an obvious question: Which model should you use?\u003c/p\u003e\n\u003cp\u003eYou can \u003ca href=\"https://github.blog/ai-and-ml/github-copilot/which-ai-model-should-i-use-with-github-copilot/\"\u003eread our recent blog post\u003c/a\u003e for an overview of the models currently available in Copilot and their strengths, or \u003ca href=\"https://docs.github.com/en/copilot/using-github-copilot/ai-models/choosing-the-right-ai-model-for-your-task\"\u003echeck out our documentation\u003c/a\u003e for a deep dive comparing different models and tasks. But the AI landscape moves quickly. \u003cstrong\u003eIn this article we’ll explore a framework—including a few strategies—for evaluating whether any given AI model is a good fit for \u003cem\u003eyour\u003c/em\u003e use, even as new models continue to appear at a rapid pace.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIt’s hard to go wrong with our base model, which has been fine-tuned specifically for programming-related tasks. But depending on what you’re working on, you likely have varying needs and preferences. There’s no single “best” model. Some may favor a more verbose model for chat, while others prefer a terse one, for example.\u003c/p\u003e\n\u003cp\u003eWe spoke with several developers about their model selection process. Keep reading to discover how to apply their strategies to your own needs.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e💡 Watch the video below for tips on prompt engineering to get the best results.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\n\t\t\t\u003ciframe loading=\"lazy\" src=\"https://www.youtube.com/embed/LAF-lACf2QY?feature=oembed\" title=\"YouTube video player\" allow=\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\t\t\u003c/p\u003e\n\u003ch2 id=\"why-use-multiple-models\" id=\"why-use-multiple-models\"\u003eWhy use multiple models?\u003ca href=\"#why-use-multiple-models\" aria-label=\"Why use multiple models?\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThere’s no reason you have to pick one model and stick with it. Since you can easily switch between models for both \u003ca href=\"https://docs.github.com/en/copilot/using-github-copilot/ai-models/changing-the-ai-model-for-copilot-chat\"\u003echat\u003c/a\u003e and \u003ca href=\"https://docs.github.com/en/copilot/using-github-copilot/ai-models/changing-the-ai-model-for-copilot-code-completion\"\u003ecode completion\u003c/a\u003e with GitHub Copilot, you can use different models for different use cases.\u003c/p\u003e\n\u003cfigure\u003e\u003cblockquote\u003e\u003cp\u003eIt\u0026#39;s kind of like dogfooding your own stack: You won’t know if it really fits your workflow until you\u0026#39;ve shipped some real code with it.\u003c/p\u003e\u003c/blockquote\u003e\u003cfigcaption\u003e - Anand Chowdhary, FirstQuadrant CTO and co-founder\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch3 id=\"chat-vs-code-completion\" id=\"chat-vs-code-completion\"\u003eChat vs. code completion\u003ca href=\"#chat-vs-code-completion\" aria-label=\"Chat vs. code completion\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eUsing one model for chat and another for autocomplete is one of the most common patterns we see among developers. Generally, developers prefer autocompletion models because they’re fast and responsive, which they need if they’re looking for suggestions as they think and type. Developers are more tolerant of latency in chat, when they’re in more of an exploratory state of mind (like considering a complex refactoring job, for instance).\u003c/p\u003e\n\u003ch3 id=\"reasoning-models-for-certain-programming-tasks\" id=\"reasoning-models-for-certain-programming-tasks\"\u003eReasoning models for certain programming tasks\u003ca href=\"#reasoning-models-for-certain-programming-tasks\" aria-label=\"Reasoning models for certain programming tasks\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eReasoning models like OpenAI o1 often respond slower than traditional LLMs such as GPT-4o or Claude Sonnet 3.5. That’s in large part because these models break a prompt down into parts and consider multiple approaches to a problem. That introduces latency in their response times, but makes them more effective at completing complex tasks. Many developers prefer these more deliberative models for particular tasks.\u003c/p\u003e\n\u003cp\u003eFor instance, Fatih Kadir Akın, a developer relations manager, uses o1 when starting new projects from scratch. “Reasoning models better ‘understand’ my vision and create more structured projects than non-reasoning models,” he explains.\u003c/p\u003e\n\u003cp\u003eFirstQuadrant CTO and co-founder Anand Chowdhary favors reasoning models for large-scale code refactoring jobs. “A model that rewrites complex backend code without careful reasoning is rarely accurate the first time,” he says. “Seeing the thought process also helps me understand the changes.”\u003c/p\u003e\n\u003cp\u003eWhen creating technical interview questions for her newsletter, GitHub Senior Director of Developer Advocacy, Cassidy Williams mixes models for certain tasks. When she writes a question, she uses GPT-4o to refine the prose, and then Claude 3.7 Sonnet Thinking to verify code accuracy. “Reasoning models help ensure technical correctness because of their multi-step process,” she says. “If they initially get something wrong, they often correct themselves in later steps so the final answer is more accurate.”\u003c/p\u003e\n\u003cfigure\u003e\u003cblockquote\u003e\u003cp\u003eThere’s some subjectivity, but I compare model output based on the code structure, patterns, comments, and adherence to best practices.\u003c/p\u003e\u003c/blockquote\u003e\u003cfigcaption\u003e - Portilla Edo, cloud infrastructure engineering lead\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch2 id=\"what-to-look-for-in-a-new-ai-model\" id=\"what-to-look-for-in-a-new-ai-model\"\u003eWhat to look for in a new AI model\u003ca href=\"#what-to-look-for-in-a-new-ai-model\" aria-label=\"What to look for in a new AI model\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eLet’s say a new model just dropped and you’re ready to try it out. Here are a few things to consider before making it your new go-to.\u003c/p\u003e\n\u003ch3 id=\"recentness\" id=\"recentness\"\u003eRecentness\u003ca href=\"#recentness\" aria-label=\"Recentness\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eDifferent models use different training data. That means one model might have more recent data than another, and therefore might be trained on new versions of the programming languages, frameworks, and libraries you use.\u003c/p\u003e\n\u003cp\u003e“When I’m trying out a new model, one of the first things I do is check how up to date it is,” says Xavier Portilla Edo, a cloud infrastructure engineering lead. He typically does this by creating a project manifest file for the project to see what version numbers Copilot autocomplete suggests. “If the versions are quite old, I’ll move on,” he says.\u003c/p\u003e\n\u003ch3 id=\"speed-and-responsiveness\" id=\"speed-and-responsiveness\"\u003eSpeed and responsiveness\u003ca href=\"#speed-and-responsiveness\" aria-label=\"Speed and responsiveness\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAs mentioned, developers tend to tolerate more latency in a chat than in autocomplete. But responsiveness is still important in chat. “I enjoy bouncing ideas off a model and getting feedback,” says Rishab Kumar, a staff developer evangelist at Twilio. “For that type of interaction, I need fast responses so I can stay in the flow.”\u003c/p\u003e\n\u003ch3 id=\"accuracy\" id=\"accuracy\"\u003eAccuracy\u003ca href=\"#accuracy\" aria-label=\"Accuracy\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eNaturally, you need to evaluate which models produce the best code. “There’s some subjectivity, but I compare model output based on the code structure, patterns, comments, and adherence to best practices,” Portilla Edo says. “I also look at how readable and maintainable the code is—does it follow naming conventions? Is it modular? Are the comments helpful or just restating what the code does? These are all signals of quality that go beyond whether the code simply runs.”\u003c/p\u003e\n\u003ch2 id=\"how-to-test-an-ai-model-in-your-workflow\" id=\"how-to-test-an-ai-model-in-your-workflow\"\u003eHow to test an AI model in your workflow\u003ca href=\"#how-to-test-an-ai-model-in-your-workflow\" aria-label=\"How to test an AI model in your workflow\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eOK, so now you know what to look for in a model. But how do you actually evaluate it for responsiveness and correctness? You use it, of course.\u003c/p\u003e\n\u003ch3 id=\"start-with-a-simple-app\" id=\"start-with-a-simple-app\"\u003eStart with a simple app\u003ca href=\"#start-with-a-simple-app\" aria-label=\"Start with a simple app\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAkın will generally start with a simple todo app written in vanilla JavaScript. “I just check the code, and how well it’s structured,” he says. Similarly, Kumar will start with a websocket server in Python. The idea is to start with something that you understand well enough to evaluate, and then layer on more complexity. “Eventually I’ll see if it can build something in 3D using 3js,” Akın says.\u003c/p\u003e\n\u003cp\u003ePortilla Edo starts by prompting a new model he wants to evaluate in Copilot Chat. “I usually ask it for simple things, like a function in Go, or a simple HTML file,” he says. Then he moves on to autocompletion to see how the model performs there.\u003c/p\u003e\n\u003ch3 id=\"use-it-as-a-daily-driver-for-a-while\" id=\"use-it-as-a-daily-driver-for-a-while\"\u003eUse it as a “daily driver” for a while\u003ca href=\"#use-it-as-a-daily-driver-for-a-while\" aria-label=\"Use it as a “daily driver” for a while\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eChowdhary prefers to just jump in and start using a model. “When a new model drops, I swap it into my workflow as my daily driver and just live with it for a bit,” he says. “Available benchmarks and tests only tell you part of the story. I think the real test is seeing if it actually improves your day to day.”\u003c/p\u003e\n\u003cp\u003eFor example, he checks to see if it actually speeds up his debugging jobs or produces cleaner refactors. “It’s kind of like dogfooding your own stack: You won’t know if it really fits your workflow until you’ve shipped some real code with it,” he says. “After evaluating it for a bit, I decide whether to stick with the new model or revert to my previous choice.”\u003c/p\u003e\n\u003ch2 id=\"take-this-with-you\" id=\"take-this-with-you\"\u003eTake this with you\u003ca href=\"#take-this-with-you\" aria-label=\"Take this with you\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWhat just about everyone agrees on is that the best way to evaluate a model is to use it.\u003c/p\u003e\n\u003cp\u003eThe important thing is to keep learning. “You don’t need to be switching models all the time, but it’s important to know what’s going on,” Chowdhary says. “The state of the art is moving quickly. It’s easy to get left behind.”\u003c/p\u003e\n\u003ch3 id=\"additional-resources\" id=\"additional-resources\"\u003eAdditional resources\u003ca href=\"#additional-resources\" aria-label=\"Additional resources\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://docs.github.com/en/copilot/using-github-copilot/ai-models/choosing-the-right-ai-model-for-your-task\"\u003eChoosing the right AI model for your task\u003c/a\u003e  \u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.github.com/en/copilot/using-github-copilot/ai-models/comparing-ai-models-using-different-tasks\"\u003eExamples for AI model comparison\u003c/a\u003e  \u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.blog/ai-and-ml/github-copilot/which-ai-model-should-i-use-with-github-copilot/\"\u003eWhich AI models should I use with GitHub Copilot?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\t\n\u003csection\u003e\n\t\u003chr/\u003e\n\t\u003cdiv\u003e\n\t\t\u003ch2\u003eTags:\u003c/h2\u003e\n\t\t\u003cul\u003e\n\t\t\t\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003ca href=\"https://github.blog/tag/ai-models/\" rel=\"tag\"\u003e\n\t\t\t\t\t\tAI models\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003ca href=\"https://github.blog/tag/generative-ai/\" rel=\"tag\"\u003e\n\t\t\t\t\t\tgenerative AI\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003ca href=\"https://github.blog/tag/github-copilot/\" rel=\"tag\"\u003e\n\t\t\t\t\t\tGitHub Copilot\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003ca href=\"https://github.blog/tag/reasoning-models/\" rel=\"tag\"\u003e\n\t\t\t\t\t\treasoning models\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\t\u003c/ul\u003e\n\t\u003c/div\u003e\n\u003c/section\u003e\n\t\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tWritten by\t\u003c/h2\u003e\n\t\n\t\t\t\u003carticle\u003e\n\t\u003cdiv\u003e\n\t\t\t\t\u003cpicture\u003e\n\t\t\t\t\t\u003csource srcset=\"https://avatars.githubusercontent.com/u/957053?v=4\u0026amp;s=200\" width=\"120\" height=\"120\" media=\"(min-width: 768px)\"/\u003e\n\t\t\t\t\t\u003cimg src=\"https://avatars.githubusercontent.com/u/957053?v=4\u0026amp;s=200\" alt=\"Klint Finley\" width=\"80\" height=\"80\" loading=\"lazy\" decoding=\"async\"/\u003e\n\t\t\t\t\u003c/picture\u003e\n\t\t\t\u003c/div\u003e\n\u003c/article\u003e\n\t\u003c/div\u003e\n\u003c/section\u003e\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tExplore more from GitHub\t\u003c/h2\u003e\n\t\u003cdiv\u003e\n\t\t\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon-Circle.svg\" width=\"44\" height=\"44\" alt=\"Docs\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tDocs\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eEverything you need to master GitHub, all in one place.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Go to Docs; ref_location:bottom recirculation;\" href=\"https://docs.github.com/\" target=\"_blank\" aria-label=\"Go to Docs\"\u003e\n\t\t\t\t\tGo to Docs\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_95220f.svg\" width=\"44\" height=\"44\" alt=\"GitHub\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tGitHub\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eBuild what’s next on GitHub, the place for anyone from anywhere to build anything.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Start building; ref_location:bottom recirculation;\" href=\"https://github.blog/developer-skills/github/\" target=\"_blank\" aria-label=\"Start building\"\u003e\n\t\t\t\t\tStart building\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" fill=\"none\"\u003e\u003cpath fill=\"currentColor\" d=\"M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z\"\u003e\u003c/path\u003e\u003cpath stroke=\"currentColor\" d=\"M1.75 8H11\" stroke-width=\"1.5\" stroke-linecap=\"round\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_da43dc.svg\" width=\"44\" height=\"44\" alt=\"Customer stories\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tCustomer stories\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eMeet the companies and engineering teams that build with GitHub.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Learn more; ref_location:bottom recirculation;\" href=\"https://github.com/customer-stories\" target=\"_blank\" aria-label=\"Learn more\"\u003e\n\t\t\t\t\tLearn more\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2022/05/careers.svg\" width=\"44\" height=\"44\" alt=\"Enterprise content\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tEnterprise content\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eExecutive insights, curated just for you\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Get started; ref_location:bottom recirculation;\" href=\"https://github.com/solutions/executive-insights\" target=\"_blank\" aria-label=\"Get started\"\u003e\n\t\t\t\t\tGet started\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\t\u003c/div\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2025-04-24T16:00:51Z",
  "modifiedTime": null
}
