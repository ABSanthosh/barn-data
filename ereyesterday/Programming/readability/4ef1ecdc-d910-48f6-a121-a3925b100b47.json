{
  "id": "4ef1ecdc-d910-48f6-a121-a3925b100b47",
  "title": "3 best practices for building software in the era of LLMs",
  "link": "https://about.gitlab.com/blog/3-best-practices-for-building-software-in-the-era-of-llms/",
  "description": "",
  "author": "Salman Ladha",
  "published": "2025-07-10T00:00:00.000Z",
  "source": "https://about.gitlab.com/atom.xml",
  "categories": null,
  "byline": "Salman Ladha",
  "length": 6840,
  "excerpt": "With AI transforming coding speed, developers need new security habits. Learn what they are and how to deploy them throughout the DevSecOps workflow.",
  "siteName": "GitLab",
  "favicon": "https://about.gitlab.com/images/ico/favicon-192x192.png",
  "text": "AI has rapidly become a core part of modern software development. Not only is it helping developers code faster than ever, but it’s also automating low-level tasks like writing test cases or summarizing documentation. According to our 2024 Global DevSecOps Survey, 81% of developers are already using AI in their workflows or plan to in the next two years. As code is written with less manual effort, we’re seeing a subtle but important behavioral change: Developers are beginning to trust AI-generated code with less scrutiny. That confidence — understandable as it may be — can quietly introduce security risks, especially as the overall volume of code increases. Developers can’t be expected to stay on top of every vulnerability or exploit, which is why we need systems and safeguards that scale with them. AI tools are here to stay. So, as security professionals, it’s incumbent on you to empower developers to adopt them in a way that improves both speed and security. Here are three practical ways to do that. Never trust, always verify As mentioned above, developers are beginning to trust AI-generated code more readily, especially when it looks clean and compiles without error. To combat this, adopt a zero-trust mindset. While we often talk about zero trust in the context of identity and access management, the same principle can be applied here with a slightly different framing. Treat AI-generated code like input from a junior developer: helpful, but not production-ready without a proper review. A developer should be able to explain what the code is doing and why it’s safe before it gets merged. Reviewing AI-generated code might even shape up to be an emerging skillset required in the world of software development. The developers who excel at this will be indispensable because they’ll marry the speed of LLMs with the risk reduction mindset to produce secure code, faster. This is where tools like GitLab Duo Code Review can help. As a feature of our AI companion across the software development lifecycle, it brings AI into the code review process, not to replace human judgment, but to enhance it. By surfacing questions, inconsistencies, and overlooked issues in the merge requests, AI can help developers keep up with the very AI that’s accelerating development cycles. Prompt for secure patterns Large language models (LLMs) are powerful, but only as precise as the prompts they’re given. That’s why prompt engineering is becoming a core part of working with AI tools. In the world of LLMs, your input is the interface. Developers who learn to write clear, security-aware prompts will play a key role in building safer software from the start. For example, vague requests like “build a login form” often produce insecure or overly simplistic results. However, by including more context, such as “build a login form with input validation, rate limiting, and hashing, and support phishing-resistant authentication methods like passkeys,” you’re more likely to produce an output that meets the security standards of your organization. Recent research from Backlash Security backs this up. They found that secure prompting improved results across popular LLMs. When developers simply asked models to “write secure code,” success rates remained low. However, when prompts referenced OWASP best practices, the rate of secure code generation increased. Prompt engineering should be part of how we train and empower security champions within development teams. Just like we teach secure coding patterns and threat modeling, we should also be teaching developers how to guide AI tools with the same security mindset. Learn more with these helpful prompt engineering tips. Scan everything, no exceptions The rise of AI means we’re writing more code, quicker, with the same number of humans. That shift should change how we think about security, not just as a final check, but as an always-on safeguard woven into every aspect of the development process. More code means a wider attack surface. And when that code is partially or fully generated, we can’t solely rely on secure coding practices or individual intuition to spot risks. That’s where automated scanning comes in. Static Application Security Testing (SAST), Software Composition Analysis (SCA), and Secret Detection become critical controls to mitigate the risk of secret leaks, supply chain attacks, and weaknesses like SQL injections. With platforms like GitLab, application security is natively built into the developer's workflow, making it a natural part of the development lifecycle. Scanners can also trace through the entire program to make sure new AI-generated code is secure in the context of all the other code — that can be hard to spot if you’re just looking at some new code in your IDE or in an AI-generated patch. But it’s not just about scanning, it’s about keeping pace. If development teams are going to match the speed of AI-assisted development, they need scans that are fast, accurate, and built to scale. Accuracy especially matters. If scanners overwhelm developers with false positives, there’s a risk of losing trust in the system altogether. The only way to move fast and stay secure is to make scanning non-negotiable. Every commit. Every branch. No exceptions. Secure your AI-generated code with GitLab AI is changing the way we build software, but the fundamentals of secure software development still apply. Code still needs to be reviewed. Threats still need to be tested. And security still needs to be embedded in the way we work. At GitLab, that’s exactly what we’ve done. As a developer platform, we’re not bolting security onto the workflow — we’re embedding it directly where developers already work: in the IDE, in merge requests, and in the pipeline. Scans run automatically and relevant security context is surfaced to facilitate faster remediation cycles. And, because it’s part of the same platform where developers build, test, and deploy software, there are fewer tools to juggle, less context switching, and a much smoother path to secure code. AI features like Duo Vulnerability Explanation and Vulnerability Resolution add another layer of speed and insight, helping developers understand risks and fix them faster, without breaking their flow. AI isn’t a shortcut to security. But with the right practices — and a platform that meets developers where they are — it can absolutely be part of building software that’s fast, secure, and scalable. Start your free 60-day trial of GitLab Ultimate with Duo Enterprise and experience what it’s like to build secure software, faster. With native security scanning, AI-powered insights, and a seamless developer experience, GitLab helps you shift security left without slowing down.",
  "image": "https://about.gitlab.com/images/open-graph/open-graph-gitlab.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-v-c4ab8db4=\"\" data-v-69df7a1a=\"\" data-v-b7392a61=\"\"\u003e\u003cp\u003eAI has rapidly become a core part of modern software development. Not only is it helping developers code faster than ever, but it’s also automating low-level tasks like writing test cases or summarizing documentation. According to our \u003ca href=\"https://about.gitlab.com/developer-survey/\"\u003e2024 Global DevSecOps Survey\u003c/a\u003e, 81% of developers are already using AI in their workflows or plan to in the next two years.\u003c/p\u003e\n\u003cp\u003eAs code is written with less manual effort, we’re seeing a subtle but important behavioral change: Developers are beginning to trust AI-generated code with less scrutiny. That confidence — understandable as it may be — can quietly introduce security risks, especially as the overall volume of code increases. Developers can’t be expected to stay on top of every vulnerability or exploit, which is why we need systems and safeguards that scale with them. AI tools are here to stay. So, as security professionals, it’s incumbent on you to empower developers to adopt them in a way that improves both speed and security.\u003c/p\u003e\n\u003cp\u003eHere are three practical ways to do that.\u003c/p\u003e\n\u003ch2 id=\"never-trust-always-verify\" tabindex=\"-1\"\u003eNever trust, always verify \u003ca href=\"#never-trust-always-verify\"\u003e\n          \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\u003cpath d=\"M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003cpath d=\"M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003cpath d=\"M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003c/svg\u003e\n        \u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAs mentioned above, developers are beginning to trust AI-generated code more readily, especially when it looks clean and compiles without error. To combat this, adopt a zero-trust mindset. While we often talk about \u003ca href=\"https://about.gitlab.com/blog/why-devops-and-zero-trust-go-together/\"\u003ezero trust\u003c/a\u003e in the context of identity and access management, the same principle can be applied here with a slightly different framing. Treat AI-generated code like input from a junior developer: helpful, but not production-ready without a proper review.\u003c/p\u003e\n\u003cp\u003eA developer should be able to explain what the code is doing and why it’s safe before it gets merged. Reviewing AI-generated code might even shape up to be an emerging skillset required in the world of software development. The developers who excel at this will be indispensable because they’ll marry the speed of LLMs with the risk reduction mindset to produce secure code, faster.\u003c/p\u003e\n\u003cp\u003eThis is where tools like \u003ca href=\"https://docs.gitlab.com/user/project/merge_requests/duo_in_merge_requests/\"\u003eGitLab Duo Code Review\u003c/a\u003e can help. As a feature of our AI companion across the software development lifecycle, it brings AI into the code review process, not to replace human judgment, but to enhance it. By surfacing questions, inconsistencies, and overlooked issues in the merge requests, AI can help developers keep up with the very AI that’s accelerating development cycles.\u003c/p\u003e\n\u003ch2 id=\"prompt-for-secure-patterns\" tabindex=\"-1\"\u003ePrompt for secure patterns \u003ca href=\"#prompt-for-secure-patterns\"\u003e\n          \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\u003cpath d=\"M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003cpath d=\"M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003cpath d=\"M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003c/svg\u003e\n        \u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eLarge language models (\u003ca href=\"https://about.gitlab.com/blog/what-is-a-large-language-model-llm/\"\u003eLLMs\u003c/a\u003e)  are powerful, but only as precise as the prompts they’re given. That’s why prompt engineering is becoming a core part of working with AI tools. In the world of LLMs, your input \u003cem\u003eis\u003c/em\u003e the interface. Developers who learn to write clear, security-aware prompts will play a key role in building safer software from the start.\u003c/p\u003e\n\u003cp\u003eFor example, vague requests like “build a login form” often produce insecure or overly simplistic results. However, by including more context, such as “build a login form \u003cstrong\u003ewith\u003c/strong\u003e input validation, rate limiting, and hashing, \u003cstrong\u003eand\u003c/strong\u003e support phishing-resistant authentication methods like passkeys,” you’re more likely to produce an output that meets the security standards of your organization.\u003c/p\u003e\n\u003cp\u003eRecent \u003ca href=\"https://www.backslash.security/press-releases/backslash-security-reveals-in-new-research-that-gpt-4-1-other-popular-llms-generate-insecure-code-unless-explicitly-prompted\"\u003eresearch\u003c/a\u003e from Backlash Security backs this up. They found that secure prompting improved results across popular LLMs. When developers simply asked models to “write secure code,” success rates remained low. However, when prompts referenced \u003ca href=\"https://cheatsheetseries.owasp.org/cheatsheets/LLM_Prompt_Injection_Prevention_Cheat_Sheet.html\"\u003eOWASP best practices\u003c/a\u003e, the rate of secure code generation increased.\u003c/p\u003e\n\u003cp\u003ePrompt engineering should be part of how we train and empower security champions within development teams. Just like we teach secure coding patterns and threat modeling, we should also be teaching developers how to guide AI tools with the same security mindset.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eLearn more with these helpful \u003ca href=\"https://docs.gitlab.com/development/ai_features/prompt_engineering/\"\u003eprompt engineering tips\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"scan-everything-no-exceptions\" tabindex=\"-1\"\u003eScan everything, no exceptions \u003ca href=\"#scan-everything-no-exceptions\"\u003e\n          \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\u003cpath d=\"M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003cpath d=\"M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003cpath d=\"M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003c/svg\u003e\n        \u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe rise of AI means we’re writing more code, quicker, with the same number of humans. That shift should change how we think about security, not just as a final check, but as an always-on safeguard woven into every aspect of the development process.\u003c/p\u003e\n\u003cp\u003eMore code means a wider attack surface. And when that code is partially or fully generated, we can’t solely rely on secure coding practices or individual intuition to spot risks. That’s where automated scanning comes in. \u003ca href=\"https://docs.gitlab.com/user/application_security/sast/\"\u003eStatic Application Security Testing (SAST)\u003c/a\u003e, \u003ca href=\"https://docs.gitlab.com/user/application_security/dependency_scanning/\"\u003eSoftware Composition Analysis (SCA)\u003c/a\u003e, and \u003ca href=\"https://docs.gitlab.com/user/application_security/secret_detection/\"\u003eSecret Detection\u003c/a\u003e become critical controls to mitigate the risk of secret leaks, supply chain attacks, and weaknesses like SQL injections. With platforms like GitLab, \u003ca href=\"https://about.gitlab.com/solutions/security-compliance/\"\u003eapplication security\u003c/a\u003e is natively built into the developer\u0026#39;s workflow, making it a natural part of the development lifecycle. Scanners can also trace through the entire program to make sure new AI-generated code is secure \u003cem\u003ein the context of all the other code\u003c/em\u003e — that can be hard to spot if you’re just looking at some new code in your IDE or in an AI-generated patch.\u003c/p\u003e\n\u003cp\u003eBut it’s not just about scanning, it’s about keeping pace. If development teams are going to match the speed of AI-assisted development, they need scans that are fast, accurate, and built to scale. Accuracy especially matters. If scanners overwhelm developers with false positives, there’s a risk of losing trust in the system altogether.\u003c/p\u003e\n\u003cp\u003eThe only way to move fast \u003cem\u003eand\u003c/em\u003e stay secure is to make scanning non-negotiable.\u003c/p\u003e\n\u003cp\u003eEvery commit. Every branch. No exceptions.\u003c/p\u003e\n\u003ch2 id=\"secure-your-ai-generated-code-with-gitlab\" tabindex=\"-1\"\u003eSecure your AI-generated code with GitLab \u003ca href=\"#secure-your-ai-generated-code-with-gitlab\"\u003e\n          \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\u003cpath d=\"M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003cpath d=\"M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003cpath d=\"M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z\" fill=\"#333333\"\u003e\u003c/path\u003e\u003c/svg\u003e\n        \u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAI is changing the way we build software, but the fundamentals of secure software development still apply. Code still needs to be reviewed. Threats still need to be tested. And security still needs to be embedded in the way we work. At GitLab, that’s exactly what we’ve done.\u003c/p\u003e\n\u003cp\u003eAs a developer platform, we’re not bolting security onto the workflow — we’re embedding it directly where developers already work: in the IDE, in merge requests, and in the pipeline. Scans run automatically and relevant security context is surfaced to facilitate faster remediation cycles. And, because it’s part of the same platform where developers build, test, and deploy software, there are fewer tools to juggle, less context switching, and a much smoother path to secure code.\u003c/p\u003e\n\u003cp\u003eAI features like \u003ca href=\"https://about.gitlab.com/the-source/ai/understand-and-resolve-vulnerabilities-with-ai-powered-gitlab-duo/\"\u003eDuo Vulnerability Explanation and Vulnerability Resolution\u003c/a\u003e add another layer of speed and insight, helping developers understand risks and fix them faster, without breaking their flow.\u003c/p\u003e\n\u003cp\u003eAI isn’t a shortcut to security. But with the right practices — and a platform that meets developers where they are — it can absolutely be part of building software that’s fast, secure, and scalable.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eStart your \u003ca href=\"https://about.gitlab.com/free-trial/\"\u003efree 60-day trial of GitLab Ultimate with Duo Enterprise\u003c/a\u003e and experience what it’s like to build secure software, faster. With native security scanning, AI-powered insights, and a seamless developer experience, GitLab helps you shift security left without slowing down.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-07-10T00:00:00Z",
  "modifiedTime": null
}
