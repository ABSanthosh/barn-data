{
  "id": "c9507b04-6aa4-4638-bca4-c3160cc63572",
  "title": "Deploy JetBrains Mellum Your Way: Now Available via NVIDIA NIM",
  "link": "https://blog.jetbrains.com/ai/2025/06/deploy-jetbrains-mellum-your-way-now-available-via-nvidia-nim/",
  "description": "Deploy Mellum as a production-grade LLM inside your own infrastructure – with NVIDIA. JetBrains Mellum – our open, focused LLM specialized on code completion – is now available to run as a containerized microservice on NVIDIA AI Factories. Using the new NVIDIA universal LLM NIM container, Mellum can be deployed in minutes on any NVIDIA-accelerated […]",
  "author": "Conrad Schwellnus",
  "published": "Wed, 11 Jun 2025 11:15:00 +0000",
  "source": "https://blog.jetbrains.com/feed",
  "categories": [
    "jetbrains-ai",
    "news",
    "partners",
    "ai",
    "mellum",
    "partnership"
  ],
  "byline": "Conrad Schwellnus",
  "length": 4484,
  "excerpt": "JetBrains Mellum – our open, focused LLM specialized on code completion – is now available to run as a containerized microservice on NVIDIA AI Factories.",
  "siteName": "The JetBrains Blog",
  "favicon": "https://blog.jetbrains.com/wp-content/uploads/2024/01/cropped-mstile-310x310-1-180x180.png",
  "text": "Supercharge your tools with AI-powered features inside many JetBrains products JetBrains AI News PartnersDeploy JetBrains Mellum Your Way: Now Available via NVIDIA NIM Deploy Mellum as a production-grade LLM inside your own infrastructure – with NVIDIA. JetBrains Mellum – our open, focused LLM specialized on code completion – is now available to run as a containerized microservice on NVIDIA AI Factories. Using the new NVIDIA universal LLM NIM container, Mellum can be deployed in minutes on any NVIDIA-accelerated infrastructure, whether in the cloud, on-premises, or across hybrid environments. Mellum is part of the early launch cohort of models that showcase coding capabilities on AI factories. We’re proud to be among the first teams contributing to this new enterprise ecosystem. But wait – isn’t Mellum already in JetBrains IDEs and on Hugging Face? Yes – and that’s not changing. Mellum is tightly integrated into our developer tools via JetBrains AI Assistant and is also available on Hugging Face. But some teams often have very different requirements, such as: Deployment on their own hardware, in environments they control Integration into custom pipelines, CI/CD flows, observability platforms Fine-tuning or customization for domain-specific use cases Security, compliance, and performance guarantees That’s where NVIDIA Enterprise AI Factory validated design comes in – it’s a reference platform for building full-stack enterprise AI systems. Mellum, available via NVIDIA NIM, becomes a plug-and-play model block that can fit directly into those pipelines. In our testing, we wanted to ensure that, as the Mellum family grows, we are able to offer JetBrains models on a performant inference solution that is enterprise-ready. What are NVIDIA NIM microservices? NVIDIA NIM microservices  are part of NVIDIA AI Enterprise, and do something very straightforward but invaluable: wrap complex AI model infrastructure into simple, fast-deployable containers optimized for inference. With the new universal LLM NIM container designed to work with a broad range of open and specialized LLMs, Mellum can now be deployed securely on NVIDIA-accelerated computing – on-premises, in the cloud, or across hybrid environments. From a technical standpoint, it means Mellum is now available through a single container interface that supports major backends like NVIDIA TensorRT-LLM, vLLM, and SGLang. This helps teams run inference efficiently and predictably using open-source models they can inspect, adapt, and improve. We’re particularly excited about how this ecosystem can help enterprise users evolve from basic chatbot integrations to deeply integrated AI assistants embedded across software engineering workflows.  Do I still need JetBrains AI Assistant if Mellum runs on NIM? Some users ask:“Is the open-source Mellum (via NIM) the same as what’s in JetBrains AI Assistant?” Not exactly. The open-source Mellum,now deployable via NIM, is great for custom, self-hosted use cases. But JetBrains AI Assistant uses enhanced proprietary versions of Mellum, with deeper IDE integration and a more polished developer experience. In short: Use NIM and Mellum for flexible, custom deployment. Use AI Assistant for the best out-of-the-box experience inside JetBrains tools. Try it now Mellum deployment is now one button away from you, so check it out here. Subscribe to JetBrains AI Blog updates Discover more",
  "image": "https://blog.jetbrains.com/wp-content/uploads/2025/06/Blog_1280x720.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n    \u003cdiv\u003e\n                        \u003ca href=\"https://blog.jetbrains.com/ai/\"\u003e\n                            \u003cimg src=\"https://blog.jetbrains.com/wp-content/uploads/2024/01/JetBrains-AI.svg\" alt=\"Ai logo\"/\u003e\n                                                                                                \n                                                                                    \u003c/a\u003e\n                                                    \u003cp\u003eSupercharge your tools with AI-powered features inside many JetBrains products\u003c/p\u003e\n                                            \u003c/div\u003e\n                            \u003csection data-clarity-region=\"article\"\u003e\n                \u003cdiv\u003e\n                    \t\t\t\t\u003cp\u003e\u003ca href=\"https://blog.jetbrains.com/ai/category/jetbrains-ai/\"\u003eJetBrains AI\u003c/a\u003e\n\t\t\t\u003ca href=\"https://blog.jetbrains.com/ai/category/news/\"\u003eNews\u003c/a\u003e\n\t\t\t\u003ca href=\"https://blog.jetbrains.com/ai/category/partners/\"\u003ePartners\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"major-updates\"\u003eDeploy JetBrains Mellum Your Way: Now Available via NVIDIA NIM\u003c/h2\u003e                    \n                    \n\u003cp\u003e\u003cem\u003eDeploy Mellum as a production-grade LLM inside your own infrastructure – with NVIDIA.\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://blog.jetbrains.com/ai/2025/04/mellum-goes-open-source-a-purpose-built-llm-for-developers-now-on-hugging-face/\"\u003eJetBrains Mellum\u003c/a\u003e – our open, focused LLM specialized on code completion – is now available to run as a containerized microservice on \u003ca href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" target=\"_blank\" rel=\"noopener\"\u003eNVIDIA AI Factories\u003c/a\u003e. Using the new \u003ca href=\"https://www.nvidia.com/en-us/ai-data-science/products/nim-microservices/\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/ai-data-science/products/nim-microservices/\" target=\"_blank\" rel=\"noopener\"\u003eNVIDIA universal LLM NIM\u003c/a\u003e container, Mellum can be deployed in minutes on any NVIDIA-accelerated infrastructure, whether in the cloud, on-premises, or across hybrid environments.\u003c/p\u003e\n\n\n\n\u003cp\u003eMellum is part of the early launch cohort of models that showcase coding capabilities on AI factories. We’re proud to be among the first teams contributing to this new enterprise ecosystem.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut wait – isn’t Mellum already in JetBrains IDEs and on Hugging Face?\u003c/p\u003e\n\n\n\n\u003cp\u003eYes – and that’s not changing. Mellum is tightly integrated into our developer tools via \u003ca href=\"https://www.jetbrains.com/ai-assistant/\" target=\"_blank\" rel=\"noopener\"\u003eJetBrains AI Assistant\u003c/a\u003e and is also available on \u003ca href=\"https://huggingface.co/JetBrains/Mellum-4b-base\" target=\"_blank\" rel=\"noopener\"\u003eHugging Face\u003c/a\u003e. But some teams often have very different requirements, such as:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eDeployment on their own hardware, in environments they control\u003c/li\u003e\n\n\n\n\u003cli\u003eIntegration into custom pipelines, CI/CD flows, observability platforms\u003c/li\u003e\n\n\n\n\u003cli\u003eFine-tuning or customization for domain-specific use cases\u003c/li\u003e\n\n\n\n\u003cli\u003eSecurity, compliance, and performance guarantees\u003cbr/\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eThat’s where \u003ca href=\"https://www.nvidia.com/en-us/solutions/ai-factories/validated-design/\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/solutions/ai-factories/validated-design/\" target=\"_blank\" rel=\"noopener\"\u003eNVIDIA Enterprise AI Factory validated design\u003c/a\u003e comes in – it’s a reference platform for building full-stack enterprise AI systems. Mellum, available via NVIDIA NIM, becomes a plug-and-play model block that can fit directly into those pipelines. In our testing, we wanted to ensure that, as the Mellum family grows, we are able to offer JetBrains models on a performant inference solution that is enterprise-ready.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat are NVIDIA NIM microservices?\u003c/h2\u003e\n\n\n\n\u003cp\u003eNVIDIA NIM microservices  are part of \u003ca href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" target=\"_blank\" rel=\"noopener\"\u003eNVIDIA AI Enterprise\u003c/a\u003e, and do something very straightforward but invaluable: wrap complex AI model infrastructure into simple, fast-deployable containers optimized for inference. With the new universal LLM NIM container designed to work with a broad range of open and specialized LLMs, Mellum can now be deployed securely on NVIDIA-accelerated computing – on-premises, in the cloud, or across hybrid environments.\u003c/p\u003e\n\n\n\n\u003cp\u003eFrom a technical standpoint, it means Mellum is now available through a single container interface that supports major backends like \u003ca href=\"https://docs.nvidia.com/tensorrt-llm/index.html\" data-type=\"link\" data-id=\"https://docs.nvidia.com/tensorrt-llm/index.html\" target=\"_blank\" rel=\"noopener\"\u003eNVIDIA TensorRT-LLM\u003c/a\u003e, vLLM, and SGLang. This helps teams run inference efficiently and predictably using open-source models they can inspect, adapt, and improve.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe’re particularly excited about how this ecosystem can help enterprise users evolve from basic chatbot integrations to deeply integrated AI assistants embedded across software engineering workflows. \u003c/p\u003e\n\n\n\n\u003ch2\u003eDo I still need JetBrains AI Assistant if Mellum runs on NIM?\u003c/h2\u003e\n\n\n\n\u003cp\u003eSome users ask:\u003cbr/\u003e“Is the open-source Mellum (via NIM) the same as what’s in JetBrains AI Assistant?”\u003c/p\u003e\n\n\n\n\u003cp\u003eNot exactly.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe open-source Mellum,now deployable via NIM, is great for custom, self-hosted use cases. But \u003ca href=\"https://www.jetbrains.com/ai-assistant/\" data-type=\"link\" data-id=\"https://www.jetbrains.com/ai-assistant/\" target=\"_blank\" rel=\"noopener\"\u003eJetBrains AI Assistant\u003c/a\u003e uses enhanced proprietary versions of Mellum, with deeper IDE integration and a more polished developer experience.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn short:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eUse NIM and Mellum for flexible, custom deployment.\u003c/li\u003e\n\n\n\n\u003cli\u003eUse AI Assistant for the best out-of-the-box experience inside JetBrains tools.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch2\u003eTry it now\u003c/h2\u003e\n\n\n\n\u003cp\u003eMellum deployment is now one button away from you, so check it out \u003ca href=\"https://blogs.nvidia.com/blog/sovereign-ai-agents-factories/\" data-type=\"link\" data-id=\"https://blogs.nvidia.com/blog/sovereign-ai-agents-factories/\" target=\"_blank\" rel=\"noopener\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n                    \n                                                                                                                                                                                                                            \u003cdiv\u003e\n                                \u003cdiv\u003e\n                                                                            \u003ch4\u003eSubscribe to JetBrains AI Blog updates\u003c/h4\u003e\n                                                                                                            \n                                \u003c/div\u003e\n                                \n                                \u003cp\u003e\u003cimg src=\"https://blog.jetbrains.com/wp-content/themes/jetbrains/assets/img/img-form.svg\" alt=\"image description\"/\u003e\n                                                                    \u003c/p\u003e\n                            \u003c/div\u003e\n                                                            \u003c/div\u003e\n                \u003ca href=\"#\"\u003e\u003c/a\u003e\n                \n                \n            \u003c/section\u003e\n                    \u003cdiv\u003e\n                \u003cp\u003e\n                    \u003ch2\u003eDiscover more\u003c/h2\u003e\n                \u003c/p\u003e\n                \n            \u003c/div\u003e\n                \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
