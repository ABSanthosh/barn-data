{
  "id": "8699977d-b2d7-4ca7-b02f-765ee09e9e2d",
  "title": "180: Reinforcement Learning",
  "link": "https://www.programmingthrowdown.com/episodes/180-reinforcement-learning/",
  "description": "Intro topic: GrillsNews/Links:You can’t call yourself a senior until you’ve worked on a legacy projecthttps://www.infobip.com/developers/blog/seniors-working-on-a-legacy-projectRecraft might be the most powerful AI image platform I’ve ever used — here’s whyhttps://www.tomsguide.com/ai/ai-image-video/recraft-might-be-the-most-powerful-ai-image-platform-ive-ever-used-heres-whyNASA has a list of 10 rules for software developmenthttps://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htmAMD Radeon RX 9070 XT performance estimates leaked: 42% to 66% faster than Radeon RX 7900 GREhttps://www.tomshardware.com/tech-industry/amd-estimates-of-radeon-rx-9070-xt-performance-leaked-42-percent-66-percent-faster-than-radeon-rx-7900-gre Book of the ShowPatrick: The Player of Games (Ian M Banks)https://a.co/d/1ZpUhGl (non-affiliate)Jason: Basic Roleplaying Universal Game Enginehttps://amzn.to/3ES4p5iPatreon Plug https://www.patreon.com/programmingthrowdown?ty=hTool of the ShowPatrick: Pokemon Sword and ShieldJason: Features and Labels ( https://fal.ai )Topic: Reinforcement LearningThree types of AISupervised LearningUnsupervised LearningReinforcement LearningOnline vs Offline RLOptimization algorithmsValue optimizationSARSAQ-LearningPolicy optimizationPolicy GradientsActor-CriticProximal Policy OptimizationValue vs Policy OptimizationValue optimization is more intuitive (Value loss)Policy optimization is less intuitive at first (policy gradients)Converting values to policies in deep learning is difficultImitation LearningSupervised policy learningOften used to bootstrap reinforcement learningPolicy EvaluationPropensity scoring versus model-basedChallenges to training RL modelTwo optimization loopsCollecting feedback vs updating the modelDifficult optimization targetPolicy evaluationRLHF \u0026  GRPO ★ Support this podcast on Patreon ★",
  "author": "Patrick Wheeler and Jason Gauci",
  "published": "Mon, 17 Mar 2025 10:00:00 -0500",
  "source": "http://feeds.feedburner.com/ProgrammingThrowdown",
  "categories": [
    "Reinforcement Learning"
  ],
  "byline": "",
  "length": 1801,
  "excerpt": "Patrick Wheeler and Jason Gauci",
  "siteName": "",
  "favicon": "",
  "text": "Programming ThrowdownPatrick Wheeler and Jason GauciIntro topic: GrillsNews/Links:You can’t call yourself a senior until you’ve worked on a legacy projecthttps://www.infobip.com/developers/blog/seniors-working-on-a-legacy-projectRecraft might be the most powerful AI image platform I’ve ever used — here’s whyhttps://www.tomsguide.com/ai/ai-image-video/recraft-might-be-the-most-powerful-ai-image-platform-ive-ever-used-heres-whyNASA has a list of 10 rules for software developmenthttps://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htmAMD Radeon RX 9070 XT performance estimates leaked: 42% to 66% faster than Radeon RX 7900 GREhttps://www.tomshardware.com/tech-industry/amd-estimates-of-radeon-rx-9070-xt-performance-leaked-42-percent-66-percent-faster-than-radeon-rx-7900-gre Book of the ShowPatrick: The Player of Games (Ian M Banks)https://a.co/d/1ZpUhGl (non-affiliate)Jason: Basic Roleplaying Universal Game Enginehttps://amzn.to/3ES4p5iTool of the ShowPatrick: Pokemon Sword and ShieldJason: Features and Labels ( https://fal.ai )Topic: Reinforcement LearningThree types of AISupervised LearningUnsupervised LearningReinforcement LearningOnline vs Offline RLOptimization algorithmsValue optimizationSARSAQ-LearningPolicy optimizationPolicy GradientsActor-CriticProximal Policy OptimizationValue vs Policy OptimizationValue optimization is more intuitive (Value loss)Policy optimization is less intuitive at first (policy gradients)Converting values to policies in deep learning is difficultImitation LearningSupervised policy learningOften used to bootstrap reinforcement learningPolicy EvaluationPropensity scoring versus model-basedChallenges to training RL modelTwo optimization loopsCollecting feedback vs updating the modelDifficult optimization targetPolicy evaluationRLHF \u0026  GRPO",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv tabindex=\"-1\" id=\"___gatsby\"\u003e\u003cnav aria-label=\"mailbox folders\"\u003e\u003c/nav\u003e\u003cmain\u003e\u003ca href=\"https://www.programmingthrowdown.com/\"\u003e\u003ch4\u003eProgramming Throwdown\u003c/h4\u003e\u003cp\u003ePatrick Wheeler and Jason Gauci\u003c/p\u003e\u003c/a\u003e\u003cdiv\u003e\u003cbr/\u003e\u003cdiv\u003e\u003cp\u003e\u003cstrong\u003eIntro topic: Grills\u003cbr/\u003e\u003c/strong\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNews/Links:\u003cbr/\u003e\u003c/strong\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eYou can’t call yourself a senior until you’ve worked on a legacy project\u003cul\u003e\u003cli\u003e\u003ca href=\"https://www.infobip.com/developers/blog/seniors-working-on-a-legacy-project\"\u003ehttps://www.infobip.com/developers/blog/seniors-working-on-a-legacy-project\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eRecraft might be the most powerful AI image platform I’ve ever used — here’s why\u003cul\u003e\u003cli\u003e\u003ca href=\"https://www.tomsguide.com/ai/ai-image-video/recraft-might-be-the-most-powerful-ai-image-platform-ive-ever-used-heres-why\"\u003ehttps://www.tomsguide.com/ai/ai-image-video/recraft-might-be-the-most-powerful-ai-image-platform-ive-ever-used-heres-why\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eNASA has a list of 10 rules for software development\u003cul\u003e\u003cli\u003e\u003ca href=\"https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm\"\u003ehttps://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eAMD Radeon RX 9070 XT performance estimates leaked: 42% to 66% faster than Radeon RX 7900 GRE\u003cul\u003e\u003cli\u003e\u003ca href=\"https://www.tomshardware.com/tech-industry/amd-estimates-of-radeon-rx-9070-xt-performance-leaked-42-percent-66-percent-faster-than-radeon-rx-7900-gre\"\u003ehttps://www.tomshardware.com/tech-industry/amd-estimates-of-radeon-rx-9070-xt-performance-leaked-42-percent-66-percent-faster-than-radeon-rx-7900-gre\u003c/a\u003e \u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBook of the Show\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003ePatrick: \u003cul\u003e\u003cli\u003eThe Player of Games (Ian M Banks)\u003cul\u003e\u003cli\u003e\u003ca href=\"https://a.co/d/1ZpUhGl\"\u003ehttps://a.co/d/1ZpUhGl\u003c/a\u003e (non-affiliate)\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eJason: \u003cul\u003e\u003cli\u003eBasic Roleplaying Universal Game Engine\u003cul\u003e\u003cli\u003e\u003ca href=\"https://amzn.to/3ES4p5i\"\u003ehttps://amzn.to/3ES4p5i\u003cbr/\u003e\u003c/a\u003e\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eTool of the Show\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003ePatrick: \u003cul\u003e\u003cli\u003ePokemon Sword and Shield\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eJason: \u003cul\u003e\u003cli\u003eFeatures and Labels ( \u003ca href=\"https://fal.ai\"\u003ehttps://fal.ai\u003c/a\u003e )\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eTopic: Reinforcement Learning\u003cbr/\u003e\u003c/strong\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eThree types of AI\u003cul\u003e\u003cli\u003eSupervised Learning\u003c/li\u003e\u003cli\u003eUnsupervised Learning\u003c/li\u003e\u003cli\u003eReinforcement Learning\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eOnline vs Offline RL\u003c/li\u003e\u003cli\u003eOptimization algorithms\u003cul\u003e\u003cli\u003eValue optimization\u003cul\u003e\u003cli\u003eSARSA\u003c/li\u003e\u003cli\u003eQ-Learning\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003ePolicy optimization\u003cul\u003e\u003cli\u003ePolicy Gradients\u003c/li\u003e\u003cli\u003eActor-Critic\u003c/li\u003e\u003cli\u003eProximal Policy Optimization\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eValue vs Policy Optimization\u003cul\u003e\u003cli\u003eValue optimization is more intuitive (Value loss)\u003c/li\u003e\u003cli\u003ePolicy optimization is less intuitive at first (policy gradients)\u003c/li\u003e\u003cli\u003eConverting values to policies in deep learning is difficult\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eImitation Learning\u003cul\u003e\u003cli\u003eSupervised policy learning\u003c/li\u003e\u003cli\u003eOften used to bootstrap reinforcement learning\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003ePolicy Evaluation\u003cul\u003e\u003cli\u003ePropensity scoring versus model-based\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eChallenges to training RL model\u003cul\u003e\u003cli\u003eTwo optimization loops\u003cul\u003e\u003cli\u003eCollecting feedback vs updating the model\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eDifficult optimization target\u003cul\u003e\u003cli\u003ePolicy evaluation\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eRLHF \u0026amp;  GRPO\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003c/main\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": null,
  "modifiedTime": null
}
