{
  "id": "596ea58e-93e4-42ee-953e-1aeffc6a1568",
  "title": "Emerging Patterns in Building GenAI Products",
  "link": "https://martinfowler.com/articles/gen-ai-patterns/",
  "description": "",
  "author": "",
  "published": "2025-01-28T07:43:00-05:00",
  "source": "https://martinfowler.com/feed.atom",
  "categories": null,
  "byline": "Bharani Subramaniam",
  "length": 13055,
  "excerpt": "Patterns from our colleagues' work building with Generative AI",
  "siteName": "martinfowler.com",
  "favicon": "",
  "text": "The transition of Generative AI powered products from proof-of-concept to production has proven to be a significant challenge for software engineers everywhere. We believe that a lot of these difficulties come from folks thinking that these products are merely extensions to traditional transactional or analytical systems. In our engagements with this technology we've found that they introduce a whole new range of problems, including hallucination, unbounded data access and non-determinism. We've observed our teams follow some regular patterns to deal with these problems. This article is our effort to capture these. This is early days for these systems, we are learning new things with every phase of the moon, and new tools flood our radar. As with any pattern, none of these are gold standards that should be used in all circumstances. The notes on when to use it are often more important than the description of how it works. In this article we describe the patterns briefly, interspersed with narrative text to better explain context and interconnections. We've identified the pattern sections with the “✣” dingbat. Any section that describes a pattern has the title surrounded by a single ✣. The pattern description ends with “✣ ✣ ✣” These patterns are our attempt to understand what we have seen in our engagements. There's a lot of research and tutorial writing on these systems out there, and some decent books are beginning to appear to act as general education on these systems and how to use them. This article is not an attempt to be such a general education, rather it's trying to organize the experience that our colleagues have had using these systems in the field. As such there will be gaps where we haven't tried some things, or we've tried them, but not enough to discern any useful pattern. As we work further we intend to revise and expand this material, as we extend this article we'll send updates to our usual feeds. Patterns in this Article Direct PromptingSend prompts directly from the user to a Foundation LLM EvalsEvaluate the responses of an LLM in the context of a specific task Direct Prompting Send prompts directly from the user to a Foundation LLM The most basic approach to using an LLM is to connect an off-the-shelf LLM directly to a user, allowing the user to type prompts to the LLM and receive responses without any intermediate steps. This is the kind of experience that LLM vendors may offer directly. When to use it While this is useful in many contexts, and its usage triggered the wide excitement about using LLMs, it has some significant shortcomings. The first problem is that the LLM is constrained by the data it was trained on. This means that the LLM will not know anything that has happened since it was trained. It also means that the LLM will be unaware of specific information that's outside of its training set. Indeed even if it's within the training set, it's still unaware of the context that's operating in, which should make it prioritize some parts of its knowledge base that's more relevant to this context. As well as knowledge base limitations, there are also concerns about how the LLM will behave, particularly when faced with malicious prompts. Can it be tricked to divulging confidential information, or to giving misleading replies that can cause problems for the organization hosting the LLM. LLMs have a habit of showing confidence even when their knowledge is weak, and freely making up plausible but nonsensical answers. While this can be amusing, it becomes a serious liability if the LLM is acting as a spoke-bot for an organization. Direct Prompting is a powerful tool, but one that often cannot be used alone. We've found that for our clients to use LLMs in practice, they need additional measures to deal with the limitations and problems that Direct Prompting alone brings with it. The first step we need to take is to figure out how good the results of an LLM really are. In our regular software development work we've learned the value of putting a strong emphasis on testing, checking that our systems reliably behave the way we intend them to. When evolving our practices to work with Gen AI, we've found it's crucial to establish a systematic approach for evaluating the effectiveness of a model's responses. This ensures that any enhancements—whether structural or contextual—are truly improving the model’s performance and aligning with the intended goals. In the world of gen-ai, this leads to... Evals Evaluate the responses of an LLM in the context of a specific task Whenever we build a software system, we need to ensure that it behaves in a way that matches our intentions. With traditional systems, we do this primarily through testing. We provided a thoughtfully selected sample of input, and verified that the system responds in the way we expect. With LLM-based systems, we encounter a system that no longer behaves deterministically. Such a system will provide different outputs to the same inputs on repeated requests. This doesn't mean we cannot examine its behavior to ensure it matches our intentions, but it does mean we have to think about it differently. The Gen-AI examines behavior through “evaluations”, usually shortened to “evals”. Although it is possible to evaluate the model on individual output, it is more common to assess its behavior across a range of scenarios. This approach ensures that all anticipated situations are addressed and the model's outputs meet the desired standards. Scoring and Judging Necessary arguments are fed through a scorer, which is a component or function that assigns numerical scores to generated outputs, reflecting evaluation metrics like relevance, coherence, factuality, or semantic similarity between the model's output and the expected answer. Model Input Model Output Expected Output Retrieval context from RAG Metrics to evaluate (accuracy, relevance…) Performance Score Ranking of Results Additional Feedback Different evaluation techniques exist based on who computes the score, raising the question: who, ultimately, will act as the judge? Self evaluation: Self-evaluation lets LLMs self-assess and enhance their own responses. Although some LLMs can do this better than others, there is a critical risk with this approach. If the model’s internal self-assessment process is flawed, it may produce outputs that appear more confident or refined than they truly are, leading to reinforcement of errors or biases in subsequent evaluations. While self-evaluation exists as a technique, we strongly recommend exploring other strategies. LLM as a judge: The output of the LLM is evaluated by scoring it with another model, which can either be a more capable LLM or a specialized Small Language Model (SLM). While this approach involves evaluating with an LLM, using a different LLM helps address some of the issues of self-evaluation. Since the likelihood of both models sharing the same errors or biases is low, this technique has become a popular choice for automating the evaluation process. Human evaluation: Vibe checking is a technique to evaluate if the LLM responses match the desired tone, style, and intent. It is an informal way to assess if the model “gets it” and responds in a way that feels right for the situation. In this technique, humans manually write prompts and evaluate the responses. While challenging to scale, it’s the most effective method for checking qualitative elements that automated methods typically miss. In our experience, combining LLM as a judge with human evaluation works better for gaining an overall sense of how LLM is performing on key aspects of your Gen AI product. This combination enhances the evaluation process by leveraging both automated judgment and human insight, ensuring a more comprehensive understanding of LLM performance. Example Here is how we can use DeepEval to test the relevancy of LLM responses from our nutrition app from deepeval import assert_test from deepeval.test_case import LLMTestCase from deepeval.metrics import AnswerRelevancyMetric def test_answer_relevancy(): answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5) test_case = LLMTestCase( input=\"What is the recommended daily protein intake for adults?\", actual_output=\"The recommended daily protein intake for adults is 0.8 grams per kilogram of body weight.\", retrieval_context=[\"\"\"Protein is an essential macronutrient that plays crucial roles in building and repairing tissues.Good sources include lean meats, fish, eggs, and legumes. The recommended daily allowance (RDA) for protein is 0.8 grams per kilogram of body weight for adults. Athletes and active individuals may need more, ranging from 1.2 to 2.0 grams per kilogram of body weight.\"\"\"] ) assert_test(test_case, [answer_relevancy_metric]) In this test, we evaluate the LLM response by embedding it directly and measuring its relevance score. We can also consider adding integration tests that generate live LLM outputs and measure it across a number of pre-defined metrics. Running the Evals As with testing, we run evals as part of the build pipeline for a Gen-AI system. Unlike tests, they aren't simple binary pass/fail results, instead we have to set thresholds, together with checks to ensure performance doesn't decline. In many ways we treat evals similarly to how we work with performance testing. Our use of evals isn't confined to pre-deployment. A live gen-AI system may change its performance while in production. So we need to carry out regular evaluations of the deployed production system, again looking for any decline in our scores. Evaluations can be used against the whole system, and against any components that have an LLM. Guardrails and Query Rewriting contain logically distinct LLMs, and can be evaluated individually, as well as part of the total request flow. Evals and Benchmarking Benchmarking is the process of establishing a baseline for comparing the output of LLMs for a well defined set of tasks. In benchmarking, the goal is to minimize variability as much as possible. This is achieved by using standardized datasets, clearly defined tasks, and established metrics to consistently track model performance over time. So when a new version of the model is released you can compare different metrics and take an informed decision to upgrade or stay with the current version. LLM creators typically handle benchmarking to assess overall model quality. As a Gen AI product owner, we can use these benchmarks to gauge how well the model performs in general. However, to determine if it’s suitable for our specific problem, we need to perform targeted evaluations. Unlike generic benchmarking, evals are used to measure the output of LLM for our specific task. There is no industry established dataset for evals, we have to create one that best suits our use case. When to use it Assessing the accuracy and value of any software system is important, we don't want users to make bad decisions based on our software's behavior. The difficult part of using evals lies in fact that it is still early days in our understanding of what mechanisms are best for scoring and judging. Despite this, we see evals as crucial to using LLM-based systems outside of situations where we can be comfortable that users treat the LLM-system with a healthy amount of skepticism. Evals provide a vital mechanism to consider the broad behavior of a generative AI powered system. We now need to turn to looking at how to structure that behavior. Before we can go there, however, we need to understand an important foundation for generative, and other AI based, systems: how they work with the vast amounts of data that they are trained on, and manipulate to determine their output. We are publishing this article in installments. Future installments will describe embeddings, (a core data handling technique), Retrieval Augmented Generation (RAG), its limitations, the patterns we've found overcome these limitations, and the alternative of Fine Tuning. To find out when we publish the next installment subscribe to this site's RSS feed, or Martin's feeds on Mastodon, Bluesky, LinkedIn, or X (Twitter).",
  "image": "https://martinfowler.com/articles/gen-ai-patterns/card.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cp\u003eThe transition of Generative AI powered products from proof-of-concept to\n    production has proven to be a significant challenge for software engineers\n    everywhere. We believe that a lot of these difficulties come from folks thinking\n    that these products are merely extensions to traditional transactional or\n    analytical systems. In our engagements with this technology we\u0026#39;ve found that\n    they introduce a whole new range of problems, including hallucination,\n    unbounded data access and non-determinism.\u003c/p\u003e\n\n\u003cp\u003eWe\u0026#39;ve observed our teams follow some regular patterns to deal with these\n    problems. This article is our effort to capture these. This is early days\n    for these systems, we are learning new things with every phase of the moon,\n    and new tools \u003ca href=\"https://www.thoughtworks.com/radar\"\u003eflood our radar\u003c/a\u003e. As with any\n    pattern, none of these are gold standards that should be used in all\n    circumstances. The notes on when to use it are often more important than the\n    description of how it works.\u003c/p\u003e\n\n\u003cp\u003eIn this article we describe the patterns briefly, interspersed with\n    narrative text to better explain context and interconnections. We\u0026#39;ve\n    identified the pattern sections with the “✣” dingbat. Any section that\n    describes a pattern has the title surrounded by a single ✣. The pattern\n    description ends with “✣ ✣ ✣”\u003c/p\u003e\n\n\u003cp\u003eThese patterns are our attempt to understand what \u003ci\u003ewe have seen\u003c/i\u003e in our\n    engagements. There\u0026#39;s a lot of research and tutorial writing on these systems\n    out there, and some decent books are beginning to appear to act as general\n    education on these systems and how to use them. This article is not an\n    attempt to be such a general education, rather it\u0026#39;s trying to organize the\n    experience that our colleagues have had using these systems in the field. As\n    such there will be gaps where we haven\u0026#39;t tried some things, or we\u0026#39;ve tried\n    them, but not enough to discern any useful pattern. As we work further we\n    intend to revise and expand this material, as we extend this article we\u0026#39;ll\n    send updates to \u003ca href=\"https://martinfowler.com/aboutMe.html#updates\"\u003eour usual feeds\u003c/a\u003e.\u003c/p\u003e\n\n\u003ctable\u003e\n\u003ccaption\u003ePatterns in this Article\u003c/caption\u003e\n\n\u003ctbody\u003e\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"#direct-prompt\"\u003eDirect Prompting\u003c/a\u003e\u003c/td\u003e\u003ctd\u003eSend prompts directly from the user to a Foundation LLM\u003c/td\u003e\u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"#evals\"\u003eEvals\u003c/a\u003e\u003c/td\u003e\u003ctd\u003eEvaluate the responses of an LLM in the context of a specific\n    task\u003c/td\u003e\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003csection id=\"direct-prompt\"\u003e\n\u003ch2\u003eDirect Prompting\u003c/h2\u003e\n\n\u003cp\u003eSend prompts directly from the user to a Foundation LLM\u003c/p\u003e\n\n\u003cdiv id=\"prompt-response.svg\"\u003e\u003cp\u003e\u003cimg src=\"https://martinfowler.com/articles/gen-ai-patterns/prompt-response.svg\"/\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eThe most basic approach to using an LLM is to connect an off-the-shelf\n      LLM directly to a user, allowing the user to type prompts to the LLM and\n      receive responses without any intermediate steps. This is the kind of\n      experience that LLM vendors may offer directly.\u003c/p\u003e\n\n\u003csection\u003e\n\u003ch4\u003eWhen to use it\u003c/h4\u003e\n\n\u003cp\u003eWhile this is useful in many contexts, and its usage triggered the wide\n      excitement about using LLMs, it has some significant shortcomings.\u003c/p\u003e\n\n\u003cp\u003eThe first problem is that the LLM is constrained by the data it\n      was trained on. This means that the LLM will not know anything that has\n      happened since it was trained. It also means that the LLM will be unaware\n      of specific information that\u0026#39;s outside of its training set. Indeed even if\n      it\u0026#39;s within the training set, it\u0026#39;s still unaware of the context that\u0026#39;s\n      operating in, which should make it prioritize some parts of its knowledge\n      base that\u0026#39;s more relevant to this context. \u003c/p\u003e\n\n\u003cp\u003eAs well as knowledge base limitations, there are also concerns about\n      how the LLM will behave, particularly when faced with malicious prompts.\n      Can it be tricked to divulging confidential information, or to giving\n      misleading replies that can cause problems for the organization hosting\n      the LLM. LLMs have a habit of showing confidence even when their\n      knowledge is weak, and freely making up plausible but nonsensical\n      answers. While this can be amusing, it becomes a serious liability if the\n      LLM is acting as a spoke-bot for an organization.\u003c/p\u003e\n\u003c/section\u003e\n\u003c/section\u003e\n\n\u003cp\u003e\u003ca href=\"#direct-prompt\"\u003eDirect Prompting\u003c/a\u003e is a powerful tool, but one that often\n    cannot be used alone. We\u0026#39;ve found that for our clients to use LLMs in\n    practice, they need additional measures to deal with the limitations and\n    problems that \u003ca href=\"#direct-prompt\"\u003eDirect Prompting\u003c/a\u003e alone brings with it. \u003c/p\u003e\n\n\u003cp\u003eThe first step we need to take is to figure out how good the results of\n    an LLM really are. In our regular software development work we\u0026#39;ve learned\n    the value of putting a strong emphasis on testing, checking that our systems\n    reliably behave the way we intend them to. When evolving our practices to\n    work with Gen AI, we\u0026#39;ve found it\u0026#39;s crucial to establish a systematic\n    approach for evaluating the effectiveness of a model\u0026#39;s responses. This\n    ensures that any enhancements—whether structural or contextual—are truly\n    improving the model’s performance and aligning with the intended goals. In\n    the world of gen-ai, this leads to...\u003c/p\u003e\n\n\u003csection id=\"evals\"\u003e\n\u003ch2\u003eEvals\u003c/h2\u003e\n\n\u003cp\u003eEvaluate the responses of an LLM in the context of a specific\n    task\u003c/p\u003e\n\n\u003cp\u003eWhenever we build a software system, we need to ensure that it behaves\n    in a way that matches our intentions. With traditional systems, we do this primarily\n    through testing. We provided a thoughtfully selected sample of input, and\n    verified that the system responds in the way we expect.\u003c/p\u003e\n\n\u003cp\u003eWith LLM-based systems, we encounter a system that no longer behaves\n    deterministically. Such a system will provide different outputs to the same\n    inputs on repeated requests. This doesn\u0026#39;t mean we cannot examine its\n    behavior to ensure it matches our intentions, but it does mean we have to\n    think about it differently.\u003c/p\u003e\n\n\u003cp\u003eThe Gen-AI examines behavior through “evaluations”, usually shortened\n    to “evals”. Although it is possible to evaluate the model on individual output, \n    it is more common to assess its behavior across a range of scenarios. \n    This approach ensures that all anticipated situations are addressed and the \n    model\u0026#39;s outputs meet the desired standards.\u003c/p\u003e\n\n\u003csection id=\"ScoringAndJudging\"\u003e\n\u003ch3\u003eScoring and Judging\u003c/h3\u003e\n\n\u003cp\u003eNecessary arguments are fed through a scorer, which is a component or\n      function that assigns numerical scores to generated outputs, reflecting\n      evaluation metrics like relevance, coherence, factuality, or semantic\n      similarity between the model\u0026#39;s output and the expected answer.\u003c/p\u003e\n\n\u003cdiv\u003e\n\u003cdiv\u003e\n\u003cp\u003eModel Input\u003c/p\u003e\n\n\u003cp\u003eModel Output\u003c/p\u003e\n\n\u003cp\u003eExpected Output\u003c/p\u003e\n\n\u003cp\u003eRetrieval context from RAG\u003c/p\u003e\n\n\u003cp\u003eMetrics to evaluate \u003cbr/\u003e (accuracy, relevance…)\u003c/p\u003e\n\u003c/div\u003e\n\n\n\n\n\n\n\n\u003cdiv\u003e\n\u003cp\u003ePerformance Score\u003c/p\u003e\n\n\u003cp\u003eRanking of Results\u003c/p\u003e\n\n\u003cp\u003eAdditional Feedback\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\n\u003cp\u003eDifferent evaluation techniques exist based on who computes the score,\n      raising the question: who, ultimately, will act as the judge?\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eSelf evaluation: \u003c/b\u003eSelf-evaluation lets LLMs self-assess and enhance\n        their own responses. Although some LLMs can do this better than others, there\n        is a critical risk with this approach. If the model’s internal self-assessment\n        process is flawed, it may produce outputs that appear more confident or refined\n        than they truly are, leading to reinforcement of errors or biases in subsequent\n        evaluations. While self-evaluation exists as a technique, we strongly recommend\n        exploring other strategies.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eLLM as a judge: \u003c/b\u003eThe output of the LLM is evaluated  by scoring it with\n        another model, which can either be a more capable LLM or a specialized\n        Small Language Model (SLM). While this approach involves evaluating with\n        an LLM, using a different LLM helps address some of the issues of self-evaluation.\n        Since the likelihood of both models sharing the same errors or biases is low,\n        this technique has become a popular choice for automating the evaluation process.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eHuman evaluation: \u003c/b\u003eVibe checking is a technique to evaluate if\n        the LLM responses match the desired tone, style, and intent. It is an\n        informal way to assess if the model “gets it” and responds in a way that\n        feels right for the situation. In this technique, humans manually write\n        prompts and evaluate the responses. While challenging to scale, it’s the\n        most effective method for checking qualitative elements that automated\n        methods typically miss. \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn our experience,\n      combining LLM as a judge with human evaluation works better for\n      gaining an overall sense of how LLM is performing on key aspects of your\n      Gen AI product. This combination enhances the evaluation process by leveraging\n      both automated judgment and human insight, ensuring a more comprehensive\n      understanding of LLM performance.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Example\"\u003e\n\u003ch3\u003eExample\u003c/h3\u003e\n\n\u003cp\u003eHere is how we can use \u003ca href=\"https://docs.confident-ai.com\"\u003eDeepEval\u003c/a\u003e to test the\n      relevancy of LLM responses from our nutrition app\u003c/p\u003e\n\n\u003cpre\u003efrom deepeval import assert_test\nfrom deepeval.test_case import LLMTestCase\nfrom deepeval.metrics import AnswerRelevancyMetric\n\ndef test_answer_relevancy():\n  answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n  test_case = LLMTestCase(\n    input=\u0026#34;What is the recommended daily protein intake for adults?\u0026#34;,\n    actual_output=\u0026#34;The recommended daily protein intake for adults is 0.8 grams per kilogram of body weight.\u0026#34;,\n    retrieval_context=[\u0026#34;\u0026#34;\u0026#34;Protein is an essential macronutrient that plays crucial roles in building and \n      repairing tissues.Good sources include lean meats, fish, eggs, and legumes. The recommended \n      daily allowance (RDA) for protein is 0.8 grams per kilogram of body weight for adults. \n      Athletes and active individuals may need more, ranging from 1.2 to 2.0 \n      grams per kilogram of body weight.\u0026#34;\u0026#34;\u0026#34;]\n  )\n  assert_test(test_case, [answer_relevancy_metric])\n\u003c/pre\u003e\n\n\u003cp\u003eIn this test, we evaluate the LLM response by embedding it directly and\n      measuring its relevance score. We can also consider adding integration tests\n      that generate live LLM outputs and measure it across a number of \u003ca href=\"https://docs.confident-ai.com/docs/metrics-introduction\"\u003epre-defined metrics.\u003c/a\u003e\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"RunningTheEvals\"\u003e\n\u003ch3\u003eRunning the Evals\u003c/h3\u003e\n\n\u003cp\u003eAs with testing, we run evals as part of the build pipeline for a\n      Gen-AI system. Unlike tests, they aren\u0026#39;t simple binary pass/fail results,\n      instead we have to set thresholds, together with checks to ensure\n      performance doesn\u0026#39;t decline. In many ways we treat evals similarly to how\n      we work with performance testing.\u003c/p\u003e\n\n\u003cp\u003eOur use of evals isn\u0026#39;t confined to pre-deployment. A live gen-AI system\n      may change its performance while in production. So we need to carry out\n      regular evaluations of the deployed production system, again looking for\n      any decline in our scores.\u003c/p\u003e\n\n\u003cp\u003eEvaluations can be used against the whole system, and against any\n      components that have an LLM. Guardrails and Query Rewriting contain logically distinct LLMs, and can be evaluated\n      individually, as well as part of the total request flow.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"EvalsAndBenchmarking\"\u003e\n\u003ch3\u003eEvals and Benchmarking\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003ci\u003eBenchmarking\u003c/i\u003e is the process of establishing a baseline for comparing the\n      output of LLMs for a well defined set of tasks. In benchmarking, the goal is\n      to minimize variability as much as possible. This is achieved by using\n      standardized datasets, clearly defined tasks, and established metrics to\n      consistently track model performance over time. So when a new version of the\n      model is released you can compare different metrics and take an informed\n      decision to upgrade or stay with the current version.\u003c/p\u003e\n\n\u003cp\u003eLLM creators typically handle benchmarking to assess overall model quality.\n      As a Gen AI product owner, we can use these benchmarks to gauge how\n      well the model performs in general. However, to determine if it’s suitable\n      for our specific problem, we need to perform targeted evaluations.\u003c/p\u003e\n\n\u003cp\u003eUnlike generic benchmarking, evals are used to measure the output of LLM\n      for our specific task. There is no industry established dataset for evals,\n      we have to create one that best suits our use case.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection\u003e\n\u003ch4\u003eWhen to use it\u003c/h4\u003e\n\n\u003cp\u003eAssessing the accuracy and value of any software system is important,\n      we don\u0026#39;t want users to make bad decisions based on our software\u0026#39;s\n      behavior. The difficult part of using evals lies in fact that it is still\n      early days in our understanding of what mechanisms are best for scoring\n      and judging. Despite this, we see evals as crucial to using LLM-based\n      systems outside of situations where we can be comfortable that users treat\n      the LLM-system with a healthy amount of skepticism.\u003c/p\u003e\n\u003c/section\u003e\n\u003c/section\u003e\n\n\u003cp\u003e\u003ca href=\"#evals\"\u003eEvals\u003c/a\u003e provide a vital mechanism to consider the broad behavior\n    of a generative AI powered system. We now need to turn to looking at how to\n    structure that behavior. Before we can go there, however, we need to\n    understand an important foundation for generative, and other AI based,\n    systems: how they work with the vast amounts of data that they are trained\n    on, and manipulate to determine their output.\u003c/p\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eWe are publishing this article in installments. Future installments\n        will describe embeddings, (a core data handling technique), Retrieval\n        Augmented Generation (RAG), its limitations, the patterns we\u0026#39;ve found\n        overcome these limitations, and the alternative of Fine Tuning.\u003c/p\u003e\n\n\n\u003cp\u003e To find out when we publish the next installment subscribe to this\n    site\u0026#39;s\n    \u003ca href=\"https://martinfowler.com/feed.atom\"\u003eRSS feed\u003c/a\u003e, or Martin\u0026#39;s feeds on\n    \u003ca href=\"https://toot.thoughtworks.com/@mfowler\"\u003eMastodon\u003c/a\u003e,\n    \u003ca href=\"https://bsky.app/profile/martinfowler.com\"\u003eBluesky\u003c/a\u003e,\n    \u003ca href=\"https://www.linkedin.com/in/martin-fowler-com/\"\u003eLinkedIn\u003c/a\u003e, or\n    \u003ca href=\"https://twitter.com/martinfowler\"\u003eX (Twitter)\u003c/a\u003e.\n    \u003c/p\u003e\n\u003c/div\u003e\n\n\u003chr/\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": null,
  "modifiedTime": "2025-01-28T00:00:00Z"
}
