{
  "id": "f92c2d07-a70c-4245-b4b9-9e40e96b43b1",
  "title": "Microsoft Introduces Mu: A Lightweight On-Device Language Model for Windows Settings",
  "link": "https://www.infoq.com/news/2025/06/microsoft-mu/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Microsoft has introduced Mu, a new small-scale language model designed to run locally on Neural Processing Units (NPUs), starting with its deployment in the Windows Settings application for Copilot+ PCs. The model allows users to control system settings using natural language, aiming to reduce reliance on cloud-based processing. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Thu, 26 Jun 2025 17:15:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Windows",
    "Artificial Intelligence",
    "Microsoft",
    "DevOps",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 3016,
  "excerpt": "Microsoft has introduced Mu, a new small-scale language model designed to run locally on Neural Processing Units (NPUs), starting with its deployment in the Windows Settings application for Copilot+ P",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250605075448/apple-touch-icon.png",
  "text": "Microsoft has introduced Mu, a new small-scale language model designed to run locally on Neural Processing Units (NPUs), starting with its deployment in the Windows Settings application for Copilot+ PCs. The model allows users to control system settings using natural language, aiming to reduce reliance on cloud-based processing. Mu is a 330 million parameter encoder–decoder transformer optimized for edge devices. According to Microsoft, this architecture reduces latency by reusing encoded input representations, unlike decoder-only models that must reprocess the full input-output sequence during generation. The result, the company says, is faster inference with lower memory overhead, meeting the performance needs for real-time interaction on personal devices. Source: Microsoft Blog Microsoft reports that on Qualcomm’s Hexagon NPU, Mu achieves a 47% reduction in first-token latency and nearly five times faster decoding compared to decoder-only models of similar size. Key features contributing to this include rotary positional embeddings (RoPE), grouped-query attention (GQA), dual LayerNorm, and model quantization techniques such as post-training quantization (PTQ) to 8- and 16-bit formats. These optimizations were developed in collaboration with chipmakers including AMD, Intel, and Qualcomm. To adapt Mu for the Windows Settings agent, Microsoft fine-tuned the model on over 3.6 million examples spanning hundreds of adjustable settings. Training included synthetic data generation, noise injection, prompt tuning, and low-rank adaptation (LoRA). The result is a system that can map user input, such as “turn off Bluetooth” or “increase brightness,” to actionable system-level changes, with Microsoft stating that typical response times remain under 500 milliseconds. The agent is currently available to Windows Insiders in the Dev Channel using Copilot+ devices. To deal with unclear input—like short or vague questions—Microsoft added a fallback system that shows regular search results when there isn’t enough context. Industry observers have taken note of Mu's potential. Michał Choiński, an AI researcher and developer, commented: If Mu delivers consistently at that speed and scale, it could quietly redefine the desktop AI experience. Muhammad Akif, a founder of Techling LLC, added: If Mu maintains that level of performance, it could shift the AI narrative from ‘cloud-first’ to ‘device-smart. George Draco, an AI solutions specialist, highlighted its broader implications: Big leap for on-device AI. Offline speed with contextual memory changes how we think about productivity tools. Curious to see how Mu reshapes daily workflows. Microsoft says it plans to expand support to more settings categories and improve performance on short queries, as Mu becomes a foundation for broader on-device AI capabilities. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2025/06/microsoft-mu/en/headerimage/generatedHeaderImage-1750956763183.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cdiv\u003e\u003cp\u003eMicrosoft has introduced \u003ca href=\"https://blogs.windows.com/windowsexperience/2025/06/23/introducing-mu-language-model-and-how-it-enabled-the-agent-in-windows-settings/\"\u003eMu\u003c/a\u003e, a new small-scale language model designed to run locally on Neural Processing Units (NPUs), starting with its deployment in the Windows Settings application for Copilot+ PCs. The model allows users to control system settings using natural language, aiming to reduce reliance on cloud-based processing.\u003c/p\u003e\u003cp\u003e\n\nMu is a 330 million parameter encoder–decoder transformer optimized for edge devices. According to Microsoft, this architecture reduces latency by reusing encoded input representations, unlike decoder-only models that must reprocess the full input-output sequence during generation. The result, the company says, is faster inference with lower memory overhead, meeting the performance needs for real-time interaction on personal devices.\u003c/p\u003e\u003c/div\u003e\n\n\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Windows diagram\" data-src=\"news/2025/06/microsoft-mu/en/resources/1Zrzut ekranu 2025-06-26 184057-1750956762263.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/06/microsoft-mu/en/resources/1Zrzut ekranu 2025-06-26 184057-1750956762263.png\" rel=\"share\"/\u003e\u003cbr/\u003e\n\u003cem\u003eSource: Microsoft Blog\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\n\nMicrosoft reports that on Qualcomm’s Hexagon NPU, Mu achieves a 47% reduction in first-token latency and nearly five times faster decoding compared to decoder-only models of similar size. Key features contributing to this include rotary positional embeddings (RoPE), grouped-query attention (GQA), dual LayerNorm, and model quantization techniques such as post-training quantization (PTQ) to 8- and 16-bit formats. These optimizations were developed in collaboration with chipmakers including AMD, Intel, and Qualcomm.\u003c/p\u003e\u003cp\u003e\n\nTo adapt Mu for the Windows Settings agent, Microsoft fine-tuned the model on over 3.6 million examples spanning hundreds of adjustable settings. Training included synthetic data generation, noise injection, prompt tuning, and low-rank adaptation (LoRA). The result is a system that can map user input, such as “turn off Bluetooth” or “increase brightness,” to actionable system-level changes, with Microsoft stating that typical response times remain under 500 milliseconds.\u003c/p\u003e\u003cp\u003e\n\nThe agent is currently available to Windows Insiders in the Dev Channel using Copilot+ devices. To deal with unclear input—like short or vague questions—Microsoft added a fallback system that shows regular search results when there isn’t enough context.\u003c/p\u003e\u003cp\u003e\n\nIndustry observers have taken note of Mu\u0026#39;s potential. Michał Choiński, an AI researcher and developer, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:ugcPost:7343587049788116994?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7343587049788116994%2C7343920944236658689%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287343920944236658689%2Curn%3Ali%3AugcPost%3A7343587049788116994%29\"\u003ecommented\u003c/a\u003e:\u003c/p\u003e\u003c/div\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eIf Mu delivers consistently at that speed and scale, it could quietly redefine the desktop AI experience.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eMuhammad Akif, a founder of Techling LLC, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:ugcPost:7343587049788116994?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7343587049788116994%2C7343920944236658689%29\u0026amp;replyUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7343587049788116994%2C7343945071454478337%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287343920944236658689%2Curn%3Ali%3AugcPost%3A7343587049788116994%29\u0026amp;dashReplyUrn=urn%3Ali%3Afsd_comment%3A%287343945071454478337%2Curn%3Ali%3AugcPost%3A7343587049788116994%29\"\u003eadded\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eIf Mu maintains that level of performance, it could shift the AI narrative from ‘cloud-first’ to ‘device-smart.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eGeorge Draco, an AI solutions specialist, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:ugcPost:7343587049788116994?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7343587049788116994%2C7343935412446334976%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287343935412446334976%2Curn%3Ali%3AugcPost%3A7343587049788116994%29\"\u003ehighlighted\u003c/a\u003e its broader implications:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eBig leap for on-device AI. Offline speed with contextual memory changes how we think about productivity tools. Curious to see how Mu reshapes daily workflows.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eMicrosoft says it plans to expand support to more settings categories and improve performance on short queries, as Mu becomes a foundation for broader on-device AI capabilities.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-06-26T00:00:00Z",
  "modifiedTime": null
}
