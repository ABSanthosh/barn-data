{
  "id": "b717e515-9282-4a12-a3a8-1834711b0974",
  "title": "Mellum Goes Open Source: A Purpose-Built LLM for Developers, Now on Hugging Face",
  "link": "https://blog.jetbrains.com/ai/2025/04/mellum-goes-open-source-a-purpose-built-llm-for-developers-now-on-hugging-face/",
  "description": "Mellum doesnâ€™t try to know everything. Itâ€™s designed to do one thing really well: code completion. We call it a focal model â€“ built with purposeful depth and not concerned with chasing breadth. But code completion is just the start. Mellum will grow into a family of focal models, each specialized for different coding tasks [â€¦]",
  "author": "Anton Semenkin",
  "published": "Wed, 30 Apr 2025 11:10:27 +0000",
  "source": "https://blog.jetbrains.com/feed",
  "categories": [
    "jetbrains-ai",
    "ai",
    "mellum"
  ],
  "byline": "",
  "length": 6300,
  "excerpt": "Mellum doesn't pretend to know everything, but it does one thing exceptionally well. We call it a focal model â€“ small, efficient, and built with a specialized focus.",
  "siteName": "The JetBrains Blog",
  "favicon": "https://blog.jetbrains.com/wp-content/uploads/2024/01/cropped-mstile-310x310-1-180x180.png",
  "text": "Supercharge your tools with AI-powered features inside many JetBrains products JetBrains AIMellum Goes Open Source: A Purpose-Built LLM for Developers, Now on Hugging Face Mellum doesnâ€™t try to know everything. Itâ€™s designed to do one thing really well: code completion. We call it a focal model â€“ built with purposeful depth and not concerned with chasing breadth. But code completion is just the start. Mellum will grow into a family of focal models, each specialized for different coding tasks â€“ from code completion to diff prediction and beyond. Now, the base model is open-sourced and available on Hugging Face. Whether youâ€™re building tools, running research experiments, or just curious, youâ€™ll have full access to a fast, multilingual model*. *Mellum supports code completion for Java, Kotlin, Python, Go, PHP, C, C++, C#, JavaScript, TypeScript, CSS, HTML, Rust, Ruby.Â  ðŸ¤” Why open-source Mellum? This question was the subject of a big internal discussion. Mellum is not just a fine-tuned version of an open-source model. Itâ€™s a model we trained from scratch to power cloud-based code completion in JetBrains IDEs, and it was released to the public last year. Itâ€™s also the first in a planned family of code-specialized models. So why open-source it? Because we believe in transparency, collaboration, and the power of shared progress. From Linux and Git to Node.js and Docker, open source has driven some of the biggest leaps in technology. With open-source LLMs now outperforming some industry leaders, itâ€™s reasonable to assume that AIâ€™s general evolution might follow a similar trajectory. Mellum isnâ€™t a plug-and-play solution. By releasing it on Hugging Face, we are offering researchers, educators, and advanced teams the opportunity to explore how a purpose-built model works under the hood.Â  What is a focal model? In machine learning, specialization isnâ€™t new â€“ itâ€™s a core approach that has guided model design for decades, with models built to solve specific tasks efficiently and effectively. Somewhere along the way, the AI conversation shifted towards general-purpose models that aim to do everything, often at a massive computational and environmental cost.Â  Focal models return to that original principle: build models to excel in one area.Think of it like T-shaped skills â€“ a concept where a person has a broad understanding across many topics (the horizontal top bar or their breadth of knowledge), but deep expertise in one specific area (the vertical stem or depth). Focal models follow this same idea: they arenâ€™t built to handle everything. Instead, they specialize and excel at a single task where depth truly delivers value.Â  Mellum is a clear example. Itâ€™s a small, efficient model designed specifically for code-related tasks, starting with code completion.Why did we take this approach? Because not every problem demands a generalist solution, and not every team has the resources or need to run large, catch-all models.Focal models like Mellum offer clear advantages: Purpose-built precision for domain-specific tasks Cost efficiency when it comes to running and deploying them Lower computation and carbon footprints Greater accessibility for researchers, educators, and smaller teams This isnâ€™t a step backward â€“ itâ€™s applying proven principles of specialization to modern AI problems. We see that as a smarter way forward. How does Mellum perform? Mellum is a multilingual, 4B parameter model optimized specifically for code completion. We benchmarked it on several datasets across multiple languages, and also ran extensive human evaluations in JetBrains IDEs. In this post, weâ€™ll provide some evaluation data for Mellum compared to some bigger models. Full details, results, and comparisons are available on the model card. HumanEval InfillingRepoBench 1.1 (2K context, py)SAFIM (avg)single-linemulti-lineMellum-4B-base66.238.528.238.1InCoder-6B69.038.6â€”33.8CodeLlama-7B-base83.050.834.145.0CodeLlama-13B-base85.656.136.252.8DeepSeek-Coder-6.7B80.7â€”â€”63.4 Who Mellum is (and isnâ€™t) for Letâ€™s be real â€“ the average developer probably wonâ€™t fine-tune or deploy Mellum. Thatâ€™s okay. Instead, the current version of Mellum on Hugging Face is meant for: AI/ML researchers: Especially those exploring AIâ€™s role in software development, benchmarking, or model interpretability. AI/ML engineers and educators: As a foundation for learning how to build, fine-tune, and adapt domain-specific language models, or to support educational programs focused on LLM architecture and specialization. Try Mellum today Mellum is now live on Hugging Face. This is just the beginning. Weâ€™re not chasing generality â€“ weâ€™re building focus. If Mellum sparks even one meaningful experiment, contribution, or collaboration, we would consider it a win. Weâ€™d love for you to join us by trying Mellum for yourself. Explore Mellum on Hugging Face Subscribe to JetBrains AI Blog updates Discover more",
  "image": "https://blog.jetbrains.com/wp-content/uploads/2025/04/Blog-Featured-1280x720_text.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n    \u003cdiv\u003e\n                        \u003ca href=\"https://blog.jetbrains.com/ai/\"\u003e\n                            \u003cimg src=\"https://blog.jetbrains.com/wp-content/uploads/2024/01/JetBrains-AI.svg\" alt=\"Ai logo\"/\u003e\n                                                                                                \n                                                                                    \u003c/a\u003e\n                                                    \u003cp\u003eSupercharge your tools with AI-powered features inside many JetBrains products\u003c/p\u003e\n                                            \u003c/div\u003e\n                            \u003csection data-clarity-region=\"article\"\u003e\n                \u003cdiv\u003e\n                    \t\t\t\t\u003cp\u003e\u003ca href=\"https://blog.jetbrains.com/ai/category/jetbrains-ai/\"\u003eJetBrains AI\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"major-updates\"\u003eMellum Goes Open Source: A Purpose-Built LLM for Developers, Now on Hugging Face\u003c/h2\u003e                    \u003cdiv\u003e\n                            \u003cp\u003e\u003cimg src=\"https://blog.jetbrains.com/wp-content/uploads/2025/04/github_copy-e1746002997805-200x200.jpg\" width=\"200\" height=\"200\" alt=\"Anton Semenkin\" loading=\"lazy\"/\u003e\n                                                                                                                    \u003cimg src=\"https://blog.jetbrains.com/wp-content/uploads/2025/04/IMG_6815-e1746002737843-200x200.jpg\" width=\"200\" height=\"200\" alt=\"Michelle Frost\" loading=\"lazy\"/\u003e\n                                                                                                \u003c/p\u003e\n                            \n                        \u003c/div\u003e\n                    \n\u003cp\u003eMellum doesnâ€™t try to know everything. Itâ€™s designed to do one thing really well: code completion. We call it a \u003cstrong\u003efocal model \u003c/strong\u003eâ€“ built with purposeful depth and not concerned with chasing breadth.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut code completion is just the start.\u003c/p\u003e\n\n\n\n\u003cp\u003eMellum will grow into a family of focal models, each specialized for different coding tasks â€“ from code completion to diff prediction and beyond.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, the base model is open-sourced and available on \u003ca href=\"https://huggingface.co/JetBrains/Mellum-4b-base\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e. Whether youâ€™re building tools, running research experiments, or just curious, youâ€™ll have full access to a fast, multilingual model*.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003e*Mellum supports code completion for Java, Kotlin, Python, Go, PHP, C, C++, C#, JavaScript, TypeScript, CSS, HTML, Rust, Ruby.Â \u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003ch2\u003eðŸ¤” Why open-source Mellum?\u003c/h2\u003e\n\n\n\n\u003cp\u003eThis question was the subject of a big internal discussion.\u003c/p\u003e\n\n\n\n\u003cp\u003eMellum is not just a fine-tuned version of an open-source model. Itâ€™s a model we trained from scratch to power cloud-based code completion in JetBrains IDEs, and it was \u003ca href=\"https://blog.jetbrains.com/blog/2024/10/22/introducing-mellum-jetbrains-new-llm-built-for-developers/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ereleased to the public last year\u003c/a\u003e. Itâ€™s also the first in a planned family of code-specialized models.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo why open-source it?\u003c/p\u003e\n\n\n\n\u003cp\u003eBecause we believe in transparency, collaboration, and the power of shared progress. From Linux and Git to Node.js and Docker, open source has driven some of the biggest leaps in technology. With \u003ca href=\"https://huggingface.co/spaces/opencompass/open_vlm_leaderboard\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eopen-source LLMs now outperforming some industry leaders\u003c/a\u003e, itâ€™s reasonable to assume that AIâ€™s general evolution might follow a similar trajectory.\u003c/p\u003e\n\n\n\n\u003cp\u003eMellum isnâ€™t a plug-and-play solution. By releasing it on \u003ca href=\"https://huggingface.co/JetBrains/Mellum-4b-base\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e, we are offering researchers, educators, and advanced teams the opportunity to explore how a purpose-built model works under the hood.Â \u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat is a focal model?\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn machine learning, specialization isnâ€™t new â€“ itâ€™s a core approach that has guided model design for decades, with models built to solve specific tasks efficiently and effectively. Somewhere along the way, the AI conversation shifted towards general-purpose models that aim to do everything, often at a massive computational and environmental cost.Â \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eFocal models return to that original principle: build models to excel in one area.\u003c/p\u003e\u003cp\u003eThink of it like T-shaped skills â€“ a concept where a person has a broad understanding across many topics (the horizontal top bar or their \u003cem\u003ebreadth\u003c/em\u003e of knowledge), but deep expertise in one specific area (the vertical stem or \u003cem\u003edepth\u003c/em\u003e). Focal models follow this same idea: they arenâ€™t built to handle everything. Instead, they specialize and excel at a single task where depth truly delivers value.Â \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eMellum is a clear example. Itâ€™s a small, efficient model designed specifically for code-related tasks, starting with code completion.\u003c/p\u003e\u003cp\u003eWhy did we take this approach? Because not every problem demands a generalist solution, and not every team has the resources or need to run large, catch-all models.\u003c/p\u003e\u003cp\u003eFocal models like Mellum offer clear advantages:\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003ePurpose-built precision for domain-specific tasks\u003c/li\u003e\n\n\n\n\u003cli\u003eCost efficiency when it comes to running and deploying them\u003c/li\u003e\n\n\n\n\u003cli\u003eLower computation and carbon footprints\u003c/li\u003e\n\n\n\n\u003cli\u003eGreater accessibility for researchers, educators, and smaller teams\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eThis isnâ€™t a step backward â€“ itâ€™s applying proven principles of specialization to modern AI problems. We see that as a smarter way forward.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow does Mellum perform?\u003c/h2\u003e\n\n\n\n\u003cp\u003eMellum is a multilingual, \u003ca href=\"https://huggingface.co/JetBrains/Mellum-4b-base\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e4B parameter model\u003c/a\u003e optimized specifically for code completion. We benchmarked it on several datasets across multiple languages, and also ran extensive human evaluations in JetBrains IDEs. In this post, weâ€™ll provide some evaluation data for Mellum compared to some bigger models. Full details, results, and comparisons are available on the model card.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd rowspan=\"2\"\u003e\u003c/td\u003e\u003ctd colspan=\"2\"\u003eHumanEval Infilling\u003c/td\u003e\u003ctd rowspan=\"2\"\u003eRepoBench 1.1 (2K context, py)\u003c/td\u003e\u003ctd rowspan=\"2\"\u003eSAFIM (avg)\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003esingle-line\u003c/td\u003e\u003ctd\u003emulti-line\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eMellum-4B-base\u003c/td\u003e\u003ctd\u003e66.2\u003c/td\u003e\u003ctd\u003e38.5\u003c/td\u003e\u003ctd\u003e28.2\u003c/td\u003e\u003ctd\u003e38.1\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eInCoder-6B\u003c/td\u003e\u003ctd\u003e69.0\u003c/td\u003e\u003ctd\u003e38.6\u003c/td\u003e\u003ctd\u003eâ€”\u003c/td\u003e\u003ctd\u003e33.8\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eCodeLlama-7B-base\u003c/td\u003e\u003ctd\u003e83.0\u003c/td\u003e\u003ctd\u003e50.8\u003c/td\u003e\u003ctd\u003e34.1\u003c/td\u003e\u003ctd\u003e45.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eCodeLlama-13B-base\u003c/td\u003e\u003ctd\u003e85.6\u003c/td\u003e\u003ctd\u003e56.1\u003c/td\u003e\u003ctd\u003e36.2\u003c/td\u003e\u003ctd\u003e52.8\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eDeepSeek-Coder-6.7B\u003c/td\u003e\u003ctd\u003e80.7\u003c/td\u003e\u003ctd\u003eâ€”\u003c/td\u003e\u003ctd\u003eâ€”\u003c/td\u003e\u003ctd\u003e63.4\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eWho Mellum is (and isnâ€™t) for\u003c/h2\u003e\n\n\n\n\u003cp\u003eLetâ€™s be real â€“ the average developer probably wonâ€™t fine-tune or deploy Mellum. Thatâ€™s okay.\u003c/p\u003e\n\n\n\n\u003cp\u003eInstead, the current version of Mellum on Hugging Face is meant for:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eAI/ML researchers:\u003c/strong\u003e Especially those exploring AIâ€™s role in software development, benchmarking, or model interpretability.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eAI/ML engineers and educators:\u003c/strong\u003e As a foundation for learning how to build, fine-tune, and adapt domain-specific language models, or to support educational programs focused on LLM architecture and specialization.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003ch2\u003eTry Mellum today\u003c/h2\u003e\n\n\n\n\u003cp\u003eMellum is now live on \u003ca href=\"https://huggingface.co/JetBrains/Mellum-4b-base\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e. This is just the beginning. Weâ€™re not chasing generality â€“ weâ€™re building focus. If Mellum sparks even one meaningful experiment, contribution, or collaboration, we would consider it a win.\u003c/p\u003e\n\n\n\n\u003cp\u003eWeâ€™d love for you to join us by trying Mellum for yourself.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://huggingface.co/JetBrains/Mellum-4b-base\" target=\"_blank\" rel=\"noopener\"\u003e\u003ci\u003e\u003c/i\u003eExplore Mellum on Hugging Face\u003c/a\u003e\u003c/p\u003e\n                    \n                                                                                                                                                                                                                            \u003cdiv\u003e\n                                \u003cdiv\u003e\n                                                                            \u003ch4\u003eSubscribe to JetBrains AI Blog updates\u003c/h4\u003e\n                                                                                                            \n                                \u003c/div\u003e\n                                \n                                \u003cp\u003e\u003cimg src=\"https://blog.jetbrains.com/wp-content/themes/jetbrains/assets/img/img-form.svg\" alt=\"image description\"/\u003e\n                                                                    \u003c/p\u003e\n                            \u003c/div\u003e\n                                                            \u003c/div\u003e\n                \u003ca href=\"#\"\u003e\u003c/a\u003e\n                \n                \n            \u003c/section\u003e\n                    \u003cdiv\u003e\n                \u003cp\u003e\n                    \u003ch2\u003eDiscover more\u003c/h2\u003e\n                \u003c/p\u003e\n                \n            \u003c/div\u003e\n                \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": null,
  "modifiedTime": null
}
