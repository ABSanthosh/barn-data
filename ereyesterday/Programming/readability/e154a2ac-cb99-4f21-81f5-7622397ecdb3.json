{
  "id": "e154a2ac-cb99-4f21-81f5-7622397ecdb3",
  "title": "Gemma explained: What’s new in Gemma 3",
  "link": "https://developers.googleblog.com/en/gemma-explained-whats-new-in-gemma-3/",
  "description": "Gemma 3's new features include vision-language capabilities and architectural changes for improved memory efficiency and longer context handling compared to previous Gemma models.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Ju-yeong Ji, Ravin Kumar",
  "length": 15397,
  "excerpt": "Gemma 3's new features include vision-language capabilities and architectural changes for improved memory efficiency and longer context handling compared to previous Gemma models.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "Ju-yeong Ji Sr. Technical Consultant Gen AI – AI Studio Ravin Kumar Senior Research Engineer Google DeepMind The previous posts in the \"Gemma explained\" series provided a detailed overview of the Gemma model family's architectures. You can find links to each post below:Gemma explained: An overview of Gemma model family architecturesGemma explained: What’s new in Gemma 2Gemma explained: RecurrentGemma architectureGemma explained: PaliGemma architectureIn this post, you will explore the latest model, Gemma 3. Let’s dig in.Gemma 3A significant change from prior versions is Gemma 3’s new support for vision-language capabilities. Those familiar with PaliGemma’s architecture might recognize a SigLIP encoder used in Gemma 3, though it has been tailored for this specific implementation.Here’s the core parameters of the new models. Let’s explore the key differences and improvements in Gemma 3.Key DifferencesWhile Gemma 3's architecture inherits aspects from its predecessors, it also features new modifications that are described below.Vision-language supportA major enhancement in Gemma 3 is its new vision-language understanding capability. The 4B, 12B and 27B models employ a custom SigLIP vision encoder, which enables the model to interpret visual input.The vision encoder operates on fixed 896x896 square images. To handle the images with different aspect ratios or high resolutions, a “Pan\u0026Scan” algorithm is employed. This involves adaptively cropping the image, resizing each crop to 896x896, and then encoding it. While this method improves performance, particularly when detailed information is critical, it leads to increased computational overhead during inference.Additionally, Gemma 3 treats images as a sequence of compact “soft tokens” produced by MultiModalProjector. This technique significantly cuts down on the inference resources needed for image processing by representing visual data with a fixed number of 256 vectors. Before proceeding, you might wonder: “When should I use Gemma 3 versus PaliGemma 2?”PaliGemma 2's strength lies in features not found in Gemma 3 like image segmentation and object detection. However, Gemma 3 integrated and extended the technology from PaliGemma, offering multi-turn chat and strong zero-shot performance for handling various vision tasks directly.Your final decision should also consider your available computational resources and the importance of advanced features like longer context or multilingual support, where Gemma 3 offers notable enhancements.Architectural Changes for Memory EfficiencyThe architecture has been modified to reduce KV-cache memory usage, which tends to increase with long context.5-to-1 interleaved attentionThe updated model architecture is composed of repeating interleaving blocks, each containing 5 local attention layers with a sliding window of 1024 and 1 global attention layer. This design enables the model to capture both short- and long-range dependencies, leading to more accurate and contextually relevant responses.Note: Gemma 1 relied solely on global attention, whereas Gemma 2 introduced a hybrid approach by alternating between local and global attention layers. Gemma 3 integrates 5 dedicated local attention layers, resulting in responses that are both more precise and contextually appropriate. No softcappingBoth Gemma 2 and Gemma 3 utilize Grouped-Query Attention (GQA) with post-norm and pre-norm with RMSNorm. However, Gemma 3 gains both improved accuracy and faster processing speeds by adopting QK-norm in place of Gemma 2’s soft-capping mechanism. Longer ContextAs a result of the architectural changes discussed above, Gemma 3 utilizes interleaved attention to decrease memory requirements, enabling support for an extended context length. This allows for the analysis of longer documents and conversations without losing context. Specifically, it can handle 32k tokens for the 1B model and 128k tokens for larger models.Note: The 128k tokens context window allows the model to process an amount of text as long as a typical novel (around 80k words). This window size is approximately 96k words, 198 pages, 500 images, or 8+ minutes of video at 1 fps. Vision Encoder and ProcessingGemma 3 only uses bidirectional attention with image inputs.Normal attention (a.k.a uni-directional attention) is like reading. Imagine reading a book. You understand each word by considering the words that came before it. This is how typical attention works in language models. It’s sequential and looks backward to build context.On the other hand, bidirectional attention is like seeing a puzzle. Think of an image as a jigsaw puzzle. “Image Tokens” are like individual puzzle pieces. So it means each piece “looks at” and connects with every other piece in the image, regardless of their position. It considers the whole picture at once, not just a sequence. This gives a complete understanding because every part is related to every other part.So why not always bidirectional? While bidirectional attention (seeing the whole context) sounds better, it’s not always used for text. It’s all about the task:Understanding (Bidirectional is Good)For tasks where we have the entire text and need to deeply understand it (like in models such as BERT), bidirectional attention is great.Prediction (Uni-directional is Better)When the task is to predict the next word or generate text, a one-directional (autoregressive) approach is more natural. Predicting what comes next is inherently sequential. It’s also often more computationally efficient.The key difference is that the bidirectional approach is used when the model isn’t creating a sequence.The following visualization illustrates the attention mechanism in Gemma 3.Code: from transformers.utils.attention_visualizer import AttentionMaskVisualizer visualizer = AttentionMaskVisualizer(\"google/gemma-3-4b-it\") visualizer(\"\u003cstart_of_turn\u003euser\\n\u003cimg\u003eWhat is this?\u003cend_of_turn\u003e\\n\u003cstart_of_turn\u003emodel\\nIt's an image of a cat.\u003cend_of_turn\u003e\") Copied Output: You can also see how this attention mechanism differs from that of PaliGemma. To provide context for this comparison, PaliGemma operates by receiving one or more images along with a text-based task description (the prompt or prefix), and subsequently generates its prediction as a text string (the answer or suffix) in an autoregressive manner.Code: visualizer = AttentionMaskVisualizer(\"google/paligemma2-3b-mix-224\") visualizer(\"\u003cimg\u003e caption en\", suffix=\"a cat standing on a beach.\") Copied Output New tokenizerGemma 3 has improved multilingual capabilities due to a revisited data mixture with an increased amount of multilingual data (both monolingual and parallel).Gemma 3 also introduces an improved tokenizer. The vocabulary size has been changed to 262k, but uses the same SentencePiece tokenizer. To avoid errors, use the new tokenizer with Gemma 3. This is the same tokenizer as Gemini which is more balanced for non-English languages.Gemma 3 27B Gemma3ForConditionalGeneration( (vision_tower): SiglipVisionModel( (vision_model): SiglipVisionTransformer( (embeddings): SiglipVisionEmbeddings( (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid) (position_embedding): Embedding(4096, 1152) ) (encoder): SiglipEncoder( (layers): ModuleList( (0-26): 27 x SiglipEncoderLayer( (self_attn): SiglipSdpaAttention( (k_proj): Linear(in_features=1152, out_features=1152, bias=True) (v_proj): Linear(in_features=1152, out_features=1152, bias=True) (q_proj): Linear(in_features=1152, out_features=1152, bias=True) (out_proj): Linear(in_features=1152, out_features=1152, bias=True) ) (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True) (mlp): SiglipMLP( (activation_fn): PytorchGELUTanh() (fc1): Linear(in_features=1152, out_features=4304, bias=True) (fc2): Linear(in_features=4304, out_features=1152, bias=True) ) (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True) ) ) ) (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True) ) ) (multi_modal_projector): Gemma3MultiModalProjector( (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06) (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0) ) (language_model): Gemma3ForCausalLM( (model): Gemma3TextModel( (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 5376, padding_idx=0) (layers): ModuleList( (0-61): 62 x Gemma3DecoderLayer( (self_attn): Gemma3Attention( (q_proj): Linear(in_features=5376, out_features=4096, bias=False) (k_proj): Linear(in_features=5376, out_features=2048, bias=False) (v_proj): Linear(in_features=5376, out_features=2048, bias=False) (o_proj): Linear(in_features=4096, out_features=5376, bias=False) (q_norm): Gemma3RMSNorm((128,), eps=1e-06) (k_norm): Gemma3RMSNorm((128,), eps=1e-06) ) (mlp): Gemma3MLP( (gate_proj): Linear(in_features=5376, out_features=21504, bias=False) (up_proj): Linear(in_features=5376, out_features=21504, bias=False) (down_proj): Linear(in_features=21504, out_features=5376, bias=False) (act_fn): PytorchGELUTanh() ) (input_layernorm): Gemma3RMSNorm((5376,), eps=1e-06) (post_attention_layernorm): Gemma3RMSNorm((5376,), eps=1e-06) (pre_feedforward_layernorm): Gemma3RMSNorm((5376,), eps=1e-06) (post_feedforward_layernorm): Gemma3RMSNorm((5376,), eps=1e-06) ) ) (norm): Gemma3RMSNorm((5376,), eps=1e-06) (rotary_emb): Gemma3RotaryEmbedding() (rotary_emb_local): Gemma3RotaryEmbedding() ) (lm_head): Linear(in_features=5376, out_features=262208, bias=False) ) ) Copied Note: Technically RoPE (Rotary Positional Embedding) is inside the SDPA(Scaled Dot-Product Attention), but we simplified it in this diagram. Please see the code for the precise architectural details.Gemma 3 1B Gemma3ForCausalLM( (model): Gemma3TextModel( (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0) (layers): ModuleList( (0-25): 26 x Gemma3DecoderLayer( (self_attn): Gemma3Attention( (q_proj): Linear(in_features=1152, out_features=1024, bias=False) (k_proj): Linear(in_features=1152, out_features=256, bias=False) (v_proj): Linear(in_features=1152, out_features=256, bias=False) (o_proj): Linear(in_features=1024, out_features=1152, bias=False) (q_norm): Gemma3RMSNorm((256,), eps=1e-06) (k_norm): Gemma3RMSNorm((256,), eps=1e-06) ) (mlp): Gemma3MLP( (gate_proj): Linear(in_features=1152, out_features=6912, bias=False) (up_proj): Linear(in_features=1152, out_features=6912, bias=False) (down_proj): Linear(in_features=6912, out_features=1152, bias=False) (act_fn): PytorchGELUTanh() ) (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06) (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06) (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06) (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06) ) ) (norm): Gemma3RMSNorm((1152,), eps=1e-06) (rotary_emb): Gemma3RotaryEmbedding() (rotary_emb_local): Gemma3RotaryEmbedding() ) (lm_head): Linear(in_features=1152, out_features=262144, bias=False) ) Copied This text-only 1B model is specifically optimized for on-device use, making advanced AI accessible on mobile and embedded systems. This significantly impacts accessibility, privacy, and performance, as AI-powered applications can now function efficiently even with limited or no network connectivity.Key FindingsOur technical report provides in-depth details, but here’s a quick summary of Gemma 3’s main findings:Superior Performance at same sizes:Gemma 3 models achieve superior performance compared to Gemma 2 on both pre-trained instruction-tuned versions across various benchmarks. It is the best model that fits in a single consumer GPU or TPU host. The Gemma 27B IT model ranks among the top 10 models in LM Arena as of Apr 12, 2025, outperforming much larger open models and showing a significantly higher Elo score than Gemma 2.KV-cache Memory Reduction:The architectural changes in Gemma 3 effectively reduce the memory overhead of the KV cache during inference with long context compared to global-only attention mechanisms used in Gemma 1 and the 1:1 local/global ratio used in Gemma 2.Effective Long Context Handling:Gemma 3 models can generalize to 128k context length after RoPE rescaling during pre-training. We increase RoPE base frequency from 10k to 1M on global self-attention layers, and keep the frequency of the local layers at 10k.Improved Multilinguality:Gemma 3 utilizes the same tokenizer as Gemini which is more balanced for non-English languages. We also revised the pre-training data mixture and post-training process to enhance its multilingual capabilities.Impact of Vision Encoder Resolution:Higher resolution vision encoders lead to better performance on vision tasks. The Pan \u0026 Scan method further improves performance on tasks involving non-square aspect ratios, high-resolution images, and text reading in images.SummaryWe explored the architecture of Gemma 3, highlighting the new features that set it apart from previous versions. These architectural choices allow Gemma 3 to perform better on a broader set of tasks, including improved multilingual abilities and image interaction, while also paving the way for future, more capable and resource-friendly multimodal language models suitable for standard hardware.We believe Gemma 3’s innovations will empower researchers and developers to create the next generation of efficient and powerful multimodal language models.Thanks for reading!ReferencesPapersGemma 3 Technical ReportPaliGemma 2: A Family of Versatile VLMs for TransferSigmoid Loss for Language Image Pre-TrainingBERT: Pre-training of Deep Bidirectional Transformers for Language UnderstandingCode ExamplesPrompt with images and text using Gemma libraryGemma 3 Meme Generator📋 The complete Gemma architecture seriesGemma explained: An overview of Gemma model family architecturesGemma explained: What’s new in Gemma 2Gemma explained: RecurrentGemma architectureGemma explained: PaliGemma architectureGemma explained: What’s new in Gemma 3",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/whats-new-gemma-3.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \u003csection\u003e\n      \n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=Ju-yeong+Ji\"\u003eJu-yeong Ji\u003c/a\u003e\n            \n              \u003cspan\u003eSr. Technical Consultant\u003c/span\u003e\n            \n            \n              \u003cspan\u003eGen AI – AI Studio\u003c/span\u003e\n            \n          \u003c/p\u003e\n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=Ravin+Kumar\"\u003eRavin Kumar\u003c/a\u003e\n            \n              \u003cspan\u003eSenior Research Engineer\u003c/span\u003e\n            \n            \n              \u003cspan\u003eGoogle DeepMind\u003c/span\u003e\n            \n          \u003c/p\u003e\n        \n\n      \n      \u003c/section\u003e\n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"1sbwi\"\u003eThe previous posts in the \u0026#34;Gemma explained\u0026#34; series provided a detailed overview of the Gemma model family\u0026#39;s architectures. You can find links to each post below:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"4ncgm\"\u003e\u003ca href=\"https://developers.googleblog.com/gemma-explained-overview-gemma-model-family-architectures\"\u003eGemma explained: An overview of Gemma model family architectures\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"7em63\"\u003e\u003ca href=\"https://developers.googleblog.com/gemma-explained-new-in-gemma-2\"\u003eGemma explained: What’s new in Gemma 2\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ajaf6\"\u003e\u003ca href=\"https://developers.googleblog.com/gemma-explained-recurrentgemma-architecture/\"\u003eGemma explained: RecurrentGemma architecture\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"6qusf\"\u003e\u003ca href=\"https://developers.googleblog.com/gemma-explained-paligemma-architecture/\"\u003eGemma explained: PaliGemma architecture\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"d5cie\"\u003eIn this post, you will explore the latest model, \u003ca href=\"https://developers.googleblog.com/introducing-gemma3/\"\u003eGemma 3\u003c/a\u003e. Let’s dig in.\u003c/p\u003e\u003ch2 data-block-key=\"lceg9\" id=\"\"\u003e\u003cbr/\u003e\u003cb\u003eGemma 3\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"6a669\"\u003eA significant change from prior versions is Gemma 3’s new support for vision-language capabilities. Those familiar with PaliGemma’s architecture might recognize a \u003ca href=\"https://arxiv.org/abs/2303.15343\"\u003eSigLIP encoder\u003c/a\u003e used in Gemma 3, though it has been tailored for this specific implementation.\u003c/p\u003e\u003cp data-block-key=\"e0r7v\"\u003eHere’s the core parameters of the new models.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/core-parameter-of-new-gemma-3-models.original.png\" alt=\"Core parameters of new Gemma 3 models\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"1sbwi\"\u003eLet’s explore the key differences and improvements in Gemma 3.\u003c/p\u003e\u003ch2 data-block-key=\"0pu49\" id=\"key-differences\"\u003e\u003cbr/\u003e\u003cb\u003eKey Differences\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"fke56\"\u003eWhile Gemma 3\u0026#39;s architecture inherits aspects from its predecessors, it also features new modifications that are described below.\u003c/p\u003e\u003ch2 data-block-key=\"g4r17\" id=\"\"\u003e\u003cbr/\u003eVision-language support\u003c/h2\u003e\u003cp data-block-key=\"9m9a0\"\u003eA major enhancement in Gemma 3 is its new vision-language understanding capability. The 4B, 12B and 27B models employ a custom SigLIP vision encoder, which enables the model to interpret visual input.\u003c/p\u003e\u003cp data-block-key=\"btlmu\"\u003eThe vision encoder operates on fixed 896x896 square images. To handle the images with different aspect ratios or high resolutions, a “Pan\u0026amp;Scan” algorithm is employed. This involves adaptively cropping the image, resizing each crop to 896x896, and then encoding it. While this method improves performance, particularly when detailed information is critical, it leads to increased computational overhead during inference.\u003c/p\u003e\u003cp data-block-key=\"2qmo8\"\u003eAdditionally, Gemma 3 treats images as a sequence of compact “soft tokens” produced by MultiModalProjector. This technique significantly cuts down on the inference resources needed for image processing by representing visual data with a fixed number of 256 vectors.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/flow-image-gemma-3-architechture.original.png\" alt=\"flow image of gemma 3 architecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"1sbwi\"\u003eBefore proceeding, you might wonder: \u003cb\u003e“When should I use Gemma 3 versus PaliGemma 2?”\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"53pcv\"\u003e\u003ca href=\"https://arxiv.org/abs/2412.03555\"\u003ePaliGemma 2\u003c/a\u003e\u0026#39;s strength lies in features not found in Gemma 3 like image segmentation and object detection. However, Gemma 3 integrated and extended the technology from PaliGemma, offering multi-turn chat and strong zero-shot performance for handling various vision tasks directly.\u003c/p\u003e\u003cp data-block-key=\"38mcd\"\u003eYour final decision should also consider your available computational resources and the importance of advanced features like longer context or multilingual support, where Gemma 3 offers notable enhancements.\u003c/p\u003e\u003ch2 data-block-key=\"q9bmy\" id=\"architectural-changes-for-memory-efficiency\"\u003e\u003cbr/\u003eArchitectural Changes for Memory Efficiency\u003c/h2\u003e\u003cp data-block-key=\"78vf\"\u003eThe architecture has been modified to reduce KV-cache memory usage, which tends to increase with long context.\u003c/p\u003e\u003ch3 data-block-key=\"msu8k\" id=\"5-to-1-interleaved-attention\"\u003e\u003cb\u003e5-to-1 interleaved attention\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"14if3\"\u003eThe updated model architecture is composed of repeating interleaving blocks, each containing 5 local attention layers with a sliding window of 1024 and 1 global attention layer. This design enables the model to capture both short- and long-range dependencies, leading to more accurate and contextually relevant responses.\u003c/p\u003e\u003cblockquote data-block-key=\"1qr7k\"\u003e\u003cb\u003e\u003csup\u003eNote:\u003c/sup\u003e\u003c/b\u003e\u003csup\u003e Gemma 1 relied solely on global attention, whereas Gemma 2 introduced a hybrid approach by alternating between local and global attention layers. Gemma 3 integrates 5 dedicated local attention layers, resulting in responses that are both more precise and contextually appropriate.\u003c/sup\u003e\u003c/blockquote\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemma-3-local-attention-layers_vowwcQB.original.png\" alt=\"Gemma 3 local and global attention layers depicted in a table, compared to Gemma 1 and Gemma 2\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"qei8u\" id=\"no-softcapping\"\u003e\u003cb\u003eNo softcapping\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"dedkp\"\u003eBoth Gemma 2 and Gemma 3 utilize Grouped-Query Attention (GQA) with post-norm and pre-norm with RMSNorm. However, Gemma 3 gains both improved accuracy and faster processing speeds by adopting QK-norm in place of Gemma 2’s soft-capping mechanism.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma-3-grouped-query-attention.original.png\" alt=\"Gemma 3 grouped query attention\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"7uacz\" id=\"longer-context\"\u003eLonger Context\u003c/h2\u003e\u003cp data-block-key=\"3h2\"\u003eAs a result of the architectural changes discussed above, Gemma 3 utilizes interleaved attention to decrease memory requirements, enabling support for an extended context length. This allows for the analysis of longer documents and conversations without losing context. Specifically, it can handle 32k tokens for the 1B model and 128k tokens for larger models.\u003c/p\u003e\u003cblockquote data-block-key=\"3qqv8\"\u003e\u003cb\u003e\u003csup\u003eNote:\u003c/sup\u003e\u003c/b\u003e\u003csup\u003e The 128k tokens context window allows the model to process an amount of text as long as a typical novel (around 80k words). This window size is approximately 96k words, 198 pages, 500 images, or 8+ minutes of video at 1 fps.\u003c/sup\u003e\u003c/blockquote\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma-3-context-window.original.png\" alt=\"Gemma 3 context window\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"cwlwx\" id=\"vision-encoder-and-processing\"\u003eVision Encoder and Processing\u003c/h2\u003e\u003cp data-block-key=\"169ib\"\u003eGemma 3 only uses bidirectional attention with image inputs.\u003c/p\u003e\u003cp data-block-key=\"cqe00\"\u003eNormal attention (a.k.a uni-directional attention) is like reading. Imagine reading a book. You understand each word by considering the words that came before it. This is how typical attention works in language models. It’s sequential and looks backward to build context.\u003c/p\u003e\u003cp data-block-key=\"dpvsa\"\u003eOn the other hand, bidirectional attention is like seeing a puzzle. Think of an image as a jigsaw puzzle. “Image Tokens” are like individual puzzle pieces. So it means each piece “looks at” and connects with every other piece in the image, regardless of their position. It considers the whole picture at once, not just a sequence. This gives a complete understanding because every part is related to every other part.\u003c/p\u003e\u003cp data-block-key=\"b2mb7\"\u003eSo why not always bidirectional? While bidirectional attention (seeing the whole context) sounds better, it’s not always used for text. It’s all about the task:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"8v396\"\u003e\u003cb\u003eUnderstanding (Bidirectional is Good)\u003c/b\u003e\u003cbr/\u003eFor tasks where we have the entire text and need to deeply understand it (like in models such as \u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eBERT\u003c/a\u003e), bidirectional attention is great.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"njjd\"\u003e\u003cb\u003ePrediction (Uni-directional is Better)\u003c/b\u003e\u003cbr/\u003eWhen the task is to predict the next word or generate text, a one-directional (autoregressive) approach is more natural. Predicting what comes next is inherently sequential. It’s also often more computationally efficient.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"19thd\"\u003eThe key difference is that the bidirectional approach is used when the model isn’t creating a sequence.\u003c/p\u003e\u003cp data-block-key=\"87l8u\"\u003eThe following visualization illustrates the attention mechanism in Gemma 3.\u003c/p\u003e\u003cp data-block-key=\"2ks1p\"\u003eCode:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003etransformers.utils.attention_visualizer\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eAttentionMaskVisualizer\u003c/span\u003e\n\u003cspan\u003evisualizer\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eAttentionMaskVisualizer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;google/gemma-3-4b-it\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003evisualizer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026lt;start_of_turn\u0026gt;user\u003c/span\u003e\u003cspan\u003e\\n\u003c/span\u003e\u003cspan\u003e\u0026lt;img\u0026gt;What is this?\u0026lt;end_of_turn\u0026gt;\u003c/span\u003e\u003cspan\u003e\\n\u003c/span\u003e\u003cspan\u003e\u0026lt;start_of_turn\u0026gt;model\u003c/span\u003e\u003cspan\u003e\\n\u003c/span\u003e\u003cspan\u003eIt\u0026#39;s an image of a cat.\u0026lt;end_of_turn\u0026gt;\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cp data-block-key=\"1sbwi\"\u003eOutput:\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/attention-mechanism-gemma-3-output.original.png\" alt=\"Attention mechanism in Gemma 3 : Output\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"1sbwi\"\u003eYou can also see how this attention mechanism differs from that of PaliGemma. To provide context for this comparison, PaliGemma operates by receiving one or more images along with a text-based task description (the prompt or prefix), and subsequently generates its prediction as a text string (the answer or suffix) in an autoregressive manner.\u003c/p\u003e\u003cp data-block-key=\"4c0c7\"\u003eCode:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003evisualizer\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eAttentionMaskVisualizer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;google/paligemma2-3b-mix-224\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003evisualizer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026lt;img\u0026gt; caption en\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003esuffix\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;a cat standing on a beach.\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cp data-block-key=\"1sbwi\"\u003eOutput\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/attention-mechanism-paligemma-output.original.png\" alt=\"Attention mechanism in PaliGemma : Output\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"2vadr\" id=\"new-tokenizer\"\u003eNew tokenizer\u003c/h2\u003e\u003cp data-block-key=\"bfivj\"\u003eGemma 3 has improved multilingual capabilities due to a revisited data mixture with an increased amount of multilingual data (both monolingual and parallel).\u003c/p\u003e\u003cp data-block-key=\"6cok3\"\u003eGemma 3 also introduces an improved tokenizer. The vocabulary size has been changed to 262k, but uses the same \u003ca href=\"https://github.com/google/sentencepiece\"\u003eSentencePiece\u003c/a\u003e tokenizer. To avoid errors, use the new tokenizer with Gemma 3. This is the same tokenizer as Gemini which is more balanced for non-English languages.\u003c/p\u003e\u003ch2 data-block-key=\"llufd\" id=\"\"\u003e\u003cb\u003e\u003cbr/\u003eGemma 3 27B\u003c/b\u003e\u003c/h2\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eGemma3ForConditionalGeneration\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003evision_tower\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipVisionModel\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003evision_model\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipVisionTransformer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eembeddings\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipVisionEmbeddings\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epatch_embedding\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eConv2d\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e3\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ekernel_size\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e14\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e14\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003estride\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e14\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e14\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003epadding\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003evalid\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eposition_embedding\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eEmbedding\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eencoder\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipEncoder\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayers\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eModuleList\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e26\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003e27\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e \u003cspan\u003eSiglipEncoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself_attn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipSdpaAttention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eout_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayer_norm1\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLayerNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eelementwise_affine\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipMLP\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eactivation_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003efc1\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4304\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003efc2\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4304\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayer_norm2\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLayerNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eelementwise_affine\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epost_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLayerNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eelementwise_affine\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emulti_modal_projector\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3MultiModalProjector\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emm_soft_emb_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eavg_pool\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eAvgPool2d\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ekernel_size\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003estride\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003epadding\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elanguage_model\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3ForCausalLM\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3TextModel\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eembed_tokens\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3TextScaledWordEmbedding\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e262208\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003epadding_idx\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayers\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eModuleList\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e61\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003e62\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e \u003cspan\u003eGemma3DecoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself_attn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3Attention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e128\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e128\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3MLP\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003egate_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e21504\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eup_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e21504\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edown_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e21504\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n          \u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003einput_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epost_attention_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epre_feedforward_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epost_feedforward_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003enorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb_local\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elm_head\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e5376\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e262208\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma-3-27-B-architecture.original.png\" alt=\"Gemma 3 27B architecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cblockquote data-block-key=\"1sbwi\"\u003e\u003cb\u003e\u003csup\u003eNote:\u003c/sup\u003e\u003c/b\u003e \u003csup\u003eTechnically RoPE (Rotary Positional Embedding) is inside the SDPA(Scaled Dot-Product Attention), but we simplified it in this diagram. Please see\u003c/sup\u003e \u003ca href=\"https://github.com/google-deepmind/gemma/blob/ebfa2c2bddcab1d7cc38b1dca0fe6d98e6e7df71/gemma/modules.py#L170-L182\"\u003e\u003csup\u003ethe code\u003c/sup\u003e\u003c/a\u003e\u003csup\u003e for the precise architectural details.\u003c/sup\u003e\u003c/blockquote\u003e\u003ch2 data-block-key=\"b2s72\" id=\"\"\u003e\u003cbr/\u003e\u003cb\u003eGemma 3 1B\u003c/b\u003e\u003c/h2\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eGemma3ForCausalLM\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3TextModel\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eembed_tokens\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3TextScaledWordEmbedding\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e262144\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003epadding_idx\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayers\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eModuleList\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e25\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003e26\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e \u003cspan\u003eGemma3DecoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself_attn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3Attention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1024\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1024\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3MLP\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003egate_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e6912\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eup_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e6912\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edown_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e6912\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003einput_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epost_attention_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epre_feedforward_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epost_feedforward_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003enorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RMSNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb_local\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma3RotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elm_head\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e262144\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"1sbwi\"\u003eThis text-only 1B model is specifically optimized for on-device use, making advanced AI accessible on mobile and embedded systems. This significantly impacts accessibility, privacy, and performance, as AI-powered applications can now function efficiently even with limited or no network connectivity.\u003c/p\u003e\u003ch2 data-block-key=\"w4tyr\" id=\"key-findings\"\u003e\u003cbr/\u003e\u003cb\u003eKey Findings\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"edav7\"\u003eOur \u003ca href=\"https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf\"\u003etechnical report\u003c/a\u003e provides in-depth details, but here’s a quick summary of Gemma 3’s main findings:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"2qfb7\"\u003e\u003cb\u003eSuperior Performance at same sizes:\u003c/b\u003e\u003cbr/\u003eGemma 3 models achieve superior performance compared to Gemma 2 on both pre-trained instruction-tuned versions across various benchmarks. It is the best model that fits in a single consumer GPU or TPU host. The Gemma 27B IT model ranks among the top 10 models in \u003ca href=\"https://lmarena.ai/\"\u003eLM Arena\u003c/a\u003e as of Apr 12, 2025, outperforming much larger open models and showing a significantly higher Elo score than Gemma 2.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"37j53\"\u003e\u003cb\u003eKV-cache Memory Reduction:\u003c/b\u003e\u003cbr/\u003eThe architectural changes in Gemma 3 effectively reduce the memory overhead of the KV cache during inference with long context compared to global-only attention mechanisms used in Gemma 1 and the 1:1 local/global ratio used in Gemma 2.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"bn5l7\"\u003e\u003cb\u003eEffective Long Context Handling:\u003c/b\u003e\u003cbr/\u003eGemma 3 models can generalize to 128k context length after RoPE rescaling during pre-training. We increase RoPE base frequency from 10k to 1M on global self-attention layers, and keep the frequency of the local layers at 10k.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"cpcag\"\u003e\u003cb\u003eImproved Multilinguality:\u003c/b\u003e\u003cbr/\u003eGemma 3 utilizes the same tokenizer as Gemini which is more balanced for non-English languages. We also revised the pre-training data mixture and post-training process to enhance its multilingual capabilities.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"cilue\"\u003e\u003cb\u003eImpact of Vision Encoder Resolution:\u003c/b\u003e\u003cbr/\u003eHigher resolution vision encoders lead to better performance on vision tasks. The Pan \u0026amp; Scan method further improves performance on tasks involving non-square aspect ratios, high-resolution images, and text reading in images.\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"83xs2\" id=\"summary\"\u003e\u003cbr/\u003eSummary\u003c/h2\u003e\u003cp data-block-key=\"9mkch\"\u003eWe explored the architecture of Gemma 3, highlighting the new features that set it apart from previous versions. These architectural choices allow Gemma 3 to perform better on a broader set of tasks, including improved multilingual abilities and image interaction, while also paving the way for future, more capable and resource-friendly multimodal language models suitable for standard hardware.\u003c/p\u003e\u003cp data-block-key=\"4jjcm\"\u003eWe believe Gemma 3’s innovations will empower researchers and developers to create the next generation of efficient and powerful multimodal language models.\u003c/p\u003e\u003cp data-block-key=\"bm8pq\"\u003eThanks for reading!\u003c/p\u003e\u003chr/\u003e\u003ch2 data-block-key=\"ok8y8\" id=\"\"\u003e\u003cbr/\u003eReferences\u003c/h2\u003e\u003ch3 data-block-key=\"8dpjg\" id=\"papers\"\u003e\u003cb\u003ePapers\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"2aaou\"\u003e\u003ca href=\"https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf\"\u003eGemma 3 Technical Report\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"6app6\"\u003e\u003ca href=\"https://arxiv.org/abs/2412.03555\"\u003ePaliGemma 2: A Family of Versatile VLMs for Transfer\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"71vg9\"\u003e\u003ca href=\"https://arxiv.org/abs/2303.15343\"\u003eSigmoid Loss for Language Image Pre-Training\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"803k7\"\u003e\u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"6gmta\" id=\"code-examples\"\u003e\u003cbr/\u003e\u003cb\u003eCode Examples\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"9rtt7\"\u003e\u003ca href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/core/gemma_library.ipynb\"\u003ePrompt with images and text using Gemma library\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"2pq7a\"\u003e\u003ca href=\"https://ai.google.dev/gemma/docs/meme-generator\"\u003eGemma 3 Meme Generator\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"oe62r\" id=\"the-complete-gemma-architecture-series\"\u003e\u003cbr/\u003e📋 The complete Gemma architecture series\u003c/h2\u003e\u003cul\u003e\u003cli data-block-key=\"f63p9\"\u003e\u003ca href=\"https://developers.googleblog.com/gemma-explained-overview-gemma-model-family-architectures\"\u003eGemma explained: An overview of Gemma model family architectures\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"7dnhe\"\u003e\u003ca href=\"https://developers.googleblog.com/gemma-explained-new-in-gemma-2\"\u003eGemma explained: What’s new in Gemma 2\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"1dcak\"\u003e\u003ca href=\"https://developers.googleblog.com/gemma-explained-recurrentgemma-architecture/\"\u003eGemma explained: RecurrentGemma architecture\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"7n2q9\"\u003e\u003ca href=\"https://developers.googleblog.com/gemma-explained-paligemma-architecture/\"\u003eGemma explained: PaliGemma architecture\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"7st6l\"\u003eGemma explained: What’s new in Gemma 3\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "16 min read",
  "publishedTime": "2025-04-30T00:00:00Z",
  "modifiedTime": null
}
