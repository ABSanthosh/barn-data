{
  "id": "0c9a53c2-c7d7-4330-91f1-864ae222c60b",
  "title": "What Is an AI Coding Agent?",
  "link": "https://blog.jetbrains.com/junie/2025/06/what-is-an-ai-coding-agent/",
  "description": "AI has quickly become one of the most discussed subjects globally – and it now seems to be able to do just about everything for us. Students are asking it to help them with their homework, and lawyers are even using it for case research. AI agents have grown rapidly in popularity due to the […]",
  "author": "Cheuk Ting Ho",
  "published": "Thu, 19 Jun 2025 15:46:58 +0000",
  "source": "https://blog.jetbrains.com/feed",
  "categories": [
    "news"
  ],
  "byline": "Cheuk Ting Ho",
  "length": 10369,
  "excerpt": "AI has quickly become one of the most discussed subjects globally – and it now seems to be able to do just about everything for us. Students are asking it to help them with their homework, and lawyers",
  "siteName": "The JetBrains Blog",
  "favicon": "https://blog.jetbrains.com/wp-content/uploads/2024/01/cropped-mstile-310x310-1-180x180.png",
  "text": "NewsWhat Is an AI Coding Agent? AI has quickly become one of the most discussed subjects globally – and it now seems to be able to do just about everything for us. Students are asking it to help them with their homework, and lawyers are even using it for case research. AI agents have grown rapidly in popularity due to the widespread usage of large language models (LLMs) like OpenAI’s ChatGPT. As a result, developers have also felt pressured to start using AI coding agents. In light of all this, it has become imperative for us to understand how AI coding agents work – and how we can come up with workable prompts to get the most out of AI in our software development or data science projects. At JetBrains, we have our own coding agent for JetBrains IDEs – we call it Junie. We invest a lot of effort in high performance of Junie explaining the reasoning and logic of the coding agent to make it clearer for all of you. LLMs and AI coding agents – what’s the connection? Without LLMs, AI agents as we know them would be totally different. As a rough analogy, LLMs are to AI agents what engines are to cars. Without engines, there would be no cars. However, not all machines with engines are cars. The work of an AI agent comprises several different stages: Perceiving the relevant information At this stage, the agent processes data in your project, including your code and any supporting files, together with your prompt, and sends that data to the LLM that it is using for processing. Reasoning with the LLM Communication with the LLM is usually carried out according to a specific protocol. This protocol makes processing easier by specifying a format that the agent has to adhere to when sending information and prompts to the LLM. Putting the plan into action After the LLM has processed the information, it will provide some suggested actions or generate some code. In the next step, the agent will take these instructions for various actions and perform them.  Evaluation and feedback In the last stage, there are options to perform various tests and checks to evaluate the correctness of the result and make adjustments if needed. AI coding agents aren’t the only type of AI agent that is driven by LLMs. So what makes them special? How are they tailored specifically to the needs of coders? Let’s find out! AI coding agents are designed to perform coding tasks with less user supervision. They can formulate an action plan that can potentially achieve the goal given by the user and execute them. An AI coding agent like Junie can also evaluate code and run tests to catch any errors that might crop up. Here is a simplified workflow of an AI coding agent: Set the scene for the LLMBefore the agent knows what to do, some basic information needs to be provided to the LLM in order to ensure a useful output. For example, we need to tell the model what tools are available and what format we want the action plan to be in. In terms of tools, the action plan might consist of functions that we created and that can be executed to perform a task, such as creating a file or listing a directory. Generate an action planNext, when a user’s prompt about the task is received, the agent asks the LLM to generate an action plan in the desired format. Optionally, we can also ask the LLM to act out a given “thought process”, which is the logic according to which this action plan is formulated. Once the action plan is received, it gets parsed into a format that can be followed and executed. In our example, we ask for an action plan in JSON format, and by using Python’s JSON library, we translate that action plan into a Python dictionary. Execute the action planNow that we have information about the action plan, the predefined tool functions can be executed according to the plan. The result of executing each step is noted and used in the evaluation stage. Evaluate the resultFinally, we ask the LLM to evaluate the result. In case of failure, the action plan can be updated (or a follow-up plan generated) to fix the error. Once we have an updated plan (or follow-up plan), we can attempt to execute the plan once again and evaluate the result. Keep in mind that there should be a maximum number of attempts or timeouts for this iteration, otherwise we could get stuck in an infinite loop. Converting your conversation with LLM into coding actions As an example of how an AI coding agent works, let’s build a simplified version of an AI coding agent that works with Python code. Set up your LLM model Earlier we reasoned that an AI coding agent without an LLM would be like a car without an engine. For this reason, it’s crucial that we start by securing access to an LLM model. While you can choose to use proprietary services from companies like OpenAI and Anthropic, for demonstration purposes, we’ll use an open-source model. In this example, we’ll use the DeepSeek-R1 model llamafile here on Hugging Face. Follow the instructions here to get the llamafile for the model. Why llamafile? What is it? llamafile is a single-file executable that runs locally on most computers, with no installation required. If you’re using macOS, Linux, or BSD, make sure you grant permission for your computer to execute this new file. chmod +x DeepSeek-R1-Distill-Qwen-7B-Q6_K.llamafile Now we can launch the model as a local server by running the llamafile. Make sure you pass in the flag `-ngl 999` to offload to your available GPUs. ./DeepSeek-R1-Distill-Qwen-7B-Q6_K.llamafile -ngl 999 Voilà! The model is now available on your local server. 2. Testing the LLM model server with Python code The llamafile server is also compatible with `openai` APIs. We can provide a prompt and get a result by using a Python script. For example: #!/usr/bin/env python3 from openai import OpenAI client = OpenAI(    base_url=\"http://127.0.0.1:8080/v1\", # \"http://\u003cYour api-server IP\u003e:port\"    api_key = \"sk-no-key-required\" ) completion = client.chat.completions.create(    model=\"DeepSeek-R1-Distill-Qwen-7B-Q6_K\",    messages=[        {\"role\": \"assistant\", \"content\": \"You are an AI coding agent. Your top priority is perform coding task requested by the user.\"},        {\"role\": \"user\", \"content\": \"create a Python function that returns the square of a number.\"},    ] ) content = completion.choices[0].message.content print(content) When you run this script, here’s what you might get back from the LLM: \u003cthink\u003eOkay, so the user asked me to create a Python function that returns the square of a number. Hmm…, I need to figure out how to approach this. Let me start by recalling what a function in Python looks like. I remember that functions are defined using the ‘def’ keyword, followed by the function name and parentheses for parameters.Alright, the function needs to take a number as input. Let’s name it ‘num’ for clarity. So the function definition would be something like ‘def square(num):’. Now, inside the function, I need to calculate the square of this number. Squaring a number means multiplying it by itself, so that would be ‘num * num’.Wait, is there another way to square a number in Python? Oh right, there’s the exponentiation operator ‘**’, so I could also write ‘num ** 2’. Either way works, but using multiplication is straightforward and easy to understand for someone new.Next, I need to make sure the function returns this squared value. So after calculating, I’ll use the ‘return’ statement. Putting it all together, the function should look like:def square(num):    return num * numI should test this function to ensure it works correctly. Let me try a few examples. If I pass 3, it should return 9. 3 squared is indeed 9, so that’s good. What about a negative number, like -4? Squaring it should give 16, which the function should handle correctly since multiplying two negatives gives a positive.Also, zero is a good test case. 0 squared is 0, so the function should return 0 in that case. I think these test cases cover the basic scenarios and confirm that the function is working as intended.Are there any edge cases I should consider? Maybe very large numbers, but since Python handles big integers well, that shouldn’t be a problem. What about non-integer inputs? The function will still work because multiplying floats is valid, but the user might expect an integer if they pass one. However, the function doesn’t enforce input type, so it’s up to the user to handle that as needed.Overall, the function seems simple enough and should meet the user’s requirements. I think this is a solid solution.\u003c/think\u003eTo create a Python function that returns the square of a number, you can follow these steps:1. **Define the function**: Use the `def` keyword to create a function named `square` that takes one parameter, `num`.2. **Calculate the square**: Inside the function, multiply `num` by itself (`num * num`) to get the square.3. **Return the result**: Use the `return` statement to return the calculated square.Here’s the code:“`pythondef square(num):    return num * num“`**Example usage**:“`pythonprint(square(3))  # Output: 9print(square(-4))  # Output: 16print(square(0))   # Output: 0“`This function takes a number as input and returns its square..\u003c｜end▁of▁sentence｜\u003e Wow, that’s a lot. And the “thought process” of the LLM is also there (the text within `\u003cthink\u003e` and `\u003c/think\u003e`). We need to be aware of that when handling the response from the LLM. We should also make sure the LLM responds with an action plan. Later on, we’ll see how we can prompt the LLM to do this. If your curiosity has been piqued and you want to try out a coding agent, I recommend installing Junie. It is compatible with IntelliJ IDEA, PyCharm, WebStorm, GoLand, PhpStorm, RubyMine and RustRover. To learn more about Junie, head over to its official webpage: https://www.jetbrains.com/junie/. Discover more",
  "image": "https://blog.jetbrains.com/wp-content/uploads/2025/06/JB-social-BlogFeatured-1280x720-2x-1.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n    \u003cdiv\u003e\n                        \u003ca href=\"https://blog.jetbrains.com/junie/\"\u003e\n                            \u003cimg src=\"https://blog.jetbrains.com/wp-content/uploads/2025/01/Junie.svg\" alt=\"Junie logo\"/\u003e\n                                                                                                \n                                                                                    \u003c/a\u003e\n                                            \u003c/div\u003e\n                            \u003csection data-clarity-region=\"article\"\u003e\n                \u003cdiv\u003e\n                    \t\t\t\t\u003cp\u003e\u003ca href=\"https://blog.jetbrains.com/junie/category/news/\"\u003eNews\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"major-updates\"\u003eWhat Is an AI Coding Agent?\u003c/h2\u003e                    \n                    \n\u003cp\u003eAI has quickly become one of the most discussed subjects globally – and it now seems to be able to do just about \u003cem\u003eeverything\u003c/em\u003e for us. \u003ca href=\"https://www.theguardian.com/technology/2024/feb/01/more-than-half-uk-undergraduates-ai-essays-artificial-intelligence\" target=\"_blank\" rel=\"noopener\"\u003eStudents are asking it to help them with their homework\u003c/a\u003e, and \u003ca href=\"https://www.bbc.co.uk/news/world-us-canada-65735769\" target=\"_blank\" rel=\"noopener\"\u003elawyers are even using it for case research\u003c/a\u003e. AI agents have grown rapidly in popularity due to the widespread usage of large language models (LLMs) like OpenAI’s ChatGPT. As a result, developers have also felt pressured to start using AI coding agents.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn light of all this, it has become imperative for us to understand how AI coding agents work – and how we can come up with workable prompts to get the most out of AI in our software development or data science projects. At JetBrains, we have our own coding agent for JetBrains IDEs – we call it \u003ca href=\"https://www.jetbrains.com/junie/\" target=\"_blank\" rel=\"noopener\"\u003eJunie\u003c/a\u003e. We invest a lot of effort in high performance of Junie explaining the reasoning and logic of the coding agent to make it clearer for all of you.\u003c/p\u003e\n\n\n\n\u003ch2\u003eLLMs and AI coding agents – what’s the connection?\u003c/h2\u003e\n\n\n\n\u003cp\u003eWithout LLMs, AI agents as we know them would be totally different. As a rough analogy, LLMs are to AI agents what engines are to cars. Without engines, there would be no cars. However, not all machines with engines are cars.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe work of an AI agent comprises several different stages:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003ePerceiving the relevant information\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eAt this stage, the agent processes data in your project, including your code and any supporting files, together with your prompt, and sends that data to the LLM that it is using for processing.\u003c/p\u003e\n\n\n\n\u003col start=\"2\"\u003e\n\u003cli\u003e\u003cstrong\u003eReasoning with the LLM\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eCommunication with the LLM is usually carried out according to a specific protocol. This protocol makes processing easier by specifying a format that the agent has to adhere to when sending information and prompts to the LLM.\u003c/p\u003e\n\n\n\n\u003col start=\"3\"\u003e\n\u003cli\u003e\u003cstrong\u003ePutting the plan into action\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eAfter the LLM has processed the information, it will provide some suggested actions or generate some code. In the next step, the agent will take these instructions for various actions and perform them. \u003c/p\u003e\n\n\n\n\u003col start=\"4\"\u003e\n\u003cli\u003e\u003cstrong\u003eEvaluation and feedback\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eIn the last stage, there are options to perform various tests and checks to evaluate the correctness of the result and make adjustments if needed.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" fetchpriority=\"high\" width=\"1280\" height=\"720\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/06/image-29.png\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAI coding agents aren’t the only type of AI agent that is driven by LLMs. So what makes them special? How are they tailored specifically to the needs of coders? Let’s find out!\u003c/p\u003e\n\n\n\n\u003cp\u003eAI coding agents are designed to perform coding tasks with less user supervision. They can formulate an action plan that can potentially achieve the goal given by the user and execute them. An AI coding agent like Junie can also evaluate code and run tests to catch any errors that might crop up. Here is a simplified workflow of an AI coding agent:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSet the scene for the LLM\u003c/strong\u003e\u003cbr/\u003eBefore the agent knows what to do, some basic information needs to be provided to the LLM in order to ensure a useful output. For example, we need to tell the model what tools are available and what format we want the action plan to be in. In terms of tools, the action plan might consist of functions that we created and that can be executed to perform a task, such as creating a file or listing a directory.\u003cbr/\u003e\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eGenerate an action plan\u003c/strong\u003e\u003cbr/\u003eNext, when a user’s prompt about the task is received, the agent asks the LLM to generate an action plan in the desired format. Optionally, we can also ask the LLM to act out a given “thought process”, which is the logic according to which this action plan is formulated. Once the action plan is received, it gets parsed into a format that can be followed and executed. In our example, we ask for an action plan in JSON format, and by using Python’s JSON library, we translate that action plan into a Python dictionary.\u003cbr/\u003e\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eExecute the action plan\u003c/strong\u003e\u003cbr/\u003eNow that we have information about the action plan, the predefined tool functions can be executed according to the plan. The result of executing each step is noted and used in the evaluation stage.\u003cbr/\u003e\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eEvaluate the result\u003c/strong\u003e\u003cstrong\u003e\u003cbr/\u003e\u003c/strong\u003eFinally, we ask the LLM to evaluate the result. In case of failure, the action plan can be updated (or a follow-up plan generated) to fix the error. Once we have an updated plan (or follow-up plan), we can attempt to execute the plan once again and evaluate the result. Keep in mind that there should be a maximum number of attempts or timeouts for this iteration, otherwise we could get stuck in an infinite loop.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003ch2\u003eConverting your conversation with LLM into coding actions\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs an example of how an AI coding agent works, let’s build a simplified version of an AI coding agent that works with Python code.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eSet up your LLM model\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eEarlier we reasoned that an AI coding agent without an LLM would be like a car without an engine. For this reason, it’s crucial that we start by securing access to an LLM model. While you can choose to use proprietary services from companies like OpenAI and Anthropic, for demonstration purposes, we’ll use an open-source model.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this example, we’ll use the \u003ca href=\"https://huggingface.co/Mozilla/DeepSeek-R1-Distill-Qwen-7B-llamafile\" target=\"_blank\" rel=\"noopener\"\u003eDeepSeek-R1 model llamafile here on Hugging Face\u003c/a\u003e. Follow the instructions \u003ca href=\"https://huggingface.co/Mozilla/DeepSeek-R1-Distill-Qwen-7B-llamafile#quickstart\" target=\"_blank\" rel=\"noopener\"\u003ehere\u003c/a\u003e to get the llamafile for the model.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eWhy llamafile? What is it?\u003c/strong\u003e\u003c/p\u003e\n\u003ccite\u003e\u003ca href=\"https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file\" target=\"_blank\" rel=\"noopener\"\u003ellamafile\u003c/a\u003e is a single-file executable that runs locally on most computers, with no installation required.\u003c/cite\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eIf you’re using macOS, Linux, or BSD, make sure you grant permission for your computer to execute this new file.\u003c/p\u003e\n\n\n\n\u003cpre data-enlighter-language=\"bash\" data-enlighter-theme=\"\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\"\u003echmod +x DeepSeek-R1-Distill-Qwen-7B-Q6_K.llamafile\n\u003c/pre\u003e\n\n\n\n\u003cp\u003eNow we can launch the model as a local server by running the llamafile. Make sure you pass in the flag `-ngl 999` to offload to your available GPUs.\u003c/p\u003e\n\n\n\n\u003cpre data-enlighter-language=\"bash\" data-enlighter-theme=\"\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\"\u003e./DeepSeek-R1-Distill-Qwen-7B-Q6_K.llamafile -ngl 999\u003c/pre\u003e\n\n\n\n\u003cp\u003eVoilà! The model is now available on your local server.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1600\" height=\"431\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/06/image-28.png\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e2. Testing the LLM model server with Python code\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe llamafile server is also compatible with `openai` APIs. We can provide a prompt and get a result by using a Python script. For example:\u003c/p\u003e\n\n\n\n\u003cpre data-enlighter-language=\"python\" data-enlighter-theme=\"\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\"\u003e#!/usr/bin/env python3\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n\n   base_url=\u0026#34;http://127.0.0.1:8080/v1\u0026#34;, # \u0026#34;http://\u0026lt;Your api-server IP\u0026gt;:port\u0026#34;\n\n   api_key = \u0026#34;sk-no-key-required\u0026#34;\n\n)\n\ncompletion = client.chat.completions.create(\n\n   model=\u0026#34;DeepSeek-R1-Distill-Qwen-7B-Q6_K\u0026#34;,\n\n   messages=[\n\n       {\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are an AI coding agent. Your top priority is perform coding task requested by the user.\u0026#34;},\n\n       {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;create a Python function that returns the square of a number.\u0026#34;},\n\n   ]\n\n)\n\ncontent = completion.choices[0].message.content\n\nprint(content)\u003c/pre\u003e\n\n\n\n\u003cp\u003eWhen you run this script, here’s what you might get back from the LLM:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;think\u0026gt;\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eOkay, so the user asked me to create a Python function that returns the square of a number. Hmm…, I need to figure out how to approach this. Let me start by recalling what a function in Python looks like. I remember that functions are defined using the ‘def’ keyword, followed by the function name and parentheses for parameters.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eAlright, the function needs to take a number as input. Let’s name it ‘num’ for clarity. So the function definition would be something like ‘def square(num):’. Now, inside the function, I need to calculate the square of this number. Squaring a number means multiplying it by itself, so that would be ‘num * num’.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eWait, is there another way to square a number in Python? Oh right, there’s the exponentiation operator ‘**’, so I could also write ‘num ** 2’. Either way works, but using multiplication is straightforward and easy to understand for someone new.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eNext, I need to make sure the function returns this squared value. So after calculating, I’ll use the ‘return’ statement. Putting it all together, the function should look like:\u003c/em\u003e\u003cbr/\u003e\u003cem\u003edef square(num):\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e    return num * num\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eI should test this function to ensure it works correctly. Let me try a few examples. If I pass 3, it should return 9. 3 squared is indeed 9, so that’s good. What about a negative number, like -4? Squaring it should give 16, which the function should handle correctly since multiplying two negatives gives a positive.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eAlso, zero is a good test case. 0 squared is 0, so the function should return 0 in that case. I think these test cases cover the basic scenarios and confirm that the function is working as intended.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eAre there any edge cases I should consider? Maybe very large numbers, but since Python handles big integers well, that shouldn’t be a problem. What about non-integer inputs? The function will still work because multiplying floats is valid, but the user might expect an integer if they pass one. However, the function doesn’t enforce input type, so it’s up to the user to handle that as needed.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eOverall, the function seems simple enough and should meet the user’s requirements. I think this is a solid solution.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e\u0026lt;/think\u0026gt;\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eTo create a Python function that returns the square of a number, you can follow these steps:\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e1. **Define the function**: Use the `def` keyword to create a function named `square` that takes one parameter, `num`.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e2. **Calculate the square**: Inside the function, multiply `num` by itself (`num * num`) to get the square.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e3. **Return the result**: Use the `return` statement to return the calculated square.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eHere’s the code:\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e“`python\u003c/em\u003e\u003cbr/\u003e\u003cem\u003edef square(num):\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e    return num * num\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e“`\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e**Example usage**:\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e“`python\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eprint(square(3))  # Output: 9\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eprint(square(-4))  # Output: 16\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eprint(square(0))   # Output: 0\u003c/em\u003e\u003cbr/\u003e\u003cem\u003e“`\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eThis function takes a number as input and returns its square..\u0026lt;｜end▁of▁sentence｜\u0026gt;\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWow, that’s a lot. And the “thought process” of the LLM is also there (the text within `\u0026lt;think\u0026gt;` and `\u0026lt;/think\u0026gt;`). We need to be aware of that when handling the response from the LLM. We should also make sure the LLM responds with an action plan. Later on, we’ll see how we can prompt the LLM to do this.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf your curiosity has been piqued and you want to try out a coding agent, I recommend installing Junie. It is compatible with IntelliJ IDEA, PyCharm, WebStorm, GoLand, PhpStorm, RubyMine and RustRover. To learn more about Junie, head over to its official webpage: \u003ca href=\"https://www.jetbrains.com/junie/\" target=\"_blank\" rel=\"noopener\"\u003ehttps://www.jetbrains.com/junie/\u003c/a\u003e.\u003c/p\u003e\n                    \n                                                                                                                                                                                                                                    \u003c/div\u003e\n                \u003ca href=\"#\"\u003e\u003c/a\u003e\n                \n                \n            \u003c/section\u003e\n                    \u003cdiv\u003e\n                \u003cp\u003e\n                    \u003ch2\u003eDiscover more\u003c/h2\u003e\n                \u003c/p\u003e\n                \n            \u003c/div\u003e\n                \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": null,
  "modifiedTime": null
}
