{
  "id": "00a44319-41f9-438a-8ac8-485611a1dcc0",
  "title": "Accelerating GPU indexes in Faiss with NVIDIA cuVS",
  "link": "https://engineering.fb.com/2025/05/08/data-infrastructure/accelerating-gpu-indexes-in-faiss-with-nvidia-cuvs/",
  "description": "Meta and NVIDIA collaborated to accelerate vector search on GPUs by integrating NVIDIA cuVS into Faiss v1.10, Meta’s open source library for similarity search. This new implementation of cuVS will be more performant than classic GPU-accelerated search in some areas. For inverted file (IVF) indexing, NVIDIA cuVS outperforms classical GPU-accelerated IVF build times by up [...] Read More... The post Accelerating GPU indexes in Faiss with NVIDIA cuVS appeared first on Engineering at Meta.",
  "author": "",
  "published": "Thu, 08 May 2025 17:00:22 +0000",
  "source": "https://engineering.fb.com/feed/",
  "categories": [
    "AI Research",
    "Data Infrastructure",
    "ML Applications",
    "Open Source"
  ],
  "byline": "By Junjie Qi, Gergely Szilvasy, Michael Norris, Vishal Gandhi",
  "length": 3934,
  "excerpt": "Meta and NVIDIA collaborated to accelerate vector search on GPUs by integrating NVIDIA cuVS into Faiss v1.10, Meta’s open source library for similarity search. This new implementation of cuVS will …",
  "siteName": "Engineering at Meta",
  "favicon": "",
  "text": "Meta and NVIDIA collaborated to accelerate vector search on GPUs by integrating NVIDIA cuVS into Faiss v1.10, Meta’s open source library for similarity search. This new implementation of cuVS will be more performant than classic GPU-accelerated search in some areas. For inverted file (IVF) indexing, NVIDIA cuVS outperforms classical GPU-accelerated IVF build times by up to 4.7x; and search latency is reduced by as much as 8.1x. For graph indexing, CUDA ANN Graph (CAGRA) outperforms CPU Hierarchical Navigable Small World graphs (HNSW) build times by up to 12.3x; and search latency is reduced by as much as 4.7x. The Faiss library The Faiss library is an open source library, developed by Meta FAIR, for efficient vector search and clustering of dense vectors. Faiss pioneered vector search on GPUs, as well as the ability to seamlessly switch between GPUs and CPUs. It has made a lasting impact in both research and industry, being used as an integrated library in several databases (e.g., Milvus and OpenSearch), machine learning libraries, data processing libraries, and AI workflows. Faiss is also used heavily by researchers and data scientists as a standalone library, often paired with PyTorch.  Collaboration with NVIDIA Three years ago, Meta and NVIDIA worked together to enhance the capabilities of vector search technology and to accelerate vector search on GPUs. Previously, in 2016, Meta had incorporated high performing vector search algorithms made for NVIDIA GPUs: GpuIndexFlat; GpuIndexIVFFlat; GpuIndexIVFPQ. After the partnership, NVIDIA rapidly contributed GpuIndexCagra, a state-of-the art graph-based index designed specifically for GPUs. In its latest release, Faiss 1.10.0 officially includes these algorithms from the NVIDIA cuVS library.  Faiss 1.10.0 also includes a new conda package that unlocks the ability to choose between the classic Faiss GPU implementations and the newer NVIDIA cuVS algorithms, making it easy for users to switch between GPU and CPU. Benchmarking The following benchmarks were conducted using the cuVS-bench tool.  We measured: A tall, slender image dataset: A subset of 100 million vectors from the Deep1B dataset by 96 dimensions. A short, wide dataset of text embeddings: 5 million vector embeddings, curated using the OpenAI text-embedding-ada-002 model. Tests for index build times and search latency were conducted on an NVIDIA H100 GPU and compared to an Intel Xeon Platinum 8480CL system. Results are reported in the tables below at 95% recall along the pareto frontiers for k=10 nearest neighbors.  Build time (95% recall@10) Index Embeddings 100M x 96 (seconds) Embeddings 5M x 1536 (seconds) Faiss Classic Faiss cuVS Faiss Classic   Faiss cuVS Faiss Classic Faiss cuVS IVF Flat IVF Flat 101.4 37.9 (2.7x) 24.4 15.2 (1.6x) IVF PQ IVF PQ 168.2 72.7 (2.3x) 42.0 9.0 (4.7x) HNSW (CPU) CAGRA 3322.1 518.5 (6.4x) 1106.1 89.7 (12.3x) Table 1: Index build times for Faiss-classic and Faiss-cuVS in seconds (with NVIDIA cuVS speedups in parentheses). Search latency (95% recall@10) Index Embeddings 100M x 96 (milliseconds) Embeddings 5M x 1536 (milliseconds) Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS IVF Flat IVF Flat 0.75 0.39 (1.9x) 1.98 1.14 (1.7x) IVF PQ IVF PQ 0.49 0.17 (2.9x) 1.78 0.22 (8.1x) HNSW (CPU) CAGRA 0.56 0.23 (2.4x) 0.71 0.15 (4.7x) Table 2: Online (i.e., one at a time) search query latency for Faiss-classic and Faiss-cuVS in milliseconds (with NVIDIA cuVS speedups in parentheses). Looking forward The emergence of state-of-the-art NVIDIA GPUs has revolutionized the field of vector search, enabling high recall and lightning-fast search speeds. The integration of Faiss and cuVS will continue to incorporate state-of-the-art algorithms, and we look forward to unlocking new innovations in this partnership between Meta and NVIDIA.  Read here for more details about NVIDIA cuVS.",
  "image": "https://engineering.fb.com/wp-content/uploads/2017/03/faiss_logo.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eMeta and NVIDIA collaborated to accelerate vector search on GPUs by integrating\u003c/span\u003e \u003ca href=\"https://github.com/rapidsai/cuvs\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eNVIDIA cuVS\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e into\u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/faiss/releases/tag/v1.10.0\" target=\"_blank\" rel=\"noopener\"\u003e \u003cspan\u003eFaiss v1.10\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, Meta’s open source library for similarity search.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThis new implementation of cuVS will be more performant than classic GPU-accelerated search in some areas.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eFor inverted file (IVF) indexing, NVIDIA cuVS outperforms classical GPU-accelerated IVF build times by up to 4.7x; and search latency is reduced by as much as 8.1x.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eFor graph indexing, CUDA ANN Graph (CAGRA) outperforms CPU Hierarchical Navigable Small World graphs (HNSW) build times by up to 12.3x; and search latency is reduced by as much as 4.7x.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003eThe Faiss library\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe\u003c/span\u003e \u003ca href=\"https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eFaiss library\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e is an open source library, developed by Meta FAIR, for efficient vector search and clustering of dense vectors. Faiss pioneered vector search on GPUs, as well as the ability to seamlessly switch between GPUs and CPUs. It has made a lasting impact in both research and industry, being used as an integrated library in several databases (e.g., Milvus and OpenSearch), machine learning libraries, data processing libraries, and AI workflows. Faiss is also used heavily by researchers and data scientists as a standalone library, often\u003c/span\u003e \u003ca href=\"https://github.com/facebookresearch/faiss/pull/1484\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003epaired with PyTorch\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eCollaboration with NVIDIA\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThree years ago, Meta and NVIDIA worked together to enhance the capabilities of vector search technology and to accelerate vector search on GPUs. Previously, in 2016, Meta had incorporated high performing vector search algorithms made for NVIDIA GPUs: \u003c/span\u003e\u003cspan\u003eGpuIndexFlat\u003c/span\u003e\u003cspan\u003e; \u003c/span\u003e\u003cspan\u003eGpuIndexIVFFlat\u003c/span\u003e\u003cspan\u003e; \u003c/span\u003e\u003cspan\u003eGpuIndexIVFPQ\u003c/span\u003e\u003cspan\u003e. After the partnership, NVIDIA rapidly contributed\u003c/span\u003e \u003ca href=\"https://arxiv.org/abs/2308.15136\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eGpuIndexCagra\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, a state-of-the art graph-based index designed specifically for GPUs. In its latest release, \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/faiss/releases/tag/v1.10.0\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eFaiss 1.10.0\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e officially includes these algorithms from the \u003c/span\u003e\u003ca href=\"https://github.com/rapidsai/cuvs\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eNVIDIA cuVS library\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFaiss 1.10.0 also includes a \u003c/span\u003e\u003ca href=\"https://anaconda.org/pytorch/faiss-gpu-cuvs\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003enew conda package\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e that unlocks the ability to choose between the classic Faiss GPU implementations and the newer \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/faiss/wiki/GPU-Faiss-with-cuVS-usage\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eNVIDIA cuVS algorithms\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, making it easy for users to switch between GPU and CPU.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eBenchmarking\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe following benchmarks were conducted using the \u003c/span\u003e\u003ca href=\"https://docs.rapids.ai/api/cuvs/nightly/cuvs_bench/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003ecuVS-bench\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e tool. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe measured:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA tall, slender image dataset: A subset of 100 million vectors from the \u003c/span\u003e\u003ca href=\"https://research.yandex.com/blog/benchmarks-for-billion-scale-similarity-search\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eDeep1B\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e dataset by 96 dimensions.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA short, wide dataset of text embeddings: \u003c/span\u003e\u003ca href=\"https://github.com/zilliztech/VectorDBBench?tab=readme-ov-file#benchmark-cases\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003e5 million vector embeddings,\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e curated using the \u003c/span\u003e\u003ca href=\"https://openai.com/index/new-and-improved-embedding-model/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eOpenAI text-embedding-ada-002 model\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eTests for index build times and search latency were conducted on an \u003c/span\u003e\u003ca href=\"https://www.nvidia.com/en-us/data-center/h100/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eNVIDIA H100 GPU\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and compared to an Intel Xeon Platinum 8480CL system. Results are reported in the tables below at 95% recall along the\u003c/span\u003e\u003ca href=\"https://docs.rapids.ai/api/cuvs/nightly/comparing_indexes/\" target=\"_blank\" rel=\"noopener\"\u003e \u003cspan\u003epareto frontiers\u003c/span\u003e\u003cspan\u003e for k=10 nearest neighbors. \u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eBuild time (95% recall@10)\u003c/span\u003e\u003c/h2\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd colspan=\"2\"\u003e\u003cb\u003eIndex\u003c/b\u003e\u003c/td\u003e\n\u003ctd colspan=\"2\"\u003e\n\u003cp\u003e\u003cb\u003eEmbeddings\u003cbr/\u003e\n\u003c/b\u003e\u003cb\u003e100M x 96\u003cbr/\u003e\n\u003c/b\u003e\u003cb\u003e(seconds)\u003c/b\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd colspan=\"2\"\u003e\n\u003cp\u003e\u003cb\u003eEmbeddings\u003cbr/\u003e\n\u003c/b\u003e\u003cb\u003e5M x 1536\u003cbr/\u003e\n\u003c/b\u003e\u003cb\u003e(seconds)\u003c/b\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cb\u003eFaiss Classic\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003eFaiss cuVS\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003eFaiss Classic\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e   Faiss cuVS\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003eFaiss Classic\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003eFaiss cuVS\u003c/b\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eIVF Flat\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003eIVF Flat\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e101.4\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e37.9 \u003c/b\u003e\u003cspan\u003e(2.7x)\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e24.4\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e15.2\u003c/b\u003e\u003cspan\u003e (1.6x)\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eIVF PQ\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003eIVF PQ\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e168.2\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e72.7\u003c/b\u003e\u003cspan\u003e (2.3x)\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e42.0\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e9.0\u003c/b\u003e\u003cspan\u003e (4.7x)\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eHNSW (CPU)\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003eCAGRA\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e3322.1\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e518.5\u003c/b\u003e\u003cspan\u003e (6.4x)\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e1106.1\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e89.7\u003c/b\u003e\u003cspan\u003e (12.3x)\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003eTable 1: Index build times for Faiss-classic and Faiss-cuVS in seconds (with NVIDIA cuVS speedups in parentheses).\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eSearch latency (95% recall@10)\u003c/span\u003e\u003c/h3\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd colspan=\"2\"\u003e\u003cb\u003eIndex\u003c/b\u003e\u003c/td\u003e\n\u003ctd colspan=\"2\"\u003e\n\u003cp\u003e\u003cb\u003eEmbeddings\u003cbr/\u003e\n\u003c/b\u003e\u003cb\u003e100M x 96\u003cbr/\u003e\n\u003c/b\u003e\u003cb\u003e(milliseconds)\u003c/b\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd colspan=\"2\"\u003e\n\u003cp\u003e\u003cb\u003eEmbeddings\u003cbr/\u003e\n\u003c/b\u003e\u003cb\u003e5M x 1536\u003cbr/\u003e\n\u003c/b\u003e\u003cb\u003e(milliseconds)\u003c/b\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cb\u003eFaiss Classic\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003eFaiss cuVS\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003eFaiss Classic\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003eFaiss cuVS\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003eFaiss Classic\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003eFaiss cuVS\u003c/b\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eIVF Flat\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003eIVF Flat\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e0.75\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e0.39 \u003c/b\u003e\u003cspan\u003e(1.9x)\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e1.98\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e1.14\u003c/b\u003e\u003cspan\u003e (1.7x)\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eIVF PQ\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003eIVF PQ\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e0.49\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e0.17\u003c/b\u003e\u003cspan\u003e (2.9x)\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e1.78\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e0.22\u003c/b\u003e\u003cspan\u003e (8.1x)\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cspan\u003eHNSW (CPU)\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003eCAGRA\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e0.56\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e0.23\u003c/b\u003e\u003cspan\u003e (2.4x)\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003e0.71\u003c/span\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003e0.15\u003c/b\u003e\u003cspan\u003e (4.7x)\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003eTable 2: Online (i.e., one at a time) search query latency for Faiss-classic and Faiss-cuVS in milliseconds (with NVIDIA cuVS speedups in parentheses).\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eLooking forward\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe emergence of state-of-the-art NVIDIA GPUs has revolutionized the field of vector search, enabling high recall and lightning-fast search speeds. The integration of Faiss and cuVS will continue to incorporate state-of-the-art algorithms, and we look forward to unlocking new innovations in this partnership between Meta and NVIDIA. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eRead here for \u003c/span\u003e\u003ca href=\"https://developer.nvidia.com/cuvs\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003emore details about NVIDIA cuVS\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-05-08T17:00:22Z",
  "modifiedTime": "2025-05-08T16:25:58Z"
}
