{
  "id": "6bf869f3-15e3-4a35-890d-05c4938f1496",
  "title": "Meta Introduces V-JEPA 2, a Video-Based World Model for Physical Reasoning",
  "link": "https://www.infoq.com/news/2025/06/meta-vjepa2/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Meta has introduced V-JEPA 2, a new video-based world model designed to improve machine understanding, prediction, and planning in physical environments. The model extends the Joint Embedding Predictive Architecture (JEPA) framework and is trained to predict outcomes in embedding space using video data. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Fri, 13 Jun 2025 18:20:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "AGI",
    "Benchmark",
    "Large language models",
    "Artificial Intelligence",
    "Robotics",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 2817,
  "excerpt": "Meta has introduced V-JEPA 2, a new video-based world model designed to improve machine understanding, prediction, and planning in physical environments. The model extends the Joint Embedding Predicti",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250605075448/apple-touch-icon.png",
  "text": "Meta has introduced V-JEPA 2, a new video-based world model designed to improve machine understanding, prediction, and planning in physical environments. The model extends the Joint Embedding Predictive Architecture (JEPA) framework and is trained to predict outcomes in embedding space using video data. The model is trained in two phases. In the first, over one million hours of video and one million images are used for self-supervised pretraining without any action labels. This enables the model to learn representations of motion, object dynamics, and interaction patterns. In the second phase, it is fine-tuned on 62 hours of robot data that includes both video and action sequences. This stage allows the model to make action-conditioned predictions and support planning. One Reddit user commented on the approach: Predicting in embedding space is going to be more compute efficient, and also it is closer to how humans reason… Really feeling the AGI with this approach, regardless of the current results using the system. Others have noted the limits of the approach. Dorian Harris, who focuses on AI strategy and education, wrote: AGI requires broader capabilities than V-JEPA 2’s specialised focus. It is a significant yet narrow breakthrough, and the AGI milestone is overstated. In robotic applications, V-JEPA 2 is used for short- and long-horizon manipulation tasks. For example, when given a goal in the form of an image, the robot uses the model to simulate possible actions and select those that move it closer to the goal. The system replans at each step, using a model-predictive control loop. Meta reports task success rates between 65% and 80% for pick-and-place tasks involving novel objects and settings. The model has also been evaluated on benchmarks such as Something-Something v2, Epic-Kitchens-100, and Perception Test. When used with lightweight readouts, it performs competitively on tasks related to motion recognition and future action prediction. Meta is also releasing three new benchmarks focused on physical reasoning from video: IntPhys 2, which tests for recognition of physically implausible events; MVPBench, which assesses video-question answering under minimal changes; and CausalVQA, which focuses on cause-effect reasoning and planning. David Eberle, CEO of Typewise, noted: The ability to anticipate and adapt to dynamic situations is exactly what is needed to make AI agents more context-aware in real-world customer interactions, too, not just in robotics. Model weights, code, and datasets are available via GitHub and Hugging Face. A leaderboard has been launched for community benchmarking. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2025/06/meta-vjepa2/en/headerimage/generatedHeaderImage-1749837833257.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eMeta has introduced \u003ca href=\"https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/\"\u003eV-JEPA 2\u003c/a\u003e, a new video-based world model designed to improve machine understanding, prediction, and planning in physical environments. The model extends the \u003ca href=\"https://ai.meta.com/blog/yann-lecun-advances-in-ai-research/\"\u003eJoint Embedding Predictive Architecture (JEPA)\u003c/a\u003e framework and is trained to predict outcomes in embedding space using video data.\u003c/p\u003e\n\n\u003cp\u003eThe model is trained in two phases. In the first, over one million hours of video and one million images are used for self-supervised pretraining without any action labels. This enables the model to learn representations of motion, object dynamics, and interaction patterns. In the second phase, it is fine-tuned on 62 hours of robot data that includes both video and action sequences. This stage allows the model to make action-conditioned predictions and support planning.\u003c/p\u003e\n\n\u003cp\u003eOne Reddit user \u003ca href=\"https://www.reddit.com/r/singularity/comments/1l8wf1r/comment/mx8dprp/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button\"\u003ecommented\u003c/a\u003e on the approach:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003ePredicting in embedding space is going to be more compute efficient, and also it is closer to how humans reason… Really feeling the AGI with this approach, regardless of the current results using the system.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eOthers have noted the limits of the approach. Dorian Harris, who focuses on AI strategy and education, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:ugcPost:7339169644315820033?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7339169644315820033%2C7339187939639275521%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287339187939639275521%2Curn%3Ali%3AugcPost%3A7339169644315820033%29\"\u003ewrote\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAGI requires broader capabilities than V-JEPA 2’s specialised focus. It is a significant yet narrow breakthrough, and the AGI milestone is overstated.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIn robotic applications, V-JEPA 2 is used for short- and long-horizon manipulation tasks. For example, when given a goal in the form of an image, the robot uses the model to simulate possible actions and select those that move it closer to the goal. The system replans at each step, using a model-predictive control loop. Meta reports task success rates between 65% and 80% for pick-and-place tasks involving novel objects and settings.\u003c/p\u003e\n\n\u003cp\u003eThe model has also been evaluated on benchmarks such as \u003ca href=\"https://paperswithcode.com/sota/action-recognition-in-videos-on-something\"\u003eSomething-Something v2\u003c/a\u003e, \u003ca href=\"https://paperswithcode.com/dataset/epic-kitchens-100\"\u003eEpic-Kitchens-100\u003c/a\u003e, and \u003ca href=\"https://arxiv.org/abs/2305.13786\"\u003ePerception Test\u003c/a\u003e. When used with lightweight readouts, it performs competitively on tasks related to motion recognition and future action prediction.\u003c/p\u003e\n\n\u003cp\u003eMeta is also releasing three new benchmarks focused on physical reasoning from video: \u003ca href=\"https://github.com/facebookresearch/IntPhys2\"\u003eIntPhys 2\u003c/a\u003e, which tests for recognition of physically implausible events; \u003ca href=\"https://github.com/facebookresearch/minimal_video_pairs\"\u003eMVPBench\u003c/a\u003e, which assesses video-question answering under minimal changes; and \u003ca href=\"https://github.com/facebookresearch/CausalVQA\"\u003eCausalVQA\u003c/a\u003e, which focuses on cause-effect reasoning and planning.\u003c/p\u003e\n\n\u003cp\u003eDavid Eberle, CEO of Typewise, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:ugcPost:7339169644315820033?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7339169644315820033%2C7339318138678796288%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287339318138678796288%2Curn%3Ali%3AugcPost%3A7339169644315820033%29\"\u003enoted\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe ability to anticipate and adapt to dynamic situations is exactly what is needed to make AI agents more context-aware in real-world customer interactions, too, not just in robotics.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eModel weights, code, and datasets are available via \u003ca href=\"https://github.com/facebookresearch/IntPhys2\"\u003eGitHub\u003c/a\u003e and \u003ca href=\"https://huggingface.co/datasets/facebook/IntPhys2\"\u003eHugging Face\u003c/a\u003e. A \u003ca href=\"https://huggingface.co/spaces/facebook/physical_reasoning_leaderboard\"\u003eleaderboard\u003c/a\u003e has been launched for community benchmarking.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-06-13T00:00:00Z",
  "modifiedTime": null
}
