{
  "id": "d8aca095-2aac-4a47-9f97-864638f6a40f",
  "title": "Nexa AI Unveils Omnivision: A Compact Vision-Language Model for Edge AI",
  "link": "https://www.infoq.com/news/2024/12/nexa-ai-unveils-omnivision/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Nexa AI unveiled Omnivision, a compact vision-language model tailored for edge devices. By significantly reducing image tokens from 729 to 81, Omnivision lowers latency and computational requirements while maintaining strong performance in tasks like visual question answering and image captioning. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Tue, 03 Dec 2024 19:55:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Benchmark",
    "Computer Vision",
    "Artificial Intelligence",
    "Machine Learning",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 3142,
  "excerpt": "Nexa AI unveiled Omnivision, a compact vision-language model tailored for edge devices. By significantly reducing image tokens from 729 to 81, Omnivision lowers latency and computational requirements",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241203101948/apple-touch-icon.png",
  "text": "Nexa AI unveiled Omnivision, a compact vision-language model tailored for edge devices. By significantly reducing image tokens from 729 to 81, Omnivision lowers latency and computational requirements while maintaining strong performance in tasks like visual question answering and image captioning. The model’s architecture integrates a Qwen-2.5-0.5B language backbone, a SigLIP-400M vision encoder, and an optimized projection layer to ensure seamless processing of multimodal inputs. Omnivision’s architecture is designed for efficient multimodal processing, featuring three core components. The Qwen-2.5-0.5B model acts as the backbone for processing text inputs, while the SigLIP-400M vision encoder generates image embeddings from input images. This encoder operates at a resolution of 384 with a 14×14 patch size, optimizing visual data extraction. A projection layer then aligns the image embeddings with the token space of the language model using a Multi-Layer Perceptron (MLP), which allows for streamlined visual-language integration.  Source: Nexa AI Blog A key innovation is its 9x reduction in image tokens, reducing processing requirements without compromising accuracy. For example, Omnivision can generate captions for high-resolution images in under two seconds on a MacBook M4 Pro, requiring less than 1 GB of RAM. To ensure accuracy and reliability, it uses Direct Preference Optimization (DPO), leveraging high-quality datasets to minimize hallucinations and enhance prediction trustworthiness. The model's training pipeline is structured in three distinct stages. The pretraining phase focuses on aligning visual and textual inputs to establish foundational capabilities. Supervised fine-tuning follows, enhancing the model's ability to interpret context and generate relevant responses. Finally, Direct Preference Optimization (DPO) refines decision-making by minimizing inaccuracies and improving precision in context-specific outputs. Omnivision has outperformed its predecessor, nanoLLAVA, in benchmark evaluations across datasets like ScienceQA, MM-VET, and POPE. It achieved notable improvements, including a 71.0% accuracy rate on ScienceQA test data and 93.3% accuracy on the POPE benchmark, demonstrating its reliability in complex reasoning tasks. Source: Nexa AI Blog Currently, Omnivision is focused on visual question answering and image captioning. However, Nexa AI revealed plans to expand the model’s capabilities to support optical character recognition (OCR). In a recent Reddit discussion, AzLy shared:  Currently, OCR is not one of this model's intended uses. It is mainly for visual question answering and image captioning. However, supporting better OCR is our next step. Omnivision can be deployed locally using the Nexa-SDK, an open-source framework that supports a wide range of multimodal tasks. The model is still in early development, and the team is actively gathering feedback from users to guide future improvements. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2024/12/nexa-ai-unveils-omnivision/en/headerimage/generatedHeaderImage-1733255172883.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://nexa.ai/blogs/omni-vision\"\u003eNexa AI unveiled Omnivision\u003c/a\u003e, a compact vision-language model tailored for edge devices. By significantly reducing image tokens from 729 to 81, Omnivision lowers latency and computational requirements while maintaining strong performance in tasks like visual question answering and image captioning. The model’s architecture integrates a Qwen-2.5-0.5B language backbone, a SigLIP-400M vision encoder, and an optimized projection layer to ensure seamless processing of multimodal inputs.\u003c/p\u003e\n\n\u003cp\u003eOmnivision’s architecture is designed for efficient multimodal processing, featuring three core components. The Qwen-2.5-0.5B model acts as the backbone for processing text inputs, while the SigLIP-400M vision encoder generates image embeddings from input images. This encoder operates at a resolution of 384 with a 14×14 patch size, optimizing visual data extraction. A projection layer then aligns the image embeddings with the token space of the language model using a Multi-Layer Perceptron (MLP), which allows for streamlined visual-language integration. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/12/nexa-ai-unveils-omnivision/en/resources/1Screenshot 2024-12-03 203758-1733255172022.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/12/nexa-ai-unveils-omnivision/en/resources/1Screenshot 2024-12-03 203758-1733255172022.png\" rel=\"share\"/\u003e\u003cbr/\u003e\n\u003cem\u003eSource: Nexa AI Blog\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eA key innovation is its 9x reduction in image tokens, reducing processing requirements without compromising accuracy. For example, Omnivision can generate captions for high-resolution images in under two seconds on a MacBook M4 Pro, requiring less than 1 GB of RAM. To ensure accuracy and reliability, it uses Direct Preference Optimization (DPO), leveraging high-quality datasets to minimize hallucinations and enhance prediction trustworthiness.\u003c/p\u003e\n\n\u003cp\u003eThe model\u0026#39;s training pipeline is structured in three distinct stages. The pretraining phase focuses on aligning visual and textual inputs to establish foundational capabilities. Supervised fine-tuning follows, enhancing the model\u0026#39;s ability to interpret context and generate relevant responses. Finally, Direct Preference Optimization (DPO) refines decision-making by minimizing inaccuracies and improving precision in context-specific outputs.\u003c/p\u003e\n\n\u003cp\u003eOmnivision has outperformed its predecessor, nanoLLAVA, in benchmark evaluations across datasets like ScienceQA, MM-VET, and POPE. It achieved notable improvements, including a 71.0% accuracy rate on ScienceQA test data and 93.3% accuracy on the POPE benchmark, demonstrating its reliability in complex reasoning tasks.\u003c/p\u003e\n\n\u003cp\u003e\u003cmeta charset=\"utf-8\"/\u003e\u003cb id=\"docs-internal-guid-5bd8adc0-7fff-ac9a-de43-0a70697baa2b\"\u003e\u003cimg height=\"412\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfezqum4-KqOSjkitA8IPGf5vavFkxJrn20XhALCDklETTm1brmu5KNz72uFpyF23s8f3Bf9pEgGunQlFjL1gU3gTaPNKNBBxWiE3-kljkrlntSS2_j0lPg8THLBqeONFcm7vwxsg?key=uxDH3nv7qW2E6SrhQLpU4MZT\" width=\"546\" rel=\"share\"/\u003e\u003c/b\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eSource: Nexa AI Blog\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eCurrently, Omnivision is focused on visual question answering and image captioning. However, Nexa AI revealed plans to expand the model’s capabilities to support optical character recognition (OCR). In a recent Reddit discussion, AzLy \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1grkq4j/comment/lx80v34/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button\"\u003eshared\u003c/a\u003e: \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eCurrently, OCR is not one of this model\u0026#39;s intended uses. It is mainly for visual question answering and image captioning. However, supporting better OCR is our next step.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eOmnivision can be deployed locally using the \u003ca href=\"https://github.com/NexaAI/nexa-sdk?tab=readme-ov-file#install-option-1-executable-installer\"\u003eNexa-SDK\u003c/a\u003e, an open-source framework that supports a wide range of multimodal tasks. The model is still in early development, and the team is actively gathering feedback from users to guide future improvements.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-12-03T00:00:00Z",
  "modifiedTime": null
}
