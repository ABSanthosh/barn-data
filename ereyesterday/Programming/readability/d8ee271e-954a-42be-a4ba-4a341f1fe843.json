{
  "id": "d8ee271e-954a-42be-a4ba-4a341f1fe843",
  "title": "HuatuoGPT-o1: Advancing Complex Medical Reasoning with AI",
  "link": "https://www.infoq.com/news/2025/01/huatuogpt-o1-ai/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Researchers from The Chinese University of Hong Kong, Shenzhen, and the Shenzhen Research Institute of Big Data have introduced HuatuoGPT-o1, a medical large language model (LLM) designed to improve reasoning in complex healthcare scenarios. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Tue, 14 Jan 2025 18:55:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Machine Learning",
    "Reinforcement Learning",
    "Benchmark",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 3821,
  "excerpt": "Researchers from The Chinese University of Hong Kong, Shenzhen, and the Shenzhen Research Institute of Big Data have introduced HuatuoGPT-o1, a medical large language model (LLM) designed to improve r",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250109115548/apple-touch-icon.png",
  "text": "Researchers from The Chinese University of Hong Kong, Shenzhen, and the Shenzhen Research Institute of Big Data have introduced HuatuoGPT-o1, a medical large language model (LLM) designed to improve reasoning in complex healthcare scenarios. Developed using a novel two-stage training process, the model aims to refine responses through step-by-step analysis, resembling the diagnostic approaches used by medical professionals. The development of HuatuoGPT-o1 followed a structured two-step approach designed to cultivate critical thinking and iterative refinement in the model's reasoning process. Source: https://arxiv.org/pdf/2412.18925 In the first stage, the model was trained to approach medical questions like a human expert. It started with an initial attempt to answer a problem and then iteratively refined its reasoning through different strategies: Exploring New Paths: Trying fresh approaches to arrive at an answer. Backtracking: Revisiting earlier ideas to find better solutions. Verification: Checking and validating its reasoning. Correction: Critiquing its logic and making improvements. This process was repeated until the model reached a correct answer or exhausted its attempts. Successful reasoning steps were then turned into natural, easy-to-follow narratives to teach the model how to approach similar problems in the future. In the second stage, reinforcement learning (RL) was used to further improve the model's reasoning skills. A specialized verifier helped guide the model by rewarding accurate and well-thought-out answers while penalizing incorrect or incomplete responses. Over time, this process refined the model's ability to produce high-quality reasoning and answers. The model is available in several configurations, including versions supporting both English and Chinese, with parameter sizes ranging from 7 billion to 72 billion. HuatuoGPT-o1 has demonstrated significant performance across a range of medical benchmarks. The 8-billion parameter version delivered an 8.5-point improvement over its baseline, while the 70-billion parameter variant outperformed leading medical-specific LLMs on datasets like MedQA and PubMedQA.  Source: https://arxiv.org/pdf/2412.18925 The efficiency of HuatuoGPT-o1 has drawn attention. Dhruv Panchal, a CEO at Neurolov AI, remarked:  Innovative training methods like this could reshape how we address complex medical problems with fewer resources. However, other community members have raised concerns about data quality and fairness. Cyrus S., an AI solution builder, commented: While the efficiency of HuatuoGPT-o1 with limited training data is remarkable, let's not forget the crucial role of data quality and bias. In my experience, even the most advanced models can be rendered ineffective or even harmful with skewed datasets. I recall a project where we were developing an AI for credit scoring, and the initial results were promising. However, when we tested it with diverse datasets, we found significant biases against certain demographics. It taught me that the quality of the data is just as vital as the model itself. In healthcare, the stakes are even higher. We must ensure these AI models are trained on diverse, representative datasets to avoid exacerbating existing health disparities. Are we ready to entrust life-or-death decisions to AI without thoroughly addressing these ethical and practical considerations? What safeguards are in place to ensure fairness and equity?  HuatuoGPT-o1’s code, models, and training datasets are available on GitHub and Hugging Face, allowing researchers and developers to test and refine the model further. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2025/01/huatuogpt-o1-ai/en/headerimage/generatedHeaderImage-1736879910904.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cdiv\u003e\u003cp\u003eResearchers from The Chinese University of Hong Kong, Shenzhen, and the Shenzhen Research Institute of Big Data have introduced \u003ca href=\"https://github.com/FreedomIntelligence/HuatuoGPT-o1\"\u003eHuatuoGPT-o1\u003c/a\u003e, a medical large language model (LLM) designed to improve reasoning in complex healthcare scenarios. Developed using a novel two-stage training process, the model aims to refine responses through step-by-step analysis, resembling the diagnostic approaches used by medical professionals.\u003c/p\u003e\u003cp\u003e\n\nThe development of HuatuoGPT-o1 followed a structured two-step approach designed to cultivate critical thinking and iterative refinement in the model\u0026#39;s reasoning process.\u003c/p\u003e\u003c/div\u003e\n\n\u003cp\u003e\u003cimg alt=\"model\" data-src=\"news/2025/01/huatuogpt-o1-ai/en/resources/1Screenshot 2025-01-14 190205-1736879909646.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/01/huatuogpt-o1-ai/en/resources/1Screenshot 2025-01-14 190205-1736879909646.png\" rel=\"share\"/\u003e\u003cbr/\u003e\n\u003cem\u003eSource: https://arxiv.org/pdf/2412.18925\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the first stage, the model was trained to approach medical questions like a human expert. It started with an initial attempt to answer a problem and then iteratively refined its reasoning through different strategies:\u003c/p\u003e\n\n\u003cul\u003e\n\t\u003cli\u003e\u003cstrong\u003eExploring New Paths\u003c/strong\u003e: Trying fresh approaches to arrive at an answer.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eBacktracking\u003c/strong\u003e: Revisiting earlier ideas to find better solutions.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eVerification\u003c/strong\u003e: Checking and validating its reasoning.\u003c/li\u003e\n\t\u003cli\u003e\u003cstrong\u003eCorrection\u003c/strong\u003e: Critiquing its logic and making improvements.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cdiv\u003e\u003cp\u003eThis process was repeated until the model reached a correct answer or exhausted its attempts. Successful reasoning steps were then turned into natural, easy-to-follow narratives to teach the model how to approach similar problems in the future.\u003c/p\u003e\u003cp\u003e\n\nIn the second stage, reinforcement learning (RL) was used to further improve the model\u0026#39;s reasoning skills. A specialized verifier helped guide the model by rewarding accurate and well-thought-out answers while penalizing incorrect or incomplete responses. Over time, this process refined the model\u0026#39;s ability to produce high-quality reasoning and answers.\u003c/p\u003e\u003cp\u003e\n\nThe model is available in \u003ca href=\"https://github.com/FreedomIntelligence/HuatuoGPT-o1\"\u003eseveral configurations\u003c/a\u003e, including versions supporting both English and Chinese, with parameter sizes ranging from 7 billion to 72 billion.\u003c/p\u003e\u003cp\u003e\n\nHuatuoGPT-o1 has demonstrated significant performance across a range of medical benchmarks. The 8-billion parameter version delivered an 8.5-point improvement over its baseline, while the 70-billion parameter variant outperformed leading medical-specific LLMs on datasets like \u003ca href=\"https://paperswithcode.com/dataset/medqa-usmle\"\u003eMedQA\u003c/a\u003e and \u003ca href=\"https://pubmedqa.github.io/\"\u003ePubMedQA\u003c/a\u003e. \u003c/p\u003e\u003c/div\u003e\n\n\u003cp\u003e\u003cimg alt=\"benchmark\" data-src=\"news/2025/01/huatuogpt-o1-ai/en/resources/1Screenshot 2025-01-14 185937-1736879909646.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/01/huatuogpt-o1-ai/en/resources/1Screenshot 2025-01-14 185937-1736879909646.png\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eSource: https://arxiv.org/pdf/2412.18925\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThe efficiency of HuatuoGPT-o1 has drawn attention. Dhruv Panchal, a CEO at Neurolov AI, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:activity:7282350501143207936?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7282350501143207936%2C7282358030443163648%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287282358030443163648%2Curn%3Ali%3Aactivity%3A7282350501143207936%29\"\u003eremarked\u003c/a\u003e: \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eInnovative training methods like this could reshape how we address complex medical problems with fewer resources.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eHowever, other community members have raised concerns about data quality and fairness. Cyrus S., an AI solution builder, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:activity:7282350501143207936?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7282350501143207936%2C7282368809842229248%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287282368809842229248%2Curn%3Ali%3Aactivity%3A7282350501143207936%29\"\u003ecommented\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhile the efficiency of HuatuoGPT-o1 with limited training data is remarkable, let\u0026#39;s not forget the crucial role of data quality and bias. In my experience, even the most advanced models can be rendered ineffective or even harmful with skewed datasets. I recall a project where we were developing an AI for credit scoring, and the initial results were promising. However, when we tested it with diverse datasets, we found significant biases against certain demographics. It taught me that the quality of the data is just as vital as the model itself. In healthcare, the stakes are even higher. We must ensure these AI models are trained on diverse, representative datasets to avoid exacerbating existing health disparities. Are we ready to entrust life-or-death decisions to AI without thoroughly addressing these ethical and practical considerations? What safeguards are in place to ensure fairness and equity? \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eHuatuoGPT-o1’s code, models, and training datasets are available on \u003ca href=\"https://github.com/FreedomIntelligence/HuatuoGPT-o1\"\u003eGitHub\u003c/a\u003e and \u003ca href=\"https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-8B\"\u003eHugging Face\u003c/a\u003e, allowing researchers and developers to test and refine the model further.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-01-14T00:00:00Z",
  "modifiedTime": null
}
