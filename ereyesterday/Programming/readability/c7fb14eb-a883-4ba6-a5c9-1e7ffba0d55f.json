{
  "id": "c7fb14eb-a883-4ba6-a5c9-1e7ffba0d55f",
  "title": "Netflix Rolls Out Service-Level Prioritized Load Shedding to Improve Resiliency",
  "link": "https://www.infoq.com/news/2024/11/netflix-load-shedding/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Netflix extended its prioritized load-shedding implementation to the individual service level to further improve system resilience. The approach uses cloud capacity more efficiently by shedding low-priority requests only when necessary instead of maintaining separate clusters for failure isolation. By Rafal Gancarz",
  "author": "Rafal Gancarz",
  "published": "Sat, 23 Nov 2024 18:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Observability",
    "Reliability",
    "API Gateway",
    "Microservices",
    "Optimization",
    "Resilience",
    "Architecture \u0026 Design",
    "news"
  ],
  "byline": "Rafal Gancarz",
  "length": 3727,
  "excerpt": "Netflix extended its prioritized load-shedding implementation to the individual service level to further improve system resilience. The approach uses cloud capacity more efficiently by shedding low-pr",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241119074149/apple-touch-icon.png",
  "text": "Netflix extended its prioritized load-shedding implementation to the individual service level to further improve system resilience. The approach uses cloud capacity more efficiently by shedding low-priority requests only when necessary instead of maintaining separate clusters for failure isolation. The company previously deployed load-shedding strategies at the API Gateway level but decided to enable service owners to implement their prioritization logic at the service level, focusing on the video streaming control plane and data plane. Netflix categorizes API requests into two types based on their criticality. User-initiated requests are considered critical as not handling these would impact user experience. On the other hand, pre-fetch requests made by browsers or apps anticipating the user’s intentions are considered non-critical and can be dropped without much impact on user experience. The previous solution didn’t distinguish between user-initiated and prefetch requests and would reduce availability for both in case of large traffic spikes. Netflix considered separating different request categories into separate clusters but decided against such an approach due to higher compute costs and additional operational overhead. Single Cluster with Prioritized Load Shedding (Source: Netflix Technology Blog) Instead, the company implemented a concurrency limiter in its Play API that prioritizes user-initiated requests over prefetch requests using the open-source Java library. The limiter is configured as a pre-processing Servlet Filter using HTTP headers sent by devices without parsing the request body. The team working on the solution could observe their efforts in preventing a secondary outage a few months after deploying their changes when an infrastructure outage caused a buildup of prefetch requests from Android devices. The limiter dropped the availability of prefetch requests to as low as 20%, while the availability of user-initialized requests remained high, above 99.4%. Availability of Prefetch and User-initialized Requests (Source: Netflix Technology Blog) After the rollout of load shedding for the Play API, the team created a generic internal library to enable service owners to configure their prioritization logic with multiple priority levels (critical, degraded, best-effort, bulk). Services can use the upstream client’s priority or map incoming requests to one of the preconfigured priority levels. Anirudh Mendiratta, staff software engineer at Netflix and co-author of the blog post, explains how the service-level load-shedding solution works with CPU-based autoscaling: Most services at Netflix autoscale on CPU utilization, so it is a natural measure of system load to tie into the prioritized load-shedding framework. Once a request is mapped to a priority bucket, services can determine when to shed traffic from a particular bucket based on CPU utilization. In order to maintain the signal to autoscaling that scaling is needed, prioritized shedding only starts shedding load after hitting the target CPU utilization, and as system load increases, more critical traffic is progressively shed in an attempt to maintain user experience. The team ran a series of experiments to test load-shedding by generating a load profile surpassing the autoscale volume six times. As expected, the limiter shed first non-critical requests, and then critical requests and latency remained acceptable. Additionally, engineers have extended the library to work for IO-bound services by adding latency-based shedding. About the Author Rafal Gancarz",
  "image": "https://res.infoq.com/news/2024/11/netflix-load-shedding/en/headerimage/generatedHeaderImage-1731923073855.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eNetflix \u003ca href=\"https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d\"\u003eextended its prioritized load-shedding implementation to the individual service level\u003c/a\u003e to further improve system resilience. The approach uses cloud capacity more efficiently by shedding low-priority requests only when necessary instead of maintaining separate clusters for failure isolation.\u003c/p\u003e\n\n\u003cp\u003eThe company \u003ca href=\"https://www.infoq.com/news/2020/11/netflix-load-shedding/\"\u003epreviously deployed load-shedding strategies at the API Gateway level \u003c/a\u003ebut decided to enable service owners to implement their prioritization logic at the service level, focusing on the video streaming control plane and data plane. Netflix categorizes API requests into two types based on their criticality. User-initiated requests are considered critical as not handling these would impact user experience. On the other hand, pre-fetch requests made by browsers or apps anticipating the user’s intentions are considered non-critical and can be dropped without much impact on user experience.\u003c/p\u003e\n\n\u003cp\u003eThe previous solution didn’t distinguish between user-initiated and prefetch requests and would reduce availability for both in case of large traffic spikes. Netflix considered separating different request categories into separate clusters but decided against such an approach due to higher compute costs and additional operational overhead.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/11/netflix-load-shedding/en/resources/1netflix-load-shedding2-1732383442857.jpeg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/11/netflix-load-shedding/en/resources/1netflix-load-shedding2-1732383442857.jpeg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eSingle Cluster with Prioritized Load Shedding (Source: \u003ca href=\"https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d\"\u003eNetflix Technology Blog\u003c/a\u003e)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eInstead, the company implemented a concurrency limiter in its Play API that prioritizes user-initiated requests over prefetch requests using \u003ca href=\"https://github.com/Netflix/concurrency-limits\"\u003ethe open-source Java library\u003c/a\u003e. The limiter is configured as a pre-processing Servlet Filter using HTTP headers sent by devices without parsing the request body.\u003c/p\u003e\n\n\u003cp\u003eThe team working on the solution could observe their efforts in preventing a secondary outage a few months after deploying their changes when an infrastructure outage caused a buildup of prefetch requests from Android devices. The limiter dropped the availability of prefetch requests to as low as 20%, while the availability of user-initialized requests remained high, above 99.4%.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/11/netflix-load-shedding/en/resources/1netflix-load-shedding-limiter-1732383442857.jpeg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/11/netflix-load-shedding/en/resources/1netflix-load-shedding-limiter-1732383442857.jpeg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eAvailability of Prefetch and User-initialized Requests (Source: \u003ca href=\"https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d\"\u003eNetflix Technology Blog\u003c/a\u003e)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eAfter the rollout of load shedding for the Play API, the team created a generic internal library to enable service owners to configure their prioritization logic with multiple priority levels (critical, degraded, best-effort, bulk). Services can use the upstream client’s priority or map incoming requests to one of the preconfigured priority levels.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.linkedin.com/in/amendira/\"\u003eAnirudh Mendiratta\u003c/a\u003e, staff software engineer at Netflix and co-author of the blog post, explains how the service-level load-shedding solution works with CPU-based autoscaling:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eMost services at Netflix autoscale on CPU utilization, so it is a natural measure of system load to tie into the prioritized load-shedding framework. Once a request is mapped to a priority bucket, services can determine when to shed traffic from a particular bucket based on CPU utilization. In order to maintain the signal to autoscaling that scaling is needed, prioritized shedding only starts shedding load after hitting the target CPU utilization, and as system load increases, more critical traffic is progressively shed in an attempt to maintain user experience.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe team ran a series of experiments to test load-shedding by generating a load profile surpassing the autoscale volume six times. As expected, the limiter shed first non-critical requests, and then critical requests and latency remained acceptable. Additionally, engineers have extended the library to work for IO-bound services by adding latency-based shedding.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Rafal-Gancarz\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRafal Gancarz\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-11-23T00:00:00Z",
  "modifiedTime": null
}
