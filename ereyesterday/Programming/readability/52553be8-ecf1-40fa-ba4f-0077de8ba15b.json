{
  "id": "52553be8-ecf1-40fa-ba4f-0077de8ba15b",
  "title": "A Rare Insight Into The Daily Challenges Of An Experiments Team",
  "link": "https://engineering.prezi.com/a-rare-insight-into-the-daily-challenges-of-an-experiments-team-349a94960b4f?source=rss----911e72786e31---4",
  "description": "",
  "author": "Attila V√°g√≥",
  "published": "Tue, 09 Jul 2024 13:21:02 GMT",
  "source": "https://engineering.prezi.com/feed",
  "categories": [
    "a-b-testing",
    "prezi",
    "product-development",
    "software-development",
    "engineering-culture"
  ],
  "byline": "Attila V√°g√≥",
  "length": 25043,
  "excerpt": "I think I‚Äôd need four hands to count all the types of projects I have touched in over a decade. From small tech start-up to mid-size web agency and from mid-size established tech company to large‚Ä¶",
  "siteName": "Prezi Engineering",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*U0lNGgJfm0Qo1ZfYDS36KA.png",
  "text": "If you thought feature development was tough, try developing A/B tests all day, every day‚Ä¶ üòâI think I‚Äôd need four hands to count all the types of projects I have touched in over a decade. From small tech start-up to mid-size web agency and from mid-size established tech company to large tech corporations, I‚Äôve seen them all and contributed to codebases more than I can or care to count. And yet, in Prezi ‚Äî two years after joining ‚Äî I found myself in an entirely new context: an experiments team, and my mind is blown. Every. Single. Day.What does an experiments team do?If you‚Äôre thinking greenfield projects, I‚Äôm going to stop you right there. This is the team that has little to no chance to such luxuries. It‚Äôs quite the opposite. In Prezi, the experiments team‚Äôs main purpose is to think of A/B tests, plan them, implement them, watch them perform and then, based on the outcome, release one of the variants, or scrap the entire test. That‚Äôs the gist of it, anyway. Why would we do that? One word: success. The success of Prezi as a whole and implicitly our customers‚Äô.Our experiments team is actually called ‚ÄúGrowth \u0026 Monetisation‚Äù or GM ‚Äî though that always makes me think of General Motors, which we have nothing to do with. As far as I know, the only cars we ever built in Prezi were all made of LEGO. üòÑ We‚Äôre in the business of inspiring, unforgettable, impactful presentations and our team makes sure to drive that message home to more happy customers than ever. Naturally, you can have the best product out there, if the path to the product isn‚Äôt frictionless, there is a high churn. So that‚Äôs what we do: we try to understand based on data generated by A/B tests where potential customers drop off, why, and how we might be able to change that for the better.I very deliberately used the word might there. Predicting success in software development is about as accurate in my experience as predicting the weather in Ireland. While there is not much we can do about Irish weather, we can test hypotheses in a software product relatively easily. You see what works, and armed with that knowledge, you make further decisions. But, you‚Äôll also quickly realise success isn‚Äôt a given in an A/B test. In fact, the industry standard ‚Äî which we‚Äôre also tracking ‚Äî is 30%. That‚Äôs 7/10 A/B tests failing or being inconclusive.Running an A/B test implies a chance of failure. You have to accept that to ultimately succeed.But, with a healthy dose of pragmatism and informed optimism, you might also see the failed tests as a great learning opportunity. If nothing else, these will either stop you from investing in the wrong features, prevent you from entering a technical rabbit-hole or even inform and shape future A/B tests and get them that much closer to a successful outcome. And when you think like that, you realise that no time spent on A/B testing is lost time, as one way or another, it feeds directly into your product strategy.Allow me to inspire you with a few examples. The first two examples will be successful experiments that ended up either increasing revenue or the number of registered users. The latter two will focus on failed experiments we learned a lot from.A privacy control call-to-action. A Prezi created by a free account is always public, and we made that abundantly clear as soon as the user entered the editor, before even getting a chance to use the product and get excited by the prospect of creating unique, engaging, multidimensional presentations or generate one with Prezi AI. Sure, the goal of the immediate ‚Äî in your face ‚Äî privacy notification was well-intentioned. We wanted users to be both aware of their presentations being public, and give them the chance to upgrade. This experience, however, was one of high friction.Our theory was that we might be able to improve on this and not lose subscriptions in the process, maybe even win some more, so instead of the notification, we just added a visible call-to-action button signalling the document was public. On click, the user ‚Äî as before ‚Äî had the option to just acknowledge and keep it public, or upgrade to a paid membership and make the Prezi private. And guess what? Not only did we not lose subscribers with the new approach, but even gained some.Confusing SSL Callout. In our paywalls we wanted to give our customers peace of mind by calling out that transactions are SSL encrypted and 100% safe. One would think this is a great example of caring about your users. Except just like parenting can go very wrong when you become a helicopter parent, caring for users can also go sideways. In this particular instance, instead of gaining subscribers, we lost a considerable number of them because:It was confusing to see that message on a free trial start page.It was placed at the beginning of the form, rather than the end, below the payment button.There is a chance it‚Äôs quite a redundant message these days when it‚Äôs assumed and expected that all transactions are safe. Heck, most browsers won‚Äôt even load unsecured sites anymore, so you expect to see something like this more on scam sites than legit products.One small change in the right place. A fantastic example of sometimes just how small but incredibly effective an A/B test can be, is adding a single line of text to our product selector. We expected that mentioning the new AI capability for creating presentations would perhaps get us a small increase in bookings. Turns out, we were just thinking small, and the real increase was significantly higher. Granted, we couldn‚Äôt have added that line without the teams. Granted, we couldn‚Äôt have added that line without the teams having built all the AI features, but it shows how important it can be to bring that to users‚Äô attention where it makes the most impact in the flow.One big change in the wrong place. How many times have you seen complete redesigns being done on websites and apps in hopes to recapture users, only to find it had no effect? In fact, it might have even made things worse. We did the same with our business users but as an A/B test ‚Äî why commit if you don‚Äôt have to, right? We expected a modest increase in bookings, and how wrong we were. Turns out, our business users got put off by the redesign, the features we thought were interesting for them, made them pause enough that we ended up with an unexpected negative impact on bookings! üò± The good news is, we quickly learned how not to do a redesign and that we might want to highlight more relevant features to them in future iterations.A/B tests are the financially responsible way of developing software. Agile development on steroids if you will.So, A/B tests to the rescue, right? But in the history of software development there hasn‚Äôt yet been a solution that didn‚Äôt bring its own set of challenges and that‚Äôs what I really want to focus on, so anyone wanting to truly invest in experiments and improve their product, does so with eyes wide open. The rewards may be undeniable, but the challenges aren‚Äôt negligible either.And what does that mean for engineers?On the web, opinions are split on whether having specialised skills as an engineer is better than being T-shaped. As a staff engineer, I have come to the conclusion that just like having the right tools for the right job, having the right engineers within the team is also crucial to success. On our team‚Äôs page, we have a short but sweet table of what the ideal engineer looks like in terms of skills to feel comfortable. In contrast, a native apps team‚Äôs table would probably look very different. See? Right people for the right job.Our team mindset table ‚Äî Hire the right people, with the right skills and the right mindset ‚Äî screenshot by authorI won‚Äôt however just leave you with a table up for interpretation, as I think all of those traits are worth a paragraph or two of clarification.Impact driven as opposed to being creative or technology driven. In over a decade, I have met more passionate engineers about technologies, coding paradigms and abstractions than those who just want to get stuff out there for the sake of learning and iterating. To be perfectly frank, you need both in an engineering organisation if you don‚Äôt want your product to become unusable and unmaintainable. Heck, even within our team, some of us care more about the architecture, software integrity and efficiency of the product than others, but we all share the conviction that whatever we do, must have a tangible benefit to us as a team, Prezi and ultimately, the user. This is not the team where you randomly get to try a new frontend-library and rewrite one of your services in Rust ‚Äî as exciting as that may sound.It‚Äôs important not to confuse being versatile with jack-of-all-trades. That being said, the ideal engineer on our team while may not necessarily be a hard-core full-stack engineer, they won‚Äôt shy away from jumping into either sides of the codebase. In our case, that means a wide range of front and backend libraries and frameworks. It sounds intimidating perhaps, but in reality it‚Äôs a lot less about the expectation of knowing everything, but rather the openness to discover it all over time.Being an efficient engineer deserves an article ‚Äî if not a book ‚Äî of its own, but let me condense it into a couple of thoughts. Us, engineers, have the tendency to polish code, refactor to the point of giving the impression we‚Äôre not writing software but creating the Milo of Venus. In an experiments team, we‚Äôre more focused on creating meaningful stick figures. As long as we‚Äôre able to gauge from the experiment the data we need, the goal is achieved. The code doesn‚Äôt have to be optimised (unless it‚Äôs getting in the way of being able to run the test), and keeping implementation as simple as possible is a prime objective. As long as it‚Äôs testable and revertible, you have yourself a candidate for release.Having a data driven attitude is key, and I think it drives a lot of the other traits. How often have we, engineers, developed useless features over weeks, months, maybe even years? It‚Äôs not uncommon. In an experiments team, however, you don‚Äôt have the luxury to do that. Unless there is data to support a code-change, a new feature, a variant of a feature, it simply won‚Äôt happen.Being an avid learner goes hand-in-hand with being data-driven. The focus in an experiments team is on understanding what happened but more importantly why, as the answer will drive the next experiments and possibly a considerable part of the product strategy.A competitive engineer, comfortable with bold ideas, doesn‚Äôt necessarily mean reckless. It also doesn‚Äôt mean a lot of ‚Äúhacking stuff together‚Äù. It‚Äôs rather a fine-tuned skill of seeing through the technical challenges in such a way that they‚Äôre able to propose the shortest technically viable path to success, and that path doesn‚Äôt have to follow the status quo.On a personal note, I would argue that many of the above skills are worth picking up over time for any engineer. As one moves from company to company, from team to team, being able to adapt to different mindsets can very positively impact one‚Äôs career.If you find yourself having the opportunity to join an experiments team, go for it, learn from it, make the most of it. You‚Äôll thank yourself later.All fingers in all piesBefore joining the GM team in Prezi, I was lead engineer on Prezi Video for Zoom, and later, on the first two waves of Prezi AI. Both, especially in the case of the former, meant that development was mostly spent in a couple of repositories, in very distinct areas of the product. Prezi Video for Zoom was a web app of its own, and Prezi Present ‚Äî where Prezi AI was released ‚Äî is mostly a self-contained entity as well, unless you start veering into service territory, but we have dedicated teams for that. In contrast, the very first day I joined the GM team, I found myself checking out not one, not two, but a list of repositories and as time passed, a few more. I have eight running at the moment in my development environment, and that still doesn‚Äôt cover all the possible flows a user could take on the Prezi website. Add to that Prezi Present, which we still contribute to with experiments, and you have yourself a context in which certain complexities are unavoidable.You may wonder, why unavoidable? Can‚Äôt other teams run their own growth and monetisation experiments in their respective areas of expertise and ownership? I have no doubt that in certain organisations that is possible. And even in Prezi, for instance, we were able to do that with Prezi Video for Zoom. Our Infogram team can also operate similarly, as it‚Äôs a distinct product. However, when it comes to the rest of what Prezi essentially is ‚Äî the Prezi website, Prezi Present and Prezi Video ‚Äî one has to approach it holistically, and we must be able to own the experiment end-to-end, which conveniently brings me to what an experiment lifecycle looks like.Experiment lifecycleA picture‚Äôs worth a 1000 words and because this article is vertiginously approaching 4000, I‚Äôll rely on a diagram to tell most of the experiment lifecycle story.Experiment lifecycle diagram created by author in Apple FreeformReleasing an A/B test is ‚Äî quite literally ‚Äî only half the work and half the story, but let‚Äôs see briefly what these 13 steps in the experiment lifecycle are:Ideation is a somewhat nebulous step, and it involves a lot of product manager/product owner (PM/PO) sorcery outside the scope of this article, but generally speaking, ideas will be based on market research, data, previous findings, user feedback, etc.Ideas there may be many, but it‚Äôs important to keep focus on what moves the company goals forward in a viable context. Sometimes ideas can be really good, but other things need to happen before they become feasible.Having a low-fidelity design ‚Äî a rough sketch ‚Äî of what the experiment and the user flow would look like can further validate the idea or uncover logical fallacies. At this point, you might already find engineers to be a great asset in the conversation.Getting to the planning stage means this is now going ahead full-steam and gets into the upcoming sprint. In our case, we tend to work kanban style, so whoever is next willing and well-suited enough to pick the work up, gets to do so. Every so often you‚Äôll find that the experiment is not just a story, but an entire epic, in which case several engineers might allocate their time to it being led by a project lead.Development is as self-explanatory as it can be. It‚Äôs the coding stage, including writing automated unit, integration and regression tests, adding the feature switches and getting everything into a (or more) pull request for code review.Our manual QA team member(s) ensure everything has been done to spec and execute some regression testing as well. Given the number of experiments we run, it‚Äôs a much-needed peace of mind to know at least one set of objective eyes checks everything.Releasing deserves a section of its own, so keep reading. For now, let‚Äôs just say it involves setting the feature switch configuration up to the desired cohorts and enabling them. Once it‚Äôs released, a cleanup task is automatically generated for a later date (see step 12).Spot checking ensures we‚Äôre on the right track with the experiment, nothing blew up, we‚Äôre not seeing any majorly negative results or collateral damage in signups or upgrades.After a few weeks, the experiment is stopped, so no more new users are getting exposed to the test. At times, we might allow the users who have been getting the test variant to keep having access to it to further observe user behaviour. This usually lasts no more than another 2‚Äì3 weeks.Evaluation is all about interpreting the data, understanding the learnings. This is the moment we may decide to release a variant (success) to all users or stick to the control (fail).Rollout is essentially the outcome of the evaluation ‚Äî all users get one variant going forward, which from that point on becomes the control.Cleanup is another phase I deemed important enough to highlight in its own section, so do keep reading, but the short of it is, we ensure that all redundant code, tests, and feature switches are done away with. This triggers steps 4, 5 and 6, all culminating in the final step‚Ä¶Everything is done. The variant is rolled out, the code is cleaned up, and we have either learned something (failed experiment) or achieved something (successful experiment).That‚Äôs the gist of the experiment lifecycle, but as I mentioned, there are a couple of stages there that are really worth digging into more to truly understand some of the complexities and challenges a team like ours can face on a daily basis.Dealing with feature switchesSome will call them the best human invention since fire, while others, a necessary evil. I, for one, think it‚Äôs a very useful tool, but like every tool, it can be overused or misused. In our case, it‚Äôs invaluable to have the option of setting up a new feature switch for every experiment and variant. The more challenging part is keeping track of them all.For context, we have 9 engineers on the team, and generally speaking, we aim for just as many experiments per sprint. Some quick maths suggests 160 experiments per year, but let‚Äôs go with a more conservative 100 experiments instead. Just assuming you have two variants per experiment already means 300 feature switches. 100 of those control the bucketing of the variants. If not handled correctly, things can get quickly out of hand, so we have devised some ways to avoid that:Adding a special prefix for feature switches that control the variants.Using team-based feature switch prefixes.By making sure each feature switch has a clear ownership marked ‚Äî we use a unique team email address.Each feature switch will have a link to the experiment note or the Jira ticket it refers to.This varies from organisation to organisation, but in Prezi, it‚Äôs mostly the software engineers who add, configure and clean up feature switches. We opted for this approach as it keeps the control of software integrity in engineering‚Äôs hands. We don‚Äôt have to worry about product owners inadvertently breaking regression tests by turning switches on and off at the wrong time.Releasing an experimentWhile releasing an experiment will ultimately come down to just flipping a switch ‚Äî a feature switch that is ‚Äî there‚Äôs a lot more to it and how much exactly, can vary from experiment to experiment. Some are a lot more involved than others. As I am writing this, I am working on an A/B test that involves three frontend bundles (think apps), and four different services. Even if you‚Äôre experienced and QA did a fantastic job making sure we haven‚Äôt broken anything, there are still a myriad of things that can fall through the cracks.To make sure releases go as smoothly as possible, we adopted an already standard practice from aviation and medicine ‚Äî a checklist.Surgeons use Surgical Safety Checklists, and pilots rely on Pre-flight Checklists to ensure the best outcomes. We call it a release document, but it‚Äôs really a checklist as clearly stated in the head of each document:This document is meant to be used as a checklist for the person who‚Äôs driving the release to be able to do it in a calm, collected, professional way. Also meant to act as a document for others, so when troubleshooting is needed, all the information about what was happening during a release is recorded. ‚Äî Prezi internal release documentAll such documents are signed off by at least one‚Äî but ideally two ‚Äî senior or lead engineers on the team.To some, this might seem excessive, and at times it really is, but in weighing the costs and benefits, as a team we concluded this approach gives us enough value and confidence to stick to it. Just to illustrate some of the items on the checklist, here‚Äôs what we look for:Have the relevant senior/lead engineers signed off on the plan?What components are meant to be deployed and have they deployed successfully?Have all relevant teams been notified about our intent to release the experiment?What‚Äôs the feature switch configuration?Is the testing scenario working on production as expected?Is the A/B test distribution as expected on OpenSearch?Any unexpected spikes in Grafana?Are there any new relevant Sentry error logs?What action(s) to take in case of needing to revert?If all of the above OK, notify internal stakeholders of successful release.It‚Äôs a cross your ‚ÄúT‚Äùs and dot your ‚ÄúI‚Äùs kind of exercise, but out of it we get a log we can reference later and the assurance that anything that could have been prevented, has been prevented because, you know‚Ä¶ Murphy‚Äôs Law. üòâHaving released, however, doesn‚Äôt mean we‚Äôre done. Far from it. There‚Äôs cleanup, and it‚Äôs such an important part of what our team does that I felt it deserved its own section, so without further ado‚Ä¶Cleaning upI hate doing the dishes, so by week‚Äôs end there‚Äôs a literal pile of them waiting to be washed. Now, remember those 300 feature switches? That‚Äôs precisely the pile we desperately want to avoid. Because feature switches as useful as they are, they quickly pollute the code to a point it becomes unmaintainable, which would result in us losing more and more velocity over time. As a team, you can easily grind to a screeching halt if code is not maintained, and as an experiments team, we‚Äôre particularly prone to having this happen if we‚Äôre not vigilant.One way we‚Äôre working on preventing such a situation is by automatically creating cleanup tickets for each experiment. Jira isn‚Äôt so bad after all, aye? üòÑ You see, once an experiment goes live, it will stay live for at least a couple of weeks. Gathering useful enough data to make pragmatic product decisions, doesn‚Äôt happen instantly, so usually a few weeks after the A/B test release a decision gets made. Either we stick to what we had before ‚Äî aka we keep the control variants ‚Äî or we keep one of the other variants. Often it‚Äôs just one, but there are times when an A/B test has a total of as many as four variants. Let me pseudocode an example:if(isActive('amazing-feature-variant-a')){ \u003cABTestComponentVariantA\u003e...\u003c/ABTestComponentVariantA\u003e} else if(isActive('amazing-feature-variant-b')){ \u003cABTestComponentVariantB\u003e...\u003c/ABTestComponentVariantB\u003e} else if(isActive('amazing-feature-variant-c')){ \u003cABTestComponentVariantC\u003e...\u003c/ABTestComponentVariantC\u003e} else { \u003cControlVariant\u003e...\u003c/ControlVariant\u003e}Regardless of which one we keep, three of those have to go. You can imagine, of course, that often times an A/B test is far more involved than just showing a component or not, so cleanup can become quite an undertaking, as you want to make sure you understand the variants that have been added, the relationship with the rest of the codebase and the overall user flows, so cleaning up doesn‚Äôt result in collateral damage. This typically means editing tests as well.You might wonder, in case of a lost A/B test where we end up sticking to control ‚Äî to what we had before ‚Äî can‚Äôt we just revert to the original PR? The answer is maybe, perhaps partially or not at all for the following reasons:You might be able to revert if the initial change was very clean, and other changes to those files haven‚Äôt been done since. In a high-traffic codebase, that‚Äôs quite unlikely to happen, though.You might only be able to do a partial revert if some of the changes happened in a low-traffic codebase, while others in higher-traffic codebases. The A/B test I am working on right now touches several repositories. I could imagine one or two of those repositories seeing light enough traffic that I could just revert, but the rest would require a more involved approach.If you‚Äôre only dealing with high-traffic codebases, you simply don‚Äôt have this option, but you will also find cases where you added code for one of the variants that‚Äôs actually going to be useful for future work. Maybe you wrote a nice utility function, or refactored some code as part of the A/B test to make your life easier. You surely don‚Äôt want to revert that.When all is clean and doneI won‚Äôt gaslight you into thinking we don‚Äôt deal with technical debt, awkward tech stacks, or breaking pipelines like every other team and engineering organisation out there. We do, and some of our challenges aren‚Äôt even new to many developers out there. It‚Äôs more like a unique flavour of what other teams deal with daily, and it‚Äôs unique enough that we found ourselves having to fine-tune how we do things, improve our processes, and continuously refine and shape ourselves as engineers into individuals reflecting the previously illustrated skills (mindset) table.This is what has worked for us. This is what gets things done. For now. Just like we experiment with features, we experiment with ourselves as individuals and as a team. Sometimes that means we succeed, other times it means we learn and move on, or we learn to move on. It‚Äôs a journey, and it requires stamina, but ultimately, it‚Äôs well-worth the effort. So, yes, A/B tests for the win! üéâ",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*n3VDMgH-u5OU5VAH",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"3d52\"\u003eIf you thought feature development was tough, try developing A/B tests all day, every day‚Ä¶ üòâ\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://attilavago.medium.com/?source=post_page---byline--349a94960b4f--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Attila V√°g√≥\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*Ilzy6aGvG__n7QzdkiL41A.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://engineering.prezi.com/?source=post_page---byline--349a94960b4f--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Prezi Engineering\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ecIYF5KMJj1G4-_pkFWy0g.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"2398\"\u003e\u003cspan\u003eI\u003c/span\u003e think I‚Äôd need four hands to count all the types of projects I have touched in over a decade. From small tech start-up to mid-size web agency and from mid-size established tech company to large tech corporations, I‚Äôve seen them all and contributed to codebases more than I can or care to count. And yet, in Prezi ‚Äî two years after joining ‚Äî I found myself in an entirely new context: an experiments team, and my mind is blown. Every. Single. Day.\u003c/p\u003e\u003ch2 id=\"9af9\"\u003eWhat does an experiments team do?\u003c/h2\u003e\u003cp id=\"7e4a\"\u003eIf you‚Äôre thinking greenfield projects, I‚Äôm going to stop you right there. This is the team that has little to no chance to such luxuries. It‚Äôs quite the opposite. In Prezi, the experiments team‚Äôs main purpose is to think of A/B tests, plan them, implement them, watch them perform and then, based on the outcome, release one of the variants, or scrap the entire test. That‚Äôs the gist of it, anyway. Why would we do that? One word: success. The success of Prezi as a whole and implicitly our customers‚Äô.\u003c/p\u003e\u003cp id=\"96cf\"\u003eOur experiments team is actually called ‚ÄúGrowth \u0026amp; Monetisation‚Äù or GM ‚Äî though that always makes me think of General Motors, which we have nothing to do with. As far as I know, the only cars we ever built in Prezi were all made of LEGO. üòÑ We‚Äôre in the business of inspiring, unforgettable, impactful presentations and our team makes sure to drive that message home to more happy customers than ever. Naturally, you can have the best product out there, if the path to the product isn‚Äôt frictionless, there is a high churn. So that‚Äôs what we do: we try to understand based on data generated by A/B tests where potential customers drop off, why, and how we \u003cem\u003emight\u003c/em\u003e be able to change that for the better.\u003c/p\u003e\u003cp id=\"0236\"\u003eI very deliberately used the word \u003cem\u003emight\u003c/em\u003e there. Predicting success in software development is about as accurate in my experience as predicting the weather in Ireland. While there is not much we can do about Irish weather, we can test hypotheses in a software product \u003cem\u003erelatively\u003c/em\u003e easily. You see what works, and armed with that knowledge, you make further decisions. But, you‚Äôll also quickly realise success isn‚Äôt a given in an A/B test. In fact, the industry standard ‚Äî which we‚Äôre also tracking ‚Äî is 30%. That‚Äôs 7/10 A/B tests failing or being inconclusive.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"b591\"\u003eRunning an A/B test implies a chance of failure. You have to accept that to ultimately succeed.\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"ae92\"\u003eBut, with a healthy dose of pragmatism and informed optimism, you might also see the failed tests as a great learning opportunity. If nothing else, these will either stop you from investing in the wrong features, prevent you from entering a technical rabbit-hole or even inform and shape future A/B tests and get them that much closer to a successful outcome. And when you think like that, you realise that no time spent on A/B testing is lost time, as one way or another, it feeds directly into \u003ca href=\"https://productcoalition.com/product-strategy-lessons-from-dr-house-f55872182164\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eyour product strategy\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"4dd3\"\u003eAllow me to inspire you with a few examples. The first two examples will be successful experiments that ended up either increasing revenue or the number of registered users. The latter two will focus on failed experiments we learned a lot from.\u003c/p\u003e\u003cp id=\"7237\"\u003e\u003cstrong\u003eA privacy control call-to-action.\u003c/strong\u003e A Prezi created by a free account is always public, and we made that abundantly clear as soon as the user entered the editor, before even getting a chance to use the product and get excited by the prospect of creating unique, engaging, multidimensional presentations or generate one with Prezi AI. Sure, the goal of the immediate ‚Äî in your face ‚Äî privacy notification was well-intentioned. We wanted users to be both aware of their presentations being public, and give them the chance to upgrade. This experience, however, was one of high friction.\u003c/p\u003e\u003cp id=\"36a5\"\u003eOur theory was that we might be able to improve on this and not lose subscriptions in the process, maybe even win some more, so instead of the notification, we just added a visible call-to-action button signalling the document was public. On click, the user ‚Äî as before ‚Äî had the option to just acknowledge and keep it public, or upgrade to a paid membership and make the Prezi private. And guess what? Not only did we not lose subscribers with the new approach, but even gained some.\u003c/p\u003e\u003cp id=\"bd87\"\u003e\u003cstrong\u003eConfusing SSL Callout.\u003c/strong\u003e In our paywalls we wanted to give our customers peace of mind by calling out that transactions are SSL encrypted and 100% safe. One would think this is a great example of caring about your users. Except just like parenting can go very wrong when you become a helicopter parent, caring for users can also go sideways. In this particular instance, instead of gaining subscribers, we lost a considerable number of them because:\u003c/p\u003e\u003cul\u003e\u003cli id=\"b99e\"\u003eIt was confusing to see that message on a free trial start page.\u003c/li\u003e\u003cli id=\"fed4\"\u003eIt was placed at the beginning of the form, rather than the end, below the payment button.\u003c/li\u003e\u003cli id=\"ae77\"\u003eThere is a chance it‚Äôs quite a redundant message these days when it‚Äôs assumed and expected that all transactions are safe. Heck, most browsers won‚Äôt even load unsecured sites anymore, so you expect to see something like this more on scam sites than legit products.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"6c3b\"\u003e\u003cstrong\u003eOne small change in the right place.\u003c/strong\u003e A fantastic example of sometimes just how small but incredibly effective an A/B test can be, is adding a single line of text to our product selector. We expected that mentioning the new AI capability for creating presentations would perhaps get us a small increase in bookings. Turns out, we were just thinking small, and the real increase was significantly higher. Granted, we couldn‚Äôt have added that line without the teams. Granted, we couldn‚Äôt have added that line without the teams having built all the AI features, but it shows how important it can be to bring that to users‚Äô attention where it makes the most impact in the flow.\u003c/p\u003e\u003cp id=\"8f44\"\u003e\u003cstrong\u003eOne big change in the wrong place.\u003c/strong\u003e How many times have you seen complete redesigns being done on websites and apps in hopes to recapture users, only to find it had no effect? In fact, it might have even made things worse. We did the same with our business users but as an A/B test ‚Äî why commit if you don‚Äôt have to, right? We expected a modest increase in bookings, and how wrong we were. Turns out, our business users got put off by the redesign, the features we thought were interesting for them, made them pause enough that we ended up with an unexpected negative impact on bookings! üò± The good news is, we quickly learned how not to do a redesign and that we might want to highlight more relevant features to them in future iterations.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"2b4f\"\u003eA/B tests are the financially responsible way of developing software. Agile development on steroids if you will.\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"8880\"\u003eSo, A/B tests to the rescue, right? But in the history of software development there hasn‚Äôt yet been a solution that didn‚Äôt bring its own set of challenges and that‚Äôs what I really want to focus on, so anyone wanting to truly invest in experiments and improve their product, does so with eyes wide open. The rewards may be undeniable, but the challenges aren‚Äôt negligible either.\u003c/p\u003e\u003ch2 id=\"679e\"\u003eAnd what does that mean for engineers?\u003c/h2\u003e\u003cp id=\"76b8\"\u003eOn the web, opinions are split on whether having specialised skills as an engineer is better than being T-shaped. As a staff engineer, I have come to the conclusion that just like having the right tools for the right job, having the right engineers within the team is also crucial to success. On our team‚Äôs page, we have a short but sweet table of what the ideal engineer looks like in terms of skills to feel comfortable. In contrast, a native apps team‚Äôs table would probably look very different. See? Right people for the right job.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eOur team mindset table ‚Äî \u003cem\u003eHire\u003c/em\u003e the right people, with the right skills and the right mindset ‚Äî screenshot by author\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"d086\"\u003eI won‚Äôt however just leave you with a table up for interpretation, as I think all of those traits are worth a paragraph or two of clarification.\u003c/p\u003e\u003cul\u003e\u003cli id=\"8adc\"\u003eImpact driven as opposed to being creative or technology driven. In over a decade, I have met more passionate engineers about technologies, coding paradigms and abstractions than those who just want to get stuff out there for the sake of learning and iterating. To be perfectly frank, you need both in an engineering organisation if you don‚Äôt want your product to become unusable and unmaintainable. Heck, even within our team, some of us care more about the architecture, software integrity and efficiency of the product than others, but \u003cstrong\u003ewe all share the conviction that whatever we do, must have a tangible benefit to us as a team, Prezi and ultimately, the user\u003c/strong\u003e. This is not the team where you randomly get to try a new frontend-library and rewrite one of your services in Rust ‚Äî as exciting as that may sound.\u003c/li\u003e\u003cli id=\"3080\"\u003eIt‚Äôs important not to confuse being versatile with jack-of-all-trades. That being said, the ideal engineer on our team \u003cstrong\u003ewhile may not necessarily be a hard-core full-stack engineer, they won‚Äôt shy away from jumping into either sides of the codebase\u003c/strong\u003e. In our case, that means a wide range of front and backend libraries and frameworks. It sounds intimidating perhaps, but in reality it‚Äôs a lot less about the expectation of knowing everything, but rather the openness to discover it all over time.\u003c/li\u003e\u003cli id=\"920d\"\u003eBeing an efficient engineer deserves an article ‚Äî if not a book ‚Äî of its own, but let me condense it into a couple of thoughts. Us, engineers, have the tendency to polish code, refactor to the point of giving the impression we‚Äôre not writing software but creating the Milo of Venus. \u003cstrong\u003eIn an experiments team, we‚Äôre more focused on creating meaningful stick figures. As long as we‚Äôre able to gauge from the experiment the data we need, the goal is achieved.\u003c/strong\u003e The code doesn‚Äôt have to be optimised (unless it‚Äôs getting in the way of being able to run the test), and keeping implementation as simple as possible is a prime objective. As long as it‚Äôs testable and revertible, you have yourself a candidate for release.\u003c/li\u003e\u003cli id=\"2ecc\"\u003eHaving a data driven attitude is key, and I think it drives a lot of the other traits. How often have we, engineers, developed useless features over weeks, months, maybe even years? It‚Äôs not uncommon. In an experiments team, however, you don‚Äôt have the luxury to do that. \u003cstrong\u003eUnless there is data to support a code-change, a new feature, a variant of a feature, it simply won‚Äôt happen.\u003c/strong\u003e\u003c/li\u003e\u003cli id=\"3014\"\u003e\u003cstrong\u003eBeing an avid learner goes hand-in-hand with being data-driven.\u003c/strong\u003e The focus in an experiments team is on understanding what happened but more importantly why, as the answer will drive the next experiments and possibly a considerable part of the product strategy.\u003c/li\u003e\u003cli id=\"386e\"\u003eA competitive engineer, comfortable with bold ideas, doesn‚Äôt necessarily mean reckless. It also doesn‚Äôt mean a lot of ‚Äúhacking stuff together‚Äù. \u003cstrong\u003eIt‚Äôs rather a fine-tuned skill of seeing through the technical challenges in such a way that they‚Äôre able to propose the shortest technically viable path to success\u003c/strong\u003e, and that path doesn‚Äôt have to follow the status quo.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"5400\"\u003eOn a personal note, I would argue that many of the above skills are worth picking up over time for any engineer. As one moves from company to company, from team to team, being able to adapt to different mindsets can very positively impact one‚Äôs career.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"6101\"\u003eIf you find yourself having the opportunity to join an experiments team, go for it, learn from it, make the most of it. You‚Äôll thank yourself later.\u003c/p\u003e\u003c/blockquote\u003e\u003ch2 id=\"cadf\"\u003eAll fingers in all pies\u003c/h2\u003e\u003cp id=\"52f1\"\u003eBefore joining the GM team in Prezi, I was lead engineer on Prezi Video for Zoom, and later, on the first two waves of Prezi AI. Both, especially in the case of the former, meant that development was mostly spent in a couple of repositories, in very distinct areas of the product. Prezi Video for Zoom was a web app of its own, and Prezi Present ‚Äî where Prezi AI was released ‚Äî is mostly a self-contained entity as well, unless you start veering into service territory, but we have dedicated teams for that. In contrast, the very first day I joined the GM team, I found myself checking out not one, not two, but a list of repositories and as time passed, a few more. I have eight running at the moment in my development environment, and that still doesn‚Äôt cover all the possible flows a user could take on the Prezi website. Add to that Prezi Present, which we still contribute to with experiments, and you have yourself a context in which certain complexities are unavoidable.\u003c/p\u003e\u003cp id=\"cc33\"\u003eYou may wonder, why unavoidable? Can‚Äôt other teams run their own growth and monetisation experiments in their respective areas of expertise and ownership? I have no doubt that in certain organisations that is possible. And even in Prezi, for instance, we were able to do that with Prezi Video for Zoom. Our Infogram team can also operate similarly, as it‚Äôs a distinct product. However, when it comes to the rest of what Prezi essentially is ‚Äî the Prezi website, Prezi Present and Prezi Video ‚Äî one has to approach it holistically, and we must be able to own the experiment end-to-end, which conveniently brings me to what an experiment lifecycle looks like.\u003c/p\u003e\u003ch2 id=\"2d91\"\u003eExperiment lifecycle\u003c/h2\u003e\u003cp id=\"e9fb\"\u003eA picture‚Äôs worth a 1000 words and because this article is vertiginously approaching 4000, I‚Äôll rely on a diagram to tell most of the experiment lifecycle story.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eExperiment lifecycle diagram created by author in Apple Freeform\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"649b\"\u003eReleasing an A/B test is ‚Äî quite literally ‚Äî only half the work and half the story, but let‚Äôs see briefly what these 13 steps in the experiment lifecycle are:\u003c/p\u003e\u003col\u003e\u003cli id=\"b634\"\u003eIdeation is a somewhat nebulous step, and it involves a lot of product manager/product owner (PM/PO) sorcery outside the scope of this article, but generally speaking, ideas will be based on market research, data, previous findings, user feedback, etc.\u003c/li\u003e\u003cli id=\"a77f\"\u003eIdeas there may be many, but it‚Äôs important to keep focus on what moves the company goals forward in a viable context. Sometimes ideas can be really good, but other things need to happen before they become feasible.\u003c/li\u003e\u003cli id=\"f693\"\u003eHaving a low-fidelity design ‚Äî a rough sketch ‚Äî of what the experiment and the user flow would look like can further validate the idea or uncover logical fallacies. At this point, you might already find engineers to be a great asset in the conversation.\u003c/li\u003e\u003cli id=\"e3fc\"\u003eGetting to the planning stage means this is now going ahead full-steam and gets into the upcoming sprint. In our case, we tend to work kanban style, so whoever is next willing and well-suited enough to pick the work up, gets to do so. Every so often you‚Äôll find that the experiment is not just a story, but an entire epic, in which case several engineers might allocate their time to it being led by a project lead.\u003c/li\u003e\u003cli id=\"6822\"\u003eDevelopment is as self-explanatory as it can be. It‚Äôs the coding stage, including writing automated unit, integration and regression tests, adding the feature switches and getting everything into a (or more) \u003ca href=\"https://medium.com/p/3fb5e1ad62d0\" rel=\"noopener\"\u003epull request for code review\u003c/a\u003e.\u003c/li\u003e\u003cli id=\"43b8\"\u003eOur manual QA team member(s) ensure everything has been done to spec and execute some regression testing as well. Given the number of experiments we run, it‚Äôs a much-needed peace of mind to know at least one set of objective eyes checks everything.\u003c/li\u003e\u003cli id=\"ab83\"\u003eReleasing deserves a section of its own, so keep reading. For now, let‚Äôs just say it involves setting the feature switch configuration up to the desired cohorts and enabling them. Once it‚Äôs released, a cleanup task is automatically generated for a later date (see step 12).\u003c/li\u003e\u003cli id=\"35e7\"\u003eSpot checking ensures we‚Äôre on the right track with the experiment, nothing blew up, we‚Äôre not seeing any majorly negative results or collateral damage in signups or upgrades.\u003c/li\u003e\u003cli id=\"e1c1\"\u003eAfter a few weeks, the experiment is stopped, so no more new users are getting exposed to the test. At times, we might allow the users who have been getting the test variant to keep having access to it to further observe user behaviour. This usually lasts no more than another 2‚Äì3 weeks.\u003c/li\u003e\u003cli id=\"fe20\"\u003eEvaluation is all about interpreting the data, understanding the learnings. This is the moment we may decide to release a variant (success) to all users or stick to the control (fail).\u003c/li\u003e\u003cli id=\"c022\"\u003eRollout is essentially the outcome of the evaluation ‚Äî all users get one variant going forward, which from that point on becomes the control.\u003c/li\u003e\u003cli id=\"4f69\"\u003eCleanup is another phase I deemed important enough to highlight in its own section, so do keep reading, but the short of it is, we ensure that all redundant code, tests, and feature switches are done away with. This triggers steps 4, 5 and 6, all culminating in the final step‚Ä¶\u003c/li\u003e\u003cli id=\"d39b\"\u003eEverything is done. The variant is rolled out, the code is cleaned up, and we have either learned something (failed experiment) or achieved something (successful experiment).\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"7492\"\u003eThat‚Äôs the gist of the experiment lifecycle, but as I mentioned, there are a couple of stages there that are really worth digging into more to truly understand some of the complexities and challenges a team like ours can face on a daily basis.\u003c/p\u003e\u003ch2 id=\"9536\"\u003eDealing with feature switches\u003c/h2\u003e\u003cp id=\"3632\"\u003eSome will call them the best human invention since fire, while others, a necessary evil. I, for one, think it‚Äôs a very useful tool, but like every tool, it can be overused or misused. In our case, it‚Äôs invaluable to have the option of setting up a new feature switch for every experiment and variant. The more challenging part is keeping track of them all.\u003c/p\u003e\u003cp id=\"3bcf\"\u003eFor context, we have 9 engineers on the team, and generally speaking, we aim for just as many experiments per sprint. Some quick maths suggests 160 experiments per year, but let‚Äôs go with a more conservative 100 experiments instead. Just assuming you have two variants per experiment already means 300 feature switches. 100 of those control the bucketing of the variants. If not handled correctly, things can get quickly out of hand, so we have devised some ways to avoid that:\u003c/p\u003e\u003cul\u003e\u003cli id=\"8aba\"\u003eAdding a special prefix for feature switches that control the variants.\u003c/li\u003e\u003cli id=\"a0cf\"\u003eUsing team-based feature switch prefixes.\u003c/li\u003e\u003cli id=\"c3de\"\u003eBy making sure each feature switch has a clear ownership marked ‚Äî we use a unique team email address.\u003c/li\u003e\u003cli id=\"43df\"\u003eEach feature switch will have a link to the experiment note or the Jira ticket it refers to.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"88dd\"\u003eThis varies from organisation to organisation, but in Prezi, it‚Äôs mostly the software engineers who add, configure and clean up feature switches. We opted for this approach as it keeps the control of software integrity in engineering‚Äôs hands. We don‚Äôt have to worry about product owners inadvertently breaking regression tests by turning switches on and off at the wrong time.\u003c/p\u003e\u003ch2 id=\"90bd\"\u003eReleasing an experiment\u003c/h2\u003e\u003cp id=\"7d33\"\u003eWhile releasing an experiment will ultimately come down to just flipping a switch ‚Äî a feature switch that is ‚Äî there‚Äôs a lot more to it and how much exactly, can vary from experiment to experiment. Some are a lot more involved than others. As I am writing this, I am working on an A/B test that involves three frontend bundles (think apps), and four different services. Even if you‚Äôre experienced and QA did a fantastic job making sure we haven‚Äôt broken anything, there are still a myriad of things that can fall through the cracks.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"58d4\"\u003eTo make sure releases go as smoothly as possible, we adopted an already standard practice from aviation and medicine ‚Äî a checklist.\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"d11a\"\u003eSurgeons use Surgical Safety Checklists, and pilots rely on Pre-flight Checklists to ensure the best outcomes. We call it a release document, but it‚Äôs really a checklist as clearly stated in the head of each document:\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"be26\"\u003eThis document is meant to be used as a \u003cstrong\u003echecklist\u003c/strong\u003e for the person who‚Äôs driving the release to be able to do it in a calm, collected, professional way. Also meant to act as a document for others, so when troubleshooting is needed, all the information about what was happening during a release is recorded. ‚Äî Prezi internal release document\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"ed44\"\u003eAll such documents are signed off by at least one‚Äî but ideally two ‚Äî senior or lead engineers on the team.\u003c/p\u003e\u003cp id=\"4d97\"\u003eTo some, this might seem excessive, and at times it really is, but in weighing the costs and benefits, as a team we concluded this approach gives us enough value and confidence to stick to it. Just to illustrate some of the items on the checklist, here‚Äôs what we look for:\u003c/p\u003e\u003cul\u003e\u003cli id=\"56e1\"\u003eHave the relevant senior/lead engineers signed off on the plan?\u003c/li\u003e\u003cli id=\"ba26\"\u003eWhat components are meant to be deployed and have they deployed successfully?\u003c/li\u003e\u003cli id=\"5aed\"\u003eHave all relevant teams been notified about our intent to release the experiment?\u003c/li\u003e\u003cli id=\"317c\"\u003eWhat‚Äôs the feature switch configuration?\u003c/li\u003e\u003cli id=\"c50c\"\u003eIs the testing scenario working on production as expected?\u003c/li\u003e\u003cli id=\"30a4\"\u003eIs the A/B test distribution as expected on OpenSearch?\u003c/li\u003e\u003cli id=\"61e2\"\u003eAny unexpected spikes in Grafana?\u003c/li\u003e\u003cli id=\"a2bf\"\u003eAre there any new relevant Sentry error logs?\u003c/li\u003e\u003cli id=\"36fe\"\u003eWhat action(s) to take in case of needing to revert?\u003c/li\u003e\u003cli id=\"8ba9\"\u003eIf all of the above OK, notify internal stakeholders of successful release.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"5336\"\u003eIt‚Äôs a cross your ‚ÄúT‚Äùs and dot your ‚ÄúI‚Äùs kind of exercise, but out of it we get a log we can reference later and the assurance that anything that could have been prevented, has been prevented because, you know‚Ä¶ Murphy‚Äôs Law. üòâ\u003c/p\u003e\u003cp id=\"2702\"\u003eHaving released, however, doesn‚Äôt mean we‚Äôre done. Far from it. There‚Äôs cleanup, and it‚Äôs such an important part of what our team does that I felt it deserved its own section, so without further ado‚Ä¶\u003c/p\u003e\u003ch2 id=\"092a\"\u003eCleaning up\u003c/h2\u003e\u003cp id=\"c388\"\u003eI hate doing the dishes, so by week‚Äôs end there‚Äôs a literal pile of them waiting to be washed. Now, remember those 300 feature switches? That‚Äôs precisely the pile we desperately want to avoid. Because feature switches as useful as they are, they quickly pollute the code to a point it becomes unmaintainable, which would result in us losing more and more velocity over time. As a team, you can easily grind to a screeching halt if code is not maintained, and as an experiments team, we‚Äôre particularly prone to having this happen if we‚Äôre not vigilant.\u003c/p\u003e\u003cp id=\"5529\"\u003eOne way we‚Äôre working on preventing such a situation is by automatically creating cleanup tickets for each experiment. Jira isn‚Äôt so bad after all, aye? üòÑ You see, once an experiment goes live, it will stay live for at least a couple of weeks. Gathering useful enough data to make pragmatic product decisions, doesn‚Äôt happen instantly, so usually a few weeks after the A/B test release a decision gets made. Either we stick to what we had before ‚Äî aka we keep the control variants ‚Äî or we keep one of the other variants. Often it‚Äôs just one, but there are times when an A/B test has a total of as many as four variants. Let me pseudocode an example:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"a79d\"\u003eif(isActive(\u0026#39;amazing-feature-variant-a\u0026#39;)){\u003cbr/\u003e   \u0026lt;ABTestComponentVariantA\u0026gt;...\u0026lt;/ABTestComponentVariantA\u0026gt;\u003cbr/\u003e} else if(isActive(\u0026#39;amazing-feature-variant-b\u0026#39;)){\u003cbr/\u003e   \u0026lt;ABTestComponentVariantB\u0026gt;...\u0026lt;/ABTestComponentVariantB\u0026gt;\u003cbr/\u003e} else if(isActive(\u0026#39;amazing-feature-variant-c\u0026#39;)){\u003cbr/\u003e   \u0026lt;ABTestComponentVariantC\u0026gt;...\u0026lt;/ABTestComponentVariantC\u0026gt;\u003cbr/\u003e} else {\u003cbr/\u003e   \u0026lt;ControlVariant\u0026gt;...\u0026lt;/ControlVariant\u0026gt;\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"f3ee\"\u003eRegardless of which one we keep, three of those have to go. You can imagine, of course, that often times an A/B test is far more involved than just showing a component or not, so cleanup can become quite an undertaking, as you want to make sure you understand the variants that have been added, the relationship with the rest of the codebase and the overall user flows, so cleaning up doesn‚Äôt result in collateral damage. This typically means editing tests as well.\u003c/p\u003e\u003cp id=\"a18b\"\u003eYou might wonder, in case of a lost A/B test where we end up sticking to control ‚Äî to what we had before ‚Äî can‚Äôt we just revert to the original PR? The answer is maybe, perhaps partially or not at all for the following reasons:\u003c/p\u003e\u003cul\u003e\u003cli id=\"296a\"\u003eYou might be able to revert if the initial change was very clean, and other changes to those files haven‚Äôt been done since. In a high-traffic codebase, that‚Äôs quite unlikely to happen, though.\u003c/li\u003e\u003cli id=\"d8e5\"\u003eYou might only be able to do a partial revert if some of the changes happened in a low-traffic codebase, while others in higher-traffic codebases. The A/B test I am working on right now touches several repositories. I could imagine one or two of those repositories seeing light enough traffic that I could just revert, but the rest would require a more involved approach.\u003c/li\u003e\u003cli id=\"acdb\"\u003eIf you‚Äôre only dealing with high-traffic codebases, you simply don‚Äôt have this option, but you will also find cases where you added code for one of the variants that‚Äôs actually going to be useful for future work. Maybe you wrote a nice utility function, or refactored some code as part of the A/B test to make your life easier. You surely don‚Äôt want to revert that.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"4855\"\u003eWhen all is clean and done\u003c/h2\u003e\u003cp id=\"4feb\"\u003eI won‚Äôt gaslight you into thinking we don‚Äôt deal with technical debt, awkward tech stacks, or breaking pipelines like every other team and engineering organisation out there. We do, and some of our challenges aren‚Äôt even new to many developers out there. It‚Äôs more like a unique flavour of what other teams deal with daily, and it‚Äôs unique enough that we found ourselves having to fine-tune how we do things, improve our processes, and continuously refine and shape ourselves as engineers into individuals reflecting the previously illustrated skills (mindset) table.\u003c/p\u003e\u003cp id=\"9fe2\"\u003eThis is what has worked for us. This is what gets things done. For now. Just like we experiment with features, we experiment with ourselves as individuals and as a team. Sometimes that means we succeed, other times it means we learn and move on, or we learn \u003cem\u003eto\u003c/em\u003e move on. It‚Äôs a journey, and it requires stamina, but ultimately, it‚Äôs well-worth the effort. So, yes, A/B tests for the win! üéâ\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "26 min read",
  "publishedTime": "2024-07-09T13:21:02.17Z",
  "modifiedTime": null
}
