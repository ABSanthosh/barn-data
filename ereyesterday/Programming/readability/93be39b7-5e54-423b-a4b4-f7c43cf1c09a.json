{
  "id": "93be39b7-5e54-423b-a4b4-f7c43cf1c09a",
  "title": "Introducing Netflix’s TimeSeries Data Abstraction Layer",
  "link": "https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Tue, 08 Oct 2024 17:05:36 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": null,
  "byline": "Netflix Technology Blog",
  "length": 30054,
  "excerpt": "As Netflix continues to expand and diversify into various sectors like Video on Demand and Gaming, the ability to ingest and store vast amounts of temporal data — often reaching petabytes — with…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "By Rajiv Shringi, Vinay Chella, Kaidan Fullerton, Oleksii Tkachuk, Joey LynchIntroductionAs Netflix continues to expand and diversify into various sectors like Video on Demand and Gaming, the ability to ingest and store vast amounts of temporal data — often reaching petabytes — with millisecond access latency has become increasingly vital. In previous blog posts, we introduced the Key-Value Data Abstraction Layer and the Data Gateway Platform, both of which are integral to Netflix’s data architecture. The Key-Value Abstraction offers a flexible, scalable solution for storing and accessing structured key-value data, while the Data Gateway Platform provides essential infrastructure for protecting, configuring, and deploying the data tier.Building on these foundational abstractions, we developed the TimeSeries Abstraction — a versatile and scalable solution designed to efficiently store and query large volumes of temporal event data with low millisecond latencies, all in a cost-effective manner across various use cases.In this post, we will delve into the architecture, design principles, and real-world applications of the TimeSeries Abstraction, demonstrating how it enhances our platform’s ability to manage temporal data at scale.Note: Contrary to what the name may suggest, this system is not built as a general-purpose time series database. We do not use it for metrics, histograms, timers, or any such near-real time analytics use case. Those use cases are well served by the Netflix Atlas telemetry system. Instead, we focus on addressing the challenge of storing and accessing extremely high-throughput, immutable temporal event data in a low-latency and cost-efficient manner.ChallengesAt Netflix, temporal data is continuously generated and utilized, whether from user interactions like video-play events, asset impressions, or complex micro-service network activities. Effectively managing this data at scale to extract valuable insights is crucial for ensuring optimal user experiences and system reliability.However, storing and querying such data presents a unique set of challenges:High Throughput: Managing up to 10 million writes per second while maintaining high availability.Efficient Querying in Large Datasets: Storing petabytes of data while ensuring primary key reads return results within low double-digit milliseconds, and supporting searches and aggregations across multiple secondary attributes.Global Reads and Writes: Facilitating read and write operations from anywhere in the world with adjustable consistency models.Tunable Configuration: Offering the ability to partition datasets in either a single-tenant or multi-tenant datastore, with options to adjust various dataset aspects such as retention and consistency.Handling Bursty Traffic: Managing significant traffic spikes during high-demand events, such as new content launches or regional failovers.Cost Efficiency: Reducing the cost per byte and per operation to optimize long-term retention while minimizing infrastructure expenses, which can amount to millions of dollars for Netflix.TimeSeries AbstractionThe TimeSeries Abstraction was developed to meet these requirements, built around the following core design principles:Partitioned Data: Data is partitioned using a unique temporal partitioning strategy combined with an event bucketing approach to efficiently manage bursty workloads and streamline queries.Flexible Storage: The service is designed to integrate with various storage backends, including Apache Cassandra and Elasticsearch, allowing Netflix to customize storage solutions based on specific use case requirements.Configurability: TimeSeries offers a range of tunable options for each dataset, providing the flexibility needed to accommodate a wide array of use cases.Scalability: The architecture supports both horizontal and vertical scaling, enabling the system to handle increasing throughput and data volumes as Netflix expands its user base and services.Sharded Infrastructure: Leveraging the Data Gateway Platform, we can deploy single-tenant and/or multi-tenant infrastructure with the necessary access and traffic isolation.Let’s dive into the various aspects of this abstraction.Data ModelWe follow a unique event data model that encapsulates all the data we want to capture for events, while allowing us to query them efficiently.Let’s start with the smallest unit of data in the abstraction and work our way up.Event Item: An event item is a key-value pair that users use to store data for a given event. For example: {“device_type”: “ios”}.Event: An event is a structured collection of one or more such event items. An event occurs at a specific point in time and is identified by a client-generated timestamp and an event identifier (such as a UUID). This combination of event_time and event_id also forms part of the unique idempotency key for the event, enabling users to safely retry requests.Time Series ID: A time_series_id is a collection of one or more such events over the dataset’s retention period. For instance, a device_id would store all events occurring for a given device over the retention period. All events are immutable, and the TimeSeries service only ever appends events to a given time series ID.Namespace: A namespace is a collection of time series IDs and event data, representing the complete TimeSeries dataset. Users can create one or more namespaces for each of their use cases. The abstraction applies various tunable options at the namespace level, which we will discuss further when we explore the service’s control plane.APIThe abstraction provides the following APIs to interact with the event data.WriteEventRecordsSync: This endpoint writes a batch of events and sends back a durability acknowledgement to the client. This is used in cases where users require a guarantee of durability.WriteEventRecords: This is the fire-and-forget version of the above endpoint. It enqueues a batch of events without the durability acknowledgement. This is used in cases like logging or tracing, where users care more about throughput and can tolerate a small amount of data loss.{ \"namespace\": \"my_dataset\", \"events\": [ { \"timeSeriesId\": \"profile100\", \"eventTime\": \"2024-10-03T21:24:23.988Z\", \"eventId\": \"550e8400-e29b-41d4-a716-446655440000\", \"eventItems\": [ { \"eventItemKey\": \"deviceType\", \"eventItemValue\": \"aW9z\" }, { \"eventItemKey\": \"deviceMetadata\", \"eventItemValue\": \"c29tZSBtZXRhZGF0YQ==\" } ] }, { \"timeSeriesId\": \"profile100\", \"eventTime\": \"2024-10-03T21:23:30.000Z\", \"eventId\": \"123e4567-e89b-12d3-a456-426614174000\", \"eventItems\": [ { \"eventItemKey\": \"deviceType\", \"eventItemValue\": \"YW5kcm9pZA==\" } ] } ]}ReadEventRecords: Given a combination of a namespace, a timeSeriesId, a timeInterval, and optional eventFilters, this endpoint returns all the matching events, sorted descending by event_time, with low millisecond latency.{ \"namespace\": \"my_dataset\", \"timeSeriesId\": \"profile100\", \"timeInterval\": { \"start\": \"2024-10-02T21:00:00.000Z\", \"end\": \"2024-10-03T21:00:00.000Z\" }, \"eventFilters\": [ { \"matchEventItemKey\": \"deviceType\", \"matchEventItemValue\": \"aW9z\" } ], \"pageSize\": 100, \"totalRecordLimit\": 1000}SearchEventRecords: Given a search criteria and a time interval, this endpoint returns all the matching events. These use cases are fine with eventually consistent reads.{ \"namespace\": \"my_dataset\", \"timeInterval\": { \"start\": \"2024-10-02T21:00:00.000Z\", \"end\": \"2024-10-03T21:00:00.000Z\" }, \"searchQuery\": { \"booleanQuery\": { \"searchQuery\": [ { \"equals\": { \"eventItemKey\": \"deviceType\", \"eventItemValue\": \"aW9z\" } }, { \"range\": { \"eventItemKey\": \"deviceRegistrationTimestamp\", \"lowerBound\": { \"eventItemValue\": \"MjAyNC0xMC0wMlQwMDowMDowMC4wMDBa\", \"inclusive\": true }, \"upperBound\": { \"eventItemValue\": \"MjAyNC0xMC0wM1QwMDowMDowMC4wMDBa\" } } } ], \"operator\": \"AND\" } }, \"pageSize\": 100, \"totalRecordLimit\": 1000}AggregateEventRecords: Given a search criteria and an aggregation mode (e.g. DistinctAggregation) , this endpoint performs the given aggregation within a given time interval. Similar to the Search endpoint, users can tolerate eventual consistency and a potentially higher latency (in seconds).{ \"namespace\": \"my_dataset\", \"timeInterval\": { \"start\": \"2024-10-02T21:00:00.000Z\", \"end\": \"2024-10-03T21:00:00.000Z\" }, \"searchQuery\": {...some search criteria...}, \"aggregationQuery\": { \"distinct\": { \"eventItemKey\": \"deviceType\", \"pageSize\": 100 } }}In the subsequent sections, we will talk about how we interact with this data at the storage layer.Storage LayerThe storage layer for TimeSeries comprises a primary data store and an optional index data store. The primary data store ensures data durability during writes and is used for primary read operations, while the index data store is utilized for search and aggregate operations. At Netflix, Apache Cassandra is the preferred choice for storing durable data in high-throughput scenarios, while Elasticsearch is the preferred data store for indexing. However, similar to our approach with the API, the storage layer is not tightly coupled to these specific data stores. Instead, we define storage API contracts that must be fulfilled, allowing us the flexibility to replace the underlying data stores as needed.Primary DatastoreIn this section, we will talk about how we leverage Apache Cassandra for TimeSeries use cases.Partitioning SchemeAt Netflix’s scale, the continuous influx of event data can quickly overwhelm traditional databases. Temporal partitioning addresses this challenge by dividing the data into manageable chunks based on time intervals, such as hourly, daily, or monthly windows. This approach enables efficient querying of specific time ranges without the need to scan the entire dataset. It also allows Netflix to archive, compress, or delete older data efficiently, optimizing both storage and query performance. Additionally, this partitioning mitigates the performance issues typically associated with wide partitions in Cassandra. By employing this strategy, we can operate at much higher disk utilization, as it reduces the need to reserve large amounts of disk space for compactions, thereby saving costs.Here is what it looks like :Time Slice: A time slice is the unit of data retention and maps directly to a Cassandra table. We create multiple such time slices, each covering a specific interval of time. An event lands in one of these slices based on the event_time. These slices are joined with no time gaps in between, with operations being start-inclusive and end-exclusive, ensuring that all data lands in one of the slices. By utilizing these time slices, we can efficiently implement retention by dropping entire tables, which reduces storage space and saves on costs.Why not use row-based Time-To-Live (TTL)?Using TTL on individual events would generate a significant number of tombstones in Cassandra, degrading performance, especially during range scans. By employing discrete time slices and dropping them, we avoid the tombstone issue entirely. The tradeoff is that data may be retained slightly longer than necessary, as an entire table’s time range must fall outside the retention window before it can be dropped. Additionally, TTLs are difficult to adjust later, whereas TimeSeries can extend the dataset retention instantly with a single control plane operation.Time Buckets: Within a time slice, data is further partitioned into time buckets. This facilitates effective range scans by allowing us to target specific time buckets for a given query range. The tradeoff is that if a user wants to read the entire range of data over a large time period, we must scan many partitions. We mitigate potential latency by scanning these partitions in parallel and aggregating the data at the end. In most cases, the advantage of targeting smaller data subsets outweighs the read amplification from these scatter-gather operations. Typically, users read a smaller subset of data rather than the entire retention range.Event Buckets: To manage extremely high-throughput write operations, which may result in a burst of writes for a given time series within a short period, we further divide the time bucket into event buckets. This prevents overloading the same partition for a given time range and also reduces partition sizes further, albeit with a slight increase in read amplification.Note: With Cassandra 4.x onwards, we notice a substantial improvement in the performance of scanning a range of data in a wide partition. See Future Enhancements at the end to see the Dynamic Event bucketing work that aims to take advantage of this.Storage TablesWe use two kinds of tablesData tables: These are the time slices that store the actual event data.Metadata table: This table stores information about how each time slice is configured per namespace.Data tablesThe partition key enables splitting events for a time_series_id over a range of time_bucket(s) and event_bucket(s), thus mitigating hot partitions, while the clustering key allows us to keep data sorted on disk in the order we almost always want to read it. The value_metadata column stores metadata for the event_item_value such as compression.Writing to the data table:User writes will land in a given time slice, time bucket, and event bucket as a factor of the event_time attached to the event. This factor is dictated by the control plane configuration of a given namespace.For example:During this process, the writer makes decisions on how to handle the data before writing, such as whether to compress it. The value_metadata column records any such post-processing actions, ensuring that the reader can accurately interpret the data.Reading from the data table:The below illustration depicts at a high-level on how we scatter-gather the reads from multiple partitions and join the result set at the end to return the final result.Metadata tableThis table stores the configuration data about the time slices for a given namespace.Note the following:No Time Gaps: The end_time of a given time slice overlaps with the start_time of the next time slice, ensuring all events find a home.Retention: The status indicates which tables fall inside and outside of the retention window.Flexible: This metadata can be adjusted per time slice, allowing us to tune the partition settings of future time slices based on observed data patterns in the current time slice.There is a lot more information that can be stored into the metadata column (e.g., compaction settings for the table), but we only show the partition settings here for brevity.Index DatastoreTo support secondary access patterns via non-primary key attributes, we index data into Elasticsearch. Users can configure a list of attributes per namespace that they wish to search and/or aggregate data on. The service extracts these fields from events as they stream in, indexing the resultant documents into Elasticsearch. Depending on the throughput, we may use Elasticsearch as a reverse index, retrieving the full data from Cassandra, or we may store the entire source data directly in Elasticsearch.Note: Again, users are never directly exposed to Elasticsearch, just like they are not directly exposed to Cassandra. Instead, they interact with the Search and Aggregate API endpoints that translate a given query to that needed for the underlying datastore.In the next section, we will talk about how we configure these data stores for different datasets.Control PlaneThe data plane is responsible for executing the read and write operations, while the control plane configures every aspect of a namespace’s behavior. The data plane communicates with the TimeSeries control stack, which manages this configuration information. In turn, the TimeSeries control stack interacts with a sharded Data Gateway Platform Control Plane that oversees control configurations for all abstractions and namespaces.Separating the responsibilities of the data plane and control plane helps maintain the high availability of our data plane, as the control plane takes on tasks that may require some form of schema consensus from the underlying data stores.Namespace ConfigurationThe below configuration snippet demonstrates the immense flexibility of the service and how we can tune several things per namespace using our control plane.\"persistence_configuration\": [ { \"id\": \"PRIMARY_STORAGE\", \"physical_storage\": { \"type\": \"CASSANDRA\", // type of primary storage \"cluster\": \"cass_dgw_ts_tracing\", // physical cluster name \"dataset\": \"tracing_default\" // maps to the keyspace }, \"config\": { \"timePartition\": { \"secondsPerTimeSlice\": \"129600\", // width of a time slice \"secondPerTimeBucket\": \"3600\", // width of a time bucket \"eventBuckets\": 4 // how many event buckets within }, \"queueBuffering\": { \"coalesce\": \"1s\", // how long to coalesce writes \"bufferCapacity\": 4194304 // queue capacity in bytes }, \"consistencyScope\": \"LOCAL\", // single-region/multi-region \"consistencyTarget\": \"EVENTUAL\", // read/write consistency \"acceptLimit\": \"129600s\" // how far back writes are allowed }, \"lifecycleConfigs\": { \"lifecycleConfig\": [ // Primary store data retention { \"type\": \"retention\", \"config\": { \"close_after\": \"1296000s\", // close for reads/writes \"delete_after\": \"1382400s\" // drop time slice } } ] } }, { \"id\": \"INDEX_STORAGE\", \"physicalStorage\": { \"type\": \"ELASTICSEARCH\", // type of index storage \"cluster\": \"es_dgw_ts_tracing\", // ES cluster name \"dataset\": \"tracing_default_useast1\" // base index name }, \"config\": { \"timePartition\": { \"secondsPerSlice\": \"129600\" // width of the index slice }, \"consistencyScope\": \"LOCAL\", \"consistencyTarget\": \"EVENTUAL\", // how should we read/write data \"acceptLimit\": \"129600s\", // how far back writes are allowed \"indexConfig\": { \"fieldMapping\": { // fields to extract to index \"tags.nf.app\": \"KEYWORD\", \"tags.duration\": \"INTEGER\", \"tags.enabled\": \"BOOLEAN\" }, \"refreshInterval\": \"60s\" // Index related settings } }, \"lifecycleConfigs\": { \"lifecycleConfig\": [ { \"type\": \"retention\", // Index retention settings \"config\": { \"close_after\": \"1296000s\", \"delete_after\": \"1382400s\" } } ] } }]Provisioning InfrastructureWith so many different parameters, we need automated provisioning workflows to deduce the best settings for a given workload. When users want to create their namespaces, they specify a list of workload desires, which the automation translates into concrete infrastructure and related control plane configuration. We highly encourage you to watch this ApacheCon talk, by one of our stunning colleagues Joey Lynch, on how we achieve this. We may go into detail on this subject in one of our future blog posts.Once the system provisions the initial infrastructure, it then scales in response to the user workload. The next section describes how this is achieved.ScalabilityOur users may operate with limited information at the time of provisioning their namespaces, resulting in best-effort provisioning estimates. Further, evolving use-cases may introduce new throughput requirements over time. Here’s how we manage this:Horizontal scaling: TimeSeries server instances can auto-scale up and down as per attached scaling policies to meet the traffic demand. The storage server capacity can be recomputed to accommodate changing requirements using our capacity planner.Vertical scaling: We may also choose to vertically scale our TimeSeries server instances or our storage instances to get greater CPU, RAM and/or attached storage capacity.Scaling disk: We may attach EBS to store data if the capacity planner prefers infrastructure that offers larger storage at a lower cost rather than SSDs optimized for latency. In such cases, we deploy jobs to scale the EBS volume when the disk storage reaches a certain percentage threshold.Re-partitioning data: Inaccurate workload estimates can lead to over or under-partitioning of our datasets. TimeSeries control-plane can adjust the partitioning configuration for upcoming time slices, once we realize the nature of data in the wild (via partition histograms). In the future we plan to support re-partitioning of older data and dynamic partitioning of current data.Design PrinciplesSo far, we have seen how TimeSeries stores, configures and interacts with event datasets. Let’s see how we apply different techniques to improve the performance of our operations and provide better guarantees.Event IdempotencyWe prefer to bake in idempotency in all mutation endpoints, so that users can retry or hedge their requests safely. Hedging is when the client sends an identical competing request to the server, if the original request does not come back with a response in an expected amount of time. The client then responds with whichever request completes first. This is done to keep the tail latencies for an application relatively low. This can only be done safely if the mutations are idempotent. For TimeSeries, the combination of event_time, event_id and event_item_key form the idempotency key for a given time_series_id event.SLO-based HedgingWe assign Service Level Objectives (SLO) targets for different endpoints within TimeSeries, as an indication of what we think the performance of those endpoints should be for a given namespace. We can then hedge a request if the response does not come back in that configured amount of time.\"slos\": { \"read\": { // SLOs per endpoint \"latency\": { \"target\": \"0.5s\", // hedge around this number \"max\": \"1s\" // time-out around this number } }, \"write\": { \"latency\": { \"target\": \"0.01s\", \"max\": \"0.05s\" } }}Partial ReturnSometimes, a client may be sensitive to latency and willing to accept a partial result set. A real-world example of this is real-time frequency capping. Precision is not critical in this case, but if the response is delayed, it becomes practically useless to the upstream client. Therefore, the client prefers to work with whatever data has been collected so far rather than timing out while waiting for all the data. The TimeSeries client supports partial returns around SLOs for this purpose. Importantly, we still maintain the latest order of events in this partial fetch.Adaptive PaginationAll reads start with a default fanout factor, scanning 8 partition buckets in parallel. However, if the service layer determines that the time_series dataset is dense — i.e., most reads are satisfied by reading the first few partition buckets — then it dynamically adjusts the fanout factor of future reads in order to reduce the read amplification on the underlying datastore. Conversely, if the dataset is sparse, we may want to increase this limit with a reasonable upper bound.Limited Write WindowIn most cases, the active range for writing data is smaller than the range for reading data — i.e., we want a range of time to become immutable as soon as possible so that we can apply optimizations on top of it. We control this by having a configurable “acceptLimit” parameter that prevents users from writing events older than this time limit. For example, an accept limit of 4 hours means that users cannot write events older than now() — 4 hours. We sometimes raise this limit for backfilling historical data, but it is tuned back down for regular write operations. Once a range of data becomes immutable, we can safely do things like caching, compressing, and compacting it for reads.Buffering WritesWe frequently leverage this service for handling bursty workloads. Rather than overwhelming the underlying datastore with this load all at once, we aim to distribute it more evenly by allowing events to coalesce over short durations (typically seconds). These events accumulate in in-memory queues running on each instance. Dedicated consumers then steadily drain these queues, grouping the events by their partition key, and batching the writes to the underlying datastore.The queues are tailored to each datastore since their operational characteristics depend on the specific datastore being written to. For instance, the batch size for writing to Cassandra is significantly smaller than that for indexing into Elasticsearch, leading to different drain rates and batch sizes for the associated consumers.While using in-memory queues does increase JVM garbage collection, we have experienced substantial improvements by transitioning to JDK 21 with ZGC. To illustrate the impact, ZGC has reduced our tail latencies by an impressive 86%:Because we use in-memory queues, we are prone to losing events in case of an instance crash. As such, these queues are only used for use cases that can tolerate some amount of data loss .e.g. tracing/logging. For use cases that need guaranteed durability and/or read-after-write consistency, these queues are effectively disabled and writes are flushed to the data store almost immediately.Dynamic CompactionOnce a time slice exits the active write window, we can leverage the immutability of the data to optimize it for read performance. This process may involve re-compacting immutable data using optimal compaction strategies, dynamically shrinking and/or splitting shards to optimize system resources, and other similar techniques to ensure fast and reliable performance.The following section provides a glimpse into the real-world performance of some of our TimeSeries datasets.Real-world PerformanceThe service can write data in the order of low single digit millisecondswhile consistently maintaining stable point-read latencies:At the time of writing this blog, the service was processing close to 15 million events/second across all the different datasets at peak globally.Time Series Usage @ NetflixThe TimeSeries Abstraction plays a vital role across key services at Netflix. Here are some impactful use cases:Tracing and Insights: Logs traces across all apps and micro-services within Netflix, to understand service-to-service communication, aid in debugging of issues, and answer support requests.User Interaction Tracking: Tracks millions of user interactions — such as video playbacks, searches, and content engagement — providing insights that enhance Netflix’s recommendation algorithms in real-time and improve the overall user experience.Feature Rollout and Performance Analysis: Tracks the rollout and performance of new product features, enabling Netflix engineers to measure how users engage with features, which powers data-driven decisions about future improvements.Asset Impression Tracking and Optimization: Tracks asset impressions ensuring content and assets are delivered efficiently while providing real-time feedback for optimizations.Billing and Subscription Management: Stores historical data related to billing and subscription management, ensuring accuracy in transaction records and supporting customer service inquiries.and more…Future EnhancementsAs the use cases evolve, and the need to make the abstraction even more cost effective grows, we aim to make many improvements to the service in the upcoming months. Some of them are:Tiered Storage for Cost Efficiency: Support moving older, lesser-accessed data into cheaper object storage that has higher time to first byte, potentially saving Netflix millions of dollars.Dynamic Event Bucketing: Support real-time partitioning of keys into optimally-sized partitions as events stream in, rather than having a somewhat static configuration at the time of provisioning a namespace. This strategy has a huge advantage of not partitioning time_series_ids that don’t need it, thus saving the overall cost of read amplification. Also, with Cassandra 4.x, we have noted major improvements in reading a subset of data in a wide partition that could lead us to be less aggressive with partitioning the entire dataset ahead of time.Caching: Take advantage of immutability of data and cache it intelligently for discrete time ranges.Count and other Aggregations: Some users are only interested in counting events in a given time interval rather than fetching all the event data for it.ConclusionThe TimeSeries Abstraction is a vital component of Netflix’s online data infrastructure, playing a crucial role in supporting both real-time and long-term decision-making. Whether it’s monitoring system performance during high-traffic events or optimizing user engagement through behavior analytics, TimeSeries Abstraction ensures that Netflix operates seamlessly and efficiently on a global scale.As Netflix continues to innovate and expand into new verticals, the TimeSeries Abstraction will remain a cornerstone of our platform, helping us push the boundaries of what’s possible in streaming and beyond.Stay tuned for Part 2, where we’ll introduce our Distributed Counter Abstraction, a key element of Netflix’s Composite Abstractions, built on top of the TimeSeries Abstraction.AcknowledgmentsSpecial thanks to our stunning colleagues who contributed to TimeSeries Abstraction’s success: Tom DeVoe Mengqing Wang, Kartik Sathyanarayanan, Jordan West, Matt Lehman, Cheng Wang, Chris Lohfink .",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*jl30Jl559Fnd29in",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page-----31552f6326f8--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://netflixtechblog.com/?source=post_page-----31552f6326f8--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix TechBlog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"b30e\"\u003eBy \u003ca href=\"https://www.linkedin.com/in/rajiv-shringi\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRajiv Shringi\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/vinaychella/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVinay Chella\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/kaidanfullerton/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKaidan Fullerton\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/oleksii-tkachuk-98b47375/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOleksii Tkachuk\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/joseph-lynch-9976a431/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJoey Lynch\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"b44d\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\u003cp id=\"2bf4\"\u003eAs Netflix continues to expand and diversify into various sectors like \u003cstrong\u003eVideo on Demand\u003c/strong\u003e and \u003cstrong\u003eGaming\u003c/strong\u003e, the ability to ingest and store vast amounts of temporal data — often reaching petabytes — with millisecond access latency has become increasingly vital. In previous blog posts, we introduced the \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30\"\u003e\u003cstrong\u003eKey-Value Data Abstraction Layer\u003c/strong\u003e\u003c/a\u003e and the \u003ca href=\"https://netflixtechblog.medium.com/data-gateway-a-platform-for-growing-and-protecting-the-data-tier-f1ed8db8f5c6\" rel=\"noopener\"\u003e\u003cstrong\u003eData Gateway Platform\u003c/strong\u003e\u003c/a\u003e, both of which are integral to Netflix’s data architecture. The Key-Value Abstraction offers a flexible, scalable solution for storing and accessing structured key-value data, while the Data Gateway Platform provides essential infrastructure for protecting, configuring, and deploying the data tier.\u003c/p\u003e\u003cp id=\"f295\"\u003eBuilding on these foundational abstractions, we developed the \u003cstrong\u003eTimeSeries Abstraction\u003c/strong\u003e — a versatile and scalable solution designed to efficiently store and query large volumes of temporal event data with low millisecond latencies, all in a cost-effective manner across various use cases.\u003c/p\u003e\u003cp id=\"f9ce\"\u003eIn this post, we will delve into the architecture, design principles, and real-world applications of the \u003cstrong\u003eTimeSeries Abstraction\u003c/strong\u003e, demonstrating how it enhances our platform’s ability to manage temporal data at scale.\u003c/p\u003e\u003cp id=\"f726\"\u003e\u003cstrong\u003eNote: \u003c/strong\u003e\u003cem\u003eContrary to what the name may suggest, this system is not built as a general-purpose time series database. We do not use it for metrics, histograms, timers, or any such near-real time analytics use case. Those use cases are well served by the Netflix \u003c/em\u003e\u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a\"\u003e\u003cem\u003eAtlas\u003c/em\u003e\u003c/a\u003e\u003cem\u003e telemetry system. Instead, we focus on addressing the challenge of storing and accessing extremely high-throughput, immutable temporal event data in a low-latency and cost-efficient manner.\u003c/em\u003e\u003c/p\u003e\u003ch2 id=\"a578\"\u003eChallenges\u003c/h2\u003e\u003cp id=\"ce8f\"\u003eAt Netflix, temporal data is continuously generated and utilized, whether from user interactions like video-play events, asset impressions, or complex micro-service network activities. Effectively managing this data at scale to extract valuable insights is crucial for ensuring optimal user experiences and system reliability.\u003c/p\u003e\u003cp id=\"11e4\"\u003eHowever, storing and querying such data presents a unique set of challenges:\u003c/p\u003e\u003cul\u003e\u003cli id=\"ef3e\"\u003e\u003cstrong\u003eHigh Throughput\u003c/strong\u003e: Managing up to 10 million writes per second while maintaining high availability.\u003c/li\u003e\u003cli id=\"35ee\"\u003e\u003cstrong\u003eEfficient Querying in Large Datasets\u003c/strong\u003e: Storing petabytes of data while ensuring primary key reads return results within low double-digit milliseconds, and supporting searches and aggregations across multiple secondary attributes.\u003c/li\u003e\u003cli id=\"6d28\"\u003e\u003cstrong\u003eGlobal Reads and Writes\u003c/strong\u003e: Facilitating read and write operations from anywhere in the world with adjustable consistency models.\u003c/li\u003e\u003cli id=\"89d9\"\u003e\u003cstrong\u003eTunable Configuration\u003c/strong\u003e: Offering the ability to partition datasets in either a single-tenant or multi-tenant datastore, with options to adjust various dataset aspects such as retention and consistency.\u003c/li\u003e\u003cli id=\"78f2\"\u003e\u003cstrong\u003eHandling Bursty Traffic\u003c/strong\u003e: Managing significant traffic spikes during high-demand events, such as new content launches or regional failovers.\u003c/li\u003e\u003cli id=\"7ea6\"\u003e\u003cstrong\u003eCost Efficiency\u003c/strong\u003e: Reducing the cost per byte and per operation to optimize long-term retention while minimizing infrastructure expenses, which can amount to millions of dollars for Netflix.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"264f\"\u003eTimeSeries Abstraction\u003c/h2\u003e\u003cp id=\"dc2a\"\u003eThe TimeSeries Abstraction was developed to meet these requirements, built around the following core design principles:\u003c/p\u003e\u003cul\u003e\u003cli id=\"1e7d\"\u003e\u003cstrong\u003ePartitioned Data\u003c/strong\u003e: Data is partitioned using a unique temporal partitioning strategy combined with an event bucketing approach to efficiently manage bursty workloads and streamline queries.\u003c/li\u003e\u003cli id=\"caa3\"\u003e\u003cstrong\u003eFlexible Storage\u003c/strong\u003e: The service is designed to integrate with various storage backends, including \u003ca href=\"https://cassandra.apache.org/_/index.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eApache Cassandra\u003c/a\u003e and \u003ca href=\"https://www.elastic.co/elasticsearch\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eElasticsearch\u003c/a\u003e, allowing Netflix to customize storage solutions based on specific use case requirements.\u003c/li\u003e\u003cli id=\"3db7\"\u003e\u003cstrong\u003eConfigurability\u003c/strong\u003e: TimeSeries offers a range of tunable options for each dataset, providing the flexibility needed to accommodate a wide array of use cases.\u003c/li\u003e\u003cli id=\"b931\"\u003e\u003cstrong\u003eScalability\u003c/strong\u003e: The architecture supports both horizontal and vertical scaling, enabling the system to handle increasing throughput and data volumes as Netflix expands its user base and services.\u003c/li\u003e\u003cli id=\"b382\"\u003e\u003cstrong\u003eSharded Infrastructure\u003c/strong\u003e: Leveraging the \u003cstrong\u003eData Gateway Platform\u003c/strong\u003e, we can deploy single-tenant and/or multi-tenant infrastructure with the necessary access and traffic isolation.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"2a47\"\u003eLet’s dive into the various aspects of this abstraction.\u003c/p\u003e\u003ch2 id=\"ea66\"\u003eData Model\u003c/h2\u003e\u003cp id=\"fbf3\"\u003eWe follow a unique event data model that encapsulates all the data we want to capture for events, while allowing us to query them efficiently.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"7228\"\u003eLet’s start with the smallest unit of data in the abstraction and work our way up.\u003c/p\u003e\u003cul\u003e\u003cli id=\"cf78\"\u003e\u003cstrong\u003eEvent Item\u003c/strong\u003e: An event item is a key-value pair that users use to store data for a given event. For example: \u003cem\u003e{“device_type”: “ios”}\u003c/em\u003e.\u003c/li\u003e\u003cli id=\"55c6\"\u003e\u003cstrong\u003eEvent\u003c/strong\u003e: An event is a structured collection of one or more such event items. An event occurs at a specific point in time and is identified by a client-generated timestamp and an event identifier (such as a UUID). This combination of \u003cstrong\u003eevent_time\u003c/strong\u003e and \u003cstrong\u003eevent_id\u003c/strong\u003e also forms part of the unique idempotency key for the event, enabling users to safely retry requests.\u003c/li\u003e\u003cli id=\"9145\"\u003e\u003cstrong\u003eTime Series ID\u003c/strong\u003e: A \u003cstrong\u003etime_series_id\u003c/strong\u003e is a collection of one or more such events over the dataset’s retention period. For instance, a \u003cstrong\u003edevice_id\u003c/strong\u003e would store all events occurring for a given device over the retention period. All events are immutable, and the TimeSeries service only ever appends events to a given time series ID.\u003c/li\u003e\u003cli id=\"ef6e\"\u003e\u003cstrong\u003eNamespace\u003c/strong\u003e: A namespace is a collection of time series IDs and event data, representing the complete TimeSeries dataset. Users can create one or more namespaces for each of their use cases. The abstraction applies various tunable options at the namespace level, which we will discuss further when we explore the service’s control plane.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"eda3\"\u003eAPI\u003c/h2\u003e\u003cp id=\"a85a\"\u003eThe abstraction provides the following APIs to interact with the event data.\u003c/p\u003e\u003cp id=\"8d2a\"\u003e\u003cstrong\u003eWriteEventRecordsSync\u003c/strong\u003e: This endpoint writes a batch of events and sends back a durability acknowledgement to the client. This is used in cases where users require a guarantee of durability.\u003c/p\u003e\u003cp id=\"9513\"\u003e\u003cstrong\u003eWriteEventRecords\u003c/strong\u003e: This is the fire-and-forget version of the above endpoint. It enqueues a batch of events without the durability acknowledgement. This is used in cases like logging or tracing, where users care more about throughput and can tolerate a small amount of data loss.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"a5f8\"\u003e{\u003cbr/\u003e  \u0026#34;namespace\u0026#34;: \u0026#34;my_dataset\u0026#34;,\u003cbr/\u003e  \u0026#34;events\u0026#34;: [\u003cbr/\u003e    {\u003cbr/\u003e      \u0026#34;timeSeriesId\u0026#34;: \u0026#34;profile100\u0026#34;,\u003cbr/\u003e      \u0026#34;eventTime\u0026#34;: \u0026#34;2024-10-03T21:24:23.988Z\u0026#34;,\u003cbr/\u003e      \u0026#34;eventId\u0026#34;: \u0026#34;550e8400-e29b-41d4-a716-446655440000\u0026#34;,\u003cbr/\u003e      \u0026#34;eventItems\u0026#34;: [\u003cbr/\u003e        {\u003cbr/\u003e          \u0026#34;eventItemKey\u0026#34;: \u0026#34;deviceType\u0026#34;,  \u003cbr/\u003e          \u0026#34;eventItemValue\u0026#34;: \u0026#34;aW9z\u0026#34;\u003cbr/\u003e        },\u003cbr/\u003e        {\u003cbr/\u003e          \u0026#34;eventItemKey\u0026#34;: \u0026#34;deviceMetadata\u0026#34;,\u003cbr/\u003e          \u0026#34;eventItemValue\u0026#34;: \u0026#34;c29tZSBtZXRhZGF0YQ==\u0026#34;\u003cbr/\u003e        }\u003cbr/\u003e      ]\u003cbr/\u003e    },\u003cbr/\u003e    {\u003cbr/\u003e      \u0026#34;timeSeriesId\u0026#34;: \u0026#34;profile100\u0026#34;,\u003cbr/\u003e      \u0026#34;eventTime\u0026#34;: \u0026#34;2024-10-03T21:23:30.000Z\u0026#34;,\u003cbr/\u003e      \u0026#34;eventId\u0026#34;: \u0026#34;123e4567-e89b-12d3-a456-426614174000\u0026#34;,\u003cbr/\u003e      \u0026#34;eventItems\u0026#34;: [\u003cbr/\u003e        {\u003cbr/\u003e          \u0026#34;eventItemKey\u0026#34;: \u0026#34;deviceType\u0026#34;,  \u003cbr/\u003e          \u0026#34;eventItemValue\u0026#34;: \u0026#34;YW5kcm9pZA==\u0026#34;\u003cbr/\u003e        }\u003cbr/\u003e      ]\u003cbr/\u003e    }\u003cbr/\u003e  ]\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"4e72\"\u003e\u003cstrong\u003eReadEventRecords\u003c/strong\u003e: Given a combination of a namespace, a timeSeriesId, a timeInterval, and optional eventFilters, this endpoint returns all the matching events, sorted descending by event_time, with low millisecond latency.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"a4e4\"\u003e{\u003cbr/\u003e  \u0026#34;namespace\u0026#34;: \u0026#34;my_dataset\u0026#34;,\u003cbr/\u003e  \u0026#34;timeSeriesId\u0026#34;: \u0026#34;profile100\u0026#34;,\u003cbr/\u003e  \u0026#34;timeInterval\u0026#34;: {\u003cbr/\u003e    \u0026#34;start\u0026#34;: \u0026#34;2024-10-02T21:00:00.000Z\u0026#34;,\u003cbr/\u003e    \u0026#34;end\u0026#34;:   \u0026#34;2024-10-03T21:00:00.000Z\u0026#34;\u003cbr/\u003e  },\u003cbr/\u003e  \u0026#34;eventFilters\u0026#34;: [\u003cbr/\u003e    {\u003cbr/\u003e      \u0026#34;matchEventItemKey\u0026#34;: \u0026#34;deviceType\u0026#34;,\u003cbr/\u003e      \u0026#34;matchEventItemValue\u0026#34;: \u0026#34;aW9z\u0026#34;\u003cbr/\u003e    }\u003cbr/\u003e  ],\u003cbr/\u003e  \u0026#34;pageSize\u0026#34;: 100,\u003cbr/\u003e  \u0026#34;totalRecordLimit\u0026#34;: 1000\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"d4d9\"\u003e\u003cstrong\u003eSearchEventRecords\u003c/strong\u003e: Given a search criteria and a time interval, this endpoint returns all the matching events. These use cases are fine with eventually consistent reads.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"71db\"\u003e{\u003cbr/\u003e  \u0026#34;namespace\u0026#34;: \u0026#34;my_dataset\u0026#34;,\u003cbr/\u003e  \u0026#34;timeInterval\u0026#34;: {\u003cbr/\u003e    \u0026#34;start\u0026#34;: \u0026#34;2024-10-02T21:00:00.000Z\u0026#34;,\u003cbr/\u003e    \u0026#34;end\u0026#34;: \u0026#34;2024-10-03T21:00:00.000Z\u0026#34;\u003cbr/\u003e  },\u003cbr/\u003e  \u0026#34;searchQuery\u0026#34;: {\u003cbr/\u003e    \u0026#34;booleanQuery\u0026#34;: {\u003cbr/\u003e      \u0026#34;searchQuery\u0026#34;: [\u003cbr/\u003e        {\u003cbr/\u003e          \u0026#34;equals\u0026#34;: {\u003cbr/\u003e            \u0026#34;eventItemKey\u0026#34;: \u0026#34;deviceType\u0026#34;,\u003cbr/\u003e            \u0026#34;eventItemValue\u0026#34;: \u0026#34;aW9z\u0026#34;\u003cbr/\u003e          }\u003cbr/\u003e        },\u003cbr/\u003e        {\u003cbr/\u003e          \u0026#34;range\u0026#34;: {\u003cbr/\u003e            \u0026#34;eventItemKey\u0026#34;: \u0026#34;deviceRegistrationTimestamp\u0026#34;,\u003cbr/\u003e            \u0026#34;lowerBound\u0026#34;: {\u003cbr/\u003e              \u0026#34;eventItemValue\u0026#34;: \u0026#34;MjAyNC0xMC0wMlQwMDowMDowMC4wMDBa\u0026#34;,\u003cbr/\u003e              \u0026#34;inclusive\u0026#34;: true\u003cbr/\u003e            },\u003cbr/\u003e            \u0026#34;upperBound\u0026#34;: {\u003cbr/\u003e              \u0026#34;eventItemValue\u0026#34;: \u0026#34;MjAyNC0xMC0wM1QwMDowMDowMC4wMDBa\u0026#34;\u003cbr/\u003e            }\u003cbr/\u003e          }\u003cbr/\u003e        }\u003cbr/\u003e      ],\u003cbr/\u003e      \u0026#34;operator\u0026#34;: \u0026#34;AND\u0026#34;\u003cbr/\u003e    }\u003cbr/\u003e  },\u003cbr/\u003e  \u0026#34;pageSize\u0026#34;: 100,\u003cbr/\u003e  \u0026#34;totalRecordLimit\u0026#34;: 1000\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"9863\"\u003e\u003cstrong\u003eAggregateEventRecords\u003c/strong\u003e: Given a search criteria and an aggregation mode (e.g. DistinctAggregation) , this endpoint performs the given aggregation within a given time interval. Similar to the Search endpoint, users can tolerate eventual consistency and a potentially higher latency (in seconds).\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0205\"\u003e{\u003cbr/\u003e  \u0026#34;namespace\u0026#34;: \u0026#34;my_dataset\u0026#34;,\u003cbr/\u003e  \u0026#34;timeInterval\u0026#34;: {\u003cbr/\u003e    \u0026#34;start\u0026#34;: \u0026#34;2024-10-02T21:00:00.000Z\u0026#34;,\u003cbr/\u003e    \u0026#34;end\u0026#34;: \u0026#34;2024-10-03T21:00:00.000Z\u0026#34;\u003cbr/\u003e  },\u003cbr/\u003e  \u0026#34;searchQuery\u0026#34;: {...some search criteria...},\u003cbr/\u003e  \u0026#34;aggregationQuery\u0026#34;: {\u003cbr/\u003e    \u0026#34;distinct\u0026#34;: {\u003cbr/\u003e      \u0026#34;eventItemKey\u0026#34;: \u0026#34;deviceType\u0026#34;,\u003cbr/\u003e      \u0026#34;pageSize\u0026#34;: 100\u003cbr/\u003e    }\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"626b\"\u003eIn the subsequent sections, we will talk about how we interact with this data at the storage layer.\u003c/p\u003e\u003ch2 id=\"006b\"\u003eStorage Layer\u003c/h2\u003e\u003cp id=\"39dd\"\u003eThe storage layer for TimeSeries comprises a primary data store and an optional index data store. The primary data store ensures data durability during writes and is used for primary read operations, while the index data store is utilized for search and aggregate operations. At Netflix, \u003cstrong\u003eApache Cassandra\u003c/strong\u003e is the preferred choice for storing durable data in high-throughput scenarios, while \u003cstrong\u003eElasticsearch\u003c/strong\u003e is the preferred data store for indexing. However, similar to our approach with the API, the storage layer is not tightly coupled to these specific data stores. Instead, we define storage API contracts that must be fulfilled, allowing us the flexibility to replace the underlying data stores as needed.\u003c/p\u003e\u003ch2 id=\"6efb\"\u003ePrimary Datastore\u003c/h2\u003e\u003cp id=\"3689\"\u003eIn this section, we will talk about how we leverage \u003cstrong\u003eApache Cassandra\u003c/strong\u003e for TimeSeries use cases.\u003c/p\u003e\u003ch2 id=\"9b30\"\u003ePartitioning Scheme\u003c/h2\u003e\u003cp id=\"6241\"\u003eAt Netflix’s scale, the continuous influx of event data can quickly overwhelm traditional databases. Temporal partitioning addresses this challenge by dividing the data into manageable chunks based on time intervals, such as hourly, daily, or monthly windows. This approach enables efficient querying of specific time ranges without the need to scan the entire dataset. It also allows Netflix to archive, compress, or delete older data efficiently, optimizing both storage and query performance. Additionally, this partitioning mitigates the performance issues typically associated with \u003ca href=\"https://thelastpickle.com/blog/2019/01/11/wide-partitions-cassandra-3-11.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ewide partitions\u003c/a\u003e in Cassandra. By employing this strategy, we can operate at much higher disk utilization, as it reduces the need to reserve large amounts of disk space for compactions, thereby saving costs.\u003c/p\u003e\u003cp id=\"04ec\"\u003eHere is what it looks like :\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"ae71\"\u003e\u003cstrong\u003eTime Slice: \u003c/strong\u003eA\u003cstrong\u003e \u003c/strong\u003etime slice is the unit of data retention and maps directly to a Cassandra table. We create multiple such time slices, each covering a specific interval of time. An event lands in one of these slices based on the \u003cstrong\u003eevent_time\u003c/strong\u003e. These slices are joined with \u003cem\u003eno time gaps\u003c/em\u003e\u003cstrong\u003e \u003c/strong\u003ein between, with operations being \u003cem\u003estart-inclusive\u003c/em\u003e and \u003cem\u003eend-exclusive\u003c/em\u003e, ensuring that all data lands in one of the slices. By utilizing these time slices, we can efficiently implement retention by dropping entire tables, which reduces storage space and saves on costs.\u003c/p\u003e\u003cp id=\"05ba\"\u003e\u003cstrong\u003eWhy not use row-based Time-To-Live (TTL)?\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"bd01\"\u003eUsing TTL on individual events would generate a significant number of \u003ca href=\"https://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003etombstones\u003c/a\u003e in Cassandra, degrading performance, especially during range scans. By employing discrete time slices and dropping them, we avoid the tombstone issue entirely. The tradeoff is that data may be retained slightly longer than necessary, as an entire table’s time range must fall outside the retention window before it can be dropped. Additionally, TTLs are difficult to adjust later, whereas TimeSeries can extend the dataset retention instantly with a single control plane operation.\u003c/p\u003e\u003cp id=\"fc7e\"\u003e\u003cstrong\u003eTime Buckets\u003c/strong\u003e: Within a time slice, data is further partitioned into time buckets. This facilitates effective range scans by allowing us to target specific time buckets for a given query range. The tradeoff is that if a user wants to read the entire range of data over a large time period, we must scan many partitions. We mitigate potential latency by scanning these partitions in parallel and aggregating the data at the end. In most cases, the advantage of targeting smaller data subsets outweighs the read amplification from these scatter-gather operations. Typically, users read a smaller subset of data rather than the entire retention range.\u003c/p\u003e\u003cp id=\"17b2\"\u003e\u003cstrong\u003eEvent Buckets\u003c/strong\u003e: To manage extremely high-throughput write operations, which may result in a burst of writes for a given time series within a short period, we further divide the time bucket into event buckets. This prevents overloading the same partition for a given time range and also reduces partition sizes further, albeit with a slight increase in read amplification.\u003c/p\u003e\u003cp id=\"ab2a\"\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003cem\u003eWith Cassandra 4.x onwards, we notice a substantial improvement in the performance of scanning a range of data in a wide partition. See \u003c/em\u003e\u003cstrong\u003e\u003cem\u003eFuture Enhancements\u003c/em\u003e\u003c/strong\u003e\u003cem\u003e at the end to see the \u003c/em\u003e\u003cstrong\u003e\u003cem\u003eDynamic Event bucketing\u003c/em\u003e\u003c/strong\u003e\u003cem\u003e work that aims to take advantage of this.\u003c/em\u003e\u003c/p\u003e\u003ch2 id=\"e7cd\"\u003eStorage Tables\u003c/h2\u003e\u003cp id=\"baaa\"\u003eWe use two kinds of tables\u003c/p\u003e\u003cul\u003e\u003cli id=\"1406\"\u003e\u003cstrong\u003eData tables\u003c/strong\u003e: These are the time slices that store the actual event data.\u003c/li\u003e\u003cli id=\"a159\"\u003e\u003cstrong\u003eMetadata table\u003c/strong\u003e: This table stores information about how each time slice is configured \u003cem\u003eper namespace\u003c/em\u003e.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"51df\"\u003eData tables\u003c/h2\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"4c23\"\u003eThe partition key enables splitting events for a \u003cstrong\u003etime_series_id\u003c/strong\u003e over a range of \u003cstrong\u003etime_bucket(s)\u003c/strong\u003e and \u003cstrong\u003eevent_bucket(s)\u003c/strong\u003e, thus mitigating hot partitions, while the clustering key allows us to keep data sorted on disk in the order we almost always want to read it. The \u003cstrong\u003evalue_metadata\u003c/strong\u003e column stores metadata for the \u003cstrong\u003eevent_item_value\u003c/strong\u003e such as compression.\u003c/p\u003e\u003cp id=\"654d\"\u003e\u003cstrong\u003eWriting to the data table:\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"524c\"\u003eUser writes will land in a given time slice, time bucket, and event bucket as a factor of the \u003cstrong\u003eevent_time\u003c/strong\u003e attached to the event. This factor is dictated by the control plane configuration of a given namespace.\u003c/p\u003e\u003cp id=\"2892\"\u003eFor example:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"4c4b\"\u003eDuring this process, the writer makes decisions on how to handle the data before writing, such as whether to compress it. The \u003cstrong\u003evalue_metadata\u003c/strong\u003e column records any such post-processing actions, ensuring that the reader can accurately interpret the data.\u003c/p\u003e\u003cp id=\"79ad\"\u003e\u003cstrong\u003eReading from the data table:\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"a48c\"\u003eThe below illustration depicts at a high-level on how we scatter-gather the reads from multiple partitions and join the result set at the end to return the final result.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"0c7b\"\u003eMetadata table\u003c/h2\u003e\u003cp id=\"20b0\"\u003eThis table stores the configuration data about the time slices for a given namespace.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"6fce\"\u003eNote the following:\u003c/p\u003e\u003cul\u003e\u003cli id=\"217e\"\u003e\u003cstrong\u003eNo Time Gaps\u003c/strong\u003e: The end_time of a given time slice overlaps with the start_time of the next time slice, ensuring all events find a home.\u003c/li\u003e\u003cli id=\"bac3\"\u003e\u003cstrong\u003eRetention\u003c/strong\u003e: The status indicates which tables fall inside and outside of the retention window.\u003c/li\u003e\u003cli id=\"e410\"\u003e\u003cstrong\u003eFlexible\u003c/strong\u003e: This metadata can be adjusted per time slice, allowing us to tune the partition settings of future time slices based on observed data patterns in the current time slice.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"f612\"\u003eThere is a lot more information that can be stored into the \u003cstrong\u003emetadata\u003c/strong\u003e column (e.g., compaction settings for the table), but we only show the partition settings here for brevity.\u003c/p\u003e\u003ch2 id=\"a523\"\u003eIndex Datastore\u003c/h2\u003e\u003cp id=\"5bab\"\u003eTo support secondary access patterns via non-primary key attributes, we index data into Elasticsearch. Users can configure a list of attributes per namespace that they wish to search and/or aggregate data on. The service extracts these fields from events as they stream in, indexing the resultant documents into Elasticsearch. Depending on the throughput, we may use Elasticsearch as a reverse index, retrieving the full data from Cassandra, or we may store the entire source data directly in Elasticsearch.\u003c/p\u003e\u003cp id=\"4df3\"\u003e\u003cstrong\u003eNote\u003c/strong\u003e:\u003cem\u003e Again, users are never directly exposed to Elasticsearch, just like they are not directly exposed to Cassandra. Instead, they interact with the Search and Aggregate API endpoints that translate a given query to that needed for the underlying datastore.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"3b37\"\u003eIn the next section, we will talk about how we configure these data stores for different datasets.\u003c/p\u003e\u003ch2 id=\"1edb\"\u003eControl Plane\u003c/h2\u003e\u003cp id=\"1f56\"\u003eThe data plane is responsible for executing the read and write operations, while the control plane configures every aspect of a namespace’s behavior. The data plane communicates with the TimeSeries control stack, which manages this configuration information. In turn, the TimeSeries control stack interacts with a sharded \u003cstrong\u003eData Gateway Platform Control Plane\u003c/strong\u003e that oversees control configurations for all abstractions and namespaces.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"7f9c\"\u003eSeparating the responsibilities of the data plane and control plane helps maintain the high availability of our data plane, as the control plane takes on tasks that may require some form of schema consensus from the underlying data stores.\u003c/p\u003e\u003ch2 id=\"cd98\"\u003eNamespace Configuration\u003c/h2\u003e\u003cp id=\"7d27\"\u003eThe below configuration snippet demonstrates the immense flexibility of the service and how we can tune several things per namespace using our control plane.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c0a2\"\u003e\u0026#34;persistence_configuration\u0026#34;: [\u003cbr/\u003e  {\u003cbr/\u003e    \u0026#34;id\u0026#34;: \u0026#34;PRIMARY_STORAGE\u0026#34;,\u003cbr/\u003e    \u0026#34;physical_storage\u0026#34;: {\u003cbr/\u003e      \u0026#34;type\u0026#34;: \u0026#34;CASSANDRA\u0026#34;,                  // type of primary storage\u003cbr/\u003e      \u0026#34;cluster\u0026#34;: \u0026#34;cass_dgw_ts_tracing\u0026#34;,     // physical cluster name\u003cbr/\u003e      \u0026#34;dataset\u0026#34;: \u0026#34;tracing_default\u0026#34;          // maps to the keyspace\u003cbr/\u003e    },\u003cbr/\u003e    \u0026#34;config\u0026#34;: {\u003cbr/\u003e      \u0026#34;timePartition\u0026#34;: {\u003cbr/\u003e        \u0026#34;secondsPerTimeSlice\u0026#34;: \u0026#34;129600\u0026#34;,    // width of a time slice\u003cbr/\u003e        \u0026#34;secondPerTimeBucket\u0026#34;: \u0026#34;3600\u0026#34;,      // width of a time bucket\u003cbr/\u003e        \u0026#34;eventBuckets\u0026#34;: 4                   // how many event buckets within\u003cbr/\u003e      },\u003cbr/\u003e      \u0026#34;queueBuffering\u0026#34;: {\u003cbr/\u003e        \u0026#34;coalesce\u0026#34;: \u0026#34;1s\u0026#34;,                   // how long to coalesce writes\u003cbr/\u003e        \u0026#34;bufferCapacity\u0026#34;: 4194304           // queue capacity in bytes\u003cbr/\u003e      },\u003cbr/\u003e      \u0026#34;consistencyScope\u0026#34;: \u0026#34;LOCAL\u0026#34;,          // single-region/multi-region\u003cbr/\u003e      \u0026#34;consistencyTarget\u0026#34;: \u0026#34;EVENTUAL\u0026#34;,      // read/write consistency\u003cbr/\u003e      \u0026#34;acceptLimit\u0026#34;: \u0026#34;129600s\u0026#34;              // how far back writes are allowed\u003cbr/\u003e    },\u003cbr/\u003e    \u0026#34;lifecycleConfigs\u0026#34;: {\u003cbr/\u003e      \u0026#34;lifecycleConfig\u0026#34;: [                  // Primary store data retention\u003cbr/\u003e        {\u003cbr/\u003e          \u0026#34;type\u0026#34;: \u0026#34;retention\u0026#34;,\u003cbr/\u003e          \u0026#34;config\u0026#34;: {\u003cbr/\u003e            \u0026#34;close_after\u0026#34;: \u0026#34;1296000s\u0026#34;,      // close for reads/writes\u003cbr/\u003e            \u0026#34;delete_after\u0026#34;: \u0026#34;1382400s\u0026#34;      // drop time slice\u003cbr/\u003e          }\u003cbr/\u003e        }\u003cbr/\u003e      ]\u003cbr/\u003e    }\u003cbr/\u003e  },\u003cbr/\u003e  {\u003cbr/\u003e    \u0026#34;id\u0026#34;: \u0026#34;INDEX_STORAGE\u0026#34;,\u003cbr/\u003e    \u0026#34;physicalStorage\u0026#34;: {\u003cbr/\u003e      \u0026#34;type\u0026#34;: \u0026#34;ELASTICSEARCH\u0026#34;,              // type of index storage\u003cbr/\u003e      \u0026#34;cluster\u0026#34;: \u0026#34;es_dgw_ts_tracing\u0026#34;,       // ES cluster name\u003cbr/\u003e      \u0026#34;dataset\u0026#34;: \u0026#34;tracing_default_useast1\u0026#34;  // base index name\u003cbr/\u003e    },\u003cbr/\u003e    \u0026#34;config\u0026#34;: {\u003cbr/\u003e      \u0026#34;timePartition\u0026#34;: {\u003cbr/\u003e        \u0026#34;secondsPerSlice\u0026#34;: \u0026#34;129600\u0026#34;         // width of the index slice\u003cbr/\u003e      },\u003cbr/\u003e      \u0026#34;consistencyScope\u0026#34;: \u0026#34;LOCAL\u0026#34;,\u003cbr/\u003e      \u0026#34;consistencyTarget\u0026#34;: \u0026#34;EVENTUAL\u0026#34;,      // how should we read/write data\u003cbr/\u003e      \u0026#34;acceptLimit\u0026#34;: \u0026#34;129600s\u0026#34;,             // how far back writes are allowed\u003cbr/\u003e      \u0026#34;indexConfig\u0026#34;: {\u003cbr/\u003e        \u0026#34;fieldMapping\u0026#34;: {                   // fields to extract to index\u003cbr/\u003e          \u0026#34;tags.nf.app\u0026#34;: \u0026#34;KEYWORD\u0026#34;,\u003cbr/\u003e          \u0026#34;tags.duration\u0026#34;: \u0026#34;INTEGER\u0026#34;,\u003cbr/\u003e          \u0026#34;tags.enabled\u0026#34;: \u0026#34;BOOLEAN\u0026#34;\u003cbr/\u003e        },\u003cbr/\u003e        \u0026#34;refreshInterval\u0026#34;: \u0026#34;60s\u0026#34;            // Index related settings\u003cbr/\u003e      }\u003cbr/\u003e    },\u003cbr/\u003e    \u0026#34;lifecycleConfigs\u0026#34;: {\u003cbr/\u003e      \u0026#34;lifecycleConfig\u0026#34;: [\u003cbr/\u003e        {\u003cbr/\u003e          \u0026#34;type\u0026#34;: \u0026#34;retention\u0026#34;,              // Index retention settings\u003cbr/\u003e          \u0026#34;config\u0026#34;: {\u003cbr/\u003e            \u0026#34;close_after\u0026#34;: \u0026#34;1296000s\u0026#34;,\u003cbr/\u003e            \u0026#34;delete_after\u0026#34;: \u0026#34;1382400s\u0026#34;\u003cbr/\u003e          }\u003cbr/\u003e        }\u003cbr/\u003e      ]\u003cbr/\u003e    }\u003cbr/\u003e  }\u003cbr/\u003e]\u003c/span\u003e\u003c/pre\u003e\u003ch2 id=\"fcef\"\u003eProvisioning Infrastructure\u003c/h2\u003e\u003cp id=\"85c4\"\u003eWith so many different parameters, we need automated provisioning workflows to deduce the best settings for a given workload. When users want to create their namespaces, they specify a list of \u003cem\u003eworkload\u003c/em\u003e \u003cem\u003edesires\u003c/em\u003e, which the automation translates into concrete infrastructure and related control plane configuration. We highly encourage you to watch this \u003ca href=\"https://www.youtube.com/watch?v=2aBVKXi8LKk\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eApacheCon talk\u003c/a\u003e, by one of our stunning colleagues \u003cstrong\u003eJoey Lynch,\u003c/strong\u003e on how we achieve this. We may go into detail on this subject in one of our future blog posts.\u003c/p\u003e\u003cp id=\"311d\"\u003eOnce the system provisions the initial infrastructure, it then scales in response to the user workload. The next section describes how this is achieved.\u003c/p\u003e\u003ch2 id=\"5890\"\u003eScalability\u003c/h2\u003e\u003cp id=\"561f\"\u003eOur users may operate with limited information at the time of provisioning their namespaces, resulting in best-effort provisioning estimates. Further, evolving use-cases may introduce new throughput requirements over time. Here’s how we manage this:\u003c/p\u003e\u003cul\u003e\u003cli id=\"37e7\"\u003e\u003cstrong\u003eHorizontal scaling\u003c/strong\u003e: TimeSeries server instances can auto-scale up and down as per attached scaling policies to meet the traffic demand. The storage server capacity can be recomputed to accommodate changing requirements using our \u003ca href=\"https://github.com/Netflix-Skunkworks/service-capacity-modeling/tree/main/service_capacity_modeling\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecapacity planner\u003c/a\u003e.\u003c/li\u003e\u003cli id=\"6f26\"\u003e\u003cstrong\u003eVertical scaling\u003c/strong\u003e: We may also choose to vertically scale our TimeSeries server instances or our storage instances to get greater CPU, RAM and/or attached storage capacity.\u003c/li\u003e\u003cli id=\"f575\"\u003e\u003cstrong\u003eScaling disk\u003c/strong\u003e: We may attach \u003ca href=\"https://aws.amazon.com/ebs/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEBS\u003c/a\u003e to store data if the capacity planner prefers infrastructure that offers larger storage at a lower cost rather than SSDs optimized for latency. In such cases, we deploy jobs to scale the EBS volume when the disk storage reaches a certain percentage threshold.\u003c/li\u003e\u003cli id=\"e2bd\"\u003e\u003cstrong\u003eRe-partitioning data\u003c/strong\u003e: Inaccurate workload estimates can lead to over or under-partitioning of our datasets. TimeSeries control-plane can adjust the partitioning configuration for upcoming time slices, once we realize the nature of data in the wild (via partition histograms). In the future we plan to support re-partitioning of older data and dynamic partitioning of current data.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"bdec\"\u003eDesign Principles\u003c/h2\u003e\u003cp id=\"b328\"\u003eSo far, we have seen how TimeSeries stores, configures and interacts with event datasets. Let’s see how we apply different techniques to improve the performance of our operations and provide better guarantees.\u003c/p\u003e\u003ch2 id=\"737f\"\u003eEvent Idempotency\u003c/h2\u003e\u003cp id=\"83da\"\u003eWe prefer to bake in idempotency in all mutation endpoints, so that users can retry or hedge their requests safely. \u003ca href=\"https://research.google/pubs/the-tail-at-scale/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHedging\u003c/a\u003e is when the client sends an identical competing request to the server, if the original request does not come back with a response in an expected amount of time. The client then responds with whichever request completes first. This is done to keep the tail latencies for an application relatively low. This can only be done safely if the mutations are idempotent. For TimeSeries, the combination of \u003cstrong\u003eevent_time\u003c/strong\u003e, \u003cstrong\u003eevent_id\u003c/strong\u003e and \u003cstrong\u003eevent_item_key\u003c/strong\u003e form the idempotency key for a given \u003cstrong\u003etime_series_id\u003c/strong\u003e event.\u003c/p\u003e\u003ch2 id=\"9ffa\"\u003eSLO-based Hedging\u003c/h2\u003e\u003cp id=\"751c\"\u003eWe assign Service Level Objectives (SLO) targets for different endpoints within TimeSeries, as an indication of what we think the performance of those endpoints should be \u003cem\u003efor a given namespace\u003c/em\u003e. We can then hedge a request if the response does not come back in that configured amount of time.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"377a\"\u003e\u0026#34;slos\u0026#34;: {\u003cbr/\u003e  \u0026#34;read\u0026#34;: {               // SLOs per endpoint\u003cbr/\u003e    \u0026#34;latency\u0026#34;: {\u003cbr/\u003e      \u0026#34;target\u0026#34;: \u0026#34;0.5s\u0026#34;,   // hedge around this number\u003cbr/\u003e      \u0026#34;max\u0026#34;: \u0026#34;1s\u0026#34;         // time-out around this number\u003cbr/\u003e    }\u003cbr/\u003e  },\u003cbr/\u003e  \u0026#34;write\u0026#34;: {\u003cbr/\u003e    \u0026#34;latency\u0026#34;: {\u003cbr/\u003e      \u0026#34;target\u0026#34;: \u0026#34;0.01s\u0026#34;,\u003cbr/\u003e      \u0026#34;max\u0026#34;: \u0026#34;0.05s\u0026#34;\u003cbr/\u003e    }\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003ch2 id=\"4152\"\u003ePartial Return\u003c/h2\u003e\u003cp id=\"87eb\"\u003eSometimes, a client may be sensitive to latency and willing to accept a partial result set. A real-world example of this is real-time frequency capping. Precision is not critical in this case, but if the response is delayed, it becomes practically useless to the upstream client. Therefore, the client prefers to work with whatever data has been collected so far rather than timing out while waiting for all the data. The TimeSeries client supports partial returns around SLOs for this purpose. Importantly, we still maintain the latest order of events in this partial fetch.\u003c/p\u003e\u003ch2 id=\"1d8b\"\u003eAdaptive Pagination\u003c/h2\u003e\u003cp id=\"fabf\"\u003eAll reads start with a default fanout factor, scanning 8 partition buckets in parallel. However, if the service layer determines that the time_series dataset is dense — i.e., most reads are satisfied by reading the first few partition buckets — then it dynamically adjusts the fanout factor of future reads in order to reduce the read amplification on the underlying datastore. Conversely, if the dataset is sparse, we may want to increase this limit with a reasonable upper bound.\u003c/p\u003e\u003ch2 id=\"7fcd\"\u003eLimited Write Window\u003c/h2\u003e\u003cp id=\"d732\"\u003eIn most cases, the active range for writing data is smaller than the range for reading data — i.e., we want a range of time to become immutable as soon as possible so that we can apply optimizations on top of it. We control this by having a configurable “\u003cstrong\u003eacceptLimit\u003c/strong\u003e” parameter that prevents users from writing events older than this time limit. For example, an accept limit of 4 hours means that users cannot write events older than \u003cem\u003enow() — 4 hours\u003c/em\u003e. We sometimes raise this limit for backfilling historical data, but it is tuned back down for regular write operations. Once a range of data becomes immutable, we can safely do things like caching, compressing, and compacting it for reads.\u003c/p\u003e\u003ch2 id=\"7223\"\u003eBuffering Writes\u003c/h2\u003e\u003cp id=\"21e8\"\u003eWe frequently leverage this service for handling bursty workloads. Rather than overwhelming the underlying datastore with this load all at once, we aim to distribute it more evenly by allowing events to coalesce over short durations (typically seconds). These events accumulate in in-memory queues running on each instance. Dedicated consumers then steadily drain these queues, grouping the events by their partition key, and batching the writes to the underlying datastore.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"fa07\"\u003eThe queues are tailored to each datastore since their operational characteristics depend on the specific datastore being written to. For instance, the batch size for writing to Cassandra is significantly smaller than that for indexing into Elasticsearch, leading to different drain rates and batch sizes for the associated consumers.\u003c/p\u003e\u003cp id=\"e4da\"\u003eWhile using in-memory queues does increase JVM garbage collection, we have experienced substantial improvements by transitioning to JDK 21 with ZGC. To illustrate the impact, ZGC has reduced our tail latencies by an impressive 86%:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"e67f\"\u003eBecause we use in-memory queues, we are prone to losing events in case of an instance crash. As such, these queues are only used for use cases that can tolerate some amount of data loss .e.g. tracing/logging. For use cases that need guaranteed durability and/or read-after-write consistency, these queues are effectively disabled and writes are flushed to the data store almost immediately.\u003c/p\u003e\u003ch2 id=\"3b16\"\u003eDynamic Compaction\u003c/h2\u003e\u003cp id=\"b018\"\u003eOnce a time slice exits the active write window, we can leverage the immutability of the data to optimize it for read performance. This process may involve re-compacting immutable data using optimal compaction strategies, dynamically shrinking and/or splitting shards to optimize system resources, and other similar techniques to ensure fast and reliable performance.\u003c/p\u003e\u003cp id=\"dbe4\"\u003eThe following section provides a glimpse into the real-world performance of some of our TimeSeries datasets.\u003c/p\u003e\u003ch2 id=\"dd6b\"\u003eReal-world Performance\u003c/h2\u003e\u003cp id=\"1625\"\u003eThe service can write data in the order of low single digit milliseconds\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"92ad\"\u003ewhile consistently maintaining stable point-read latencies:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"4890\"\u003eAt the time of writing this blog, the service was processing close to \u003cem\u003e15 million events/second\u003c/em\u003e across all the different datasets at peak globally.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"8793\"\u003eTime Series Usage @ Netflix\u003c/h2\u003e\u003cp id=\"3d90\"\u003eThe TimeSeries Abstraction plays a vital role across key services at Netflix. Here are some impactful use cases:\u003c/p\u003e\u003cul\u003e\u003cli id=\"3163\"\u003e\u003cstrong\u003eTracing and Insights: \u003c/strong\u003eLogs traces across all apps and micro-services within Netflix, to understand service-to-service communication, aid in debugging of issues, and answer support requests.\u003c/li\u003e\u003cli id=\"a327\"\u003e\u003cstrong\u003eUser Interaction Tracking\u003c/strong\u003e: Tracks millions of user interactions — such as video playbacks, searches, and content engagement — providing insights that enhance Netflix’s recommendation algorithms in real-time and improve the overall user experience.\u003c/li\u003e\u003cli id=\"7ea2\"\u003e\u003cstrong\u003eFeature Rollout and Performance Analysis\u003c/strong\u003e: Tracks the rollout and performance of new product features, enabling Netflix engineers to measure how users engage with features, which powers data-driven decisions about future improvements.\u003c/li\u003e\u003cli id=\"660c\"\u003e\u003cstrong\u003eAsset Impression Tracking and Optimization\u003c/strong\u003e: Tracks asset impressions ensuring content and assets are delivered efficiently while providing real-time feedback for optimizations.\u003c/li\u003e\u003cli id=\"03cc\"\u003e\u003cstrong\u003eBilling and Subscription Management:\u003c/strong\u003e Stores historical data related to billing and subscription management, ensuring accuracy in transaction records and supporting customer service inquiries.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"a1ea\"\u003eand more…\u003c/p\u003e\u003ch2 id=\"3624\"\u003eFuture Enhancements\u003c/h2\u003e\u003cp id=\"451d\"\u003eAs the use cases evolve, and the need to make the abstraction even more cost effective grows, we aim to make many improvements to the service in the upcoming months. Some of them are:\u003c/p\u003e\u003cul\u003e\u003cli id=\"6046\"\u003e\u003cstrong\u003eTiered Storage for Cost Efficiency: \u003c/strong\u003eSupport moving older, lesser-accessed data into cheaper object storage that has higher time to first byte, potentially saving Netflix millions of dollars.\u003c/li\u003e\u003cli id=\"4073\"\u003e\u003cstrong\u003eDynamic Event Bucketing: \u003c/strong\u003eSupport real-time partitioning of keys into optimally-sized partitions as events stream in, rather than having a \u003cem\u003esomewhat\u003c/em\u003e static configuration at the time of provisioning a namespace. This strategy has a huge advantage of \u003cem\u003enot\u003c/em\u003e partitioning time_series_ids that don’t need it, thus saving the overall cost of read amplification. Also, with Cassandra 4.x, we have noted major improvements in reading a subset of data in a wide partition that could lead us to be less aggressive with partitioning the entire dataset ahead of time.\u003c/li\u003e\u003cli id=\"07ae\"\u003e\u003cstrong\u003eCaching: \u003c/strong\u003eTake advantage of immutability of data and cache it intelligently for discrete time ranges.\u003c/li\u003e\u003cli id=\"78ff\"\u003e\u003cstrong\u003eCount and other Aggregations: \u003c/strong\u003eSome users are only interested in counting events in a given time interval rather than fetching all the event data for it.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"d63b\"\u003eConclusion\u003c/h2\u003e\u003cp id=\"5ab6\"\u003eThe TimeSeries Abstraction is a vital component of Netflix’s online data infrastructure, playing a crucial role in supporting both real-time and long-term decision-making. Whether it’s monitoring system performance during high-traffic events or optimizing user engagement through behavior analytics, TimeSeries Abstraction ensures that Netflix operates seamlessly and efficiently on a global scale.\u003c/p\u003e\u003cp id=\"5764\"\u003eAs Netflix continues to innovate and expand into new verticals, the TimeSeries Abstraction will remain a cornerstone of our platform, helping us push the boundaries of what’s possible in streaming and beyond.\u003c/p\u003e\u003cp id=\"c076\"\u003eStay tuned for Part 2, where we’ll introduce our \u003cstrong\u003eDistributed Counter Abstraction\u003c/strong\u003e, a key element of \u003cstrong\u003eNetflix’s Composite Abstractions\u003c/strong\u003e, built on top of the TimeSeries Abstraction.\u003c/p\u003e\u003ch2 id=\"d6d2\"\u003eAcknowledgments\u003c/h2\u003e\u003cp id=\"06af\"\u003eSpecial thanks to our stunning colleagues who contributed to TimeSeries Abstraction’s success: \u003ca href=\"https://www.linkedin.com/in/tomdevoe/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTom DeVoe\u003c/a\u003e \u003ca href=\"https://www.linkedin.com/in/mengqingwang/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMengqing Wang\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/kartik894/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKartik Sathyanarayanan\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/jordan-west-8aa1731a3/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJordan West\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/matt-lehman-39549719b/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMatt Lehman\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/cheng-wang-10323417/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCheng Wang\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/clohfink/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eChris Lohfink\u003c/a\u003e .\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "32 min read",
  "publishedTime": "2024-10-08T17:01:53.282Z",
  "modifiedTime": null
}
