{
  "id": "a102d36f-8804-4b40-b6cd-5ac307afc1b2",
  "title": "Building multimodal AI for Ray-Ban Meta glasses",
  "link": "https://engineering.fb.com/2025/03/04/virtual-reality/building-multimodal-ai-for-ray-ban-meta-glasses/",
  "description": "Multimodal AI – models capable of processing multiple different types of inputs like speech, text, and images – have been transforming user experiences in the wearables space. With our Ray-Ban Meta glasses, multimodal AI helps the glasses see what the wearer is seeing. This means anyone wearing Ray-Ban Meta glasses can ask them questions about [...] Read More... The post Building multimodal AI for Ray-Ban Meta glasses appeared first on Engineering at Meta.",
  "author": "",
  "published": "Tue, 04 Mar 2025 21:24:18 +0000",
  "source": "https://engineering.fb.com/feed/",
  "categories": [
    "AI Research",
    "ML Applications",
    "Virtual Reality",
    "Meta Tech Podcast"
  ],
  "byline": "By Pascal Hartig",
  "length": 2611,
  "excerpt": "Multimodal AI – models capable of processing multiple different types of inputs like speech, text, and images – have been transforming user experiences in the wearables space. With our Ray-Ban Meta…",
  "siteName": "Engineering at Meta",
  "favicon": "",
  "text": "Multimodal AI – models capable of processing multiple different types of inputs like speech, text, and images – have been transforming user experiences in the wearables space. With our Ray-Ban Meta glasses, multimodal AI helps the glasses see what the wearer is seeing. This means anyone wearing Ray-Ban Meta glasses can ask them questions about what they’re looking at. The glasses can provide information about a landmark, translate text you’re looking at, and many other features. But what does it take to bring AI into a wearable device? On this episode of the Meta Tech Podcast, meet Shane, a research scientist at Meta who has spent the last seven years focusing on computer vision and multimodal AI for wearables. Shane and his team have been behind cutting edge AI research like AnyMAL, a unified language model that can reason over an array of input signals including text, audio, video, and even IMU motion sensor data. Shane sits down with Pascal Hartig to share how his team is building foundational models for the Ray-Ban Meta glasses. They talk about the unique challenges of AI glasses and pushing the boundaries of AI-driven wearable technology. Whether you’re an engineer, a tech enthusiast, or simply curious, this episode has something for everyone! Download or listen to the episode below: You can also find the episode wherever you get your podcasts, including: Spotify Apple Podcasts Pocket Casts Overcast The Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features. Send us feedback on Instagram, Threads, or X. And if you’re interested in learning more about career opportunities at Meta visit the Meta Careers page. Links AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model Inside The Be My Eyes-Meta Collaboration Cachelib Meta Open Source on Threads Meta’s AI-Powered Ray-Bans Are Life-Enhancing for the Blind Timestamps Intro 0:06 OSS News 0:56 Introduction Shane 1:30 The role of research scientist over time 3:03 What’s Multi-Modal AI? 5:45 Applying Multi-Modal AI in Meta’s products 7:21 Acoustic modalities beyond speech 9:17 AnyMAL 12:23 Encoder zoos 13:53 0-shot performance 16:25 Iterating on models 17:28 LLM parameter size 19:29 How do we process a request from the glasses? 21:53 Processing moving images 23:44 Scaling to billions of users 26:01 Where lies the optimization potential? 28:12 Incorporating feedback 29:08 Open-source influence 31:30 Be My Eyes Program 33:57 Working with industry experts at Meta 36:18 Outro 38:55",
  "image": "https://engineering.fb.com/wp-content/uploads/2025/03/Meta-Tech-Podcast-ep-71.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"single-wrapper\" tabindex=\"-1\"\u003e\n\t\t\u003cmain id=\"main\"\u003e\n\n\t\t\t\n\t\t\t\t\n\u003carticle id=\"post-22308\"\u003e\n\n\t\n\n\t\t\t\u003cfigure id=\"post-feat-image-container\"\u003e\n\t\t\t\t\u003cimg width=\"1200\" height=\"634\" src=\"https://engineering.fb.com/wp-content/uploads/2025/03/Meta-Tech-Podcast-ep-71.png\" alt=\"\"/\u003e\t\t\t\t\t\t\u003c/figure\u003e\n\t\t\n\t\n\n\t\n\t\u003cdiv\u003e\n\n\t\t\u003cp\u003eMultimodal AI – models capable of processing multiple different types of inputs like speech, text, and images – have been \u003ca href=\"https://www.wsj.com/tech/ai/metas-ai-powered-ray-bans-are-life-enhancing-for-the-blind-3ae38026\"\u003etransforming user experiences in the wearables space\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWith our Ray-Ban Meta glasses, \u003ca href=\"https://www.meta.com/blog/ray-ban-meta-smart-glasses-new-styles-multimodal-ai-ferrari/?srsltid=AfmBOoo5_UKTrC8M-l1bQ3rwyqZzG5AygmsYPeeXE6rLSTE-xAjsVeTo\"\u003emultimodal AI\u003c/a\u003e helps the glasses see what the wearer is seeing. This means anyone wearing Ray-Ban Meta glasses can ask them questions about what they’re looking at. The glasses can provide information about a landmark, translate text you’re looking at, and many other features.\u003c/p\u003e\n\u003cp\u003eBut what does it take to bring AI into a wearable device?\u003c/p\u003e\n\u003cp\u003eOn this episode of the Meta Tech Podcast, meet Shane, a research scientist at Meta who has spent the last seven years focusing on computer vision and multimodal AI for wearables. Shane and his team have been behind cutting edge AI research like \u003ca href=\"https://arxiv.org/pdf/2309.16058\"\u003eAnyMAL\u003c/a\u003e, a unified language model that can reason over an array of input signals including text, audio, video, and even IMU motion sensor data.\u003c/p\u003e\n\u003cp\u003eShane sits down with \u003ca href=\"https://www.threads.net/@passy_\"\u003ePascal Hartig\u003c/a\u003e to share how his team is building foundational models for the Ray-Ban Meta glasses. They talk about the unique challenges of AI glasses and pushing the boundaries of AI-driven wearable technology.\u003c/p\u003e\n\u003cp\u003eWhether you’re an engineer, a tech enthusiast, or simply curious, this episode has something for everyone!\u003c/p\u003e\n\u003cp\u003eDownload or listen to the episode below:\u003c/p\u003e\n\u003cp\u003e\u003cbr/\u003e\nYou can also find the episode wherever you get your podcasts, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://open.spotify.com/episode/3KKHyDHl6LIgTCgtv5KuVJ\"\u003eSpotify\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://podcasts.apple.com/us/podcast/meta-tech-podcast/id1370910331\"\u003eApple Podcasts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pca.st/fefw0wwy\"\u003ePocket Casts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://overcast.fm/login\"\u003eOvercast\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003ca href=\"https://insidefacebookmobile.libsyn.com/\"\u003eMeta Tech Podcast\u003c/a\u003e is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.\u003c/p\u003e\n\u003cp\u003eSend us feedback on \u003ca href=\"https://instagram.com/metatechpod\"\u003eInstagram\u003c/a\u003e, \u003ca href=\"https://threads.net/@metatechpod\"\u003eThreads\u003c/a\u003e, or \u003ca href=\"https://twitter.com/metatechpod\"\u003eX\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAnd if you’re interested in learning more about career opportunities at Meta visit the \u003ca href=\"https://www.metacareers.com/?ref=engineering.fb.com\"\u003eMeta Careers\u003c/a\u003e page.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLinks\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2309.16058\"\u003eAnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.forbes.com/sites/stevenaquino/2024/10/11/inside-the-be-my-eyes-meta-collaboration-and-the-allure-to--impact-humanity/\"\u003eInside The Be My Eyes-Meta Collaboration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://engineering.fb.com/2021/09/02/core-infra/cachelib/\"\u003eCachelib\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.threads.net/@metaopensource\"\u003eMeta Open Source on Threads\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.wsj.com/tech/ai/metas-ai-powered-ray-bans-are-life-enhancing-for-the-blind-3ae38026\"\u003eMeta’s AI-Powered Ray-Bans Are Life-Enhancing for the Blind\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTimestamps\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIntro 0:06\u003c/li\u003e\n\u003cli\u003eOSS News 0:56\u003c/li\u003e\n\u003cli\u003eIntroduction Shane 1:30\u003c/li\u003e\n\u003cli\u003eThe role of research scientist over time 3:03\u003c/li\u003e\n\u003cli\u003eWhat’s Multi-Modal AI? 5:45\u003c/li\u003e\n\u003cli\u003eApplying Multi-Modal AI in Meta’s products 7:21\u003c/li\u003e\n\u003cli\u003eAcoustic modalities beyond speech 9:17\u003c/li\u003e\n\u003cli\u003eAnyMAL 12:23\u003c/li\u003e\n\u003cli\u003eEncoder zoos 13:53\u003c/li\u003e\n\u003cli\u003e0-shot performance 16:25\u003c/li\u003e\n\u003cli\u003eIterating on models 17:28\u003c/li\u003e\n\u003cli\u003eLLM parameter size 19:29\u003c/li\u003e\n\u003cli\u003eHow do we process a request from the glasses? 21:53\u003c/li\u003e\n\u003cli\u003eProcessing moving images 23:44\u003c/li\u003e\n\u003cli\u003eScaling to billions of users 26:01\u003c/li\u003e\n\u003cli\u003eWhere lies the optimization potential? 28:12\u003c/li\u003e\n\u003cli\u003eIncorporating feedback 29:08\u003c/li\u003e\n\u003cli\u003eOpen-source influence 31:30\u003c/li\u003e\n\u003cli\u003eBe My Eyes Program 33:57\u003c/li\u003e\n\u003cli\u003eWorking with industry experts at Meta 36:18\u003c/li\u003e\n\u003cli\u003eOutro 38:55\u003c/li\u003e\n\u003c/ul\u003e\n\n\t\t\n\t\u003c/div\u003e\n\n\n\u003c/article\u003e\n\n\n\n\n\n\n\n\n\t\t\t\t\t\n\t \n\n\t\t\n\t\t\t\t\n\t\t\t\n\t\t\u003c/main\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-03-04T21:24:18Z",
  "modifiedTime": "2025-03-06T18:54:03Z"
}
