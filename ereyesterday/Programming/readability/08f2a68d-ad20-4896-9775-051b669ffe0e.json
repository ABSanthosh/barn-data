{
  "id": "08f2a68d-ad20-4896-9775-051b669ffe0e",
  "title": "Which AI model should I use with GitHub Copilot?",
  "link": "https://github.blog/ai-and-ml/github-copilot/which-ai-model-should-i-use-with-github-copilot/",
  "description": "Ever wondered which AI model is the best fit for your GitHub Copilot project? Here are some things to consider. The post Which AI model should I use with GitHub Copilot? appeared first on The GitHub Blog.",
  "author": "Cassidy Williams",
  "published": "Thu, 17 Apr 2025 21:19:31 +0000",
  "source": "https://github.blog/feed/",
  "categories": [
    "AI \u0026 ML",
    "GitHub Copilot",
    "AI coding tools",
    "AI models",
    "generative AI"
  ],
  "byline": "Cassidy Williams",
  "length": 6931,
  "excerpt": "Ever wondered which AI model is the best fit for your GitHub Copilot project? Here are some things to consider.",
  "siteName": "The GitHub Blog",
  "favicon": "https://github.blog/wp-content/uploads/2019/01/cropped-github-favicon-512.png?fit=192%2C192",
  "text": "This was originally published on our developer newsletter, GitHub Insider, which offers tips and tricks for devs at every level. If you’re not subscribed, go do that now—you won’t regret it (we promise). If you’ve ever wondered which AI model is the best fit for your GitHub Copilot project, you’re not alone. Since each model has its own strengths, picking the right one can feel somewhat mysterious. With models that prioritize speed, depth, or a balance of both, it helps to know what each one brings to the table. Let’s break it down together. 👇 The TL;DR 💳 Balance between cost and performance: Go with GPT-4.1, GPT-4o, or Claude 3.5 Sonnet. 🪙 Fast, lightweight tasks: o4-mini or Claude 3.5 Sonnet are your buddies. 💎 Deep reasoning or complex debugging: Think Claude 3.7 Sonnet, o3, or GPT 4.5. 🖼️ Multimodal inputs (like images): Check out Gemini 2.0 Flash or GPT-4o. Your mileage may vary and it’s always good to try things yourself before taking someone else’s word for it, but this is how these models were designed to be used. All that being said… Let’s talk models. 🏎️ AI models designed for coding speed o4-mini and o3-mini: The speed demons 😈 Fast, efficient, and cost-effective, o4-mini and o3-mini are ideal for simple coding questions and quick iterations. If you’re looking for a no-frills model, use these. ✅ Use them for: Quick prototyping. Explaining code snippets. Learning new programming concepts. Generating boilerplate code. 👀 You may prefer another model: If your task spans multiple files or calls for deep reasoning, a higher‑capacity model such as GPT‑4.5 or o3 can keep more context in mind. Looking for extra expressive flair? Try GPT‑4o. ⚖️ AI models designed for balance Claude 3.5 Sonnet: The budget-friendly helper 😊 Need solid performance but watching your costs? Claude 3.5 Sonnet is like a dependable sidekick. It’s great for everyday coding tasks without burning through your monthly usage. ✅ Use it for: Writing documentation. Answering language-specific questions. Generating code snippets. 👀 You may prefer another model: For elaborate multi‑step reasoning or big‑picture planning, consider stepping up to Claude 3.7 Sonnet or GPT‑4.5. GPT-4o and GPT-4.1: The all-rounders 🌎 These are your go-to models for general tasks. Need fast responses? Check. Want to work with text *and* images? Double check. GPT-4o and GPT-4.1 are like the Swiss Army knives of AI models: flexible, dependable, and cost-efficient. ✅ Use them for: Explaining code blocks. Writing comments or docs. Generating small, reusable snippets. Multilingual prompts. 👀 You may prefer another model: Complex architectural reasoning or multi‑step debugging may land more naturally with GPT‑4.5 or Claude 3.7 Sonnet. 🧠 AI models designed for deep thinking and big projects Claude 3.7 Sonnet: The architect 🏠 This one’s the power tool for large, complex projects. From multi-file refactoring to feature development across front end and back end, Claude 3.7 Sonnet shines when context and depth matter most. ✅ Use it for: Refactoring large codebases. Planning complex architectures. Designing algorithms. Combining high-level summaries with deep analysis. 👀 You may prefer another model: For quick iterations or straightforward tasks, Claude 3.5 Sonnet or GPT‑4o may deliver results with less overhead. Gemini 2.5 Pro: The researcher 🔎 Gemini 2.5 Pro is the powerhouse for advanced reasoning and coding. It’s built for complex tasks (think: deep debugging, algorithm design, and even scientific research). With its long-context capabilities, it can handle extensive datasets or documents with ease. ✅ Use it for: Writing full functions, classes, or multi-file logic. Debugging complex systems. Analyzing scientific data and generating insights. Processing long documents, datasets, or codebases. 👀 You may prefer another model: For cost-sensitive tasks, o4-mini or Gemini 2.0 Flash are more budget-friendly options. GPT-4.5: The thinker 💭 Got a tricky problem? Whether you’re debugging multi-step issues or crafting full-on systems architectures, GPT-4.5 thrives on nuance and complexity. ✅ Use it for: Writing detailed README files. Generating full functions or multi-file solutions. Debugging complex errors. Making architectural decisions. 👀 You may prefer another model: When you just need a quick iteration on something small—or you’re watching tokens—GPT‑4o can finish faster and cheaper. o3 and o1: The deep diver 🥽 These models are perfect for tasks that need precision and logic. Whether you’re optimizing performance-critical code or refactoring a messy codebase, o3 and o1 excel in breaking down problems step by step. ✅ Use them for: Code optimization. Debugging complex systems. Writing structured, reusable code. Summarizing logs or benchmarks. 👀 You may prefer another model: During early prototyping or lightweight tasks, a nimble model such as o4‑mini or GPT‑4o may feel snappier. 🖼️ Multimodal AI models designed to handle it all Gemini 2.0 Flash: The visual thinker 🤔 Got visual inputs like UI mockups or diagrams? Gemini 2.0 Flash lets you bring images into the mix, making it a great choice for front-end prototyping or layout debugging. ✅ Use it for: Analyzing diagrams or screenshots. Debugging UI layouts. Generating code snippets. Getting design feedback. 👀 You may prefer another model: If the job demands step‑by‑step algorithmic reasoning, GPT‑4.5 or Claude 3.7 Sonnet will keep more moving parts in scope. So… which model do I choose? Here’s the rule of thumb: Match the model to the task. Practice really does make perfect, and as you work with different models, it’ll become clearer which ones work best for different tasks. The more I’ve personally used certain models, the more I’ve learned, “oh, I should switch for this particular task,” and “this one will get me there.” And because I enjoy staying employed, I would love to cheekily mention that you can (and should!) use these models with… GitHub Copilot in your favorite IDE GitHub Copilot on GitHub.com With agent mode or Copilot Edits With agent mode in Codespaces With agent mode in VS Code Good luck, go forth, and happy coding! Written by Sr. Director, Developer Advocacy, GitHub Related posts Explore more from GitHub Docs Everything you need to master GitHub, all in one place. Go to Docs GitHub Build what’s next on GitHub, the place for anyone from anywhere to build anything. Start building Customer stories Meet the companies and engineering teams that build with GitHub. Learn more Enterprise content Executive insights, curated just for you Get started",
  "image": "https://github.blog/wp-content/uploads/2025/04/wallpaper_copilot_generic_logo.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\n\t\n\u003cp\u003e\u003cem\u003eThis was originally published on our developer newsletter, GitHub Insider, which offers tips and tricks for devs at every level. If you’re not subscribed, \u003ca href=\"https://resources.github.com/newsletter/\"\u003ego do that now\u003c/a\u003e—you won’t regret it (we promise).\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eIf you’ve ever wondered which AI model is the best fit for your \u003ca href=\"https://github.com/features/copilot\"\u003eGitHub Copilot\u003c/a\u003e project, you’re not alone. Since each model has its own strengths, picking the right one can feel somewhat mysterious.\u003c/p\u003e\n\n\u003cp\u003eWith models that prioritize speed, depth, or a balance of both, it helps to know what each one brings to the table. Let’s break it down together. 👇\u003c/p\u003e\n\u003ch2 id=\"the-tldr\" id=\"the-tldr\"\u003eThe TL;DR\u003ca href=\"#the-tldr\" aria-label=\"The TL;DR\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e💳 \u003cstrong\u003eBalance between cost and performance\u003c/strong\u003e: Go with GPT-4.1, GPT-4o, or Claude 3.5 Sonnet.    \u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e🪙 Fast, lightweight tasks\u003c/strong\u003e: o4-mini or Claude 3.5 Sonnet are your buddies.    \u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e💎 Deep reasoning or complex debugging\u003c/strong\u003e: Think Claude 3.7 Sonnet, o3, or GPT 4.5.    \u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e🖼️ Multimodal inputs (like images)\u003c/strong\u003e: Check out Gemini 2.0 Flash or GPT-4o.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYour mileage may vary and it’s always good to try things yourself before taking someone else’s word for it, but this is how these models were designed to be used. All that being said…\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLet’s talk models.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"%f0%9f%8f%8e%ef%b8%8f-ai-models-designed-for-coding-speed\" id=\"%f0%9f%8f%8e%ef%b8%8f-ai-models-designed-for-coding-speed\"\u003e🏎️ AI models designed for coding speed\u003ca href=\"#%f0%9f%8f%8e%ef%b8%8f-ai-models-designed-for-coding-speed\" aria-label=\"🏎️ AI models designed for coding speed\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"o4-mini-and-o3-mini-the-speed-demons-%f0%9f%98%88\" id=\"o4-mini-and-o3-mini-the-speed-demons-%f0%9f%98%88\"\u003eo4-mini and o3-mini: The speed demons 😈\u003ca href=\"#o4-mini-and-o3-mini-the-speed-demons-%f0%9f%98%88\" aria-label=\"o4-mini and o3-mini: The speed demons 😈\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFast, efficient, and cost-effective, o4-mini and o3-mini are ideal for simple coding questions and quick iterations. If you’re looking for a no-frills model, use these.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e✅ Use them for:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQuick prototyping.  \u003c/li\u003e\n\u003cli\u003eExplaining code snippets.  \u003c/li\u003e\n\u003cli\u003eLearning new programming concepts.  \u003c/li\u003e\n\u003cli\u003eGenerating boilerplate code.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e👀 You may prefer another model:\u003c/strong\u003e If your task spans multiple files or calls for deep reasoning, a higher‑capacity model such as \u003cstrong\u003eGPT‑4.5\u003c/strong\u003e or \u003cstrong\u003eo3\u003c/strong\u003e can keep more context in mind. Looking for extra expressive flair? Try \u003cstrong\u003eGPT‑4o\u003c/strong\u003e.\u003c/p\u003e\n\u003chr/\u003e\n\u003ch2 id=\"%e2%9a%96%ef%b8%8f-ai-models-designed-for-balance\" id=\"%e2%9a%96%ef%b8%8f-ai-models-designed-for-balance\"\u003e⚖️ AI models designed for balance\u003ca href=\"#%e2%9a%96%ef%b8%8f-ai-models-designed-for-balance\" aria-label=\"⚖️ AI models designed for balance\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"claude-3-5-sonnet-the-budget-friendly-helper-%f0%9f%98%8a\" id=\"claude-3-5-sonnet-the-budget-friendly-helper-%f0%9f%98%8a\"\u003eClaude 3.5 Sonnet: The budget-friendly helper 😊\u003ca href=\"#claude-3-5-sonnet-the-budget-friendly-helper-%f0%9f%98%8a\" aria-label=\"Claude 3.5 Sonnet: The budget-friendly helper 😊\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eNeed solid performance but watching your costs? Claude 3.5 Sonnet is like a dependable sidekick. It’s great for everyday coding tasks without burning through your monthly usage.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e✅ Use it for:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWriting documentation.  \u003c/li\u003e\n\u003cli\u003eAnswering language-specific questions.  \u003c/li\u003e\n\u003cli\u003eGenerating code snippets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e👀 \u003cstrong\u003eYou may prefer another model:\u003c/strong\u003e For elaborate multi‑step reasoning or big‑picture planning, consider stepping up to \u003cstrong\u003eClaude 3.7 Sonnet\u003c/strong\u003e or \u003cstrong\u003eGPT‑4.5\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 id=\"gpt-4o-and-gpt-4-1-the-all-rounders-%f0%9f%8c%8e\" id=\"gpt-4o-and-gpt-4-1-the-all-rounders-%f0%9f%8c%8e\"\u003eGPT-4o and \u003ca href=\"https://github.blog/changelog/2025-04-14-openai-gpt-4-1-now-available-in-public-preview-for-github-copilot-and-github-models/\"\u003eGPT-4.1\u003c/a\u003e: The all-rounders 🌎\u003ca href=\"#gpt-4o-and-gpt-4-1-the-all-rounders-%f0%9f%8c%8e\" aria-label=\"GPT-4o and \u0026lt;a href=\u0026#34;https://github.blog/changelog/2025-04-14-openai-gpt-4-1-now-available-in-public-preview-for-github-copilot-and-github-models/\u0026#34;\u0026gt;GPT-4.1\u0026lt;/a\u0026gt;: The all-rounders 🌎\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThese are your go-to models for general tasks. Need fast responses? Check. Want to work with text *and* images? Double check. GPT-4o and GPT-4.1 are like the Swiss Army knives of AI models: flexible, dependable, and cost-efficient.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e✅ Use them for:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplaining code blocks.  \u003c/li\u003e\n\u003cli\u003eWriting comments or docs.  \u003c/li\u003e\n\u003cli\u003eGenerating small, reusable snippets.  \u003c/li\u003e\n\u003cli\u003eMultilingual prompts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e👀 \u003cstrong\u003eYou may prefer another model:\u003c/strong\u003e Complex architectural reasoning or multi‑step debugging may land more naturally with \u003cstrong\u003eGPT‑4.5\u003c/strong\u003e or \u003cstrong\u003eClaude 3.7 Sonnet\u003c/strong\u003e.\u003c/p\u003e\n\u003chr/\u003e\n\u003ch2 id=\"%f0%9f%a7%a0-ai-models-designed-for-deep-thinking-and-big-projects\" id=\"%f0%9f%a7%a0-ai-models-designed-for-deep-thinking-and-big-projects\"\u003e🧠 AI models designed for deep thinking and big projects\u003ca href=\"#%f0%9f%a7%a0-ai-models-designed-for-deep-thinking-and-big-projects\" aria-label=\"🧠 AI models designed for deep thinking and big projects\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"claude-3-7-sonnet-the-architect-%f0%9f%8f%a0\" id=\"claude-3-7-sonnet-the-architect-%f0%9f%8f%a0\"\u003eClaude 3.7 Sonnet: The architect 🏠\u003ca href=\"#claude-3-7-sonnet-the-architect-%f0%9f%8f%a0\" aria-label=\"Claude 3.7 Sonnet: The architect 🏠\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThis one’s the power tool for large, complex projects. From multi-file refactoring to feature development across front end and back end, Claude 3.7 Sonnet shines when context and depth matter most.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e✅ Use it for:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRefactoring large codebases.  \u003c/li\u003e\n\u003cli\u003ePlanning complex architectures.  \u003c/li\u003e\n\u003cli\u003eDesigning algorithms.  \u003c/li\u003e\n\u003cli\u003eCombining high-level summaries with deep analysis.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e👀 \u003cstrong\u003eYou may prefer another model:\u003c/strong\u003e For quick iterations or straightforward tasks, \u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e or \u003cstrong\u003eGPT‑4o\u003c/strong\u003e may deliver results with less overhead.\u003c/p\u003e\n\u003ch3 id=\"gemini-2-5-pro-the-researcher-%f0%9f%94%8e\" id=\"gemini-2-5-pro-the-researcher-%f0%9f%94%8e\"\u003eGemini 2.5 Pro: The researcher 🔎\u003ca href=\"#gemini-2-5-pro-the-researcher-%f0%9f%94%8e\" aria-label=\"Gemini 2.5 Pro: The researcher 🔎\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eGemini 2.5 Pro is the powerhouse for advanced reasoning and coding. It’s built for complex tasks (think: deep debugging, algorithm design, and even scientific research). With its long-context capabilities, it can handle extensive datasets or documents with ease.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e✅ Use it for:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWriting full functions, classes, or multi-file logic.  \u003c/li\u003e\n\u003cli\u003eDebugging complex systems.  \u003c/li\u003e\n\u003cli\u003eAnalyzing scientific data and generating insights.  \u003c/li\u003e\n\u003cli\u003eProcessing long documents, datasets, or codebases.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e👀 \u003cstrong\u003eYou may prefer another model:\u003c/strong\u003e For cost-sensitive tasks, \u003cstrong\u003eo4-mini\u003c/strong\u003e or \u003cstrong\u003eGemini 2.0 Flash\u003c/strong\u003e are more budget-friendly options.\u003c/p\u003e\n\u003ch3 id=\"gpt-4-5-the-thinker-%f0%9f%92%ad\" id=\"gpt-4-5-the-thinker-%f0%9f%92%ad\"\u003eGPT-4.5: The thinker 💭\u003ca href=\"#gpt-4-5-the-thinker-%f0%9f%92%ad\" aria-label=\"GPT-4.5: The thinker 💭\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eGot a tricky problem? Whether you’re debugging multi-step issues or crafting full-on systems architectures, GPT-4.5 thrives on nuance and complexity.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e✅ Use it for:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWriting detailed README files.  \u003c/li\u003e\n\u003cli\u003eGenerating full functions or multi-file solutions.  \u003c/li\u003e\n\u003cli\u003eDebugging complex errors.  \u003c/li\u003e\n\u003cli\u003eMaking architectural decisions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e👀 \u003cstrong\u003eYou may prefer another model:\u003c/strong\u003e When you just need a quick iteration on something small—or you’re watching tokens—\u003cstrong\u003eGPT‑4o\u003c/strong\u003e can finish faster and cheaper.\u003c/p\u003e\n\u003ch3 id=\"o3-and-o1-the-deep-diver-%f0%9f%a5%bd\" id=\"o3-and-o1-the-deep-diver-%f0%9f%a5%bd\"\u003eo3 and o1: The deep diver 🥽\u003ca href=\"#o3-and-o1-the-deep-diver-%f0%9f%a5%bd\" aria-label=\"o3 and o1: The deep diver 🥽\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThese models are perfect for tasks that need precision and logic. Whether you’re optimizing performance-critical code or refactoring a messy codebase, o3 and o1 excel in breaking down problems step by step.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e✅ Use them for:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCode optimization.  \u003c/li\u003e\n\u003cli\u003eDebugging complex systems.  \u003c/li\u003e\n\u003cli\u003eWriting structured, reusable code.  \u003c/li\u003e\n\u003cli\u003eSummarizing logs or benchmarks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e👀 \u003cstrong\u003eYou may prefer another model:\u003c/strong\u003e During early prototyping or lightweight tasks, a nimble model such as \u003cstrong\u003eo4‑mini\u003c/strong\u003e or \u003cstrong\u003eGPT‑4o\u003c/strong\u003e may feel snappier.\u003c/p\u003e\n\u003chr/\u003e\n\u003ch2 id=\"%f0%9f%96%bc%ef%b8%8f-multimodal-ai-models-designed-to-handle-it-all\" id=\"%f0%9f%96%bc%ef%b8%8f-multimodal-ai-models-designed-to-handle-it-all\"\u003e🖼️ Multimodal AI models designed to handle it all\u003ca href=\"#%f0%9f%96%bc%ef%b8%8f-multimodal-ai-models-designed-to-handle-it-all\" aria-label=\"🖼️ Multimodal AI models designed to handle it all\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"gemini-2-0-flash-the-visual-thinker-%f0%9f%a4%94\" id=\"gemini-2-0-flash-the-visual-thinker-%f0%9f%a4%94\"\u003eGemini 2.0 Flash: The visual thinker 🤔\u003ca href=\"#gemini-2-0-flash-the-visual-thinker-%f0%9f%a4%94\" aria-label=\"Gemini 2.0 Flash: The visual thinker 🤔\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eGot visual inputs like UI mockups or diagrams? Gemini 2.0 Flash lets you bring images into the mix, making it a great choice for front-end prototyping or layout debugging.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e✅ Use it for:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAnalyzing diagrams or screenshots.  \u003c/li\u003e\n\u003cli\u003eDebugging UI layouts.  \u003c/li\u003e\n\u003cli\u003eGenerating code snippets.  \u003c/li\u003e\n\u003cli\u003eGetting design feedback.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e👀 \u003cstrong\u003eYou may prefer another model:\u003c/strong\u003e If the job demands step‑by‑step algorithmic reasoning, \u003cstrong\u003eGPT‑4.5\u003c/strong\u003e or \u003cstrong\u003eClaude 3.7 Sonnet\u003c/strong\u003e will keep more moving parts in scope.\u003c/p\u003e\n\u003chr/\u003e\n\u003ch2 id=\"so-which-model-do-i-choose\" id=\"so-which-model-do-i-choose\"\u003eSo… which model do I choose?\u003ca href=\"#so-which-model-do-i-choose\" aria-label=\"So… which model do I choose?\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eHere’s the rule of thumb: Match the model to the task. Practice really does make perfect, and as you work with different models, it’ll become clearer which ones work best for different tasks. The more I’ve personally used certain models, the more I’ve learned, “oh, I should switch for this particular task,” and “this one will get me there.”\u003c/p\u003e\n\u003cp\u003eAnd because I enjoy staying employed, I would love to cheekily mention that you can (and should!) use these models with…\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/features/copilot\"\u003eGitHub Copilot in your favorite IDE\u003c/a\u003e  \u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/copilot\"\u003eGitHub Copilot on GitHub.com\u003c/a\u003e  \u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.blog/ai-and-ml/github-copilot/mastering-github-copilot-when-to-use-ai-agent-mode/\"\u003eWith agent mode or Copilot Edits\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.blog/changelog/2025-04-11-vscode-copilot-agent-mode-in-codespaces/\"\u003eWith agent mode in Codespaces\u003c/a\u003e  \u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.blog/news-insights/product-news/github-copilot-agent-mode-activated/\"\u003eWith agent mode in VS Code\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eGood luck, go forth, and happy coding!\u003c/p\u003e\n\n\n\t\n\n\t\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tWritten by\t\u003c/h2\u003e\n\t\n\t\t\t\u003carticle\u003e\n\t\u003cdiv\u003e\n\t\t\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cpicture\u003e\n\t\t\t\t\t\u003csource srcset=\"https://avatars.githubusercontent.com/u/1454517?v=4\u0026amp;s=200\" width=\"120\" height=\"120\" media=\"(min-width: 768px)\"/\u003e\n\t\t\t\t\t\u003cimg src=\"https://avatars.githubusercontent.com/u/1454517?v=4\u0026amp;s=200\" alt=\"Cassidy Williams\" width=\"80\" height=\"80\" loading=\"lazy\" decoding=\"async\"/\u003e\n\t\t\t\t\u003c/picture\u003e\n\t\t\t\u003c/div\u003e\n\t\t\t\t\n\t\t\t\t\t\u003cp\u003eSr. Director, Developer Advocacy, GitHub\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003c/article\u003e\n\t\u003c/div\u003e\n\u003c/section\u003e\u003csection\u003e\n\t\u003ch2\u003e\n\t\tRelated posts\t\u003c/h2\u003e\n\t\n\u003c/section\u003e\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tExplore more from GitHub\t\u003c/h2\u003e\n\t\u003cdiv\u003e\n\t\t\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon-Circle.svg\" width=\"44\" height=\"44\" alt=\"Docs\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tDocs\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eEverything you need to master GitHub, all in one place.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Go to Docs; ref_location:bottom recirculation;\" href=\"https://docs.github.com/\" target=\"_blank\" aria-label=\"Go to Docs\"\u003e\n\t\t\t\t\tGo to Docs\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_95220f.svg\" width=\"44\" height=\"44\" alt=\"GitHub\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tGitHub\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eBuild what’s next on GitHub, the place for anyone from anywhere to build anything.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Start building; ref_location:bottom recirculation;\" href=\"https://github.blog/developer-skills/github/\" target=\"_blank\" aria-label=\"Start building\"\u003e\n\t\t\t\t\tStart building\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" fill=\"none\"\u003e\u003cpath fill=\"currentColor\" d=\"M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z\"\u003e\u003c/path\u003e\u003cpath stroke=\"currentColor\" d=\"M1.75 8H11\" stroke-width=\"1.5\" stroke-linecap=\"round\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_da43dc.svg\" width=\"44\" height=\"44\" alt=\"Customer stories\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tCustomer stories\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eMeet the companies and engineering teams that build with GitHub.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Learn more; ref_location:bottom recirculation;\" href=\"https://github.com/customer-stories\" target=\"_blank\" aria-label=\"Learn more\"\u003e\n\t\t\t\t\tLearn more\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2022/05/careers.svg\" width=\"44\" height=\"44\" alt=\"Enterprise content\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tEnterprise content\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eExecutive insights, curated just for you\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Get started; ref_location:bottom recirculation;\" href=\"https://github.com/solutions/executive-insights\" target=\"_blank\" aria-label=\"Get started\"\u003e\n\t\t\t\t\tGet started\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\t\u003c/div\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-04-17T21:19:31Z",
  "modifiedTime": null
}
