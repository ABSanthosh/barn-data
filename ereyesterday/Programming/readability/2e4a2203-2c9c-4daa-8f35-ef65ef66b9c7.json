{
  "id": "2e4a2203-2c9c-4daa-8f35-ef65ef66b9c7",
  "title": "Cloudflare AutoRAG Streamlines Retrieval-Augmented Generation",
  "link": "https://www.infoq.com/news/2025/04/cloudflare-autorag-rag-llm/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Cloudflare has launched a managed service for using retrieval-augmented generationin LLM-based systems. Now in beta, CloudFlare AutoRAG aims to make it easier for developers to build pipelines that integrate rich context data into LLMs. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Wed, 30 Apr 2025 18:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Large language models",
    "Cloudflare",
    "Cloud",
    "Retrieval-Augmented Generation",
    "Development",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3460,
  "excerpt": "Cloudflare has launched a managed service for using retrieval-augmented generationin LLM-based systems. Now in beta, CloudFlare AutoRAG aims to make it easier for developers to build pipelines that in",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250422123044_u1/apple-touch-icon.png",
  "text": "Cloudflare has launched a managed service for using retrieval-augmented generationin LLM-based systems. Now in beta, CloudFlare AutoRAG aims to make it easier for developers to build pipelines that integrate rich context data into LLMs. Retrieval-augmented generation can significantly improve how accurately LLMs answer questions involving proprietary or domain-specific knowledge. However, its implementation is far from trivial, explains Cloudflare product manager Anni Wang. Building a RAG pipeline is a patchwork of moving parts. You have to stitch together multiple tools and services — your data storage, a vector database, an embedding model, LLMs, and custom indexing, retrieval, and generation logic — all just to get started. To make matters worse, the whole process must be repeated each time your knowledge base changes. To improve on this, Cloudflare AutoRAG automates all steps required for retrieval-augmented generation: it ingests the data, automatically chunks and embeds it, stores the resulting vectors in Cloudflare’s Vectorize database, performs semantic retrieval, and generates responses using Workers AI. It also monitors all data sources in the background and reruns the pipeline when needed. The two main processes behind AutoRAG are indexing and querying, explains Wang. Indexing begins by connecting a data source, which is ingested, transformed, vectorized using an embeddings model, and optimized for queries. Currently, AutoRAG supports only Cloudflare R2-based sources and can to process PDFs, images, text, HTML, CSV, and more. All files are converted into structured Markdown, including images for which a combination of object detection and vision-to-language transformation is used. The querying process starts when an end user makes a request through the AutoRAG API. The prompt is optionally rewritten to improve its effectiveness, then vectorized using the same embeddings model applied during indexing. The resulting vector is used to search the Vectorize database, returning the relevant chunks and metadata that help retrieve the original content from the R2 data source. Finally, the retrieved context is combined with the user prompt and passed to the LLM. On Linkedn, Stratus Cyber CEO Ajay Chandhok noted that \"in most cases AutoRAG implementation requires just pointing to an existing R2 bucket. You drop your content in, and the system automatically handles everything else\". Another benefit of AutoRAG, says BBC senior software engineer Nicholas Griffin, is that it \"makes querying just a few lines of code\". Some skepticism surfaced on X, where Poojan Dalal pointed out that \"production grade scalable RAG systems for enterprises have much more requirements and components than just a single pipeline\" adding that it’s not just about semantic search. Engineer Pranit Bauva, who successfully used AutoRAG to create a RAG app, also pointed out several limitations in its current form: few options for embedding and chunking, slow query rewriting, and an AI Gateway that only works with Llama models—possibly due to an early-stage bug. He also noted that retrieval quality is lacking and emphasized that, for AutoRAG to be production-ready, it must offer a way to evaluate whether the correct context was retrieved to answer a given question. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/04/cloudflare-autorag-rag-llm/en/headerimage/cloudflare-autorag-1746032444724.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://blog.cloudflare.com/introducing-autorag-on-cloudflare/\"\u003eCloudflare has launched a managed service for using retrieval-augmented generation\u003c/a\u003ein LLM-based systems. Now in beta, CloudFlare AutoRAG aims to make it easier for developers to build pipelines that integrate rich context data into LLMs.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.infoq.com/retrieval-augmented-generation/\"\u003eRetrieval-augmented generation\u003c/a\u003e can significantly \u003ca href=\"https://www.infoq.com/articles/multimodal-rag-advanced-information-retrieval/\"\u003eimprove how accurately LLMs answer questions involving proprietary or domain-specific knowledge\u003c/a\u003e. However, its implementation is far from trivial, explains Cloudflare product manager Anni Wang.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eBuilding a RAG pipeline is a patchwork of moving parts. You have to stitch together multiple tools and services — your data storage, a vector database, an embedding model, LLMs, and custom indexing, retrieval, and generation logic — all just to get started.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eTo make matters worse, the whole process must be repeated each time your knowledge base changes.\u003c/p\u003e\n\n\u003cp\u003eTo improve on this, Cloudflare AutoRAG automates all steps required for retrieval-augmented generation: it ingests the data, automatically chunks and embeds it, stores the resulting vectors in Cloudflare’s Vectorize database, performs semantic retrieval, and generates responses using Workers AI. It also monitors all data sources in the background and reruns the pipeline when needed.\u003c/p\u003e\n\n\u003cp\u003eThe two main processes behind AutoRAG are indexing and querying, explains Wang. Indexing begins by connecting a data source, which is ingested, transformed, vectorized using an embeddings model, and optimized for queries. Currently, AutoRAG supports only Cloudflare R2-based sources and can to process PDFs, images, text, HTML, CSV, and more. All files are converted into structured Markdown, including images for which a combination of object detection and vision-to-language transformation is used.\u003c/p\u003e\n\n\u003cp\u003eThe querying process starts when an end user makes a request through the AutoRAG API. The prompt is optionally rewritten to improve its effectiveness, then vectorized using the same embeddings model applied during indexing. The resulting vector is used to search the Vectorize database, returning the relevant chunks and metadata that help retrieve the original content from the R2 data source. Finally, the retrieved context is combined with the user prompt and passed to the LLM.\u003c/p\u003e\n\n\u003cp\u003eOn Linkedn, Stratus Cyber CEO Ajay Chandhok noted that \u003ca href=\"https://www.linkedin.com/posts/ajay-chandhok_cloudflare-autorag-making-advanced-ai-activity-7315531305562120193-60gv/\"\u003e\u0026#34;in most cases AutoRAG implementation requires just pointing to an existing R2 bucket. You drop your content in, and the system automatically handles everything else\u0026#34;\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eAnother benefit of AutoRAG, says BBC senior software engineer Nicholas Griffin, is that it \u003ca href=\"https://nicholasgriffin.dev/blog/cloudflare-launches-autorag\"\u003e\u0026#34;makes querying just a few lines of code\u0026#34;\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eSome skepticism surfaced on X, where Poojan Dalal pointed out that \u003ca href=\"https://x.com/poojandalal/status/1909481401373606020\"\u003e\u0026#34;production grade scalable RAG systems for enterprises have much more requirements and components than just a single pipeline\u0026#34;\u003c/a\u003e adding that it’s not just about semantic search.\u003c/p\u003e\n\n\u003cp\u003eEngineer Pranit Bauva, who successfully used AutoRAG to create a RAG app, \u003ca href=\"https://bauva.com/blog/cloudflare-autorag-first-impressions/\"\u003ealso pointed out several limitations in its current form\u003c/a\u003e: few options for embedding and chunking, slow query rewriting, and an AI Gateway that only works with Llama models—possibly due to an early-stage bug. He also noted that retrieval quality is lacking and emphasized that, for AutoRAG to be production-ready, it must offer a way to evaluate whether the correct context was retrieved to answer a given question.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-04-30T00:00:00Z",
  "modifiedTime": null
}
