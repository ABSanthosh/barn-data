{
  "id": "0832cb6d-b690-489b-ba94-8cfa93b64bcc",
  "title": "A case for QLC SSDs in the data center",
  "link": "https://engineering.fb.com/2025/03/04/data-center-engineering/a-case-for-qlc-ssds-in-the-data-center/",
  "description": "The growth of data and need for increased power efficiency are leading to innovative storage solutions. HDDs have been growing in density, but not performance, and TLC flash remains at a price point that is restrictive for scaling.  QLC technology addresses these challenges by forming a middle tier between HDDs and TLC SSDs.   QLC [...] Read More... The post A case for QLC SSDs in the data center appeared first on Engineering at Meta.",
  "author": "",
  "published": "Tue, 04 Mar 2025 17:00:26 +0000",
  "source": "https://engineering.fb.com/feed/",
  "categories": [
    "Data Center Engineering"
  ],
  "byline": "By Sumit Gupta, Jens Axboe, Madhavan Ravi, Kaushal Upadhyaya, Paul Saab",
  "length": 6803,
  "excerpt": "The growth of data and need for increased power efficiency are leading to innovative storage solutions. HDDs have been growing in density, but not performance, and TLC flash remains at a price poin…",
  "siteName": "Engineering at Meta",
  "favicon": "",
  "text": "The growth of data and need for increased power efficiency are leading to innovative storage solutions. HDDs have been growing in density, but not performance, and TLC flash remains at a price point that is restrictive for scaling.  QLC technology addresses these challenges by forming a middle tier between HDDs and TLC SSDs.   QLC provides higher density, improved power efficiency, and better cost than existing TLC SSDs.  Today, HDDs are the go-to storage solution for most data centers because of their lower cost and power footprint compared to other solutions like TLC flash. But while HDDs are growing in size, they haven’t been growing in I/O performance. In other words, the bandwidth per TB for HDDs has been dropping. This has been forcing data center engineers to meet their storage performance needs by shifting hot (frequently accessed) data to a TLC flash tier or by overprovisioning storage. QLC flash as a technology has been around since 2009. Adoption has been slow because it has historically operated at lower drive capacity points – less than 32TB. As well, high cost and limited write endurance didn’t make it an attractive alternative to TLC in the datacenter.  In the meantime, HDD densities have been growing without any significant increase in the throughput. As more data is stored on a given drive the need for I/O goes up proportionally. The continued densification of HDD capacity has led to a consistent decline in BW/TB. This has negatively affected a portion of hot workloads and forced bytes to get stranded on HDDs. QLC flash occupies a unique space in the performance spectrum in between HDDs and SSDs for servicing workloads that still depend upon performance at 10 MB/s/TB range i.e., where we had 16-20TB HDDs. Additionally there are workloads doing large batch IOs which do not need very high performance but still are in the 15-20 MB/s/TB range and use TLC flash today. QLC flash introduced as a tier above HDDs can meet write performance requirements with sufficient headroom in endurance specifications. The workloads being targeted are read-bandwidth-intensive with infrequent as well as comparatively low write bandwidth requirements. Since the bulk of power consumption in any NAND flash media comes from writes, we expect our workloads to consume lower power with QLC SSDs.  The advent of the 2Tb QLC NAND die along with 32-die stack becoming mainstream illustrates just how rapidly the density scaling of QLC flash is growing at a NAND package level as well as at drive level. We expect QLC SSD density will scale much higher than TLC SSD density in the near-term and long-term. This will bring meaningful impact to server and rack level bytes densification as well as help lower per-TB acquisition and power costs at both the drive and server level.  QLC at Meta Meta’s storage teams have started working closely with partners like Pure Storage, utilizing their DirectFlash Module (DFM) and DirectFlash software solution to bring reliable QLC storage to Meta. We are also working with other NAND vendors to integrate standard NVMe QLC SSDs into our data centers.  While today QLC is lower in cost than TLC, it is not yet price competitive enough for a broader deployment. Still, the gains in power consumption efficiency are material and the above mentioned use cases are expected to greatly benefit from that. Given that HDDs are continuing to get colder as their density increases (decreasing BW/TB), and that NAND cost structures are improving with technology advancements, we believe that adding a QLC tier is the right path forward. Hardware considerations for adopting QLC While E1.S as a form factor has been great for our TLC deployments, it’s not an ideal form factor to scale our QLC roadmap because its size limits the number of NAND packages per drive. The Industry standard U.2-15mm is still a prevalent form factor across SSD suppliers and it enables us to potentially scale to 512TB capacity. E3 doesn’t bring additional value over U.2 at the moment and the market adoption split between the 4 variants of E3 makes it less attractive. Pure Storage’s DFMs can allow scaling up to 600TB with the same NAND package technology. Designing a server to support DFMs allows the drive slot to also accept U.2 drives. This strategy enables us to reap the most benefits in cost competition, schedule acceleration, power efficiency, and vendor diversity.   The primary benefit of QLC drives is byte density at the drive and server level and the associated power efficiency. Within Meta, the byte density target of the QLC-based server is 6x the densest TLC-based server we ship today. Even though the BW/TB expected of QLC is lower than TLC, the QLC server bytes density requires a more performant CPU, faster memory and network subsystem to take advantage of the media capabilities.   Adapting our storage software for QLC  Adopting Meta’s existing storage software to QLC has presented some interesting challenges. As discussed above, our QLC systems are very high in density. And we are targeting QLC SSDs as a higher performance media compared to HDDs. This raises throughput expectations beyond any single server throughput we ever had.  Scaling such high throughput across CPU cores and sockets requires careful placement of data and compute to process that I/O. We need to make sure we minimize data touchpoints and can separate the I/O by type. The software stack in Pure Storage’s solutions uses Linux userspace block device driver (ublk) devices over io_uring to both expose the storage as a regular block device and enable zero copy for data copy elimination – as well as talk to their userspace FTL (DirectFlash software) in the background.  For other vendors, the stack uses io_uring to directly interact with the NVMe block device. Further, QLC SSDs have a significant delta between read and write throughput. Read throughput in the case of QLC can be as high as 4x or more than write throughput. What’s more, typical use cases around reads are latency sensitive so we need to make sure that the I/O delivering this massive read BW is not getting serialized behind the writes. This requires building, and carefully tuning, rate controllers and I/O schedulers. Looking forward Meta recognizes QLC flash’s potential as a viable and promising optimization opportunity for storage cost, performance, and power for data center workloads. As flash suppliers continue to invest in advanced fab processes and package designs and increase the QLC flash production output, we anticipate substantial cost improvements, making QLC flash progressively more attractive for a broader range of data center workloads. We are excited about driving innovation, fostering collaboration, and promoting ecosystem alignment in this evolving storage space.",
  "image": "https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-Server2.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe growth of data and need for increased power efficiency are leading to innovative storage solutions.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eHDDs have been growing in density, but not performance, and TLC flash remains at a price point that is restrictive for scaling. \u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eQLC technology addresses these challenges by forming a middle tier between HDDs and TLC SSDs.   QLC provides higher density, improved power efficiency, and better cost than existing TLC SSDs. \u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eToday, HDDs are the go-to storage solution for most data centers because of their lower cost and power footprint compared to other solutions like TLC flash. But while HDDs are growing in size, they haven’t been growing in I/O performance. In other words, the bandwidth per TB for HDDs has been dropping. This has been forcing data center engineers to meet their storage performance needs by shifting hot (frequently accessed) data to a TLC flash tier or by overprovisioning storage.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eQLC flash as a technology has been around since 2009. Adoption has been slow because it has historically operated at lower drive capacity points – less than 32TB. As well, high cost and limited write endurance didn’t make it an attractive alternative to TLC in the datacenter. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn the meantime, HDD densities have been growing without any significant increase in the throughput. As more data is stored on a given drive the need for I/O goes up proportionally. The continued densification of HDD capacity has led to a consistent decline in BW/TB. This has negatively affected a portion of hot workloads and forced bytes to get stranded on HDDs.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-capacity-graph.png?w=1024\" alt=\"\" width=\"1024\" height=\"641\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-capacity-graph.png 1346w, https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-capacity-graph.png?resize=916,573 916w, https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-capacity-graph.png?resize=768,480 768w, https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-capacity-graph.png?resize=1024,641 1024w, https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-capacity-graph.png?resize=96,60 96w, https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-capacity-graph.png?resize=192,120 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eQLC flash occupies a unique space in the performance spectrum in between HDDs and SSDs for servicing workloads that still depend upon performance at 10 MB/s/TB range i.e., where we had 16-20TB HDDs. Additionally there are workloads doing large batch IOs which do not need very high performance but still are in the 15-20 MB/s/TB range and use TLC flash today.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eQLC flash introduced as a tier above HDDs can meet write performance requirements with sufficient headroom in endurance specifications. The workloads being targeted are read-bandwidth-intensive with infrequent as well as comparatively low write bandwidth requirements. Since the bulk of power consumption in any NAND flash media comes from writes, we expect our workloads to consume lower power with QLC SSDs. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe advent of the 2Tb QLC NAND die along with 32-die stack becoming mainstream illustrates just how rapidly the density scaling of QLC flash is growing at a NAND package level as well as at drive level.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe expect QLC SSD density will scale much higher than TLC SSD density in the near-term and long-term. This will bring meaningful impact to server and rack level bytes densification as well as help lower per-TB acquisition and power costs at both the drive and server level. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-TLC-comparison-chart.png?w=999\" alt=\"\" width=\"999\" height=\"289\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-TLC-comparison-chart.png 999w, https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-TLC-comparison-chart.png?resize=916,265 916w, https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-TLC-comparison-chart.png?resize=768,222 768w, https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-TLC-comparison-chart.png?resize=96,28 96w, https://engineering.fb.com/wp-content/uploads/2025/03/Meta-QLC-HDD-TLC-comparison-chart.png?resize=192,56 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eQLC at Meta\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eMeta’s storage teams have started working closely with partners like \u003c/span\u003e\u003ca href=\"https://www.purestorage.com/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003ePure Storage\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, utilizing their DirectFlash Module (DFM) and DirectFlash software solution to bring reliable QLC storage to Meta. We are also working with other NAND vendors to integrate standard NVMe QLC SSDs into our data centers. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhile today QLC is lower in cost than TLC, it is not yet price competitive enough for a broader deployment. Still, the gains in power consumption efficiency are material and the above mentioned use cases are expected to greatly benefit from that. Given that HDDs are continuing to get colder as their density increases (decreasing BW/TB), and that NAND cost structures are improving with technology advancements, we believe that adding a QLC tier is the right path forward.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHardware considerations for adopting QLC\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWhile E1.S as a form factor has been great for our TLC deployments, it’s not an ideal form factor to scale our QLC roadmap because its size limits the number of NAND packages per drive.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe Industry standard U.2-15mm is still a prevalent form factor across SSD suppliers and it enables us to potentially scale to 512TB capacity. E3 doesn’t bring additional value over U.2 at the moment and the market adoption split between \u003c/span\u003e\u003ca href=\"https://www.snia.org/forums/cmsi/knowledge/formfactors\"\u003e\u003cspan\u003ethe 4 variants of E3\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e makes it less attractive. Pure Storage’s DFMs can allow scaling up to 600TB with the same NAND package technology. Designing a server to support DFMs allows the drive slot to also accept U.2 drives. This strategy enables us to reap the most benefits in cost competition, schedule acceleration, power efficiency, and vendor diversity.  \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe primary benefit of QLC drives is byte density at the drive and server level and the associated power efficiency. Within Meta, the byte density target of the QLC-based server is 6x the densest TLC-based server we ship today. Even though the BW/TB expected of QLC is lower than TLC, the QLC server bytes density requires a more performant CPU, faster memory and network subsystem to take advantage of the media capabilities.  \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eAdapting our storage software for QLC \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eAdopting Meta’s existing storage software to QLC has presented some interesting challenges. As discussed above, our QLC systems are very high in density. And we are targeting QLC SSDs as a higher performance media compared to HDDs. This raises throughput expectations beyond any single server throughput we ever had. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eScaling such high throughput across CPU cores and sockets requires careful placement of data and compute to process that I/O. We need to make sure we minimize data touchpoints and can separate the I/O by type. The software stack in Pure Storage’s solutions uses Linux userspace block device driver (ublk) devices over io_uring to both expose the storage as a regular block device and enable zero copy for data copy elimination – as well as talk to their userspace FTL (DirectFlash software) in the background. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor other vendors, the stack uses io_uring to directly interact with the NVMe block device.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFurther, QLC SSDs have a significant delta between read and write throughput. Read throughput in the case of QLC can be as high as 4x or more than write throughput. What’s more, typical use cases around reads are latency sensitive so we need to make sure that the I/O delivering this massive read BW is not getting serialized behind the writes. This requires building, and carefully tuning, rate controllers and I/O schedulers.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eLooking forward\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eMeta recognizes QLC flash’s potential as a viable and promising optimization opportunity for storage cost, performance, and power for data center workloads. As flash suppliers continue to invest in advanced fab processes and package designs and increase the QLC flash production output, we anticipate substantial cost improvements, making QLC flash progressively more attractive for a broader range of data center workloads. We are excited about driving innovation, fostering collaboration, and promoting ecosystem alignment in this evolving storage space.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-03-04T17:00:26Z",
  "modifiedTime": "2025-03-24T20:29:55Z"
}
