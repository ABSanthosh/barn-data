{
  "id": "6c3e6c26-42e9-4753-bf8e-51c6f9290856",
  "title": "Google Cloud Launches Sixth Generation Trillium TPUs: More Performance, Scalability and Efficiency",
  "link": "https://www.infoq.com/news/2024/12/google-trillium-tpu-is-ga/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Google Cloud's Trillium, its sixth-generation TPU, is now available. It enhances AI workloads with unmatched performance and 67% better energy efficiency. Integral to the AI Hypercomputer, Trillium boasts training speeds over 4x faster and triples inference throughput. This leap positions Google as a contender against Nvidia in the AI data center market. By Steef-Jan Wiggers",
  "author": "Steef-Jan Wiggers",
  "published": "Sat, 28 Dec 2024 10:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Supercomputer",
    "Infrastructure",
    "Google Cloud Platform",
    "Cloud Computing",
    "Google",
    "TensorFlow",
    "Cloud",
    "AI, ML \u0026 Data Engineering",
    "Architecture \u0026 Design",
    "Development",
    "DevOps",
    "news"
  ],
  "byline": "Steef-Jan Wiggers",
  "length": 2899,
  "excerpt": "Google Cloud's Trillium, its sixth-generation TPU, is now available. It enhances AI workloads with unmatched performance and 67% better energy efficiency. Integral to the AI Hypercomputer, Trillium bo",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241210082921/apple-touch-icon.png",
  "text": "Google Cloud has officially announced the general availability (GA) of its sixth-generation Tensor Processing Unit (TPU), known as Trillium. According to the company, the AI accelerator is designed to meet the growing demands of large-scale artificial intelligence workloads, offering more performance, energy efficiency, and scalability. Trillium was announced in May and is a key component of Google Cloud's AI Hypercomputer, a supercomputer architecture that utilizes a cohesive system of performance-optimized hardware, open-source software, leading machine learning frameworks, and adaptable consumption models. With the GA of Trillium TPUs, Google enhanced the AI Hypercomputer's software layer, optimizing the XLA compiler and popular frameworks like JAX, PyTorch, and TensorFlow for better price performance in AI training and serving. Features like host-offloading with large host DRAM complement High Bandwidth Memory (HBM) for improved efficiency. The company states that Trillium delivers training performance over four times and up to three times the inference throughput compared to the previous generation. With a 67% improvement in energy efficiency, Trillium is faster and greener, aligning with the increasing emphasis on sustainable technology. Its peak compute performance per chip is 4.7 times higher than its predecessor, making it suitable for computationally intensive tasks. Trillium TPUs were also used to train Google’s Gemini 2.0 AI model, with a correspondent on a Hacker News thread commenting: Google silicon TPUs have been used for training for at least 5 years, probably more (I think it's 10 years). They do not depend on Nvidia GPUs for the majority of their projects. It took TPUs a while to catch up on some details, like sparsity. This is followed by a comment that notes that TPUs have been used for training deep prediction models in ads since at least 2018, with TPU capacity now likely surpassing the combined capacity of CPUs and GPUs. Currently, Nvidia holds between 70% and 95 % of the AI data center chip market, while the remaining percentage comprises different versions like Google's TPUs. Google does not sell the chips directly but offers access through its cloud computing platform. In a Reddit thread, a correspondent commented regarding not selling the chips: That's right, but I think Google is more future-focused, and efficient AI will ultimately be much more valuable than chips. In my country, we often say that we should make wood products rather than export wood because making furniture creates more value. I think this is similar: TPUs and AI create more value than the two things alone. More details on pricing and availability are available on the pricing page. About the Author Steef-Jan Wiggers",
  "image": "https://res.infoq.com/news/2024/12/google-trillium-tpu-is-ga/en/headerimage/generatedHeaderImage-1735199124271.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eGoogle Cloud has officially announced the general availability (GA) of its sixth-generation \u003ca href=\"https://cloud.google.com/tpu/docs/intro-to-tpu\"\u003eTensor Processing Unit\u003c/a\u003e (TPU), known as \u003ca href=\"https://cloud.google.com/tpu/docs/v6e\"\u003eTrillium\u003c/a\u003e. According to the company, the AI accelerator is designed to meet the growing demands of large-scale artificial intelligence workloads, offering more performance, energy efficiency, and scalability.\u003c/p\u003e\n\n\u003cp\u003eTrillium was \u003ca href=\"https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus\"\u003eannounced\u003c/a\u003e in May and is a key component of Google Cloud\u0026#39;s \u003ca href=\"https://cloud.google.com/solutions/ai-hypercomputer\"\u003eAI Hypercomputer\u003c/a\u003e, a supercomputer architecture that utilizes a cohesive system of performance-optimized hardware, open-source software, leading machine learning frameworks, and adaptable consumption models.\u003c/p\u003e\n\n\u003cp\u003eWith the GA of Trillium TPUs, Google enhanced the AI Hypercomputer\u0026#39;s software layer, optimizing the XLA compiler and popular frameworks like \u003ca href=\"https://jax.readthedocs.io/en/latest/\"\u003eJAX\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/tpu/docs/tutorials/pytorch-pod\"\u003ePyTorch\u003c/a\u003e, and \u003ca href=\"https://www.tensorflow.org/\"\u003eTensorFlow\u003c/a\u003e for better price performance in AI training and serving. Features like host-offloading with large host DRAM complement High Bandwidth Memory (HBM) for improved efficiency.\u003c/p\u003e\n\n\u003cp\u003eThe company \u003ca href=\"https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga/\"\u003estates\u003c/a\u003e that Trillium delivers training performance over four times and up to three times the inference throughput compared to the previous generation. With a 67% improvement in energy efficiency, Trillium is faster and greener, aligning with the increasing emphasis on sustainable technology. Its peak compute performance per chip is 4.7 times higher than its predecessor, making it suitable for computationally intensive tasks.\u003c/p\u003e\n\n\u003cp\u003eTrillium TPUs were also used to train Google’s \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024\"\u003eGemini 2.0\u003c/a\u003e AI model, with a correspondent on a Hacker News \u003ca href=\"https://news.ycombinator.com/item?id=42388901\"\u003ethread\u003c/a\u003e commenting:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eGoogle silicon TPUs have been used for training for at least 5 years, probably more (I think it\u0026#39;s 10 years). They do not depend on Nvidia GPUs for the majority of their projects. It took TPUs a while to catch up on some details, like sparsity.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis is followed by a comment that notes that TPUs have been used for training deep prediction models in ads since at least 2018, with TPU capacity now likely surpassing the combined capacity of CPUs and GPUs.\u003c/p\u003e\n\n\u003cp\u003eCurrently, Nvidia holds \u003ca href=\"https://www.cnbc.com/2024/06/02/nvidia-dominates-the-ai-chip-market-but-theres-rising-competition\"\u003ebetween 70% and 95 % of the AI data center chip market\u003c/a\u003e, while the remaining percentage comprises different versions like Google\u0026#39;s TPUs. Google does not sell the chips directly but \u003ca href=\"https://cloud.google.com/tpu/\"\u003eoffers access through its cloud computing platform\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eIn a Reddit \u003ca href=\"https://www.reddit.com/r/singularity/comments/1hc6myi/googles_trillium_ai_chip_sets_new_performance/\"\u003ethread\u003c/a\u003e, a correspondent commented regarding not selling the chips:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThat\u0026#39;s right, but I think Google is more future-focused, and efficient AI will ultimately be much more valuable than chips.\u003c/p\u003e\n\n\u003cp\u003eIn my country, we often say that we should make wood products rather than export wood because making furniture creates more value. I think this is similar: TPUs and AI create more value than the two things alone.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eMore details on pricing and availability are available on the \u003ca href=\"https://cloud.google.com/tpu/pricing?hl=en\"\u003epricing page\u003c/a\u003e.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Steef~Jan-Wiggers\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSteef-Jan Wiggers\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-12-28T00:00:00Z",
  "modifiedTime": null
}
