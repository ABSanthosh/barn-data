{
  "id": "23e0184a-4390-4cb2-8431-2acff7aa4bf0",
  "title": "See the Similarity: Personalizing Visual Search with Multimodal Embeddings",
  "link": "https://developers.googleblog.com/en/see-the-similarity-personalizing-visual-search-with-multimodal-embeddings/",
  "description": "Learn how to build a visual search tool using Google's Multimodal Embeddings API and explore how to apply this technology for searching images, slides, and more.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Anthony Tripaldi",
  "length": 12982,
  "excerpt": "Learn how to build a visual search tool using Google's Multimodal Embeddings API and explore how to apply this technology for searching images, slides, and more.",
  "siteName": "",
  "favicon": "",
  "text": "What are Vector Embeddings?Vector embeddings are a way to represent real-world data – like text, images, or audio – mathematically, as points in a multidimensional map. This sounds incredibly dry, but with enough dimensions, they allow computers (and by extension, us) to uncover and understand the relationships in that data.For instance, you might remember \"word2vec.\" It was a revolutionary technique developed by Google in 2013 that transformed words into numerical vectors, unlocking the power of semantic understanding for machines. This breakthrough paved the way for countless advancements in natural language processing, from machine translation to sentiment analysis. We then built upon this foundation with the release of a powerful text embedding model called text-gecko, enabling developers to explore the rich semantic relationships within text.The Vertex Multimodal Embeddings API takes this a step further, by allowing you to represent text, images, and video into that same shared vector space, preserving contextual and semantic meaning across different modalities.In this post, we'll explore two practical applications of this technology: searching all of the slides and decks our team has made in the past 10 years, and an intuitive visual search tool designed for artists. We'll dive into the code and share practical tips on how you can unlock the full potential of multimodal embeddings. Section 1: Empowering Artists with Visual SearchHow it all startedRecently, our team was exploring how we might explore the recently released Multimodal Embeddings API. We recognized its potential for large corporate datasets, and we were also eager to explore more personal and creative applications.Khyati, a designer on our team who’s also a prolific illustrator, was particularly intrigued by how this technology could help her better manage and understand her work. In her words:\"Artists often struggle to locate past work based on visual similarity or conceptual keywords. Traditional file organization methods simply aren't up to the task, especially when searching by uncommon terms or abstract concepts.\"And so, our open source multimodal-embeddings demo was born! Sorry, your browser doesn't support playback for this video The demo repo is a Svelte app, whipped up during a hackathon frenzy. It might be a bit rough around the edges, but the README will steer you true. A Brief Technical OverviewWhile Khyati's dataset was considerably smaller than the million-document scale referenced in the Multimodal Embeddings API documentation, it provided an ideal test case for the new Cloud Firestore Vector Search, announced at Google Cloud Next in April.So we set up a Firebase project and sent approximately 250 of Khyati’s illustrations to the Multimodal Embeddings API. This process generated 1408-dimensional float array embeddings (providing maximum context), which we then stored in our Firestore database: mm_embedding_model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding\") # create embeddings for each image: embedding = mm_embedding_model.get_embeddings( image=image, dimension=1408, ) # create a Firestore doc to store and add to a collection doc = { \"name\": \"Illustration 1\", \"imageEmbedding\": Vector(embedding.image_embedding), ... # other metadata } khyati_collection.add(doc) Make sure to index the imageEmbedding field with the Firestore CLI .This code block was shortened for brevity, check out this notebook for a complete example. Grab the embedding model from the vertexai.vision_models packageSearching with Firestore's K-nearest neighbors (KNN) vector search is straightforward. Embed your query (just like you embedded the images) and send it to the API: // Our frontend is typescript but we have access to the same embedding API: const myQuery = 'fuzzy'; // could also be an image const myQueryVector = await getEmbeddingsForQuery(myQuery); // MM API call const vectorQuery: VectorQuery = await khyati_collection.findNearest({ vectorField: 'imageEmbedding', // name of your indexed field queryVector: myQueryVector, limit: 10, // how many documents to retrieve distanceMeasure: 'DOT_PRODUCT' // one of three algorithms for distance }); That's it! The findNearest method returns the documents closest to your query embedding, along with all associated metadata, just like a standard Firestore query.You can find our demo /search implementation here. Notice how we’re using the @google-cloud/firestore NPM library, which is the current home of this technology, as opposed to the normal firebase NPM package.Dimension Reduction BonusIf you’ve made it this far and still don’t really understand what these embedding vectors look like, that's understandable – we didn't either, at the start of this project.. We exist in a three-dimensional world, so 1408-dimensional space is pretty sci-fi.Luckily, there are lots of tools available to reduce the dimensionality of these vectors, including a wonderful implementation by the folks at Google PAIR called UMAP. Similar to t-SNE, you can take your multimodal embedding vectors and visualize them in three dimensions easily with UMAP. We’ve included the code to handle this on GitHub, including an open-source dataset of weather images and their embeddings that should be plug-and-play. Section 2: Enterprise-Scale Document SearchWhile building Khyati’s demo, we were also exploring how to flex the Multimodal Embeddings API’s muscles at its intended scale. It makes sense that Google excels in the realm of embeddings – after all, similar technology powers many of our core search products.“We have how many decks?”But how could we test it at scale? Turns out, our team's equally prolific deck creation offered an excellent proving ground. We're talking about thousands of Google Slides presentations accumulated over the past decade. Think of it as a digital archaeological dig into the history of our team's ideas.The question became: could the Multimodal Embeddings API unearth hidden treasures within this vast archive? Could our team leads finally locate that long-lost \"what was that idea, from the sprint about the thing, someone wrote it on a sticky note?\"? Could our designers easily rediscover That Amazing Poster everyone raved about? Spoiler alert: yes! Sorry, your browser doesn't support playback for this video A Brief(er) Technical OverviewThe bulk of our development time was spent wrangling the thousands of presentations and extracting thumbnails for each slide using the Drive and Slides APIs. The embedding process itself was nearly identical to the artist demo and can be summarized as follows: for preso in all_decks: for slide in preso.slides: thumbnail = slides_api.getThumbnail(slide.id, preso.id) slide_embedding = mm_embedding_model.get_embeddings( image=thumbnail, dimension=1408, ) # store slide_embedding.image_embedding in a document This process generated embeddings for over 775,000 slides across more than 16,000 presentations. To store and search this massive dataset efficiently, we turned to Vertex AI's Vector Search, specifically designed for such large-scale applications.Vertex AI's Vector Search, powered by the same technology behind Google Search, YouTube, and Play, can search billions of documents in milliseconds. It operates on similar principles to the Firestore approach we used in the artist demo, but with significantly greater scale and performance.In order to take advantage of this incredible powerful technology, you’ll need to complete a few extra steps prior to searching: # Vector Search relies on Indexes, created via code or UI, so first make sure your embeddings from the previous step are stored in a Cloud bucket, then: my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index( display_name = 'my_index_name', contents_delta_uri = BUCKET_URI, dimensions = 1408, # use same number as when you created them approximate_neighbors_count = 10, # ) # Create and Deploy this Index to an Endpoint my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create( display_name = \"my_endpoint_name\", public_endpoint_enabled = True ) my_index_endpoint.deploy_index( index = my_index, deployed_index_id = \"my_deployed_index_id\" ) # Once that's online and ready, you can query like before from your app! response = my_index_endpoint.find_neighbors( deployed_index_id = \"my_deployed_index_id\", queries = [some_query_embedding], num_neighbors = 10 ) The process is similar to Khyati's demo, but with a key difference: we create a dedicated Vector Search Index to unleash the power of ScaNN, Google's highly efficient vector similarity search algorithm.Section 3: Comparing Vertex AI and Firebase Vector SearchNow that you’ve seen both options, let’s dive into their differences.KNN vs ScaNNYou might have noticed that there were two types of algorithms associated with each vector search service: K-nearest neighbor for Firestore and ScaNN for the Vertex AI implementation. We started both demos working with Firestore as we don’t typically work with enterprise-scale solutions in our team’s day-to-day.But Firestore’s KNN search is a brute force O(n) algorithm, meaning it scales linearly with the amount of documents you add to your index. So once we started breaking 10-, 15-, 20-thousand document embeddings, things began to slow down dramatically.This slow down can be mitigated, though, with Firestore’s standard query predicates used in a “pre-filtering” step. So instead of searching through every embedding you’ve indexed, you can do a where query to limit your set to only relevant documents. This does require another composite index on the fields you want to use to filter. # creating additional indexes is easy, but still needs to be considered gcloud alpha firestore indexes composite create --collection-group=all_slides --query-scope=COLLECTION --field-config=order=ASCENDING,field-path=\"project\" # additional fields --field-config field-path=slide_embedding,vector-config='{\"dimension\":\"1408\", \"flat\": \"{}\"}' ScaNNSimilar to KNN, but relying on intelligent indexing based on the “approximate” locations (as in “Scalable Approximate Nearest Neighbor”), ScaNN was a Google Research breakthrough that was released publicly in 2020.Billions of documents can be queried in milliseconds, but that power comes at a cost, especially compared to Firestore read/writes. Plus, the indexes are slim by default — simple key/value pairs — requiring secondary lookups to your other collections or tables once the nearest neighbors are returned. But for our 775,000 slides, a ~100ms lookup + ~50ms Firestore read for the metadata was still orders of magnitude faster than what Cloud Firestore Vector Search could provide natively.There’s also some great documentation on how to combine the vector search with traditional keyword search in an approach called Hybrid Search. Read more about that here.Quick formatting asideCreating indexes for Vertex AI also required a separate jsonl key/value file format, which took some effort to convert from our original Firestore implementation. If you are unsure which to use, it might be worth writing the embeddings to an agnostic format that can easily be ingested by either system, as to not deal with the relative horror of LevelDB Firestore exports.Open Source / Local AlternativesIf a fully Cloud-hosted solution isn’t for you, you can still harness the power of the Multimodal Embeddings API with a local solution.We also tested a new library called sqlite-vec, an extremely fast, zero dependency implementation of sqlite that can run almost anywhere, and handles the 1408-dimension vectors returned by the Multimodal Embeddings API with ease. Porting over 20,000 of our slides for a test showed lookups in the ~200ms range. You’re still creating document and query embeddings online, but can handle your searching wherever you need to once they are created and stored.Some final thoughtsFrom the foundations of word2vec to today's Multimodal Embeddings API, there are new exciting possibilities for building your own multimodal AI systems to search for information.Choosing the right vector search solution depends on your needs. Firebase provides an easy-to-use and cost-effective option for smaller projects, while Vertex AI offers the scalability and performance required for large datasets and millisecond search times. For local development, tools like sqlite-vec allow you to harness the power of embeddings mostly offline.Ready to explore the future of multimodal search? Dive into our open-source multimodal-embeddings demo on GitHub, experiment with the code, and share your own creations. We're excited to see what you build.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Multimodal-Embeddings-Creative-La.2e16d0ba.fill-1200x600_qVh5IBH.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003ch2 data-block-key=\"yndf1\"\u003eWhat are Vector Embeddings?\u003c/h2\u003e\u003cp data-block-key=\"rdq8\"\u003eVector embeddings are a way to represent real-world data – like text, images, or audio – mathematically, as points in a multidimensional map. This sounds incredibly dry, but with enough dimensions, they allow computers (and by extension, us) to uncover and understand the relationships in that data.\u003c/p\u003e\u003cp data-block-key=\"ftt6k\"\u003eFor instance, you might remember \u0026#34;\u003ca href=\"https://code.google.com/archive/p/word2vec/\"\u003eword2vec\u003c/a\u003e.\u0026#34; It was a revolutionary technique developed by Google in 2013 that transformed words into numerical vectors, unlocking the power of semantic understanding for machines. This breakthrough paved the way for countless advancements in natural language processing, from machine translation to sentiment analysis.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"yndf1\"\u003eWe then built upon this foundation with the release of a powerful text embedding model called \u003ca href=\"https://arxiv.org/abs/2403.20327\"\u003e\u003ccode\u003etext-gecko\u003c/code\u003e\u003c/a\u003e, enabling developers to explore the rich semantic relationships within text.\u003c/p\u003e\u003cp data-block-key=\"1q353\"\u003eThe Vertex \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings\"\u003eMultimodal Embeddings API\u003c/a\u003e takes this a step further, by allowing you to represent text, images, and video into that same shared vector space, preserving contextual and semantic meaning across different modalities.\u003c/p\u003e\u003cp data-block-key=\"aj38h\"\u003eIn this post, we\u0026#39;ll explore two practical applications of this technology: searching all of the slides and decks our team has made in the past 10 years, and an intuitive visual search tool designed for artists. \u003ci\u003eWe\u0026#39;ll dive into the code and share practical tips on how you can unlock the full potential of multimodal embeddings.\u003c/i\u003e\u003c/p\u003e\n\u003c/div\u003e   \n\n\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"guxu8\"\u003eSection 1: Empowering Artists with Visual Search\u003c/h2\u003e\u003ch3 data-block-key=\"7449g\"\u003e\u003cb\u003eHow it all started\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"2ouc5\"\u003eRecently, our team was exploring how we might explore the recently released Multimodal Embeddings API. We recognized its potential for large corporate datasets, and we were also eager to explore more personal and creative applications.\u003c/p\u003e\u003cp data-block-key=\"75d9o\"\u003eKhyati, a designer on our team who’s also a prolific illustrator, was particularly intrigued by how this technology could help her better manage and understand her work. In her words:\u003c/p\u003e\u003cblockquote data-block-key=\"76q0r\"\u003e\u003ci\u003e\u0026#34;Artists often struggle to locate past work based on visual similarity or conceptual keywords. Traditional file organization methods simply aren\u0026#39;t up to the task, especially when searching by uncommon terms or abstract concepts.\u0026#34;\u003c/i\u003e\u003c/blockquote\u003e\u003cp data-block-key=\"baanb\"\u003eAnd so, our \u003ca href=\"https://github.com/googlecreativelab/gemini-demos/tree/main/multimodal-embeddings\"\u003e\u003ccode\u003eopen source multimodal-embeddings demo\u003c/code\u003e\u003c/a\u003e was born!\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-xzprf1xg_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/Personal-Search-Vector-Embedding.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n        \n            \u003cp\u003e\n                The demo repo is a Svelte app, whipped up during a hackathon frenzy. It might be a bit rough around the edges, but the README will steer you true.\n            \u003c/p\u003e\n        \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"4r9jq\"\u003e\u003cb\u003eA Brief Technical Overview\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"7i7oa\"\u003eWhile Khyati\u0026#39;s dataset was considerably smaller than the million-document scale referenced in the Multimodal Embeddings API documentation, it provided an ideal test case for the new \u003ca href=\"https://cloud.google.com/blog/products/databases/get-started-with-firestore-vector-similarity-search\"\u003eCloud Firestore Vector Search\u003c/a\u003e, announced at Google Cloud Next in April.\u003c/p\u003e\u003cp data-block-key=\"17nkb\"\u003eSo we \u003ca href=\"https://firebase.google.com/docs/firestore/quickstart\"\u003eset up a Firebase project\u003c/a\u003e and sent approximately 250 of Khyati’s illustrations to the Multimodal Embeddings API. This process generated 1408-dimensional float array embeddings (providing maximum context), which we then stored in our Firestore database:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003emm_embedding_model\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eMultiModalEmbeddingModel\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_pretrained\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;multimodalembedding\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e# create embeddings for each image:\u003c/span\u003e\n\u003cspan\u003eembedding\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003emm_embedding_model\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eget_embeddings\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003eimage\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eimage\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003edimension\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1408\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e# create a Firestore doc to store  and add to a collection\u003c/span\u003e\n\u003cspan\u003edoc\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n    \u003cspan\u003e\u0026#34;name\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#34;Illustration 1\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#34;imageEmbedding\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eVector\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eembedding\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eimage_embedding\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e\n    \u003cspan\u003e...\u003c/span\u003e \u003cspan\u003e# other metadata\u003c/span\u003e\n\u003cspan\u003e}\u003c/span\u003e\n\u003cspan\u003ekhyati_collection\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eadd\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edoc\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"4r9jq\"\u003eMake sure to \u003ca href=\"https://firebase.google.com/docs/firestore/vector-search#create_and_manage_vector_indexes\"\u003eindex the \u003ccode\u003eimageEmbedding\u003c/code\u003e field with the Firestore CLI\u003c/a\u003e .\u003c/p\u003e\u003cblockquote data-block-key=\"bijjf\"\u003e\u003csup\u003eThis code block was shortened for brevity, check out\u003c/sup\u003e \u003ca href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/intro_multimodal_embeddings.ipynb\"\u003e\u003csup\u003ethis notebook\u003c/sup\u003e\u003c/a\u003e\u003csup\u003e for a complete example. Grab the embedding model from the\u003c/sup\u003e \u003ccode\u003e\u003csup\u003evertexai.vision_models\u003c/sup\u003e\u003c/code\u003e\u003csup\u003e package\u003c/sup\u003e\u003c/blockquote\u003e\u003cp data-block-key=\"aaah2\"\u003e\u003cbr/\u003eSearching with \u003ca href=\"https://firebase.google.com/docs/firestore/vector-search\"\u003eFirestore\u0026#39;s K-nearest neighbors (KNN) vector search\u003c/a\u003e is straightforward. Embed your query (just like you embedded the images) and send it to the API:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e// Our frontend is typescript but we have access to the same embedding API:\u003c/span\u003e\n\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emyQuery\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#39;fuzzy\u0026#39;\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e// could also be an image\u003c/span\u003e\n\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emyQueryVector\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eawait\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003egetEmbeddingsForQuery\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emyQuery\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e// MM API call\u003c/span\u003e\n\u003cspan\u003econst\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003evectorQuery\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eVectorQuery\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eawait\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ekhyati_collection\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efindNearest\u003c/span\u003e\u003cspan\u003e({\u003c/span\u003e\n\u003cspan\u003e  \u003c/span\u003e\u003cspan\u003evectorField\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#39;imageEmbedding\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e// name of your indexed field\u003c/span\u003e\n\u003cspan\u003e  \u003c/span\u003e\u003cspan\u003equeryVector\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emyQueryVector\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003cspan\u003e  \u003c/span\u003e\u003cspan\u003elimit\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e10\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e// how many documents to retrieve\u003c/span\u003e\n\u003cspan\u003e  \u003c/span\u003e\u003cspan\u003edistanceMeasure\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#39;DOT_PRODUCT\u0026#39;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e// one of three algorithms for distance\u003c/span\u003e\n\u003cspan\u003e});\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"4r9jq\"\u003eThat\u0026#39;s it! The \u003ccode\u003efindNearest\u003c/code\u003e method returns the documents closest to your query embedding, along with all associated metadata, just like a standard Firestore query.\u003c/p\u003e\u003cblockquote data-block-key=\"1n86s\"\u003e\u003csup\u003eYou can find our demo\u003c/sup\u003e \u003ccode\u003e\u003csup\u003e/search\u003c/sup\u003e\u003c/code\u003e\u003csup\u003e implementation\u003c/sup\u003e \u003ca href=\"https://github.com/googlecreativelab/gemini-demos/blob/main/multimodal-embeddings/src/lib/cloud-firebase.ts#L28\"\u003e\u003csup\u003ehere\u003c/sup\u003e\u003c/a\u003e\u003csup\u003e. Notice how we’re using the\u003c/sup\u003e \u003ccode\u003e\u003csup\u003e@google-cloud/firestore\u003c/sup\u003e\u003c/code\u003e\u003csup\u003e NPM library, which is the current home of this technology, as opposed to the normal\u003c/sup\u003e \u003ccode\u003e\u003csup\u003efirebase\u003c/sup\u003e\u003c/code\u003e\u003csup\u003e NPM package.\u003c/sup\u003e\u003c/blockquote\u003e\u003ch3 data-block-key=\"1grmv\"\u003e\u003cb\u003e\u003cbr/\u003eDimension Reduction Bonus\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"3ul7a\"\u003eIf you’ve made it this far and still don’t \u003ci\u003ereally\u003c/i\u003e understand what these embedding vectors look like, that\u0026#39;s understandable – we didn\u0026#39;t either, at the start of this project.. We exist in a three-dimensional world, so 1408-dimensional space is pretty sci-fi.\u003c/p\u003e\u003cp data-block-key=\"60u4i\"\u003eLuckily, there are lots of tools available to reduce the dimensionality of these vectors, including a wonderful implementation by the folks at \u003ca href=\"https://pair-code.github.io/understanding-umap/\"\u003eGoogle PAIR called UMAP\u003c/a\u003e. Similar to t-SNE, you can take your multimodal embedding vectors and visualize them in three dimensions easily with UMAP. We’ve \u003ca href=\"https://github.com/googlecreativelab/gemini-demos/tree/main/multimodal-embeddings/src/routes/viz\"\u003eincluded the code to handle this\u003c/a\u003e on GitHub, including an open-source dataset of weather images and their embeddings that should be plug-and-play.\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"4r9jq\"\u003eSection 2: Enterprise-Scale Document Search\u003c/h2\u003e\u003cp data-block-key=\"fcdv9\"\u003eWhile building Khyati’s demo, we were also exploring how to flex the Multimodal Embeddings API’s muscles at its intended scale. It makes sense that Google excels in the realm of embeddings – after all, \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings\"\u003esimilar technology powers many of our core search products\u003c/a\u003e.\u003c/p\u003e\u003ch3 data-block-key=\"94bjc\"\u003e\u003cb\u003e“We have\u003c/b\u003e \u003cb\u003e\u003ci\u003ehow many\u003c/i\u003e\u003c/b\u003e\u003cb\u003e decks?”\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"erte9\"\u003eBut how could we test it at scale? Turns out, our team\u0026#39;s equally prolific deck creation offered an excellent proving ground. We\u0026#39;re talking about \u003ci\u003ethousands\u003c/i\u003e of Google Slides presentations accumulated over the past decade. Think of it as a digital archaeological dig into the history of our team\u0026#39;s ideas.\u003c/p\u003e\u003cp data-block-key=\"fjb2l\"\u003eThe question became: could the Multimodal Embeddings API unearth hidden treasures within this vast archive? Could our team leads finally locate that long-lost \u0026#34;what was that idea, from the sprint about the thing, someone wrote it on a sticky note?\u0026#34;? Could our designers easily rediscover \u003ci\u003eThat Amazing Poster\u003c/i\u003e everyone raved about? Spoiler alert: yes!\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-enn2fc_1_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/Magic-Slide-Search.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"4r9jq\"\u003e\u003cb\u003eA Brief(er) Technical Overview\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"humo\"\u003eThe bulk of our development time was spent wrangling the thousands of presentations and extracting thumbnails for each slide using the Drive and Slides APIs. The embedding process itself was nearly identical to the artist demo and can be summarized as follows:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003efor\u003c/span\u003e \u003cspan\u003epreso\u003c/span\u003e \u003cspan\u003ein\u003c/span\u003e \u003cspan\u003eall_decks\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n  \u003cspan\u003efor\u003c/span\u003e \u003cspan\u003eslide\u003c/span\u003e \u003cspan\u003ein\u003c/span\u003e \u003cspan\u003epreso\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eslides\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n    \u003cspan\u003ethumbnail\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eslides_api\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egetThumbnail\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eslide\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eid\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003epreso\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eid\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003eslide_embedding\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003emm_embedding_model\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eget_embeddings\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n      \u003cspan\u003eimage\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003ethumbnail\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n      \u003cspan\u003edimension\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1408\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e# store slide_embedding.image_embedding in a document\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"4r9jq\"\u003eThis process generated embeddings for over \u003cb\u003e775,000 slides across more than 16,000 presentations\u003c/b\u003e. To store and search this massive dataset efficiently, we turned to \u003ca href=\"https://cloud.google.com/vertex-ai/docs/vector-search/quickstart\"\u003eVertex AI\u0026#39;s Vector Search\u003c/a\u003e, specifically designed for such large-scale applications.\u003c/p\u003e\u003cp data-block-key=\"k5ti\"\u003eVertex AI\u0026#39;s Vector Search, powered by the same technology behind Google Search, YouTube, and Play, can search billions of documents in milliseconds. It operates on similar principles to the Firestore approach we used in the artist demo, but with significantly greater scale and performance.\u003c/p\u003e\u003cp data-block-key=\"a6e05\"\u003eIn order to take advantage of this incredible powerful technology, you’ll need to complete a few extra steps prior to searching:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e# Vector Search relies on Indexes, created via code or UI, so first make sure your embeddings from the previous step are stored in a Cloud bucket, then:\u003c/span\u003e\n\u003cspan\u003emy_index\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eaiplatform\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eMatchingEngineIndex\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ecreate_tree_ah_index\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003edisplay_name\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#39;my_index_name\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003econtents_delta_uri\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eBUCKET_URI\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003edimensions\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e1408\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e# use same number as when you created them\u003c/span\u003e\n    \u003cspan\u003eapproximate_neighbors_count\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e10\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e# \u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e# Create and Deploy this Index to an Endpoint\u003c/span\u003e\n\u003cspan\u003emy_index_endpoint\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eaiplatform\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eMatchingEngineIndexEndpoint\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ecreate\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003edisplay_name\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;my_endpoint_name\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003epublic_endpoint_enabled\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eTrue\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003emy_index_endpoint\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edeploy_index\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003eindex\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003emy_index\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003edeployed_index_id\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;my_deployed_index_id\u0026#34;\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e# Once that\u0026#39;s online and ready, you can query like before from your app!\u003c/span\u003e\n\u003cspan\u003eresponse\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003emy_index_endpoint\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efind_neighbors\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003edeployed_index_id\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;my_deployed_index_id\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003equeries\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003esome_query_embedding\u003c/span\u003e\u003cspan\u003e],\u003c/span\u003e\n    \u003cspan\u003enum_neighbors\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e10\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"4r9jq\"\u003eThe process is similar to Khyati\u0026#39;s demo, but with a key difference: we create a dedicated Vector Search Index to unleash the power of \u003ca href=\"https://research.google/blog/announcing-scann-efficient-vector-similarity-search/\"\u003eScaNN\u003c/a\u003e, Google\u0026#39;s highly efficient vector similarity search algorithm.\u003c/p\u003e\u003ch2 data-block-key=\"78ra8\"\u003e\u003cbr/\u003eSection 3: Comparing Vertex AI and Firebase Vector Search\u003c/h2\u003e\u003cp data-block-key=\"ejt1s\"\u003eNow that you’ve seen both options, let’s dive into their differences.\u003c/p\u003e\u003ch3 data-block-key=\"6n1o5\"\u003e\u003cb\u003eKNN vs ScaNN\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"41tqd\"\u003eYou might have noticed that there were two types of algorithms associated with each vector search service: K-nearest neighbor for Firestore and ScaNN for the Vertex AI implementation. We started both demos working with Firestore as we don’t typically work with enterprise-scale solutions in our team’s day-to-day.\u003c/p\u003e\u003cp data-block-key=\"1hopm\"\u003eBut Firestore’s KNN search is a brute force O(n) algorithm, meaning it scales linearly with the amount of documents you add to your index. So once we started breaking 10-, 15-, 20-thousand document embeddings, things began to slow down dramatically.\u003c/p\u003e\u003cp data-block-key=\"9ack\"\u003eThis slow down \u003ci\u003ecan\u003c/i\u003e be mitigated, though, with Firestore’s standard query predicates used in a “pre-filtering” step. So instead of searching through every embedding you’ve indexed, you can do a \u003ccode\u003ewhere\u003c/code\u003e query to limit your set to only relevant documents. This does require another composite index on the fields you want to use to filter.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e# creating additional indexes is easy, but still needs to be considered\u003c/span\u003e\ngcloud\u003cspan\u003e \u003c/span\u003ealpha\u003cspan\u003e \u003c/span\u003efirestore\u003cspan\u003e \u003c/span\u003eindexes\u003cspan\u003e \u003c/span\u003ecomposite\u003cspan\u003e \u003c/span\u003ecreate\n--collection-group\u003cspan\u003e=\u003c/span\u003eall_slides\n--query-scope\u003cspan\u003e=\u003c/span\u003eCOLLECTION\n--field-config\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eorder\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003eASCENDING,field-path\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;project\u0026#34;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e# additional fields\u003c/span\u003e\n--field-config\u003cspan\u003e \u003c/span\u003efield-path\u003cspan\u003e=\u003c/span\u003eslide_embedding,vector-config\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#39;{\u0026#34;dimension\u0026#34;:\u0026#34;1408\u0026#34;, \u0026#34;flat\u0026#34;: \u0026#34;{}\u0026#34;}\u0026#39;\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"4r9jq\"\u003e\u003cb\u003eScaNN\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"chr48\"\u003eSimilar to KNN, but relying on intelligent indexing based on the “approximate” locations (as in “Scalable \u003ci\u003eApproximate\u003c/i\u003e Nearest Neighbor”), \u003ca href=\"https://research.google/blog/announcing-scann-efficient-vector-similarity-search/\"\u003eScaNN\u003c/a\u003e was a Google Research breakthrough that was released publicly in 2020.\u003c/p\u003e\u003cp data-block-key=\"5oara\"\u003eBillions of documents can be queried in milliseconds, but that power comes at a cost, especially compared to Firestore read/writes. Plus, the indexes are slim by default — simple key/value pairs — requiring secondary lookups to your other collections or tables once the nearest neighbors are returned. But for our 775,000 slides, a ~100ms lookup + ~50ms Firestore read for the metadata was still orders of magnitude faster than what Cloud Firestore Vector Search could provide natively.\u003c/p\u003e\u003cp data-block-key=\"81baq\"\u003eThere’s also some great documentation on how to combine the vector search with traditional keyword search in an approach called Hybrid Search. Read more about that \u003ca href=\"https://cloud.google.com/vertex-ai/docs/vector-search/about-hybrid-search\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cblockquote data-block-key=\"46c2t\"\u003e\u003cb\u003e\u003csup\u003eQuick formatting aside\u003c/sup\u003e\u003c/b\u003e\u003csup\u003e\u003cbr/\u003eCreating indexes for Vertex AI also required a separate\u003c/sup\u003e \u003ccode\u003e\u003csup\u003ejsonl\u003c/sup\u003e\u003c/code\u003e\u003csup\u003e key/value file format, which took some effort to convert from our original Firestore implementation. If you are unsure which to use, it might be worth writing the embeddings to an agnostic format that can easily be ingested by either system, as to not deal with the relative horror of LevelDB Firestore exports.\u003c/sup\u003e\u003c/blockquote\u003e\u003ch3 data-block-key=\"35gav\"\u003e\u003cbr/\u003e\u003cb\u003eOpen Source / Local Alternatives\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"e1j6n\"\u003eIf a fully Cloud-hosted solution isn’t for you, you can still harness the power of the Multimodal Embeddings API with a local solution.\u003c/p\u003e\u003cp data-block-key=\"6mdm8\"\u003eWe also tested a new library called \u003ca href=\"https://github.com/asg017/sqlite-vec\"\u003e\u003ccode\u003esqlite-vec\u003c/code\u003e\u003c/a\u003e, an extremely fast, zero dependency implementation of sqlite that can run almost anywhere, and handles the 1408-dimension vectors returned by the Multimodal Embeddings API with ease. Porting over 20,000 of our slides for a test showed lookups in the ~200ms range. You’re still creating document and query embeddings \u003ci\u003eonline\u003c/i\u003e, but can handle your searching wherever you need to once they are created and stored.\u003c/p\u003e\u003ch2 data-block-key=\"7vbng\"\u003e\u003cbr/\u003eSome final thoughts\u003c/h2\u003e\u003cp data-block-key=\"2cd3a\"\u003eFrom the foundations of word2vec to today\u0026#39;s Multimodal Embeddings API, there are new exciting possibilities for building your own multimodal AI systems to search for information.\u003c/p\u003e\u003cp data-block-key=\"11o2i\"\u003eChoosing the right vector search solution depends on your needs. Firebase provides an easy-to-use and cost-effective option for smaller projects, while Vertex AI offers the scalability and performance required for large datasets and millisecond search times. For local development, tools like sqlite-vec allow you to harness the power of embeddings \u003ci\u003emostly\u003c/i\u003e offline.\u003c/p\u003e\u003cp data-block-key=\"c3e9j\"\u003eReady to explore the future of multimodal search? Dive into our \u003ca href=\"https://github.com/googlecreativelab/gemini-demos/tree/main/multimodal-embeddings\"\u003eopen-source multimodal-embeddings demo on GitHub\u003c/a\u003e, experiment with the code, and share your own creations. We\u0026#39;re excited to see what you build.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": "2024-12-23T00:00:00Z",
  "modifiedTime": null
}
