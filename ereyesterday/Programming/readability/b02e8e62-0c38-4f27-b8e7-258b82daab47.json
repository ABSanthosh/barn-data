{
  "id": "b02e8e62-0c38-4f27-b8e7-258b82daab47",
  "title": "Designing Data Products: next steps",
  "link": "https://martinfowler.com/articles/designing-data-products.html#HowBigShouldDataProductsBe",
  "description": "",
  "author": "",
  "published": "2024-12-10T15:22:00+01:00",
  "source": "https://martinfowler.com/feed.atom",
  "categories": [
    "skip-home-page"
  ],
  "byline": "Kiran Prakash",
  "length": 22889,
  "excerpt": "A step-by-step, systematic approach to defining data products by working backward from use cases.",
  "siteName": "martinfowler.com",
  "favicon": "",
  "text": "One of the earliest questions organisations need to answer when adopting data mesh is: “Which data products should we build first, and how do we identify them?” Questions like “What are the boundaries of data product?”, “How big or small should it be?”, and “Which domain do they belong to?” often arise. We’ve seen many organisations get stuck in this phase, engaging in elaborate design exercises that last for months and involve endless meetings. We’ve been practicing a methodical approach to quickly answer these important design questions, offering just enough details for wider stakeholders to align on goals and understand the expected high-level outcome, while granting data product teams the autonomy to work out the implementation details and jump into action. What are data products? Before we begin designing data products, let’s first establish a shared understanding of what they are and what they aren’t. Data products are the building blocks of a data mesh, they serve analytical data, and must exhibit the eight characteristics outlined by Zhamak in her book Data Mesh: Delivering Data-Driven Value at Scale. Discoverable Data consumers should be able to easily explore available data products, locate the ones they need, and determine if they fit their use case. Addressable A data product should offer a unique, permanent address (e.g., URL, URI) that allows it to be accessed programmatically or manually. Understandable (Self Describable) Data consumers should be able to easily grasp the purpose and usage patterns of the data product by reviewing its documentation, which should include details such as its purpose, field-level descriptions, access methods, and, if applicable, a sample dataset. Trustworthy A data product should transparently communicate its service level objectives (SLOs) and adherence to them (SLIs), ensuring consumers can trust it enough to build their use cases with confidence. Natively Accessible A data product should cater to its different user personas through their preferred modes of access. For example, it might provide a canned report for managers, an easy SQL-based connection for data science workbenches, and an API for programmatic access by other backend services. Interoperable (Composable) A data product should be seamlessly composable with other data products, enabling easy linking, such as joining, filtering, and aggregation, regardless of the team or domain that created it. This requires supporting standard business keys and supporting standard access patterns. Valuable on its own A data product should represent a cohesive information concept within its domain and provide value independently, without needing joins with other data products to be useful. Secure A data product must implement robust access controls to ensure that only authorized users or systems have access, whether programmatic or manual. Encryption should be employed where appropriate, and all relevant domain-specific regulations must be strictly followed. Simply put, it's a self-contained, deployable, and valuable way to work with data. The concept applies the proven mindset and methodologies of software product development to the data space. Data products package structured, semi-structured or unstructured analytical data for effective consumption and data driven decision making, keeping in mind specific user groups and their consumption pattern for these analytical data In modern software development, we decompose software systems into easily composable units, ensuring they are discoverable, maintainable, and have committed service level objectives (SLOs). 1 Similarly, a data product is the smallest valuable unit of analytical data, sourced from data streams, operational systems, or other external sources and also other data products, packaged specifically in a way to deliver meaningful business value. It includes all the necessary machinery to efficiently achieve its stated goal using automation. Data products package structured, semi-structured or unstructured analytical data for effective consumption and data driven decision making, keeping in mind specific user groups and their consumption pattern for these analytical data. What they are not I believe a good definition not only specifies what something is, but also clarifies what it isn’t. Since data products are the foundational building blocks of your data mesh, a narrower and more specific definition makes them more valuable to your organization. A well-defined scope simplifies the creation of reusable blueprints and facilitates the development of “paved paths” for building and managing data products efficiently. Conflating data product with too many different concepts not only creates confusion among teams but also makes it significantly harder to develop reusable blueprints. With data products, we apply many effective software engineering practices to analytical data to address common ownership and quality issues. These issues, however, aren't limited to analytical data—they exist across software engineering. There’s often a tendency to tackle all ownership and quality problems in the enterprise by riding on the coattails of data mesh and data products. While the intentions are good, we've found that this approach can undermine broader data mesh transformation efforts by diluting the language and focus. One of the most prevalent misunderstandings is conflating data products with data-driven applications. Data products are natively designed for programmatic access and composability, whereas data-driven applications are primarily intended for human interaction and are not inherently composable. Here are some common misrepresentations that I’ve observed and the reasoning behind it : NameReasonsMissing Characteristic Data warehouseToo large to be an independent composable unit. not interoperable not self-describing PDF reportNot meant for programmatic access. not interoperable not native-access DashboardNot meant for programmatic access. While a data product can have a dashboard as one of its outputs or dashboards can be created by consuming one or more data products, a dashboard on its own do not qualify as a data product. not interoperable not native-access Table in a warehouseWithout proper metadata or documentation is not a data product. not self-describing not valuable on its own Kafka topic They are typically not meant for analytics. This is reflected in their storage structure — Kafka stores data as a sequence of messages in topics, unlike the column-based storage commonly used in data analytics for efficient filtering and aggregation. They can serve as sources or input ports for data products. not analytical data Working backwards from a use case Working backwards from the end goal is a core principle of software development, and we’ve found it to be highly effective in modelling data products as well. This approach forces us to focus on end users and systems, considering how they prefer to consume data products (through natively accessible output ports). It provides the data product team with a clear objective to work towards, while also introducing constraints that prevent over-design and minimise wasted time and effort. It may seem like a minor detail, but we can’t stress this enough: there's a common tendency to start with the data sources and define data products. Without the constraints of a tangible use case, you won’t know when your design is good enough to move forward with implementation, which often leads to analysis paralysis and lots of wasted effort. How to do it? The setup This process is typically conducted through a series of short workshops. Participants should include potential users of the data product, domain experts, and the team responsible for building and maintaining it. A white-boarding tool and a dedicated facilitator are essential to ensure a smooth workflow. The process Let's take a common use case we find in fashion retail. Use case: As a customer relationship manager, I need timely reports that provide insights into our most valuable and least valuable customers. This will help me take action to retain high-value customers and improve the experience of low-value customers. To address this use case, let's define a data product called “Customer Lifetime Value” (CLV). This product will assign each registered customer a score that represents their value to the business, along with recommendations for the next best action that a customer relationship manager can take based on the predicted score. Figure 1: The Customer Relations team uses the Customer Lifetime Value data product through a weekly report to guide their engagement strategies with high-value customers. Working backwards from CLV, we should consider what additional data products are needed to calculate it. These would include a basic customer profile (name, age, email, etc.) and their purchase history. Figure 2: Additional source data products are required to calculate Customer Lifetime Values If you find it difficult to describe a data product in one or two simple sentences, it’s likely not well-defined The key question we need to ask, where domain expertise is crucial, is whether each proposed data product represents a cohesive information concept. Are they valuable on their own? A useful test is to define a job description for each data product. If you find it difficult to do so concisely in one or two simple sentences, or if the description becomes too long, it’s likely not a well-defined data product. Let’s apply this test to above data products Customer Lifetime Value (CLV) : Delivers a predicted customer lifetime value as a score along with a suggested next best action for customer representatives. Customer-marketing 360 : Offers a comprehensive view of the customer from a marketing perspective. Historical Purchases: Provides a list of historical purchases (SKUs) for each customer. Returns : List of customer-initiated returns. By working backwards from the “Customer - Marketing 360”, “Historical Purchases”, and “Returns” data products, we should identify the system of records for this data. This will lead us to the relevant transactional systems that we need to integrate with in order to ingest the necessary data. Figure 3: System of records or transactional systems that expose source data products Overlay additional use cases and generalise Now, let's explore another use case that can be addressed using the same data products. We'll apply the same method of working backwards, but this time we'll first attempt to generalise the existing data products to fit the new use case. If that approach isn't sufficient, we'll then consider developing new data products. This way we’ll ensure that we are not overfitting our data products just one specific use case and they are mostly reusable. Use case: As the marketing backend team, we need to identify high-probability recommendations for upselling or cross-selling to our customers. This will enable us to drive increased revenue.. To address this use case, let's create a data product called “Product Recommendations” which will generate a list of suggested products for each customer based on their purchase history. While we can reuse most of the existing data products, we’ll need to introduce a new data product called “Products” containing details about all the items we sell. Additionally, we need to expand the “Customer-Marketing 360” data product to include gender information. Figure 4: Overlaying Product Recommendations use case while generalizing existing data products So far, we’ve been incrementally building a portfolio (interaction map) of data products to address two use cases. We recommend continuing this exercise up to five use cases; beyond that, the marginal value decreases, as most of the essential data products within a given domain should be mapped out by then. Assigning domain ownership After identifying the data products, the next step is to determine the Bounded Context or domains they logically belong to. No single data product should be owned by multiple domains, as this can lead to confusion and finger-pointing over quality issues. This is done by consulting domain experts and discussing each data product in detail. Key factors include who owns the source systems that contribute to the data product, which domain has the greatest need for it, and who is best positioned to build and manage it. In most cases, if the data product is well defined and cohesive, i.e. “valuable on its own”, the ownership will be clear. When there are multiple contenders, it's more important to assign a single owner and move forward—usually, this should be the domain with the most pressing need. A key principle is that no single data product should be owned by multiple domains, as this can lead to confusion and finger-pointing over quality issues. Figure 5: Mapping data products to their respective domains. The process of identifying the set of domains in your organization is beyond the scope of this article. For that, I recommend referring to Eric Evans' canonical book on Domain-Driven Design and the Event Storming technique. While it's important to consider domain ownership early, it’s often more efficient to have a single team develop all the necessary data products to realise the use case at the start of your data mesh journey. Splitting the work among multiple teams too early can increase coordination overhead, which is best delayed. Our recommendation is to begin with a small, cohesive team that handles all data products for the use case. As you progress, use “team cognitive load” as a guide for when to split into specific domain teams. Having a consistent blueprints for all data products will make this transition of ownership easier when the time comes. The new team can focus solely on the business logic encapsulated within the data products, while the organization-wide knowledge of how data products are built and operated is carried forward. Defining service level objectives (SLOs) SLOs will guide the architecture, solution design and implementation of the data product The next step is to define service level objectives (SLOs) for the identified data products. This process involves asking several key questions, outlined below. It is crucial to perform this exercise, particularly for consumer-oriented data products, as the desired SLOs for source-oriented products can often be inferred from these. The defined SLOs will guide the architecture, solution design and implementation of the data product, such as whether to implement a batch or real-time processing pipeline, and will also shape the initial platform capabilities needed to support it Figure 6: Guiding questions to help define Service level objectives for data products During implementation, measurable Service Level Indicators (SLIs) are derived from the defined SLOs, and platform capabilities are utilized to automatically measure and publish the results to a central dashboard or a catalog. This approach enhances transparency for data product consumers and helps build trust. Here are some excellent resources on how to achieve this: A step-by-step guide and Building An “Amazon.com” For Your Data Products. How big should data products be? For structured data, this usually means a single denormalized table, and for semi-structured or unstructured data, a single dataset. Anything larger is likely trying to do too much This is a common question during the design phase and will sound familiar to those with experience in microservices. A data product should be just large enough to represent a cohesive information concept within its domain. For structured data, this usually means a single denormalized table, and for semi-structured or unstructured data, a single dataset. Anything larger is likely trying to do too much, making it harder to explain its purpose in a clear, concise sentence and reducing its composability and reusability. While additional tables or interim datasets may exist within a data product’s pipeline, these are implementation details, similar to private methods in a class. What truly matters is the dataset or table the data product exposes for broader consumption, where aspects like SLOs, backward compatibility, and data quality come into play We’ve designed data products - what next? So far, we’ve established the logical boundaries of data products, defined their purpose, set their service level objectives (SLOs) and identified the domains they’d belong to. This foundation sets us up well for implementation. Although a complete implementation approach could warrant its own article (Implementing Data Products), I’ll highlight some key points to consider that build directly on the design work we've done so far. Identify patterns and establish paved roads Identify common patterns and create reusable blueprints for data products. When designing data products, we focus on making them simple and cohesive, with each data product dedicated to a single, well-defined function. This simplicity allows us to identify common patterns and develop reusable blueprints for data products. We focus on identifying shared patterns across input, output, transformation, data quality measurement, service levels, and access control that our defined set of dat products must adhere to. Here’s what it might look like for the above-identified set of data products: PatternOptions InputFTP, S3 bucket, API , Other data products OutputAPIs, Table, S3 bucket, ML model with an inference endpoint TransformationSQL transformations, Spark jobs Service LevelsSLIs specified by data product team; centrally measured and published by the platform Access controlRules specified by data product team; enforced by the platform Provide a seamless developer experience Once the common shared patterns are identified, it is the platform's responsibility to provide a “paved road” — an easy, compliant and self-service way to build and operate data products. Figure 7: Clear separation of responsibilities between the platform team and the data product team. In our implementations, this has been achieved through a specification-driven developer experience. The platform offers blueprints and capabilities that data product developers can leverage using declarative specifications, enabling them to assemble data products based on predefined blueprints and patterns. This approach allows developers to focus on delivering business value while the platform abstracts away common engineering concerns shared across all data products. Setup independent source control and deployment pipelines In our experience, it's beneficial for each data product identified earlier to have its own source control repository and associated deployment pipeline, allowing for independent management of its lifecycle. This repository would ideally contain all the essential structural elements needed to build and operate the data product, including: In our experience, it's beneficial for each data product to have its own source control repository and associated deployment pipeline Code or specifications to provision necessary infrastructure, such as storage and compute resources. Code for data ingestion, transformation, and output processes. Access policies and rules, defined as code or specifications. Code for measuring and reporting data quality metrics and service level indicators. Automate governance In a data mesh, data products are typically built and owned by different independent teams. We rely on automation to ensure data products are built following best practices and align with organization-wide standards, enabling seamless interoperability. Fitness functions are an excellent technique for automating governance rules. They can be implemented and executed centrally in the platform, with dashboards used to publish the results of these automated checks. This, in turn, encourages teams to play by the rules. Conclusion Since data mesh came to the fore half a decade ago, we've seen many organisations embrace its vision but struggle to operationalise it effectively. This series of articles on data products aims to provide practical, experience-based guidance to help organisations get started. I often advise my clients that if they need to prioritise one aspect of data mesh, it should be “data as a product”. Focusing on getting that right establishes a strong foundation, enabling the other pillars to follow naturally. Hopefully, the techniques outlined in this article will help you design better data products and set you up for success in your data mesh journey. Let us know how it goes!",
  "image": "https://martinfowler.com/articles/designing-data-products/card.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cp\u003eOne of the earliest questions organisations need to answer when adopting\n    data mesh is: “Which data products should we build first, and how do we\n    identify them?” Questions like “What are the boundaries of data product?”,\n    “How big or small should it be?”, and “Which domain do they belong to?”\n    often arise. We’ve seen many organisations get stuck in this phase, engaging\n    in elaborate design exercises that last for months and involve endless\n    meetings.\u003c/p\u003e\n\n\u003cp\u003eWe’ve been practicing a methodical approach to quickly answer these\n    important design questions, offering just enough details for wider\n    stakeholders to align on goals and understand the expected high-level\n    outcome, while granting data product teams the autonomy to work\n    out the implementation details and jump into action.\u003c/p\u003e\n\n\u003csection id=\"WhatAreDataProducts\"\u003e\n\u003ch2\u003eWhat are data products?\u003c/h2\u003e\n\n\u003cp\u003eBefore we begin designing data products, let’s first establish a shared\n      understanding of what they are and what they aren’t.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://martinfowler.com/articles/data-mesh-principles.html#DataAsAProduct\"\u003eData products\u003c/a\u003e are the building blocks\n      of a data mesh, they serve analytical data, and must exhibit the \u003ca href=\"https://martinfowler.com/articles/fitness-functions-data-products.html#ArchitecturalCharacteristicsOfADataProduct\"\u003e\n      eight characteristics \u003c/a\u003e outlined by Zhamak in her book\n      \u003ca href=\"https://www.amazon.com/gp/product/1492092398/ref=as_li_tl?ie=UTF8\u0026amp;camp=1789\u0026amp;creative=9325\u0026amp;creativeASIN=1492092398\u0026amp;linkCode=as2\u0026amp;tag=martinfowlerc-20\"\u003eData Mesh: Delivering Data-Driven Value\n        at Scale.\u003c/a\u003e\u003c/p\u003e\n\n\u003cdiv\u003e\n\u003cdiv\u003e\n\u003cp\u003eDiscoverable\u003c/p\u003e\n\n\u003cp\u003eData consumers should be able to easily explore available data\n            products, locate the ones they need, and determine if they fit their\n            use case.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eAddressable\u003c/p\u003e\n\n\u003cp\u003eA data product should offer a unique, permanent address\n            (e.g., URL, URI) that allows it to be accessed programmatically or manually.\n          \u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eUnderstandable (Self Describable)\u003c/p\u003e\n\n\u003cp\u003eData consumers should be able to\n            easily grasp the purpose and usage patterns of the data product by\n            reviewing its documentation, which should include details such as\n            its purpose, field-level descriptions, access methods, and, if\n            applicable, a sample dataset.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eTrustworthy\u003c/p\u003e\n\n\u003cp\u003eA data product should transparently communicate its service level\n              objectives (SLOs) and adherence to them (SLIs), ensuring consumers\n              can\n              trust\n              it enough to build their use cases with confidence.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eNatively Accessible\u003c/p\u003e\n\n\u003cp\u003eA data product should cater to its different user personas through\n            their preferred modes of access. For example, it might provide a canned\n            report for managers, an easy SQL-based connection for data science\n            workbenches, and an API for programmatic access by other backend services.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eInteroperable (Composable)\u003c/p\u003e\n\n\u003cp\u003eA data product should be seamlessly composable with other data products,\n            enabling easy linking, such as joining, filtering, and aggregation,\n            regardless of the team or domain that created it. This requires\n            supporting standard business keys and supporting standard access\n            patterns.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eValuable on its own\u003c/p\u003e\n\n\u003cp\u003eA data product should represent a cohesive information concept\n            within its domain and provide value independently, without needing\n            joins with other data products to be useful.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eSecure\u003c/p\u003e\n\n\u003cp\u003eA data product must implement robust access controls to ensure that\n          only authorized users or systems have access, whether programmatic or manual.\n          Encryption should be employed where appropriate, and all relevant\n          domain-specific regulations must be strictly followed.\n          \u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\n\u003cp\u003eSimply put, it\u0026#39;s a\n      self-contained, deployable, and valuable way to work with data. The\n      concept applies the proven mindset and methodologies of software product\n      development to the data space.\u003c/p\u003e\n\n\u003cp\u003eData products package structured, semi-structured or unstructured\n      analytical data for effective consumption and data driven decision making,\n      keeping in mind specific user groups and their consumption pattern for\n      these analytical data\u003c/p\u003e\n\n\u003cp\u003eIn modern software development, we decompose software systems into\n      easily composable units, ensuring they are discoverable, maintainable, and\n      have committed service level objectives (SLOs). \u003cspan data-footnote=\"footnote-arch-quantum\"\u003e1\u003c/span\u003e\n      Similarly, a data product\n      is the smallest valuable unit of analytical data, sourced from data\n      streams, operational systems, or other external sources and also other\n      data products, packaged specifically in a way to deliver meaningful\n      business value. It includes all the necessary machinery to efficiently\n      achieve its stated goal using automation.\u003c/p\u003e\n\n\n\n\u003cp\u003eData products package structured, semi-structured or unstructured\n      analytical data for effective consumption and data driven decision making,\n      keeping in mind specific user groups and their consumption pattern for\n      these analytical data.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"WhatTheyAreNot\"\u003e\n\u003ch2\u003eWhat they are not\u003c/h2\u003e\n\n\u003cp\u003eI believe a good definition not only specifies what something is, but\n      also clarifies what it isn’t.\u003c/p\u003e\n\n\u003cp\u003eSince data products are the foundational building blocks of your\n      data mesh, a narrower and more specific definition makes them more\n      valuable to your organization. A well-defined scope simplifies the\n      creation of reusable blueprints and facilitates the development of\n      “paved paths” for building and managing data products efficiently.\u003c/p\u003e\n\n\u003cp\u003eConflating data product with too many different concepts not only creates\n      confusion among teams but also makes it significantly harder to develop\n      reusable blueprints.\u003c/p\u003e\n\n\u003cp\u003eWith data products, we apply many\n      effective software engineering practices to analytical data to address\n      common ownership and quality issues. These issues, however, aren\u0026#39;t limited\n      to analytical data—they exist across software engineering. There’s often a\n      tendency to tackle all ownership and quality problems in the enterprise by\n      riding on the coattails of data mesh and data products. While the\n      intentions are good, we\u0026#39;ve found that this approach can undermine broader\n      data mesh transformation efforts by diluting the language and focus.\u003c/p\u003e\n\n\u003cp\u003eOne of the most prevalent misunderstandings is conflating data\n        products with data-driven applications. Data products are natively\n        designed for programmatic access and composability, whereas\n        data-driven applications are primarily intended for human interaction\n        and are not inherently composable.\n      \u003c/p\u003e\n\n\u003cp\u003eHere are some common misrepresentations that I’ve observed and the\n      reasoning behind it :\u003c/p\u003e\n\n\u003ctable\u003e\n\u003cthead\u003e\u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eReasons\u003c/th\u003e\u003cth\u003eMissing Characteristic\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\n\n\u003ctbody\u003e\n\u003ctr\u003e\u003ctd\u003eData warehouse\u003c/td\u003e\u003ctd\u003eToo large to be an independent composable unit. \u003c/td\u003e\u003ctd\u003e\n\u003cul\u003e\n\u003cli\u003enot interoperable\u003c/li\u003e\n\n\u003cli\u003enot self-describing\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/td\u003e\u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003ePDF report\u003c/td\u003e\u003ctd\u003eNot meant for programmatic access.\u003c/td\u003e\u003ctd\u003e\n\u003cul\u003e\n\u003cli\u003enot interoperable\u003c/li\u003e\n\n\u003cli\u003enot native-access\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/td\u003e\u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003eDashboard\u003c/td\u003e\u003ctd\u003eNot meant for programmatic access. While a data product can\n          have a dashboard as one of its outputs or dashboards can be created by\n          consuming one or more data products, a dashboard on its own do not\n          qualify as a data product.\u003c/td\u003e\u003ctd\u003e\n\u003cul\u003e\n\u003cli\u003enot interoperable\u003c/li\u003e\n\n\u003cli\u003enot native-access\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/td\u003e\u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003eTable in a warehouse\u003c/td\u003e\u003ctd\u003eWithout proper metadata or documentation is not a data\n          product.\u003c/td\u003e\u003ctd\u003e\n\u003cul\u003e\n\u003cli\u003enot self-describing\u003c/li\u003e\n\n\u003cli\u003enot valuable on its own\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/td\u003e\u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003eKafka topic\u003c/td\u003e\u003ctd\u003e They are typically not meant for analytics. This is reflected\n          in their storage structure — Kafka stores data as a sequence of\n          messages in topics, unlike the column-based storage commonly used in\n          data analytics for efficient filtering and aggregation. They can serve\n          as sources or input ports for data products.\u003c/td\u003e\u003ctd\u003e\n\u003cul\u003e\n\u003cli\u003enot analytical data\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/td\u003e\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/section\u003e\n\n\u003csection id=\"WorkingBackwardsFromAUseCase\"\u003e\n\u003ch2\u003eWorking backwards from a use case\u003c/h2\u003e\n\n\u003cp\u003eWorking backwards from the end goal is a core principle of software\n      development,\n      and we’ve found it to be highly effective\n      in modelling data products as well. This approach forces us to focus on\n      end users and systems, considering how they prefer to consume data\n      products (through natively accessible output ports). It provides the data\n      product team with a clear objective to work towards, while also\n      introducing constraints that prevent over-design and minimise wasted time\n      and effort.\u003c/p\u003e\n\n\u003cp\u003eIt may seem like a minor detail, but we can’t stress this enough:\n      there\u0026#39;s a common tendency to start with the data sources and define data\n      products. Without the constraints of a tangible use case, you won’t know\n      when your design is good enough to move forward with implementation, which\n      often leads to analysis paralysis and lots of wasted effort.\u003c/p\u003e\n\n\u003csection id=\"HowToDoIt\"\u003e\n\u003ch3\u003eHow to do it?\u003c/h3\u003e\n\n\u003csection id=\"TheSetup\"\u003e\n\u003ch4\u003eThe setup\u003c/h4\u003e\n\n\u003cp\u003eThis process is typically conducted through a series of \u003ca href=\"https://martinfowler.com/articles/data-mesh-accelerate-workshop.html\"\u003eshort workshops.\u003c/a\u003e Participants\n          should include potential users of the data\n          product, domain experts, and the team responsible for building and\n          maintaining it. A white-boarding tool and a dedicated facilitator\n          are essential to ensure a smooth workflow.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"TheProcess\"\u003e\n\u003ch4\u003eThe process\u003c/h4\u003e\n\n\u003cp\u003eLet\u0026#39;s take a common use case we find in fashion retail.\u003c/p\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eUse case:\u003c/p\u003e\n\n\u003cp\u003eAs a customer relationship manager, I need timely reports that\n          provide insights into our most valuable and least valuable customers.\n          This will help me take action to retain high-value customers and\n          improve the experience of low-value customers.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eTo address this use case, let\u0026#39;s define a data product called\n          \u003ci\u003e“Customer Lifetime Value”\u003c/i\u003e (CLV). This product will assign each\n          registered customer a score that represents their value to the\n          business, along with recommendations for the next best action that a\n          customer relationship manager can take based on the predicted\n          score.\u003c/p\u003e\n\n\u003cdiv id=\"interaction_map_1.png\"\u003e\u003cp\u003e\u003cimg src=\"https://martinfowler.com/articles/designing-data-products/interaction_map_1.png\"/\u003e\u003c/p\u003e\u003cp\u003eFigure 1: The Customer Relations team\n            uses the Customer Lifetime Value data product through a weekly\n            report to guide their engagement strategies with high-value customers.\u003c/p\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eWorking backwards from CLV, we should consider what additional\n          data products are needed to calculate it. These would include a basic\n          customer profile (name, age, email, etc.) and their purchase\n          history.\u003c/p\u003e\n\n\u003cdiv id=\"interaction_map_2.png\"\u003e\u003cp\u003e\u003cimg src=\"https://martinfowler.com/articles/designing-data-products/interaction_map_2.png\" width=\"900\"/\u003e\u003c/p\u003e\u003cp\u003eFigure 2: Additional source data\n            products are required to calculate Customer Lifetime Values\n          \u003c/p\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eIf you find it difficult to describe a data product in one\n          or two simple sentences, it’s likely not well-defined\u003c/p\u003e\n\n\u003cp\u003eThe key question we need to ask, where domain expertise is\n          crucial, is whether each proposed data product represents a cohesive\n          information concept. Are they valuable on their own? A useful test is\n          to define a job description for each data product. If you find it\n          difficult to do so concisely in one or two simple sentences, or if\n          the description becomes too long, it’s likely not a well-defined data\n          product.\u003c/p\u003e\n\n\u003cp\u003eLet’s apply this test to above data products\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\n\u003cdiv\u003e\n\u003cp\u003eCustomer Lifetime Value (CLV) :\u003c/p\u003e\n\n\u003cp\u003eDelivers a predicted customer lifetime value as a score along\n              with a suggested next best action for customer representatives.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eCustomer-marketing 360 : \u003c/p\u003e\n\n\u003cp\u003eOffers a comprehensive view of the\n              customer from a marketing perspective.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eHistorical Purchases: \u003c/p\u003e\n\n\u003cp\u003eProvides a list of historical purchases\n              (SKUs) for each customer.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eReturns : \u003c/p\u003e\n\n\u003cp\u003eList of customer-initiated returns.\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\n\u003cp\u003eBy working backwards from the \u003ci\u003e“Customer - Marketing 360”\u003c/i\u003e,\n            \u003ci\u003e“Historical Purchases”\u003c/i\u003e, and \u003ci\u003e“Returns”\u003c/i\u003e data\n            products, we should identify the system\n          of records for this data. This will lead us to the relevant\n          transactional systems that we need to integrate with in order to\n          ingest the necessary data.\u003c/p\u003e\n\n\u003cdiv id=\"interaction_map_3.png\"\u003e\u003cp\u003e\u003cimg src=\"https://martinfowler.com/articles/designing-data-products/interaction_map_3.png\" width=\"900\"/\u003e\u003c/p\u003e\u003cp\u003eFigure 3: System of records\n            or transactional systems that expose source data products\n            \u003c/p\u003e\n\u003c/div\u003e\n\n\n\u003c/section\u003e\n\u003c/section\u003e\n\u003c/section\u003e\n\n\u003csection id=\"OverlayAdditionalUseCasesAndGeneralise\"\u003e\n\u003ch2\u003eOverlay additional use cases and generalise\u003c/h2\u003e\n\n\u003cp\u003eNow, let\u0026#39;s explore another use case that can be addressed using the\n      same data products. We\u0026#39;ll apply the same method of working backwards, but\n      this time we\u0026#39;ll first attempt to \u003ci\u003egeneralise the existing data products\n      to fit the new use case\u003c/i\u003e. If that approach isn\u0026#39;t sufficient, we\u0026#39;ll then\n      consider developing new data products. This way we’ll ensure that we are\n      not overfitting our data products just one specific use case and they are\n      mostly reusable.\u003c/p\u003e\n\n\u003cdiv\u003e\n\u003cp\u003eUse case:\u003c/p\u003e\n\n\u003cp\u003eAs the marketing backend team, we need to identify high-probability\n        recommendations for upselling or cross-selling to our customers. This\n        will enable us to drive increased revenue..\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eTo address this use case, let\u0026#39;s create a data product called\n      \u003ci\u003e“Product Recommendations”\u003c/i\u003e which will generate a list of suggested\n      products for each customer based on their purchase history.\u003c/p\u003e\n\n\u003cp\u003eWhile we can reuse most of the existing data products, we’ll need to\n      introduce a new data product called \u003ci\u003e“Products”\u003c/i\u003e containing details about\n      all the items we sell. Additionally, we need to expand the\n      \u003ci\u003e“Customer-Marketing 360”\u003c/i\u003e data product to include gender\n        information.\u003c/p\u003e\n\n\u003cdiv id=\"interaction_map_4.png\"\u003e\u003cp\u003e\u003cimg src=\"https://martinfowler.com/articles/designing-data-products/interaction_map_4.png\" width=\"900\"/\u003e\u003c/p\u003e\u003cp\u003eFigure 4: Overlaying Product\n        Recommendations use case while generalizing existing\n      data products\u003c/p\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eSo far, we’ve been incrementally building a portfolio (interaction map) of\n      data products to address two use cases. We recommend continuing this exercise up\n      to five use cases; beyond that, the marginal value decreases, as most of the\n      essential data products within a given domain should be mapped out by then. \u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"AssigningDomainOwnership\"\u003e\n\u003ch2\u003eAssigning domain ownership\u003c/h2\u003e\n\n\u003cp\u003eAfter identifying the data products, the next step is to determine the\n      \u003ca href=\"https://martinfowler.com/bliki/BoundedContext.html\"\u003eBounded Context\u003c/a\u003e or\n      domains they logically belong to.\u003c/p\u003e\n\n\u003cp\u003eNo\n      single data product should be owned by multiple domains, as this can\n      lead to confusion and finger-pointing over quality issues.\u003c/p\u003e\n\n\u003cp\u003eThis is done by consulting domain experts and discussing each data\n      product in detail. Key factors include who owns the source systems that\n      contribute to the data product, which domain has the greatest need for it,\n      and who is best positioned to build and manage it. In most cases, if the\n      data product is well defined and cohesive, i.e. “valuable on its own”, the\n      ownership will be clear. When there are multiple contenders, it\u0026#39;s more\n      important to assign a single owner and move forward—usually, this should\n      be the domain with the most pressing need. A key principle is that \u003ci\u003eno\n      single data product should be owned by multiple domains\u003c/i\u003e, as this can\n      lead to confusion and finger-pointing over quality issues. \u003c/p\u003e\n\n\u003cdiv id=\"interaction_map_5.png\"\u003e\u003cp\u003e\u003cimg src=\"https://martinfowler.com/articles/designing-data-products/interaction_map_5.png\" width=\"900\"/\u003e\u003c/p\u003e\u003cp\u003eFigure 5: Mapping data products to their\n        respective domains.\u003c/p\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eThe process of identifying the set of domains in\n      your organization is beyond the scope of this article. For that, I\n      recommend referring to Eric Evans\u0026#39; canonical book on \u003ca href=\"https://www.amazon.com/gp/product/0321125215/ref=as_li_tl?ie=UTF8\u0026amp;camp=1789\u0026amp;creative=9325\u0026amp;creativeASIN=0321125215\u0026amp;linkCode=as2\u0026amp;tag=martinfowlerc-20\"\u003eDomain-Driven Design\u003c/a\u003e and the \u003ca href=\"https://ziobrando.blogspot.com/2013/11/introducing-event-storming.html\"\u003eEvent Storming\u003c/a\u003e technique.\u003c/p\u003e\n\n\u003cp\u003eWhile it\u0026#39;s important to consider domain ownership early, it’s\n      often more efficient to have a single team develop all the necessary data\n      products to realise the use case at the start of your data mesh journey.\n      Splitting the work among multiple teams too early can increase\n      coordination overhead, which is best delayed. Our recommendation is to\n      begin with a small, cohesive team that handles all data products for the\n      use case. As you progress, use “\u003ca href=\"https://martinfowler.com/bliki/TeamTopologies.html\"\u003eteam cognitive\n      load\u003c/a\u003e” as a guide for when to split into specific domain teams.\u003c/p\u003e\n\n\u003cp\u003eHaving a consistent blueprints for all data products will make this\n      transition of ownership easier when the time comes. The new team can\n      focus solely on the business logic encapsulated within the data\n      products, while the organization-wide knowledge of how data products are\n      built and operated is carried forward.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"DefiningServiceLevelObjectivesslos\"\u003e\n\u003ch2\u003eDefining service level objectives (SLOs)\u003c/h2\u003e\n\n\u003cp\u003eSLOs will guide the architecture, solution\n        design and implementation of the data product\u003c/p\u003e\n\n\u003cp\u003eThe next step is to define service level objectives (SLOs) for the\n      identified data products. This process involves asking several key\n      questions, outlined below. It is crucial to perform this exercise,\n      particularly for consumer-oriented data products, as the desired SLOs for\n      source-oriented products can often be inferred from these. \u003ci\u003eThe defined\n      SLOs will guide the architecture, solution design and implementation of\n      the data product\u003c/i\u003e, such as whether to implement a batch or real-time\n      processing pipeline, and will also shape the initial platform capabilities\n      needed to support it\u003c/p\u003e\n\n\u003cdiv id=\"data_product_slos.png\"\u003e\u003cp\u003e\u003cimg src=\"https://martinfowler.com/articles/designing-data-products/data_product_slos.png\" width=\"900\"/\u003e\u003c/p\u003e\u003cp\u003eFigure 6: Guiding questions to help define\n        Service level objectives for data products\u003c/p\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eDuring implementation, measurable Service Level Indicators (SLIs) are\n      derived from the defined SLOs, and platform capabilities are utilized to\n      automatically measure and publish the results to a central dashboard or a\n      catalog. This approach enhances transparency for data product consumers\n      and helps build trust. Here are some excellent resources on how to\n      achieve this:\n        \u003ca href=\"https://towardsdatascience.com/a-step-by-step-guide-to-build-an-effective-data-quality-strategy-from-scratch-9fa0b8b4900a\"\u003eA step-by-step guide\u003c/a\u003e and\n        \u003ca href=\"https://www.thoughtworks.com/insights/blog/data-strategy/building-an-amazon-com-for-your-data-products\"\u003eBuilding An “Amazon.com” For Your Data Products\u003c/a\u003e.\n      \u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"HowBigShouldDataProductsBe\"\u003e\n\u003ch2\u003eHow big should data products be?\u003c/h2\u003e\n\n\u003cp\u003eFor structured data, this usually means a single\n      denormalized table, and for semi-structured or unstructured data, a single\n      dataset. Anything larger is likely trying to do too much\u003c/p\u003e\n\n\u003cp\u003eThis is a common question during the design phase and will sound\n      familiar to those with experience in microservices. A data product should\n      be just large enough to represent a cohesive information concept within\n      its domain. \u003cb\u003eFor structured data, this usually means a single\n      denormalized table, and for semi-structured or unstructured data, a single\n      dataset\u003c/b\u003e. Anything larger is likely trying to do too much, making it\n      harder to explain its purpose in a clear, concise sentence and reducing\n      its composability and reusability.\u003c/p\u003e\n\n\u003cp\u003eWhile additional tables or interim datasets may exist within a data\n      product’s pipeline, these are implementation details, similar to private\n      methods in a class. What truly matters is the dataset or table the data\n      product exposes for broader consumption, where aspects like SLOs, backward\n      compatibility, and data quality come into play\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Wex2019veDesignedDataProducts-WhatNext\"\u003e\n\u003ch2\u003eWe’ve designed data products - what next?\u003c/h2\u003e\n\n\u003cp\u003eSo far, we’ve established the logical boundaries of data products,\n      defined their purpose, set their service level objectives (SLOs) and\n      identified the domains they’d belong to. This foundation sets us up well\n      for implementation.\u003c/p\u003e\n\n\u003cp\u003eAlthough a complete implementation approach could warrant its own\n      article (Implementing Data Products), I’ll highlight some key points to\n      consider that build directly on the design work we\u0026#39;ve done so far.\u003c/p\u003e\n\n\u003csection id=\"IdentifyPatternsAndEstablishPavedRoads\"\u003e\n\u003ch3\u003eIdentify patterns and establish paved roads\u003c/h3\u003e\n\n\u003cp\u003eIdentify common patterns and create reusable blueprints for\n        data products.\n      \u003c/p\u003e\n\n\u003cp\u003eWhen designing data\n      products, we focus on making them simple and cohesive, with each data\n      product dedicated to a single, well-defined function. This simplicity\n      allows us to identify common patterns and develop reusable blueprints for\n      data products.\u003c/p\u003e\n\n\u003cp\u003eWe focus on identifying shared patterns across input, output,\n      transformation, data quality measurement, service levels, and access\n      control that our defined set of dat products must adhere to.\u003c/p\u003e\n\n\u003cp\u003eHere’s what it might look like for the above-identified set of data products:\u003c/p\u003e\n\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\u003cth\u003ePattern\u003c/th\u003e\u003cth\u003eOptions\u003c/th\u003e\u003c/tr\u003e\n\u003c/thead\u003e\n\n\u003ctbody\u003e\n\u003ctr\u003e\u003ctd\u003eInput\u003c/td\u003e\u003ctd\u003eFTP, S3 bucket, API , Other data products\u003c/td\u003e\u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003eOutput\u003c/td\u003e\u003ctd\u003eAPIs, Table, S3 bucket, ML model with an inference endpoint\u003c/td\u003e\u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003eTransformation\u003c/td\u003e\u003ctd\u003eSQL transformations, Spark jobs\u003c/td\u003e\u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003eService Levels\u003c/td\u003e\u003ctd\u003eSLIs specified by data product team; centrally measured and published by the platform\u003c/td\u003e\u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003eAccess control\u003c/td\u003e\u003ctd\u003eRules specified by data product team; enforced by the platform\u003c/td\u003e\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/section\u003e\n\n\u003csection id=\"ProvideASeamlessDeveloperExperience\"\u003e\n\u003ch3\u003eProvide a seamless developer experience\u003c/h3\u003e\n\n\u003cp\u003eOnce the common shared patterns are identified, it is the platform\u0026#39;s\n      responsibility to provide a “paved road” — an easy, compliant and\n      self-service way to build and operate data products.\u003c/p\u003e\n\n\u003cdiv id=\"platform_product.png\"\u003e\u003cp\u003e\u003cimg src=\"https://martinfowler.com/articles/designing-data-products/platform_product.png\" width=\"900\"/\u003e\u003c/p\u003e\u003cp\u003eFigure 7: Clear separation of responsibilities\n        between the platform team and the data product team.\u003c/p\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eIn our implementations, this has been achieved through a\n      specification-driven developer experience. The platform offers\n      blueprints and capabilities that data product developers can leverage\n      using declarative specifications, enabling them to assemble data\n      products based on predefined blueprints and patterns.\u003c/p\u003e\n\n\u003cp\u003e This approach allows developers to focus on delivering\n      business value while the platform abstracts away common engineering\n      concerns shared across all data products.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"SetupIndependentSourceControlAndDeploymentPipelines\"\u003e\n\u003ch3\u003eSetup independent source control and deployment pipelines\u003c/h3\u003e\n\n\u003cp\u003e In our\n      experience, it\u0026#39;s beneficial for each data product identified earlier to\n      have its own source control repository and associated deployment pipeline,\n      allowing for independent management of its lifecycle. This repository\n      would ideally contain all the essential structural elements needed to\n      build and operate the data product, including:\u003c/p\u003e\n\n\u003cp\u003eIn our experience, it\u0026#39;s beneficial for each data product to\n      have its own source control repository and associated deployment pipeline\n      \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCode or specifications to provision necessary infrastructure, such as\n        storage and compute resources. \u003c/li\u003e\n\n\u003cli\u003eCode for data ingestion, transformation, and output processes. \u003c/li\u003e\n\n\u003cli\u003eAccess policies and rules, defined as code or specifications. \u003c/li\u003e\n\n\u003cli\u003eCode for measuring and reporting data quality metrics and service level\n        indicators.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/section\u003e\n\n\u003csection id=\"AutomateGovernance\"\u003e\n\u003ch3\u003eAutomate governance\u003c/h3\u003e\n\n\u003cp\u003eIn a data mesh, data products are typically built and owned by\n        different independent teams. We rely on automation to ensure data\n        products are built following best practices and align with\n        organization-wide standards, enabling seamless interoperability.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://martinfowler.com/articles/fitness-functions-data-products.html#WhatAreArchitecturalFitnessFunctions\"\u003eFitness functions\u003c/a\u003e are an\n          excellent technique for\n          automating\n                    governance\n        rules. They can be implemented and executed centrally in the platform,\n        with dashboards used to publish the results of these automated checks.\n        This, in turn, encourages teams to play by the rules.\u003c/p\u003e\n\u003c/section\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Conclusion\"\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\u003cp\u003eSince data mesh came to the fore half a decade ago, we\u0026#39;ve seen many\n        organisations embrace its vision but struggle to operationalise it effectively.\n        This series of articles on data products aims to provide practical,\n        experience-based guidance to help organisations get started. I often\n        advise my clients that if they need to prioritise one aspect of data\n        mesh, it should be \u003ci\u003e“data as a product”\u003c/i\u003e. Focusing on getting\n        that right establishes a strong foundation, enabling the other\n        pillars to follow naturally. Hopefully, the techniques outlined in this\n        article will help you design better data products and set you\n        up for success in your data mesh journey.\u003c/p\u003e\n\n\u003cp\u003eLet us know how it goes!\u003c/p\u003e\n\u003c/section\u003e\n\n\u003chr/\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "24 min read",
  "publishedTime": null,
  "modifiedTime": "2024-12-10T00:00:00Z"
}
