{
  "id": "84ecd079-f2db-4e2c-895d-f76452efdc31",
  "title": "Introducing TxGemma: Open models to improve therapeutics development",
  "link": "https://developers.googleblog.com/en/introducing-txgemma-open-models-improving-therapeutics-development/",
  "description": "Google DeepMind releases TxGemma, built on Gemma, which predicts therapeutic properties, and Agentic-Tx, powered by Gemini 2.0 Pro, which tackles complex research problem-solving with advanced tools.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Shekoofeh Azizi",
  "length": 5392,
  "excerpt": "Google DeepMind releases TxGemma, built on Gemma, which predicts therapeutic properties, and Agentic-Tx, powered by Gemini 2.0 Pro, which tackles complex research problem-solving with advanced tools.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "Developing a new therapeutic is risky, notoriously slow, and can cost billions of dollars. 90% of drug candidates fail beyond phase 1 trials. Today, we're excited to release TxGemma, a collection of open models designed to improve the efficiency of therapeutic development by leveraging the power of large language models.Building on Google DeepMind's Gemma, a family of lightweight, state-of-the-art open models, TxGemma is specifically trained to understand and predict the properties of therapeutic entities throughout the entire discovery process, from identifying promising targets to helping predict clinical trial outcomes. This can potentially shorten the time from lab to bedside, and reduce the costs associated with traditional methods.From Tx-LLM to TxGemmaLast October, we introduced Tx-LLM, a language model trained for a variety of therapeutic tasks related to drug development. After huge interest to use and fine-tune this model for therapeutic applications, we have developed its open successor at a practical scale: TxGemma, which we are releasing today for developers to adapt to their own therapeutic data and tasks.TxGemma models, fine-tuned from Gemma 2 using 7 million training examples, are open models designed for prediction and conversational therapeutic data analysis. These models are available in three sizes: 2B, 9B and 27B. Each size includes a ‘predict’ version, specifically tailored for narrow tasks drawn from Therapeutic Data Commons, for example predicting if a molecule is toxic.These tasks encompass:classification (e.g., will this molecule cross the blood-brain barrier?)regression (e.g., predicting a drug's binding affinity)and generation (e.g., given the product of some reaction, generate the reactant set)The largest TxGemma model (27B predict version) delivers strong performance. It's not only better than, or roughly equal to, our previous state-of-the-art generalist model (Tx-LLM) on almost every task, but it also rivals or beats many models that are specifically designed for single tasks. Specifically, it outperforms or has comparable performance to our previous model on 64 of 66 tasks (beating it on 45), and does the same against specialized models on 50 of the tasks (beating them on 26). See the TxGemma paper for detailed results.Conversational AI for deeper insightsTxGemma also includes 9B and 27B ‘chat’ versions. These models have general instruction tuning data added to their training, enabling them to explain their reasoning, answer complex questions, and engage in multi-turn discussions. For example, a researcher could ask TxGemma-Chat why it predicted a particular molecule to be toxic and receive an explanation based on the molecule's structure. This conversational capability comes at a small cost to the raw performance on therapeutic tasks compared to TxGemma-Predict.Extending TxGemma's capabilities through fine-tuningAs part of the release, we’re including a fine-tuning example Colab notebook that demonstrates how developers can adapt TxGemma to their own therapeutic data and tasks. This notebook uses the TrialBench dataset to show how to fine-tune TxGemma for predicting adverse events in clinical trials. Fine-tuning allows researchers to leverage their proprietary data to create models tailored to their unique research needs, possibly leading to even more accurate predictions that help researchers assess how safe or or effective a potential new therapy might be.Orchestrating workflows for advanced therapeutic discovery with Agentic-TxBeyond single-step predictions, we’re demonstrating how TxGemma can be integrated into agentic systems to tackle more complex research problems. Standard language models often struggle with tasks requiring up-to-date external knowledge or multi-step reasoning. To address this, we've developed Agentic-Tx, a therapeutics-focused agentic system powered by Gemini 2.0 Pro. Agentic-Tx is equipped with 18 tools, including:TxGemma as a tool for multi-step reasoningGeneral search tools from PubMed, Wikipedia and the webSpecific molecular toolsGene and protein toolsAgentic-Tx achieves state-of-the-art results on reasoning-intensive chemistry and biology tasks from benchmarks including Humanity's Last Exam and ChemBench. We are including a Colab notebook with our release to demonstrate how Agentic-Tx can be used to orchestrate complex workflows and answer multi-step research questions. Get started with TxGemmaYou can access TxGemma on both Vertex AI Model Garden and Hugging Face today. We encourage you to explore the models, try out the inference, fine-tuning, and agent Colab notebooks, and share your feedback! As an open model, TxGemma is designed to be further improved – researchers can fine-tune it with their data for specific therapeutic development use-cases. We're excited to see how the community will use TxGemma to accelerate therapeutic discovery.AcknowledgementsKey contributors to this project include: Eric Wang, Samuel Schmidgall, Fan Zhang, Paul F. Jaeger, Rory Pilgrim and Tiffany Chen. We also thank Shravya Shetty, Dale Webster, Avinatan Hassidim, Yossi Matias, Yun Liu, Rachelle Sico, Phoebe Kirk, Fereshteh Mahvar, Can \"John\" Kirmizi, Fayaz Jamil, Tim Thelin, Glenn Cameron, Victor Cotruta, David Fleet, Jon Shlens, Omar Sanseviero, Joe Fernandez, and Joëlle Barral, for their feedback and support throughout this project.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/TxGemma-metadata.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"bpefo\"\u003eDeveloping a new therapeutic is risky, notoriously slow, and can cost billions of dollars. \u003ca href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC9293739/\"\u003e90% of drug candidates fail beyond phase 1 trials\u003c/a\u003e. Today, we\u0026#39;re excited to release TxGemma, a collection of open models designed to improve the efficiency of therapeutic development by leveraging the power of large language models.\u003c/p\u003e\u003cp data-block-key=\"91njj\"\u003eBuilding on \u003ca href=\"https://ai.google.dev/gemma\"\u003eGoogle DeepMind\u0026#39;s Gemma\u003c/a\u003e, a family of lightweight, state-of-the-art open models, TxGemma is specifically trained to understand and predict the properties of therapeutic entities throughout the entire discovery process, from identifying promising targets to helping predict clinical trial outcomes. This can potentially shorten the time from lab to bedside, and reduce the costs associated with traditional methods.\u003c/p\u003e\u003ch2 data-block-key=\"b7e2u\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eFrom Tx-LLM to TxGemma\u003c/h2\u003e\u003cp data-block-key=\"empje\"\u003eLast October, we introduced \u003ca href=\"https://research.google/blog/tx-llm-supporting-therapeutic-development-with-large-language-models/\"\u003eTx-LLM\u003c/a\u003e, a language model trained for a variety of therapeutic tasks related to drug development. After huge interest to use and fine-tune this model for therapeutic applications, we have developed its open successor at a practical scale: \u003ca href=\"https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87\"\u003eTxGemma\u003c/a\u003e, which we are releasing today for developers to adapt to their own therapeutic data and tasks.\u003c/p\u003e\u003cp data-block-key=\"dc0at\"\u003eTxGemma models, fine-tuned from \u003ca href=\"https://blog.google/technology/developers/google-gemma-2/\"\u003eGemma 2\u003c/a\u003e using 7 million training examples, are open models designed for prediction and conversational therapeutic data analysis. These models are available in three sizes: 2B, 9B and 27B. Each size includes a ‘predict’ version, specifically tailored for narrow tasks drawn from \u003ca href=\"https://tdcommons.ai/\"\u003eTherapeutic Data Commons\u003c/a\u003e, for example predicting if a molecule is toxic.\u003c/p\u003e\u003cp data-block-key=\"8gc13\"\u003eThese tasks encompass:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"1jiic\"\u003eclassification (e.g., will this molecule cross the blood-brain barrier?)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"c44vq\"\u003eregression (e.g., predicting a drug\u0026#39;s binding affinity)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ceo6a\"\u003eand generation (e.g., given the product of some reaction, generate the reactant set)\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"e21kd\"\u003eThe largest TxGemma model (27B predict version) delivers strong performance. It\u0026#39;s not only better than, or roughly equal to, our previous state-of-the-art generalist model (Tx-LLM) on almost every task, but it also rivals or beats many models that are specifically designed for single tasks. Specifically, it outperforms or has comparable performance to our previous model on 64 of 66 tasks (beating it on 45), and does the same against specialized models on 50 of the tasks (beating them on 26). See the \u003ca href=\"https://goo.gle/TxGemma\"\u003eTxGemma paper\u003c/a\u003e for detailed results.\u003c/p\u003e\u003ch3 data-block-key=\"bdvt9\"\u003e\u003cb\u003e\u003cbr/\u003eConversational AI for deeper insights\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"8j4as\"\u003eTxGemma also includes 9B and 27B ‘chat’ versions. These models have general instruction tuning data added to their training, enabling them to explain their reasoning, answer complex questions, and engage in multi-turn discussions. For example, a researcher could ask TxGemma-Chat why it predicted a particular molecule to be toxic and receive an explanation based on the molecule\u0026#39;s structure. This conversational capability comes at a small cost to the raw performance on therapeutic tasks compared to TxGemma-Predict.\u003c/p\u003e\u003ch3 data-block-key=\"7guei\"\u003e\u003cb\u003e\u003cbr/\u003eExtending TxGemma\u0026#39;s capabilities through fine-tuning\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"9kkrm\"\u003eAs part of the release, we’re including a \u003ca href=\"https://github.com/google-gemini/gemma-cookbook/blob/main/TxGemma/%5BTxGemma%5DFinetune_with_Hugging_Face.ipynb\"\u003efine-tuning example Colab notebook\u003c/a\u003e that demonstrates how developers can adapt TxGemma to their own therapeutic data and tasks. This notebook uses the \u003ca href=\"https://arxiv.org/html/2407.00631v2#S5\"\u003eTrialBench dataset\u003c/a\u003e to show how to fine-tune TxGemma for predicting adverse events in clinical trials. Fine-tuning allows researchers to leverage their proprietary data to create models tailored to their unique research needs, possibly leading to even more accurate predictions that help researchers assess how safe or or effective a potential new therapy might be.\u003c/p\u003e\u003ch3 data-block-key=\"d0nls\"\u003e\u003cb\u003e\u003cbr/\u003eOrchestrating workflows for advanced therapeutic discovery with Agentic-Tx\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"24ncd\"\u003eBeyond single-step predictions, we’re demonstrating how TxGemma can be integrated into agentic systems to tackle more complex research problems. Standard language models often struggle with tasks requiring up-to-date external knowledge or multi-step reasoning. To address this, we\u0026#39;ve developed Agentic-Tx, a therapeutics-focused agentic system powered by Gemini 2.0 Pro. Agentic-Tx is equipped with 18 tools, including:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"32ga\"\u003eTxGemma as a tool for multi-step reasoning\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"28va0\"\u003eGeneral search tools from PubMed, Wikipedia and the web\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"908qd\"\u003eSpecific molecular tools\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"8prh6\"\u003eGene and protein tools\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"7h1si\"\u003eAgentic-Tx achieves state-of-the-art results on reasoning-intensive chemistry and biology tasks from benchmarks including \u003ca href=\"https://agi.safe.ai/\"\u003eHumanity\u0026#39;s Last Exam\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/2404.01475\"\u003eChemBench\u003c/a\u003e. We are including a \u003ca href=\"https://github.com/google-gemini/gemma-cookbook/blob/main/TxGemma/%5BTxGemma%5DAgentic_Demo_with_Hugging_Face.ipynb\"\u003eColab notebook\u003c/a\u003e with our release to demonstrate how Agentic-Tx can be used to orchestrate complex workflows and answer multi-step research questions.\u003c/p\u003e\n\u003c/div\u003e   \n\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"bpefo\"\u003eGet started with TxGemma\u003c/h2\u003e\u003cp data-block-key=\"euhd0\"\u003eYou can access TxGemma on both \u003ca href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/txgemma\"\u003eVertex AI Model Garden\u003c/a\u003e and \u003ca href=\"https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87\"\u003eHugging Face\u003c/a\u003e today. We encourage you to explore the models, try out the \u003ca href=\"https://github.com/google-gemini/gemma-cookbook/blob/main/TxGemma/%5BTxGemma%5DQuickstart_with_Hugging_Face.ipynb\"\u003einference\u003c/a\u003e, \u003ca href=\"https://github.com/google-gemini/gemma-cookbook/blob/main/TxGemma/%5BTxGemma%5DFinetune_with_Hugging_Face.ipynb\"\u003efine-tuning\u003c/a\u003e, and \u003ca href=\"https://github.com/google-gemini/gemma-cookbook/blob/main/TxGemma/%5BTxGemma%5DAgentic_Demo_with_Hugging_Face.ipynb\"\u003eagent\u003c/a\u003e Colab notebooks, and share your feedback! As an open model, TxGemma is designed to be further improved – researchers can fine-tune it with their data for specific therapeutic development use-cases. We\u0026#39;re excited to see how the community will use TxGemma to accelerate therapeutic discovery.\u003c/p\u003e\u003chr/\u003e\u003ch3 data-block-key=\"e5ojr\"\u003e\u003cb\u003eAcknowledgements\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"38t1m\"\u003e\u003csub\u003eKey contributors to this project include: Eric Wang, Samuel Schmidgall, Fan Zhang, Paul F. Jaeger, Rory Pilgrim and Tiffany Chen. We also thank Shravya Shetty, Dale Webster, Avinatan Hassidim, Yossi Matias, Yun Liu, Rachelle Sico, Phoebe Kirk, Fereshteh Mahvar, Can \u0026#34;John\u0026#34; Kirmizi, Fayaz Jamil, Tim Thelin, Glenn Cameron, Victor Cotruta, David Fleet, Jon Shlens, Omar Sanseviero, Joe Fernandez, and Joëlle Barral, for their feedback and support throughout this project.\u003c/sub\u003e\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-03-25T00:00:00Z",
  "modifiedTime": null
}
