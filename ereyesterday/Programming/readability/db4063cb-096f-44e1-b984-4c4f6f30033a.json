{
  "id": "db4063cb-096f-44e1-b984-4c4f6f30033a",
  "title": "OpenAI Introduces New Speech Models for Transcription and Voice Generation",
  "link": "https://www.infoq.com/news/2025/03/openai-speech-models/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "OpenAI has introduced new speech-to-text and text-to-speech models in its API, focusing on improving transcription accuracy and offering more control over AI-generated voices. These updates aim to enhance automated speech applications, making them more adaptable to different environments and use cases. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Mon, 31 Mar 2025 18:20:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Transcripts",
    "OpenAI",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 2996,
  "excerpt": "OpenAI has introduced new speech-to-text and text-to-speech models in its API, focusing on improving transcription accuracy and offering more control over AI-generated voices. These updates aim to enh",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250328105021-1/apple-touch-icon.png",
  "text": "OpenAI has introduced new speech-to-text and text-to-speech models in its API, focusing on improving transcription accuracy and offering more control over AI-generated voices. These updates aim to enhance automated speech applications, making them more adaptable to different environments and use cases. The new gpt-4o-transcribe and gpt-4o-mini-transcribe models improve word error rate (WER), outperforming previous versions, including Whisper v2 and v3. These models are designed to handle better accents, background noise, and variations in speech speed, making them more reliable in real-world scenarios such as customer support calls, meeting transcriptions, and multilingual conversations. Source: OpenAI Blog Training improvements, including reinforcement learning and exposure to a more diverse dataset, contribute to fewer transcription errors and better recognition of spoken language. These models are now available through the speech-to-text API. The gpt-4o-mini-tts model introduces a new level of steerability, allowing developers to guide how the AI speaks. For example, users can specify that a response should sound like a sympathetic customer service agent or an engaging storyteller. This added flexibility makes it easier to tailor AI-generated speech to different contexts, including automated assistance, narration, and content creation. While the voices remain synthetic, OpenAI has focused on maintaining consistency and quality to ensure they meet the needs of various applications. Reactions to the new models have been positive. Harald Wagener, a head of project management at BusinessCoDe GmbH, highlighted the range of available voice options, saying: Great playground to find the perfect style for your use case. And it sounds amazing, thanks for building and sharing! Luke McPhail compared OpenAI’s models to other industry offerings, stating: First impressions of OpenAI FM: It does not quite match AI audio leaders like ElevenLabs, but that might not matter. Its huge market share and easy-to-use API will make it appealing for developers. Developers have also appreciated the models for their seamless integration and usability. Some noted that while OpenAI's speech models may not yet surpass specialized audio solutions, their accessibility and well-structured API make them a practical choice for many applications. These new speech-to-text and text-to-speech models are now available. Developers can integrate them into their applications using the Agents SDK, streamlining the process of adding voice capabilities. OpenAI plans to further improve the intelligence and accuracy of its audio models while exploring ways for developers to create custom voices for more personalized applications. Ensuring these capabilities align with safety and ethical standards remains a priority. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2025/03/openai-speech-models/en/headerimage/generatedHeaderImage-1743443738730.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eOpenAI has introduced new \u003ca href=\"https://openai.com/index/introducing-our-next-generation-audio-models/\"\u003espeech-to-text and text-to-speech models\u003c/a\u003e in its API, focusing on improving transcription accuracy and offering more control over AI-generated voices. These updates aim to enhance automated speech applications, making them more adaptable to different environments and use cases.\u003c/p\u003e\n\n\u003cp\u003eThe new gpt-4o-transcribe and gpt-4o-mini-transcribe models improve \u003ca href=\"https://en.wikipedia.org/wiki/Word_error_rate\"\u003eword error rate (WER)\u003c/a\u003e, outperforming previous versions, including Whisper v2 and v3. These models are designed to handle better accents, background noise, and variations in speech speed, making them more reliable in real-world scenarios such as customer support calls, meeting transcriptions, and multilingual conversations.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"diagram wer\" data-src=\"news/2025/03/openai-speech-models/en/resources/1Zrzut ekranu 2025-03-31 194419-1743443737873.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/03/openai-speech-models/en/resources/1Zrzut ekranu 2025-03-31 194419-1743443737873.png\" rel=\"share\"/\u003e\u003cbr/\u003e\n\u003cem\u003eSource: OpenAI Blog\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eTraining improvements, including reinforcement learning and exposure to a more diverse dataset, contribute to fewer transcription errors and better recognition of spoken language. These models are now available through the \u003ca href=\"https://platform.openai.com/docs/guides/speech-to-text\"\u003espeech-to-text API\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe gpt-4o-mini-tts model introduces a new level of steerability, allowing developers to guide how the AI speaks. For example, users can specify that a response should sound like a sympathetic customer service agent or an engaging storyteller. This added flexibility makes it easier to tailor AI-generated speech to different contexts, including automated assistance, narration, and content creation.\u003c/p\u003e\n\n\u003cp\u003eWhile the voices remain synthetic, OpenAI has focused on maintaining consistency and quality to ensure they meet the needs of various applications.\u003c/p\u003e\n\n\u003cp\u003eReactions to the new models have been positive. Harald Wagener, a head of project management at BusinessCoDe GmbH, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:activity:7308567315841785857?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7308567315841785857%2C7308840071560916992%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287308840071560916992%2Curn%3Ali%3Aactivity%3A7308567315841785857%29\"\u003ehighlighted\u003c/a\u003e the range of available voice options, saying:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eGreat playground to find the perfect style for your use case. And it sounds amazing, thanks for building and sharing!\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eLuke McPhail compared OpenAI’s models to other industry offerings, \u003ca href=\"https://www.linkedin.com/posts/lukemcphail_openaifm-just-dropped-we-built-an-interactive-activity-7308575246666616833-30CT?utm_source=share\u0026amp;utm_medium=member_desktop\u0026amp;rcm=ACoAACX5yoEBhsg1xPtc5iaJXHCu_Rv298CmfZA\"\u003estating\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eFirst impressions of OpenAI FM: It does not quite match AI audio leaders like ElevenLabs, but that might not matter. Its huge market share and easy-to-use API will make it appealing for developers.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eDevelopers have also \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:ugcPost:7308809035955277824?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7308809035955277824%2C7308893583539884032%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287308893583539884032%2Curn%3Ali%3AugcPost%3A7308809035955277824%29\"\u003eappreciated\u003c/a\u003e the models for their seamless integration and usability. Some noted that while OpenAI\u0026#39;s speech models may not yet surpass specialized audio solutions, their accessibility and well-structured API make them a practical choice for many applications.\u003c/p\u003e\n\n\u003cp\u003eThese new speech-to-text and text-to-speech models are now available. Developers can integrate them into their applications using the \u003ca href=\"https://openai.github.io/openai-agents-python/voice/quickstart/\"\u003eAgents SDK\u003c/a\u003e, streamlining the process of adding voice capabilities.\u003c/p\u003e\n\n\u003cp\u003eOpenAI plans to further improve the intelligence and accuracy of its audio models while exploring ways for developers to create custom voices for more personalized applications. Ensuring these capabilities align with safety and ethical standards remains a priority.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-03-31T00:00:00Z",
  "modifiedTime": null
}
