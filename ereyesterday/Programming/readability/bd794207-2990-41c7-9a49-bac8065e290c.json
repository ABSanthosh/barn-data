{
  "id": "bd794207-2990-41c7-9a49-bac8065e290c",
  "title": "Investigation of a Workbench UI Latency Issue",
  "link": "https://netflixtechblog.com/investigation-of-a-workbench-ui-latency-issue-faa017b4653d?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Mon, 14 Oct 2024 20:02:47 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": [
    "debugging",
    "cpu",
    "jupyter-notebook",
    "performance"
  ],
  "byline": "Netflix Technology Blog",
  "length": 17295,
  "excerpt": "With special thanks to our stunning colleagues Amer Ather, Itay Dafna, Luca Pozzi, Matheus Leão, and Ye Ji. At Netflix, the Analytics and Developer Experience organization, part of the Data Platform…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "By: Hechao Li and Marcelo MaywormWith special thanks to our stunning colleagues Amer Ather, Itay Dafna, Luca Pozzi, Matheus Leão, and Ye Ji.OverviewAt Netflix, the Analytics and Developer Experience organization, part of the Data Platform, offers a product called Workbench. Workbench is a remote development workspace based on Titus that allows data practitioners to work with big data and machine learning use cases at scale. A common use case for Workbench is running JupyterLab Notebooks.Recently, several users reported that their JupyterLab UI becomes slow and unresponsive when running certain notebooks. This document details the intriguing process of debugging this issue, all the way from the UI down to the Linux kernel.SymptomMachine Learning engineer Luca Pozzi reported to our Data Platform team that their JupyterLab UI on their workbench becomes slow and unresponsive when running some of their Notebooks. Restarting the ipykernel process, which runs the Notebook, might temporarily alleviate the problem, but the frustration persists as more notebooks are run.Quantify the SlownessWhile we observed the issue firsthand, the term “UI being slow” is subjective and difficult to measure. To investigate this issue, we needed a quantitative analysis of the slowness.Itay Dafna devised an effective and simple method to quantify the UI slowness. Specifically, we opened a terminal via JupyterLab and held down a key (e.g., “j”) for 15 seconds while running the user’s notebook. The input to stdin is sent to the backend (i.e., JupyterLab) via a WebSocket, and the output to stdout is sent back from the backend and displayed on the UI. We then exported the .har file recording all communications from the browser and loaded it into a Notebook for analysis.Using this approach, we observed latencies ranging from 1 to 10 seconds, averaging 7.4 seconds.Blame The NotebookNow that we have an objective metric for the slowness, let’s officially start our investigation. If you have read the symptom carefully, you must have noticed that the slowness only occurs when the user runs certain notebooks but not others.Therefore, the first step is scrutinizing the specific Notebook experiencing the issue. Why does the UI always slow down after running this particular Notebook? Naturally, you would think that there must be something wrong with the code running in it.Upon closely examining the user’s Notebook, we noticed a library called pystan , which provides Python bindings to a native C++ library called stan, looked suspicious. Specifically, pystan uses asyncio. However, because there is already an existing asyncio event loop running in the Notebook process and asyncio cannot be nested by design, in order for pystan to work, the authors of pystan recommend injecting pystan into the existing event loop by using a package called nest_asyncio, a library that became unmaintained because the author unfortunately passed away.Given this seemingly hacky usage, we naturally suspected that the events injected by pystan into the event loop were blocking the handling of the WebSocket messages used to communicate with the JupyterLab UI. This reasoning sounds very plausible. However, the user claimed that there were cases when a Notebook not using pystan runs, the UI also became slow.Moreover, after several rounds of discussion with ChatGPT, we learned more about the architecture and realized that, in theory, the usage of pystan and nest_asyncio should not cause the slowness in handling the UI WebSocket for the following reasons:Even though pystan uses nest_asyncio to inject itself into the main event loop, the Notebook runs on a child process (i.e., the ipykernel process) of the jupyter-lab server process, which means the main event loop being injected by pystan is that of the ipykernel process, not the jupyter-server process. Therefore, even if pystan blocks the event loop, it shouldn’t impact the jupyter-lab main event loop that is used for UI websocket communication. See the diagram below:In other words, pystan events are injected to the event loop B in this diagram instead of event loop A. So, it shouldn’t block the UI WebSocket events.You might also think that because event loop A handles both the WebSocket events from the UI and the ZeroMQ socket events from the ipykernel process, a high volume of ZeroMQ events generated by the notebook could block the WebSocket. However, when we captured packets on the ZeroMQ socket while reproducing the issue, we didn’t observe heavy traffic on this socket that could cause such blocking.A stronger piece of evidence to rule out pystan was that we were ultimately able to reproduce the issue even without it, which I’ll dive into later.Blame Noisy NeighborsThe Workbench instance runs as a Titus container. To efficiently utilize our compute resources, Titus employs a CPU oversubscription feature, meaning the combined virtual CPUs allocated to containers exceed the number of available physical CPUs on a Titus agent. If a container is unfortunate enough to be scheduled alongside other “noisy” containers — those that consume a lot of CPU resources — it could suffer from CPU deficiency.However, after examining the CPU utilization of neighboring containers on the same Titus agent as the Workbench instance, as well as the overall CPU utilization of the Titus agent, we quickly ruled out this hypothesis. Using the top command on the Workbench, we observed that when running the Notebook, the Workbench instance uses only 4 out of the 64 CPUs allocated to it. Simply put, this workload is not CPU-bound.Blame The NetworkThe next theory was that the network between the web browser UI (on the laptop) and the JupyterLab server was slow. To investigate, we captured all the packets between the laptop and the server while running the Notebook and continuously pressing ‘j’ in the terminal.When the UI experienced delays, we observed a 5-second pause in packet transmission from server port 8888 to the laptop. Meanwhile, traffic from other ports, such as port 22 for SSH, remained unaffected. This led us to conclude that the pause was caused by the application running on port 8888 (i.e., the JupyterLab process) rather than the network.The Minimal ReproductionAs previously mentioned, another strong piece of evidence proving the innocence of pystan was that we could reproduce the issue without it. By gradually stripping down the “bad” Notebook, we eventually arrived at a minimal snippet of code that reproduces the issue without any third-party dependencies or complex logic:import timeimport osfrom multiprocessing import ProcessN = os.cpu_count()def launch_worker(worker_id): time.sleep(60)if __name__ == '__main__': with open('/root/2GB_file', 'r') as file: data = file.read() processes = [] for i in range(N): p = Process(target=launch_worker, args=(i,)) processes.append(p) p.start() for p in processes: p.join()The code does only two things:Read a 2GB file into memory (the Workbench instance has 480G memory in total so this memory usage is almost negligible).Start N processes where N is the number of CPUs. The N processes do nothing but sleep.There is no doubt that this is the most silly piece of code I’ve ever written. It is neither CPU bound nor memory bound. Yet it can cause the JupyterLab UI to stall for as many as 10 seconds!QuestionsThere are a couple of interesting observations that raise several questions:We noticed that both steps are required in order to reproduce the issue. If you don’t read the 2GB file (that is not even used!), the issue is not reproducible. Why using 2GB out of 480GB memory could impact the performance?When the UI delay occurs, the jupyter-lab process CPU utilization spikes to 100%, hinting at contention on the single-threaded event loop in this process (event loop A in the diagram before). What does the jupyter-lab process need the CPU for, given that it is not the process that runs the Notebook?The code runs in a Notebook, which means it runs in the ipykernel process, that is a child process of the jupyter-lab process. How can anything that happens in a child process cause the parent process to have CPU contention?The workbench has 64CPUs. But when we printed os.cpu_count(), the output was 96. That means the code starts more processes than the number of CPUs. Why is that?Let’s answer the last question first. In fact, if you run lscpu and nproc commands inside a Titus container, you will also see different results — the former gives you 96, which is the number of physical CPUs on the Titus agent, whereas the latter gives you 64, which is the number of virtual CPUs allocated to the container. This discrepancy is due to the lack of a “CPU namespace” in the Linux kernel, causing the number of physical CPUs to be leaked to the container when calling certain functions to get the CPU count. The assumption here is that Python os.cpu_count() uses the same function as the lscpu command, causing it to get the CPU count of the host instead of the container. Python 3.13 has a new call that can be used to get the accurate CPU count, but it’s not GA’ed yet.It will be proven later that this inaccurate number of CPUs can be a contributing factor to the slowness.More CluesNext, we used py-spy to do a profiling of the jupyter-lab process. Note that we profiled the parent jupyter-lab process, not the ipykernel child process that runs the reproduction code. The profiling result is as follows:As one can see, a lot of CPU time (89%!!) is spent on a function called __parse_smaps_rollup. In comparison, the terminal handler used only 0.47% CPU time. From the stack trace, we see that this function is inside the event loop A, so it can definitely cause the UI WebSocket events to be delayed.The stack trace also shows that this function is ultimately called by a function used by a Jupyter lab extension called jupyter_resource_usage. We then disabled this extension and restarted the jupyter-lab process. As you may have guessed, we could no longer reproduce the slowness!But our puzzle is not solved yet. Why does this extension cause the UI to slow down? Let’s keep digging.Root Cause AnalysisFrom the name of the extension and the names of the other functions it calls, we can infer that this extension is used to get resources such as CPU and memory usage information. Examining the code, we see that this function call stack is triggered when an API endpoint /metrics/v1 is called from the UI. The UI apparently calls this function periodically, according to the network traffic tab in Chrome’s Developer Tools.Now let’s look at the implementation starting from the call get(jupter_resource_usage/api.py:42) . The full code is here and the key lines are shown below:cur_process = psutil.Process()all_processes = [cur_process] + cur_process.children(recursive=True)for p in all_processes: info = p.memory_full_info()Basically, it gets all children processes of the jupyter-lab process recursively, including both the ipykernel Notebook process and all processes created by the Notebook. Obviously, the cost of this function is linear to the number of all children processes. In the reproduction code, we create 96 processes. So here we will have at least 96 (sleep processes) + 1 (ipykernel process) + 1 (jupyter-lab process) = 98 processes when it should actually be 64 (allocated CPUs) + 1 (ipykernel process) + 1 (jupyter-lab process) = 66 processes, because the number of CPUs allocated to the container is, in fact, 64.This is truly ironic. The more CPUs we have, the slower we are!At this point, we have answered one question: Why does starting many grandchildren processes in the child process cause the parent process to be slow? Because the parent process runs a function that’s linear to the number all children process recursively.However, this solves only half of the puzzle. If you remember the previous analysis, starting many child processes ALONE doesn’t reproduce the issue. If we don’t read the 2GB file, even if we create 2x more processes, we can’t reproduce the slowness.So now we must answer the next question: Why does reading a 2GB file in the child process affect the parent process performance, especially when the workbench has as much as 480GB memory in total?To answer this question, let’s look closely at the function __parse_smaps_rollup. As the name implies, this function parses the file /proc/\u003cpid\u003e/smaps_rollup.def _parse_smaps_rollup(self): uss = pss = swap = 0 with open_binary(\"{}/{}/smaps_rollup\".format(self._procfs_path, self.pid)) as f: for line in f: if line.startswith(b”Private_”): # Private_Clean, Private_Dirty, Private_Hugetlb s uss += int(line.split()[1]) * 1024 elif line.startswith(b”Pss:”): pss = int(line.split()[1]) * 1024 elif line.startswith(b”Swap:”): swap = int(line.split()[1]) * 1024return (uss, pss, swap)Naturally, you might think that when memory usage increases, this file becomes larger in size, causing the function to take longer to parse. Unfortunately, this is not the answer because:First, the number of lines in this file is constant for all processes.Second, this is a special file in the /proc filesystem, which should be seen as a kernel interface instead of a regular file on disk. In other words, I/O operations of this file are handled by the kernel rather than disk.This file was introduced in this commit in 2017, with the purpose of improving the performance of user programs that determine aggregate memory statistics. Let’s first focus on the handler of open syscall on this /proc/\u003cpid\u003e/smaps_rollup.Following through the single_open function, we will find that it uses the function show_smaps_rollup for the show operation, which can translate to the read system call on the file. Next, we look at the show_smaps_rollup implementation. You will notice a do-while loop that is linear to the virtual memory area.static int show_smaps_rollup(struct seq_file *m, void *v) { … vma_start = vma-\u003evm_start; do { smap_gather_stats(vma, \u0026mss, 0); last_vma_end = vma-\u003evm_end; … } for_each_vma(vmi, vma); …}This perfectly explains why the function gets slower when a 2GB file is read into memory. Because the handler of reading the smaps_rollup file now takes longer to run the while loop. Basically, even though smaps_rollup already improved the performance of getting memory information compared to the old method of parsing the /proc/\u003cpid\u003e/smaps file, it is still linear to the virtual memory used.More Quantitative AnalysisEven though at this point the puzzle is solved, let’s conduct a more quantitative analysis. How much is the time difference when reading the smaps_rollup file with small versus large virtual memory utilization? Let’s write some simple benchmark code like below:import osdef read_smaps_rollup(pid): with open(\"/proc/{}/smaps_rollup\".format(pid), \"rb\") as f: for line in f: passif __name__ == “__main__”: pid = os.getpid() read_smaps_rollup(pid) with open(“/root/2G_file”, “rb”) as f: data = f.read() read_smaps_rollup(pid)This program performs the following steps:Reads the smaps_rollup file of the current process.Reads a 2GB file into memory.Repeats step 1.We then use strace to find the accurate time of reading the smaps_rollup file.$ sudo strace -T -e trace=openat,read python3 benchmark.py 2\u003e\u00261 | grep “smaps_rollup” -A 1openat(AT_FDCWD, “/proc/3107492/smaps_rollup”, O_RDONLY|O_CLOEXEC) = 3 \u003c0.000023\u003eread(3, “560b42ed4000–7ffdadcef000 — -p 0”…, 1024) = 670 \u003c0.000259\u003e...openat(AT_FDCWD, “/proc/3107492/smaps_rollup”, O_RDONLY|O_CLOEXEC) = 3 \u003c0.000029\u003eread(3, “560b42ed4000–7ffdadcef000 — -p 0”…, 1024) = 670 \u003c0.027698\u003eAs you can see, both times, the read syscall returned 670, meaning the file size remained the same at 670 bytes. However, the time it took the second time (i.e., 0.027698 seconds) is 100x the time it took the first time (i.e., 0.000259 seconds)! This means that if there are 98 processes, the time spent on reading this file alone will be 98 * 0.027698 = 2.7 seconds! Such a delay can significantly affect the UI experience.SolutionThis extension is used to display the CPU and memory usage of the notebook process on the bar at the bottom of the Notebook:We confirmed with the user that disabling the jupyter-resource-usage extension meets their requirements for UI responsiveness, and that this extension is not critical to their use case. Therefore, we provided a way for them to disable the extension.SummaryThis was such a challenging issue that required debugging from the UI all the way down to the Linux kernel. It is fascinating that the problem is linear to both the number of CPUs and the virtual memory size — two dimensions that are generally viewed separately.Overall, we hope you enjoyed the irony of:The extension used to monitor CPU usage causing CPU contention.An interesting case where the more CPUs you have, the slower you get!If you’re excited by tackling such technical challenges and have the opportunity to solve complex technical challenges and drive innovation, consider joining our Data Platform teams. Be part of shaping the future of Data Security and Infrastructure, Data Developer Experience, Analytics Infrastructure and Enablement, and more. Explore the impact you can make with us!",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*ltV3CYtNjLCzolXD",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page---byline--faa017b4653d--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://netflixtechblog.com/?source=post_page---byline--faa017b4653d--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Netflix TechBlog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cp id=\"c66a\"\u003eBy: \u003ca href=\"https://www.linkedin.com/in/hechaoli/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHechao Li\u003c/a\u003e and \u003ca href=\"https://www.linkedin.com/in/mayworm/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMarcelo Mayworm\u003c/a\u003e\u003c/p\u003e\u003cp id=\"2b76\"\u003eWith special thanks to our stunning colleagues \u003ca href=\"https://www.linkedin.com/in/amer-ather-9071181/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAmer Ather\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/itaydafna\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eItay Dafna\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/lucaepozzi/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLuca Pozzi\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/matheusdeoleao/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMatheus Leão\u003c/a\u003e, and \u003ca href=\"https://www.linkedin.com/in/yeji682/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eYe Ji\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"c1c3\"\u003eOverview\u003c/h2\u003e\u003cp id=\"072a\"\u003eAt Netflix, the Analytics and Developer Experience organization, part of the Data Platform, offers a product called Workbench. Workbench is a remote development workspace based on\u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436\"\u003e Titus\u003c/a\u003e that allows data practitioners to work with big data and machine learning use cases at scale. A common use case for Workbench is running\u003ca href=\"https://jupyterlab.readthedocs.io/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e JupyterLab\u003c/a\u003e Notebooks.\u003c/p\u003e\u003cp id=\"d5f7\"\u003eRecently, several users reported that their JupyterLab UI becomes slow and unresponsive when running certain notebooks. This document details the intriguing process of debugging this issue, all the way from the UI down to the Linux kernel.\u003c/p\u003e\u003ch2 id=\"1fae\"\u003eSymptom\u003c/h2\u003e\u003cp id=\"6f03\"\u003eMachine Learning engineer \u003ca href=\"https://www.linkedin.com/in/lucaepozzi/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLuca Pozzi\u003c/a\u003e reported to our Data Platform team that their \u003cstrong\u003eJupyterLab UI on their workbench becomes slow and unresponsive when running some of their Notebooks.\u003c/strong\u003e Restarting the \u003cem\u003eipykernel\u003c/em\u003e process, which runs the Notebook, might temporarily alleviate the problem, but the frustration persists as more notebooks are run.\u003c/p\u003e\u003ch2 id=\"c9b9\"\u003eQuantify the Slowness\u003c/h2\u003e\u003cp id=\"35ea\"\u003eWhile we observed the issue firsthand, the term “UI being slow” is subjective and difficult to measure. To investigate this issue, \u003cstrong\u003ewe needed a quantitative analysis of the slowness\u003c/strong\u003e.\u003c/p\u003e\u003cp id=\"2efa\"\u003e\u003ca href=\"https://www.linkedin.com/in/itaydafna\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eItay Dafna\u003c/a\u003e devised an effective and simple method to quantify the UI slowness. Specifically, we opened a terminal via JupyterLab and held down a key (e.g., “j”) for 15 seconds while running the user’s notebook. The input to stdin is sent to the backend (i.e., JupyterLab) via a WebSocket, and the output to stdout is sent back from the backend and displayed on the UI. We then exported the \u003cem\u003e.har \u003c/em\u003efile recording all communications from the browser and loaded it into a Notebook for analysis.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"e91b\"\u003eUsing this approach, we observed latencies ranging from 1 to 10 seconds, averaging 7.4 seconds.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"6cd5\"\u003eBlame The Notebook\u003c/h2\u003e\u003cp id=\"ef5b\"\u003eNow that we have an objective metric for the slowness, let’s officially start our investigation. If you have read the symptom carefully, you must have noticed that the slowness only occurs when the user runs \u003cstrong\u003ecertain\u003c/strong\u003e notebooks but not others.\u003c/p\u003e\u003cp id=\"c042\"\u003eTherefore, the first step is scrutinizing the specific Notebook experiencing the issue. Why does the UI always slow down after running this particular Notebook? Naturally, you would think that there must be something wrong with the code running in it.\u003c/p\u003e\u003cp id=\"cf81\"\u003eUpon closely examining the user’s Notebook, we noticed a library called \u003cem\u003epystan\u003c/em\u003e , which provides Python bindings to a native C++ library called stan, looked suspicious. Specifically, \u003cem\u003epystan\u003c/em\u003e uses \u003cem\u003easyncio\u003c/em\u003e. However, \u003cstrong\u003ebecause there is already an existing \u003cem\u003easyncio\u003c/em\u003e event loop running in the Notebook process and \u003cem\u003easyncio\u003c/em\u003e cannot be nested by design, in order for \u003cem\u003epystan\u003c/em\u003e to work, the authors of \u003cem\u003epystan\u003c/em\u003e \u003c/strong\u003e\u003ca href=\"https://pystan.readthedocs.io/en/latest/faq.html#how-can-i-use-pystan-with-jupyter-notebook-or-jupyterlab\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003erecommend\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e injecting \u003cem\u003epystan\u003c/em\u003e into the existing event loop by using a package called \u003c/strong\u003e\u003ca href=\"https://pypi.org/project/nest-asyncio/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003e\u003cem\u003enest_asyncio\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e, a library that became unmaintained because \u003ca href=\"https://github.com/erdewit/ib_insync/commit/ef5ea29e44e0c40bbadbc16c2281b3ac58aa4a40\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ethe author unfortunately passed away\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"de21\"\u003eGiven this seemingly hacky usage, we naturally suspected that the events injected by \u003cem\u003epystan\u003c/em\u003e into the event loop were blocking the handling of the WebSocket messages used to communicate with the JupyterLab UI. This reasoning sounds very plausible. However, \u003cstrong\u003ethe user claimed that there were cases when a Notebook not using \u003cem\u003epystan\u003c/em\u003e runs, the UI also became slow\u003c/strong\u003e.\u003c/p\u003e\u003cp id=\"ca77\"\u003eMoreover, after several rounds of discussion with ChatGPT, we learned more about the architecture and realized that, in theory, \u003cstrong\u003ethe usage of \u003cem\u003epystan\u003c/em\u003e and \u003cem\u003enest_asyncio\u003c/em\u003e should not cause the slowness in handling the UI WebSocket\u003c/strong\u003e for the following reasons:\u003c/p\u003e\u003cp id=\"17ba\"\u003eEven though \u003cem\u003epystan\u003c/em\u003e uses \u003cem\u003enest_asyncio\u003c/em\u003e to inject itself into the main event loop, \u003cstrong\u003ethe Notebook runs on a child process (i.e.\u003c/strong\u003e,\u003cstrong\u003e the \u003cem\u003eipykernel\u003c/em\u003e process) of the \u003cem\u003ejupyter-lab\u003c/em\u003e server process\u003c/strong\u003e, which means the main event loop being injected by \u003cem\u003epystan\u003c/em\u003e is that of the \u003cem\u003eipykernel\u003c/em\u003e process, not the \u003cem\u003ejupyter-server\u003c/em\u003e process. Therefore, even if \u003cem\u003epystan\u003c/em\u003e blocks the event loop, it shouldn’t impact the \u003cem\u003ejupyter-lab\u003c/em\u003e main event loop that is used for UI websocket communication. See the diagram below:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"c601\"\u003eIn other words, \u003cstrong\u003e\u003cem\u003epystan\u003c/em\u003e events are injected to the event loop B in this diagram instead of event loop A\u003c/strong\u003e. So, it shouldn’t block the UI WebSocket events.\u003c/p\u003e\u003cp id=\"1d8c\"\u003eYou might also think that because event loop A handles both the WebSocket events from the UI and the ZeroMQ socket events from the \u003cem\u003eipykernel\u003c/em\u003e process, a high volume of ZeroMQ events generated by the notebook could block the WebSocket. However, \u003cstrong\u003ewhen we captured packets on the ZeroMQ socket while reproducing the issue, we didn’t observe heavy traffic on this socket that could cause such blocking\u003c/strong\u003e.\u003c/p\u003e\u003cp id=\"f5d9\"\u003eA stronger piece of evidence to rule out \u003cem\u003epystan\u003c/em\u003e was that we were ultimately able to reproduce the issue even without it, which I’ll dive into later.\u003c/p\u003e\u003ch2 id=\"ccc2\"\u003eBlame Noisy Neighbors\u003c/h2\u003e\u003cp id=\"87ad\"\u003eThe Workbench instance runs as a \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436\"\u003eTitus container\u003c/a\u003e. To efficiently utilize our compute resources, \u003cstrong\u003eTitus employs a CPU oversubscription feature\u003c/strong\u003e, meaning the combined virtual CPUs allocated to containers exceed the number of available physical CPUs on a Titus agent. \u003cstrong\u003eIf a container is unfortunate enough to be scheduled alongside other “noisy” containers — those that consume a lot of CPU resources — it could suffer from CPU deficiency.\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"d99f\"\u003eHowever, after examining the CPU utilization of neighboring containers on the same Titus agent as the Workbench instance, as well as the overall CPU utilization of the Titus agent, we quickly ruled out this hypothesis. Using the top command on the Workbench, we observed that when running the Notebook, \u003cstrong\u003ethe Workbench instance uses only 4 out of the 64 CPUs allocated to it\u003c/strong\u003e. Simply put, \u003cstrong\u003ethis workload is not CPU-bound.\u003c/strong\u003e\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"ac15\"\u003eBlame The Network\u003c/h2\u003e\u003cp id=\"8e12\"\u003eThe next theory was that the network between the web browser UI (on the laptop) and the JupyterLab server was slow. To investigate, we \u003cstrong\u003ecaptured all the packets between the laptop and the server\u003c/strong\u003e while running the Notebook and continuously pressing ‘j’ in the terminal.\u003c/p\u003e\u003cp id=\"0018\"\u003eWhen the UI experienced delays, we observed a 5-second pause in packet transmission from server port 8888 to the laptop. Meanwhile,\u003cstrong\u003e traffic from other ports, such as port 22 for SSH, remained unaffected\u003c/strong\u003e. This led us to conclude that the pause was caused by the application running on port 8888 (i.e., the JupyterLab process) rather than the network.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"ff04\"\u003eThe Minimal Reproduction\u003c/h2\u003e\u003cp id=\"b5d7\"\u003eAs previously mentioned, another strong piece of evidence proving the innocence of pystan was that we could reproduce the issue without it. By gradually stripping down the “bad” Notebook, we eventually arrived at a minimal snippet of code that reproduces the issue without any third-party dependencies or complex logic:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d392\"\u003eimport time\u003cbr/\u003eimport os\u003cbr/\u003efrom multiprocessing import Process\u003cp\u003eN = os.cpu_count()\u003c/p\u003e\u003cp\u003edef launch_worker(worker_id):\u003cbr/\u003e  time.sleep(60)\u003c/p\u003e\u003cp\u003eif __name__ == \u0026#39;__main__\u0026#39;:\u003cbr/\u003e  with open(\u0026#39;/root/2GB_file\u0026#39;, \u0026#39;r\u0026#39;) as file:\u003cbr/\u003e    data = file.read()\u003cbr/\u003e    processes = []\u003cbr/\u003e    for i in range(N):\u003cbr/\u003e      p = Process(target=launch_worker, args=(i,))\u003cbr/\u003e      processes.append(p)\u003cbr/\u003e      p.start()\u003c/p\u003e\u003cp\u003e     for p in processes:\u003cbr/\u003e      p.join()\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"04dc\"\u003eThe code does only two things:\u003c/p\u003e\u003col\u003e\u003cli id=\"2d46\"\u003eRead a 2GB file into memory (the Workbench instance has 480G memory in total so this memory usage is almost negligible).\u003c/li\u003e\u003cli id=\"9c35\"\u003eStart N processes where N is the number of CPUs. The N processes do nothing but sleep.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"9ea8\"\u003eThere is no doubt that this is the most silly piece of code I’ve ever written. It is neither CPU bound nor memory bound. Yet \u003cstrong\u003eit can cause the JupyterLab UI to stall for as many as 10 seconds!\u003c/strong\u003e\u003c/p\u003e\u003ch2 id=\"2dca\"\u003eQuestions\u003c/h2\u003e\u003cp id=\"24d9\"\u003eThere are a couple of interesting observations that raise several questions:\u003c/p\u003e\u003cul\u003e\u003cli id=\"bba4\"\u003eWe noticed that \u003cstrong\u003eboth steps are required in order to reproduce the issue\u003c/strong\u003e. If you don’t read the 2GB file (that is not even used!), the issue is not reproducible. \u003cstrong\u003eWhy using 2GB out of 480GB memory could impact the performance?\u003c/strong\u003e\u003c/li\u003e\u003cli id=\"3e91\"\u003e\u003cstrong\u003eWhen the UI delay occurs, the \u003cem\u003ejupyter-lab\u003c/em\u003e process CPU utilization spikes to 100%\u003c/strong\u003e, hinting at contention on the single-threaded event loop in this process (event loop A in the diagram before). \u003cstrong\u003eWhat does the \u003cem\u003ejupyter-lab\u003c/em\u003e process need the CPU for, given that it is not the process that runs the Notebook?\u003c/strong\u003e\u003c/li\u003e\u003cli id=\"72c2\"\u003eThe code runs in a Notebook, which means it runs in the \u003cem\u003eipykernel\u003c/em\u003e process, that is a child process of the \u003cem\u003ejupyter-lab\u003c/em\u003e process. \u003cstrong\u003eHow can anything that happens in a child process cause the parent process to have CPU contention?\u003c/strong\u003e\u003c/li\u003e\u003cli id=\"101d\"\u003eThe workbench has 64CPUs. But when we printed \u003cem\u003eos.cpu_count()\u003c/em\u003e, the output was 96. That means \u003cstrong\u003ethe code starts more processes than the number of CPUs\u003c/strong\u003e. \u003cstrong\u003eWhy is that?\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"9d59\"\u003eLet’s answer the last question first. In fact, if you run \u003cem\u003elscpu\u003c/em\u003e and \u003cem\u003enproc\u003c/em\u003e commands inside a Titus container, you will also see different results — the former gives you 96, which is the number of physical CPUs on the Titus agent, whereas the latter gives you 64, which is the number of virtual CPUs allocated to the container. This discrepancy is due to the lack of a “CPU namespace” in the Linux kernel, causing the number of physical CPUs to be leaked to the container when calling certain functions to get the CPU count. The assumption here is that Python \u003cstrong\u003e\u003cem\u003eos.cpu_count()\u003c/em\u003e uses the same function as the \u003cem\u003elscpu\u003c/em\u003e command, causing it to get the CPU count of the host instead of the container\u003c/strong\u003e. Python 3.13 has \u003ca href=\"https://docs.python.org/3.13/library/os.html#os.process_cpu_count\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ea new call that can be used to get the accurate CPU count\u003c/a\u003e, but it’s not GA’ed yet.\u003c/p\u003e\u003cp id=\"ea87\"\u003eIt will be proven later that this inaccurate number of CPUs can be a contributing factor to the slowness.\u003c/p\u003e\u003ch2 id=\"cdd1\"\u003eMore Clues\u003c/h2\u003e\u003cp id=\"b480\"\u003eNext, we used \u003cem\u003epy-spy\u003c/em\u003e to do a profiling of the \u003cem\u003ejupyter-lab\u003c/em\u003e process. Note that we profiled the parent \u003cem\u003ejupyter-lab \u003c/em\u003eprocess, \u003cstrong\u003enot\u003c/strong\u003e the \u003cem\u003eipykernel\u003c/em\u003e child process that runs the reproduction code. The profiling result is as follows:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"55b0\"\u003eAs one can see, \u003cstrong\u003ea lot of CPU time (89%!!) is spent on a function called \u003cem\u003e__parse_smaps_rollup\u003c/em\u003e\u003c/strong\u003e. In comparison, the terminal handler used only 0.47% CPU time. From the stack trace, we see that \u003cstrong\u003ethis function is inside the event loop A\u003c/strong\u003e,\u003cstrong\u003e so it can definitely cause the UI WebSocket events to be delayed\u003c/strong\u003e.\u003c/p\u003e\u003cp id=\"fd28\"\u003eThe stack trace also shows that this function is ultimately called by a function used by a Jupyter lab extension called \u003cem\u003ejupyter_resource_usage\u003c/em\u003e. \u003cstrong\u003eWe then disabled this extension and restarted the \u003cem\u003ejupyter-lab\u003c/em\u003e process. As you may have guessed, we could no longer reproduce the slowness!\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"5e8f\"\u003eBut our puzzle is not solved yet. Why does this extension cause the UI to slow down? Let’s keep digging.\u003c/p\u003e\u003ch2 id=\"ff5d\"\u003eRoot Cause Analysis\u003c/h2\u003e\u003cp id=\"694f\"\u003eFrom the name of the extension and the names of the other functions it calls, we can infer that this extension is used to get resources such as CPU and memory usage information. Examining the code, we see that this function call stack is triggered when an API endpoint \u003cem\u003e/metrics/v1\u003c/em\u003e is called from the UI. \u003cstrong\u003eThe UI apparently calls this function periodically\u003c/strong\u003e, according to the network traffic tab in Chrome’s Developer Tools.\u003c/p\u003e\u003cp id=\"5465\"\u003eNow let’s look at the implementation starting from the call \u003cem\u003eget(jupter_resource_usage/api.py:42)\u003c/em\u003e . The full code is \u003ca href=\"https://github.com/jupyter-server/jupyter-resource-usage/blob/6f15ef91d5c7e50853516b90b5e53b3913d2ed34/jupyter_resource_usage/api.py#L28\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e and the key lines are shown below:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"e4f2\"\u003ecur_process = psutil.Process()\u003cbr/\u003eall_processes = [cur_process] + cur_process.children(recursive=True)\u003cp\u003efor p in all_processes:\u003cbr/\u003e  info = p.memory_full_info()\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"1f1a\"\u003eBasically, it gets all children processes of the \u003cem\u003ejupyter-lab\u003c/em\u003e process recursively, including both the \u003cem\u003eipykernel\u003c/em\u003e Notebook process and all processes created by the Notebook. Obviously, \u003cstrong\u003ethe cost of this function is linear to the number of all children processes\u003c/strong\u003e. In the reproduction code, we create 96 processes. So here we will have at least 96 (sleep processes) + 1 (\u003cem\u003eipykernel\u003c/em\u003e process) + 1 (\u003cem\u003ejupyter-lab\u003c/em\u003e process) = 98 processes when it should actually be 64 (allocated CPUs) + 1 (\u003cem\u003eipykernel\u003c/em\u003e process) + 1 \u003cem\u003e(jupyter-lab\u003c/em\u003e process) = 66 processes, because the number of CPUs allocated to the container is, in fact, 64.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"6210\"\u003eThis is truly ironic. \u003cstrong\u003eThe more CPUs we have, the slower we are!\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"98c2\"\u003eAt this point, we have answered one question: \u003cstrong\u003eWhy does starting many grandchildren processes in the child process cause the parent process to be slow? \u003c/strong\u003eBecause the parent process runs a function that’s linear to the number all children process recursively.\u003c/p\u003e\u003cp id=\"1f1f\"\u003eHowever, this solves only half of the puzzle. If you remember the previous analysis, \u003cstrong\u003estarting many child processes ALONE doesn’t reproduce the issue\u003c/strong\u003e. If we don’t read the 2GB file, even if we create 2x more processes, we can’t reproduce the slowness.\u003c/p\u003e\u003cp id=\"147b\"\u003eSo now we must answer the next question: \u003cstrong\u003eWhy does reading a 2GB file in the child process affect the parent process performance, \u003c/strong\u003eespecially when the workbench has as much as 480GB memory in total?\u003c/p\u003e\u003cp id=\"49ac\"\u003eTo answer this question, let’s look closely at the function \u003cem\u003e__parse_smaps_rollup\u003c/em\u003e. As the name implies, \u003ca href=\"https://github.com/giampaolo/psutil/blob/c034e6692cf736b5e87d14418a8153bb03f6cf42/psutil/_pslinux.py#L1978\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ethis function\u003c/a\u003e parses the file \u003cem\u003e/proc/\u0026lt;pid\u0026gt;/smaps_rollup\u003c/em\u003e.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"67a3\"\u003edef _parse_smaps_rollup(self):\u003cbr/\u003e  uss = pss = swap = 0\u003cbr/\u003e  with open_binary(\u0026#34;{}/{}/smaps_rollup\u0026#34;.format(self._procfs_path, self.pid)) as f:\u003cbr/\u003e  for line in f:\u003cbr/\u003e    if line.startswith(b”Private_”):\u003cbr/\u003e    # Private_Clean, Private_Dirty, Private_Hugetlb\u003cbr/\u003e      s uss += int(line.split()[1]) * 1024\u003cbr/\u003e    elif line.startswith(b”Pss:”):\u003cbr/\u003e      pss = int(line.split()[1]) * 1024\u003cbr/\u003e    elif line.startswith(b”Swap:”):\u003cbr/\u003e      swap = int(line.split()[1]) * 1024\u003cbr/\u003ereturn (uss, pss, swap)\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"6952\"\u003eNaturally, you might think that when memory usage increases, this file becomes larger in size, causing the function to take longer to parse. Unfortunately, this is not the answer because:\u003c/p\u003e\u003cul\u003e\u003cli id=\"2f67\"\u003eFirst, \u003ca href=\"https://www.kernel.org/doc/Documentation/ABI/testing/procfs-smaps_rollup\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003ethe number of lines in this file is constant\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e for all processes\u003c/strong\u003e.\u003c/li\u003e\u003cli id=\"173a\"\u003eSecond, \u003cstrong\u003ethis is a special file in the /proc filesystem, which should be seen as a kernel interface\u003c/strong\u003e instead of a regular file on disk. In other words, \u003cstrong\u003eI/O operations of this file are handled by the kernel rather than disk\u003c/strong\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"5700\"\u003eThis file was introduced in \u003ca href=\"https://github.com/torvalds/linux/commit/493b0e9d945fa9dfe96be93ae41b4ca4b6fdb317#diff-cb79e2d6ea6f9627ff68d1342a219f800e04ff6c6fa7b90c7e66bb391b2dd3ee\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ethis commit\u003c/a\u003e in 2017, with the purpose of improving the performance of user programs that determine aggregate memory statistics. Let’s first focus on \u003ca href=\"https://elixir.bootlin.com/linux/v6.5.13/source/fs/proc/task_mmu.c#L1025\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ethe handler of \u003cem\u003eopen\u003c/em\u003e syscall\u003c/a\u003e on this \u003cem\u003e/proc/\u0026lt;pid\u0026gt;/smaps_rollup\u003c/em\u003e.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"52cf\"\u003eFollowing through the \u003cem\u003esingle_open\u003c/em\u003e \u003ca href=\"https://elixir.bootlin.com/linux/v6.5.13/source/fs/seq_file.c#L582\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003efunction\u003c/a\u003e, we will find that it uses the function \u003cem\u003eshow_smaps_rollup\u003c/em\u003e for the show operation, which can translate to the \u003cem\u003eread\u003c/em\u003e system call on the file. Next, we look at the \u003cem\u003eshow_smaps_rollup\u003c/em\u003e \u003ca href=\"https://elixir.bootlin.com/linux/v6.5.13/source/fs/proc/task_mmu.c#L916\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eimplementation\u003c/a\u003e. You will notice \u003cstrong\u003ea do-while loop that is linear to the virtual memory area\u003c/strong\u003e.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0e83\"\u003estatic int show_smaps_rollup(struct seq_file *m, void *v) {\u003cbr/\u003e  …\u003cbr/\u003e  vma_start = vma-\u0026gt;vm_start;\u003cbr/\u003e  do {\u003cbr/\u003e    smap_gather_stats(vma, \u0026amp;mss, 0);\u003cbr/\u003e    last_vma_end = vma-\u0026gt;vm_end;\u003cbr/\u003e    …\u003cbr/\u003e  } for_each_vma(vmi, vma);\u003cbr/\u003e  …\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"976c\"\u003eThis perfectly \u003cstrong\u003eexplains why the function gets slower when a 2GB file is read into memory\u003c/strong\u003e. \u003cstrong\u003eBecause the handler of reading the \u003cem\u003esmaps_rollup\u003c/em\u003e file now takes longer to run the while loop\u003c/strong\u003e. Basically, even though \u003cstrong\u003e\u003cem\u003esmaps_rollup\u003c/em\u003e\u003c/strong\u003e already improved the performance of getting memory information compared to the old method of parsing the \u003cem\u003e/proc/\u0026lt;pid\u0026gt;/smaps\u003c/em\u003e file, \u003cstrong\u003eit is still linear to the virtual memory used\u003c/strong\u003e.\u003c/p\u003e\u003ch2 id=\"d903\"\u003eMore Quantitative Analysis\u003c/h2\u003e\u003cp id=\"3a6e\"\u003eEven though at this point the puzzle is solved, let’s conduct a more quantitative analysis. How much is the time difference when reading the \u003cem\u003esmaps_rollup\u003c/em\u003e file with small versus large virtual memory utilization? Let’s write some simple benchmark code like below:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"964f\"\u003eimport os\u003cp\u003edef read_smaps_rollup(pid):\u003cbr/\u003e  with open(\u0026#34;/proc/{}/smaps_rollup\u0026#34;.format(pid), \u0026#34;rb\u0026#34;) as f:\u003cbr/\u003e    for line in f:\u003cbr/\u003e      pass\u003c/p\u003e\u003cp\u003eif __name__ == “__main__”:\u003cbr/\u003e  pid = os.getpid()\u003c/p\u003e\u003cp\u003e    read_smaps_rollup(pid)\u003c/p\u003e\u003cp\u003e  with open(“/root/2G_file”, “rb”) as f:\u003cbr/\u003e    data = f.read()\u003c/p\u003e\u003cp\u003e  read_smaps_rollup(pid)\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"56c3\"\u003eThis program performs the following steps:\u003c/p\u003e\u003col\u003e\u003cli id=\"d3b3\"\u003eReads the \u003cem\u003esmaps_rollup\u003c/em\u003e file of the current process.\u003c/li\u003e\u003cli id=\"2032\"\u003eReads a 2GB file into memory.\u003c/li\u003e\u003cli id=\"7966\"\u003eRepeats step 1.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"12da\"\u003eWe then use \u003cem\u003estrace\u003c/em\u003e to find the accurate time of reading the \u003cem\u003esmaps_rollup\u003c/em\u003e file.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"a221\"\u003e$ sudo strace -T -e trace=openat,read python3 benchmark.py 2\u0026gt;\u0026amp;1 | grep “smaps_rollup” -A 1\u003cp\u003eopenat(AT_FDCWD, “/proc/3107492/smaps_rollup”, O_RDONLY|O_CLOEXEC) = 3 \u0026lt;0.000023\u0026gt;\u003cbr/\u003eread(3, “560b42ed4000–7ffdadcef000 — -p 0”…, 1024) = 670 \u0026lt;0.000259\u0026gt;\u003cbr/\u003e...\u003cbr/\u003eopenat(AT_FDCWD, “/proc/3107492/smaps_rollup”, O_RDONLY|O_CLOEXEC) = 3 \u0026lt;0.000029\u0026gt;\u003cbr/\u003eread(3, “560b42ed4000–7ffdadcef000 — -p 0”…, 1024) = 670 \u0026lt;0.027698\u0026gt;\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"2e29\"\u003eAs you can see, both times, the read \u003cem\u003esyscall\u003c/em\u003e returned 670, meaning the file size remained the same at 670 bytes. However, \u003cstrong\u003ethe time it took the second time (i.e.\u003c/strong\u003e,\u003cstrong\u003e 0.027698 seconds) is 100x the time it took the first time (i.e.\u003c/strong\u003e,\u003cstrong\u003e 0.000259 seconds)\u003c/strong\u003e! This means that if there are 98 processes, the time spent on reading this file alone will be 98 * 0.027698 = 2.7 seconds! Such a delay can significantly affect the UI experience.\u003c/p\u003e\u003ch2 id=\"8c5e\"\u003eSolution\u003c/h2\u003e\u003cp id=\"9ac7\"\u003eThis extension is used to display the CPU and memory usage of the notebook process on the bar at the bottom of the Notebook:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"0389\"\u003eWe confirmed with the user that disabling the \u003cem\u003ejupyter-resource-usage\u003c/em\u003e extension meets their requirements for UI responsiveness, and that this extension is not critical to their use case. Therefore, we provided a way for them to disable the extension.\u003c/p\u003e\u003ch2 id=\"2e46\"\u003eSummary\u003c/h2\u003e\u003cp id=\"5cb4\"\u003eThis was such a challenging issue that required debugging from the UI all the way down to the Linux kernel. It is fascinating that the problem is linear to both the number of CPUs and the virtual memory size — two dimensions that are generally viewed separately.\u003c/p\u003e\u003cp id=\"dde1\"\u003eOverall, we hope you enjoyed the irony of:\u003c/p\u003e\u003col\u003e\u003cli id=\"b7b8\"\u003eThe extension used to monitor CPU usage causing CPU contention.\u003c/li\u003e\u003cli id=\"93e2\"\u003eAn interesting case where the more CPUs you have, the slower you get!\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"dfe6\"\u003eIf you’re excited by tackling such technical challenges and have the opportunity to solve complex technical challenges and drive innovation, consider joining our \u003ca href=\"https://explore.jobs.netflix.net/careers?query=Data+Platform\u0026amp;pid=790298020581\u0026amp;domain=netflix.com\u0026amp;sort_by=relevance\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eData Platform team\u003c/a\u003es. Be part of shaping the future of Data Security and Infrastructure, Data Developer Experience, Analytics Infrastructure and Enablement, and more. Explore the impact you can make with us!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "18 min read",
  "publishedTime": "2024-10-14T20:02:31.194Z",
  "modifiedTime": null
}
