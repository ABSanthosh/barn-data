{
  "id": "6472dcc9-f704-476d-bbda-ae37b28efff8",
  "title": "Ai2 Launches OLMo 2, a Fully Open-Source Foundation Model",
  "link": "https://www.infoq.com/news/2024/12/olmo-2-ai2/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "The Allen Institute for AI research team has introduced OLMo 2, a new family of open-source language models available in 7 billion (7B) and 13 billion (13B) parameter configurations. Trained on up to 5 trillion tokens, these models redefines training stability, adopting staged training processes, and incorporating diverse datasets. By Daniel Dominguez",
  "author": "Daniel Dominguez",
  "published": "Thu, 05 Dec 2024 15:36:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Large language models",
    "Open Source",
    "Artificial Intelligence",
    "Microsoft",
    "OpenAI",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Daniel Dominguez",
  "length": 3123,
  "excerpt": "The Allen Institute for AI research team has introduced OLMo 2, a new family of open-source language models available in 7 billion (7B) and 13 billion (13B) parameter configurations. Trained on up to",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241205201239/apple-touch-icon.png",
  "text": "The Allen Institute for AI research team has introduced OLMo 2, a new family of open-source language models available in 7 billion (7B) and 13 billion (13B) parameter configurations. Trained on up to 5 trillion tokens, these models redefines training stability, adopting staged training processes, and incorporating diverse datasets. OLMo 2's architecture leverages improvements in layer normalization, employing RMSNorm, and rotary positional embeddings, as well as Z-loss regularization to enhance model robustness. The training process utilized a two-stage curriculum approach, with the first stage focusing on the OLMo-Mix-1124 dataset, comprising 3.9 trillion tokens from high-quality repositories like DCLM and Starcoder. The second stage involved fine-tuning Dolmino-Mix-1124, a curated dataset of 843 billion tokens featuring web-based and domain-specific content. Techniques like model souping, which merges checkpoints to optimize performance, were crucial in achieving the final versions of the 7B and 13B models. The performance of OLMo 2 sets new benchmarks in open-source language modeling, demonstrating a significant boost across all evaluation tasks compared to its predecessor, OLMo-0424. Notably, OLMo 2 7B outperforms Llama-3.1 8B, and OLMo 2 13B surpasses Qwen 2.5 7B, despite utilizing fewer training FLOPs. Evaluation using the Open Language Modeling Evaluation System (OLMES), a suite of 20 benchmarks, confirmed these gains, highlighting strengths in knowledge recall, reasoning, and general language capabilities. The development of OLMo 2 marks a significant shift in the language modeling landscape, addressing challenges such as training stability and evaluation transparency. By setting a new standard for open-source AI, these models demonstrate the potential of collaborative innovation in advancing artificial intelligence, paving the way for more equitable technological advancements. The AI community has responded enthusiastically to OLMo 2’s launch, recognizing Ai2 for its commitment to open-source. AI Researcher Constantine Dee commented on X: Ai2 has unveiled OLMo 2, the world's leading open-source AI model. Built with transparent datasets and training, it's a game-changer for creating diverse content. While user Billy462 shared on Reddit: This release is extremely significant. For those that don't know Allen AI are a research institute who are releasing completely open models. That means that all of their results can be reproduced (and improved upon) from scratch. The OLMo 2 models are available, along with their weights, data, code, recipes, and intermediate checkpoints. The introduction of OLMES provides structured benchmarks to guide model development and track progress effectively. Additionally, post-training methodologies, including supervised fine-tuning, preference tuning, and reinforcement learning with verifiable rewards, have enhanced the models' instruction-following capabilities. About the Author Daniel Dominguez",
  "image": "https://res.infoq.com/news/2024/12/olmo-2-ai2/en/headerimage/generatedHeaderImage-1733068036293.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://allenai.org/\"\u003eThe Allen Institute for AI\u003c/a\u003e research team has introduced \u003ca href=\"https://allenai.org/blog/olmo2\"\u003eOLMo 2\u003c/a\u003e, a new family of open-source language models available in 7 billion (7B) and 13 billion (13B) parameter configurations. Trained on up to 5 trillion tokens, these models redefines training stability, adopting staged training processes, and incorporating diverse datasets.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://huggingface.co/collections/allenai/olmo-2-674117b93ab84e98afc72edc\"\u003eOLMo 2\u003c/a\u003e\u0026#39;s architecture leverages improvements in layer normalization, employing \u003ca href=\"https://paperswithcode.com/method/rmsnorm\"\u003eRMSNorm\u003c/a\u003e, and \u003ca href=\"https://paperswithcode.com/method/rope\"\u003erotary positional embeddings\u003c/a\u003e, as well as \u003ca href=\"https://arxiv.org/abs/1604.08859\"\u003eZ-loss regularization\u003c/a\u003e to enhance model robustness. The training process utilized a two-stage curriculum approach, with the first stage focusing on the \u003ca href=\"https://huggingface.co/datasets/allenai/olmo-mix-1124\"\u003eOLMo-Mix-1124\u003c/a\u003e dataset, comprising 3.9 trillion tokens from high-quality repositories like DCLM and Starcoder. The second stage involved fine-tuning \u003ca href=\"https://huggingface.co/datasets/allenai/dolmino-mix-1124\"\u003eDolmino-Mix-1124\u003c/a\u003e, a curated dataset of 843 billion tokens featuring web-based and domain-specific content.\u003c/p\u003e\n\n\u003cp\u003eTechniques like \u003ca href=\"https://paperswithcode.com/method/soups\"\u003emodel souping\u003c/a\u003e, which merges checkpoints to optimize performance, were crucial in achieving the final versions of the 7B and 13B models. The performance of OLMo 2 sets new benchmarks in open-source language modeling, demonstrating a significant boost across all evaluation tasks compared to its predecessor, \u003ca href=\"https://huggingface.co/allenai/OLMo-7B-0424\"\u003eOLMo-0424\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/12/olmo-2-ai2/en/resources/1olmo-1733068535659.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/12/olmo-2-ai2/en/resources/1olmo-1733068535659.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eNotably, OLMo 2 7B outperforms Llama-3.1 8B, and OLMo 2 13B surpasses Qwen 2.5 7B, despite utilizing fewer training FLOPs. Evaluation using the \u003ca href=\"https://arxiv.org/html/2406.08446v1\"\u003eOpen Language Modeling Evaluation System (OLMES)\u003c/a\u003e, a suite of 20 benchmarks, confirmed these gains, highlighting strengths in knowledge recall, reasoning, and general language capabilities.\u003c/p\u003e\n\n\u003cp\u003eThe development of OLMo 2 marks a significant shift in the language modeling landscape, addressing challenges such as training stability and evaluation transparency. By setting a new standard for open-source AI, these models demonstrate the potential of collaborative innovation in advancing artificial intelligence, paving the way for more equitable technological advancements.\u003c/p\u003e\n\n\u003cp\u003eThe AI community has responded enthusiastically to OLMo 2’s launch, recognizing Ai2 for its commitment to open-source.\u003c/p\u003e\n\n\u003cp\u003eAI Researcher \u003ca href=\"https://x.com/Constantinu/status/1861789812501434639\"\u003eConstantine Dee\u003c/a\u003e commented on \u003ca href=\"https://x.com/Constantinu/status/1861789812501434639\"\u003eX\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAi2 has unveiled OLMo 2, the world\u0026#39;s leading open-source AI model. Built with transparent datasets and training, it\u0026#39;s a game-changer for creating diverse content.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWhile user \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1h0mnfv/comment/lz5coej/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button\"\u003eBilly462\u003c/a\u003e shared on \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1h0mnfv/comment/lz5coej/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button\"\u003eReddit\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis release is extremely significant. For those that don\u0026#39;t know Allen AI are a research institute who are releasing completely open models. That means that all of their results can be reproduced (and improved upon) from scratch.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe OLMo 2 models are available, along with their weights, data, code, recipes, and intermediate checkpoints. The introduction of \u003ca href=\"https://github.com/allenai/olmes\"\u003eOLMES\u003c/a\u003e provides structured benchmarks to guide model development and track progress effectively. Additionally, post-training methodologies, including supervised fine-tuning, preference tuning, and reinforcement learning with verifiable rewards, have enhanced the models\u0026#39; instruction-following capabilities.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Daniel-Dominguez\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eDaniel Dominguez\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-12-05T00:00:00Z",
  "modifiedTime": null
}
