{
  "id": "714e0948-9ae3-4728-8c77-a0ce3f5a2621",
  "title": "Title Launch Observability at Netflix Scale",
  "link": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-8efe69ebd653?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Wed, 05 Mar 2025 01:24:53 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": [
    "system-design-concepts",
    "netflix",
    "software-engineering",
    "observability"
  ],
  "byline": "Netflix Technology Blog",
  "length": 9968,
  "excerpt": "This blog post is a continuation of Part 2, where we cleared the ambiguity around title launch observability at Netflix. In this installment, we will explore the strategies, tools, and methodologies…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "Title Launch Observability at Netflix ScalePart 3: System Strategies and ArchitectureBy: Varun KhaitanWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo MarquesThis blog post is a continuation of Part 2, where we cleared the ambiguity around title launch observability at Netflix. In this installment, we will explore the strategies, tools, and methodologies that were employed to achieve comprehensive title observability at scale.Defining the observability endpointTo create a comprehensive solution, we decided to introduce observability endpoints first. Each microservice involved in our Personalization stack that integrated with our observability solution had to introduce a new “Title Health” endpoint. Our goal was for each new endpoint to adhere to a few principles:Accurate reflection of production behaviorStandardization across all endpointsAnswering the Insight Triad: “Healthy” or not, why not and how to fix it.Accurately Reflecting Production BehaviorA key part of our solution is insights into production behavior, which necessitates our requests to the endpoint result in traffic to the real service functions that mimics the same pathways the traffic would take if it came from the usual callers.In order to allow for this mimicking, many systems implement an “event” handling, where they convert our request into a call to the real service with properties enabled to log when titles are filtered out of their response and why. Building services that adhere to software best practices, such as Object-Oriented Programming (OOP), the SOLID principles, and modularization, is crucial to have success at this stage. Without these practices, service endpoints may become tightly coupled to business logic, making it challenging and costly to add a new endpoint that seamlessly integrates with the observability solution while following the same production logic.A service with modular business logic facilitates the seamless addition of an observability endpoint.StandardizationTo standardize communication between our observability service and the personalization stack’s observability endpoints, we’ve developed a stable proto request/response format. This centralized format, defined and maintained by our team, ensures all endpoints adhere to a consistent protocol. As a result, requests are uniformly handled, and responses are processed cohesively. This standardization enhances adoption within the personalization stack, simplifies the system, and improves understanding and debuggability for engineers.The request schema for the observability endpoint.The Insight Triad APITo efficiently understand the health of a title and triage issues quickly, all implementations of the observability endpoint must answer: is the title eligible for this phase of promotion, if not — why is it not eligible, and what can be done to fix any problems.The end-users of this observability system are Launch Managers, whose job it is to ensure smooth title launches. As such, they must be able to quickly see whether there is a problem, what the problem is, and how to solve it. Teams implementing the endpoint must provide as much information as possible so that a non-engineer (Launch Manager) can understand the root cause of the issue and fix any title setup issues as they arise. They must also provide enough information for partner engineers to identify the problem with the underlying service in cases of system-level issues.These requirements are captured in the following protobuf object that defines the endpoint response.The response schema for the observability endpoint.High level architectureWe’ve distilled our comprehensive solution into the following key steps, capturing the essence of our approach:Establish observability endpoints across all services within our Personalization and Discovery Stack.Implement proactive monitoring for each of these endpoints.Track real-time title impressions from the Netflix UI.Store the data in an optimized, highly distributed datastore.Offer easy-to-integrate APIs for our dashboard, enabling stakeholders to track specific titles effectively.“Time Travel” to validate ahead of time.Observability stack high level architecture diagramIn the following sections, we will explore each of these concepts and components as illustrated in the diagram above.Key FeaturesProactive monitoring through scheduled collectors jobsOur Title Health microservice runs a scheduled collector job every 30 minutes for most of our personalization stack.For each Netflix row we support (such as Trending Now, Coming Soon, etc.), there is a dedicated collector. These collectors retrieve the relevant list of titles from our catalog that qualify for a specific row by interfacing with our catalog services. These services are informed about the expected subset of titles for each row, for which we are assessing title health.Once a collector retrieves its list of candidate titles, it orchestrates batched calls to assigned row services using the above standardized schema to retrieve all the relevant health information of the titles. Additionally, some collectors will instead poll our kafka queue for impressions data.Real-time Title Impressions and Kafka QueueIn addition to evaluating title health via our personalization stack services, we also keep an eye on how our recommendation algorithms treat titles by reviewing impressions data. It’s essential that our algorithms treat all titles equitably, for each one has limitless potential.This data is processed from a real-time impressions stream into a Kafka queue, which our title health system regularly polls. Specialized collectors access the Kafka queue every two minutes to retrieve impressions data. This data is then aggregated in minute(s) intervals, calculating the number of impressions titles receive in near-real-time, and presented as an additional health status indicator for stakeholders.Data storage and distribution through Hollow FeedsNetflix Hollow is an Open Source java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Given the shape of our data, hollow feeds are an excellent strategy to distribute the data across our service boxes.Once collectors gather health data from partner services in the personalization stack or from our impressions stream, this data is stored in a dedicated Hollow feed for each collector. Hollow offers numerous features that help us monitor the overall health of a Netflix row, including ensuring there are no large-scale issues across a feed publish. It also allows us to track the history of each title by maintaining a per-title data history, calculate differences between previous and current data versions, and roll back to earlier versions if a problematic data change is detected.Observability Dashboard using Health Check EngineWe maintain several dashboards that utilize our title health service to present the status of titles to stakeholders. These user interfaces access an endpoint in our service, enabling them to request the current status of a title across all supported rows. This endpoint efficiently reads from all available Hollow Feeds to obtain the current status, thanks to Hollow’s in-memory capabilities. The results are returned in a standardized format, ensuring easy support for future UIs.Additionally, we have other endpoints that can summarize the health of a title across subsets of sections to highlight specific member experiences.Message depicting a dashboard request.Time Traveling: Catching before launchTitles launching at Netflix go through several phases of pre-promotion before ultimately launching on our platform. For each of these phases, the first several hours of promotion are critical for the reach and effective personalization of a title, especially once the title has launched. Thus, to prevent issues as titles go through the launch lifecycle, our observability system needs to be capable of simulating traffic ahead of time so that relevant teams can catch and fix issues before they impact members. We call this capability “Time Travel”.Many of the metadata and assets involved in title setup have specific timelines for when they become available to members. To determine if a title will be viewable at the start of an experience, we must simulate a request to a partner service as if it were from a future time when those specific metadata or assets are available. This is achieved by including a future timestamp in our request to the observability endpoint, corresponding to when the title is expected to appear for a given experience. The endpoint then communicates with any further downstream services using the context of that future timestamp.An example request with a future timestamp.ConclusionThroughout this series, we’ve explored the journey of enhancing title launch observability at Netflix. In Part 1, we identified the challenges of managing vast content launches and the need for scalable solutions to ensure each title’s success. Part 2 highlighted the strategic approach to navigating ambiguity, introducing “Title Health” as a framework to align teams and prioritize core issues. In this final part, we detailed the sophisticated system strategies and architecture, including observability endpoints, proactive monitoring, and “Time Travel” capabilities; all designed to ensure a thrilling viewing experience.By investing in these innovative solutions, we enhance the discoverability and success of each title, fostering trust with content creators and partners. This journey not only bolsters our operational capabilities but also lays the groundwork for future innovations, ensuring that every story reaches its intended audience and that every member enjoys their favorite titles on Netflix.Thank you for joining us on this exploration, and stay tuned for more insights and innovations as we continue to entertain the world.",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*Zz2Y8yjPAsbG5WVR",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003e\u003ch2 id=\"474b\" data-testid=\"storyTitle\"\u003eTitle Launch Observability at Netflix Scale\u003c/h2\u003e\u003c/p\u003e\u003cdiv\u003e\u003ch2 id=\"645e\"\u003ePart 3: System Strategies and Architecture\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page---byline--8efe69ebd653---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://netflixtechblog.com/?source=post_page---byline--8efe69ebd653---------------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Netflix TechBlog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cp id=\"a132\"\u003e\u003cstrong\u003eBy:\u003c/strong\u003e \u003ca href=\"https://www.linkedin.com/in/varun-khaitan/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVarun Khaitan\u003c/a\u003e\u003c/p\u003e\u003cp id=\"6090\"\u003eWith special thanks to my stunning colleagues: \u003ca href=\"https://www.linkedin.com/in/mallikarao/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMallika Rao\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/esmir-mesic/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEsmir Mesic\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/hugodesmarques/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHugo Marques\u003c/a\u003e\u003c/p\u003e\u003cp id=\"6db9\"\u003eThis blog post is a continuation of \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed\"\u003ePart 2\u003c/a\u003e, where we cleared the ambiguity around title launch observability at Netflix. In this installment, we will explore the strategies, tools, and methodologies that were employed to achieve comprehensive title observability at scale.\u003c/p\u003e\u003ch2 id=\"69a2\"\u003eDefining the observability endpoint\u003c/h2\u003e\u003cp id=\"bc9b\"\u003eTo create a comprehensive solution, we decided to introduce observability endpoints first. Each microservice involved in our \u003cstrong\u003ePersonalization stack\u003c/strong\u003e that integrated with our observability solution had to introduce a new “Title Health” endpoint. Our goal was for each new endpoint to adhere to a few principles:\u003c/p\u003e\u003col\u003e\u003cli id=\"1298\"\u003eAccurate reflection of production behavior\u003c/li\u003e\u003cli id=\"cade\"\u003eStandardization across all endpoints\u003c/li\u003e\u003cli id=\"7204\"\u003eAnswering the Insight Triad: “Healthy” or not, why not and how to fix it.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"0cac\"\u003e\u003cstrong\u003eAccurately Reflecting Production Behavior\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"4dbf\"\u003eA key part of our solution is insights into production behavior, which necessitates our requests to the endpoint result in traffic to the real service functions that mimics the same pathways the traffic would take if it came from the usual callers.\u003c/p\u003e\u003cp id=\"377d\"\u003eIn order to allow for this mimicking, many systems implement an “event” handling, where they convert our request into a call to the real service with properties enabled to log when titles are filtered out of their response and why. Building services that adhere to software best practices, such as Object-Oriented Programming (OOP), the SOLID principles, and modularization, is crucial to have success at this stage. Without these practices, service endpoints may become tightly coupled to business logic, making it challenging and costly to add a new endpoint that seamlessly integrates with the observability solution while following the same production logic.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eA service with modular business logic facilitates the seamless addition of an observability endpoint.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"2da9\"\u003e\u003cstrong\u003eStandardization\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"e9b6\"\u003eTo standardize communication between our observability service and the personalization stack’s observability endpoints, we’ve developed a stable proto request/response format. This centralized format, defined and maintained by our team, ensures all endpoints adhere to a consistent protocol. As a result, requests are uniformly handled, and responses are processed cohesively. This standardization enhances adoption within the personalization stack, simplifies the system, and improves understanding and debuggability for engineers.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eThe request schema for the observability endpoint.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"97c5\"\u003e\u003cstrong\u003eThe Insight Triad API\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"95d2\"\u003eTo efficiently understand the health of a title and triage issues quickly, all implementations of the observability endpoint must answer: is the title eligible for this phase of promotion, if not — why is it not eligible, and what can be done to fix any problems.\u003c/p\u003e\u003cp id=\"a1a7\"\u003eThe end-users of this observability system are Launch Managers, whose job it is to ensure smooth title launches. As such, they must be able to quickly see whether there is a problem, what the problem is, and how to solve it. Teams implementing the endpoint must provide as much information as possible so that a non-engineer (Launch Manager) can understand the root cause of the issue and fix any title setup issues as they arise. They must also provide enough information for partner engineers to identify the problem with the underlying service in cases of system-level issues.\u003c/p\u003e\u003cp id=\"29f8\"\u003eThese requirements are captured in the following protobuf object that defines the endpoint response.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eThe response schema for the observability endpoint.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"b501\"\u003eHigh level architecture\u003c/h2\u003e\u003cp id=\"34a0\"\u003eWe’ve distilled our comprehensive solution into the following key steps, capturing the essence of our approach:\u003c/p\u003e\u003col\u003e\u003cli id=\"7946\"\u003eEstablish observability endpoints across all services within our Personalization and Discovery Stack.\u003c/li\u003e\u003cli id=\"d917\"\u003eImplement proactive monitoring for each of these endpoints.\u003c/li\u003e\u003cli id=\"feab\"\u003eTrack real-time title impressions from the Netflix UI.\u003c/li\u003e\u003cli id=\"08ec\"\u003eStore the data in an optimized, highly distributed datastore.\u003c/li\u003e\u003cli id=\"109e\"\u003eOffer easy-to-integrate APIs for our dashboard, enabling stakeholders to track specific titles effectively.\u003c/li\u003e\u003cli id=\"51d0\"\u003e“Time Travel” to validate ahead of time.\u003c/li\u003e\u003c/ol\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eObservability stack high level architecture diagram\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"9f94\"\u003eIn the following sections, we will explore each of these concepts and components as illustrated in the diagram above.\u003c/p\u003e\u003ch2 id=\"ea0e\"\u003eKey Features\u003c/h2\u003e\u003ch2 id=\"5bfa\"\u003eProactive monitoring through scheduled collectors jobs\u003c/h2\u003e\u003cp id=\"2070\"\u003eOur Title Health microservice runs a scheduled collector job every 30 minutes for most of our personalization stack.\u003c/p\u003e\u003cp id=\"9419\"\u003eFor each Netflix row we support (such as Trending Now, Coming Soon, etc.), there is a dedicated collector. These collectors retrieve the relevant list of titles from our catalog that qualify for a specific row by interfacing with our catalog services. These services are informed about the expected subset of titles for each row, for which we are assessing title health.\u003c/p\u003e\u003cp id=\"0bb9\"\u003eOnce a collector retrieves its list of candidate titles, it orchestrates batched calls to assigned row services using the above standardized schema to retrieve all the relevant health information of the titles. Additionally, some collectors will instead poll our kafka queue for impressions data.\u003c/p\u003e\u003ch2 id=\"07cf\"\u003eReal-time Title Impressions and Kafka Queue\u003c/h2\u003e\u003cp id=\"c0e6\"\u003eIn addition to evaluating title health via our personalization stack services, we also keep an eye on how our recommendation algorithms treat titles by reviewing impressions data. It’s essential that our algorithms treat all titles equitably, for each one has limitless potential.\u003c/p\u003e\u003cp id=\"1754\"\u003eThis data is processed from a real-time impressions stream into a Kafka queue, which our title health system regularly polls. Specialized collectors access the Kafka queue every two minutes to retrieve impressions data. This data is then aggregated in minute(s) intervals, calculating the number of impressions titles receive in near-real-time, and presented as an additional health status indicator for stakeholders.\u003c/p\u003e\u003ch2 id=\"958b\"\u003eData storage and distribution through Hollow Feeds\u003c/h2\u003e\u003cp id=\"0fe5\"\u003e\u003ca href=\"https://hollow.how/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNetflix Hollow\u003c/a\u003e is an Open Source java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Given the shape of our data, hollow feeds are an excellent strategy to distribute the data across our service boxes.\u003c/p\u003e\u003cp id=\"7314\"\u003eOnce collectors gather health data from partner services in the personalization stack or from our impressions stream, this data is stored in a dedicated Hollow feed for each collector. Hollow offers numerous features that help us monitor the overall health of a Netflix row, including ensuring there are no large-scale issues across a feed publish. It also allows us to track the history of each title by maintaining a per-title data history, calculate differences between previous and current data versions, and roll back to earlier versions if a problematic data change is detected.\u003c/p\u003e\u003ch2 id=\"321f\"\u003eObservability Dashboard using Health Check Engine\u003c/h2\u003e\u003cp id=\"d7ac\"\u003eWe maintain several dashboards that utilize our title health service to present the status of titles to stakeholders. These user interfaces access an endpoint in our service, enabling them to request the current status of a title across all supported rows. This endpoint efficiently reads from all available Hollow Feeds to obtain the current status, thanks to Hollow’s in-memory capabilities. The results are returned in a standardized format, ensuring easy support for future UIs.\u003c/p\u003e\u003cp id=\"5c46\"\u003eAdditionally, we have other endpoints that can summarize the health of a title across subsets of sections to highlight specific member experiences.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eMessage depicting a dashboard request.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"e3b2\"\u003eTime Traveling: Catching before launch\u003c/h2\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"3f42\"\u003eTitles launching at Netflix go through several phases of pre-promotion before ultimately launching on our platform. For each of these phases, the first several hours of promotion are critical for the reach and effective personalization of a title, especially once the title has launched. Thus, to prevent issues as titles go through the launch lifecycle, our observability system needs to be capable of simulating traffic ahead of time so that relevant teams can catch and fix issues before they impact members. We call this capability \u003cstrong\u003e“Time Travel”\u003c/strong\u003e.\u003c/p\u003e\u003cp id=\"6a4a\"\u003eMany of the metadata and assets involved in title setup have specific timelines for when they become available to members. To determine if a title will be viewable at the start of an experience, we must simulate a request to a partner service as if it were from a future time when those specific metadata or assets are available. This is achieved by including a future timestamp in our request to the observability endpoint, corresponding to when the title is expected to appear for a given experience. The endpoint then communicates with any further downstream services using the context of that future timestamp.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eAn example request with a future timestamp.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"a91c\"\u003eConclusion\u003c/h2\u003e\u003cp id=\"0f5b\"\u003eThroughout this series, we’ve explored the journey of enhancing title launch observability at Netflix. In \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/title-launch-observability-at-netflix-scale-c88c586629eb\"\u003ePart 1\u003c/a\u003e, we identified the challenges of managing vast content launches and the need for scalable solutions to ensure each title’s success. \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed\"\u003ePart 2\u003c/a\u003e highlighted the strategic approach to navigating ambiguity, introducing “Title Health” as a framework to align teams and prioritize core issues. In this final part, we detailed the sophisticated system strategies and architecture, including observability endpoints, proactive monitoring, and “Time Travel” capabilities; all designed to ensure a thrilling viewing experience.\u003c/p\u003e\u003cp id=\"ad08\"\u003eBy investing in these innovative solutions, we enhance the discoverability and success of each title, fostering trust with content creators and partners. This journey not only bolsters our operational capabilities but also lays the groundwork for future innovations, ensuring that every story reaches its intended audience and that every member enjoys their favorite titles on Netflix.\u003c/p\u003e\u003cp id=\"4e68\"\u003eThank you for joining us on this exploration, and stay tuned for more insights and innovations as we continue to entertain the world.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-03-05T01:24:53.778Z",
  "modifiedTime": null
}
