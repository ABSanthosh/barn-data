{
  "id": "56a63ed2-9c32-4337-a7cd-ce7b4de40d10",
  "title": "Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix",
  "link": "https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Thu, 12 Jun 2025 14:56:32 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": [
    "data-catalog",
    "knowledge-management",
    "data-management",
    "data-engineering",
    "rdf"
  ],
  "byline": "Netflix Technology Blog",
  "length": 24037,
  "excerpt": "Introducing UDA, the knowledge-graph-based architecture that translates conceptual domain models into consistent schemas and data pipelines.",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "By Alex Hutter, Alexandre Bertails, Claire Wang, Haoyuan He, Kishore Banala, Peter Royal, Shervin AfsharAs Netflix’s offerings grow — across films, series, games, live events, and ads — so does the complexity of the systems that support it. Core business concepts like ‘actor’ or ‘movie’ are modeled in many places: in our Enterprise GraphQL Gateway powering internal apps, in our asset management platform storing media assets, in our media computing platform that powers encoding pipelines, to name a few. Each system models these concepts differently and in isolation, with little coordination or shared understanding. While they often operate on the same concepts, these systems remain largely unaware of that fact, and of each other.As a result, several challenges emerge:Duplicated and Inconsistent Models — Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile.Inconsistent Terminology — Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder.Data Quality Issues — Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues.Limited Connectivity — Within systems, relationships between data are constrained by what each system supports. Across systems, they are effectively non-existent.To address these challenges, we need new foundations that allow us to define a model once, at the conceptual level, and reuse those definitions everywhere. But it isn’t enough to just document concepts; we need to connect them to real systems and data. And more than just connect, we have to project those definitions outward, generating schemas and enforcing consistency across systems. The conceptual model must become part of the control plane.These were the core ideas that led us to build UDA.Introducing UDAUDA (Unified Data Architecture) is the foundation for connected data in Content Engineering. It enables teams to model domains once and represent them consistently across systems — powering automation, discoverability, and semantic interoperability.Using UDA, users and systems can:Register and connect domain models — formal conceptualizations of federated business domains expressed as data.Why? So everyone uses the same official definitions for business concepts, which avoids confusion and stops different teams from rebuilding similar models in conflicting ways.Catalog and map domain models to data containers, such as GraphQL type resolvers served by a Domain Graph Service, Data Mesh sources, or Iceberg tables, through their representation as a graph.Why? To make it easy to find where the actual data for these business concepts lives (e.g., in which specific database, table, or service) and understand how it’s structured there.Transpile domain models into schema definition languages like GraphQL, Avro, SQL, RDF, and Java, while preserving semantics.Why? To automatically create consistent technical data structures (schemas) for various systems directly from the domain models, saving developers manual effort and reducing errors caused by out-of-sync definitions.Move data faithfully between data containers, such as from federated GraphQL entities to Data Mesh (a general purpose data movement and processing platform for moving data between Netflix systems at scale), Change Data Capture (CDC) sources to joinable Iceberg Data Products.Why? To save developer time by automatically handling how data is moved and correctly transformed between different systems. This means less manual work to configure data movement, ensuring data shows up consistently and accurately wherever it’s needed.Discover and explore domain concepts via search and graph traversal.Why? So anyone can more easily find the specific business information they’re looking for, understand how different concepts and data are related, and be confident they are accessing the correct information.Programmatically introspect the knowledge graph using Java, GraphQL, or SPARQL.Why? So developers can build smarter applications that leverage this connected business information, automate more complex data-dependent workflows, and help uncover new insights from the relationships in the data.This post introduces the foundations of UDA as a knowledge graph, connecting domain models to data containers through mappings, and grounded in an in-house metamodel, or model of models, called Upper. Upper defines the language for domain modeling in UDA and enables projections that automatically generate schemas and pipelines across systems.The same domain model can be connected to semantically equivalent data containers in the UDA knowledge graph.This post also highlights two systems that leverage UDA in production:Primary Data Management (PDM) is our platform for managing authoritative reference data and taxonomies. PDM turns domain models into flat or hierarchical taxonomies that drive a generated UI for business users. These taxonomy models are projected into Avro and GraphQL schemas, automatically provisioning data products in the Warehouse and GraphQL APIs in the Enterprise Gateway.Sphere is our self-service operational reporting tool for business users. Sphere uses UDA to catalog and relate business concepts across systems, enabling discovery through familiar terms like ‘actor’ or ‘movie.’ Once concepts are selected, Sphere walks the knowledge graph and generates SQL queries to retrieve data from the warehouse, no manual joins or technical mediation required.UDA is a Knowledge GraphUDA needs to solve the data integration problem. We needed a data catalog unified with a schema registry, but with a hard requirement for semantic integration. Connecting business concepts to schemas and data containers in a graph-like structure, grounded in strong semantic foundations, naturally led us to consider a knowledge graph approach.We chose RDF and SHACL as the foundation for UDA’s knowledge graph. But operationalizing them at enterprise scale surfaced several challenges:RDF lacked a usable information model. While RDF offers a flexible graph structure, it provides little guidance on how to organize data into named graphs, manage ontology ownership, or define governance boundaries. Standard follow-your-nose mechanisms like owl:imports apply only to ontologies and don’t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them.SHACL is not a modeling language for enterprise data. Designed to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems.Teams lacked shared authoring practices. Without strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas.Ontology tooling lacked support for collaborative modeling. Unlike GraphQL Federation, ontology frameworks had no built-in support for modular contributions, team ownership, or safe federation. Most engineers found the tools and concepts unfamiliar, and available authoring environments lacked the structure needed for coordinated contributions.To address these challenges, UDA adopts a named-graph-first information model. Each named graph conforms to a governing model, itself a named graph in the knowledge graph. This systematic approach ensures resolution, modularity, and enables governance across the entire graph. While a full description of UDA’s information infrastructure is beyond the scope of this post, the next sections explain how UDA bootstraps the knowledge graph with its metamodel and uses it to model data container representations and mappings.Upper is Domain ModelingUpper is a language for formally describing domains — business or system — and their concepts. These concepts are organized into domain models: controlled vocabularies that define classes of keyed entities, their attributes, and their relationships to other entities, which may be keyed or nested, within the same domain or across domains. Keyed concepts within a domain model can be organized in taxonomies of types, which can be as complex as the business or the data system needs them to be. Keyed concepts can also be extended from other domain models — that is, new attributes and relationships can be contributed monotonically. Finally, Upper ships with a rich set of datatypes for attribute values, which can also be customized per domain.The graph representation of the onepiece: domain model from our UI. Depicted here you can see how Characters are related to Devil Fruit, and that each Devil Fruit has a type.Upper domain models are data. They are expressed as conceptual RDF and organized into named graphs, making them introspectable, queryable, and versionable within the UDA knowledge graph. This graph unifies not just the domain models themselves, but also the schemas they transpile to — GraphQL, Avro, Iceberg, Java — and the mappings that connect domain concepts to concrete data containers, such as GraphQL type resolvers served by a Domain Graph Service, Data Mesh sources, or Iceberg tables, through their representations. Upper raises the level of abstraction above traditional ontology languages: it defines a strict subset of semantic technologies from the W3C tailored and generalized for domain modeling. It builds on ontology frameworks like RDFS, OWL, and SHACL so domain authors can model effectively without even needing to learn what an ontology is.UDA domain model for One Piece. Link to full definition.Upper is the metamodel for Connected Data in UDA — the model for all models. It is designed as a bootstrapping upper ontology, which means that Upper is self-referencing, because it models itself as a domain model; self-describing, because it defines the very concept of a domain model; and self-validating, because it conforms to its own model. This approach enables UDA to bootstrap its own infrastructure: Upper itself is projected into a generated Jena-based Java API and GraphQL schema used in GraphQL service federated into Netflix’s Enterprise GraphQL gateway. These same generated APIs are then used by the projections and the UI. Because all domain models are conservative extensions of Upper, other system domain models — including those for GraphQL, Avro, Data Mesh, and Mappings — integrate seamlessly into the same runtime, enabling consistent data semantics and interoperability across schemas.Traversing a domain model programmatically using the Java API generated from the Upper metamodel.Data Container RepresentationsData containers are repositories of information. They contain instance data that conform to their own schema languages or type systems: federated entities from GraphQL services, Avro records from Data Mesh sources, rows from Iceberg tables, or objects from Java APIs. Each container operates within the context of a system that imposes its own structural and operational constraints.A Data Mesh source is a data container.Data container representations are data. They are faithful interpretations of the members of data systems as graph data. UDA captures the definition of these systems as their own domain models, the system domains. These models encode both the information architecture of the systems and the schemas of the data containers within. They provide a blueprint for translating the systems into graph representations.UDA catalogs the data container representations into the knowledge graph. It records the coordinates and metadata of the underlying data assets, but unlike a traditional catalog, it only tracks assets that are semantically connected to domain models. This enables users and systems to connect concepts from domain models to the concrete locations where corresponding instance data can be accessed. Those connections are called Mappings.MappingsMappings are data that connect domain models to data containers. Every element in a domain model is addressable, from the domain model itself down to specific attributes and relationships. Likewise, data container representations make all components addressable, from an Iceberg table to an individual column, or from a GraphQL type to a specific field. A Mapping connects nodes in a subgraph of the domain model to nodes in a subgraph of a container representation. Visually, the Mapping is the set of arcs that link those two graphs together.A mapping between a domain model and a Data Mesh Source from the UDA UI. Link to full mapping.Mappings enable discovery. Starting from a domain concept, users and systems can walk the knowledge graph to find where that concept is materialized — in which data system, in which container, and even how a specific attribute or relationship is physically accessed. The inverse is also supported: given a data container, one can trace back to the domain concepts it participates in.Mappings shape UDA’s approach to semantic data integration. Most existing schema languages are not expressive enough in capturing richer semantics of a domain to address requirements for data integration (for example, “accessibility of data, providing semantic context to support its interpretation, and establishing meaningful links between data”). A trivial example of this could be seen in the lack of built-in facilities in Avro to represent foreign keys, making it very hard to express how entities relate across Data Mesh sources. Mappings, together with the corresponding system domain models, allow for such relationships, and many other constraints, to be defined in the domain models and used programmatically in actual data systems.Mappings enable intent-based automation. Data is not always available in the systems where consumers need it. Because Mappings encode both meaning and location, UDA can reason about how data should move, preserving semantics, without requiring the consumer to specify how it should be done. Beyond the cataloging use case, connecting to existing containers, UDA automatically derives canonical Mappings from registered domain models as part of the projection process.ProjectionsA projection produces a concrete data container. These containers, such as a GraphQL schema or a Data Mesh source, implement the characteristics derived from a registered domain model. Each projection is a concrete realization of Upper’s denotational semantics, ensuring semantic interoperability across all containers projected from the same domain model.Projections produce consistent public contracts across systems. The data containers generated by projections encode data contracts in the form of schemas, derived by transpiling a domain model into the target container’s schema language. UDA currently supports transpilation to GraphQL and Avro schemas.The GraphQL transpilation produces a schema that adheres to the official GraphQL spec with the ability to generate all GraphQL types defined in the spec. Given that the UDA domain model can be federated, it also supports generating federated graphQL schemas. Below is an example of a transpiled GraphQL schema.The Avro transpilation produces a schema that is a Data Mesh flavor of Avro, which includes some customization on top of the official Avro spec. This schema is used to automatically create a Data Mesh source container. Below is an example of a transpiled Avro schema.Projections can automatically populate data containers. Some projections, such as those to GraphQL schemas or Data Mesh sources produce empty containers that require developers to populate the data. This might be creating GraphQL APIs or pushing events onto Data Mesh sources. Conversely, other containers, like Iceberg Tables, are automatically created and populated by UDA. For Iceberg Tables, UDA leverages the Data Mesh platform to automatically create data streams to move data into tables. This process utilizes much of the same infrastructure detailed in this blog post here.Projections have mappings. UDA automatically generates and manages mappings between the newly created data containers and the projected domain model.Early AdoptersControlled Vocabularies (PDM)The full range of Netflix’s business activities relies on a sprawling data model that captures the details of our many business processes. Teams need to be able to coordinate operational activities to ensure that content production is complete, advertising campaigns are in place, and promotional assets are ready to deploy. We implicitly depend upon a singular definition of shared concepts, such as content production is complete. Multiple definitions create coordination challenges. Software (and humans) don’t know that the definitions mean the same thing.We started the Primary Data Management (PDM) initiative to create unified and consistent definitions for the core concepts in our data model. These definitions form controlled vocabularies, standardized and governed lists for what values are permitted within certain fields in our data model.Primary Data Management (PDM) is a single place where business users can manage controlled vocabularies. Our data model governance has been scattered across different tools and teams creating coordination challenges. This is an information management problem relating to the definition, maintenance and consistent use of reference data and taxonomies. This problem is not unique to Netflix, so we looked outward for existing solutions to this problem.Managing the taxonomy of One Piece characters in PDM.PDM uses the Simple Knowledge Organization System (SKOS) model. It is a W3C data standard designed for modeling knowledge. Its terminology is abstract, with Concepts that can be organized into ConceptSchemes and properties to describe various types of relationships. Every system is hardcoded against something, that’s how software knows how to manipulate data. We want a system that can work with a data model as its input, so we still need something concrete to build the software against. This is what SKOS provides, a generic basis for modeling knowledge that our system can understand.PDM uses Domain Models to integrate SKOS into the rest of Content Engineering’s ecosystem. A core premise of the system is that it takes a domain model as input, and everything that can be derived is derived from that model. PDM builds a user interface based upon the model definition and leverages UDA to project this model into type-safe interfaces for other systems to use. The system will provision a Domain Graph Service (DGS) within our federated GraphQL API environment using a GraphQL schema that UDA projects from the domain model. UDA is also used to provision data movement pipelines which are able to feed our GraphSearch infrastructure as well as move data into the warehouse. The data movement systems use Avro schemas, and UDA creates a projection from the domain model to Avro.Consumers of controlled vocabularies never know they’re using SKOS. Domain models use terms that fit in with the domain. SKOS’s generic notion of broader and narrower to define a hierarchy are hidden from consumers as super-properties within the model. This allows consumers to work with language that is familiar to them while enabling PDM to work with any model. The best of both worlds.Operational Reporting (Sphere)Operational reporting serves the detailed day-to-day activities and processes of a business domain. It is a reporting paradigm specialized in covering high-resolution, low-latency data sets.Operational reporting systems should generate reports without relying on technical intermediaries. Operational reporting systems need to address the persistent challenge of empowering business users to explore and obtain the data they need, when they need it. Without such self-service systems, requests for new reports or data extracts often result in back-and-forth exchanges, where the initial query may not exactly meet business users’ expectations, requiring further clarification and refinement.Data discovery and query generation are two relevant aspects of data integration. Supplying end-users with an accurate, contextual, and user-friendly data discovery experience provides a basis for query generation mechanism which produces syntactically correct and semantically reliable queries.Operational reports are predominantly run on data hydrated from GraphQL services into the Data Warehouse. You can read about our journey from conventional data movement to streaming data pipelines based on CDC and GraphQL hydration in this blog post. Among the challenging byproducts of this approach was that a single, distinct data concept is now present in two places (GraphQL and data warehouse), with some disparity in semantic context to guide and support the interpretations and connectivity of that data. To address this, we formulate a mechanism to use the syntax and semantics captured in the federated schema from Netflix’s Enterprise GraphQL and populate representational domain models in UDA to preserve those details and add more.Domain models enable the data discovery experience. Metadata aggregated from various data-producing systems is captured in UDA domain models using a unified vocabulary. This metadata is surfaced for the users’ search and discovery needs; instead of specifying exact tables and join keys, users simply can search for familiar business concepts such as ‘actors’ or ‘movies’. We use UDA models to disambiguate and resolve the intended concepts and their related data entities.UDA knowledge graph is the data landscape for query generation. Once concepts are discovered and their mappings to corresponding data containers are identified and located in the knowledge graph, we use them to establish join strategies. Through graph traversal, we identify boundaries and islands within the data landscape. This ensures only feasible, joinable combinations are selected while weeding out semantically incorrect and non-executable query candidates.Generating a report in Sphere.Sphere is a UDA-powered self-service operational reporting system. The solution based on knowledge graphs described above is called Sphere. Seeing self-service operational reporting through this lens, we can improve business users’ agency in access to operational data. They are empowered to explore, assemble, and refine reports at the conceptual level, while technical complexities are managed by the system.Stay TunedUDA marks a fundamental shift in how we approach data modeling within Content Engineering. By providing a unified knowledge graph composed of what we know about our various data systems and the business concepts within them, we’ve made information more consistent, connected, and discoverable across our organization. We’re excited about future applications of these ideas such as:Supporting additional projections like Protobuf/gRPCMaterializing the knowledge graph of instance data for querying, profiling, and managementFinally solving some of the initial challenges posed by Graph Search (that actually inspired some of this work)If you’re interested in this space, we’d love to connect — whether you’re exploring new roles down the road or just want to swap ideas.Expect to see future blog posts exploring PDM and Sphere in more detail soon!CreditsThanks to Andreas Legenbauer, Bernardo Gomez Palacio Valdes, Charles Zhao, Christopher Chong, Deepa Krishnan, George Pesmazoglou, Jessica Silva, Katherine Anderson, Malik Day, Rita Bogdanova, Ruoyun Zheng, Shawn Stedman, Suchita Goyal, Utkarsh Shrivastava, Yoomi Koh, Yulia Shmeleva",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*j1I2cLD0vtfE9IQfNiUwVQ.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv tabindex=\"-1\" aria-hidden=\"false\"\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page---byline--6a6aee261d8d---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"32\" height=\"32\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"0456\"\u003eBy \u003ca href=\"https://www.linkedin.com/in/ahutter/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAlex Hutter\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/bertails/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAlexandre Bertails\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/clairezwang0612/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eClaire Wang\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/haoyuan-h-98b587134/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHaoyuan He\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/kishore-banala/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKishore Banala\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/peterroyal/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePeter Royal\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/shervinafshar/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eShervin Afshar\u003c/a\u003e\u003c/p\u003e\u003cp id=\"e595\"\u003eAs Netflix’s offerings grow — across films, series, games, live events, and ads — so does the complexity of the systems that support it. Core business concepts like ‘actor’ or ‘movie’ are modeled in many places: in our Enterprise GraphQL Gateway powering internal apps, in our asset management platform storing media assets, in our media computing platform that powers encoding pipelines, to name a few. Each system models these concepts differently and in isolation, with little coordination or shared understanding. While they often operate on the same concepts, these systems remain largely unaware of that fact, and of each other.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"9f61\"\u003eAs a result, several challenges emerge:\u003c/p\u003e\u003cul\u003e\u003cli id=\"c101\"\u003e\u003cstrong\u003eDuplicated and Inconsistent Models\u003c/strong\u003e — Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile.\u003c/li\u003e\u003cli id=\"e1ed\"\u003e\u003cstrong\u003eInconsistent Terminology\u003c/strong\u003e — Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder.\u003c/li\u003e\u003cli id=\"b67d\"\u003e\u003cstrong\u003eData Quality Issues\u003c/strong\u003e — Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues.\u003c/li\u003e\u003cli id=\"86d8\"\u003e\u003cstrong\u003eLimited Connectivity\u003c/strong\u003e — Within systems, relationships between data are constrained by what each system supports. Across systems, they are effectively non-existent.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"c853\"\u003eTo address these challenges, we need new foundations that allow us to define a model once, at the conceptual level, and reuse those definitions everywhere. But it isn’t enough to just document concepts; we need to connect them to real systems and data. And more than just connect, we have to project those definitions outward, generating schemas and enforcing consistency across systems. The conceptual model must become part of the control plane.\u003c/p\u003e\u003cp id=\"e618\"\u003eThese were the core ideas that led us to build UDA.\u003c/p\u003e\u003ch2 id=\"03da\"\u003eIntroducing UDA\u003c/h2\u003e\u003cp id=\"0faf\"\u003e\u003cstrong\u003eUDA (Unified Data Architecture)\u003c/strong\u003e is the foundation for connected data in \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/netflix-studio-engineering-overview-ed60afcfa0ce\" target=\"_blank\" data-discover=\"true\"\u003eContent Engineering\u003c/a\u003e. It enables teams to model domains once and represent them consistently across systems — powering automation, discoverability, and \u003ca href=\"https://en.wikipedia.org/wiki/Semantic_interoperability\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esemantic interoperability\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"5baf\"\u003e\u003cstrong\u003eUsing UDA, users and systems can:\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"105b\"\u003e\u003cstrong\u003eRegister and connect domain models \u003c/strong\u003e— formal conceptualizations of federated business domains expressed as data.\u003c/p\u003e\u003cul\u003e\u003cli id=\"27d4\"\u003e\u003cstrong\u003eWhy? \u003c/strong\u003eSo everyone uses the same official definitions for business concepts, which avoids confusion and stops different teams from rebuilding similar models in conflicting ways.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"6a56\"\u003e\u003cstrong\u003eCatalog and map domain models to data containers\u003c/strong\u003e, such as GraphQL type resolvers served by a \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/open-sourcing-the-netflix-domain-graph-service-framework-graphql-for-spring-boot-92b9dcecda18\" target=\"_blank\" data-discover=\"true\"\u003eDomain Graph Service\u003c/a\u003e, \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\" target=\"_blank\" data-discover=\"true\"\u003eData Mesh sources\u003c/a\u003e, or Iceberg tables, through their representation as a graph.\u003c/p\u003e\u003cul\u003e\u003cli id=\"36c0\"\u003e\u003cstrong\u003eWhy?\u003c/strong\u003e To make it easy to find where the actual data for these business concepts lives (e.g., in which specific database, table, or service) and understand how it’s structured there.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"b0a9\"\u003e\u003cstrong\u003eTranspile domain models into schema definition languages\u003c/strong\u003e like GraphQL, Avro, SQL, RDF, and Java, while preserving semantics.\u003c/p\u003e\u003cul\u003e\u003cli id=\"5c58\"\u003e\u003cstrong\u003eWhy? \u003c/strong\u003eTo automatically create consistent technical data structures (schemas) for various systems directly from the domain models, saving developers manual effort and reducing errors caused by out-of-sync definitions.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"d376\"\u003e\u003cstrong\u003eMove data faithfully between data containers\u003c/strong\u003e, such as from federated GraphQL entities to \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\" target=\"_blank\" data-discover=\"true\"\u003eData Mesh\u003c/a\u003e (a general purpose data movement and processing platform for moving data between Netflix systems at scale), Change Data Capture (CDC) sources to joinable Iceberg Data Products.\u003c/p\u003e\u003cul\u003e\u003cli id=\"f165\"\u003e\u003cstrong\u003eWhy? \u003c/strong\u003eTo save developer time by automatically handling how data is moved and correctly transformed between different systems. This means less manual work to configure data movement, ensuring data shows up consistently and accurately wherever it’s needed.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"1067\"\u003e\u003cstrong\u003eDiscover and explore domain concepts \u003c/strong\u003evia search and graph traversal.\u003c/p\u003e\u003cul\u003e\u003cli id=\"2c8b\"\u003e\u003cstrong\u003eWhy? \u003c/strong\u003eSo anyone can more easily find the specific business information they’re looking for, understand how different concepts and data are related, and be confident they are accessing the correct information.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"c87b\"\u003e\u003cstrong\u003eProgrammatically introspect the knowledge graph\u003c/strong\u003e using Java, GraphQL, or SPARQL.\u003c/p\u003e\u003cul\u003e\u003cli id=\"a8f7\"\u003e\u003cstrong\u003eWhy?\u003c/strong\u003e So developers can build smarter applications that leverage this connected business information, automate more complex data-dependent workflows, and help uncover new insights from the relationships in the data.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"cb20\"\u003e\u003cstrong\u003eThis post introduces the foundations of UDA\u003c/strong\u003e as a knowledge graph, connecting domain models to data containers through mappings, and grounded in an in-house \u003ca href=\"https://en.wikipedia.org/wiki/Metamodeling#:~:text=A%20metamodel%2F%20surrogate%20model%20is,representing%20input%20and%20output%20relations\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003emetamodel\u003c/a\u003e, or model of models, called Upper. Upper defines the language for domain modeling in UDA and enables projections that automatically generate schemas and pipelines across systems.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eThe same domain model can be connected to semantically equivalent data containers in the UDA knowledge graph.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"b7d0\"\u003e\u003cstrong\u003eThis post also highlights two systems\u003c/strong\u003e that leverage UDA in production:\u003c/p\u003e\u003cp id=\"995a\"\u003e\u003cstrong\u003ePrimary Data Management (PDM)\u003c/strong\u003e is our platform for managing authoritative reference data and taxonomies. PDM turns domain models into flat or hierarchical taxonomies that drive a generated UI for business users. These taxonomy models are projected into Avro and GraphQL schemas, automatically provisioning data products in the Warehouse and GraphQL APIs in the \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2\" target=\"_blank\" data-discover=\"true\"\u003eEnterprise Gateway\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"8584\"\u003e\u003cstrong\u003eSphere\u003c/strong\u003e is our self-service operational reporting tool for business users. Sphere uses UDA to catalog and relate business concepts across systems, enabling discovery through familiar terms like ‘actor’ or ‘movie.’ Once concepts are selected, Sphere walks the knowledge graph and generates SQL queries to retrieve data from the warehouse, no manual joins or technical mediation required.\u003c/p\u003e\u003ch2 id=\"ba47\"\u003eUDA is a Knowledge Graph\u003c/h2\u003e\u003cp id=\"b725\"\u003e\u003cstrong\u003eUDA needs to solve the \u003c/strong\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Data_integration\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003edata integration\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e problem. \u003c/strong\u003eWe needed a data catalog unified with a schema registry, but with a hard requirement for \u003ca href=\"https://en.wikipedia.org/wiki/Semantic_integration#:~:text=Semantic%20integration%20is%20the%20process,from%20diverse%20sources\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esemantic integration\u003c/a\u003e. Connecting business concepts to schemas and data containers in a graph-like structure, grounded in strong semantic foundations, naturally led us to consider a \u003ca href=\"https://en.wikipedia.org/wiki/Knowledge_graph\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eknowledge graph\u003c/a\u003e approach.\u003c/p\u003e\u003cp id=\"0e88\"\u003e\u003cstrong\u003eWe chose RDF and SHACL as the foundation for UDA’s knowledge graph\u003c/strong\u003e. But operationalizing them at enterprise scale surfaced several challenges:\u003c/p\u003e\u003cul\u003e\u003cli id=\"2065\"\u003e\u003cstrong\u003eRDF lacked a usable information model.\u003c/strong\u003e While RDF offers a flexible graph structure, it provides little guidance on how to organize data into \u003ca href=\"https://www.w3.org/TR/rdf12-concepts/#dfn-named-graph\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003enamed graphs\u003c/a\u003e, manage ontology ownership, or define governance boundaries. Standard \u003ca href=\"https://www.w3.org/2001/sw/wiki/Linking_patterns\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003efollow-your-nose mechanisms\u003c/a\u003e like owl:imports apply only to ontologies and don’t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them.\u003c/li\u003e\u003cli id=\"fd63\"\u003e\u003cstrong\u003eSHACL is not a modeling language for enterprise data.\u003c/strong\u003e Designed to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems.\u003c/li\u003e\u003cli id=\"bcdb\"\u003e\u003cstrong\u003eTeams lacked shared authoring practices.\u003c/strong\u003e Without strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas.\u003c/li\u003e\u003cli id=\"f055\"\u003e\u003cstrong\u003eOntology tooling lacked support for collaborative modeling.\u003c/strong\u003e Unlike GraphQL Federation, ontology frameworks had no built-in support for modular contributions, team ownership, or safe federation. Most engineers found the tools and concepts unfamiliar, and available authoring environments lacked the structure needed for coordinated contributions.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"4a57\"\u003e\u003cstrong\u003eTo address these challenges, UDA adopts a named-graph-first information model.\u003c/strong\u003e Each named graph conforms to a governing model, itself a named graph in the knowledge graph. This systematic approach ensures resolution, modularity, and enables governance across the entire graph. While a full description of UDA’s information infrastructure is beyond the scope of this post, the next sections explain how UDA bootstraps the knowledge graph with its metamodel and uses it to model data container representations and mappings.\u003c/p\u003e\u003ch2 id=\"5d0e\"\u003eUpper is Domain Modeling\u003c/h2\u003e\u003cp id=\"440f\"\u003e\u003cstrong\u003eUpper is a language for formally describing domains — business or system — and their concepts\u003c/strong\u003e. \u003ca href=\"https://en.wikipedia.org/wiki/Conceptualization_(information_science)\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eThese concepts are organized into domain models\u003c/a\u003e: controlled vocabularies that define classes of keyed entities, their attributes, and their relationships to other entities, which may be keyed or nested, within the same domain or across domains. Keyed concepts within a domain model can be organized in taxonomies of types, which can be as complex as the business or the data system needs them to be. Keyed concepts can also be extended from other domain models — that is, new attributes and relationships can be \u003ca href=\"https://tomgruber.org/writing/onto-design.pdf#page=4\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003econtributed monotonically\u003c/a\u003e. Finally, Upper ships with a rich set of datatypes for attribute values, which can also be customized per domain.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eThe graph representation of the onepiece: domain model from our UI. Depicted here you can see how Characters are related to Devil Fruit, and that each Devil Fruit has a type.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"82dc\"\u003e\u003cstrong\u003eUpper domain models are data\u003c/strong\u003e. They are expressed as \u003ca href=\"https://www.w3.org/TR/rdf12-concepts/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003econceptual RDF\u003c/a\u003e and organized into named graphs, making them introspectable, queryable, and versionable within the UDA knowledge graph. This graph unifies not just the domain models themselves, but also the schemas they transpile to — GraphQL, Avro, Iceberg, Java — and the mappings that connect domain concepts to concrete data containers, such as GraphQL type resolvers served by a \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/open-sourcing-the-netflix-domain-graph-service-framework-graphql-for-spring-boot-92b9dcecda18\" target=\"_blank\" data-discover=\"true\"\u003eDomain Graph Service\u003c/a\u003e, \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\" target=\"_blank\" data-discover=\"true\"\u003eData Mesh sources\u003c/a\u003e, or Iceberg tables, through their representations. Upper raises the level of abstraction above traditional ontology languages: it defines a strict subset of \u003ca href=\"https://www.w3.org/2001/sw/wiki/Main_Page\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esemantic technologies\u003c/a\u003e from the W3C tailored and generalized for domain modeling. It builds on ontology frameworks like RDFS, OWL, and SHACL so domain authors can model effectively without even needing to learn what an ontology is.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eUDA domain model for One Piece. \u003ca href=\"https://github.com/Netflix-Skunkworks/uda/blob/9627a97fcd972a41ec910be3f928ea7692d38714/uda-intro-blog/onepiece.ttl\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLink to full definition\u003c/a\u003e.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"eed1\"\u003e\u003cstrong\u003eUpper is the metamodel for Connected Data in UDA — the model for all models\u003c/strong\u003e. It is designed as a bootstrapping \u003ca href=\"https://en.wikipedia.org/wiki/Upper_ontology\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eupper ontology\u003c/a\u003e, which means that Upper is \u003cem\u003eself-referencing\u003c/em\u003e, because it models itself as a domain model; \u003cem\u003eself-describing\u003c/em\u003e, because it defines the very concept of a domain model; and \u003cem\u003eself-validating\u003c/em\u003e, because it conforms to its own model. This approach enables UDA to bootstrap its own infrastructure: Upper itself is projected into a generated Jena-based Java API and GraphQL schema used in GraphQL service federated into Netflix’s Enterprise GraphQL gateway. These same generated APIs are then used by the projections and the UI. Because all domain models are \u003ca href=\"https://en.wikipedia.org/wiki/Conservative_extension\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003econservative extensions\u003c/a\u003e of Upper, other system domain models — including those for GraphQL, Avro, Data Mesh, and Mappings — integrate seamlessly into the same runtime, enabling consistent data semantics and interoperability across schemas.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eTraversing a domain model programmatically using the Java API generated from the Upper metamodel.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"67b9\"\u003eData Container Representations\u003c/h2\u003e\u003cp id=\"168d\"\u003e\u003cstrong\u003eData containers are repositories of information. \u003c/strong\u003eThey contain instance data that conform to their own schema languages or type systems: federated entities from GraphQL services, Avro records from Data Mesh sources, rows from Iceberg tables, or objects from Java APIs. Each container operates within the context of a system that imposes its own structural and operational constraints.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eA Data Mesh source is a data container.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"707f\"\u003e\u003cstrong\u003eData container \u003c/strong\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003erepresentations\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e are data.\u003c/strong\u003e They are faithful interpretations of the members of data systems as graph data. UDA captures the definition of these systems as their own domain models, the system domains. These models encode both the information architecture of the systems and the schemas of the data containers within. They provide a blueprint for translating the systems into graph representations.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"c76d\"\u003e\u003cstrong\u003eUDA catalogs the data container representations into the knowledge graph.\u003c/strong\u003e It records the coordinates and metadata of the underlying data assets, but unlike a traditional catalog, it only tracks assets that are semantically connected to domain models. This enables users and systems to connect concepts from domain models to the concrete locations where corresponding instance data can be accessed. Those connections are called \u003cem\u003eMappings\u003c/em\u003e.\u003c/p\u003e\u003ch2 id=\"31d6\"\u003eMappings\u003c/h2\u003e\u003cp id=\"886e\"\u003e\u003cstrong\u003eMappings are data that connect domain models to data containers.\u003c/strong\u003e Every element in a domain model is addressable, from the domain model itself down to specific attributes and relationships. Likewise, data container representations make all components addressable, from an Iceberg table to an individual column, or from a GraphQL type to a specific field. A Mapping connects nodes in a subgraph of the domain model to nodes in a subgraph of a container representation. Visually, the Mapping is the set of arcs that link those two graphs together.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eA mapping between a domain model and a Data Mesh Source from the UDA UI. \u003c/em\u003e\u003ca href=\"https://github.com/Netflix-Skunkworks/uda/blob/9627a97fcd972a41ec910be3f928ea7692d38714/uda-intro-blog/onepiece_character_mappings.ttl\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eLink to full mapping\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"900c\"\u003e\u003cstrong\u003eMappings enable discovery.\u003c/strong\u003e Starting from a domain concept, users and systems can walk the knowledge graph to find where that concept is materialized — in which data system, in which container, and even how a specific attribute or relationship is physically accessed. The inverse is also supported: given a data container, one can trace back to the domain concepts it participates in.\u003c/p\u003e\u003cp id=\"d391\"\u003e\u003cstrong\u003eMappings shape UDA’s approach to semantic data integration.\u003c/strong\u003e Most existing schema languages are not expressive enough in capturing richer semantics of a domain to address requirements for data integration (\u003ca href=\"https://doi.org/10.1007/978-3-319-49340-4_8\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003efor example\u003c/a\u003e, “accessibility of data, providing semantic context to support its interpretation, and establishing meaningful links between data”). A trivial example of this could be seen in the lack of built-in facilities in Avro to represent foreign keys, making it very hard to express how entities relate across Data Mesh sources. Mappings, together with the corresponding system domain models, allow for such relationships, and many other constraints, to be defined in the domain models and used programmatically in actual data systems.\u003c/p\u003e\u003cp id=\"936f\"\u003e\u003cstrong\u003eMappings enable intent-based automation.\u003c/strong\u003e Data is not always available in the systems where consumers need it. Because Mappings encode both meaning and location, UDA can reason about how data should move, preserving semantics, without requiring the consumer to specify how it should be done. Beyond the cataloging use case, connecting to existing containers, UDA automatically derives \u003cem\u003ecanonical Mappings\u003c/em\u003e from registered domain models as part of the projection process.\u003c/p\u003e\u003ch2 id=\"4cec\"\u003eProjections\u003c/h2\u003e\u003cp id=\"b219\"\u003e\u003cstrong\u003eA projection produces a concrete data container.\u003c/strong\u003e These containers, such as a GraphQL schema or a Data Mesh source, implement the characteristics derived from a registered domain model. Each projection is a concrete realization of Upper’s denotational semantics, ensuring \u003ca href=\"https://en.wikipedia.org/wiki/Semantic_interoperability\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esemantic interoperability\u003c/a\u003e across all containers projected from the same domain model.\u003c/p\u003e\u003cp id=\"c0d9\"\u003e\u003cstrong\u003eProjections produce consistent public contracts across systems.\u003c/strong\u003e The data containers generated by projections encode data contracts in the form of schemas, derived by transpiling a domain model into the target container’s schema language. UDA currently supports transpilation to GraphQL and Avro schemas.\u003c/p\u003e\u003cp id=\"10a4\"\u003eThe GraphQL transpilation produces a schema that adheres to the \u003ca href=\"https://spec.graphql.org/October2021/#sec-Overview\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eofficial GraphQL spec\u003c/a\u003e with the ability to generate all GraphQL types defined in the spec. Given that the UDA domain model can be federated, it also supports generating federated graphQL schemas. Below is an example of a transpiled GraphQL schema.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"252b\"\u003eThe Avro transpilation produces a schema that is a Data Mesh flavor of Avro, which includes some customization on top of the \u003ca href=\"https://avro.apache.org/docs/1.12.0/specification/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eofficial Avro spec\u003c/a\u003e. This schema is used to automatically create a Data Mesh source container. Below is an example of a transpiled Avro schema.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"6ead\"\u003e\u003cstrong\u003eProjections can automatically populate data containers. \u003c/strong\u003eSome projections, such as those to GraphQL schemas or Data Mesh sources produce empty containers that require developers to populate the data. This might be creating GraphQL APIs or pushing events onto Data Mesh sources. Conversely, other containers, like Iceberg Tables, are automatically created and populated by UDA. For Iceberg Tables, UDA leverages the Data Mesh platform to automatically create data streams to move data into tables. This process utilizes much of the same infrastructure detailed in this blog post \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059\" target=\"_blank\" data-discover=\"true\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"bf20\"\u003e\u003cstrong\u003eProjections have mappings. \u003c/strong\u003eUDA automatically generates and manages mappings between the newly created data containers and the projected domain model.\u003c/p\u003e\u003ch2 id=\"240f\"\u003eEarly Adopters\u003c/h2\u003e\u003ch2 id=\"76f7\"\u003eControlled Vocabularies (PDM)\u003c/h2\u003e\u003cp id=\"da3d\"\u003eThe full range of Netflix’s business activities relies on a sprawling data model that captures the details of our many business processes. Teams need to be able to coordinate operational activities to ensure that content production is complete, advertising campaigns are in place, and promotional assets are ready to deploy. We implicitly depend upon a singular definition of shared concepts, such as content production is complete. Multiple definitions create coordination challenges. Software (and humans) don’t know that the definitions mean the same thing.\u003c/p\u003e\u003cp id=\"800e\"\u003eWe started the Primary Data Management (PDM) initiative to create unified and consistent definitions for the core concepts in our data model. These definitions form \u003cstrong\u003econtrolled vocabularies\u003c/strong\u003e, standardized and governed lists for what values are permitted within certain fields in our data model.\u003c/p\u003e\u003cp id=\"be83\"\u003e\u003cstrong\u003ePrimary Data Management (PDM) is a single place where business users can manage controlled vocabularies. \u003c/strong\u003eOur data model governance has been scattered across different tools and teams creating coordination challenges. This is an information management problem relating to the definition, maintenance and consistent use of reference data and taxonomies. This problem is not unique to Netflix, so we looked outward for existing solutions to this problem.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eManaging the taxonomy of One Piece characters in PDM.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"3a7b\"\u003e\u003cstrong\u003ePDM uses the Simple Knowledge Organization System (\u003c/strong\u003e\u003ca href=\"https://www.w3.org/TR/skos-primer\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003eSKOS\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e)\u003c/strong\u003e \u003cstrong\u003emodel\u003c/strong\u003e. It is a W3C data standard designed for modeling knowledge. Its terminology is abstract, with Concepts that can be organized into ConceptSchemes and properties to describe various types of relationships. Every system is hardcoded against \u003cem\u003esomething\u003c/em\u003e, that’s how software knows how to manipulate data. We want a system that can work with a data model as its input, so we still need \u003cem\u003esomething\u003c/em\u003e concrete to build the software against. This is what SKOS provides, a generic basis for modeling knowledge that our system can understand.\u003c/p\u003e\u003cp id=\"cd41\"\u003e\u003cstrong\u003ePDM uses Domain Models to integrate SKOS into the rest of Content Engineering’s ecosystem. \u003c/strong\u003eA core premise of the system is that it takes a domain model as input, and everything that \u003cem\u003ecan\u003c/em\u003e be derived \u003cem\u003eis\u003c/em\u003e derived from that model. PDM builds a user interface based upon the model definition and leverages UDA to project this model into type-safe interfaces for other systems to use. The system will provision a Domain Graph Service (DGS) within our federated GraphQL API environment using a GraphQL schema that UDA projects from the domain model. UDA is also used to provision data movement pipelines which are able to feed our \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf\" target=\"_blank\" data-discover=\"true\"\u003eGraphSearch\u003c/a\u003e infrastructure as well as move data into the warehouse. The data movement systems use Avro schemas, and UDA creates a projection from the domain model to Avro.\u003c/p\u003e\u003cp id=\"2309\"\u003e\u003cstrong\u003eConsumers of controlled vocabularies never know they’re using SKOS. \u003c/strong\u003eDomain models use terms that fit in with the domain. SKOS’s generic notion of \u003cem\u003ebroader\u003c/em\u003e and \u003cem\u003enarrower\u003c/em\u003e to define a hierarchy are hidden from consumers as super-properties within the model. This allows consumers to work with language that is familiar to them while enabling PDM to work with any model. The best of both worlds.\u003c/p\u003e\u003ch2 id=\"47a4\"\u003eOperational Reporting (Sphere)\u003c/h2\u003e\u003cp id=\"cb2d\"\u003e\u003cstrong\u003eOperational reporting serves the detailed day-to-day activities and processes of a business domain.\u003c/strong\u003e It is a reporting paradigm specialized in covering high-resolution, low-latency data sets.\u003c/p\u003e\u003cp id=\"1d94\"\u003e\u003cstrong\u003eOperational reporting systems should generate reports without relying on technical intermediaries. \u003c/strong\u003eOperational reporting systems need to address the persistent challenge of empowering business users to explore and obtain the data they need, when they need it. Without such self-service systems, requests for new reports or data extracts often result in back-and-forth exchanges, where the initial query may not exactly meet business users’ expectations, requiring further clarification and refinement.\u003c/p\u003e\u003cp id=\"905e\"\u003e\u003cstrong\u003eData discovery and query generation are two relevant aspects of data integration. \u003c/strong\u003eSupplying end-users with an accurate, contextual, and user-friendly data discovery experience provides a basis for query generation mechanism which produces syntactically correct and semantically reliable queries.\u003c/p\u003e\u003cp id=\"0968\"\u003e\u003cstrong\u003eOperational reports are predominantly run on data hydrated from GraphQL services into the Data Warehouse. \u003c/strong\u003eYou can read about our journey from conventional data movement to streaming data pipelines based on CDC and GraphQL hydration in \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059\" target=\"_blank\" data-discover=\"true\"\u003ethis blog post\u003c/a\u003e. Among the challenging byproducts of this approach was that a single, distinct data concept is now present in two places (GraphQL and data warehouse), with some disparity in semantic context to guide and support the interpretations and connectivity of that data. To address this, we formulate a mechanism to use the syntax and semantics captured in the federated schema from \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2\" target=\"_blank\" data-discover=\"true\"\u003eNetflix’s Enterprise GraphQL\u003c/a\u003e and populate \u003cem\u003erepresentational domain models\u003c/em\u003e in UDA to preserve those details and add more.\u003c/p\u003e\u003cp id=\"28bb\"\u003e\u003cstrong\u003eDomain models enable the data discovery experience. \u003c/strong\u003eMetadata aggregated from various data-producing systems is captured in UDA domain models using a unified vocabulary. This metadata is surfaced for the users’ search and discovery needs; instead of specifying exact tables and join keys, users simply can search for familiar business concepts such as ‘actors’ or ‘movies’. We use UDA models to disambiguate and resolve the intended concepts and their related data entities.\u003c/p\u003e\u003cp id=\"7eee\"\u003e\u003cstrong\u003eUDA knowledge graph is the data landscape for query generation. \u003c/strong\u003eOnce concepts are discovered and their mappings to corresponding data containers are identified and located in the knowledge graph, we use them to establish join strategies. Through graph traversal, we identify \u003cem\u003eboundaries\u003c/em\u003e and \u003cem\u003eislands\u003c/em\u003e within the data landscape. This ensures only feasible, joinable combinations are selected while weeding out semantically incorrect and non-executable query candidates.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eGenerating a report in Sphere.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"107a\"\u003e\u003cstrong\u003eSphere is a UDA-powered self-service operational reporting system. \u003c/strong\u003eThe solution based on knowledge graphs described above is called Sphere. Seeing self-service operational reporting through this lens, we can improve business users’ agency in access to operational data. They are empowered to explore, assemble, and refine reports at the conceptual level, while technical complexities are managed by the system.\u003c/p\u003e\u003ch2 id=\"76ef\"\u003eStay Tuned\u003c/h2\u003e\u003cp id=\"844c\"\u003eUDA marks a fundamental shift in how we approach data modeling within Content Engineering. By providing a unified knowledge graph composed of what we know about our various data systems and the business concepts within them, we’ve made information more consistent, connected, and discoverable across our organization. We’re excited about future applications of these ideas such as:\u003c/p\u003e\u003cul\u003e\u003cli id=\"8106\"\u003eSupporting additional projections like Protobuf/gRPC\u003c/li\u003e\u003cli id=\"9af0\"\u003eMaterializing the knowledge graph of instance data for querying, profiling, and management\u003c/li\u003e\u003cli id=\"ec0a\"\u003eFinally solving some of the initial \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf\" target=\"_blank\" data-discover=\"true\"\u003echallenges\u003c/a\u003e posed by Graph Search (that actually inspired some of this work)\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"2012\"\u003eIf you’re interested in this space, we’d love to connect — whether you’re exploring new roles down the road or just want to swap ideas.\u003c/p\u003e\u003cp id=\"844f\"\u003eExpect to see future blog posts exploring PDM and Sphere in more detail soon!\u003c/p\u003e\u003ch2 id=\"e536\"\u003eCredits\u003c/h2\u003e\u003cp id=\"b15e\"\u003eThanks to \u003ca href=\"https://www.linkedin.com/in/andreaslegenbauer/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAndreas Legenbauer\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/bernardo-g-4414b41/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBernardo Gomez Palacio Valdes\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/czhao/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCharles Zhao\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/christopherchonguw/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eChristopher Chong\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/deepa-krishnan-593b60/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDeepa Krishnan\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/gpesma/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGeorge Pesmazoglou\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/jsilvax/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJessica Silva\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/katherine-anderson-77074159/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKatherine Anderson\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/malikday/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMalik Day\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/ritabogdanovashapkina/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRita Bogdanova\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/ruoyunzheng/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRuoyun Zheng\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/shawn-s-b80821b0/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eShawn Stedman\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/suchitagoyal/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSuchita Goyal\u003c/a\u003e, \u003ca href=\"http://www.linkedin.com/in/utkarshshrivastava/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eUtkarsh Shrivastava\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/yoomikoh/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eYoomi Koh\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/yuliashmeleva/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eYulia Shmeleva\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "25 min read",
  "publishedTime": "2025-06-12T14:48:32.694Z",
  "modifiedTime": null
}
