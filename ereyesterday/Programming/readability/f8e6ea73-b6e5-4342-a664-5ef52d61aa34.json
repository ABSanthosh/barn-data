{
  "id": "f8e6ea73-b6e5-4342-a664-5ef52d61aa34",
  "title": "How a Software Architect Uses Artificial Intelligence in His Daily Work",
  "link": "https://www.infoq.com/news/2025/02/software-architect-AI-LLM/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Software architects and system architects will not be replaced anytime soon by generative artificial intelligence (AI) or large language models (LLMs), Avraham Poupko said. They will be replaced by software architects who know how to leverage generative AI and LLMs, and just as importantly, know how NOT to use generative AI. By Ben Linders",
  "author": "Ben Linders",
  "published": "Thu, 20 Feb 2025 11:07:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Requirements",
    "Stories \u0026 Case Studies",
    "Artifacts \u0026 Tools",
    "Artificial Intelligence",
    "Agile Conferences",
    "Large language models",
    "Architecture \u0026 Design",
    "Culture \u0026 Methods",
    "news"
  ],
  "byline": "Ben Linders",
  "length": 4059,
  "excerpt": "Software architects and system architects will not be replaced anytime soon by generative artificial intelligence (AI) or large language models (LLMs), Avraham Poupko said. They will be replaced by so",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250213201535/apple-touch-icon.png",
  "text": "Software architects and system architects will not be replaced anytime soon by generative artificial intelligence (AI) or large language models (LLMs), Avraham Poupko said. They will be replaced by software architects who know how to leverage generative AI and LLMs, and just as importantly, know how NOT to use generative AI. Avraham Poupko gave a talk about how he uses artificial intelligence in his daily work as an architect at OOP conference. The word LLM means Large Language Model. Poupko argued that the question of how humans and machines differ is a fundamental question, and understanding it is critical to the understanding of LLMs. Humans do not really have a language model, they have a world model, Poupko said. Humans have an understanding of the world that consists of an understanding of how the objects in the world behave, and how they interact with each other. That world model is the result of many experiences and interactions, Poupko explained: When we use spoken and written language to communicate about the world, that is only one representation of the world model and a very limited representation at that. LLMs on the other hand only have a word or language model, Poupko mentioned. The LLMs only know how words relate to each other. While that model often does give an astonishing illusion of understanding and comprehension, it is not real understanding. It is just a sequence of words, he said. While it is true that a great deal of world knowledge can be captured in texts and in words, other parts of our world knowledge and world understanding are based on experience and cannot be properly captured in words, Poupko said. This is particularly true with situations that are highly contextual and where the person present is aware of the context, but where not all the context is verbal, he added. Architects and large language models can work together to create better software architecture, Poupko said, which is somewhat similar to how humans and books can work together: Humans will use books to learn and understand. A human will decide if the case at hand is similar to the case outlined in the book, and if so will apply the knowledge learned from the book in a context-appropriate way. That is exactly what we do with an LLM, Poupko stated. We give it a prompt or series of prompts and receive a response. The response is usually neither correct nor incorrect, rather the response is either useful or not useful (as the famous George Box quote goes \"All models are wrong. Some are useful\"), he mentioned. When we say useful, we mean useful to humans. It is the human that gets to decide if indeed the model is useful and in what context, and then decides to apply it, he mentioned. Poupko mentioned that that AI is most useful in tasks that involve written language. A case where he often uses LLMs is when there is a need to read a requirements document and discover ambiguities, i.e. those cases where a single requirement can mean multiple things. In the talk, he gave an example where an online system had the requirement: The system should be able to handle a large number of users. When asked to detect ambiguities, the LLM he was working with, detected two ambiguities: The term \"a large number of users\" is not useful as it is not specific enough. How many users is a \"large number\"? 100? 1,000,000? The term \"a large number of users\" can either mean a database that supports a large number of users that are registered to the system, or it can mean a large number of concurrent users. It might of course mean both. Next Poupko used the LLM to explore what information was needed in order to resolve these ambiguities. AI does not do design work for me, Poupko said. The system knowledge, the domain knowledge, and the organizational knowledge needed to do effective architecture are such that AI can’t replace me, he concluded. About the Author Ben Linders",
  "image": "https://cdn.infoq.com/statics_s2_20250213201535/styles/static/images/logo/logo-big.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eSoftware architects and system architects will not be replaced anytime soon by generative artificial intelligence (AI) or large language models (LLMs), \u003ca href=\"https://www.linkedin.com/in/avrahampoupko/\"\u003eAvraham Poupko\u003c/a\u003e said. They will be replaced by software architects who know how to leverage generative AI and LLMs, and just as importantly, know how NOT to use generative AI.\u003c/p\u003e\n\n\u003cp\u003eAvraham Poupko gave a talk about how he uses artificial intelligence in his daily work as an architect at \u003ca href=\"https://www.oop-konferenz.de/en\"\u003eOOP conference\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe word LLM means Large Language Model. Poupko argued that the question of how humans and machines differ is a fundamental question, and understanding it is critical to the understanding of LLMs.\u003c/p\u003e\n\n\u003cp\u003eHumans do not really have a language model, they have a world model, Poupko said. Humans have an understanding of the world that consists of an understanding of how the objects in the world behave, and how they interact with each other. That world model is the result of many experiences and interactions, Poupko explained:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhen we use spoken and written language to communicate about the world, that is only one representation of the world model and a very limited representation at that.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eLLMs on the other hand only have a word or language model, Poupko mentioned. The LLMs only know how words relate to each other. While that model often does give an astonishing illusion of understanding and comprehension, it is not real understanding. It is just a sequence of words, he said.\u003c/p\u003e\n\n\u003cp\u003eWhile it is true that a great deal of world knowledge can be captured in texts and in words, other parts of our world knowledge and world understanding are based on experience and cannot be properly captured in words, Poupko said. This is particularly true with situations that are highly contextual and where the person present is aware of the context, but where not all the context is verbal, he added.\u003c/p\u003e\n\n\u003cp\u003eArchitects and large language models can work together to create better software architecture, Poupko said, which is somewhat similar to how humans and books can work together:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eHumans will use books to learn and understand. A human will decide if the case at hand is similar to the case outlined in the book, and if so will apply the knowledge learned from the book in a context-appropriate way.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThat is exactly what we do with an LLM, Poupko stated. We give it a prompt or series of prompts and receive a response. The response is usually neither correct nor incorrect, rather the response is either useful or not useful (as the famous George Box quote goes \u0026#34;All models are wrong. Some are useful\u0026#34;), he mentioned. When we say useful, we mean useful to humans\u003cstrong\u003e.\u003c/strong\u003e It is the human that gets to decide if indeed the model is useful and in what context, and then decides to apply it, he mentioned.\u003c/p\u003e\n\n\u003cp\u003ePoupko mentioned that that AI is most useful in tasks that involve written language. A case where he often uses LLMs is when there is a need to read a requirements document and discover ambiguities, i.e. those cases where a single requirement can mean multiple things.\u003c/p\u003e\n\n\u003cp\u003eIn the talk, he gave an example where an online system had the requirement:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe system should be able to handle a large number of users.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWhen asked to detect ambiguities, the LLM he was working with, detected two ambiguities:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003col\u003e\n\t\u003cli\u003eThe term \u0026#34;a \u003cstrong\u003elarge number\u003c/strong\u003e of users\u0026#34; is not useful as it is not specific enough. How many users is a \u0026#34;large number\u0026#34;? 100? 1,000,000?\u003c/li\u003e\n\t\u003cli\u003eThe term \u0026#34;a large number of \u003cstrong\u003eusers\u003c/strong\u003e\u0026#34; can either mean a database that supports a large number of users that are \u003cstrong\u003eregistered to the system\u003c/strong\u003e, or it can mean a large number of \u003cstrong\u003e\u003cem\u003econcurrent\u003c/em\u003e\u003c/strong\u003e users. It might of course mean both.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eNext Poupko used the LLM to explore what information was needed in order to resolve these ambiguities.\u003c/p\u003e\n\n\u003cp\u003eAI does not do design work for me, Poupko said. The system knowledge, the domain knowledge, and the organizational knowledge needed to do effective architecture are such that AI can’t replace me, he concluded.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Ben-Linders\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eBen Linders\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-02-20T00:00:00Z",
  "modifiedTime": null
}
