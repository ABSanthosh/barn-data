{
  "id": "90647f97-70ac-4f3e-b3d9-8d83d9a04b1c",
  "title": "Multilingual innovation in LLMs: How open models help unlock global communication",
  "link": "https://developers.googleblog.com/en/unlock-global-communication-gemma-projects/",
  "description": "Developers adapt LLMs like Gemma for diverse languages and cultural contexts, demonstrating AI's potential to bridge global communication gaps by addressing challenges like translating ancient texts, localizing mathematical understanding, and enhancing cultural sensitivity in lyric translation.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Glenn Cameron",
  "length": 7968,
  "excerpt": "Developers adapt LLMs like Gemma for diverse languages and cultural contexts, demonstrating AI's potential to bridge global communication gaps by addressing challenges like translating ancient texts, localizing mathematical understanding, and enhancing cultural sensitivity in lyric translation.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "We are thrilled to celebrate the incredible contributions of the community to the Unlock Global Communication with Gemma competition on Kaggle! Developers tackled the critical challenge in AI of adapting state-of-the-art large language models (LLMs) for diverse cultural and linguistic contexts.Models often exhibit a bias towards high-resource languages due to the predominant language of their training and evaluation datasets. This can lead to a performance gap, where the latest AI advancements may not be realized in lower-resourced languages. Additionally, these models may not only lack understanding of the language, but also culturally-relevant context that would make these models helpful for the communities.We were incredibly impressed by the community's creative solutions for translation of languages, lyrics, old texts, and more.Honoring the innovatorsThrough hundreds of submissions, developers demonstrated how to bring the transformative power of LLMs to languages everywhere. Projects leveraged custom datasets and efficient post-training methods to adapt Gemma for instruction following, translation, and specific domains. We encourage you to explore the notebooks on Kaggle to see these techniques in action and apply them to your own multilingual projects.Gemma 2 SwahiliThe first place project adapted Gemma for Swahili understanding, opening up new possibilities to reach 200+ million language speakers. Gemma models were fine-tuned using parameter-efficient fine-tuning techniques for the 2B, 9B, and 27B parameter sizes.A key aspect of their tuning was Gemma’s “remarkable flexibility in instruction-response formatting,” which allowed the models to parse instructions with minimal structural constraints and generate coherent responses across different input formats.Kyara: Retrieval Augmentation for LLM Fine-TuningKnowledge Yielding Adaptive Retrieval Augmentation (Kyara) explored retrieval processes for LLM fine-tuning, demonstrating how to enhance Gemma’s ability to generate informed responses in Traditional Chinese.The project focused on building high-quality question \u0026 answer (Q\u0026A) datasets using a graph-based approach to knowledge retrieval, inspired on how humans learn by connecting concepts.ArGemma: Fine-Tuning Gemma for ArabicThe project fine-tuned Gemma for Arabic language tasks, including translation, summarization, storytelling, and dialogue generation.As a language with a rich historical past, the project also aimed to enhance comprehension of older forms of Arabic used in literary texts and art, employing multiple techniques to bridge tasks between Modern Standard Arabic and Classical Arabic.Post-Training Gemma for Italian and beyondThis project focused on improving Italian language understanding for Gemma using a cost-effective post-training approach that addresses pitfalls such as hallucinations and catastrophic forgetting.The 2B and 9B model sizes were fine-tuned on a mix of data, including a new instruction tuning dataset created using LLM-as-a-judge to ensure the quality of translations.Ancient Chinese Expert: Gemma 2\u003eChatGPTThis project developed an “Ancient Chinese Expert” using Gemma to understand and generate translations for ancient Chinese texts, highlighting the potential of LLMs for historical cultural preservation.The model was fine-tuned on a comprehensive dataset to improve linguistic understanding, and post-training included techniques to improve instruction following.Lyric-Gemma 2: One Song, Different StoriesThis project tackled nuanced challenges specific to AI-driven lyric translation, enhancing Gemma’s sensitivity to cultural references and symbolic language, while also ensuring rhythmic fidelity to the original song.A multilingual dataset contained lyric translations annotated to capture crucial cultural context, emotional tone, and rhythmic features, enabling the model to grasp and replicate the artistic depth of lyrical content.Fine-tuning Gemma 2 JPN for YomiganaThis project adapted Gemma 2 JPN to generate Yomigana/Furigana, a reading aid for Japanese text and assist language learners or readers encountering complex Kanji.While other rule-based tools currently exist, LLMs can recognize rare Kanji better and “interpret the context of a sentence, enabling accurate disambiguation of polyphonic Kanji”. The notebook also noted that conversational capabilities had degraded due to training on the singular translation task.Mathematical Minds: Fine-tuning Gemma 2 for HindiThis project enhances Gemma’s mathematical and logical understanding in Hindi numeric words, which presents a challenge for models to interpret given complex word formations, for example “दो सौ” for “200” or “ढाई” for “2.5”.The 9B model was fine-tuned on a curated and human expert-verified dataset featuring a wide array of question types, unlocking uses for AI-driven educational tools, automated tutoring, and localized contentGemma-2-9b-kk-it: Learning to translate KazakhThis project fine-tuned the Gemma 2 9B model for translation tasks in Kazakh. A language written in three distinct scripts (Cyrillic, Latin, and Arabic), the Cyrillic version requires approximately twice as many tokens as English, presenting a challenge for training with limited resources.Model performance showed better benchmarks than the 27B Gemma variant and Google Translate, demonstrating how to adapt LLMs for underrepresented languages using a cost-effective approach.THEODEN: The Old English GemmaThis project enables Gemma to understand and translate Old English, the earliest recorded form of the English language. A custom dataset with Old English-Modern English language pairs was created to help tackle the challenge of working with historical languages and limited publicly available data.The notebook also features a bonus audio generation component, based on an open-source Icelandic text-to-speech model, offering an approximation of how speech might have sounded.10 more awesome projectsGemma 2 Reasoning for Japanese Math: This project created reasoning variants to perform chain-of-thought processes and handle complex problems.Multitask Gemma2 Agents - Summarise \u0026 Translate: This project focused on developing agents capable of multiple tasks.Korean AI Doctor Gemma2: This project adapted Gemma for medical applications in Korean.Gemma Fine-Tuning for Ru-En Medical Translations: This project enhanced Gemma translation accuracy in ophthalmology.Gemma PT: This project fine-tuned the ShieldGemma content classifier to detect prejudice and disinformation in Portuguese.How to Fine-tune Gemma 2 for Advanced Reasoning: This project enhanced Gemma reasoning capabilities by implementing the Coconut (Chain of Continuous Thought) paradigm.Finetune Gemma Turkish Chat: This project fine-tuned on Gemma on a Q\u0026A dataset to improve accuracy and conversational ability.Finetuning Gemma2 Customized Dataset: This project fine-tuned Gemma for English-Arabic translation and medical understanding.Gemma-2 Finetuning on Telugu News Dataset: This project adapted Gemma to generate Telugu headlines from news articles.Finetuned Gemma2 9B Math Reasoning Model Russian: This project enhanced Gemma performance for math problems in Russian.Looking ahead with Gemma 3With over 7,000 languages spoken worldwide, the potential for AI to bridge communication gaps is immense. The Gemma open model family provides a powerful foundation for developers to adapt high-performing models to low-resource languages.The innovation and dedication demonstrated by the Kaggle community in adapting Gemma 2 for various languages are truly inspiring. As we continue to build a future where AI empowers global communication for everyone, we're excited for Gemma 3, which brings pretrained support for over 140 languages, making it a great foundation to build on.We encourage developers to explore the possibilities of Gemma, to share their datasets and models with others, and continue to advance multilingual AI together.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Kaggle-Competition-Metacard-1600x.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n    \u003cp data-block-key=\"3oxel\"\u003eWe are thrilled to celebrate the incredible contributions of the community to the \u003ca href=\"https://www.kaggle.com/c/gemma-language-tuning\"\u003eUnlock Global Communication with Gemma\u003c/a\u003e competition on Kaggle! Developers tackled the critical challenge in AI of adapting state-of-the-art large language models (LLMs) for diverse cultural and linguistic contexts.\u003c/p\u003e\u003cp data-block-key=\"e9nec\"\u003eModels often exhibit a bias towards high-resource languages due to the predominant language of their training and evaluation datasets. This can lead to a performance gap, where the latest AI advancements may not be realized in lower-resourced languages. Additionally, these models may not only lack understanding of the language, but also culturally-relevant context that would make these models helpful for the communities.\u003c/p\u003e\u003cp data-block-key=\"2vaag\"\u003eWe were incredibly impressed by the community\u0026#39;s creative solutions for translation of languages, lyrics, old texts, and more.\u003c/p\u003e\u003ch2 data-block-key=\"1a1el\" id=\"honoring-the-innovators\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eHonoring the innovators\u003c/h2\u003e\u003cp data-block-key=\"7vqj8\"\u003eThrough hundreds of submissions, developers demonstrated how to bring the transformative power of LLMs to languages everywhere. Projects leveraged custom datasets and efficient post-training methods to adapt \u003ca href=\"http://deepmind.google/models/gemma\"\u003eGemma\u003c/a\u003e for instruction following, translation, and specific domains. We encourage you to \u003ca href=\"https://www.kaggle.com/competitions/gemma-language-tuning/code\"\u003eexplore the notebooks on Kaggle\u003c/a\u003e to see these techniques in action and apply them to your own multilingual projects.\u003c/p\u003e\u003ch3 data-block-key=\"kkps0\" id=\"gemma-2-swahili\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://www.kaggle.com/code/alfaxadeyembe/introducing-gemma-2-swahili\"\u003e\u003cb\u003eGemma 2 Swahili\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"ch14m\"\u003eThe first place project adapted Gemma for Swahili understanding, opening up new possibilities to reach 200+ million language speakers. Gemma models were fine-tuned using parameter-efficient fine-tuning techniques for the 2B, 9B, and 27B parameter sizes.\u003c/p\u003e\u003cp data-block-key=\"3topq\"\u003eA key aspect of their tuning was Gemma’s “remarkable flexibility in instruction-response formatting,” which allowed the models to parse instructions with minimal structural constraints and generate coherent responses across different input formats.\u003c/p\u003e\u003ch3 data-block-key=\"qqx5l\" id=\"kyara:-retrieval-augmentation-for-llm-fine-tuning\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://www.kaggle.com/code/zake7749/kyara-retrieval-augmentation-for-llm-fine-tuning\"\u003e\u003cb\u003eKyara: Retrieval Augmentation for LLM Fine-Tuning\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"fui7\"\u003e\u003cb\u003eK\u003c/b\u003enowledge \u003cb\u003eY\u003c/b\u003eielding \u003cb\u003eA\u003c/b\u003edaptive \u003cb\u003eR\u003c/b\u003eetrieval \u003cb\u003eA\u003c/b\u003eugmentation (Kyara) explored retrieval processes for LLM fine-tuning, demonstrating how to enhance Gemma’s ability to generate informed responses in Traditional Chinese.\u003c/p\u003e\u003cp data-block-key=\"3ssvc\"\u003eThe project focused on building high-quality question \u0026amp; answer (Q\u0026amp;A) datasets using a graph-based approach to knowledge retrieval, inspired on how humans learn by connecting concepts.\u003c/p\u003e\u003ch3 data-block-key=\"028p4\" id=\"argemma:-fine-tuning-gemma-for-arabic\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://www.kaggle.com/code/tahaalselwii/fine-tuning-gemma-for-arabic-argemma\"\u003e\u003cb\u003eArGemma: Fine-Tuning Gemma for Arabic\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"9uu2k\"\u003eThe project fine-tuned Gemma for Arabic language tasks, including translation, summarization, storytelling, and dialogue generation.\u003c/p\u003e\u003cp data-block-key=\"ffb35\"\u003eAs a language with a rich historical past, the project also aimed to enhance comprehension of older forms of Arabic used in literary texts and art, employing multiple techniques to bridge tasks between Modern Standard Arabic and Classical Arabic.\u003c/p\u003e\u003ch3 data-block-key=\"uwj1f\" id=\"post-training-gemma-for-italian-and-beyond\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://www.kaggle.com/code/anakin87/post-training-gemma-for-italian-and-beyond\"\u003e\u003cb\u003ePost-Training Gemma for Italian and beyond\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"akgej\"\u003eThis project focused on improving Italian language understanding for Gemma using a cost-effective post-training approach that addresses pitfalls such as hallucinations and catastrophic forgetting.\u003c/p\u003e\u003cp data-block-key=\"5ea3g\"\u003eThe 2B and 9B model sizes were fine-tuned on a mix of data, including a new instruction tuning dataset created using LLM-as-a-judge to ensure the quality of translations.\u003c/p\u003e\u003ch3 data-block-key=\"mm1pw\" id=\"ancient-chinese-expert:-gemma-2greaterchatgpt\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://www.kaggle.com/code/judith007/ancient-chinese-expert-gemma2-chatgpt/notebook?scriptVersionId=216814746\"\u003e\u003cb\u003eAncient Chinese Expert: Gemma 2\u0026gt;ChatGPT\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"949ao\"\u003eThis project developed an “Ancient Chinese Expert” using Gemma to understand and generate translations for ancient Chinese texts, highlighting the potential of LLMs for historical cultural preservation.\u003c/p\u003e\u003cp data-block-key=\"fqth1\"\u003eThe model was fine-tuned on a comprehensive dataset to improve linguistic understanding, and post-training included techniques to improve instruction following.\u003c/p\u003e\u003ch3 data-block-key=\"1ubsz\" id=\"lyric-gemma-2:-one-song-different-stories\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://www.kaggle.com/code/ansehen/lyric-gemma-2-one-song-different-stories\"\u003e\u003cb\u003eLyric-Gemma 2: One Song, Different Stories\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"2kq3m\"\u003eThis project tackled nuanced challenges specific to AI-driven lyric translation, enhancing Gemma’s sensitivity to cultural references and symbolic language, while also ensuring rhythmic fidelity to the original song.\u003c/p\u003e\u003cp data-block-key=\"7mt15\"\u003eA multilingual dataset contained lyric translations annotated to capture crucial cultural context, emotional tone, and rhythmic features, enabling the model to grasp and replicate the artistic depth of lyrical content.\u003c/p\u003e\u003ch3 data-block-key=\"m6cit\" id=\"fine-tuning-gemma-2-jpn-for-yomigana\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://www.kaggle.com/code/iamleonie/fine-tuning-gemma-2-jpn-for-yomigana-with-lora\"\u003e\u003cb\u003eFine-tuning Gemma 2 JPN for Yomigana\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"32uqh\"\u003eThis project adapted Gemma 2 JPN to generate Yomigana/Furigana, a reading aid for Japanese text and assist language learners or readers encountering complex Kanji.\u003c/p\u003e\u003cp data-block-key=\"ftjeu\"\u003eWhile other rule-based tools currently exist, LLMs can recognize rare Kanji better and “interpret the context of a sentence, enabling accurate disambiguation of polyphonic Kanji”. The notebook also noted that conversational capabilities had degraded due to training on the singular translation task.\u003c/p\u003e\u003ch3 data-block-key=\"pje49\" id=\"mathematical-minds:-fine-tuning-gemma-2-for-hindi\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://www.kaggle.com/code/dnyaneshwalwadkar/mathematical-minds-fine-tuning-gemma-2-for-hindi\"\u003e\u003cb\u003eMathematical Minds: Fine-tuning Gemma 2 for Hindi\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"2880a\"\u003eThis project enhances Gemma’s mathematical and logical understanding in Hindi numeric words, which presents a challenge for models to interpret given complex word formations, for example “दो सौ” for “200” or “ढाई” for “2.5”.\u003c/p\u003e\u003cp data-block-key=\"6iqk7\"\u003eThe 9B model was fine-tuned on a curated and human expert-verified dataset featuring a wide array of question types, unlocking uses for AI-driven educational tools, automated tutoring, and localized content\u003c/p\u003e\u003ch3 data-block-key=\"io1bi\" id=\"gemma-2-9b-kk-it:-learning-to-translate-kazakh\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://www.kaggle.com/code/spacedoge/gemma-2-9b-kk-it-learning-to-translate-kazakh\"\u003e\u003cb\u003eGemma-2-9b-kk-it: Learning to translate Kazakh\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"i3v3\"\u003eThis project fine-tuned the Gemma 2 9B model for translation tasks in Kazakh. A language written in three distinct scripts (Cyrillic, Latin, and Arabic), the Cyrillic version requires approximately twice as many tokens as English, presenting a challenge for training with limited resources.\u003c/p\u003e\u003cp data-block-key=\"cs1rs\"\u003eModel performance showed better benchmarks than the 27B Gemma variant and Google Translate, demonstrating how to adapt LLMs for underrepresented languages using a cost-effective approach.\u003c/p\u003e\u003ch3 data-block-key=\"fn7yu\" id=\"theoden:-the-old-english-gemma\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://www.kaggle.com/code/alejopaullier/theoden-the-old-english-gemma\"\u003e\u003cb\u003eTHEODEN: The Old English Gemma\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"8978h\"\u003eThis project enables Gemma to understand and translate Old English, the earliest recorded form of the English language. A custom dataset with Old English-Modern English language pairs was created to help tackle the challenge of working with historical languages and limited publicly available data.\u003c/p\u003e\u003cp data-block-key=\"3g5n2\"\u003eThe notebook also features a bonus audio generation component, based on an open-source Icelandic text-to-speech model, offering an approximation of how speech might have sounded.\u003c/p\u003e\u003ch2 data-block-key=\"qki1v\" id=\"10-more-awesome-projects\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e10 more awesome projects\u003c/h2\u003e\u003cul\u003e\u003cli data-block-key=\"3io38\"\u003e\u003ca href=\"https://www.kaggle.com/code/inoueu1/gemma-2-reasoning-for-japanese-math\"\u003eGemma 2 Reasoning for Japanese Math\u003c/a\u003e: This project created reasoning variants to perform chain-of-thought processes and handle complex problems.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"4fgb2\"\u003e\u003ca href=\"https://www.kaggle.com/code/dretii/multitask-gemma2-agents-summarise-translate\"\u003eMultitask Gemma2 Agents - Summarise \u0026amp; Translate\u003c/a\u003e: This project focused on developing agents capable of multiple tasks.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"6ppm\"\u003e\u003ca href=\"https://www.kaggle.com/code/koohack/korean-ai-doctor-gemma2\"\u003eKorean AI Doctor Gemma2\u003c/a\u003e: This project adapted Gemma for medical applications in Korean.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"9lm3j\"\u003e\u003ca href=\"https://www.kaggle.com/code/cheshrcat/gemma-fine-tuning-for-ru-en-medical-translations\"\u003eGemma Fine-Tuning for Ru-En Medical Translations\u003c/a\u003e: This project enhanced Gemma translation accuracy in ophthalmology.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"e9l1e\"\u003e\u003ca href=\"https://www.kaggle.com/code/fernandosr85/gemma-pt?scriptVersionId=200113673\"\u003eGemma PT\u003c/a\u003e: This project fine-tuned the ShieldGemma content classifier to detect prejudice and disinformation in Portuguese.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"djikr\"\u003e\u003ca href=\"https://www.kaggle.com/code/victorumesiobi/how-to-fine-tune-gemma-2-for-advanced-reasoning\"\u003eHow to Fine-tune Gemma 2 for Advanced Reasoning\u003c/a\u003e: This project enhanced Gemma reasoning capabilities by implementing the Coconut (Chain of Continuous Thought) paradigm.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"bik15\"\u003e\u003ca href=\"https://www.kaggle.com/code/tosrsa/finetune-gemma-turkish-chat\"\u003eFinetune Gemma Turkish Chat\u003c/a\u003e: This project fine-tuned on Gemma on a Q\u0026amp;A dataset to improve accuracy and conversational ability.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"3mj9k\"\u003e\u003ca href=\"https://www.kaggle.com/code/jackren000/finetuninggemma2-customizeddataset\"\u003eFinetuning Gemma2 Customized Dataset\u003c/a\u003e: This project fine-tuned Gemma for English-Arabic translation and medical understanding.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"38ind\"\u003e\u003ca href=\"https://www.kaggle.com/code/saidineshpola/gemma-2-finetuning-on-telugu-news-mi-with-sae\"\u003eGemma-2 Finetuning on Telugu News Dataset\u003c/a\u003e: This project adapted Gemma to generate Telugu headlines from news articles.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ep9ah\"\u003e\u003ca href=\"https://www.kaggle.com/code/lhagiimn/finetuned-gemma2-9b-math-reasoning-model-russian\"\u003eFinetuned Gemma2 9B Math Reasoning Model Russian\u003c/a\u003e: This project enhanced Gemma performance for math problems in Russian.\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"3nls1\" id=\"looking-ahead-with-gemma-3\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLooking ahead with Gemma 3\u003c/h2\u003e\u003cp data-block-key=\"1cj2c\"\u003eWith over 7,000 languages spoken worldwide, the potential for AI to bridge communication gaps is immense. The Gemma open model family provides a powerful foundation for developers to adapt high-performing models to low-resource languages.\u003c/p\u003e\u003cp data-block-key=\"9kpkg\"\u003eThe innovation and dedication demonstrated by the Kaggle community in adapting Gemma 2 for various languages are truly inspiring. As we continue to build a future where AI empowers global communication for everyone, we\u0026#39;re excited for \u003ca href=\"https://blog.google/technology/developers/gemma-3/\"\u003eGemma 3\u003c/a\u003e, which brings pretrained support for over 140 languages, making it a great foundation to build on.\u003c/p\u003e\u003cp data-block-key=\"eh5un\"\u003eWe encourage developers to explore the possibilities of Gemma, to share their datasets and models with others, and continue to advance multilingual AI together.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2025-06-23T00:00:00Z",
  "modifiedTime": null
}
