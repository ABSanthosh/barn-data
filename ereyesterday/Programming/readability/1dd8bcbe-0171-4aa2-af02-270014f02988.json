{
  "id": "1dd8bcbe-0171-4aa2-af02-270014f02988",
  "title": "Coding Assistants Threaten the Software Supply Chain",
  "link": "https://martinfowler.com/articles/exploring-gen-ai/software-supply-chain-attack-surface.html",
  "description": "",
  "author": "",
  "published": "2025-05-13T09:11:00-04:00",
  "source": "https://martinfowler.com/feed.atom",
  "categories": null,
  "byline": "Jim Gumbley Lilly Ryan",
  "length": 6743,
  "excerpt": "Notes from my Thoughtworks colleagues on AI-assisted software delivery",
  "siteName": "martinfowler.com",
  "favicon": "",
  "text": "We have long recognized that developer environments represent a weak point in the software supply chain. Developers, by necessity, operate with elevated privileges and a lot of freedom, integrating diverse components directly into production systems. As a result, any malicious code introduced at this stage can have a broad and significant impact radius particularly with sensitive data and services. The introduction of agentic coding assistants (such as Cursor, Windsurf, Cline, and lately also GitHub Copilot) introduces new dimensions to this landscape. These tools operate not merely as suggestive code generators but actively interact with developer environments through tool-use and Reasoning-Action (ReAct) loops. Coding assistants introduce new components and vulnerabilities to the software supply chain, but can also be owned or compromised themselves in novel and intriguing ways. Understanding the Agent Loop Attack Surface A compromised MCP server, rules file or even a code or dependency has the scope to feed manipulated instructions or commands that the agent executes. This isn't just a minor detail – as it increases the attack surface compared to more traditional development practices, or AI-suggestion based systems. Figure 1: CD pipeline, emphasizing how instructions and code move between these layers. It also highlights supply chain elements where poisoning can happen, as well as key elements of escalation of privilege Each step of the agent flow introduces risk: Context Poisoning: Malicious responses from external tools or APIs can trigger unintended behaviors within the assistant, amplifying malicious instructions through feedback loops. Escalation of privilege: A compromised assistant, particularly if lightly supervised, can execute deceptive or harmful commands directly via the assistant’s execution flow. This complex, iterative environment creates a fertile ground for subtle yet powerful attacks, significantly expanding traditional threat models. Traditional monitoring tools might struggle to identify malicious activity as malicious activity or subtle data leakage will be harder to spot when embedded within complex, iterative conversations between components, as the tools are new and unknown and still developing at a rapid pace. New weak spots: MCP and Rules Files The introduction of MCP servers and rules files create openings for context poisoning—where malicious inputs or altered states can silently propagate through the session, enabling command injection, tampered outputs, or supply chain attacks via compromised code. Model Context Protocol (MCP) acts as a flexible, modular interface enabling agents to connect with external tools and data sources, maintain persistent sessions, and share context across workflows. However, as has been highlighted elsewhere, MCP fundamentally lacks built-in security features like authentication, context encryption, or tool integrity verification by default. This absence can leave developers exposed. Rules Files, such as for example “cursor rules”, consist of predefined prompts, constraints, and pointers that guide the agent's behavior within its loop. They enhance stability and reliability by compensating for the limitations of LLM reasoning—constraining the agent's possible actions, defining error handling procedures, and ensuring focus on the task. While designed to improve predictability and efficiency, these rules represent another layer where malicious prompts can be injected. Tool-calling and privilege escalation Coding assistants go beyond LLM generated code suggestions to operate with tool-use via function calling. For example, given any given coding task, the assistant may execute commands, read and modify files, install dependencies, and even call external APIs. The threat of privilege escalation is an emerging risk with agentic coding assistants. Malicious instructions, can prompt the assistant to: Execute arbitrary system commands. Modify critical configuration or source code files. Introduce or propagate compromised dependencies. Given the developer's typically elevated local privileges, a compromised assistant can pivot from the local environment to broader production systems or the kinds of sensitive infrastructure usually accessible by software developers in organisations. What can you do to safeguard security with coding agents? Coding assistants are pretty new and emerging as of when this was published. But some themes in appropriate security measures are starting to emerge, and many of them represent very traditional best practices. Sandboxing and Least Privilege Access control: Take care to limit the privileges granted to coding assistants. Restrictive sandbox environments can limit the blast radius. Supply Chain scrutiny: Carefully vet your MCP Servers and Rules Files as critical supply chain components just as you would with library and framework dependencies. Monitoring and observability: Implement logging and auditing of file system changes initiated by the agent, network calls to MCP servers, dependency modifications etc. Explicitly include coding assistant workflows and external interactions in your threat modeling exercises. Consider potential attack vectors introduced by the assistant. Human in the loop: The scope for malicious action increases dramatically when you auto accept changes. Don’t become over reliant on the LLM The final point is particularly salient. Rapid code generation by AI can lead to approval fatigue, where developers implicitly trust AI outputs without understanding or verifying. Overconfidence in automated processes, or “vibe coding,” heightens the risk of inadvertently introducing vulnerabilities. Cultivating vigilance, good coding hygiene, and a culture of conscientious custodianship remain really important in professional software teams that ship production software. Agentic coding assistants can undeniably provide a boost. However, the enhanced capabilities come with significantly expanded security implications. By clearly understanding these new risks and diligently applying consistent, adaptive security controls, developers and organizations can better hope to safeguard against emerging threats in the evolving AI-assisted software landscape.",
  "image": "https://martinfowler.com/articles/exploring-gen-ai/donkey-card.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cp\u003eWe have long recognized that developer environments represent a weak\n    point in the software supply chain. Developers, by necessity, operate with\n    elevated privileges and a lot of freedom, integrating diverse components\n    directly into production systems. As a result, any malicious code introduced\n    at this stage can have a broad and significant impact radius particularly\n    with sensitive data and services.\u003c/p\u003e\n\n\u003cp\u003eThe introduction of agentic coding assistants (such as Cursor, Windsurf,\n    Cline, and lately also GitHub Copilot) introduces new dimensions to this\n    landscape. These tools operate not merely as suggestive code generators but\n    actively interact with developer environments through tool-use and\n    Reasoning-Action (ReAct) loops. Coding assistants introduce new components\n    and vulnerabilities to the software supply chain, but can also be owned or\n    compromised themselves in novel and intriguing ways.\u003c/p\u003e\n\n\u003csection id=\"UnderstandingTheAgentLoopAttackSurface\"\u003e\n\u003ch2\u003eUnderstanding the Agent Loop Attack Surface\u003c/h2\u003e\n\n\u003cp\u003eA compromised MCP server, rules file or even a code or dependency has the\n    scope to feed manipulated instructions or commands that the agent executes.\n    This isn\u0026#39;t just a minor detail – as it increases the attack surface compared\n    to more traditional development practices, or AI-suggestion based systems.\n    \u003c/p\u003e\n\n\u003cdiv id=\"supply-chain-attack-surface.png\"\u003e\u003cp\u003e\u003cimg src=\"https://martinfowler.com/articles/exploring-gen-ai/supply-chain-attack-surface.png\" width=\"900\"/\u003e\u003c/p\u003e\u003cp\u003eFigure 1: CD pipeline, emphasizing how\n    instructions and code move between these layers. It also highlights supply\n    chain elements where poisoning can happen, as well as key elements of\n    escalation of privilege\u003c/p\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eEach step of the agent flow introduces risk:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eContext Poisoning\u003c/b\u003e: Malicious responses from external tools or APIs\n      can trigger unintended behaviors within the assistant, amplifying malicious\n      instructions through feedback loops.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eEscalation of privilege\u003c/b\u003e: A compromised assistant, particularly if\n      lightly supervised, can execute deceptive or harmful commands directly via\n      the assistant’s execution flow.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis complex, iterative environment creates a fertile ground for subtle\n    yet powerful attacks, significantly expanding traditional threat models.\n    \u003c/p\u003e\n\n\u003cp\u003eTraditional monitoring tools might struggle to identify malicious\n    activity as malicious activity or subtle data leakage will be harder to spot\n    when embedded within complex, iterative conversations between components, as\n    the tools are new and unknown and still developing at a rapid pace. \u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"NewWeakSpotsMcpAndRulesFiles\"\u003e\n\u003ch2\u003eNew weak spots: MCP and Rules Files \u003c/h2\u003e\n\n\u003cp\u003eThe introduction of MCP servers and rules files create openings for\n      context poisoning—where malicious inputs or altered states can silently\n      propagate through the session, enabling command injection, tampered\n      outputs, or supply chain attacks via compromised code.\u003c/p\u003e\n\n\u003cp\u003eModel Context Protocol (MCP) acts as a flexible, modular interface\n      enabling agents to connect with external tools and data sources, maintain\n      persistent sessions, and share context across workflows. However, as has\n      been \u003ca href=\"https://scribe.rip/%EF%B8%8F-the-s-in-mcp-stands-for-security-91407b33ed6b\"\u003ehighlighted\n      elsewhere\u003c/a\u003e,\n      MCP fundamentally lacks built-in security features like authentication,\n      context encryption, or tool integrity verification by default. This\n      absence can leave developers exposed.\u003c/p\u003e\n\n\u003cp\u003eRules Files, such as for example “cursor rules”, consist of predefined\n      prompts, constraints, and pointers that guide the agent\u0026#39;s behavior within\n      its loop. They enhance stability and reliability by compensating for the\n      limitations of LLM reasoning—constraining the agent\u0026#39;s possible actions,\n      defining error handling procedures, and ensuring focus on the task. While\n      designed to improve predictability and efficiency, these rules represent\n      another layer where malicious prompts can be injected.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Tool-callingAndPrivilegeEscalation\"\u003e\n\u003ch2\u003eTool-calling and privilege escalation\u003c/h2\u003e\n\n\u003cp\u003eCoding assistants go beyond LLM generated code suggestions to operate\n      with tool-use via function calling. For example, given any given coding\n      task, the assistant may execute commands, read and modify files, install\n      dependencies, and even call external APIs. \u003c/p\u003e\n\n\u003cp\u003eThe threat of privilege escalation is an emerging risk with agentic\n      coding assistants. Malicious instructions, can prompt the assistant\n      to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExecute arbitrary system commands. \u003c/li\u003e\n\n\u003cli\u003eModify critical configuration or source code files. \u003c/li\u003e\n\n\u003cli\u003eIntroduce or propagate compromised dependencies.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eGiven the developer\u0026#39;s typically elevated local privileges, a\n      compromised assistant can pivot from the local environment to broader\n      production systems or the kinds of sensitive infrastructure usually\n      accessible by software developers in organisations.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"WhatCanYouDoToSafeguardSecurityWithCodingAgents\"\u003e\n\u003ch2\u003eWhat can you do to safeguard security with coding agents?\u003c/h2\u003e\n\n\u003cp\u003eCoding assistants are pretty new and emerging as of when this was\n      published. But some themes in appropriate security measures are starting\n      to emerge, and many of them represent very traditional best practices.\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eSandboxing and Least Privilege Access control: Take care to limit the\n        privileges granted to coding assistants. Restrictive sandbox environments\n        can limit the blast radius.\u003c/li\u003e\n\n\u003cli\u003eSupply Chain scrutiny: Carefully vet your MCP Servers and Rules Files\n        as critical supply chain components just as you would with library and\n        framework dependencies.\u003c/li\u003e\n\n\u003cli\u003eMonitoring and observability: Implement logging and auditing of file\n        system changes initiated by the agent, network calls to MCP servers,\n        dependency modifications etc.\u003c/li\u003e\n\n\u003cli\u003eExplicitly include coding assistant workflows and external\n        interactions in your \u003ca href=\"https://martinfowler.com/articles/agile-threat-modelling.html\"\u003ethreat\n        modeling\u003c/a\u003e\n        exercises. Consider potential attack vectors introduced by the\n        assistant.\u003c/li\u003e\n\n\u003cli\u003eHuman in the loop: The scope for malicious action increases\n        dramatically when you auto accept changes. Don’t become over reliant on\n        the LLM\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe final point is particularly salient. Rapid code generation by AI\n      can lead to approval fatigue, where developers implicitly trust AI outputs\n      without understanding or verifying. Overconfidence in automated processes,\n      or “vibe coding,” heightens the risk of inadvertently introducing\n      vulnerabilities. Cultivating vigilance, good coding hygiene, and a culture\n      of conscientious custodianship remain really important in professional\n      software teams that ship production software.\u003c/p\u003e\n\n\u003cp\u003eAgentic coding assistants can undeniably provide a boost. However, the\n      enhanced capabilities come with significantly expanded security\n      implications. By clearly understanding these new risks and diligently\n      applying consistent, adaptive security controls, developers and\n      organizations can better hope to safeguard against emerging threats in the\n      evolving AI-assisted software landscape.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003chr/\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": null,
  "modifiedTime": "2025-05-13T00:00:00Z"
}
