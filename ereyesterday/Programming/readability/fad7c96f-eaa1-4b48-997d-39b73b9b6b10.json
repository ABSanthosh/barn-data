{
  "id": "fad7c96f-eaa1-4b48-997d-39b73b9b6b10",
  "title": "Meta AI Releases Llama 4: Early Impressions and Community Feedback",
  "link": "https://www.infoq.com/news/2025/04/meta-ai-llama-4/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Meta has officially released the first models in its new Llama 4 family—Scout and Maverick—marking a step forward in its open-weight large language model ecosystem. Designed with a native multimodal architecture and a mixture-of-experts (MoE) framework, these models aim to support a broader range of applications, from image understanding to long-context reasoning. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Mon, 07 Apr 2025 17:55:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Community",
    "Large language models",
    "Benchmark",
    "Feedback",
    "Hugging Face",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 3122,
  "excerpt": "Meta has officially released the first models in its new Llama 4 family—Scout and Maverick—marking a step forward in its open-weight large language model ecosystem. Designed with a native multimodal a",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250328105021-1/apple-touch-icon.png",
  "text": "Meta has officially released the first models in its new Llama 4 family—Scout and Maverick—marking a step forward in its open-weight large language model ecosystem. Designed with a native multimodal architecture and a mixture-of-experts (MoE) framework, these models aim to support a broader range of applications, from image understanding to long-context reasoning. Llama 4 Scout includes 17 billion active parameters distributed across 16 experts, optimized to run on a single NVIDIA H100 GPU. It supports a 10 million token context window, making it suitable for general-purpose AI tasks. On the other hand, Llama 4 Maverick, also with 17 billion active parameters but utilizing 128 experts, provides enhanced capabilities in reasoning and coding, outperforming several models in its class based on Meta’s benchmarks. Both models were distilled from Meta’s still-training flagship model, Llama 4 Behemoth, which has 288 billion active parameters and nearly two trillion total. Meta claims the Behemoth surpasses GPT-4.5, Claude 3 Sonnet, and Gemini 2.0 Pro on multiple STEM benchmarks. Despite not being fully released, Behemoth serves as a key training teacher for the smaller Scout and Maverick models. Source: Meta AI Blog Beyond model architecture, Meta emphasized a revamped training and post-training strategy, including lightweight supervised fine-tuning, reinforcement learning, and a new curriculum design for handling multimodal input. These changes were aimed at improving performance across difficult tasks while maintaining efficiency and reducing model bias. While benchmark numbers show the Llama 4 models performing competitively with industry leaders like Gemini 2.0 and GPT-4, some early users are expressing skepticism:  Either they are terrible or there is something really wrong with their release/implementations. They seem bad at everything I have tried. Worse than 20-30Bs even and completely lack the most general of knowledge. Another Reddit user added: This has been my experience as well. I am genuinely hoping they are being run with the wrong settings right now and with a magic fix, they will perform at the levels their benchmark scores claim. Some professionals in the field are noting inconsistencies. Uli Hitzel, an AI expert, shared a telling example: The first results from Llama 4 Maverick are indeed impressive, but look - Maverick has 128 experts and it still tells me there are two T's in \"strawberry.\" (We have moved on from counting R's to T's now...) This is a good reminder that even the most advanced, bare LLMs can produce utterly stupid results if we do not integrate them into a properly designed agentic workflow with appropriate checks and balances. Meta has not yet directly addressed these performance concerns in public channels but encourages developers and researchers to try the models themselves. Llama 4 Scout and Maverick are now available for download on llama.com and Hugging Face. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2025/04/meta-ai-llama-4/en/headerimage/generatedHeaderImage-1744047535849.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eMeta has officially released the \u003ca href=\"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\"\u003efirst models in its new Llama 4 family—Scout and Maverick\u003c/a\u003e—marking a step forward in its open-weight large language model ecosystem. Designed with a native multimodal architecture and a mixture-of-experts (MoE) framework, these models aim to support a broader range of applications, from image understanding to long-context reasoning.\u003c/p\u003e\n\n\u003cp\u003eLlama 4 Scout includes 17 billion active parameters distributed across 16 experts, optimized to run on a single NVIDIA H100 GPU. It supports a 10 million token context window, making it suitable for general-purpose AI tasks. On the other hand, Llama 4 Maverick, also with 17 billion active parameters but utilizing 128 experts, provides enhanced capabilities in reasoning and coding, outperforming several models in its class based on Meta’s benchmarks.\u003c/p\u003e\n\n\u003cp\u003eBoth models were distilled from Meta’s still-training flagship model, Llama 4 Behemoth, which has 288 billion active parameters and nearly two trillion total. Meta claims the Behemoth surpasses GPT-4.5, Claude 3 Sonnet, and Gemini 2.0 Pro on multiple STEM benchmarks. Despite not being fully released, Behemoth serves as a key training teacher for the smaller Scout and Maverick models.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"Llama 4 benchmark\" data-src=\"news/2025/04/meta-ai-llama-4/en/resources/1Zrzut ekranu 2025-04-07 192233-1744047534891.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/04/meta-ai-llama-4/en/resources/1Zrzut ekranu 2025-04-07 192233-1744047534891.png\" rel=\"share\"/\u003e\u003cem\u003eSource: Meta AI Blog\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eBeyond model architecture, Meta emphasized a revamped training and post-training strategy, including lightweight supervised fine-tuning, reinforcement learning, and a new curriculum design for handling multimodal input. These changes were aimed at improving performance across difficult tasks while maintaining efficiency and reducing model bias.\u003c/p\u003e\n\n\u003cp\u003eWhile benchmark numbers show the Llama 4 models performing competitively with industry leaders like Gemini 2.0 and GPT-4, some early users \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jsr8ie/comment/mloipot/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button\"\u003eare expressing skepticism\u003c/a\u003e: \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eEither they are terrible or there is something really wrong with their release/implementations. They seem bad at everything I have tried. Worse than 20-30Bs even and completely lack the most general of knowledge.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAnother Reddit user \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jsr8ie/comment/mlojeft/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button\"\u003eadded\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis has been my experience as well. I am genuinely hoping they are being run with the wrong settings right now and with a magic fix, they will perform at the levels their benchmark scores claim.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eSome professionals in the field are noting inconsistencies. Uli Hitzel, an AI expert, \u003ca href=\"https://www.linkedin.com/feed/update/urn:li:activity:7314361939717984256?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7314361939717984256%2C7314458281853235200%29\u0026amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287314458281853235200%2Curn%3Ali%3Aactivity%3A7314361939717984256%29\"\u003eshared\u003c/a\u003e a telling example:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe first results from Llama 4 Maverick are indeed impressive, but look - Maverick has 128 experts and it still tells me there are two T\u0026#39;s in \u0026#34;strawberry.\u0026#34; (We have moved on from counting R\u0026#39;s to T\u0026#39;s now...) This is a good reminder that even the most advanced, bare LLMs can produce utterly stupid results if we do not integrate them into a properly designed agentic workflow with appropriate checks and balances.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eMeta has not yet directly addressed these performance concerns in public channels but encourages developers and researchers to try the models themselves. Llama 4 Scout and Maverick are now available for download on llama.com and \u003ca href=\"https://huggingface.co/meta-llama\"\u003eHugging Face\u003c/a\u003e.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-04-07T00:00:00Z",
  "modifiedTime": null
}
