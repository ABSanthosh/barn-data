{
  "id": "b172a711-d4bc-42dc-8b14-12d50a90ef01",
  "title": "Variants of LoRA",
  "link": "https://stackoverflow.blog/2025/02/26/variants-of-lora/",
  "description": "Want to train a specialized LLM on your own data? The easiest way to do this is with low rank adaptation (LoRA), but many variants of LoRA exist.",
  "author": "Cameron R. Wolfe, PhD",
  "published": "Wed, 26 Feb 2025 17:00:00 GMT",
  "source": "https://stackoverflow.blog/feed/",
  "categories": [
    "se-tech",
    "se-stackoverflow",
    "llm",
    "ai"
  ],
  "byline": "Cameron R. Wolfe, PhD",
  "length": 2919,
  "excerpt": "Want to train a specialized LLM on your own data? The easiest way to do this is with low rank adaptation (LoRA), but many variants of LoRA exist.",
  "siteName": "",
  "favicon": "https://stackoverflow.blog/apple-touch-icon.png",
  "text": "February 26, 2025Want to train a specialized LLM on your own data? The easiest way to do this is with low rank adaptation (LoRA), but many variants of LoRA exist. Credit: Alexandra FrancisThere are many variants of LoRA you can use to train a specialized LLM on your own data. Here’s an overview of all (or at least most) of the techniques that are out there.LoRA models the update derived for a model’s weights during finetuning with a low rank decomposition, implemented in practice as a pair of linear projections. LoRA leaves the pretrained layers of the LLM fixed and injects a trainable rank decomposition matrix into each layer of the model.QLoRA is (arguably) the most popular LoRA variant and uses model quantization techniques to reduce memory usage during finetuning while maintaining (roughly) equal levels of performance. QLoRA uses 4-bit quantization on the pretrained model weights and trains LoRA modules on top of this. In practice, QLoRA saves memory at the cost of slightly-reduced training speed.QA-LoRA is an extension of LoRA/QLoRA that further reduces the computational burden of training and deploying LLMs. It does this by combining parameter-efficient finetuning with quantization (i.e., group-wise quantization applied during training/inference).LoftQ studies a similar idea to QA-LoRA—applying quantization and LoRA finetuning on a pretrained model simultaneously.LongLoRA attempts to cheaply adapt LLMs to longer context lengths using a parameter-efficient (LoRA-based) finetuning scheme. In particular, we start with a pretrained model and finetune it to have a longer context length. This finetuning is made efficient by:Using sparse local attention instead of dense global attention (optional at inference time).Using LoRA (authors find that this works well for context extension).S-LoRA aims to solve the problem of deploying multiple LoRA modules that are used to adapt the same pretrained model to a variety of different tasks. Put simply, S-LoRA does the following to serve thousands of LoRA modules on a single GPU (or across GPUs):Stores all LoRA modules in main memory.Puts modules being used to run the current query into GPU memory.Uses unified paging to allocate GPU memory and avoid fragmentation.Proposes a new tensor parallelism strategy to batch LoRA computations.Many other LoRA variants exist as well…LQ-LoRA: uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.MultiLoRA: extension of LoRA that better handles complex multi-task learning scenarios.LoRA-FA: freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead.Tied-LoRA: leverages weight tying to further improve the parameter efficiency of LoRA.GLoRA: extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.",
  "image": "https://cdn.stackoverflow.co/images/jo7n4k8s/production/f4473993106f8912780f3034210971f5dd8391e1-12000x6293.jpg?w=1200\u0026fm=png\u0026auto=format",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle itemscope=\"\" itemtype=\"https://schema.org/Article\"\u003e\u003cheader\u003e\u003ctime datetime=\"2025-02-26T17:00:00.000Z\" itemprop=\"datePublished\"\u003e February 26, 2025\u003c/time\u003e\u003cp itemprop=\"abstract\"\u003eWant to train a specialized LLM on your own data? The easiest way to do this is with low rank adaptation (LoRA), but many variants of LoRA exist. \u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://cdn.stackoverflow.co/images/jo7n4k8s/production/f4473993106f8912780f3034210971f5dd8391e1-12000x6293.jpg?rect=7,0,11987,6293\u0026amp;w=1200\u0026amp;h=630\u0026amp;auto=format\u0026amp;dpr=2\" width=\"1200\" height=\"630\" alt=\"Article hero image\" itemprop=\"image\"/\u003e\u003cfigcaption\u003e  \u003cspan\u003e Credit: Alexandra Francis\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/header\u003e\u003cdiv itemprop=\"articleBody\"\u003e\u003cp\u003eThere are many variants of LoRA you can use to train a specialized LLM on your own data. Here’s an overview of all (or at least most) of the techniques that are out there.\u003c/p\u003e\u003cp\u003eLoRA models the update derived for a model’s weights during finetuning with a low rank decomposition, implemented in practice as a pair of linear projections. LoRA leaves the pretrained layers of the LLM fixed and injects a trainable rank decomposition matrix into each layer of the model.\u003c/p\u003e\u003cp\u003eQLoRA is (arguably) the most popular LoRA variant and uses model quantization techniques to reduce memory usage during finetuning while maintaining (roughly) equal levels of performance. QLoRA uses 4-bit quantization on the pretrained model weights and trains LoRA modules on top of this. In practice, QLoRA saves memory at the cost of slightly-reduced training speed.\u003c/p\u003e\u003cp\u003eQA-LoRA is an extension of LoRA/QLoRA that further reduces the computational burden of training and deploying LLMs. It does this by combining parameter-efficient finetuning with quantization (i.e., group-wise quantization applied during training/inference).\u003c/p\u003e\u003cp\u003eLoftQ studies a similar idea to QA-LoRA—applying quantization and LoRA finetuning on a pretrained model simultaneously.\u003c/p\u003e\u003cp\u003eLongLoRA attempts to cheaply adapt LLMs to longer context lengths using a parameter-efficient (LoRA-based) finetuning scheme. In particular, we start with a pretrained model and finetune it to have a longer context length. This finetuning is made efficient by:\u003c/p\u003e\u003col\u003e\u003cli\u003eUsing sparse local attention instead of dense global attention (optional at inference time).\u003c/li\u003e\u003cli\u003eUsing LoRA (authors find that this works well for context extension).\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eS-LoRA aims to solve the problem of deploying multiple LoRA modules that are used to adapt the same pretrained model to a variety of different tasks. Put simply, S-LoRA does the following to serve thousands of LoRA modules on a single GPU (or across GPUs):\u003c/p\u003e\u003cul\u003e\u003cli\u003eStores all LoRA modules in main memory.\u003c/li\u003e\u003cli\u003ePuts modules being used to run the current query into GPU memory.\u003c/li\u003e\u003cli\u003eUses unified paging to allocate GPU memory and avoid fragmentation.\u003c/li\u003e\u003cli\u003eProposes a new tensor parallelism strategy to batch LoRA computations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eMany other LoRA variants exist as well…\u003c/p\u003e\u003cul\u003e\u003cli\u003eLQ-LoRA: uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.\u003c/li\u003e\u003cli\u003eMultiLoRA: extension of LoRA that better handles complex multi-task learning scenarios.\u003c/li\u003e\u003cli\u003eLoRA-FA: freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead.\u003c/li\u003e\u003cli\u003eTied-LoRA: leverages weight tying to further improve the parameter efficiency of LoRA.\u003c/li\u003e\u003cli\u003eGLoRA: extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://cdn.stackoverflow.co/images/jo7n4k8s/production/6e51e6782d8114b0ebf3018df478d46cbbd6b92a-2416x1356.jpg?auto=format\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": null,
  "modifiedTime": null
}
