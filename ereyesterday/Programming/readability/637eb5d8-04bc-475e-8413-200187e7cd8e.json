{
  "id": "637eb5d8-04bc-475e-8413-200187e7cd8e",
  "title": "Behind the Streams: Live at Netflix. Part 1",
  "link": "https://netflixtechblog.com/behind-the-streams-live-at-netflix-part-1-d23f917c2f40?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Tue, 15 Jul 2025 16:04:08 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": [
    "live-streaming",
    "architecture",
    "netflix"
  ],
  "byline": "Netflix Technology Blog",
  "length": 14607,
  "excerpt": "Many great ideas at Netflix begin with a question, and three years ago, we asked one of our boldest yet: if we were to entertain the world through Live — a format almost as old as television itself —…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "By Sergey Fedorov, Chris Pham, Flavio Ribeiro, Chris Newton, and Wei WeiMany great ideas at Netflix begin with a question, and three years ago, we asked one of our boldest yet: if we were to entertain the world through Live — a format almost as old as television itself — how would we do it?What began with an engineering plan to pave the path towards our first Live comedy special, Chris Rock: Selective Outrage, has since led to hundreds of Live events ranging from the biggest comedy shows and NFL Christmas Games to record-breaking boxing fights and becoming the home of WWE.In our series Behind the Streams — where we take you through the technical journey of our biggest bets — we will do a multiple part deep-dive into the architecture of Live and what we learned while building it. Part one begins with the foundation we set for Live, and the critical decisions we made that influenced our approach.| But First: What Makes Live Streaming Different?While Live as a television format is not new, the streaming experience we intended to build required capabilities we did not have at the time. Despite 15 years of on-demand streaming under our belt, Live introduced new considerations influencing architecture and technology choices:References: 1. Content Pre-Positioning on Open Connect, 2.Load-Balancing Netflix Traffic at Global ScaleThis means that we had a lot to build in order to make Live work well on Netflix. That starts with making the right choices regarding the fundamentals of our Live Architecture.| Key Pillars of Netflix Live ArchitectureOur Live Technology needed to extend the same promise to members that we’ve made with on-demand streaming: great quality on as many devices as possible without interruptions. Live is one of many entertainment formats on Netflix, so we also needed to seamlessly blend Live events into the user experience, all while scaling to over 300 million global subscribers.When we started, we had nine months until the first launch. While we needed to execute quickly, we also wanted to architect for future growth in both magnitude and multitude of events. As a key principle, we leveraged our unique position of building support for a single product — Netflix — and having control over the full Live lifecycle, from Production to Screen.Dedicated Broadcast Facilities to Ingest Live Content from ProductionLive events can happen anywhere in the world, but not every location has Live facilities or great connectivity. To ensure secure and reliable live signal transport, we leverage distributed and highly connected broadcast operations centers, with specialized equipment for signal ingest and inspection, closed-captioning, graphics and advertisement management. We prioritized repeatability, conditioning engineering to launch live events consistently, reliably, and cost-effectively, leaning into automation wherever possible. As a result, we have been able to reduce the event-specific setup to the transmission between production and the Broadcast Operations Center, reusing the rest across events.Cloud-based Redundant Transcoding and Packaging PipelinesThe feed received at the Broadcast Center contains a fully produced program, but still needs to be encoded and packaged for streaming on devices. We chose a Cloud-based approach to allow for dynamic scaling, flexibility in configuration, and ease of integration with our Digital Rights Management (DRM), content management, and content delivery services already deployed in the cloud. We leverage AWS MediaConnect and AWS MediaLive to acquire feeds in the cloud and transcode them into various video quality levels with bitrates tailored per show. We built a custom packager to better integrate with our delivery and playback systems. We also built a custom Live Origin to ensure strict read and write SLAs for Live segments.Scaling Live Content Delivery to millions of viewers with Open Connect CDNIn order for the produced media assets to be streamed, they need to be transferred from a few AWS locations, where Live Origin is deployed, to hundreds of millions of devices worldwide. We leverage Netflix’s CDN, Open Connect, to scale Live asset delivery. Open Connect servers are placed close to the viewers at over 6K locations and connected to AWS locations via a dedicated Open Connect Backbone network.18K+ servers in 6K+ locations, in Internet Exchanges, or embedded into ISP networksOpen Connect Backbone connects servers in Internet Exchange locations to 5 AWS regionsBy enabling Live delivery on Open Connect, we build on top of $1B+ in Netflix investments over the last 12 years focused on scaling the network and optimizing the performance of delivery servers. By sharing capacity across on-demand and Live viewership we improve utilization, and by caching past Live content on the same servers used for on-demand streaming, we can easily enable catch-up viewing.Optimizing Live Playback for Device Compatibility, Scale, Quality, and StabilityTo make Live accessible to the majority of our customers without upgrading their streaming devices, we settled on using HTTPS-based Live Streaming. While UDP-based protocols can provide additional features like ultra-low latency, HTTPS has ubiquitous support among devices and compatibility with delivery and encoding systems. Furthermore, we use AVC and HEVC video codecs, transcode with multiple quality levels up from SD to 4K, and use a 2-second segment duration to balance compression efficiency, infrastructure load, and latency. While prioritizing streaming quality and playback stability, we have also achieved industry standard latency from camera to device, and continue to improve it.To configure playback, the device player receives a playback manifest at the play start. The manifest contains items like the encoding bitrates and CDN servers players should use. We deliver the manifest from the cloud instead of the CDN, as it allows us to personalize the configuration for each device. To reference segments of the stream, the manifest includes a segment template that is used by devices to map a wall-clock time to URLs on the CDN. Using a segment template vs periodic polling for manifest updates minimizes network dependencies, CDN server load, and overhead on resource-constrained devices, like smart TVs, thus improving both scalability and stability of our system. While streaming, the player monitors network performance and dynamically chooses the bitrate and CDN server, maximizing streaming quality while minimizing rebuffering.Run Discovery and Playback Control Services in the CloudSo far, we have covered the streaming path from Camera to Device. To make the stream fully work, we also need to orchestrate across all systems, and ensure viewers can find and start the Live event. This functionality is performed by dozens of Cloud services, with functions like playback configuration, personalization, or metrics collection. These services tend to receive disproportionately higher loads around Live event start time, and Cloud deployment provides flexibility in dynamically scaling compute resources. Moreover, as Live demand tends to be localized, we are able to balance load across multiple AWS regions, better utilizing our global footprint. Deployment in the cloud also allows us to build a user experience where we embed Live content into a broader selection of entertainment options in the UI, like on-demand titles or Games.Centralize Real-time Metrics in the Cloud with Specialized Tools and FacilitiesWith control over ingest, encoding pipelines, the Open Connect CDN, and device players, we have nearly end-to-end observability into the Live workflow. During Live, we collect system and user metrics in real-time (e.g., where members see the title on Netflix and their quality of experience), alerting us to poor user experiences or degraded system performance. Our real-time monitoring is built using a mix of internally developed tools, such as Atlas, Mantis, and Lumen, and open-source technologies, such as Kafka and Druid, processing up to 38 million events per second during some of our largest live events while providing critical metrics and operational insights in a matter of seconds. Furthermore, we set up dedicated “Control Center” facilities, which bring key metrics together to the operational team that monitors the event in real-time.| Our key learnings so farBuilding new functionality always brings fresh challenges and opportunities to learn, especially with a system as complex as Live. Even after three years, we’re still learning every day how to deliver Live events more effectively. Here are a few key highlights:Extensive testing: Prior to Live we heavily relied on the predictable flow of on-demand traffic for pre-release canaries or A/B tests to validate deployments. But Live traffic was not always available, especially not at the scale representative of a big launch. As a result, we spent considerable effort to:Generate internal “test streams,” which engineers use to run integration, regression, or smoke tests as part of the development lifecycle.Build synthetic load testing capabilities to stress test cloud and CDN systems. We use 2 approaches, allowing us to generate up to 100K starts-per-second: — Capture, modify, and replay past Live production traffic, representing a diversity of user devices and request patterns. — Virtualize Netflix devices and generate traffic against CDN or Cloud endpoints to test the impact of the latest changes across all systems.Run automated failure injection, forcing missing or corrupted segments from the encoding pipeline, loss of a cloud region, network drop, or server timeouts.Regular practice: Despite rigorous pre-release testing, nothing beats a production environment, especially when operating at scale. We learned that having a regular schedule with diverse Live content is essential to making improvements while balancing the risks of member impact. We run A/B tests, perform chaos testing, operational exercises, and train operational teams for upcoming launches.Viewership predictions: We use prediction-based techniques to pre-provision Cloud and CDN capacity, and share forecasts with our ISP and Cloud partners ahead of time so they can plan network and compute resources. Then we complement them with reactive scaling of cloud systems powering sign-up, log-in, title discovery, and playback services to account for viewership exceeding our predictions. We have found success with forward-looking real-time viewership predictions during a live event, allowing us to take steps to mitigate risks earlier, before more members are impacted.Graceful degradation: Despite our best efforts, we can (and did!) find ourselves in a situation where viewership exceeded our predictions and provisioned capacity. In this case, we developed a number of levers to continue streaming, even if it means gradually removing some nice-to-have features. For example, we use service-level prioritized load shedding to prioritize live traffic over non-critical traffic (like pre-fetch). Beyond that, we can lighten the experience, like dialing down personalization, disabling bookmarks, or lowering the maximum streaming quality. Our load tests include scenarios where we under-scale systems to validate desired behavior.Retry storms: When systems reach capacity, our key focus is to avoid cascading issues or further overloading systems with retries. Beyond system retries, users may retry manually — we’ve seen a 10x increase in traffic load due to stream restarts after viewing interruptions of as little as 30 seconds. We spent considerable time understanding device retry behavior in the presence of issues like network timeouts or missing segments. As a result, we implemented strategies like server-guided backoff for device retries, absorbing spikes via prioritized traffic shedding at Cloud Edge Gateway, and re-balancing traffic between cloud regions.Contingency planning: “Everyone has a plan until they get punched in the mouth” is very relevant for Live. When something breaks, there is practically no time for troubleshooting. For large events, we set up in-person launch rooms with engineering owners of critical systems. For quick detection and response, we developed a small set of metrics as early indicators of issues, and have extensive runbooks for common operational issues. We don’t learn on launch day; instead, launch teams practice failure response via Game Day exercises ahead of time. Finally, our runbooks extend beyond engineering, covering escalation to executive leadership and coordination across functions like Customer Service, Production, Communications, or Social.Our commitment to enhancing the member experience doesn’t end at the “Thanks for Watching!” screen. Shortly after each live stream, we dive into metrics to identify areas for improvement. Our Data \u0026 Insights team conducts comprehensive analyses, A/B tests, and consumer research to ensure the next event is even more delightful for our members. We leverage insights on member behavior, preferences, and expectations to refine the Netflix product experience and optimize our Live technology — like reducing latency by ~10 seconds through A/B tests, without affecting quality or stability.| What’s next on our Live journey?Despite three years of effort, we are far from done! In fact, we are just getting started, actively building on the learnings shared above to deliver more joy to our members with Live events. To support the growing number of Live titles and new formats, like FIFA WWC in 2027, we keep building our broadcast and delivery infrastructure and are actively working to further improve the Live experience.In this post, we’ve provided a broad overview and have barely scratched the surface. In the upcoming posts, we will dive deeper into key pillars of our Live systems, covering our encoding, delivery, playback, and user experience investments in more detail.Getting this far would not have been possible without the hard work of dozens of teams across Netflix, who collaborate closely to design, build, and operate Live systems: Operations and Reliability, Encoding Technologies, Content Delivery, Device Playback, Streaming Algorithms, UI Engineering, Search and Discovery, Messaging, Content Promotion and Distribution, Data Platform, Cloud Infrastructure, Tooling and Productivity, Program Management, Data Science \u0026 Engineering, Product Management, Globalization, Consumer Insights, Ads, Security, Payments, Live Production, Experience and Design, Product Marketing and Customer Service, amongst many others.",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*eMwIzmDUqURUESqu",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv tabindex=\"-1\" aria-hidden=\"false\"\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page---byline--d23f917c2f40---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"32\" height=\"32\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"de94\"\u003eBy \u003ca href=\"https://www.linkedin.com/in/sfedov/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSergey Fedorov\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/phamchristopher/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eChris Pham\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/flavioribeiro/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFlavio Ribeiro\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/chrisnewton2/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eChris Newton\u003c/a\u003e, and \u003ca href=\"https://www.linkedin.com/in/wei-wei-1571794/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eWei Wei\u003c/a\u003e\u003c/p\u003e\u003cp id=\"13a1\"\u003eMany great ideas at Netflix begin with a question, and three years ago, we asked one of our boldest yet: if we were to entertain the world through Live — a format almost as old as television itself — how would \u003cem\u003ewe\u003c/em\u003e do it?\u003c/p\u003e\u003cp id=\"0e1c\"\u003eWhat began with an engineering plan to pave the path towards our first Live comedy special, \u003ca href=\"https://www.netflix.com/title/80167499\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eChris Rock: Selective Outrage\u003c/a\u003e, has since led to hundreds of Live events ranging from the biggest \u003ca href=\"https://www.netflix.com/tudum/articles/greatest-roast-of-all-time-tom-brady-live\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecomedy shows\u003c/a\u003e and \u003ca href=\"https://about.netflix.com/en/news/nfl-christmas-day-games-on-netflix-average-over-30-million-global-viewers\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNFL Christmas Games\u003c/a\u003e to record-breaking \u003ca href=\"https://about.netflix.com/en/news/jake-paul-vs-mike-tyson-over-108-million-live-global-viewers\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eboxing fights\u003c/a\u003e and becoming the \u003ca href=\"https://about.netflix.com/en/news/netflix-to-become-new-home-of-wwe-raw-beginning-2025\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehome of WWE\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"1d28\"\u003eIn our series \u003cem\u003eBehind the Streams\u003c/em\u003e — where we take you through the technical journey of our biggest bets — we will do a multiple part deep-dive into the architecture of Live and what we learned while building it. Part one begins with the foundation we set for Live, and the critical decisions we made that influenced our approach.\u003c/p\u003e\u003ch2 id=\"9094\"\u003e| But First: What Makes Live Streaming Different?\u003c/h2\u003e\u003cp id=\"5adf\"\u003eWhile Live as a television format is not new, the streaming experience we intended to build required capabilities we did not have at the time. Despite 15 years of on-demand streaming under our belt, Live introduced new considerations influencing architecture and technology choices:\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eReferences: 1. \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/distributing-content-to-open-connect-3e3e391d4dc9\" target=\"_blank\" data-discover=\"true\"\u003eContent Pre-Positioning on Open Connect\u003c/a\u003e, 2.\u003ca href=\"https://www.infoq.com/presentations/load-balancing-netflix/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLoad-Balancing Netflix Traffic at Global Scale\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"6b2b\"\u003eThis means that we had a lot to build in order to make Live work well on Netflix. That starts with making the right choices regarding the fundamentals of our Live Architecture.\u003c/p\u003e\u003ch2 id=\"c6a9\"\u003e| Key Pillars of Netflix Live Architecture\u003c/h2\u003e\u003cp id=\"f409\"\u003eOur Live Technology needed to extend the same promise to members that we’ve made with on-demand streaming: \u003cstrong\u003egreat quality\u003c/strong\u003e on as \u003cstrong\u003emany devices\u003c/strong\u003e as possible \u003cstrong\u003ewithout interruptions\u003c/strong\u003e. Live is one of many entertainment formats on Netflix, so we also needed to seamlessly blend Live events into the user experience, all while scaling to over 300 million global subscribers.\u003c/p\u003e\u003cp id=\"ca8d\"\u003eWhen we started, we had \u003cstrong\u003enine months\u003c/strong\u003e until the first launch. While we needed to execute quickly, we also wanted to \u003cstrong\u003earchitect for future growth\u003c/strong\u003e in both \u003cstrong\u003emagnitude\u003c/strong\u003e and \u003cstrong\u003emultitude\u003c/strong\u003e of events. As a key principle, we leveraged our unique position of building support for a single product — Netflix — and having control over the full Live lifecycle, from Production to Screen.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"acf2\"\u003e\u003cstrong\u003eDedicated Broadcast Facilities to Ingest Live Content from Production\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"9608\"\u003eLive events can happen anywhere in the world, but not every location has Live facilities or great connectivity. To ensure secure and reliable live signal transport, we leverage distributed and highly connected broadcast operations centers, with specialized equipment for signal ingest and inspection, closed-captioning, graphics and advertisement management. We prioritized \u003cstrong\u003erepeatability\u003c/strong\u003e, conditioning engineering to launch live events consistently, reliably, and cost-effectively, leaning into \u003cstrong\u003eautomation\u003c/strong\u003e wherever possible. As a result, we have been able to reduce the event-specific setup to the transmission between production and the Broadcast Operations Center, reusing the rest across events.\u003c/p\u003e\u003cp id=\"f6b4\"\u003e\u003cstrong\u003eCloud-based Redundant Transcoding and Packaging Pipelines\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"92de\"\u003eThe feed received at the Broadcast Center contains a fully produced program, but still needs to be encoded and packaged for streaming on devices. We chose a Cloud-based approach to allow for \u003cstrong\u003edynamic scaling\u003c/strong\u003e, \u003cstrong\u003eflexibility\u003c/strong\u003e in configuration, and \u003cstrong\u003eease of integration\u003c/strong\u003e with our Digital Rights Management (DRM), content management, and content delivery services already deployed in the cloud. We leverage AWS MediaConnect and AWS MediaLive to acquire feeds in the cloud and transcode them into various video quality levels with bitrates tailored per show. We built a \u003cstrong\u003ecustom packager\u003c/strong\u003e to better integrate with our delivery and playback systems. We also built a \u003cstrong\u003ecustom Live Origin\u003c/strong\u003e to ensure strict read and write SLAs for Live segments.\u003c/p\u003e\u003cp id=\"7d57\"\u003e\u003cstrong\u003eScaling Live Content Delivery to millions of viewers with Open Connect CDN\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"2393\"\u003eIn order for the produced media assets to be streamed, they need to be transferred from a few AWS locations, where Live Origin is deployed, to hundreds of millions of devices worldwide. We leverage Netflix’s CDN, \u003ca href=\"https://www.theverge.com/22787426/netflix-cdn-open-connect\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOpen Connect\u003c/a\u003e, to scale Live asset delivery. Open Connect servers are placed close to the viewers at \u003cstrong\u003eover 6K locations\u003c/strong\u003e and connected to AWS locations via a \u003cstrong\u003ededicated Open Connect Backbone network\u003c/strong\u003e.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e18K+\u003cem\u003e servers in 6K+ locations, in Internet Exchanges, or embedded into ISP networks\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eOpen Connect Backbone connects servers in Internet Exchange locations to 5 AWS regions\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"11ff\"\u003eBy enabling Live delivery on Open Connect, we build on top of $1B+ in Netflix investments over the last 12 years focused on scaling the network and optimizing the performance of delivery servers. By sharing capacity across on-demand and Live viewership we improve utilization, and by caching past Live content on the same servers used for on-demand streaming, we can easily enable catch-up viewing.\u003c/p\u003e\u003cp id=\"e6ba\"\u003e\u003cstrong\u003eOptimizing Live Playback for Device Compatibility, Scale, Quality, and Stability\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"e965\"\u003eTo make Live accessible to the majority of our customers without upgrading their streaming devices, we settled on using \u003cstrong\u003eHTTPS\u003c/strong\u003e-based Live Streaming. While UDP-based protocols can provide additional features like ultra-low latency, HTTPS has ubiquitous support among devices and compatibility with delivery and encoding systems. Furthermore, we use \u003cstrong\u003eAVC\u003c/strong\u003e and \u003cstrong\u003eHEVC\u003c/strong\u003e video codecs, transcode with multiple quality levels up \u003cstrong\u003efrom SD to 4K\u003c/strong\u003e, and use a \u003cstrong\u003e2-second segment\u003c/strong\u003e duration to balance compression efficiency, infrastructure load, and latency. While prioritizing streaming quality and playback stability, we have also achieved industry standard latency from camera to device, and continue to improve it.\u003c/p\u003e\u003cp id=\"104e\"\u003eTo configure playback, the device player receives a playback manifest at the play start. The manifest contains items like the encoding bitrates and CDN servers players should use. We deliver the manifest from the cloud instead of the CDN, as it allows us to personalize the configuration for each device. To reference segments of the stream, the manifest includes a segment template that is used by devices to map a wall-clock time to URLs on the CDN. Using a segment template vs periodic polling for manifest updates minimizes network dependencies, CDN server load, and overhead on resource-constrained devices, like smart TVs, thus improving both scalability and stability of our system. While streaming, the player monitors network performance and dynamically chooses the bitrate and CDN server, maximizing streaming quality while minimizing rebuffering.\u003c/p\u003e\u003cp id=\"e72a\"\u003e\u003cstrong\u003eRun Discovery and Playback Control Services in the Cloud\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"e33b\"\u003eSo far, we have covered the streaming path from Camera to Device. To make the stream fully work, we also need to orchestrate across all systems, and ensure viewers can find and start the Live event. This functionality is performed by \u003cstrong\u003edozens of Cloud services\u003c/strong\u003e, with functions like playback configuration, personalization, or metrics collection. These services tend to receive disproportionately \u003cstrong\u003ehigher loads around Live event start time\u003c/strong\u003e, and Cloud deployment provides flexibility in dynamically scaling compute resources. Moreover, as Live demand tends to be localized, we are able to balance load across \u003cstrong\u003emultiple AWS regions\u003c/strong\u003e, better utilizing our global footprint. Deployment in the cloud also allows us to build a user experience where we embed Live content into a broader selection of entertainment options in the UI, like on-demand titles or Games.\u003c/p\u003e\u003cp id=\"8c6e\"\u003e\u003cstrong\u003eCentralize Real-time Metrics in the Cloud with Specialized Tools and Facilities\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"bce4\"\u003eWith control over ingest, encoding pipelines, the Open Connect CDN, and device players, we have nearly \u003cstrong\u003eend-to-end observability\u003c/strong\u003e into the Live workflow. During Live, we collect system and user metrics in real-time (e.g., where members see the title on Netflix and their quality of experience), alerting us to poor user experiences or degraded system performance. Our real-time monitoring is built using a mix of internally developed tools, such as \u003ca href=\"https://netflix.github.io/atlas-docs/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAtlas\u003c/a\u003e, \u003ca href=\"https://netflix.github.io/mantis/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMantis\u003c/a\u003e, and \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c\" target=\"_blank\" data-discover=\"true\"\u003eLumen\u003c/a\u003e, and open-source technologies, such as Kafka and \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06\" target=\"_blank\" data-discover=\"true\"\u003eDruid\u003c/a\u003e, processing up to \u003cstrong\u003e38 million events per second\u003c/strong\u003e during some of our largest live events while providing critical metrics and operational insights in a matter of seconds. Furthermore, we set up dedicated \u003cstrong\u003e“Control Center” facilities\u003c/strong\u003e, which bring key metrics together to the operational team that monitors the event in real-time.\u003c/p\u003e\u003ch2 id=\"6604\"\u003e| Our key learnings so far\u003c/h2\u003e\u003cp id=\"0598\"\u003eBuilding new functionality always brings fresh challenges and opportunities to learn, especially with a system as complex as Live. Even after three years, we’re still learning every day how to deliver Live events more effectively. Here are a few key highlights:\u003c/p\u003e\u003cp id=\"3f57\"\u003e\u003cstrong\u003eExtensive testing: \u003c/strong\u003ePrior to Live we heavily relied on the predictable flow of on-demand traffic for pre-release canaries or A/B tests to validate deployments. But Live traffic was not always available, especially not at the scale representative of a big launch. As a result, we spent considerable effort to:\u003c/p\u003e\u003col\u003e\u003cli id=\"3982\"\u003eGenerate internal “test streams,” which engineers use to run \u003cstrong\u003eintegration\u003c/strong\u003e, \u003cstrong\u003eregression\u003c/strong\u003e, or \u003cstrong\u003esmoke tests\u003c/strong\u003e as part of the development lifecycle.\u003c/li\u003e\u003cli id=\"892f\"\u003eBuild synthetic \u003cstrong\u003eload testing\u003c/strong\u003e capabilities to stress test cloud and CDN systems. We use 2 approaches, allowing us to generate up to \u003cstrong\u003e100K starts-per-second\u003c/strong\u003e:\u003cbr/\u003e — Capture, modify, and replay past Live production traffic, representing a diversity of user devices and request patterns.\u003cbr/\u003e — Virtualize Netflix devices and generate traffic against CDN or Cloud endpoints to test the impact of the latest changes across all systems.\u003c/li\u003e\u003cli id=\"b6ec\"\u003eRun automated \u003cstrong\u003efailure injection\u003c/strong\u003e, forcing missing or corrupted segments from the encoding pipeline, loss of a cloud region, network drop, or server timeouts.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"82d4\"\u003e\u003cstrong\u003eRegular practice: \u003c/strong\u003eDespite rigorous pre-release testing, nothing beats a production environment, especially when operating at scale. We learned that having a regular schedule with diverse Live content is essential to making improvements while balancing the risks of member impact. We run\u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/decision-making-at-netflix-33065fa06481\" target=\"_blank\" data-discover=\"true\"\u003e A/B tests\u003c/a\u003e, perform \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/chap-chaos-automation-platform-53e6d528371f\" target=\"_blank\" data-discover=\"true\"\u003echaos testing\u003c/a\u003e, operational exercises, and train operational teams for upcoming launches.\u003c/p\u003e\u003cp id=\"4319\"\u003e\u003cstrong\u003eViewership predictions:\u003c/strong\u003e We use prediction-based techniques to pre-provision Cloud and CDN capacity, and share forecasts with our ISP and Cloud partners ahead of time so they can plan network and compute resources. Then we complement them with reactive scaling of cloud systems powering sign-up, log-in, title discovery, and playback services to account for viewership exceeding our predictions. We have found success with forward-looking real-time viewership predictions \u003cem\u003eduring\u003c/em\u003e a live event, allowing us to take steps to mitigate risks earlier, before more members are impacted.\u003c/p\u003e\u003cp id=\"3d52\"\u003e\u003cstrong\u003eGraceful degradation: \u003c/strong\u003eDespite our best efforts, we can (and did!) find ourselves in a situation where viewership exceeded our predictions and provisioned capacity. In this case, we developed a number of levers to continue streaming, even if it means gradually removing some nice-to-have features. For example, we use \u003ca rel=\"noopener ugc nofollow\" href=\"https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d\" target=\"_blank\" data-discover=\"true\"\u003eservice-level prioritized load shedding\u003c/a\u003e to prioritize live traffic over non-critical traffic (like pre-fetch). Beyond that, we can lighten the experience, like dialing down personalization, disabling bookmarks, or lowering the maximum streaming quality. Our load tests include scenarios where we under-scale systems to validate desired behavior.\u003c/p\u003e\u003cp id=\"255c\"\u003e\u003cstrong\u003eRetry storms: \u003c/strong\u003eWhen systems reach capacity, our key focus is to avoid cascading issues or further overloading systems with retries.\u003cstrong\u003e \u003c/strong\u003eBeyond system retries, users may retry manually — we’ve seen a 10x increase in traffic load due to stream restarts after viewing interruptions of as little as 30 seconds. We spent considerable time understanding device retry behavior in the presence of issues like network timeouts or missing segments. As a result, we implemented strategies like server-guided backoff for device retries, absorbing spikes via prioritized traffic shedding at Cloud Edge Gateway, and re-balancing traffic between cloud regions.\u003c/p\u003e\u003cp id=\"ace2\"\u003e\u003cstrong\u003eContingency planning: \u003c/strong\u003e“\u003cem\u003eEveryone has a plan until they get punched in the mouth\u003c/em\u003e” is very relevant for Live. When something breaks, there is practically no time for troubleshooting. For large events, we set up \u003cstrong\u003ein-person launch rooms\u003c/strong\u003e with engineering owners of critical systems. For quick detection and response, we developed a small set of metrics as early indicators of issues, and have extensive runbooks for common operational issues. We don’t learn on launch day; instead, launch teams practice failure response via \u003cstrong\u003eGame Day exercises ahead of time\u003c/strong\u003e. Finally, our runbooks extend beyond engineering, covering escalation to executive leadership and coordination across functions like Customer Service, Production, Communications, or Social.\u003c/p\u003e\u003cp id=\"018a\"\u003eOur commitment to enhancing the member experience doesn’t end at the “Thanks for Watching!” screen. Shortly after each live stream, we dive into metrics to identify areas for improvement. Our Data \u0026amp; Insights team conducts comprehensive analyses, A/B tests, and consumer research to ensure the next event is even more delightful for our members. We leverage insights on member behavior, preferences, and expectations to refine the Netflix product experience and optimize our Live technology — like reducing latency by ~10 seconds through A/B tests, without affecting quality or stability.\u003c/p\u003e\u003ch2 id=\"1acd\"\u003e| What’s next on our Live journey?\u003c/h2\u003e\u003cp id=\"de44\"\u003eDespite three years of effort, we are far from done! In fact, we are just getting started, actively building on the learnings shared above to deliver more joy to our members with Live events. To support the growing number of Live titles and new formats, like \u003ca href=\"https://www.netflix.com/tudum/articles/womens-world-cup-netflix\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFIFA WWC in 2027\u003c/a\u003e, we keep building our broadcast and delivery infrastructure and are actively working to further improve the Live experience.\u003c/p\u003e\u003cp id=\"2732\"\u003eIn this post, we’ve provided a broad overview and have barely scratched the surface. In the upcoming posts, we will dive deeper into key pillars of our Live systems, covering our encoding, delivery, playback, and user experience investments in more detail.\u003c/p\u003e\u003cp id=\"95fd\"\u003eGetting this far would not have been possible without the hard work of dozens of teams across Netflix, who collaborate closely to design, build, and operate Live systems: Operations and Reliability, Encoding Technologies, Content Delivery, Device Playback, Streaming Algorithms, UI Engineering, Search and Discovery, Messaging, Content Promotion and Distribution, Data Platform, Cloud Infrastructure, Tooling and Productivity, Program Management, Data Science \u0026amp; Engineering, Product Management, Globalization, Consumer Insights, Ads, Security, Payments, Live Production, Experience and Design, Product Marketing and Customer Service, amongst many others.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "16 min read",
  "publishedTime": "2025-07-15T14:59:21.998Z",
  "modifiedTime": null
}
