{
  "id": "2dc6864b-8368-407c-889d-546b4ba22e74",
  "title": "Google DeepMind’s AlphaGeometry2 AI Achieves Gold-Medal Math Olympiad Performance",
  "link": "https://www.infoq.com/news/2025/02/deepmind-alphageom2/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Google DeepMind's AlphaGeometry2 (AG2) AI model solved 84% of the geometry problems from the last 25 years of International Math Olympiads (IMO), outperforming the average human gold-medalist performance. By Anthony Alford",
  "author": "Anthony Alford",
  "published": "Tue, 25 Feb 2025 14:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Google DeepMind",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Anthony Alford",
  "length": 3746,
  "excerpt": "Google DeepMind's AlphaGeometry2 (AG2) AI model solved 84% of the geometry problems from the last 25 years of International Math Olympiads (IMO), outperforming the average human gold-medalist performa",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250225074537/apple-touch-icon.png",
  "text": "Google DeepMind's AlphaGeometry2 (AG2) AI model solved 84% of the geometry problems from the last 25 years of International Math Olympiads (IMO), outperforming the average human gold-medalist performance. AlphaGeometry2 is a new iteration of DeepMind's earlier geometry AI, AlphaGeometry (AG1), which could only solve 54% of the IMO problems. Both models operate by using a domain-specific formal language to describe the problems and a symbolic deductive engine to generate proofs. The new model's improvements include a more powerful LLM based on Gemini, which translates the natural language form of the problem into formal language. AG2 solved 42 of the 50 IMO geometry problems from the years 2000 to 2024, while the average gold medalist solves about 41. Flagship commercial reasoning LLMs, such as OpenAI's o1 and Gemini Thinking, cannot solve any of the problems. According to DeepMind, Despite achieving an impressive 84% solve rate on all 2000-2024 IMO geometry problems, there is still room for improvement...AG2 has not solved all IMO and IMO [short list] problems. We hypothesize that breaking problems into subproblems and applying reinforcement learning approaches could close this gap. Finally, in this paper we reported progress on building a fully automated geometry problem solving system, which takes input in natural language and outputs a solution reliably without any hallucinations. Despite good initial results, we think the auto-formalization can be further improved with more formalization examples and supervised fine-tuning. AG2, like AG1, solves geometry problems by stating them in a formal language which consists of predicates: for example, acompute a b c d means \"Find the angle between AB and CD.\" AG2's predicates can cover 88% of the IMO problems; the model will not attempt to solve the other problems. But first, the problems written in natural language must be expressed in this formal language. To do this, DeepMind uses a Gemini LLM with few-shot prompting: the prompts contain \"several dozens\" of examples of problem translation. This approach is \"very consistent and makes almost no mistakes\" on the easier problems. Once the problems are specified as formal predicates, they are solved using a symbolic engine called Deductive Database Arithmetic Reasoning (DDAR). If the engine fails to find a proof, AG2 uses a language model and tree search algorithm to generate auxiliary constructions, then it re-runs the DDAR engine; this loop is repeated until a proof is found. Writing on X, Berkeley CS PhD student Yuxi Liu said,  AlphaGeometry2 is pretty cool, but clearly not bitter-lessoned. It has a very 1950s auto theorem proving feel, with handcrafted representation language, logical inference engine, etc...They are just doing autoformalization (succeeding 30/39) and proposing auxiliary constructions during tree search. Many of them require just a single auxiliary construction! Though there are cursed examples that required 12. Oxford University ML researcher Simon Frieder also wrote on X: AlphaGeometry2 was published, 2.5 months since we released Newclid without much fanfare (in true scientist style! :D) and two months after TongGeometry. It seems no code was provided for AG2. So now we have two closed systems, AlphaGeometry2 and TongGeometry that we cannot compare. Newclid...is fully open-source, fixed many AlphaGeometry bugs and slightly improved it in terms of performance - and we also have GeoGebra support for better input. Although the AG2 code has not been released, the code for AG1 is available on GitHub. About the Author Anthony Alford",
  "image": "https://res.infoq.com/news/2025/02/deepmind-alphageom2/en/headerimage/generatedHeaderImage-1739742473212.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://deepmind.google/\"\u003eGoogle DeepMind\u0026#39;s\u003c/a\u003e \u003ca href=\"https://arxiv.org/abs/2502.03544\"\u003eAlphaGeometry2\u003c/a\u003e (AG2) AI model solved 84% of the geometry problems from the last 25 years of \u003ca href=\"https://www.imo-official.org/problems.aspx\"\u003eInternational Math Olympiads\u003c/a\u003e (IMO), outperforming the average human gold-medalist performance.\u003c/p\u003e\n\n\u003cp\u003eAlphaGeometry2 is a new iteration of DeepMind\u0026#39;s earlier geometry AI, \u003ca href=\"https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/\"\u003eAlphaGeometry\u003c/a\u003e (AG1), which could only solve 54% of the IMO problems. Both models operate by using a domain-specific formal language to describe the problems and a symbolic deductive engine to generate proofs. The new model\u0026#39;s improvements include a more powerful LLM based on \u003ca href=\"https://deepmind.google/technologies/gemini/\"\u003eGemini\u003c/a\u003e, which translates the natural language form of the problem into formal language. AG2 solved 42 of the 50 IMO geometry problems from the years 2000 to 2024, while the average gold medalist solves about 41. Flagship commercial reasoning LLMs, such as OpenAI\u0026#39;s o1 and Gemini Thinking, cannot solve any of the problems. According to DeepMind,\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eDespite achieving an impressive 84% solve rate on all 2000-2024 IMO geometry problems, there is still room for improvement...AG2 has not solved all IMO and IMO [short list] problems. We hypothesize that breaking problems into subproblems and applying reinforcement learning approaches could close this gap. Finally, in this paper we reported progress on building a fully automated geometry problem solving system, which takes input in natural language and outputs a solution reliably without any hallucinations. Despite good initial results, we think the auto-formalization can be further improved with more formalization examples and supervised fine-tuning.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAG2, like AG1, solves geometry problems by stating them in a formal language which consists of predicates: for example, \u003cem\u003eacompute a b c d\u003c/em\u003e means \u0026#34;Find the angle between AB and CD.\u0026#34; AG2\u0026#39;s predicates can cover 88% of the IMO problems; the model will not attempt to solve the other problems.\u003c/p\u003e\n\n\u003cp\u003eBut first, the problems written in natural language must be expressed in this formal language. To do this, DeepMind uses a Gemini LLM with few-shot prompting: the prompts contain \u0026#34;several dozens\u0026#34; of examples of problem translation. This approach is \u0026#34;very consistent and makes almost no mistakes\u0026#34; on the easier problems.\u003c/p\u003e\n\n\u003cp\u003eOnce the problems are specified as formal predicates, they are solved using a symbolic engine called Deductive Database Arithmetic Reasoning (DDAR). If the engine fails to find a proof, AG2 uses a language model and tree search algorithm to generate \u003ca href=\"https://en.wikipedia.org/wiki/Auxiliary_line\"\u003eauxiliary constructions\u003c/a\u003e, then it re-runs the DDAR engine; this loop is repeated until a proof is found.\u003c/p\u003e\n\n\u003cp\u003eWriting on X, Berkeley CS PhD student \u003ca href=\"https://x.com/layer07_yuxi/status/1887978891945656618\"\u003eYuxi Liu said\u003c/a\u003e, \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAlphaGeometry2 is pretty cool, but clearly not \u003ca href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\"\u003ebitter-lessoned\u003c/a\u003e. It has a very 1950s auto theorem proving feel, with handcrafted representation language, logical inference engine, etc...They are just doing autoformalization (succeeding 30/39) and proposing auxiliary constructions during tree search. Many of them require just a single auxiliary construction! Though there are cursed examples that required 12.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eOxford University ML researcher \u003ca href=\"https://x.com/friederrrr/status/1888412444491501767\"\u003eSimon Frieder also wrote\u003c/a\u003e on X:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAlphaGeometry2 was published, 2.5 months since we released \u003ca href=\"https://github.com/LMCRC/Newclid\"\u003eNewclid\u003c/a\u003e without much fanfare (in true scientist style! :D) and two months after \u003ca href=\"https://arxiv.org/abs/2412.10673\"\u003eTongGeometry\u003c/a\u003e. It seems no code was provided for AG2. So now we have two closed systems, AlphaGeometry2 and TongGeometry that we cannot compare. Newclid...is fully open-source, fixed many AlphaGeometry bugs and slightly improved it in terms of performance - and we also have GeoGebra support for better input.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAlthough the AG2 code has not been released, the code for \u003ca href=\"https://github.com/google-deepmind/alphageometry\"\u003eAG1\u003c/a\u003e is available on GitHub.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Anthony-Alford\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAnthony Alford\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-02-25T00:00:00Z",
  "modifiedTime": null
}
