{
  "id": "2db01b48-0ba2-485a-8cae-a439d3fb3da4",
  "title": "Google Debuts OpenAI-compatible API for Gemini",
  "link": "https://www.infoq.com/news/2024/11/google-gemini-openai-compatible/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "In an effort to make it easier for developers who adopted OpenAI for their LLM-based solutions to switch to Gemini, Google has launched a new endpoint for its Gemini API that allows them to easily switch from one service to the other. The new endpoint is still in beta and provides only partial coverage of OpenAI capabilities. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Tue, 12 Nov 2024 19:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "API",
    "Google",
    "OpenAI",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 2988,
  "excerpt": "In an effort to make it easier for developers who adopted OpenAI for their LLM-based solutions to switch to Gemini, Google has launched a new endpoint for its Gemini API that allows them to easily swi",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20241112123128/apple-touch-icon.png",
  "text": "In an effort to make it easier for developers who adopted OpenAI for their LLM-based solutions to switch to Gemini, Google has launched a new endpoint for its Gemini API that allows them to easily switch from one service to the other. The new endpoint is still in beta and provides only partial coverage of OpenAI capabilities. According to Google, their new openai endpoint can replace OpenAI's own endpoint when using direct REST calls or with any of OpenAI's official SDK. For example, if you have a program using the OpenAI SDK, say, in Python, you can replace its initialization code as shown in the following snippet to use Google's models instead of OpenAI's: from openai import OpenAI client = OpenAI( api_key=\"gemini_api_key\", base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\" ) Notice how you will need to provide a Gemini API key, either in the code or through the OPENAI_API_KEY environment variable. To generate text, you can use the Chat Completions API as shown below, where you specify the name of the Gemini model you would like to use: response = client.chat.completions.create( model=\"gemini-1.5-flash\", n=1, messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, { \"role\": \"user\", \"content\": \"Explain to me how AI works\" } ] ) print(response.choices[0].message) The new Gemini endpoint also support OpenAI's Embeddings API, used to measure the relatedness of text strings. In short, the Embeddings API map text into a vector of floating point numbers you can use to search for a specific value, group text into clusters, detect anomalies, make recommendations, and so on. The following snippet shows how you can use it on Gemini with the OpenAI SDK: response = client.embeddings.create( input=\"Your text string goes here\", model=\"text-embedding-004\" ) print(response.data[0].embedding) At the moment, the Chat Completions API and Embeddings API are the only two OpenAI capabilities that you can use on Gemini models through the new openai endpoint. In addition, image upload and structured output have only limited support. Google says they have plans to also add more OpenAI capabilities to make it easier to adopt Gemini as a replacement for OpenAI on existing solutions, but it is not clear in which timeframe. Reddit commentators praised Google's move as a workaround to lock-in for OpenAI's API users, although this is still far away from having a standard API to make it possible to easily switch from one model provider to another. As a more generic approach, the vLLM project aims to support a variety of generative and embedding models and provides an OpenAI compatible server. With vLLM you can use Mistral, Llama, Llava, and many other major models currently available. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2024/11/google-gemini-openai-compatible/en/headerimage/google-gemini-openai-compatible-1731436913166.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eIn an effort to make it easier for developers who adopted OpenAI for their LLM-based solutions to switch to Gemini, \u003ca href=\"https://developers.googleblog.com/en/gemini-is-now-accessible-from-the-openai-library/\"\u003eGoogle has launched a new endpoint for its Gemini API that allows them to easily switch from one service to the other\u003c/a\u003e. The new endpoint is still in beta and provides only partial coverage of OpenAI capabilities.\u003c/p\u003e\n\n\u003cp\u003eAccording to Google, their new \u003ccode\u003eopenai\u003c/code\u003e endpoint can replace OpenAI\u0026#39;s own endpoint when using direct REST calls or with any of \u003ca href=\"https://platform.openai.com/docs/libraries\"\u003eOpenAI\u0026#39;s official SDK\u003c/a\u003e. For example, if you have a program using the OpenAI SDK, say, in Python, you can replace its initialization code as shown in the following snippet to use Google\u0026#39;s models instead of OpenAI\u0026#39;s:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003efrom openai import OpenAI\nclient = OpenAI(\n    api_key=\u0026#34;gemini_api_key\u0026#34;,\n    base_url=\u0026#34;https://generativelanguage.googleapis.com/v1beta/openai/\u0026#34;\n)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eNotice how you will need to provide a Gemini API key, either in the code or through the \u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e environment variable. To generate text, you can use the \u003ca href=\"https://platform.openai.com/docs/guides/text-generation\"\u003eChat Completions API\u003c/a\u003e as shown below, where you specify the name of the Gemini model you would like to use:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003eresponse = client.chat.completions.create(\n    model=\u0026#34;gemini-1.5-flash\u0026#34;,\n    n=1,\n    messages=[\n        {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;},\n        {\n            \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;,\n            \u0026#34;content\u0026#34;: \u0026#34;Explain to me how AI works\u0026#34;\n        }\n    ]\n)\n\nprint(response.choices[0].message)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThe new Gemini endpoint also support \u003ca href=\"https://platform.openai.com/docs/api-reference/embeddings\"\u003eOpenAI\u0026#39;s Embeddings API\u003c/a\u003e, used to measure the relatedness of text strings. In short, the Embeddings API map text into a vector of floating point numbers you can use to search for a specific value, group text into clusters, detect anomalies, make recommendations, and so on. The following snippet shows how you can use it on Gemini with the OpenAI SDK:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003eresponse = client.embeddings.create(\n    input=\u0026#34;Your text string goes here\u0026#34;,\n    model=\u0026#34;text-embedding-004\u0026#34;\n)\n\nprint(response.data[0].embedding)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eAt the moment, the Chat Completions API and Embeddings API are the only two OpenAI capabilities that you can use on Gemini models through the new \u003ccode\u003eopenai\u003c/code\u003e endpoint. In addition, image upload and structured output have only limited support. Google says they have plans to also add more OpenAI capabilities to make it easier to adopt Gemini as a replacement for OpenAI on existing solutions, but it is not clear in which timeframe.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1gn3zhh/gemini_is_now_accessible_from_the_openai_library/\"\u003eReddit commentators\u003c/a\u003e praised Google\u0026#39;s move as a workaround to lock-in for OpenAI\u0026#39;s API users, although this is still far away from having a standard API to make it possible to easily switch from one model provider to another.\u003c/p\u003e\n\n\u003cp\u003eAs a more generic approach, the \u003ca href=\"https://docs.vllm.ai/en\"\u003evLLM project\u003c/a\u003e aims to support a \u003ca href=\"https://docs.vllm.ai/en/latest/models/supported_models.html\"\u003evariety of generative and embedding models\u003c/a\u003e and provides an \u003ca href=\"https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\"\u003eOpenAI compatible server\u003c/a\u003e. With vLLM you can use Mistral, Llama, Llava, and many other major models currently available.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-11-12T00:00:00Z",
  "modifiedTime": null
}
