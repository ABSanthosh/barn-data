{
  "id": "27424dbc-0104-425e-81aa-c265a6090246",
  "title": "How It’s Made: Little Language Lessons uses Gemini’s multilingual capabilities to personalize language learning",
  "link": "https://developers.googleblog.com/en/how-its-made-little-language-lessons-to-personalize-learning/",
  "description": "Little Language Lessons, a project leveraging Gemini's API and Cloud services to generate content, translate, and provide text-to-speech functionalities, includes vocabulary lessons, slang practice, and object recognition for language learning.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Aaron Wade",
  "length": 8901,
  "excerpt": "Little Language Lessons, a project leveraging Gemini's API and Cloud services to generate content, translate, and provide text-to-speech functionalities, includes vocabulary lessons, slang practice, and object recognition for language learning.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "As an engineer, I’ve always been fascinated by languages—both the kind we code in and the kind we speak. Learning a new programming language typically begins by building something tangible, instantly putting theory into practice. Learning a new spoken language, on the other hand, often happens in a vacuum—through textbooks or exercises that feel strangely disconnected from the situations where language actually matters. As is the case with programming, language is best learned through meaningful contexts: the conversations we have, the objects around us, the moments we find ourselves in. Unlike traditional learning tools, AI can adapt to a learner’s context, making it uniquely suited to help us practice languages in ways that feel more natural and personal.This led me, along with a small group of colleagues, to experiment with the Gemini API, which enables developers to access the latest generative models from Google. The result is Little Language Lessons: a collection of three bite-sized learning experiments, all powered by Google’s Gemini models.Experiment 1, Tiny Lesson: Learning what you need, when you need itOne of the most frustrating parts about learning a language is finding yourself in a situation where you need a specific word or phrase—and it’s one that you haven’t learned yet.That’s the idea behind Tiny Lesson. You describe a situation—maybe it’s “asking for directions” or “finding a lost passport”—and receive useful vocabulary, phrases, and grammar tips tailored to that context. Sorry, your browser doesn't support playback for this video We were able to accomplish this using a simple prompt recipe. The prompt begins with a persona-setting preamble that looks like this: You are a(n) {target language} tutor who is bilingual in {target language} and {source language} and an expert at crafting educational content that is custom-tailored to students' language usage goals. Copied In this prompt and in all of the prompts to come, we took advantage of Gemini’s ability to provide outputs as structured JSON, defining desired result as a list of keys in an object: For the given usage context, provide a JSON object containing two keys: \"vocabulary\" and \"phrases\". The value of \"vocabulary\" should be an array of objects, each containing three keys: \"term\", “transliteration”, and \"translation\". The value of \"term\" should be a {target language} word that is highly relevant and useful in the given context. If the language of interest is ordinarily written in the Latin script, the value of “transliteration” should be an empty string. Otherwise, the value of “transliteration” should be a transliteration of the term. The value of \"translation\" should be the {source language} translation of the term. ... Copied In total, each lesson is the result of two calls to the Gemini API. One prompt handles generating all of the vocabulary and phrases, and the other deals with generating relevant grammar topics.And the end of each prompt, we interpolate the user’s desired usage context as follows: INPUT (usage context): {user input} Copied Experiment 2, Slang Hang: Learning to sound less like a textbookThere’s a moment in the journey of learning a language when you start feeling comfortable. You can hold conversations, express yourself, and mostly get by. But then you realize, you still sound… off. Too formal. Stiff.We built Slang Hang to help address this. The idea is simple: generate a realistic conversation between native speakers and let users learn from it. You can watch the dialogue unfold, revealing one message at a time and unpacking unfamiliar terms as they appear. Sorry, your browser doesn't support playback for this video The preamble for the Slang Hang prompt looks like this: You are a screenwriter who is bilingual in {source language} and {target language} and an expert and crafting captivating dialogues. You are also a linguist and highly attuned to the cultural nuances that shape natural speech. Copied Although users can only reveal messages one at a time, everything—the setting, the conversation, the explanations for highlighted terms—is generated from a single call to the Gemini API. We define the structure of the JSON output as follows: Generate a short scene that contains two interlocutors speaking authentic {target language}. Give the result as a JSON object that contains two keys: \"context\" and \"dialogue\". The value of \"context\" should be a short paragraph in {SOURCE LANGUAGE} that describes the setting of the scene, what is happening, who the speakers are, and speakers' relationship to each other. The value of \"dialogue\" should be an array of objects, where each object contains information about a single conversational turn. Each object in the \"dialogue\" array should contain four keys: \"speaker\", \"gender\", \"message\", and \"notes\". ... Copied The dialogue is generated in the user’s target language, but users can also translate messages into their native language (a functionality powered by the Cloud Translation API).One of the more interesting aspects of this experiment is the element of emergent storytelling. Each scene is unique and generated on the fly—it could be a street vendor chatting with a customer, two coworkers meeting on the subway, or even a pair of long-lost friends unexpectedly reuniting at an exotic pet show.That said, we found that this experiment is somewhat susceptible to accuracy errors: it occasionally misuses certain expressions and slang, or even makes them up. LLMs still aren’t perfect, and for that reason it’s important to cross-reference with reliable sources.Experiment 3, Word Cam: Learning from your surroundingsSometimes, you just need words for the things in front of you. It can be extremely humbling to realize just how much you don’t know how to say in your target language. You know the word for “window”, but how do you say “windowsill”? Or “blinds”?Word Cam turns your camera into an instant vocabulary helper. Snap a photo, and Gemini will detect objects, label them in your target language, and give you additional words that you can use to describe them. Sorry, your browser doesn't support playback for this video This experiment leverages Gemini’s vision capabilities for object detection. We send the model an image and ask it for the bounding box coordinates of the different objects in that image: Provide insights about the objects that are present in the given image. Give the result as a JSON object that contains a single key called \"objects\". The value of \"objects\" should be an array of objects whose length is no more than the number of distinct objects present in the image. Each object in the array should contain four keys: \"name\", \"transliteration\", \"translation\", and \"coordinates\". ... The value of \"coordinates\" should be an integer array representing the coordinates of the bounding box for the object. Give the coordinates as [ymin, xmin, ymax, xmax]. Copied Once the user selects an object, we send the cropped image to Gemini in a separate prompt and ask it to generate descriptors for that object in the user’s target language: For the object represented in the given image, provide descriptors that describe the object. Give the result as a JSON object that contains a single key called \"descriptors\". The value of \"descriptors\" should be an array of objects, where each object contains five keys: \"descriptor\", \"transliteration\", \"translation\", \"exampleSentence\", \"exampleSentenceTransliteration\", and \"exampleSentenceTranslation\". ... Copied Across all three experiments, we also integrated text-to-speech functionality, allowing users to hear pronunciations in their target language. We did this using the Cloud Text-to-Speech API, which offers natural-sounding voices for widely spoken languages but limited options for less common ones. Regional accents aren’t well-represented, and thus there are sometimes mismatches between the user’s selected dialect and the accent of the playback.What’s next?Although Little Language Lessons is just an early exploration, experiments like these hint at exciting possibilities for the future. This work also raises a few important questions: what might it look like to collaborate with linguists and educators to refine the approaches we investigate in Little Language Lessons? More broadly, how can AI make independent learning more dynamic and personalized?For now, we’re continuing to explore, iterate, and ask questions. If you’d like to check out more experiments like this one, head over to labs.google.com.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Metadata.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"s61gi\"\u003eAs an engineer, I’ve always been fascinated by languages—both the kind we code in and the kind we speak. Learning a new programming language typically begins by building something tangible, instantly putting theory into practice. Learning a new spoken language, on the other hand, often happens in a vacuum—through textbooks or exercises that feel strangely disconnected from the situations where language actually matters. As is the case with programming, language is best learned through meaningful contexts: the conversations we have, the objects around us, the moments we find ourselves in. Unlike traditional learning tools, AI can adapt to a learner’s context, making it uniquely suited to help us practice languages in ways that feel more natural and personal.\u003c/p\u003e\u003cp data-block-key=\"ai2hu\"\u003eThis led me, along with a small group of colleagues, to experiment with the \u003ca href=\"https://ai.google.dev/\"\u003eGemini API\u003c/a\u003e, which enables developers to access the latest generative models from Google. The result is \u003ca href=\"https://labs.google/lll/en\"\u003eLittle Language Lessons\u003c/a\u003e: a collection of three bite-sized learning experiments, all powered by Google’s Gemini models.\u003c/p\u003e\u003ch2 data-block-key=\"r4jut\" id=\"\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eExperiment 1, Tiny Lesson: Learning what you need, when you need it\u003c/h2\u003e\u003cp data-block-key=\"edr2k\"\u003eOne of the most frustrating parts about learning a language is finding yourself in a situation where you need a specific word or phrase—and it’s one that you haven’t learned yet.\u003c/p\u003e\u003cp data-block-key=\"cnm1i\"\u003eThat’s the idea behind Tiny Lesson. You describe a situation—maybe it’s “asking for directions” or “finding a lost passport”—and receive useful vocabulary, phrases, and grammar tips tailored to that context.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-650saubi_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/01_TinyLessons_BLOG_250418a.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cp data-block-key=\"s61gi\"\u003eWe were able to accomplish this using a simple prompt recipe. The prompt begins with a persona-setting preamble that looks like this:\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003eYou are a(n) {target language} tutor who is bilingual in {target language} and\n {source language} and an expert at crafting educational content that is \ncustom-tailored to students\u0026#39; language usage goals.\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cp data-block-key=\"s61gi\"\u003eIn this prompt and in all of the prompts to come, we took advantage of Gemini’s ability to provide \u003ca href=\"https://ai.google.dev/gemini-api/docs/structured-output\"\u003eoutputs as structured JSON\u003c/a\u003e, defining desired result as a list of keys in an object:\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003eFor the given usage context, provide a JSON object containing two keys:\n\u0026#34;vocabulary\u0026#34; and \u0026#34;phrases\u0026#34;.\n\nThe value of \u0026#34;vocabulary\u0026#34; should be an array of objects, each containing three \nkeys: \u0026#34;term\u0026#34;, “transliteration”,  and \u0026#34;translation\u0026#34;.\n\nThe value of \u0026#34;term\u0026#34; should be a {target language} word that is highly relevant \nand useful in the given context.\nIf the language of interest is ordinarily written in the Latin script, the \nvalue of “transliteration” should be an empty string. Otherwise, the value of\n“transliteration” should be a transliteration of the term.\nThe value of \u0026#34;translation\u0026#34; should be the {source language} translation of \nthe term.\n\n...\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"7k17b\"\u003eIn total, each lesson is the result of two calls to the Gemini API. One prompt handles generating all of the vocabulary and phrases, and the other deals with generating relevant grammar topics.\u003c/p\u003e\u003cp data-block-key=\"f391f\"\u003eAnd the end of each prompt, we interpolate the user’s desired usage context as follows:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003eINPUT (usage context): {user input}\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"xcw9h\" id=\"\"\u003eExperiment 2, Slang Hang: Learning to sound less like a textbook\u003c/h2\u003e\u003cp data-block-key=\"5d5ld\"\u003eThere’s a moment in the journey of learning a language when you start feeling comfortable. You can hold conversations, express yourself, and mostly get by. But then you realize, you still sound… off. Too formal. Stiff.\u003c/p\u003e\u003cp data-block-key=\"9dojc\"\u003eWe built Slang Hang to help address this. The idea is simple: generate a realistic conversation between native speakers and let users learn from it. You can watch the dialogue unfold, revealing one message at a time and unpacking unfamiliar terms as they appear.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-wsaknhbi_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/02_SlangHang_BLOG_V2_250225b.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cp data-block-key=\"7k17b\"\u003eThe preamble for the Slang Hang prompt looks like this:\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003eYou are a screenwriter who is bilingual in {source language} and \n{target language} and an expert and crafting captivating dialogues. \nYou are also a linguist and highly attuned to the cultural nuances that \nshape natural speech.\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cp data-block-key=\"7k17b\"\u003eAlthough users can only reveal messages one at a time, everything—the setting, the conversation, the explanations for highlighted terms—is generated from a single call to the Gemini API. We define the structure of the JSON output as follows:\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003eGenerate a short scene that contains two interlocutors speaking authentic \n{target language}. Give the result as a JSON object that contains two keys: \n\u0026#34;context\u0026#34; and \u0026#34;dialogue\u0026#34;.\n\nThe value of \u0026#34;context\u0026#34; should be a short paragraph in {SOURCE LANGUAGE} \nthat describes the setting of the scene, what is happening, who the speakers \nare, and speakers\u0026#39; relationship to each other.\n\nThe value of \u0026#34;dialogue\u0026#34; should be an array of objects, where each object \ncontains information about a single conversational turn. Each object in the \n\u0026#34;dialogue\u0026#34; array should contain four keys: \u0026#34;speaker\u0026#34;, \u0026#34;gender\u0026#34;, \u0026#34;message\u0026#34;, \nand \u0026#34;notes\u0026#34;.\n\n...\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"7k17b\"\u003eThe dialogue is generated in the user’s target language, but users can also translate messages into their native language (a functionality powered by the \u003ca href=\"https://cloud.google.com/translate\"\u003eCloud Translation API\u003c/a\u003e).\u003c/p\u003e\u003cp data-block-key=\"2oj33\"\u003eOne of the more interesting aspects of this experiment is the element of emergent storytelling. Each scene is unique and generated on the fly—it could be a street vendor chatting with a customer, two coworkers meeting on the subway, or even a pair of long-lost friends unexpectedly reuniting at an exotic pet show.\u003c/p\u003e\u003cp data-block-key=\"eno5g\"\u003eThat said, we found that this experiment is somewhat susceptible to accuracy errors: it occasionally misuses certain expressions and slang, or even makes them up. LLMs still aren’t perfect, and for that reason it’s important to cross-reference with reliable sources.\u003c/p\u003e\u003ch2 data-block-key=\"9txvc\" id=\"\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eExperiment 3, Word Cam: Learning from your surroundings\u003c/h2\u003e\u003cp data-block-key=\"b8lli\"\u003eSometimes, you just need words for the things in front of you. It can be extremely humbling to realize just how much you \u003ci\u003edon’t\u003c/i\u003e know how to say in your target language. You know the word for “window”, but how do you say “windowsill”? Or “blinds”?\u003c/p\u003e\u003cp data-block-key=\"12pvr\"\u003eWord Cam turns your camera into an instant vocabulary helper. Snap a photo, and Gemini will detect objects, label them in your target language, and give you additional words that you can use to describe them.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-e4j8_ttv_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/03_WordCam_BLOG_250418a.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cp data-block-key=\"7k17b\"\u003eThis experiment leverages Gemini’s vision capabilities for object detection. We send the model an image and ask it for the \u003ca href=\"https://ai.google.dev/gemini-api/docs/vision#bbox\"\u003ebounding box coordinates\u003c/a\u003e of the different objects in that image:\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003eProvide insights about the objects that are present in the given image. \nGive the result as a JSON object that contains a single key called \u0026#34;objects\u0026#34;.\n\nThe value of \u0026#34;objects\u0026#34; should be an array of objects whose length is no more \nthan the number of distinct objects present in the image. Each object in the \narray should contain four keys: \u0026#34;name\u0026#34;, \u0026#34;transliteration\u0026#34;, \u0026#34;translation\u0026#34;, and \n\u0026#34;coordinates\u0026#34;.\n\n...\n\nThe value of \u0026#34;coordinates\u0026#34; should be an integer array representing the \ncoordinates of the bounding box for the object. Give the coordinates as [ymin, \nxmin, ymax, xmax].\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cp data-block-key=\"7k17b\"\u003eOnce the user selects an object, we send the cropped image to Gemini in a separate prompt and ask it to generate descriptors for that object in the user’s target language:\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003eFor the object represented in the given image, provide descriptors \nthat describe the object. Give the result as a JSON object that contains \na single key called \u0026#34;descriptors\u0026#34;.\n\nThe value of \u0026#34;descriptors\u0026#34; should be an array of objects, where each \nobject contains five keys: \u0026#34;descriptor\u0026#34;, \u0026#34;transliteration\u0026#34;, \u0026#34;translation\u0026#34;, \n\u0026#34;exampleSentence\u0026#34;, \u0026#34;exampleSentenceTransliteration\u0026#34;, and \n\u0026#34;exampleSentenceTranslation\u0026#34;.\n\n...\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"7k17b\"\u003eAcross all three experiments, we also integrated text-to-speech functionality, allowing users to hear pronunciations in their target language. We did this using the \u003ca href=\"https://cloud.google.com/text-to-speech\"\u003eCloud Text-to-Speech API\u003c/a\u003e, which offers natural-sounding voices for widely spoken languages but limited options for less common ones. Regional accents aren’t well-represented, and thus there are sometimes mismatches between the user’s selected dialect and the accent of the playback.\u003c/p\u003e\u003ch2 data-block-key=\"jvo8o\" id=\"\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhat’s next?\u003c/h2\u003e\u003cp data-block-key=\"6lcrh\"\u003eAlthough Little Language Lessons is just an early exploration, experiments like these hint at exciting possibilities for the future. This work also raises a few important questions: what might it look like to collaborate with linguists and educators to refine the approaches we investigate in \u003ca href=\"https://labs.google/lll/en\"\u003eLittle Language Lessons\u003c/a\u003e? More broadly, how can AI make independent learning more dynamic and personalized?\u003c/p\u003e\u003cp data-block-key=\"3oq19\"\u003eFor now, we’re continuing to explore, iterate, and ask questions. If you’d like to check out more experiments like this one, head over to \u003ca href=\"http://labs.google.com/\"\u003elabs.google.com\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-04-29T00:00:00Z",
  "modifiedTime": null
}
