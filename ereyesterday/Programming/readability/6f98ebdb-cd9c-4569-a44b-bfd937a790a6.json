{
  "id": "6f98ebdb-cd9c-4569-a44b-bfd937a790a6",
  "title": "State Space Models Can Enable AI in Low-Power Edge Computing",
  "link": "https://www.infoq.com/news/2025/07/state-space-models-edge-compute/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "At the the 2025 Embedded Vision Summit, Tony Lewis, chief technology officer at BrainChip, presented research done by his company into state space models (SSMs) and how they can provide LLM capabilities with very low power consumption in limited computing environments, such as those found on dashcams, medical devices, security cameras, and even toys. By Patrick Farry",
  "author": "Patrick Farry",
  "published": "Thu, 24 Jul 2025 14:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "State Space Models",
    "Edge Computing",
    "Embedded Devices",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "Architecture \u0026 Design",
    "news"
  ],
  "byline": "Patrick Farry",
  "length": 3120,
  "excerpt": "At the the 2025 Embedded Vision Summit, Tony Lewis, chief technology officer at BrainChip, presented research done by his company into state space models (SSMs) and how they can provide LLM capabiliti",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250724113909/apple-touch-icon.png",
  "text": "At the the 2025 Embedded Vision Summit, in May 2025, in Santa Clara, California, Tony Lewis, chief technology officer at BrainChip, presented research done by his company into state space models (SSMs) and how they can provide LLM capabilities with very low power consumption in limited computing environments, such as those found on dashcams, medical devices, security cameras, and even toys. He presented the example of the BrainChip TENN 1B LLM using a SSM architecture.  One of the core goals of SSMs is to bypass the context-handling constraints inherent to transformer-based models. They do this by utilizing matrices to generate outputs based on only the last token seen, meaning that all history of the process can be represented by the current state, something called the Markov property. In contrast, transformer models require access to every preceding token, which is stored in the context.  Due to their memoryless nature, state space models can solve for a number of constraints that appear in low-powered computing environments, including better utilization of CPU cache and reduced memory paging, which impact device power consumption and increase costs. They can also use slower read only memory to store the model parameters and state. BrainChip have developed their own model called TENN (Temporal Event-Based Neural Network), which is currently a 1-billion-parameter model with 24 SSM layers that can run with read-only flash memory and under 0.5 watts power consumption, while returning results in under 100 ms. Lewis explained that these surprising metrics are the result of the Markov property of the TENN model, saying “One cool thing about the state space model is that the actual cache used is incredibly small, so in the terms of a transformer based model, you don’t have a compact state, what you have to remember is a representation of everything that has come before”. Additionally, BrainChip is working on quantizing the model to 4 bits, so that it will efficiently run on edge device hardware. In benchmark tests conducted by BrainChip, the TENN model compares favorably to Llama 3.2 1B, although Lewis cautions that the performance of the TENN model depends on the particular application, and he recommends the use of a RAG application architecture to guard against hallucinations.  SSMs are an active area of research and seem particularly promising where there are computing resource constraints or high performance requirements. Their unique characteristics could unlock a new generation of edge devices, enabling sophisticated AI capabilities previously confined to the cloud. See the InfoQ article “The State Space Solution to Hallucinations: How State Space Models are Slicing the Competition” for more information on how SSM models perform compared to Transformer models.  A technical overview of state space models and how they work can be found in the Hugging Face blog post Introduction to State Space Models (SSM). About the Author Patrick Farry",
  "image": "https://res.infoq.com/news/2025/07/state-space-models-edge-compute/en/headerimage/state-space-model-low-power-1753257525965.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eAt the the 2025 Embedded Vision Summit, in May 2025, in Santa Clara, California, Tony Lewis, chief technology officer at BrainChip, presented \u003ca href=\"https://www.edge-ai-vision.com/2025/06/state-space-models-vs-transformers-for-ultra-low-power-edge-ai-a-presentation-from-brainchip/\"\u003eresearch done by his company into state space models (SSMs) and how they can provide LLM capabilities\u003c/a\u003e with very low power consumption in limited computing environments, such as those found on dashcams, medical devices, security cameras, and even toys. He presented the example of the \u003ca href=\"https://www.edge-ai-vision.com/2025/06/state-space-models-vs-transformers-for-ultra-low-power-edge-ai-a-presentation-from-brainchip/\"\u003eBrainChip TENN 1B LLM using a SSM architecture\u003c/a\u003e. \u003c/p\u003e\n\n\u003cp\u003eOne of the core goals of SSMs is to bypass the context-handling constraints inherent to transformer-based models. They do this by utilizing matrices to generate outputs based on only the last token seen, meaning that all history of the process can be represented by the current state, something called the Markov property. In contrast, transformer models require access to every preceding token, which is stored in the context. \u003c/p\u003e\n\n\u003cp\u003eDue to their memoryless nature, state space models can solve for a number of constraints that appear in low-powered computing environments, including better utilization of CPU cache and reduced memory paging, which impact device power consumption and increase costs. They can also use slower read only memory to store the model parameters and state.\u003c/p\u003e\n\n\u003cp\u003eBrainChip have developed their own model called \u003ca href=\"https://brainchip.com/wp-content/uploads/2023/06/TENNs_Whitepaper_Final.pdf\"\u003eTENN (Temporal Event-Based Neural Network)\u003c/a\u003e, which is currently a 1-billion-parameter model with 24 SSM layers that can run with read-only flash memory and under 0.5 watts power consumption, while returning results in under 100 ms.\u003c/p\u003e\n\n\u003cp\u003eLewis explained that these surprising metrics are the result of the Markov property of the TENN model, saying “One cool thing about the state space model is that the actual cache used is incredibly small, so in the terms of a transformer based model, you don’t have a compact state, what you have to remember is a representation of everything that has come before”.\u003c/p\u003e\n\n\u003cp\u003eAdditionally, BrainChip is working on quantizing the model to 4 bits, so that it will efficiently run on edge device hardware.\u003c/p\u003e\n\n\u003cp\u003eIn benchmark tests conducted by BrainChip, the TENN model compares favorably to Llama 3.2 1B, although Lewis cautions that the performance of the TENN model depends on the particular application, and he recommends the use of a RAG application architecture to guard against hallucinations. \u003c/p\u003e\n\n\u003cp\u003eSSMs are an active area of research and seem particularly promising where there are computing resource constraints or high performance requirements. Their unique characteristics could unlock a new generation of edge devices, enabling sophisticated AI capabilities previously confined to the cloud. See the InfoQ article “\u003ca href=\"https://www.infoq.com/articles/state-space-solution-to-hallucinations-state-space-models/\"\u003eThe State Space Solution to Hallucinations: How State Space Models are Slicing the Competition\u003c/a\u003e” for more information on how SSM models perform compared to Transformer models. \u003c/p\u003e\n\n\u003cp\u003eA technical overview of state space models and how they work can be found in the Hugging Face blog post \u003ca href=\"https://huggingface.co/blog/lbourdois/get-on-the-ssm-train\"\u003eIntroduction to State Space Models (SSM)\u003c/a\u003e.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Patrick-Farry\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003ePatrick Farry\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-07-24T00:00:00Z",
  "modifiedTime": null
}
