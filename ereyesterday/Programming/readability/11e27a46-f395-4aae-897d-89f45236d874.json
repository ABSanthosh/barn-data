{
  "id": "11e27a46-f395-4aae-897d-89f45236d874",
  "title": "Four approaches to creating a specialized LLM",
  "link": "https://stackoverflow.blog/2024/12/05/four-approaches-to-creating-a-specialized-llm/",
  "description": "Wondering how to go about creating an LLM that understands your custom data? Start here.",
  "author": "Cameron R. Wolfe, PhD",
  "published": "Thu, 05 Dec 2024 17:00:00 GMT",
  "source": "https://stackoverflow.blog/feed/",
  "categories": [
    "se-tech",
    "se-stackoverflow",
    "llm",
    "ai"
  ],
  "byline": "Cameron R. Wolfe, PhD",
  "length": 2353,
  "excerpt": "Trying to create a language model that understands your own custom data? Here are four techniques you can use to create a specialized LLM, ordered in terms of the amount of complexity/compute involved.",
  "siteName": "",
  "favicon": "https://stackoverflow.blog/apple-touch-icon.png",
  "text": "Trying to create a language model that understands your own custom data? Here are four techniques you can use to create a specialized LLM, ordered in terms of the amount of complexity/compute involved.TL;DR: When trying to solve problems with language models, we should start simple and only introduce complexity as needed. Along these lines, we can just try to prompt the model first, then try a RAG approach. If this doesn’t work, we can then finetune the model, starting with LoRA (i.e., a much cheaper finetuning approach) instead of end-to-end finetuning.(1) Prompting: The first step in attempting to solve a problem with an LLM is just writing a prompt! Start with a simple prompt, try adding few-shot exemplars, test different instructions, and potentially even use a more complex prompting approach (e.g., chain of thought prompting). If your desired application can be solved via prompting, this is the easiest approach in terms of time/effort.(2) Retrieval Augmented Generation (RAG) is still just a form of prompting. However, we retrieve extra context to include in the language model’s prompt. First, we take all of the domain-specific data we have, split it into chunks and index/store these chunks for search (i.e., a reverse index and/or a vector database). Then, we can retrieve relevant data to include in the model’s prompt when generating output to reduce hallucinations.(3) LoRA is a parameter-efficient finetuning technique that decomposes the weight update derived from finetuning into a low rank decomposition. By doing this, we minimize the number of trainable parameters during finetuning, thus drastically reducing memory overhead. However, we can achieve comparable performance to full finetuning and add no extra inference latency because this low rank weight update can be “baked in” to the existing weight matrix.(4) Full Finetuning: If none of the above techniques work, one of the last strategies that we can try is finetuning the language model end-to-end. To make this successful, we want to curate a large corpus of data that contains useful knowledge relevant to problems that we are trying to solve. Then, we can further train the full model using a next token prediction strategy that is similar to pretraining, thus specializing the model over the specific domain of knowledge contained in the finetuning corpus.",
  "image": "https://cdn.stackoverflow.co/images/jo7n4k8s/production/1842af09277124c00aa4c015f2852dba5b225e1a-3000x1575.jpg?w=1200\u0026fm=png\u0026auto=format",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv itemprop=\"articleBody\"\u003e\u003cp\u003eTrying to create a language model that understands your own custom data? Here are four techniques you can use to create a specialized LLM, ordered in terms of the amount of complexity/compute involved.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTL;DR:\u003c/strong\u003e When trying to solve problems with language models, we should start simple and only introduce complexity as needed. Along these lines, we can just try to prompt the model first, then try a \u003ca href=\"https://stackoverflow.blog/2024/08/15/practical-tips-for-retrieval-augmented-generation-rag/\"\u003eRAG approach\u003c/a\u003e. If this doesn’t work, we can then \u003ca href=\"https://stackoverflow.blog/2024/10/31/a-brief-summary-of-language-model-finetuning/\"\u003efinetune\u003c/a\u003e the model, starting with LoRA (i.e., a much cheaper finetuning approach) instead of end-to-end finetuning.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e(1) Prompting: \u003c/strong\u003eThe first step in attempting to solve a problem with an LLM is just writing a prompt! Start with a simple prompt, try adding few-shot exemplars, test different instructions, and potentially even use a more complex prompting approach (e.g., chain of thought prompting). If your desired application can be solved via prompting, this is the easiest approach in terms of time/effort.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e(2) Retrieval Augmented Generation (RAG)\u003c/strong\u003e is still just a form of prompting. However, we retrieve extra context to include in the language model’s prompt. First, we take all of the domain-specific data we have, split it into chunks and index/store these chunks for search (i.e., a reverse index and/or a vector database). Then, we can retrieve relevant data to include in the model’s prompt when generating output to reduce hallucinations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e(3) LoRA\u003c/strong\u003e is a parameter-efficient finetuning technique that decomposes the weight update derived from finetuning into a low rank decomposition. By doing this, we minimize the number of trainable parameters during finetuning, thus drastically reducing memory overhead. However, we can achieve comparable performance to full finetuning and add no extra inference latency because this low rank weight update can be “baked in” to the existing weight matrix.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e(4) Full Finetuning:\u003c/strong\u003e If none of the above techniques work, one of the last strategies that we can try is finetuning the language model end-to-end. To make this successful, we want to curate a large corpus of data that contains useful knowledge relevant to problems that we are trying to solve. Then, we can further train the full model using a next token prediction strategy that is similar to pretraining, thus specializing the model over the specific domain of knowledge contained in the finetuning corpus.\u003c/p\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://cdn.stackoverflow.co/images/jo7n4k8s/production/f146e2effe0240b66dfec5e8f28dde5c98bf88ce-2400x1348.jpg?auto=format\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": null,
  "modifiedTime": null
}
