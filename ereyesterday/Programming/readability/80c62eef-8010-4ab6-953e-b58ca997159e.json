{
  "id": "80c62eef-8010-4ab6-953e-b58ca997159e",
  "title": "Vercel Fluid: A New Compute Model and an Alternative to Serverless?",
  "link": "https://www.infoq.com/news/2025/03/vercel-fluid/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Vercel has recently introduced Vercel Fluid, an elastic compute model that allows a single worker to handle multiple requests, similar to a traditional server, while preserving the elasticity of serverless. By scaling functions before instances, Fluid maximizes available compute time, optimizing compute footprint and resource efficiency for long-running tasks and AI inference. By Renato Losio",
  "author": "Renato Losio",
  "published": "Sun, 09 Mar 2025 07:27:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Serverless",
    "Cost Optimization",
    "AWS Lambda",
    "Cloud",
    "Development",
    "Architecture \u0026 Design",
    "news"
  ],
  "byline": "Renato Losio",
  "length": 3957,
  "excerpt": "Vercel has recently introduced Vercel Fluid, an elastic compute model that allows a single worker to handle multiple requests, similar to a traditional server, while preserving the elasticity of serve",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250306134231/apple-touch-icon.png",
  "text": "Vercel has recently introduced Vercel Fluid, an elastic compute model that allows a single worker to handle multiple requests, similar to a traditional server, while preserving the elasticity of serverless. By scaling functions before instances, Fluid maximizes available compute time, optimizing compute footprint and resource efficiency for long-running tasks and AI inference. According to the development team, functions with Fluid compute prioritize existing resources before creating new instances, eliminating hard scaling limits and leveraging warm compute for more efficient scaling. This allows shifting to a many-to-one model that can handle tens of thousands of concurrent invocations on a single function. Source: Vercel blog Vercel claims the new model offers several benefits including cold start prevention, efficient auto-scaling, horizontal \u0026 vertical concurrency, and optimized I/O efficiency, all with a pay-as-you-go pricing model. Jones Zachariah Noel N, senior developer advocate at Freshworks and AWS Serverless Hero, questions if Fluid Compute is the next big thing: Vercel is bringing the best of the server-based approach for a cost efficiency along with a runtime power, best of a Developer Experience (DevX) and security of Serverless. As Vercel calls it - the power of Servers in the Serverless way, which additionally addresses the Cold Start problem and having a function ready for execution. Reducing the need to spin up a function for each incoming request, in-function concurrency reduces both the chance of paying for idle compute time and the likelihood of hitting a cold start. Tackling the problem of idle compute time is especially important in scenarios where the downstream service is a slow responder, either by nature (LLMs) or because of performance issues. Claiming to optimize performance and cost, the new option relies on compute triggers only when needed, with real-time scaling from zero to peak traffic and existing resources being used before scaling new ones. Vercel Fluid is designed for tasks like video streaming and post-response processing that might have high response times but low spikes of CPU usage. Malte Ubl, Vercel CTO, explains on Hacker News the main difference between Vercel Fluid and traditional serverless approaches: The big difference is how the microvm is utilized. Lambda reserves the entire VM to handle a request end to end. Fluid can use a VM for multiple concurrent requests. Since most workloads are often idle waiting for IO, this ends up being much more efficient. To support the new functionality, Fluid Compute introduces the waitUntil API to handle tasks after the HTTP response is sent, providing an observability dashboard that includes metrics such as execution time, concurrency levels, cold start occurrences, and overall compute utilization. In \"Vercel’s Fluid Compute and what it means for AWS Lambda\", Andreas Casen writes: Whether AWS Lambda has yet to figure out in-function concurrency or already has and simply chose to not \"pass the savings on\", Fluid Compute provides a competitive edge in terms of cost efficiency, and it’s hard for me to imagine Lambda won’t keep up. The ball is now in AWS’s court, will they respond? The new option has been discussed in a popular Reddit thread, where a user warns: As someone that runs a modest SaaS business with the frontend on Vercel, the pricing model changes are just another source of fatigue these days. I understand Vercel iterating their revenue model, but as an end user another thing that is promising \"reduce your compute costs\" via some vague concept called \"fluid compute\" is honestly just annoying. Functions are billed according to GB-hours, determined by the memory allocated to the function and the duration of the execution. About the Author Renato Losio",
  "image": "https://res.infoq.com/news/2025/03/vercel-fluid/en/headerimage/generatedHeaderImage-1741252714837.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eVercel has recently introduced \u003ca href=\"https://vercel.com/resources/fluid-a-new-compute-model-for-modern-workloads\"\u003eVercel Fluid\u003c/a\u003e, an elastic compute model that allows a single worker to handle multiple requests, similar to a traditional server, while preserving the elasticity of serverless. By scaling functions before instances, Fluid maximizes available compute time, optimizing compute footprint and resource efficiency for long-running tasks and AI inference.\u003c/p\u003e\n\n\u003cp\u003eAccording to the development team, functions with \u003ca href=\"https://vercel.com/docs/functions/fluid-compute\"\u003eFluid compute\u003c/a\u003e prioritize existing resources before creating new instances, eliminating hard scaling limits and leveraging warm compute for more efficient scaling. This allows shifting to a many-to-one model that can handle tens of thousands of concurrent invocations on a single function.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"Vercel Fluid\" data-src=\"news/2025/03/vercel-fluid/en/resources/1Traditional_vs_ICF__1_-1741252835008.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/03/vercel-fluid/en/resources/1Traditional_vs_ICF__1_-1741252835008.png\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eSource: Vercel blog\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eVercel claims the new model offers several benefits including cold start prevention, efficient auto-scaling, horizontal \u0026amp; vertical concurrency, and optimized I/O efficiency, all with a pay-as-you-go pricing model. Jones Zachariah Noel N, senior developer advocate at Freshworks and AWS Serverless Hero, \u003ca href=\"https://www.theserverlessterminal.com/p/is-fluid-compute-the-next-thing-73\"\u003equestions\u003c/a\u003e if Fluid Compute is the next big thing:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eVercel is bringing the best of the server-based approach for a cost efficiency along with a runtime power, best of a Developer Experience (DevX) and security of Serverless. As Vercel calls it - the power of Servers in the Serverless way, which additionally addresses the Cold Start problem and having a function ready for execution.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eReducing the need to spin up a function for each incoming request, in-function concurrency reduces both the chance of paying for idle compute time and the likelihood of hitting a cold start. Tackling the problem of idle compute time is especially important in scenarios where the downstream service is a slow responder, either by nature (LLMs) or because of performance issues.\u003c/p\u003e\n\n\u003cp\u003eClaiming to optimize performance and cost, the new option relies on compute triggers only when needed, with real-time scaling from zero to peak traffic and existing resources being used before scaling new ones. Vercel Fluid is designed for tasks like video streaming and post-response processing that might have high response times but low spikes of CPU usage. Malte Ubl, Vercel CTO, explains on \u003ca href=\"https://news.ycombinator.com/item?id=43067938\"\u003eHacker News\u003c/a\u003e the main difference between Vercel Fluid and traditional serverless approaches:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe big difference is how the microvm is utilized. Lambda reserves the entire VM to handle a request end to end. Fluid can use a VM for multiple concurrent requests. Since most workloads are often idle waiting for IO, this ends up being much more efficient.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eTo support the new functionality, Fluid Compute introduces the \u003ccode\u003ewaitUntil\u003c/code\u003e API to handle tasks after the HTTP response is sent, providing an observability dashboard that includes metrics such as execution time, concurrency levels, cold start occurrences, and overall compute utilization. In \u0026#34;\u003ca href=\"https://dev.to/andycasen/vercels-fluid-compute-and-what-it-means-for-aws-lambda-38dl\"\u003eVercel’s Fluid Compute and what it means for AWS Lambda\u003c/a\u003e\u0026#34;, Andreas Casen writes:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhether AWS Lambda has yet to figure out in-function concurrency or already has and simply chose to not \u0026#34;pass the savings on\u0026#34;, Fluid Compute provides a competitive edge in terms of cost efficiency, and it’s hard for me to imagine Lambda won’t keep up. The ball is now in AWS’s court, will they respond?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe new option has been discussed in a \u003ca href=\"https://www.reddit.com/r/vercel/comments/1iho1yk/introducing_fluid_compute_the_power_of_servers_in/\"\u003epopular Reddit thread\u003c/a\u003e, where a user warns:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAs someone that runs a modest SaaS business with the frontend on Vercel, the pricing model changes are just another source of fatigue these days. I understand Vercel iterating their revenue model, but as an end user another thing that is promising \u0026#34;reduce your compute costs\u0026#34; via some vague concept called \u0026#34;fluid compute\u0026#34; is honestly just annoying.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eFunctions are \u003ca href=\"https://vercel.com/docs/functions/usage-and-pricing#usage-\u0026amp;-pricing-for-functions\"\u003ebilled according to GB-hours\u003c/a\u003e, determined by the memory allocated to the function and the duration of the execution.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Renato-Losio\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRenato Losio\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-03-09T00:00:00Z",
  "modifiedTime": null
}
