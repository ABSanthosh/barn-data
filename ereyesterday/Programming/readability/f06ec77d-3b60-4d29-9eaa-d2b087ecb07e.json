{
  "id": "f06ec77d-3b60-4d29-9eaa-d2b087ecb07e",
  "title": "Gemini API I/O updates",
  "link": "https://developers.googleblog.com/en/gemini-api-io-updates/",
  "description": "Announcing new features and models for the Gemini API, with the introduction of Gemini 2.5 Flash Preview with improved reasoning and efficiency, Gemini 2.5 Pro and Flash text-to-speech supporting multiple languages and speakers, and Gemini 2.5 Flash native audio dialog for conversational AI.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Shrestha Basu Mallick, Logan Kilpatrick, Alisa Fortin, Ivan Solovyev",
  "length": 8732,
  "excerpt": "Announcing new features and models for the Gemini API, with the introduction of Gemini 2.5 Flash Preview with improved reasoning and efficiency, Gemini 2.5 Pro and Flash text-to-speech supporting multiple languages and speakers, and Gemini 2.5 Flash native audio dialog for conversational AI.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "The Gemini API offers developers a streamlined way to build innovative applications with cutting-edge generative AI models. Google AI Studio simplifies this process of testing all the API capabilities allowing for rapid prototyping and experimentation with text, image, and even video prompts. When developers want to test and build at scale they can leverage all the capabilities available through the Gemini API.New models available through the APIGemini 2.5 Flash Preview - We’ve added a new 2.5 Flash preview (gemini-2.5-flash-preview-05-20) which is better over the previous preview at reasoning, code, and long context. This version of 2.5 Flash is currently #2 on the LMarena leaderboard behind only 2.5 Pro. We’ve also improved Flash cost-efficiency with this latest update reducing the number of tokens needed for the same performance, resulting in 22% efficiency gains on our evals. Our goal is to keep improving based on your feedback, and make both generally available soon.Gemini 2.5 Pro and Flash text-to-speech (TTS) - We also announced 2.5 Pro and Flash previews for text-to-speech (TTS) that support native audio output for both single and multiple speakers, across 24 languages. With these models, you can control TTS expression and style, creating rich audio output. With multispeaker, you can generate conversations with multiple distinct voices for dynamic interactions.Gemini 2.5 Flash native audio dialog - In preview, this model is available via the Live API to generate natural sounding voices for conversation, in over 30 distinct voices and 24+ languages. We’ve also added proactive audio so the model can distinguish between the speaker and background conversations, so it knows when to respond. In addition, the model responds appropriately to a user's emotional expression and tone. A separate thinking model enables more complex queries. This now makes it possible for you to build conversational AI agents and experiences that feel more intuitive and natural, like enhancing call center interactions, developing dynamic personas, crafting unique voice characters, and more.Lyria RealTime - Live music generation is now available in the Gemini API and Google AI Studio to create a continuous stream of instrumental music using text prompts. With Lyria RealTime, we use WebSockets to establish a persistent, real-time communication channel. The model continuously produces music in small, flowing chunks and adapts based on inputs. Imagine adding a responsive soundtrack to your app or designing a new type of musical instrument! Try out Lyria RealTime with the PromptDJ-MIDI app in Google AI Studio.Gemini 2.5 Pro Deep Think - We are also testing an experimental reasoning mode for 2.5 Pro. We’ve seen incredible performance with these Deep Thinking capabilities for highly complex math and coding prompts. We look forward to making it broadly available for you to experiment with soon.Gemma 3n - Gemma 3n is a generative AI open model optimized for use in everyday devices, such as phones, laptops, and tablets. It can handle text, audio and vision inputs. This model includes innovations in parameter-efficient processing, including Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture that provides the flexibility to reduce compute and memory requirements.New functionality in the APIThought summariesTo help developers understand and debug model responses, we’ve added thought summaries for 2.5 Pro and Flash in the Gemini API. We take the model’s raw thoughts and synthesize them into a helpful summary with headers, relevant details and tool calls. The raw chain-of-thoughts in Google AI Studio has also been updated with the new thought summaries.Thinking budgetsWe launched 2.5 Flash with thinking budgets to provide developers control over how much models think to balance performance, latency, and cost for the apps they are building. We will be extending this capability to 2.5 Pro soon. from google import genai from google.genai import types client = genai.Client(api_key=\"GOOGLE_API_KEY\") prompt = \"What is the sum of the first 50 prime numbers?\" response = client.models.generate_content( model=\"gemini-2.5-flash-preview-05-20\", contents=prompt, config=types.GenerateContentConfig( thinking_config=types.ThinkingConfig(thinking_budget=1024, include_thoughts=True ) ) ) for part in response.candidates[0].content.parts: if not part.text: continue if part.thought: print(\"Thought summary:\") print(part.text) print() else: print(\"Answer:\") print(part.text) print() Python Copied Sample code to enable and retrieve thought summaries without streaming, returning a final thought summary with the response. New URL Context toolWe added a new experimental tool, URL context, to retrieve more context from links that you provide. This can be used by itself or in conjunction with other tools such as Grounding with Google Search. This tool is a key building block for developers looking to build their own version of research agents with the Gemini API. from google import genai from google.genai.types import Tool, GenerateContentConfig, GoogleSearch client = genai.Client() model_id = \"gemini-2.5-flash-preview-05-20\" tools = [] tools.append(Tool(url_context=types.UrlContext)) tools.append(Tool(google_search=types.GoogleSearch)) response = client.models.generate_content( model=model_id, contents=\"Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.\", config=GenerateContentConfig( tools=tools, response_modalities=[\"TEXT\"], ) ) for each in response.candidates[0].content.parts: print(each.text) # get URLs retrieved for context print(response.candidates[0].url_context_metadata) Python Copied Sample code for Grounding with Google Search and URL Context Computer use toolWe're bringing Project Mariner's browser control capabilities to the Gemini API via a new computer use tool. To make it easier for developers to use this tool, we are enabling the creation of Cloud Run instances optimally configured for running browser control agents via one click from Google AI Studio. We’ve begun early testing with companies like Automation Anywhere, UiPath and Browserbase. Their valuable feedback will be instrumental in refining its capabilities for a broader experimental developer release this summer.Improvements to structured outputsThe Gemini API now has broader support for JSON Schema, including much-requested keywords such as \"$ref\" (for references) and those enabling the definition of tuple-like structures (e.g., prefixItems).Video understanding improvementsThe Gemini API now allows YouTube video URLs or video uploads to be added to a prompt, enabling users to to summarize, translate, or analyze the video content. With this recent update, the API supports video clipping, enabling flexibility in analyzing specific parts of a video. This is particularly beneficial for videos longer than 8 hours. We have also added support for dynamic frames per second (FPS), allowing 60 FPS for videos like games or sports where speed is critical, and 0.1 FPS for videos where speed is less of a priority. To help users save tokens, we have also introduced support for 3 different video resolutions: high (720p), standard (480p), and low (360p).Async function callingThe cascaded architecture in the Live API now supports asynchronous function calling, ensuring user conversations remain smooth and uninterrupted. This means your Live agent can continue generating responses even while it's busy executing functions in the background, by simply adding the behavior field to the function definition and setting it to NON-BLOCKING. Read more about this in the Gemini API developer documentation.Batch APIWe are also testing a new API, which lets you easily batch up your requests and get them back in a max 24 hour turnaround time. The API will come at half the price of the interactive API and with much higher rate limits. We hope to roll that out more widely later this summer.Start buildingThat’s a wrap on I/O for this year! With the Gemini API and Google AI Studio, you can turn your ideas into reality, whether you're building conversational AI agents with natural-sounding audio or developing tools to analyze and generate code. As always, check out the Gemini API developer docs for all the latest code samples and more.Explore this announcement and all Google I/O 2025 updates on io.google.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/metadata.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"4hm9k\"\u003eThe Gemini API offers developers a streamlined way to build innovative applications with cutting-edge generative AI models. \u003ca href=\"https://developers.googleblog.com/en/google-ai-studio-native-code-generation-agentic-tools-upgrade\"\u003eGoogle AI Studio simplifies this process\u003c/a\u003e of testing all the API capabilities allowing for rapid prototyping and experimentation with text, image, and even video prompts. When developers want to test and build at scale they can leverage all the capabilities available through the Gemini API.\u003c/p\u003e\u003ch2 data-block-key=\"4ckcc\" id=\"new-models-available-through-the-api\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNew models available through the API\u003c/h2\u003e\u003cp data-block-key=\"3434o\"\u003e\u003cb\u003eGemini 2.5 Flash Preview\u003c/b\u003e - We’ve added a new 2.5 Flash preview (gemini-2.5-flash-preview-05-20) which is better over the previous preview at reasoning, code, and long context. This version of 2.5 Flash is currently #2 on the LMarena leaderboard behind only 2.5 Pro. We’ve also improved Flash cost-efficiency with this latest update reducing the number of tokens needed for the same performance, resulting in 22% efficiency gains on our evals. Our goal is to keep improving based on your feedback, and make both generally available soon.\u003c/p\u003e\u003cp data-block-key=\"5nv6d\"\u003e\u003cb\u003eGemini 2.5 Pro and Flash text-to-speech (TTS)\u003c/b\u003e - We also announced 2.5 Pro and Flash previews for text-to-speech (TTS) that support native audio output for both single and multiple speakers, across 24 languages. With these models, you can control TTS expression and style, creating rich audio output. With multispeaker, you can generate conversations with multiple distinct voices for dynamic interactions.\u003c/p\u003e\u003cp data-block-key=\"2i0d6\"\u003e\u003cb\u003eGemini 2.5 Flash native audio dialog\u003c/b\u003e - In preview, this model is available via the Live API to generate natural sounding voices for conversation, in over 30 distinct voices and 24+ languages. We’ve also added proactive audio so the model can distinguish between the speaker and background conversations, so it knows when to respond. In addition, the model responds appropriately to a user\u0026#39;s emotional expression and tone. A separate thinking model enables more complex queries. This now makes it possible for you to build conversational AI agents and experiences that feel more intuitive and natural, like enhancing call center interactions, developing dynamic personas, crafting unique voice characters, and more.\u003c/p\u003e\u003cp data-block-key=\"3gtfm\"\u003e\u003cb\u003eLyria RealTime -\u003c/b\u003e Live music generation is now available in the Gemini API and Google AI Studio to create a continuous stream of instrumental music using text prompts. With Lyria RealTime, we use WebSockets to establish a persistent, real-time communication channel. The model continuously produces music in small, flowing chunks and adapts based on inputs. Imagine adding a responsive soundtrack to your app or designing a new type of musical instrument! Try out Lyria RealTime with the \u003ca href=\"https://aistudio.google.com/app/apps/bundled/promptdj-midi\"\u003ePromptDJ-MIDI\u003c/a\u003e app in Google AI Studio.\u003c/p\u003e\u003cp data-block-key=\"ak9di\"\u003e\u003cb\u003eGemini 2.5 Pro Deep Think\u003c/b\u003e - We are also testing an experimental reasoning mode for 2.5 Pro. We’ve seen incredible performance with these Deep Thinking capabilities for highly complex math and coding prompts. We look forward to making it broadly available for you to experiment with soon.\u003c/p\u003e\u003cp data-block-key=\"rnc2\"\u003e\u003cb\u003eGemma 3n -\u003c/b\u003e Gemma 3n is a generative AI open model optimized for use in everyday devices, such as phones, laptops, and tablets. It can handle text, audio and vision inputs. This model includes innovations in parameter-efficient processing, including Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture that provides the flexibility to reduce compute and memory requirements.\u003c/p\u003e\u003ch2 data-block-key=\"r0oa0\" id=\"new-functionality-in-the-api\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNew functionality in the API\u003c/h2\u003e\u003ch3 data-block-key=\"mzb57\" id=\"thought-summaries\"\u003e\u003cb\u003eThought summaries\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"cuogh\"\u003eTo help developers understand and debug model responses, we’ve added thought summaries for 2.5 Pro and Flash in the Gemini API. We take the model’s raw thoughts and synthesize them into a helpful summary with headers, relevant details and tool calls. The raw chain-of-thoughts in Google AI Studio has also been updated with the new thought summaries.\u003c/p\u003e\u003ch3 data-block-key=\"4l2co\" id=\"thinking-budgets\"\u003e\u003cb\u003e\u003cbr/\u003eThinking budgets\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"e8fsf\"\u003eWe launched 2.5 Flash with thinking budgets to provide developers control over how much models think to balance performance, latency, and cost for the apps they are building. We will be extending this capability to 2.5 Pro soon.\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003efrom google import genai\nfrom google.genai import types\n\nclient = genai.Client(api_key=\u0026#34;GOOGLE_API_KEY\u0026#34;)\nprompt = \u0026#34;What is the sum of the first 50 prime numbers?\u0026#34;\nresponse = client.models.generate_content(\n  model=\u0026#34;gemini-2.5-flash-preview-05-20\u0026#34;,\n  contents=prompt,\n  config=types.GenerateContentConfig(\n    thinking_config=types.ThinkingConfig(thinking_budget=1024,\n      include_thoughts=True\n    )\n  )\n)\n\nfor part in response.candidates[0].content.parts:\n  if not part.text:\n    continue\n  if part.thought:\n    print(\u0026#34;Thought summary:\u0026#34;)\n    print(part.text)\n    print()\n  else:\n    print(\u0026#34;Answer:\u0026#34;)\n    print(part.text)\n    print()\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        Python\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n        \n            \u003cp\u003eSample code to enable and retrieve thought summaries without streaming, returning a final thought summary with the response.\u003c/p\u003e\n        \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"dgrhn\" id=\"\"\u003e\u003cb\u003eNew URL Context tool\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"84kca\"\u003eWe added a new experimental tool, URL context, to retrieve more context from links that you provide. This can be used by itself or in conjunction with other tools such as \u003ca href=\"https://ai.google.dev/gemini-api/docs/grounding\"\u003eGrounding with Google Search\u003c/a\u003e. This tool is a key building block for developers looking to build their own version of research agents with the Gemini API.\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003efrom google import genai\nfrom google.genai.types import Tool, GenerateContentConfig, GoogleSearch\n\nclient = genai.Client()\nmodel_id = \u0026#34;gemini-2.5-flash-preview-05-20\u0026#34;\n\ntools = []\ntools.append(Tool(url_context=types.UrlContext))\ntools.append(Tool(google_search=types.GoogleSearch))\n\nresponse = client.models.generate_content(\n    model=model_id,\n    contents=\u0026#34;Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.\u0026#34;,\n    config=GenerateContentConfig(\n        tools=tools,\n        response_modalities=[\u0026#34;TEXT\u0026#34;],\n    )\n)\n\nfor each in response.candidates[0].content.parts:\n    print(each.text)\n# get URLs retrieved for context\nprint(response.candidates[0].url_context_metadata)\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        Python\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n        \n            \u003cp\u003eSample code for Grounding with Google Search and URL Context\u003c/p\u003e\n        \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"zetga\" id=\"computer-use-tool\"\u003e\u003cb\u003eComputer use tool\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"ei5tq\"\u003eWe\u0026#39;re bringing \u003ca href=\"https://deepmind.google/models/project-mariner/\"\u003eProject Mariner\u0026#39;s\u003c/a\u003e browser control capabilities to the Gemini API via a new computer use tool. To make it easier for developers to use this tool, we are enabling the creation of Cloud Run instances optimally configured for running browser control agents via one click from Google AI Studio. We’ve begun early testing with companies like Automation Anywhere, UiPath and Browserbase. Their valuable feedback will be instrumental in refining its capabilities for a broader experimental developer release this summer.\u003c/p\u003e\u003ch3 data-block-key=\"qqgpr\" id=\"improvements-to-structured-outputs\"\u003e\u003cb\u003e\u003cbr/\u003eImprovements to structured outputs\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"a733\"\u003eThe Gemini API now has broader support for JSON Schema, including much-requested keywords such as \u0026#34;$ref\u0026#34; (for references) and those enabling the definition of tuple-like structures (e.g., prefixItems).\u003c/p\u003e\u003ch3 data-block-key=\"t1a65\" id=\"video-understanding-improvements\"\u003e\u003cb\u003e\u003cbr/\u003eVideo understanding improvements\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"9307a\"\u003eThe Gemini API now allows YouTube video URLs or video uploads to be added to a prompt, enabling users to to summarize, translate, or analyze the video content. With this recent update, the API supports video clipping, enabling flexibility in analyzing specific parts of a video. This is particularly beneficial for videos longer than 8 hours. We have also added support for dynamic frames per second (FPS), allowing 60 FPS for videos like games or sports where speed is critical, and 0.1 FPS for videos where speed is less of a priority. To help users save tokens, we have also introduced support for 3 different video resolutions: high (720p), standard (480p), and low (360p).\u003c/p\u003e\u003ch3 data-block-key=\"7pa3y\" id=\"async-function-calling\"\u003e\u003cb\u003e\u003cbr/\u003eAsync function calling\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"eb8co\"\u003eThe cascaded architecture in the Live API now supports asynchronous function calling, ensuring user conversations remain smooth and uninterrupted. This means your Live agent can continue generating responses even while it\u0026#39;s busy executing functions in the background, by simply adding the behavior field to the function definition and setting it to NON-BLOCKING. Read more about this in the Gemini API developer \u003ca href=\"https://ai.google.dev/gemini-api/docs/live#async-function-calling\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003ch3 data-block-key=\"uepc6\" id=\"batch-api\"\u003e\u003cb\u003e\u003cbr/\u003eBatch API\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"8m4gl\"\u003eWe are also testing a new API, which lets you easily batch up your requests and get them back in a max 24 hour turnaround time. The API will come at half the price of the interactive API and with much higher rate limits. We hope to roll that out more widely later this summer.\u003c/p\u003e\u003ch2 data-block-key=\"hdi9n\" id=\"start-building\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eStart building\u003c/h2\u003e\u003cp data-block-key=\"3kb5j\"\u003eThat’s a wrap on I/O for this year! With the Gemini API and Google AI Studio, you can turn your ideas into reality, whether you\u0026#39;re building conversational AI agents with natural-sounding audio or developing tools to analyze and generate code. As always, check out the Gemini API \u003ca href=\"https://ai.google.dev/gemini-api/docs\"\u003edeveloper docs\u003c/a\u003e for all the latest code samples and more.\u003c/p\u003e\u003cp data-block-key=\"e6fgd\"\u003eExplore this announcement and all Google I/O 2025 updates on \u003ca href=\"https://io.google/2025/?utm_source=blogpost\u0026amp;utm_medium=pr\u0026amp;utm_campaign=event\u0026amp;utm_content=\"\u003eio.google\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-05-23T00:00:00Z",
  "modifiedTime": null
}
