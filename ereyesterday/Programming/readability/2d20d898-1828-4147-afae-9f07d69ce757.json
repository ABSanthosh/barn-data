{
  "id": "2d20d898-1828-4147-afae-9f07d69ce757",
  "title": "Additional explanatory material for the Deepseek Overview",
  "link": "https://martinfowler.com/articles/deepseek-papers.html",
  "description": "",
  "author": "",
  "published": "2025-04-21T09:07:00-04:00",
  "source": "https://martinfowler.com/feed.atom",
  "categories": null,
  "byline": "Shayan Mohanty",
  "length": 29020,
  "excerpt": "An overview of the papers describing the evolution of DeepSeek",
  "siteName": "martinfowler.com",
  "favicon": "",
  "text": "This article provides a cohesive overview of four technical reports from DeepSeek: DeepSeek-LLM (Jan '24): an early investigation of scaling laws and data-model tradeoffs. DeepSeek-V2 (Jun '24): introducing Multi-Head Latent Attention (MLA) and DeepSeekMoE to improve memory and training efficiency. DeepSeek-V3 (Dec '24): scaling sparse MoE networks to 671B parameters, with FP8 mixed precision training and intricate HPC co-design DeepSeek-R1 (Jan '25): building upon the efficiency foundations of the previous papers and using large-scale reinforcement learning to incentivize emergent chain-of-thought capabilities, including a “zero-SFT” variant. For additional context on DeepSeek itself and the market backdrop that has caused claims made by the DeepSeek team to be taken out of context and spread widely, please take a look at my colleague Prasanna Pendse's post: Demystifying Deepseek. For the purposes of this article, we'll be focusing analysis and commentary on the technical work itself, its merits, and what it may signal for the future. Much of this article assumes significant knowledge of the terminology and concepts of building LLMs, more so than is typical for articles on this site. In future weeks we hope to expand this article to provide explanations of these concepts to make this article easier to follow for those not familiar with this world. We shall post any such updates on this site's usual channels. All four papers revolve around a singular challenge: building ever-larger language models with minimal cost, memory overhead, and training instability. In each iteration, the authors refine both architecture and infrastructure - a strategy often referred to as HPC co-design. Key arcs in this series include: Cost and Memory Efficiency: Methods like Multi-Head Latent Attention (MLA) compression, mixture-of-experts (MoE), and FP8-based optimizations all aim to make massive-scale training and inference feasible. Sparsity + HPC Co-Design: From V2 to V3, we see mixture-of-experts architecture evolve alongside specialized HPC scheduling—allowing 671B-parameter models to be trained on H800 clusters without blowing up the budget. Emergent Reasoning: In R1, large-scale Reinforcement Learning (RL) unlocks advanced chain-of-thought capabilities, culminating in “R1-Zero” and its purely RL-driven approach to reasoning tasks. DeepSeek-LLM: Laying the Foundation Motivation \u0026 Overview The authors set out to answer an important question: Given a fixed compute budget for pre-training, how do we choose the scale of the model and how much training data to use? Prior studies (e.g. Chinchilla vs. GPT-3) differed on the ratio between these two factors. DeepSeek-LLM addresses that by measuring scale in a different way. Earlier work measured scale in terms of how many parameters were in the model, DeepSeek-LLM instead measured scale as non-embedding FLOPs/token1 They then found they could predict computation with: 1: Non-embedding FLOPs are the amount of FLOPs (Floating Point Operations per Second) used for pre-training certain layers of the transformer (non-embedding). The authors found only some layers contributed to the scaling formula. $$ C = M \\times D $$ where $C$ is the compute budget, $M$ is non-embedding FLOPs/token, and $D$ is data size. This more granular representation helps them predict how a 7B or 67B model might train on 2T tokens of bilingual data. Training Instability A central concern they grapple with is training instability (sudden irrecoverable divergences in the training process), which can often manifest in large-scale language models—especially those with mixture-of-experts or very long contexts. By carefully tuning learning rates, batch sizes, and other hyperparameters 2, DeepSeek-LLM demonstrates that stable large-scale training is achievable, but it requires meticulous design of the architecture of the transformer model together with the infrastructure of the High Performance Computing (HPC) data center used to train it. This interwoven design of both architecture and infrastructure is called HPC Co-Design. 2: A model consists of billions of internal variables, which are called its parameters. These parameters gain their values (weights) during training. Before training, developers will set a number of different variables that control the training process itself, these are called hyperparameters. Data Quality \u0026 Model Scale A point the authors make is about how data quality shifts the optimal ratio—i.e., higher-quality data can justify a bigger model for the same number of tokens. You can intuit this by imagining two scenarios: Scenario A: You have a 100-billion-token corpus full of duplicates, spammy text, or incomplete sentences. The model might not glean much new knowledge because the data is partly redundant or low-value. Scenario B: You have a carefully curated 100-billion-token corpus with broad coverage of code, math, multi-lingual dialogues, factual text, etc. Each token is more “information-rich,” so the model can “afford” to use more parameters without hitting diminishing returns prematurely. In other words, when data is denser in useful information, scaling the model further pays off because each parameter can learn from richer signals. Key Takeaways Hyperparameter Scaling: They propose simple power-law fits to pick batch size and learning rate as compute $C$ grows. Bilingual Data: They train two base sizes (7B, 67B) on 2T tokens covering English/Chinese, then do Supervised Fine Tuning (SFT) and a simpler preference-based alignment called Direct Preference Optimization (DPO). Results: The resulting DeepSeek-LLM67B “Outperforms LLaMA-2 70B” on math/coding tasks, illustrating how HPC co-designed approaches can keep training stable while efficiently pushing scale. The seeds planted here - scaling laws and infrastructure for extremely large training - will reappear in subsequent works. DeepSeek-V2: Multi-Head Latent Attention \u0026 MoE Expanding the Model While Reducing Memory Where DeepSeek-LLM mostly explored high-level scale tradeoffs, DeepSeek-V2 dives into specifics of Transformer architecture overhead. Two big obstacles in large LLMs are: Attention KV Cache: Storing Key/Value vectors for thousands of tokens is memory-intensive. Feed-Forward Computation: Typically the largest consumption of FLOPs in a Transformer. To tame both, they propose: Multi-Head Latent Attention (MLA): compresses Key/Value vectors to reduce memory. DeepSeekMoE: a sparse Mixture-of-Experts approach that activates a fraction of the feed-forward capacity per token. Multi-Head Latent Attention (MLA) Attention is the process by which the model decides which tokens in the input stream to pay attention to when it's trying to predict the next token. In standard attention, each token's Q/K/V3 can be as large as $d_{model}$4 times the number of heads5. MLA folds them into smaller “latent” vectors: 3: Q/K/V stand for “Query,” “Key,” and “Value” vectors. At each layer, the model uses learned linear transformations to produce these vectors from the hidden states of the input. The attention mechanism then computes similarities between Q and K to decide how much of each Value vector to incorporate. 4: $d_{model}$ is the dimension of the model’s hidden representation. You can think of this hidden representation as the internal “space” in which the model’s computations occur. A larger dimension can capture richer and more complex patterns, though at increased computational and memory cost. 5: In multi-head attention, a head is a parallel attention mechanism with its own parameters. From a software-engineering perspective, each head is a distinct transform that runs in parallel with the others, letting the model attend to different aspects of the input simultaneously. $$ \\quad \\mathbf{c}_{t}^{KV} = W^{DKV}\\mathbf{h}_t, \\quad \\mathbf{k}_{t}^{C} = W^{UK}\\mathbf{c}_t^{KV}, \\quad \\mathbf{v}_{t}^{C} = W^{UV}\\mathbf{c}_t^{KV}, \\quad $$ Where $c_{t}^{KV}$ is the compressed latent vector for keys and values. $W^{DKV}$ is the down-projection matrix, and $W^{UK}, W^{UV}$ are the up-projection matrices for keys and values, respectively. In simpler terms: Replaces the standard QKV computation by using low rank factorization to turn one matrix of dim (in, out) into two matrices of (in, rank) and (rank, out) Project the compressed KV latent vector for each head to get the full K and V head corresponding to each Q head Cache the compressed KV latent vector instead of each of the KV heads in full, and compute the KV heads on the fly from the latent vector. DeepSeekMoE: Sparsely Activated FFNs Next, they adopt a Mixture-of-Experts (MoE) in the feed-forward blocks. Mixture-of-Experts (MoE) is a design technique that logically splits the model into separate areas (experts) each having specialized parameters for different domains of knowledge. Because each token is only routed to the experts most relevant to it, MoE can drastically reduce the compute required compared to a fully dense approach. This approach ties directly into the HPC co-design arc, as each expert can reside on different GPU devices. By limiting cross-device communication (e.g., device-limited routing), MoE effectively scales to extremely large parameter counts without incurring prohibitive memory or data-transfer costs. DeepSeek uses more fine-grained experts than previous models, dividing them into two kinds: Shared Experts handle universal patterns for every token. Routed Experts handle specialized sub-problems, chosen dynamically via gating. During training, they consider Auxiliary Loss to ensure balanced usage so no expert collapses (i.e. is never used). They further limit cross-device6 routing with a “device-limited routing” scheme - instead of allowing any token to access any expert, DeepSeekMoE selects a limited number of devices ($M$) per token, and performs expert selection only within these devices. The basic process is as follows: 6: Here, a device usually means a single GPU (or specialized accelerator). Large-scale training typically distributes the model across many devices in a cluster to handle computation and memory constraints. Identify top $M$ devices that contain experts with the highest affinity to the token Perform top $K_r$ expert selection within these $M$ devices Assign the selected experts to process the token Without device-limited routing, MoE models can generate excessive communication overhead which is incompatible with the hardware limitations imposed on the DeepSeek team. In addition, MoE models typically risk uneven expert utilization, where some experts are overused while others remain inactive. To prevent this, DeepSeekMoE introduces three balancing loss functions: Expert-level Balance Loss ($L_{ExpBal}$): Ensures uniform distribution of tokens across experts to prevent expert collapse Uses a loss function based on softmax scores of token-expert affinity Device-level Balance Loss ($L_{DevBal}$): Ensures workload is evenly distributed across devices Communication Balance Loss ($L_{CommBal}$): Balances incoming and outgoing token routing to each device Training \u0026 Outcomes DeepSeek-V2, with ~236B total params (21B activated), is pre-trained on 8.1T tokens. They do Supervised Fine Tuning (SFT) on 1.5M instruction samples, then reinforcement learning (RL) for alignment7. The end result: 7: Alignment refers to techniques (like supervised fine-tuning or reinforcement learning) that steer the model toward producing responses considered correct, helpful, or safe within certain guidelines or objectives. Inference and training are both faster and cheaper (MLA + sparse experts) They remain stable at scale This paper is really when iteration gains due to HPC Co-Design start to become apparent. By designing the model architecture with the training infrastructure in mind, and implementing a training regime that considers the realities of the hardware (e.g. low interconnect speeds on H800s), the team was able to lay the foundation for their most notable breakthrough. DeepSeek-V3: HPC Co-Design Scaling MoE to 671B While Preserving Efficiency Building on V2, DeepSeek-V3 further extends sparse models to 671B parameters (37B activated), training on 14.8T tokens in under 2.8M H800 GPU hours. The authors credit extensive HPC co-design: Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware. -- DeepSeek-V3 Tech. Report, p.5 The major novelties are: Refined MLA Refined DeepSeekMoE Co-Designed Training \u0026 Inference Frameworks Refined MLA Multi-Head Latent Attention was introduced in V2 to reduce KV cache overhead. In V3, it is further refined with several new features: Dynamic Low-Rank Projection: Instead of a static compression dimension, MLA adjusts how strongly it compresses Key/Value vectors depending on sequence length. For shorter sequences, less compression preserves fidelity; for extremely long sequences (32K–128K tokens), deeper compression manages memory growth. Adaptive Query Compression: Where V2 used a fixed $d_c$ dimension, V3 employs an adaptive scaling of the query up/down at different layer depths. Early layers use higher-dimensional queries for expressiveness; deeper layers more aggressively compress to save activation memory. Improved RoPE Handling: V2 only partially decoupled keys8, but V3 extends the concept for more stable 128K context. They track a “decoupled shared key” that reduces numerical drift9 in extremely long generations. 8: “Decoupling” the keys refers to a strategy of separating or factoring out certain positional or rotational encodings so that they can be processed with reduced interference, improving numerical stability for extended context windows. 9: Numerical drift occurs when small floating-point rounding errors accumulate over long sequences or many operations, potentially causing the model’s outputs (e.g. attention scores) to diverge or become unstable for very long context lengths. Joint KV Storage: V2 stored compressed keys and values separately. V3 merges them into a shared compressed representation to further reduce memory traffic during multi-node inference10. 10: Multi-node inference refers to running the model across multiple interconnected machines (or “nodes”) in a cluster—distinct from simply using multiple GPUs in one node. Although both multi-GPU and multi-node setups distribute the model, multi-node adds additional networking and scheduling layers that must be co-designed for efficiency. Layer-Wise Adaptive Cache: Instead of caching all past tokens for all layers, V3 prunes older KV entries at deeper layers. This helps keep memory usage in check when dealing with 128K context windows. Together, these MLA refinements ensure that while DeepSeek-V3 can attend across very long sequences, the memory overhead remains manageable. While many popular LLMs cap out around 4K to 32K tokens, 128K pushes that envelope significantly, enabling the model to process or reference an entire large document in a single pass. This bigger context window puts added pressure on GPU memory—hence the importance of refining MLA to keep overhead in check. Refined DeepSeekMoE: Auxiliary-Loss-Free, Higher Capacity On the MoE side, DeepSeek-V3 drops the auxiliary-loss approach from V2. Instead of an explicit penalty term, each expert acquires a dynamic bias $b_i$. If an expert is overloaded at a step, $b_i$ decreases; if underloaded, $b_i$ increases. The gating decision then adds $b_i$ to the token's affinity: $$ s'_{i,t} = s_{i,t} + b_i $$ Key Improvements: No Token Dropping: V2 occasionally dropped tokens if certain experts got overloaded, but the new bias-based method keeps everything. More Activated Experts: They raise the number of routed experts from 6 to 8 per token, improving representational power. Higher Stability: By removing auxiliary losses, they avoid potential interference with the main training objective, focusing purely on the intrinsic gating signals plus bias adjustments11. 11: By removing auxiliary losses that directly penalize or encourage certain expert usage, the gating mechanism is now free to learn solely from the main optimization objective. This prevents external penalty terms from overshadowing or disrupting the natural distribution of tokens across experts. Hence, the final feed-forward module is a combination of a small set of shared experts plus up to 8 specialized experts chosen adaptively. Co-Designed Frameworks: FP8, DualPipe, and PTX Optimizations Scaling an MoE model to 671B demanded HPC-level solutions for training and inference. The authors emphasize: Through the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computation- communication overlap. -- DeepSeek-V3 Tech. Report, p.5 FP8 Mixed Precision They adopt an FP8 data format for General Matrix Multiplications (GEMMs), halving memory. The risk is reduced numeric range so they offset it with: Block-wise scaling (e.g., 1x128 or 128x128 tiles). Periodic “promotion” to FP32 after short accumulation intervals to avoid overflow/underflow. DualPipe Parallelism They propose DualPipe to overlap forward/backward computation with the MoE all-to-all dispatch. It rearranges pipeline stages to ensure that network communication (particularly across InfiniBand12) is hidden behind local matrix multiplications. 12: InfiniBand is a high-speed, low-latency network interconnect often used in HPC clusters. “Hiding” network traffic behind other computations means overlapping communication with local GPU operations so that data transfer time has minimal impact on the total runtime. PTX-Level \u0026 Warp Specialization To fully exploit InifiniBand(IB) and NVLink: They tune warp-level13 instructions in PTX (a level lower than CUDA), auto-tuning the chunk size for all-to-all dispatch. 13: At the GPU’s warp level, instructions control how threads are batched and scheduled. Hand-tuned or specialized PTX code can exploit specific GPU hardware features for more efficient parallel processing, especially in all-to-all (MoE) communication. They dynamically partition Streaming Microcontrollers into communication vs. compute tasks so that token dispatch never stalls local GEMM14. 14: Streaming multiprocessors (SMs or “microcontrollers”) can be partially allocated to communication or computation tasks. Dynamically partitioning them ensures that the model’s token-routing (communication) can happen without forcing compute-bound operations to idle, boosting overall utilization. As a result, training costs were cut to 2.8M H800 GPU hours per run - low for a 14.8T token corpus. Outcomes The resulting DeepSeek-V3 excels at code, math, and some multilingual tasks, outperforming other open-source LLMs of similar scale. Deep HPC co-design (FP8, DualPipe, PTX-level optimization) plus refined MLA/MoE implementation achieve extreme scale with stable training. DeepSeek-R1: Reinforcement Learning for Deeper Reasoning It's worth noting that both DeepSeek R1 and DeepSeek R1-Zero are architecturally identical to DeepSeek V3. They both take the pre-trained base model of V3 and apply differing amounts of post-training. Emergent Reasoning Behaviors Through RL-Only All prior DeepSeek releases used Supervised Fine-Tuning (SFT), plus occasional Reinforcement Learning (RL). By contrast, DeepSeek-R1-Zero tries an extreme: no supervised warmup, just RL from the base model. They adopt Group Relative Policy Optimization (GRPO), which: Samples a group of old-policy outputs ${o_1, ..., o_G}$ Scores each with a reward (in this case, rule-based) Normalizes the advantage $A_i$ by group mean/stdev Optimizes a clipped PPO-like objective15 15: PPO stands for Proximal Policy Optimization, a popular reinforcement learning algorithm that optimizes a clipped objective to keep the new policy from deviating too far from the old policy. The reward function for the R1 models is rule-based - a simple weighted sum between 2 components Accuracy Reward - if the task has an objective correct answer (e.g. a math problem, coding task, etc.), correctness is verified using mathematical equation solvers for step-by-step proof checking, and code execution \u0026 test cases for code correctness verification Format Reward - the model is rewarded for following a structured reasoning process using explicit reasoning markers \u003cthink\u003e\u003c/think\u003e and \u003canswer\u003e\u003c/answer\u003e The relative advantage $A_i$ for a given output is calculated as: $$ A_i = \\frac{r_i - mean(\\{r_1, r_2, ..., r_G\\})}{std(\\{r_1, r_2, ..., r_G\\})} $$ where $r_i$ is the reward calculated for the given output. The model's policy is updated to favor responses with higher rewards while constraining changes using a clipping function which ensures that the new policy remains close to the old. In so many words: the authors created a testing/verification harness around the model which they exercised using reinforcement learning, and gently guided the model using simple Accuracy and Format rewards. In doing so, emergent reasoning behaviors were observed: Self-verification where the model double-checks its own answers Extended chain-of-thought where the model learns to explain its reasoning more thoroughly Exploratory reasoning - the model tries different approaches before converging on an answer Reflection - the model starts questioning its own solutions and adjusting reasoning paths dynamically R1-Zero is probably the most interesting outcome of the R1 paper for researchers because it learned complex chain-of-thought patterns from raw reward signals alone. However, the model exhibited notable issues: Readability Problems: Because it never saw any human-curated language style, its outputs were sometimes jumbled or mixed multiple languages. Instability in Non-Reasoning Tasks: Lacking SFT data for general conversation, R1-Zero would produce valid solutions for math or code but be awkward on simpler Q\u0026A or safety prompts. Limited Domain: Rule-based rewards worked well for verifiable tasks (math/coding), but handling creative/writing tasks demanded broader coverage. Hence, the authors concluded that while “pure RL” yields strong reasoning in verifiable tasks, the model’s overall user-friendliness was lacking. This led them to DeepSeek-R1: an alignment pipeline combining small cold-start data, RL, rejection sampling, and more RL, to “fill in the gaps” from R1-Zero’s deficits. Refined Reasoning Through SFT + RL DeepSeek-R1 addresses R1-Zero's limitations by injecting a small amount of supervised data before RL and weaving in additional alignment steps. Stage 1: “Cold-Start” SFT They gather a small number (~thousands) of curated, “human-friendly” chain-of-thought data covering common sense Q\u0026A, basic math, standard instruction tasks, etc. Then, they do a short SFT pass on the base model. This ensures the model acquires: Better readability: Polished language style and formatting. Non-reasoning coverage: Some conversation, factual QA, or creative tasks not easily rewarded purely by rule-based checks. In essence, the authors realized you can avoid the “brittleness” of a zero-SFT approach by giving the model a seed of user-friendly behaviors. Stage 2: Reasoning-Oriented RL Next, as in R1-Zero, they apply large-scale RL for tasks like math and code. The difference is that now the model starts from a “cold-start SFT” checkpoint—so it retains decent language style while still learning verifiable tasks from a rule-based or tool-based reward. This RL stage fosters the same emergent chain-of-thought expansions but without the random “language mixing” or bizarre structure. Stage 3: Rejection Sampling + Additional SFT Once that RL converges, they generate multiple completions per prompt from the RL checkpoint. Using a combination of automatic verifiers and some human checks, they pick the best outputs (“rejection sampling”) and build a new SFT dataset. They also incorporate standard writing/factual/safety data from DeepSeek-V3 to keep the model balanced in non-verifiable tasks. Finally, they re-fine-tune the base model on this curated set. This step addresses the “spotty coverage” problem even further: The best RL answers become training targets, so the model improves at chain-of-thought and clarity. Stage 4: RL for “All Scenarios” Lastly, they do another RL pass on diverse prompts—not just math/code but general helpfulness, safety, or role-playing tasks. Rewards may come from a combination of rule-based checks and large “preference” models (trained from user preference pairs). The final result is a model that: Retains strong chain-of-thought for verifiable tasks, Aligns to broad user requests in everyday usage, Maintains safer, more controlled outputs. Connecting the Arcs: Efficiency \u0026 Emergence Despite covering different angles - scaling laws, MoE, HPC scheduling, and large-scale RL - DeepSeek's work consistently follows these arcs: Cost and Memory Efficiency They systematically design methods (MLA, MoE gating, device-limited routing, FP8 training, DualPipe) to maximize hardware utilization even in constrained environments HPC-level scheduling (PTX instructions, warp specialization) hides communication overhead and overcomes the limitations imposed by limited interconnect speeds on H800s Sparse + HPC Co-Design From V2 to V3, we see an evolving mixture-of-experts approach, culminating in a 671B-parameter model feasible on H800 clusters. The authors repeatedly stress that HPC co-design is the only path to cheaply train multi-hundred-billion-parameter LLMs. Emergent Reasoning R1 pushes beyond standard supervised training, letting RL signals shape deep chain-of-thought. The synergy between pre-trained scale and targeted post-training yields advanced reasoning patterns like reflection or multi-step verification. Taken as a whole, the DeepSeek series highlights how architecture, algorithms, frameworks, and hardware must be co-designed to handle LLM training at trillion-token scales. Looking to the future, it indicates that toolchain builders may want to find ways to capture some of these HPC optimizations as part of the model compilation path or training apparatus, and AI research teams may want to work closely with HPC expertise even in the early days of architecture ideation.",
  "image": "https://martinfowler.com/logo-sq.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cp\u003eThis article provides a cohesive overview of four technical reports from\n    DeepSeek:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2401.02954v1\"\u003eDeepSeek-LLM\u003c/a\u003e \u003ci\u003e(Jan \u0026#39;24)\u003c/i\u003e: an early\n      investigation of scaling laws and data-model tradeoffs.\u003c/li\u003e\n\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2405.04434\"\u003eDeepSeek-V2\u003c/a\u003e \u003ci\u003e(Jun \u0026#39;24)\u003c/i\u003e: introducing\n      Multi-Head Latent Attention (MLA) and DeepSeekMoE to improve memory and\n      training efficiency. \u003c/li\u003e\n\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2412.19437\"\u003eDeepSeek-V3\u003c/a\u003e \u003ci\u003e(Dec \u0026#39;24)\u003c/i\u003e: scaling sparse MoE networks to 671B\n      parameters, with FP8 mixed precision training and intricate HPC co-design \u003c/li\u003e\n\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2501.12948\"\u003eDeepSeek-R1\u003c/a\u003e \u003ci\u003e(Jan \u0026#39;25)\u003c/i\u003e:\n      building upon the efficiency foundations of the previous papers and using\n      \u003ci\u003elarge-scale reinforcement learning\u003c/i\u003e to incentivize emergent\n      chain-of-thought capabilities, including a “zero-SFT” variant.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eFor additional context on DeepSeek itself and the market backdrop that\n    has caused claims made by the DeepSeek team to be taken out of context and\n    spread widely, please take a look at my colleague Prasanna Pendse\u0026#39;s post:\n    \u003ca href=\"https://www.thoughtworks.com/insights/blog/generative-ai/demystifying-deepseek\"\u003eDemystifying Deepseek\u003c/a\u003e. For the purposes of\n    this article, we\u0026#39;ll be focusing analysis and commentary on the technical\n    work itself, its merits, and what it may signal for the future.\u003c/p\u003e\n\n\u003cp\u003eMuch of this article assumes significant knowledge of the terminology and\n    concepts of building LLMs, more so than is typical for articles on this\n    site. In future weeks we hope to expand this article to provide explanations\n    of these concepts to make this article easier to follow for those not\n    familiar with this world. We shall post any such updates on this site\u0026#39;s\n    \u003ca href=\"https://martinfowler.com/recent-changes.html\"\u003eusual channels\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eAll four papers revolve around a \u003ci\u003esingular challenge\u003c/i\u003e: building\n    ever-larger language models with minimal cost, memory overhead, and training\n    instability. In each iteration, the authors refine both \u003ci\u003earchitecture\u003c/i\u003e and\n    \u003ci\u003einfrastructure\u003c/i\u003e - a strategy often referred to as \u003ci\u003eHPC co-design\u003c/i\u003e.\u003c/p\u003e\n\n\u003cp\u003eKey arcs in this series include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eCost and Memory Efficiency:\u003c/b\u003e Methods like Multi-Head Latent Attention\n      (MLA) compression, mixture-of-experts (MoE), and FP8-based optimizations all aim\n      to make massive-scale training and inference feasible.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eSparsity + HPC Co-Design:\u003c/b\u003e From V2 to V3, we see mixture-of-experts\n      architecture evolve alongside specialized HPC scheduling—allowing 671B-parameter\n      models to be trained on H800 clusters without blowing up the budget.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eEmergent Reasoning:\u003c/b\u003e In R1, large-scale Reinforcement Learning (RL)\n      unlocks advanced chain-of-thought capabilities, culminating in “R1-Zero” and its\n      purely RL-driven approach to reasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003csection id=\"Deepseek-llmLayingTheFoundation\"\u003e\n\u003ch2\u003eDeepSeek-LLM: Laying the Foundation\u003c/h2\u003e\n\n\u003csection id=\"MotivationampOverview\"\u003e\n\u003ch3\u003eMotivation \u0026amp; Overview\u003c/h3\u003e\n\n\u003cp\u003eThe authors set out to answer an important question: Given a fixed\n        compute budget for pre-training, how do we choose the scale of the model\n        and how much training data to use? Prior studies (e.g. Chinchilla vs.\n        GPT-3) differed on the ratio between these two factors. DeepSeek-LLM\n        addresses that by measuring scale in a different way. Earlier work\n        measured scale in terms of how many parameters were in the model,\n        DeepSeek-LLM instead measured scale as \u003ci\u003enon-embedding FLOPs/token\u003c/i\u003e\u003cspan data-footnote=\"footnote-non-embed-flop\"\u003e1\u003c/span\u003e They then found they could predict\n        computation with:\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003e1: \u003c/span\u003eNon-embedding FLOPs are the amount of FLOPs (Floating\n        Point Operations per Second) used for pre-training certain layers of the transformer\n        (non-embedding). The authors found only some layers contributed to the\n        scaling formula.\u003c/p\u003e\n\n\u003cp\u003e$$\n          C = M \\times D\n        $$\u003c/p\u003e\n\n\u003cp\u003ewhere $C$ is the compute budget, $M$ is non-embedding\n        FLOPs/token, and $D$ is data size.\u003c/p\u003e\n\n\u003cp\u003eThis more granular representation helps them predict how a 7B or 67B model\n        might train on 2T tokens of bilingual data.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"TrainingInstability\"\u003e\n\u003ch3\u003eTraining Instability\u003c/h3\u003e\n\n\u003cp\u003eA central concern they grapple with is training instability (sudden irrecoverable \n        divergences in the training process), which can often manifest in \n        large-scale language models—especially those with mixture-of-experts \n        or very long contexts.\u003c/p\u003e\n\n\u003cp\u003eBy carefully tuning learning rates, batch sizes, and other\n        hyperparameters \u003cspan data-footnote=\"footnote-hyperparameters\"\u003e2\u003c/span\u003e, DeepSeek-LLM\n        demonstrates that stable large-scale training is achievable, but it\n        requires meticulous design of the architecture of the transformer model\n        together with the infrastructure of the High Performance Computing (HPC)\n        data center used to train it. This interwoven design of both architecture and\n        infrastructure is called \u003cb\u003eHPC Co-Design\u003c/b\u003e. \u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003e2: \u003c/span\u003eA model consists of billions of internal\n        variables, which are called its \u003ci\u003eparameters\u003c/i\u003e. These parameters gain\n        their values (weights) during training. Before training, developers will\n        set a number of different variables that control the training process\n        itself, these are called \u003ci\u003ehyperparameters\u003c/i\u003e.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"DataQualityampModelScale\"\u003e\n\u003ch3\u003eData Quality \u0026amp; Model Scale\u003c/h3\u003e\n\n\u003cp\u003eA point the authors make is about how data quality shifts the optimal\n        ratio—i.e., \u003ci\u003ehigher-quality data can justify a bigger model for the same\n        number of tokens\u003c/i\u003e. You can intuit this by imagining two scenarios:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eScenario A: You have a 100-billion-token corpus full of duplicates,\n          spammy text, or incomplete sentences. The model might not glean much new\n          knowledge because the data is partly redundant or low-value.\u003c/li\u003e\n\n\u003cli\u003eScenario B: You have a carefully curated 100-billion-token corpus with\n          broad coverage of code, math, multi-lingual dialogues, factual text, etc. Each\n          token is more “information-rich,” so the model can “afford” to use more\n          parameters without hitting diminishing returns prematurely.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn other words, when data is denser in useful information, scaling the\n        model further pays off because each parameter can learn from richer\n        signals.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"KeyTakeaways\"\u003e\n\u003ch3\u003eKey Takeaways\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003eHyperparameter Scaling: They propose simple power-law fits to pick batch\n          size and learning rate as compute $C$ grows.\u003c/li\u003e\n\n\u003cli\u003eBilingual Data: They train two base sizes (7B, 67B) on 2T tokens\n          covering English/Chinese, then do Supervised Fine Tuning (SFT) and a simpler\n          preference-based alignment called \u003ci\u003eDirect Preference Optimization (DPO)\u003c/i\u003e.\u003c/li\u003e\n\n\u003cli\u003eResults: The resulting DeepSeek-LLM67B “Outperforms LLaMA-2 70B” on\n          math/coding tasks, illustrating how HPC co-designed approaches can keep training\n          stable while efficiently pushing scale.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe seeds planted here - scaling laws and infrastructure for extremely large\n        training - will reappear in subsequent works.\u003c/p\u003e\n\u003c/section\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Deepseek-v2Multi-headLatentAttentionampMoe\"\u003e\n\u003ch2\u003eDeepSeek-V2: Multi-Head Latent Attention \u0026amp; MoE\u003c/h2\u003e\n\n\u003csection id=\"ExpandingTheModelWhileReducingMemory\"\u003e\n\u003ch3\u003eExpanding the Model While Reducing Memory\u003c/h3\u003e\n\n\u003cp\u003eWhere DeepSeek-LLM mostly explored high-level scale tradeoffs,\n        DeepSeek-V2 dives into specifics of Transformer architecture overhead. Two\n        big obstacles in large LLMs are:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eAttention KV Cache: Storing Key/Value vectors for thousands of tokens\n          is memory-intensive.\u003c/li\u003e\n\n\u003cli\u003eFeed-Forward Computation: Typically the largest consumption of FLOPs\n          in a Transformer.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eTo tame both, they propose:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eMulti-Head Latent Attention (MLA): compresses Key/Value vectors to\n          reduce memory.\u003c/li\u003e\n\n\u003cli\u003eDeepSeekMoE: a sparse Mixture-of-Experts approach that\n          activates a fraction of the feed-forward capacity per token.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Multi-headLatentAttentionmla\"\u003e\n\u003ch3\u003eMulti-Head Latent Attention (MLA)\u003c/h3\u003e\n\n\u003cp\u003e\u003cb\u003eAttention\u003c/b\u003e is the process by which the model decides\n        which tokens in the input stream to pay attention to when it\u0026#39;s trying to\n        predict the next token. In standard attention, each token\u0026#39;s Q/K/V\u003cspan data-footnote=\"footnote-qkv-definition\"\u003e3\u003c/span\u003e \n        can be as large as $d_{model}$\u003cspan data-footnote=\"footnote-d-model-definition\"\u003e4\u003c/span\u003e times the \n        number of heads\u003cspan data-footnote=\"footnote-heads-definition\"\u003e5\u003c/span\u003e. MLA folds them into smaller “latent”\n        vectors:\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003e3: \u003c/span\u003e Q/K/V stand for “Query,” “Key,” and “Value” vectors. At each layer, \n        the model uses learned linear transformations to produce these vectors from \n        the hidden states of the input. The attention mechanism then computes \n        similarities between Q and K to decide how much of each Value vector to incorporate. \u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003e4: \u003c/span\u003e$d_{model}$ is the dimension of the model’s hidden representation. You \n        can think of this hidden representation as the internal “space” in which the \n        model’s computations occur. A larger dimension can capture richer and more \n        complex patterns, though at increased computational and memory cost. \u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003e5: \u003c/span\u003e In multi-head attention, a \u003ci\u003ehead\u003c/i\u003e is a parallel attention mechanism with \n        its own parameters. From a software-engineering perspective, each head is a \n        distinct transform that runs in parallel with the others, letting the model attend \n        to different aspects of the input simultaneously. \u003c/p\u003e\n\n\u003cp\u003e$$\n          \\quad\n          \\mathbf{c}_{t}^{KV} = W^{DKV}\\mathbf{h}_t,\n          \\quad\n          \\mathbf{k}_{t}^{C} = W^{UK}\\mathbf{c}_t^{KV},\n          \\quad\n          \\mathbf{v}_{t}^{C} = W^{UV}\\mathbf{c}_t^{KV},\n          \\quad\n        $$\u003c/p\u003e\n\n\u003cp\u003eWhere $c_{t}^{KV}$ is the compressed latent vector for keys and values.\n        $W^{DKV}$ is the down-projection matrix, and $W^{UK}, W^{UV}$ are the\n        up-projection matrices for keys and values, respectively. In simpler terms:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eReplaces the standard QKV computation by using low rank factorization to\n          turn one matrix of dim (\u003ci\u003ein, out\u003c/i\u003e) into two matrices of (\u003ci\u003ein, rank\u003c/i\u003e) and\n          (\u003ci\u003erank, out\u003c/i\u003e)\u003c/li\u003e\n\n\u003cli\u003eProject the compressed KV latent vector for each head to get the full K\n          and V head corresponding to each Q head\u003c/li\u003e\n\n\u003cli\u003eCache the compressed KV latent vector instead of each of the KV heads in\n          full, and compute the KV heads on the fly from the latent vector.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/section\u003e\n\n\u003csection id=\"DeepseekmoeSparselyActivatedFfns\"\u003e\n\u003ch3\u003eDeepSeekMoE: Sparsely Activated FFNs\u003c/h3\u003e\n\n\u003cp\u003eNext, they adopt a Mixture-of-Experts (MoE) in the feed-forward\n        blocks. \u003cb\u003eMixture-of-Experts (MoE)\u003c/b\u003e is a design technique that logically\n        splits the model into separate areas (experts) each having specialized parameters \n        for different domains of knowledge. Because each token is only routed to the experts \n        most relevant to it, MoE can drastically reduce the compute required compared to a fully \n        dense approach. This approach ties directly into the HPC co-design arc, as each expert \n        can reside on different GPU devices. By limiting cross-device communication \n        (e.g., device-limited routing), MoE effectively scales to extremely large parameter \n        counts without incurring prohibitive memory or data-transfer costs.\u003c/p\u003e\n\n\u003cp\u003eDeepSeek uses more fine-grained experts than previous models, dividing them into\n        two kinds:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ci\u003eShared Experts\u003c/i\u003e handle universal patterns for every token.\u003c/li\u003e\n\n\u003cli\u003e\u003ci\u003eRouted Experts\u003c/i\u003e handle specialized sub-problems, chosen dynamically via\n          gating.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eDuring training, they consider \u003ci\u003eAuxiliary Loss\u003c/i\u003e to ensure\n        balanced usage so no expert collapses (i.e. is never used).\u003c/p\u003e\n\n\u003cp\u003eThey further limit cross-device\u003cspan data-footnote=\"footnote-device-definition\"\u003e6\u003c/span\u003e routing with \n        a “device-limited routing” scheme - instead of allowing any token to access any expert, \n        DeepSeekMoE selects a limited number of devices ($M$) per token, and performs expert\n        selection only within these devices. The basic process is as\n        follows:\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003e6: \u003c/span\u003e Here, a \u003ci\u003edevice\u003c/i\u003e usually means a single GPU (or specialized accelerator). Large-scale \n        training typically distributes the model across many devices in a cluster to handle \n        computation and memory constraints. \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify top $M$ devices that contain experts with the highest affinity\n          to the token\u003c/li\u003e\n\n\u003cli\u003ePerform top $K_r$ expert selection within these $M$ devices\u003c/li\u003e\n\n\u003cli\u003eAssign the selected experts to process the token\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWithout device-limited routing, MoE models can generate excessive\n        communication overhead which is incompatible with the hardware limitations\n        imposed on the DeepSeek team. In addition, MoE models typically risk uneven\n        expert utilization, where some experts are overused while others remain\n        inactive. To prevent this, DeepSeekMoE introduces three balancing loss\n        functions:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExpert-level Balance Loss ($L_{ExpBal}$):\u003c/li\u003e\n\n\u003cul\u003e\n\u003cli\u003eEnsures uniform distribution of tokens across experts to prevent expert\n            collapse\u003c/li\u003e\n\n\u003cli\u003eUses a loss function based on softmax scores of token-expert\n            affinity\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cli\u003eDevice-level Balance Loss ($L_{DevBal}$):\u003c/li\u003e\n\n\u003cul\u003e\n\u003cli\u003eEnsures workload is evenly distributed across devices\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cli\u003eCommunication Balance Loss ($L_{CommBal}$):\u003c/li\u003e\n\n\u003cul\u003e\n\u003cli\u003eBalances incoming and outgoing token routing to each device\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/ul\u003e\n\u003c/section\u003e\n\n\u003csection id=\"TrainingampOutcomes\"\u003e\n\u003ch3\u003eTraining \u0026amp; Outcomes\u003c/h3\u003e\n\n\u003cp\u003eDeepSeek-V2, with ~236B total params (21B activated), is pre-trained on\n        8.1T tokens. They do Supervised Fine Tuning (SFT) on 1.5M instruction samples,\n        then reinforcement learning (RL) for alignment\u003cspan data-footnote=\"footnote-alignment-definition\"\u003e7\u003c/span\u003e. The end result:\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003e7: \u003c/span\u003e Alignment refers to techniques (like supervised fine-tuning or reinforcement \n        learning) that steer the model toward producing responses considered correct, \n        helpful, or safe within certain guidelines or objectives. \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eInference and training are both faster and cheaper (MLA + sparse\n          experts)\u003c/li\u003e\n\n\u003cli\u003eThey remain stable at scale\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis paper is really when iteration gains due to HPC Co-Design start to\n        become apparent. By designing the model architecture with the training\n        infrastructure in mind, and implementing a training regime that considers the\n        realities of the hardware (e.g. low interconnect speeds on H800s), the team\n        was able to lay the foundation for their most notable breakthrough.\u003c/p\u003e\n\u003c/section\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Deepseek-v3HpcCo-design\"\u003e\n\u003ch2\u003eDeepSeek-V3: HPC Co-Design\u003c/h2\u003e\n\n\u003csection id=\"ScalingMoeTo671bWhilePreservingEfficiency\"\u003e\n\u003ch3\u003eScaling MoE to 671B While Preserving Efficiency\u003c/h3\u003e\n\n\u003cp\u003eBuilding on V2, DeepSeek-V3 further extends sparse models to \u003ci\u003e671B\u003c/i\u003e\n        parameters (\u003ci\u003e37B activated\u003c/i\u003e), training on 14.8T tokens in under \u003cb\u003e2.8M H800\n        GPU hours\u003c/b\u003e. The authors credit extensive HPC co-design:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eLastly, we emphasize again the economical training costs of\n          DeepSeek-V3, summarized in Table 1, achieved through our optimized\n          co-design of algorithms, frameworks, and hardware.\u003c/p\u003e\n\n\u003cp\u003e-- \u003ca href=\"https://arxiv.org/abs/2412.19437\"\u003eDeepSeek-V3 Tech. Report, p.5\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe major novelties are:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eRefined MLA\u003c/li\u003e\n\n\u003cli\u003eRefined DeepSeekMoE\u003c/li\u003e\n\n\u003cli\u003eCo-Designed Training \u0026amp; Inference\n          Frameworks\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/section\u003e\n\n\u003csection id=\"RefinedMla\"\u003e\n\u003ch3\u003eRefined MLA\u003c/h3\u003e\n\n\u003cp\u003eMulti-Head Latent Attention was introduced in V2 to reduce KV cache overhead.\n        In V3, it is further refined with several new features:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eDynamic Low-Rank Projection\u003c/b\u003e: Instead of a static compression dimension,\n          MLA adjusts how strongly it compresses Key/Value vectors depending on sequence\n          length. For shorter sequences, less compression preserves fidelity; for\n          extremely long sequences (32K–128K tokens), deeper compression manages memory\n          growth.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eAdaptive Query Compression\u003c/b\u003e: Where V2 used a fixed $d_c$ dimension, V3\n          employs an adaptive scaling of the query up/down at different layer depths.\n          Early layers use higher-dimensional queries for expressiveness; deeper layers\n          more aggressively compress to save activation memory.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eImproved RoPE Handling\u003c/b\u003e: V2 only partially decoupled keys\u003cspan data-footnote=\"footnote-decoupled-keys\"\u003e8\u003c/span\u003e, but V3 extends\n          the concept for more stable 128K context. They track a “decoupled shared key”\n          that reduces numerical drift\u003cspan data-footnote=\"footnote-numerical-drift\"\u003e9\u003c/span\u003e in extremely long generations.\u003c/li\u003e\n\n\u003cp\u003e\u003cspan\u003e8: \u003c/span\u003e“Decoupling” the keys refers to a strategy of separating or factoring out \n          certain positional or rotational encodings so that they can be processed with reduced interference, \n          improving numerical stability for extended context windows.\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003e9: \u003c/span\u003eNumerical drift occurs when small floating-point rounding errors accumulate \n          over long sequences or many operations, potentially causing the model’s outputs (e.g. attention scores) \n          to diverge or become unstable for very long context lengths.\u003c/p\u003e\n\n\u003cli\u003e\u003cb\u003eJoint KV Storage\u003c/b\u003e: V2 stored compressed keys and values separately. V3\n          merges them into a shared compressed representation to further reduce memory\n          traffic during multi-node inference\u003cspan data-footnote=\"footnote-multi-node-definition\"\u003e10\u003c/span\u003e.\u003c/li\u003e\n\n\u003cp\u003e\u003cspan\u003e10: \u003c/span\u003eMulti-node inference refers to running the model across multiple interconnected \n          machines (or “nodes”) in a cluster—distinct from simply using multiple GPUs in one node. Although both multi-GPU \n          and multi-node setups distribute the model, multi-node adds additional networking and scheduling layers that must \n          be co-designed for efficiency.\u003c/p\u003e\n\n\u003cli\u003e\u003cb\u003eLayer-Wise Adaptive Cache\u003c/b\u003e: Instead of caching \u003ci\u003eall\u003c/i\u003e past tokens for all\n          layers, V3 prunes older KV entries at deeper layers. This helps keep memory\n          usage in check when dealing with 128K context windows.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eTogether, these MLA refinements ensure that while DeepSeek-V3 can attend across \u003ci\u003every\u003c/i\u003e long sequences, \n        the memory overhead remains manageable. While many popular LLMs cap out around 4K to 32K tokens, 128K pushes \n        that envelope significantly, enabling the model to process or reference an entire large document in a single pass. \n        This bigger context window puts added pressure on GPU memory—hence the importance of refining MLA \n        to keep overhead in check.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"RefinedDeepseekmoeAuxiliary-loss-freeHigherCapacity\"\u003e\n\u003ch3\u003eRefined DeepSeekMoE: Auxiliary-Loss-Free, Higher Capacity\u003c/h3\u003e\n\n\u003cp\u003eOn the MoE side, DeepSeek-V3 drops the auxiliary-loss approach from V2.\n        Instead of an explicit penalty term, each expert acquires a dynamic bias\n        $b_i$. If an expert is overloaded at a step, $b_i$ decreases; if\n        underloaded, $b_i$ increases. The gating decision then adds $b_i$ to the\n        token\u0026#39;s affinity:\u003c/p\u003e\n\n\u003cp\u003e$$\n          s\u0026#39;_{i,t} = s_{i,t} + b_i\n        $$\u003c/p\u003e\n\n\u003cp\u003eKey Improvements:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eNo Token Dropping\u003c/b\u003e: V2 occasionally dropped tokens if certain experts got\n          overloaded, but the new bias-based method keeps everything.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eMore Activated Experts\u003c/b\u003e: They raise the number of routed experts from 6\n          to 8 per token, improving representational power.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eHigher Stability\u003c/b\u003e: By removing auxiliary losses, they avoid potential\n          interference with the main training objective, focusing purely on the\n          \u003ci\u003eintrinsic\u003c/i\u003e gating signals plus bias adjustments\u003cspan data-footnote=\"footnote-aux-loss-removal-explanation\"\u003e11\u003c/span\u003e.\u003c/li\u003e\n\n\u003cp\u003e\u003cspan\u003e11: \u003c/span\u003eBy removing auxiliary losses that directly penalize \n        or encourage certain expert usage, the gating mechanism is now free to learn solely from the main \n        optimization objective. This prevents external penalty terms from overshadowing or disrupting the \n        natural distribution of tokens across experts.\u003c/p\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eHence, the final feed-forward module is a combination of a small set of\n        shared experts plus up to 8 specialized experts chosen adaptively.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Co-designedFrameworksFp8DualpipeAndPtxOptimizations\"\u003e\n\u003ch3\u003eCo-Designed Frameworks: FP8, DualPipe, and PTX Optimizations\u003c/h3\u003e\n\n\u003cp\u003eScaling an MoE model to 671B demanded HPC-level solutions for training and\n        inference. The authors emphasize:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThrough the co-design of algorithms, frameworks, and hardware, we\n        overcome the communication bottleneck in cross-node MoE training, achieving\n          near-full computation- communication overlap.\u003c/p\u003e\n\n\u003cp\u003e-- \u003ca href=\"https://arxiv.org/abs/2412.19437\"\u003eDeepSeek-V3 Tech. Report, p.5\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003cb\u003eFP8 Mixed Precision\u003c/b\u003e\u003c/p\u003e\n\n\u003cp\u003eThey adopt an FP8 data format for General Matrix Multiplications (GEMMs),\n        halving memory. The risk is \u003ci\u003ereduced numeric range\u003c/i\u003e so they offset it\n        with:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eBlock-wise scaling (e.g., 1x128 or 128x128 tiles).\u003c/li\u003e\n\n\u003cli\u003ePeriodic “promotion” to FP32 after short accumulation intervals to avoid\n          overflow/underflow.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cb\u003eDualPipe Parallelism\u003c/b\u003e\u003c/p\u003e\n\n\u003cp\u003eThey propose \u003cb\u003eDualPipe\u003c/b\u003e to overlap forward/backward computation with the MoE\n        all-to-all dispatch. It rearranges pipeline stages to ensure that network\n        communication (particularly across InfiniBand\u003cspan data-footnote=\"footnote-infiniband-definition\"\u003e12\u003c/span\u003e) is hidden behind local matrix\n        multiplications.\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003e12: \u003c/span\u003eInfiniBand is a high-speed, low-latency network interconnect often used in HPC clusters. \n        “Hiding” network traffic behind other computations means overlapping communication with local GPU operations \n        so that data transfer time has minimal impact on the total runtime.\u003c/p\u003e\n\n\u003cp\u003e\u003cb\u003ePTX-Level \u0026amp; Warp Specialization\u003c/b\u003e\u003c/p\u003e\n\n\u003cp\u003eTo fully exploit InifiniBand(IB) and NVLink:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThey tune \u003ci\u003ewarp-level\u003c/i\u003e\u003cspan data-footnote=\"footnote-warp-level-optimizations\"\u003e13\u003c/span\u003e instructions in PTX (a level lower than CUDA),\n          auto-tuning the chunk size for all-to-all dispatch.\u003c/li\u003e\n\n\u003cp\u003e\u003cspan\u003e13: \u003c/span\u003eAt the GPU’s warp level, instructions control how threads are batched and scheduled. \n        Hand-tuned or specialized PTX code can exploit specific GPU hardware features for more efficient parallel processing, \n        especially in all-to-all (MoE) communication.\u003c/p\u003e\n\n\u003cli\u003eThey dynamically partition Streaming Microcontrollers into communication vs.\n          compute tasks so that token dispatch never stalls local GEMM\u003cspan data-footnote=\"footnote-dynamic-partitioning\"\u003e14\u003c/span\u003e.\u003c/li\u003e\n\n\u003cp\u003e\u003cspan\u003e14: \u003c/span\u003eStreaming multiprocessors (SMs or “microcontrollers”) can be partially allocated to \n        communication or computation tasks. Dynamically partitioning them ensures that the model’s token-routing (communication) \n        can happen without forcing compute-bound operations to idle, boosting overall utilization.\u003c/p\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAs a result, training costs were cut to 2.8M H800 GPU hours per run - low\n        for a 14.8T token corpus.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Outcomes\"\u003e\n\u003ch3\u003eOutcomes\u003c/h3\u003e\n\n\u003cp\u003eThe resulting DeepSeek-V3 excels at code, math, and some multilingual\n        tasks, outperforming other open-source LLMs of similar scale. Deep HPC\n        co-design (FP8, DualPipe, PTX-level optimization) plus refined MLA/MoE\n        implementation achieve \u003ci\u003eextreme\u003c/i\u003e scale with stable training.\u003c/p\u003e\n\u003c/section\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Deepseek-r1ReinforcementLearningForDeeperReasoning\"\u003e\n\u003ch2\u003eDeepSeek-R1: Reinforcement Learning for Deeper Reasoning\u003c/h2\u003e\n\n\u003cp\u003eIt\u0026#39;s worth noting that both DeepSeek R1 and DeepSeek R1-Zero are\n      architecturally identical to DeepSeek V3. They both take the pre-trained\n      base model of V3 and apply differing amounts of post-training.\u003c/p\u003e\n\n\u003csection id=\"EmergentReasoningBehaviorsThroughRl-only\"\u003e\n\u003ch3\u003eEmergent Reasoning Behaviors Through RL-Only\u003c/h3\u003e\n\n\u003cp\u003eAll prior DeepSeek releases used Supervised Fine-Tuning (SFT), plus\n        occasional Reinforcement Learning (RL). By contrast,\n        DeepSeek-R1-Zero tries an extreme: \u003ci\u003eno supervised warmup\u003c/i\u003e, just RL from\n        the base model. They adopt Group Relative Policy Optimization (GRPO),\n        which:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eSamples a group of old-policy outputs ${o_1, ..., o_G}$\u003c/li\u003e\n\n\u003cli\u003eScores each with a reward (in this case, rule-based)\u003c/li\u003e\n\n\u003cli\u003eNormalizes the advantage $A_i$ by group mean/stdev\u003c/li\u003e\n\n\u003cli\u003eOptimizes a clipped PPO-like objective\u003cspan data-footnote=\"footnote-ppo-definition\"\u003e15\u003c/span\u003e\u003c/li\u003e\n\n\u003cp\u003e\u003cspan\u003e15: \u003c/span\u003ePPO stands for Proximal Policy Optimization, a popular reinforcement \n        learning algorithm that optimizes a clipped objective to keep the new policy from deviating too far \n        from the old policy.\u003c/p\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThe reward function for the R1 models is rule-based - a simple weighted\n        sum between 2 components\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ci\u003eAccuracy Reward\u003c/i\u003e - if the task has an objective correct answer (e.g. a\n          math problem, coding task, etc.), correctness is verified using mathematical\n          equation solvers for step-by-step proof checking, and code execution \u0026amp; test\n          cases for code correctness verification\u003c/li\u003e\n\n\u003cli\u003e\u003ci\u003eFormat Reward\u003c/i\u003e - the model is rewarded for following a structured\n          reasoning process using explicit reasoning markers \u003ccode\u003e\u0026lt;think\u0026gt;\u0026lt;/think\u0026gt;\u003c/code\u003e and\n          \u003ccode\u003e\u0026lt;answer\u0026gt;\u0026lt;/answer\u0026gt;\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe relative advantage $A_i$ for a given output is calculated as:\u003c/p\u003e\n\n\u003cp\u003e$$\n          A_i = \\frac{r_i - mean(\\{r_1, r_2, ..., r_G\\})}{std(\\{r_1, r_2, ..., r_G\\})}\n        $$\u003c/p\u003e\n\n\u003cp\u003ewhere $r_i$ is the reward calculated for the given output. The model\u0026#39;s\n        policy is updated to favor responses with higher rewards while constraining\n        changes using a clipping function which ensures that the new policy remains\n        close to the old.\u003c/p\u003e\n\n\u003cp\u003eIn so many words: the authors created a testing/verification harness around\n        the model which they exercised using reinforcement learning, and gently guided\n        the model using simple Accuracy and Format rewards. In doing so, emergent\n        reasoning behaviors were observed:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eSelf-verification\u003c/b\u003e where the model double-checks its own answers\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eExtended chain-of-thought\u003c/b\u003e where the model learns to explain its\n          reasoning more thoroughly\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eExploratory reasoning\u003c/b\u003e - the model tries different approaches before\n          converging on an answer\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eReflection\u003c/b\u003e - the model starts questioning its own solutions and\n          adjusting reasoning paths dynamically\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eR1-Zero is probably the most interesting outcome of the R1 paper for\n        researchers because it learned complex chain-of-thought patterns from raw reward\n        signals alone. However, the model exhibited notable issues:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eReadability Problems\u003c/b\u003e: Because it never saw any human-curated language\n          style, its outputs were sometimes jumbled or mixed multiple languages.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eInstability in Non-Reasoning Tasks\u003c/b\u003e: Lacking SFT data for general\n          conversation, R1-Zero would produce valid solutions for math or code but be\n          awkward on simpler Q\u0026amp;A or safety prompts.\u003c/li\u003e\n\n\u003cli\u003e\u003cb\u003eLimited Domain\u003c/b\u003e: Rule-based rewards worked well for verifiable tasks\n          (math/coding), but handling creative/writing tasks demanded broader\n          coverage.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eHence, the authors concluded that while “pure RL” yields strong reasoning\n        \u003ci\u003ein verifiable tasks\u003c/i\u003e, the model’s overall user-friendliness was lacking. This\n        led them to \u003cb\u003eDeepSeek-R1\u003c/b\u003e: an alignment pipeline combining small cold-start\n        data, RL, rejection sampling, and more RL, to “fill in the gaps” from R1-Zero’s\n        deficits.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"RefinedReasoningThroughSftRl\"\u003e\n\u003ch3\u003eRefined Reasoning Through SFT + RL\u003c/h3\u003e\n\n\u003cp\u003eDeepSeek-R1 addresses R1-Zero\u0026#39;s limitations by injecting a small amount\n        of supervised data before RL and weaving in additional alignment steps.\u003c/p\u003e\n\n\u003csection id=\"Stage1x201ccold-startx201dSft\"\u003e\n\u003ch4\u003eStage 1: “Cold-Start” SFT\u003c/h4\u003e\n\n\u003cp\u003eThey gather a small number (~thousands) of\n          curated, “human-friendly” chain-of-thought data covering common sense Q\u0026amp;A,\n          basic math, standard instruction tasks, etc. Then, they do a short SFT pass on\n          the base model. This ensures the model acquires:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eBetter readability: Polished language style and formatting.\u003c/li\u003e\n\n\u003cli\u003eNon-reasoning coverage: Some conversation, factual QA, or creative tasks\n            not easily rewarded purely by rule-based checks.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn essence, the authors realized you can avoid the “brittleness” of a\n          zero-SFT approach by giving the model a \u003ci\u003eseed\u003c/i\u003e of user-friendly behaviors.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Stage2Reasoning-orientedRl\"\u003e\n\u003ch4\u003eStage 2: Reasoning-Oriented RL\u003c/h4\u003e\n\n\u003cp\u003eNext, as in R1-Zero, they apply large-scale RL for tasks like math and\n          code. The difference is that now the model starts from a “cold-start SFT”\n          checkpoint—so it retains decent language style while still learning verifiable\n          tasks from a rule-based or tool-based reward. This RL stage fosters the same\n          emergent chain-of-thought expansions but \u003ci\u003ewithout\u003c/i\u003e the random “language\n          mixing” or bizarre structure.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Stage3RejectionSamplingAdditionalSft\"\u003e\n\u003ch4\u003eStage 3: Rejection Sampling + Additional SFT\u003c/h4\u003e\n\n\u003cp\u003eOnce that RL converges, they generate multiple completions per prompt from\n          the RL checkpoint. Using a combination of automatic verifiers and some human\n          checks, they pick the best outputs (“rejection sampling”) and build a new SFT\n          dataset. They also incorporate standard writing/factual/safety data from\n          DeepSeek-V3 to keep the model balanced in non-verifiable tasks. Finally, they\n          re-fine-tune the base model on this curated set.\u003c/p\u003e\n\n\u003cp\u003eThis step addresses the “spotty coverage” problem even further: The best RL\n          answers become training targets, so the model improves at chain-of-thought\n          \u003ci\u003eand\u003c/i\u003e clarity.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003csection id=\"Stage4RlForx201callScenariosx201d\"\u003e\n\u003ch4\u003eStage 4: RL for “All Scenarios”\u003c/h4\u003e\n\n\u003cp\u003eLastly, they do another RL pass on diverse prompts—not just math/code but\n          general helpfulness, safety, or role-playing tasks. Rewards may come from a\n          combination of rule-based checks and large “preference” models (trained from\n          user preference pairs). The final result is a model that:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRetains strong chain-of-thought for verifiable tasks,\u003c/li\u003e\n\n\u003cli\u003eAligns to broad user requests in everyday usage,\u003c/li\u003e\n\n\u003cli\u003eMaintains safer, more controlled outputs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/section\u003e\n\u003c/section\u003e\n\u003c/section\u003e\n\n\u003csection id=\"ConnectingTheArcsEfficiencyampEmergence\"\u003e\n\u003ch2\u003eConnecting the Arcs: Efficiency \u0026amp; Emergence\u003c/h2\u003e\n\n\u003cp\u003eDespite covering different angles - scaling laws, MoE, HPC scheduling, and\n      large-scale RL - DeepSeek\u0026#39;s work consistently follows these arcs:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eCost and Memory Efficiency\u003c/b\u003e\u003c/li\u003e\n\n\u003cul\u003e\n\u003cli\u003eThey systematically design methods (MLA, MoE gating, device-limited\n          routing, FP8 training, DualPipe) to maximize hardware utilization even in\n          constrained environments\u003c/li\u003e\n\n\u003cli\u003eHPC-level scheduling (PTX instructions, warp specialization) hides\n          communication overhead and overcomes the limitations imposed by\n          limited interconnect speeds on H800s\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cli\u003e\u003cb\u003eSparse + HPC Co-Design\u003c/b\u003e\u003c/li\u003e\n\n\u003cul\u003e\n\u003cli\u003eFrom V2 to V3, we see an evolving mixture-of-experts approach,\n          culminating in a 671B-parameter model feasible on H800 clusters.\u003c/li\u003e\n\n\u003cli\u003eThe authors repeatedly stress that HPC co-design is the only path\n          to cheaply train multi-hundred-billion-parameter LLMs.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cli\u003e\u003cb\u003eEmergent Reasoning\u003c/b\u003e\u003c/li\u003e\n\n\u003cul\u003e\n\u003cli\u003eR1 pushes beyond standard supervised training, letting RL signals\n          shape deep chain-of-thought. The synergy between \u003ci\u003epre-trained\u003c/i\u003e\n          scale and \u003ci\u003etargeted post-training\u003c/i\u003e yields advanced reasoning\n          patterns like reflection or multi-step verification.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eTaken as a whole, the DeepSeek series highlights how architecture,\n      algorithms, frameworks, and hardware must be co-designed to handle LLM\n      training at trillion-token scales. Looking to the future, it indicates that\n      toolchain builders may want to find ways to capture some of these HPC\n      optimizations as part of the model compilation path or training apparatus,\n      and AI research teams may want to work closely with HPC expertise even in\n      the early days of architecture ideation.\u003c/p\u003e\n\u003c/section\u003e\n\n\u003chr/\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "31 min read",
  "publishedTime": null,
  "modifiedTime": "2025-04-21T00:00:00Z"
}
