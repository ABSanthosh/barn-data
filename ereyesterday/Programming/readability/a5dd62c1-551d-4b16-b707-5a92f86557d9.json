{
  "id": "a5dd62c1-551d-4b16-b707-5a92f86557d9",
  "title": "Beyond English: How Gemma open models are bridging the language gap",
  "link": "https://developers.googleblog.com/en/building-more-inclusive-llms-using-gemma-open-models/",
  "description": "AI Singapore and INSAIT teams have leveraged Gemma, a family of open-source language models, to create LLMs tailored to the unique needs of their communities, in a show of innovation and inclusivity in AI.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Francesca Di Felice",
  "length": 7518,
  "excerpt": "AI Singapore and INSAIT teams have leveraged Gemma, a family of open-source language models, to create LLMs tailored to the unique needs of their communities, in a show of innovation and inclusivity in AI.",
  "siteName": "",
  "favicon": "",
  "text": "At Google, we believe AI should be helpful for everyone. But it’s hard for AI to be inclusive when so many prominent large language models (LLM) only understand a small fraction of the thousands of languages spoken around the world. This leads many models to unintentionally overlook the cultural and linguistic differences that make each society unique, limiting the immense benefits that LLMs can offer to potentially billions of people.With Gemma, our family of lightweight and efficient open models, developers and researchers across the globe now have the tools to build LLMs that address these specific cultural differences. Leveraging the same research and technology used to create Gemini, Gemma efficiently understands text across languages, leading to improved multilingual performance, reduced costs, and greater flexibility for creating truly inclusive AI.Teams like those at INSAIT and AI Singapore have already been empowered to create new possibilities using Gemma variants. INSAIT’s recent release of BgGPT, a state-of the-art Bulgarian model based on gemma-2-27b and AI Singapore’s SEA-LIONv3, a groundbreaking new model for Southeast Asian languages based on gemma-2-9b show how through blending their cultural knowledge and AI expertise, both teams have managed to create new LLMs that meet the unique needs of their communities.Inspired? You can contribute to pushing the boundaries of inclusivity and innovation in AI by joining the Unlock Global Communication with Gemma competition on Kaggle, open till January 14.SEA-LION: Building LLMs for diverse SEA communitiesRecognizing that Southeast Asia’s (SEA) diverse languages and cultures were underrepresented in existing LLMs, AI Singapore developers created SEA-LION to better reflect the region’s nuances, contexts, and cultural diversity. This family of models has already had an immense impact on local SEA communities. For example, the latest SEA-LION’s model based on Gemma has become the foundation for Sahabat-AI, an Indonesian LLM built by GoTo to power the AI voice assistant on their GoPay app and Gojek app. This allows millions of Indonesians to more naturally use these app services in their local languages and dialects.The biggest challenge in building a leading LLM for SEA languages was finding high-quality diverse training data. This is why the team collaborated with Google DeepMind \u0026 Google Research on Project SEALD, an effort to enhance datasets that can be used to train, fine-tune, and evaluate large language models (LLMs) in languages spoken across Southeast Asia. The team also had to ensure the data they used was relevant, which meant filtering out gambling content or ads that didn’t reflect the region’s true linguistic and cultural heritage. To solve this, they built a working group of native speakers and linguists to ensure each model’s translation was accurate and felt natural for users of different backgrounds. Benchmarks plotting the relationship between SEA-LION’s English Tasks performance and SEA Average performance. SEA-LION’s latest V3 iteration is the team’s most advanced yet. Continuously pre-trained on Gemma 2-9B, this version significantly improves multilingual proficiency and task performance, making it their best-performing model to date. This version also supports 11 Southeast Asian languages, as well as major dialects such as Javanese and Sundanese, while maintaining strong performance in English.According to William Tjhi, head of applied research for foundation models at AI Singapore, the team chose the 9 billion parameter model over the larger base model to ensure greater accessibility: “Many SEA users are ‘throughput constrained’ and may not have the computational resources required to run inferences at scale with larger models.”INSAIT: Building leading Bulgarian language models on Gemma 2Researchers at the Institute for Computer Science, Artificial Intelligence, and Technology (INSAIT) have also made incredible gains in AI language inclusivity by creating three new LLMs for the Bulgarian language. INSAIT’s latest models are built on top of the Gemma 2 family and outperform much larger Bulgarian models while importantly maintaining the skills of the base Gemma 2 model, like English and mathematical proficiency.INSAIT’s new LLMs underscore the power of how open AI development can drive innovation in diverse linguistic contexts. The team's success highlights how collaborative, openLLMs can rival—and often exceed—the capabilities of larger proprietary models. Benchmarks showing INSAIT’s latest models’ performance in Bulgarian (blue) versus previous models’ performance (grey). INSAIT’s state-of-the-art Bulgarian language models demonstrate a scalable approach for other languages. Its researchers added many improvements to the base Gemma 2 model, including continuous pre-training on around 85 billion tokens in Bulgarian. They also included novel continuous pre-training, instruction-fine tuning, and a model merging scheme based on new research from EMNLP 2024, a popular conference for natural language processing. The research introduces a new method for mitigating “catastrophic forgetting,” a phenomenon where AI models forget previously learned skills (English, math) after being trained on new ones (Bulgarian).\"The result shown by INSAIT is significant because it visibly demonstrates that even a country the size of Bulgaria can build its own state-of-the-art AI models by relying on open models, advanced AI research, and special data acquisition and training techniques,” said Martin Vechev, a full professor at ETH Zurich and scientific director of INSAIT. \"While our models target Bulgarian, the branch-and-merge method we introduced in EMNLP 2024 to mitigate catastrophic forgetting applies to acquiring new languages.” Today, INSAIT’s open models provide free access to high-performing Bulgarian language models, advancing natural language processing within Bulgaria and offering greater opportunities for others interested in developing localized AI solutions. INSAIT has even launched a nationwide public chat system based on its BgGPT-Gemma model variants. This is the first time a European government institution has launched a nationwide chat system based on its own publicly available, free, and open generative AI models.Connecting communities through AIThe release of these open models from AI Singapore and INSAIT represents a significant step towards democratizing AI access and empowering local communities. Both teams highlight the importance of linguistic diversity in developing AI solutions and have shown that it is easily achievable through open-model solutions like Gemma.The possibilities with localized LLMs are vast, and we’re proud to see ambitious developers using the latest AI technologies to create new opportunities for their communities. That’s why we invite anyone inspired by these stories to join our Kaggle competition focused on adapting the Gemma 2 open model family for 73 eligible languages.With this diverse selection of languages, we are compiling a foundation of resources and best practices to help developers create better and more inclusive LLMs for communities all over the world. Join the competition today; the final submission deadline is January 14, 2025!",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma-SEALION_1.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"dpqfs\"\u003eAt Google, we believe \u003ca href=\"https://ai.google/\"\u003eAI should be helpful for everyone\u003c/a\u003e. But it’s hard for AI to be inclusive when so many prominent large language models (LLM) only understand a small fraction of the thousands of languages spoken around the world. This leads many models to unintentionally overlook the cultural and linguistic differences that make each society unique, limiting the immense benefits that LLMs can offer to potentially billions of people.\u003c/p\u003e\u003cp data-block-key=\"eb7jn\"\u003eWith \u003ca href=\"https://ai.google.dev/gemma/?utm_source=keyword\u0026amp;utm_medium=referral\u0026amp;utm_campaign=gemma_cta\u0026amp;utm_content#gemma-2\"\u003eGemma\u003c/a\u003e, our family of lightweight and efficient open models, developers and researchers across the globe now have the tools to build LLMs that address these specific cultural differences. Leveraging the same research and technology used to create Gemini, Gemma efficiently understands text across languages, leading to improved multilingual performance, reduced costs, and greater flexibility for creating truly inclusive AI.\u003c/p\u003e\u003cp data-block-key=\"bngnc\"\u003eTeams like those at \u003ca href=\"https://insait.ai/\"\u003eINSAIT\u003c/a\u003e and \u003ca href=\"https://aisingapore.org/\"\u003eAI Singapore\u003c/a\u003e have already been empowered to create new possibilities using Gemma variants. INSAIT’s recent release of BgGPT, a state-of the-art Bulgarian model based on gemma-2-27b and AI Singapore’s SEA-LIONv3, a groundbreaking new model for Southeast Asian languages based on gemma-2-9b show how through blending their cultural knowledge and AI expertise, both teams have managed to create new LLMs that meet the unique needs of their communities.\u003c/p\u003e\u003cp data-block-key=\"32prq\"\u003eInspired? You can contribute to pushing the boundaries of inclusivity and innovation in AI by joining the \u003ca href=\"https://www.kaggle.com/competitions/gemma-language-tuning\"\u003eUnlock Global Communication with Gemma\u003c/a\u003e competition on Kaggle, open till January 14.\u003c/p\u003e\u003ch2 data-block-key=\"2qadq\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eSEA-LION: Building LLMs for diverse SEA communities\u003c/h2\u003e\u003cp data-block-key=\"8iiju\"\u003eRecognizing that Southeast Asia’s (SEA) diverse languages and cultures were underrepresented in existing LLMs, AI Singapore developers created \u003ca href=\"https://sea-lion.ai/\"\u003eSEA-LION\u003c/a\u003e to better reflect the region’s nuances, contexts, and cultural diversity. This family of models has already had an immense impact on local SEA communities. For example, the latest SEA-LION’s model based on Gemma has become the foundation for \u003ca href=\"https://huggingface.co/GoToCompany\"\u003eSahabat-AI\u003c/a\u003e, an Indonesian LLM built by \u003ca href=\"https://gotocompany.com/en\"\u003eGoTo\u003c/a\u003e to power the AI voice assistant on their \u003ca href=\"https://play.google.com/store/apps/details?id=com.gojek.gopay\u0026amp;hl=en_US\"\u003eGoPay\u003c/a\u003e app and \u003ca href=\"https://play.google.com/store/apps/details?id=com.gojek.app\u0026amp;hl=en\"\u003eGojek\u003c/a\u003e app. This allows millions of Indonesians to more naturally use these app services in their local languages and dialects.\u003c/p\u003e\u003cp data-block-key=\"fgc7j\"\u003eThe biggest challenge in building a leading LLM for SEA languages was finding high-quality diverse training data. This is why the team collaborated with Google DeepMind \u0026amp; Google Research on \u003ca href=\"https://aisingapore.org/aiproducts/southeast-asian-languages-in-one-network-data-seald/\"\u003eProject SEALD\u003c/a\u003e, an effort to enhance datasets that can be used to train, fine-tune, and evaluate large language models (LLMs) in languages spoken across Southeast Asia. The team also had to ensure the data they used was relevant, which meant filtering out gambling content or ads that didn’t reflect the region’s true linguistic and cultural heritage. To solve this, they built a working group of native speakers and linguists to ensure each model’s translation was accurate and felt natural for users of different backgrounds.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_vshNn7x.original.png\" alt=\"A scatterplot graph plotting the relationship between SEA-LION’s English Tasks performance and SEA Average performance.\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    Benchmarks plotting the relationship between SEA-LION’s English Tasks performance and SEA Average performance.\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"dpqfs\"\u003eSEA-LION’s latest V3 iteration is the team’s most advanced yet. Continuously pre-trained on \u003ca href=\"https://ai.google.dev/gemma/?utm_source=keyword\u0026amp;utm_medium=referral\u0026amp;utm_campaign=gemma_cta\u0026amp;utm_content#gemma-2\"\u003eGemma 2-9B\u003c/a\u003e, this version significantly improves multilingual proficiency and task performance, making it their best-performing model to date. This version also supports 11 Southeast Asian languages, as well as major dialects such as Javanese and Sundanese, while maintaining strong performance in English.\u003c/p\u003e\u003cp data-block-key=\"eorr5\"\u003eAccording to William Tjhi, head of applied research for foundation models at AI Singapore, the team chose the 9 billion parameter model over the larger base model to ensure greater accessibility: “Many SEA users are ‘throughput constrained’ and may not have the computational resources required to run inferences at scale with larger models.”\u003c/p\u003e\u003ch2 data-block-key=\"bq6ni\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eINSAIT: Building leading Bulgarian language models on Gemma 2\u003c/h2\u003e\u003cp data-block-key=\"8mese\"\u003eResearchers at the Institute for Computer Science, Artificial Intelligence, and Technology (\u003ca href=\"https://insait.ai/\"\u003eINSAIT\u003c/a\u003e) have also made incredible gains in AI language inclusivity by creating three new LLMs for the Bulgarian language. INSAIT’s latest models are built on top of the Gemma 2 family and outperform much larger Bulgarian models while importantly maintaining the skills of the base Gemma 2 model, like English and mathematical proficiency.\u003c/p\u003e\u003cp data-block-key=\"74ttt\"\u003eINSAIT’s new LLMs underscore the power of how open AI development can drive innovation in diverse linguistic contexts. The team\u0026#39;s success highlights how collaborative, openLLMs can rival—and often exceed—the capabilities of larger proprietary models.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1.original_IwAVSHb.png\" alt=\"A bar graph showing INSAIT’s latest models’ performance in Bulgarian (blue) versus previous models’ performance (grey).\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    Benchmarks showing INSAIT’s latest models’ performance in Bulgarian (blue) versus previous models’ performance (grey).\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"5uuj6\"\u003eINSAIT’s state-of-the-art Bulgarian language models demonstrate a scalable approach for other languages. Its researchers added many improvements to the base Gemma 2 model, including continuous pre-training on around 85 billion tokens in Bulgarian. They also included novel continuous pre-training, instruction-fine tuning, and a model merging scheme based on new \u003ca href=\"https://arxiv.org/abs/2407.08699v1\"\u003eresearch\u003c/a\u003e from \u003ca href=\"https://2024.emnlp.org/\"\u003eEMNLP 2024\u003c/a\u003e, a popular conference for natural language processing. The research introduces a new method for mitigating “catastrophic forgetting,” a phenomenon where AI models forget previously learned skills (English, math) after being trained on new ones (Bulgarian).\u003c/p\u003e\u003cp data-block-key=\"cf92u\"\u003e\u0026#34;The result shown by INSAIT is significant because it visibly demonstrates that even a country the size of Bulgaria can build its own state-of-the-art AI models by relying on open models, advanced AI research, and special data acquisition and training techniques,” said Martin Vechev, a full professor at ETH Zurich and scientific director of INSAIT. \u0026#34;While our models target Bulgarian, the branch-and-merge method we introduced in EMNLP 2024 to mitigate catastrophic forgetting applies to acquiring new languages.”\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiChart_chatpreference.original.png\" alt=\"Chat preference in Bulgarian based on GPT4o-as-a-Judge\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"dpqfs\"\u003eToday, INSAIT’s open models provide free access to high-performing Bulgarian language models, advancing natural language processing within Bulgaria and offering greater opportunities for others interested in developing localized AI solutions. INSAIT has even launched a \u003ca href=\"https://bggpt.ai/\"\u003enationwide public chat system\u003c/a\u003e based on its BgGPT-Gemma model variants. This is the first time a European government institution has launched a nationwide chat system based on its own publicly available, free, and open generative AI models.\u003c/p\u003e\u003ch2 data-block-key=\"cmpp2\"\u003e\u003cbr/\u003eConnecting communities through AI\u003c/h2\u003e\u003cp data-block-key=\"2bug0\"\u003eThe release of these open models from AI Singapore and INSAIT represents a significant step towards democratizing AI access and empowering local communities. Both teams highlight the importance of linguistic diversity in developing AI solutions and have shown that it is easily achievable through open-model solutions like Gemma.\u003c/p\u003e\u003cp data-block-key=\"13i9e\"\u003eThe possibilities with localized LLMs are vast, and we’re proud to see ambitious developers using the latest AI technologies to create new opportunities for their communities. That’s why we invite anyone inspired by these stories to join our \u003ca href=\"https://www.kaggle.com/competitions/gemma-language-tuning\"\u003eKaggle\u003c/a\u003e competition focused on adapting the Gemma 2 open model family for 73 eligible languages.\u003c/p\u003e\u003cp data-block-key=\"9flud\"\u003eWith this diverse selection of languages, we are compiling a foundation of resources and best practices to help developers create better and more inclusive LLMs for communities all over the world. \u003ca href=\"https://www.kaggle.com/competitions/gemma-language-tuning\"\u003eJoin the competition\u003c/a\u003e today; the final submission deadline is January 14, 2025!\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-12-20T00:00:00Z",
  "modifiedTime": null
}
