{
  "id": "1524f352-460e-4a6e-827a-d49797902d94",
  "title": "Trump revoking Biden AI EO will make industry more chaotic, experts say",
  "link": "https://venturebeat.com/ai/trump-revoking-biden-ai-eo-will-make-industry-more-chaotic-experts-say/",
  "description": "A potential repeal of President Biden's AI rules could mean enterprises will have trouble navigating state-specific laws.",
  "author": "Emilia David",
  "published": "Fri, 15 Nov 2024 22:30:55 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI regulation",
    "AI regulatory compliance",
    "AI, ML and Deep Learning",
    "Biden administration",
    "Biden administration and AI",
    "category-/Law \u0026 Government/Government",
    "category-/News/Politics",
    "election",
    "Microsoft",
    "responsible AI",
    "SB 1047",
    "Trump administration"
  ],
  "byline": "Emilia David",
  "length": 6004,
  "excerpt": "A potential repeal of President Biden's AI rules could mean enterprises will have trouble navigating state-specific laws.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "November 15, 2024 2:30 PM Credit: VentureBeat generated with MidJourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Come the new year, the incoming Trump administration is expected to make many changes to existing policies, and AI regulation will not be exempt. This will likely include repealing an AI executive order by current President Joe Biden. The Biden order established government oversight offices and encouraged model developers to implement safety standards. While the Biden AI executive order rules focus on model developers, its repeal could present some challenges for enterprises to overcome. Some companies, like Trump-ally Elon Musk’s xAI, could benefit from a repeal of the order, while others are expected to face some issues. This could include having to deal with a patchwork of regulations, less open sharing of data sources, less government-funded research and more emphasis on voluntary responsible AI programs.  Patchwork of local rules Before the EO’s signing, policymakers held several listening tours and hearings with industry leaders to determine how best to regulate technology appropriately. Under the Democratic-controlled Senate, there was a strong possibility AI regulations could move forward, but insiders believe the appetite for federal rules around AI has cooled significantly.  Gaurab Bansal, executive director of Responsible Innovation Labs, said during the ScaleUp: AI conference in New York that the lack of federal oversight of AI could lead states to write their policies.  “There’s a sense that both parties in Congress will not be regulating AI, so it will be states who may run the same playbook as California’s SB 1047,” Bansal said. “Enterprises need standards for consistency, but it’s going to be bad when there’s a patchwork of standards in different areas.”  California state legislators pushed SB 1047 — which would have mandated a “kill switch” to models among other government controls — with the bill landing on Gov. Gavin Newsom’s desk. Newsom’s veto of the bill was celebrated by industry luminaries like Meta’s Yann Le Cunn. Bansal said states are more likely to pass similar bills.  Dean Ball, a research fellow at George Mason University’s Mercatus Center, said companies may have difficulty navigating different regulations.  “Those laws may well create complex compliance regimes and a patchwork of laws for both AI developers and companies hoping to use AI; how a Republican Congress will respond to this potential challenge is unclear,” Ball said.  Voluntary responsible AI  Industry-led responsible AI has always existed. However, the burden on companies to be more proactive in being accountable and fair may heighten because their customers demand a focus on safety. Model developers and enterprise users should spend time implementing responsible AI policies and building standards that meet laws like the European Union’s AI Act.  During the ScaleUp: AI conference, Microsoft Chief Product Officer for Responsible AI Sarah Bird said many developers and their customers, including Microsoft, are readying their systems for the EU’s AI act.  But even if no sprawling law governs AI, Bird said it’s always good practice to bake responsible AI and safety into the models and applications from the onset.  “This will be helpful for start-ups, a lot of the high level of what the AI act is asking you to do is just good sense,” Bird said. “If you’re building models, you should govern the data going into them; you should test them. For smaller organizations, compliance becomes easier if you’re doing it from scratch, so invest in a solution that will govern your data as it grows.” However, understanding what is in the data used to train large language models (LLMs) that enterprises use might be harder. Jason Corso, a professor of robotics at the University of Michigan and a co-founder of computer vision company Voxel51, told VentureBeat the Biden EO encouraged a lot of openness from model developers.  “We can’t fully know the impact of one sample on a model that presents a high degree of potential bias risk, right? So model users’ businesses could be at stake if there’s no governance around the use of these models and the data that went in,” Corso said. Fewer research dollars  AI companies enjoy significant investor interest right now. However, the government has often supported research that some investors feel is too risky. Corso noted that the new Trump administration might choose not to invest in AI research to save on costs.  “I just worry about not having the government resources to put it behind those types of high-risk, early-stage projects,” Corso said. However, a new administration does not mean money will not be allocated to AI. While it’s unclear if the Trump administration will abolish the newly created AI Safety Institute and other AI oversight offices, the Biden administration did guarantee budgets until 2025. “A pending question that must color Trump’s replacement for the Biden EO is how to organize the authorities and allocate the dollars appropriated under the AI Initiative Act. This bill is the source for many of the authorities and activities Biden has tasked to agencies such as NIST and funding is set to continue in 2025. With these dollars already allocated, many activities will likely continue in some form. What that form looks like, however, has yet to be revealed,” Mercatus Center research fellow Matt Mittelsteadt said.  We’ll know how the next administration sees AI policy in January, but enterprises should prepare for whatever comes next.  VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/11/crimedy7_illustration_of_a_regulation_with_computer_code_behind_4bf03432-2247-4673-a232-c46451a1998b.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-11-15T22:30:55+00:00\" datetime=\"2024-11-15T22:30:55+00:00\"\u003eNovember 15, 2024 2:30 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2024/11/crimedy7_illustration_of_a_regulation_with_computer_code_behind_4bf03432-2247-4673-a232-c46451a1998b.png?w=750\" alt=\"Credit: VentureBeat generated with MidJourney\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat generated with MidJourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eCome the new year, the incoming Trump administration is expected to make many changes to existing policies, and AI regulation will not be exempt. This will likely include \u003ca href=\"https://venturebeat.com/ai/trump-v-p-pick-j-d-vance-praised-for-comments-seemingly-in-support-of-open-source-ai/\"\u003erepealing an AI executive order\u003c/a\u003e by current President Joe Biden.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe Biden order established government oversight offices and encouraged model developers to implement safety standards. While the Biden AI executive order rules focus on model developers, its repeal could present some challenges for enterprises to overcome. Some companies, like \u003ca href=\"https://venturebeat.com/ai/trumps-victory-will-benefit-elon-musk-and-xai/\"\u003eTrump-ally Elon Musk’s xAI\u003c/a\u003e, could benefit from a repeal of the order, while others are expected to face some issues. This could include having to deal with a patchwork of regulations, less open sharing of data sources, less government-funded research and more emphasis on voluntary responsible AI programs. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-patchwork-of-local-rules\"\u003ePatchwork of local rules\u003c/h2\u003e\n\n\n\n\u003cp\u003eBefore the EO’s signing, policymakers held several listening tours and hearings with industry leaders to determine how best to regulate technology appropriately. Under the Democratic-controlled Senate, there was a strong possibility AI regulations could move forward, but insiders believe the appetite for federal rules around AI has cooled significantly. \u003c/p\u003e\n\n\n\n\u003cp\u003eGaurab Bansal, executive director of \u003ca href=\"https://www.rilabs.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eResponsible Innovation Labs\u003c/a\u003e, said during the ScaleUp: AI conference in New York that the lack of federal oversight of AI could lead states to write their policies. \u003c/p\u003e\n\n\n\n\u003cp\u003e“There’s a sense that both parties in Congress will not be regulating AI, so it will be states who may run the same playbook as California’s SB 1047,” Bansal said. “Enterprises need standards for consistency, but it’s going to be bad when there’s a patchwork of standards in different areas.” \u003c/p\u003e\n\n\n\n\u003cp\u003eCalifornia state legislators \u003ca href=\"https://venturebeat.com/ai/proposed-law-to-control-powerful-ai-models-will-destroy-californias-nascent-industry/\"\u003epushed SB 1047\u003c/a\u003e — which would have mandated a “kill switch” to models among other government controls — with the bill landing on Gov. Gavin Newsom’s desk. \u003ca href=\"https://venturebeat.com/ai/california-ai-bill-veto-could-allow-smaller-devs-models-to-flourish/\"\u003eNewsom’s veto of the bill\u003c/a\u003e was celebrated by industry luminaries like Meta’s Yann Le Cunn. Bansal said states are more likely to pass similar bills. \u003c/p\u003e\n\n\n\n\u003cp\u003eDean Ball, a research fellow at \u003ca href=\"https://www.mercatus.org/\"\u003eGeorge Mason University’s Mercatus Center\u003c/a\u003e, said companies may have difficulty navigating different regulations. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Those laws may well create complex compliance regimes and a patchwork of laws for both AI developers and companies hoping to use AI; how a Republican Congress will respond to this potential challenge is unclear,” Ball said. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-voluntary-responsible-ai-nbsp\"\u003eVoluntary responsible AI \u003c/h2\u003e\n\n\n\n\u003cp\u003eIndustry-led responsible AI has always existed. However, the burden on companies to be more proactive in being accountable and fair may heighten because their customers demand a focus on safety. Model developers and enterprise users should spend time implementing responsible AI policies and building standards that meet \u003ca href=\"https://venturebeat.com/ai/eu-parliament-officially-adopts-ai-act-landmark-regulation-likely-to-become-law-in-may/\"\u003elaws like the European Union’s AI Act\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eDuring the ScaleUp: AI conference, \u003ca href=\"https://www.microsoft.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMicrosoft\u003c/a\u003e Chief Product Officer for Responsible AI Sarah Bird said many developers and their customers, \u003ca href=\"https://venturebeat.com/ai/at-venturebeats-ai-impact-tour-microsoft-explores-the-risks-and-rewards-of-gen-ai/\"\u003eincluding Microsoft\u003c/a\u003e, are readying their systems for the EU’s AI act. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut even if no sprawling law governs AI, Bird said it’s always good practice to bake responsible AI and safety into the models and applications from the onset. \u003c/p\u003e\n\n\n\n\u003cp\u003e“This will be helpful for start-ups, a lot of the high level of what the AI act is asking you to do is just good sense,” Bird said. “If you’re building models, you should govern the data going into them; you should test them. For smaller organizations, compliance becomes easier if you’re doing it from scratch, so invest in a solution that will govern your data as it grows.”\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, understanding what is in the data used to train large language models (LLMs) that enterprises use might be harder. Jason Corso, a professor of robotics at the University of Michigan and a co-founder of computer vision company \u003ca href=\"https://voxel51.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eVoxel51\u003c/a\u003e, told VentureBeat the Biden EO encouraged a lot of openness from model developers. \u003c/p\u003e\n\n\n\n\u003cp\u003e“We can’t fully know the impact of one sample on a model that presents a high degree of potential bias risk, right? So model users’ businesses could be at stake if there’s no governance around the use of these models and the data that went in,” Corso said.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-fewer-research-dollars-nbsp\"\u003eFewer research dollars \u003c/h2\u003e\n\n\n\n\u003cp\u003eAI companies enjoy significant investor interest right now. However, the government has often supported research that some investors feel is too risky. Corso noted that the new Trump administration might choose not to invest in AI research to save on costs. \u003c/p\u003e\n\n\n\n\u003cp\u003e“I just worry about not having the government resources to put it behind those types of high-risk, early-stage projects,” Corso said.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, a new administration does not mean money will not be allocated to AI. While it’s unclear if the Trump administration will abolish the newly created AI Safety Institute and other AI oversight offices, the Biden administration did guarantee budgets until 2025.\u003c/p\u003e\n\n\n\n\u003cp\u003e“A pending question that must color Trump’s replacement for the Biden EO is how to organize the authorities and allocate the dollars appropriated under the AI Initiative Act. This bill is the source for many of the authorities and activities Biden has tasked to agencies such as NIST and funding is set to continue in 2025. With these dollars already allocated, many activities will likely continue in some form. What that form looks like, however, has yet to be revealed,” Mercatus Center research fellow Matt Mittelsteadt said. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe’ll know how the next administration sees AI policy in January, but enterprises should prepare for whatever comes next. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-11-15T22:30:55Z",
  "modifiedTime": "2024-11-15T22:31:03Z"
}
