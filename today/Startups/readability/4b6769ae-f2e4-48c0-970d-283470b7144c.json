{
  "id": "4b6769ae-f2e4-48c0-970d-283470b7144c",
  "title": "The ‘strawberrry’ problem: How to overcome AI’s limitations",
  "link": "https://venturebeat.com/ai/the-strawberrry-problem-how-to-overcome-ais-limitations/",
  "description": "A simple letter counting experiment exposes a fundamental limitation of LLMs ChatGPT and Claude, proving they can't yet “think” like humans.",
  "author": "Chinmay Jog, Pangiam",
  "published": "Sat, 12 Oct 2024 19:05:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "AI, ML and Deep Learning",
    "category-/News",
    "ChatGPT",
    "Claude",
    "Conversational AI",
    "Generative AI",
    "large language models",
    "NLP",
    "prompt engineering",
    "prompting",
    "tokenization"
  ],
  "byline": "Chinmay Jog, Pangiam",
  "length": 4851,
  "excerpt": "A simple letter counting experiment exposes a fundamental limitation of LLMs ChatGPT and Claude, proving they can't yet “think” like humans.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More By now, large language models (LLMs) like ChatGPT and Claude have become an everyday word across the globe. Many people have started worrying that AI is coming for their jobs, so it is ironic to see almost all LLM-based systems flounder at a straightforward task: Counting the number of “r”s in the word “strawberry.” They are not exclusively failing at the alphabet “r”; other examples include counting “m”s in “mammal”, and “p”s in “hippopotamus.” In this article, I will break down the reason for these failures and provide a simple workaround. LLMs are powerful AI systems trained on vast amounts of text to understand and generate human-like language. They excel at tasks like answering questions, translating languages, summarizing content and even generating creative writing by predicting and constructing coherent responses based on the input they receive. LLMs are designed to recognize patterns in text, which allows them to handle a wide range of language-related tasks with impressive accuracy. Despite their prowess, failing at counting the number of “r”s in the word “strawberry” is a reminder that LLMs are not capable of “thinking” like humans. They do not process the information we feed them like a human would. Conversation with ChatGPT and Claude about the number of “r”s in strawberry. Almost all the current high performance LLMs are built on transformers. This deep learning architecture doesn’t directly ingest text as their input. They use a process called tokenization, which transforms the text into numerical representations, or tokens. Some tokens might be full words (like “monkey”), while others could be parts of a word (like “mon” and “key”). Each token is like a code that the model understands. By breaking everything down into tokens, the model can better predict the next token in a sentence.  LLMs don’t memorize words; they try to understand how these tokens fit together in different ways, making them good at guessing what comes next. In the case of the word “hippopotamus,” the model might see the tokens of letters “hip,” “pop,” “o” and “tamus”, and not know that the word “hippopotamus” is made of the letters — “h”, “i”, “p”, “p”, “o”, “p”, “o”, “t”, “a”, “m”, “u”, “s”. A model architecture that can directly look at individual letters without tokenizing them may potentially not have this problem, but for today’s transformer architectures, it is not computationally feasible. Further, looking at how LLMs generate output text: They predict what the next word will be based on the previous input and output tokens. While this works for generating contextually aware human-like text, it is not suitable for simple tasks like counting letters. When asked to answer the number of “r”s in the word “strawberry”, LLMs are purely predicting the answer based on the structure of the input sentence. Here’s a workaround While LLMs might not be able to “think” or logically reason, they are adept at understanding structured text. A splendid example of structured text is computer code, of many many programming languages. If we ask ChatGPT to use Python to count the number of “r”s in “strawberry”, it will most likely get the correct answer. When there is a need for LLMs to do counting or any other task that may require logical reasoning or arithmetic computation, the broader software can be designed such that the prompts include asking the LLM to use a programming language to process the input query. Conclusion A simple letter counting experiment exposes a fundamental limitation of LLMs like ChatGPT and Claude. Despite their impressive capabilities in generating human-like text, writing code and answering any question thrown at them, these AI models cannot yet “think” like a human. The experiment shows the models for what they are, pattern matching predictive algorithms, and not “intelligence” capable of understanding or reasoning. However, having a prior knowledge of what type of prompts work well can alleviate the problem to some extent. As the integration of AI in our lives increases, recognizing its limitations is crucial for responsible usage and realistic expectations of these models.  Chinmay Jog is a senior machine learning engineer at Pangiam. DataDecisionMakers Welcome to the VentureBeat community! DataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation. If you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers. You might even consider contributing an article of your own! Read More From DataDecisionMakers",
  "image": "https://venturebeat.com/wp-content/uploads/2024/10/image4.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eBy now, large language models (\u003ca href=\"https://www.ibm.com/topics/large-language-models\"\u003eLLMs\u003c/a\u003e) like ChatGPT and Claude have become an everyday word across the globe. Many people have started worrying that \u003ca href=\"https://www.fastcompany.com/91054351/ai-job-security-2024-survey\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAI is coming for their jobs\u003c/a\u003e, so it is ironic to see almost all LLM-based systems flounder at a straightforward task: Counting the number of “r”s in the word “strawberry.” They are not exclusively failing at the alphabet “r”; other examples include counting “m”s in “mammal”, and “p”s in “hippopotamus.” In this article, I will break down the reason for these failures and provide a simple workaround. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://venturebeat.com/ai/why-its-healthy-to-be-skeptical-about-ai/\"\u003eLLMs are powerful AI systems\u003c/a\u003e trained on vast amounts of text to understand and generate human-like language. They excel at tasks like answering questions, translating languages, summarizing content and even generating creative writing by predicting and constructing coherent responses based on the input they receive. LLMs are designed to recognize patterns in text, which allows them to handle a wide range of language-related tasks with impressive accuracy.\u003c/p\u003e\n\n\n\n\u003cp\u003eDespite their prowess, failing at counting the number of “r”s in the word “\u003ca href=\"https://venturebeat.com/ai/sam-altman-stokes-rumors-of-new-openai-foundation-model-strawberry/\"\u003estrawberry\u003c/a\u003e” is a reminder that LLMs are not capable of “thinking” like humans. They do not process the information we feed them like a human would.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1075\" height=\"270\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/image2.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/10/image2.png 1075w, https://venturebeat.com/wp-content/uploads/2024/10/image2.png?resize=300,75 300w, https://venturebeat.com/wp-content/uploads/2024/10/image2.png?resize=768,193 768w, https://venturebeat.com/wp-content/uploads/2024/10/image2.png?resize=800,201 800w, https://venturebeat.com/wp-content/uploads/2024/10/image2.png?resize=400,100 400w, https://venturebeat.com/wp-content/uploads/2024/10/image2.png?resize=750,188 750w, https://venturebeat.com/wp-content/uploads/2024/10/image2.png?resize=578,145 578w, https://venturebeat.com/wp-content/uploads/2024/10/image2.png?resize=930,234 930w\" sizes=\"(max-width: 1075px) 100vw, 1075px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"956\" height=\"240\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/image3.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/10/image3.png 956w, https://venturebeat.com/wp-content/uploads/2024/10/image3.png?resize=300,75 300w, https://venturebeat.com/wp-content/uploads/2024/10/image3.png?resize=768,193 768w, https://venturebeat.com/wp-content/uploads/2024/10/image3.png?resize=800,201 800w, https://venturebeat.com/wp-content/uploads/2024/10/image3.png?resize=400,100 400w, https://venturebeat.com/wp-content/uploads/2024/10/image3.png?resize=750,188 750w, https://venturebeat.com/wp-content/uploads/2024/10/image3.png?resize=578,145 578w, https://venturebeat.com/wp-content/uploads/2024/10/image3.png?resize=930,233 930w\" sizes=\"(max-width: 956px) 100vw, 956px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eConversation with ChatGPT and Claude about the number of “r”s in strawberry.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAlmost all the current high performance LLMs are built on \u003ca href=\"https://blogs.nvidia.com/blog/what-is-a-transformer-model/\"\u003etransformers\u003c/a\u003e. This deep learning architecture  doesn’t directly ingest text as their input. They use a process called \u003ca href=\"https://christophergs.com/blog/understanding-llm-tokenization\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etokenization\u003c/a\u003e, which transforms the text into numerical representations, or tokens. Some tokens might be full words (like “monkey”), while others could be parts of a word (like “mon” and “key”). Each token is like a code that the model understands. By breaking everything down into tokens, the model can better predict the next token in a sentence. \u003c/p\u003e\n\n\n\n\u003cp\u003eLLMs don’t memorize words; they try to understand how these tokens fit together in different ways, making them good at guessing what comes next. In the case of the word “hippopotamus,” the model might see the tokens of letters “hip,” “pop,” “o” and “tamus”, and not know that the word “hippopotamus” is made of the letters — “h”, “i”, “p”, “p”, “o”, “p”, “o”, “t”, “a”, “m”, “u”, “s”.\u003c/p\u003e\n\n\n\n\u003cp\u003eA model architecture that can directly look at individual letters without tokenizing them may potentially not have this problem, but for today’s transformer architectures, it is not computationally feasible.\u003c/p\u003e\n\n\n\n\u003cp\u003eFurther, looking at how LLMs generate output text: They \u003ca href=\"https://www.nytimes.com/2023/03/28/technology/ai-chatbots-chatgpt-bing-bard-llm.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epredict\u003c/a\u003e what the next word will be based on the previous input and output tokens. While this works for generating contextually aware human-like text, it is not suitable for simple tasks like counting letters. When asked to answer the number of “r”s in the word “strawberry”, LLMs are purely predicting the answer based on the structure of the input sentence.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-here-s-a-workaround\"\u003eHere’s a workaround\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile LLMs might not be able to “think” or logically reason, they are adept at understanding structured text. A splendid example of structured text is computer code, of many many programming languages. If we ask ChatGPT to use Python to count the number of “r”s in “strawberry”, it will most likely get the correct answer. When there is a need for LLMs to do counting or any other task that may require logical reasoning or arithmetic computation, the broader software can be designed such that the \u003ca href=\"https://venturebeat.com/ai/how-to-prompt-on-openai-o1/\"\u003eprompts\u003c/a\u003e include asking the LLM to use a programming language to process the input query.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"912\" height=\"658\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/image1.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/10/image1.png 912w, https://venturebeat.com/wp-content/uploads/2024/10/image1.png?resize=300,216 300w, https://venturebeat.com/wp-content/uploads/2024/10/image1.png?resize=768,554 768w, https://venturebeat.com/wp-content/uploads/2024/10/image1.png?resize=800,577 800w, https://venturebeat.com/wp-content/uploads/2024/10/image1.png?resize=400,289 400w, https://venturebeat.com/wp-content/uploads/2024/10/image1.png?resize=750,541 750w, https://venturebeat.com/wp-content/uploads/2024/10/image1.png?resize=578,417 578w\" sizes=\"(max-width: 912px) 100vw, 912px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-conclusion\"\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eA simple letter counting experiment exposes a fundamental limitation of LLMs like ChatGPT and Claude. Despite their impressive capabilities in generating human-like text, writing code and answering any question thrown at them, these AI models cannot yet “think” like a human. The experiment shows the models for what they are, pattern matching predictive algorithms, and not “intelligence” capable of understanding or reasoning. However, having a prior knowledge of what type of prompts work well can alleviate the problem to some extent. As the integration of AI in our lives increases, recognizing its limitations is crucial for responsible usage and realistic expectations of these models.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003e Chinmay Jog is a senior machine learning engineer at \u003ca href=\"https://pangiam.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePangiam\u003c/a\u003e. \u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2736392\"\u003e\n\u003cp\u003e\u003cstrong\u003eDataDecisionMakers\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWelcome to the VentureBeat community!\u003c/p\u003e\n\n\n\n\u003cp\u003eDataDecisionMakers is where experts, including the technical people doing data work, can share data-related insights and innovation.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to read about cutting-edge ideas and up-to-date information, best practices, and the future of data and data tech, join us at DataDecisionMakers.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might even consider \u003ca rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://venturebeat.com/guest-posts/\"\u003econtributing an article\u003c/a\u003e of your own!\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca rel=\"noreferrer noopener\" href=\"https://venturebeat.com/category/DataDecisionMakers/\" target=\"_blank\"\u003eRead More From DataDecisionMakers\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-10-12T19:05:00Z",
  "modifiedTime": "2024-10-12T17:01:43Z"
}
