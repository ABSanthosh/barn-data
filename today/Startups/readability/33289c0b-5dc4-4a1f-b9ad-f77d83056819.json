{
  "id": "33289c0b-5dc4-4a1f-b9ad-f77d83056819",
  "title": "Beyond ARC-AGI: GAIA and the search for a real intelligence benchmark",
  "link": "https://venturebeat.com/ai/beyond-arc-agi-gaia-and-the-search-for-a-real-intelligence-benchmark/",
  "description": "GUEST: Intelligence is pervasive, yet its measurement seems subjective. At best, we approximate its measure through tests and benchmarks. Think of college entrance exams: Every year, countless students sign up, memorize test-prep tricks and sometimes walk away with perfect scores. Does a single number, say a 100%, mean those who got it share the same intelligence […]",
  "author": "Sri Ambati, H2O.ai",
  "published": "Sun, 13 Apr 2025 23:05:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "AI, ML and Deep Learning",
    "category-/News",
    "GAIA benchmark",
    "Generative AI",
    "large language models",
    "NLP"
  ],
  "byline": "Sri Ambati, H2O.ai",
  "length": 5736,
  "excerpt": "Guest Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Intelligence is pervasive, yet its measurement seems subjective. At best, we approximate its measure through tests and benchmarks. Think of college entrance exams: Every year, countless students sign up, memorize test-prep tricks and sometimes walk away […]",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Intelligence is pervasive, yet its measurement seems subjective. At best, we approximate its measure through tests and benchmarks. Think of college entrance exams: Every year, countless students sign up, memorize test-prep tricks and sometimes walk away with perfect scores. Does a single number, say a 100%, mean those who got it share the same intelligence — or that they’ve somehow maxed out their intelligence? Of course not. Benchmarks are approximations, not exact measurements of someone’s — or something’s — true capabilities. The generative AI community has long relied on benchmarks like MMLU (Massive Multitask Language Understanding) to evaluate model capabilities through multiple-choice questions across academic disciplines. This format enables straightforward comparisons, but fails to truly capture intelligent capabilities. Both Claude 3.5 Sonnet and GPT-4.5, for instance, achieve similar scores on this benchmark. On paper, this suggests equivalent capabilities. Yet people who work with these models know that there are substantial differences in their real-world performance. What does it mean to measure ‘intelligence’ in AI? On the heels of the new ARC-AGI benchmark release — a test designed to push models toward general reasoning and creative problem-solving — there’s renewed debate around what it means to measure “intelligence” in AI. While not everyone has tested the ARC-AGI benchmark yet, the industry welcomes this and other efforts to evolve testing frameworks. Every benchmark has its merit, and ARC-AGI is a promising step in that broader conversation.  Another notable recent development in AI evaluation is ‘Humanity’s Last Exam,’ a comprehensive benchmark containing 3,000 peer-reviewed, multi-step questions across various disciplines. While this test represents an ambitious attempt to challenge AI systems at expert-level reasoning, early results show rapid progress — with OpenAI reportedly achieving a 26.6% score within a month of its release. However, like other traditional benchmarks, it primarily evaluates knowledge and reasoning in isolation, without testing the practical, tool-using capabilities that are increasingly crucial for real-world AI applications. In one example, multiple state-of-the-art models fail to correctly count the number of “r”s in the word strawberry. In another, they incorrectly identify 3.8 as being smaller than 3.1111. These kinds of failures — on tasks that even a young child or basic calculator could solve — expose a mismatch between benchmark-driven progress and real-world robustness, reminding us that intelligence is not just about passing exams, but about reliably navigating everyday logic. The new standard for measuring AI capability As models have advanced, these traditional benchmarks have shown their limitations — GPT-4 with tools achieves only about 15% on more complex, real-world tasks in the GAIA benchmark, despite impressive scores on multiple-choice tests. This disconnect between benchmark performance and practical capability has become increasingly problematic as AI systems move from research environments into business applications. Traditional benchmarks test knowledge recall but miss crucial aspects of intelligence: The ability to gather information, execute code, analyze data and synthesize solutions across multiple domains. GAIA is the needed shift in AI evaluation methodology. Created through collaboration between Meta-FAIR, Meta-GenAI, HuggingFace and AutoGPT teams, the benchmark includes 466 carefully crafted questions across three difficulty levels. These questions test web browsing, multi-modal understanding, code execution, file handling and complex reasoning — capabilities essential for real-world AI applications. Level 1 questions require approximately 5 steps and one tool for humans to solve. Level 2 questions demand 5 to 10 steps and multiple tools, while Level 3 questions can require up to 50 discrete steps and any number of tools. This structure mirrors the actual complexity of business problems, where solutions rarely come from a single action or tool. By prioritizing flexibility over complexity, an AI model reached 75% accuracy on GAIA — outperforming industry giants Microsoft’s Magnetic-1 (38%) and Google’s Langfun Agent (49%). Their success stems from using a combination of specialized models for audio-visual understanding and reasoning, with Anthropic’s Sonnet 3.5 as the primary model. This evolution in AI evaluation reflects a broader shift in the industry: We’re moving from standalone SaaS applications to AI agents that can orchestrate multiple tools and workflows. As businesses increasingly rely on AI systems to handle complex, multi-step tasks, benchmarks like GAIA provide a more meaningful measure of capability than traditional multiple-choice tests. The future of AI evaluation lies not in isolated knowledge tests but in comprehensive assessments of problem-solving ability. GAIA sets a new standard for measuring AI capability — one that better reflects the challenges and opportunities of real-world AI deployment. Sri Ambati is the founder and CEO of H2O.ai. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/04/upscalemedia-transformed_8247a6.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eIntelligence is pervasive, yet its measurement seems subjective. At best, we approximate its measure through tests and benchmarks. Think of college entrance exams: Every year, countless students sign up, memorize test-prep tricks and sometimes walk away with perfect scores. Does a single number, say a 100%, mean those who got it share the same intelligence — or that they’ve somehow maxed out their intelligence? Of course not. Benchmarks are approximations, not exact measurements of someone’s — or something’s — true capabilities.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe \u003ca href=\"https://venturebeat.com/ai/bigger-isnt-always-better-examining-the-business-case-for-multi-million-token-llms/\"\u003egenerative AI\u003c/a\u003e community has long relied on benchmarks like \u003ca href=\"https://paperswithcode.com/dataset/mmlu\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMMLU\u003c/a\u003e (Massive Multitask Language Understanding) to evaluate model capabilities through multiple-choice questions across academic disciplines. This format enables straightforward comparisons, but fails to truly capture intelligent capabilities.\u003c/p\u003e\n\n\n\n\u003cp\u003eBoth Claude 3.5 Sonnet and GPT-4.5, for instance, achieve similar scores on this benchmark. On paper, this suggests equivalent capabilities. Yet people who work with these models know that there are substantial differences in their real-world performance.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-does-it-mean-to-measure-intelligence-in-ai\"\u003eWhat does it mean to measure ‘intelligence’ in AI? \u003c/h2\u003e\n\n\n\n\u003cp\u003eOn the heels of the new \u003ca href=\"https://arcprize.org/arc-agi\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eARC-AGI\u003c/a\u003e benchmark release — a test designed to push models toward general reasoning and creative problem-solving — there’s renewed debate around what it means to measure “intelligence” in AI. While not everyone has tested the ARC-AGI benchmark yet, the industry welcomes this and other efforts to evolve testing frameworks. Every benchmark has its merit, and ARC-AGI is a promising step in that broader conversation. \u003c/p\u003e\n\n\n\n\u003cp\u003eAnother notable recent development in AI evaluation is ‘\u003ca href=\"https://agi.safe.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHumanity’s Last Exam\u003c/a\u003e,’ a comprehensive benchmark containing 3,000 peer-reviewed, multi-step questions across various disciplines. While this test represents an ambitious attempt to challenge AI systems at expert-level reasoning, early results show rapid progress — with OpenAI reportedly achieving a 26.6% score within a month of its release. However, like other traditional benchmarks, it primarily evaluates knowledge and reasoning in isolation, without testing the practical, tool-using capabilities that are increasingly crucial for real-world AI applications.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn one example, multiple \u003ca href=\"https://venturebeat.com/ai/mips-to-exaflops-in-just-40-years-compute-power-is-exploding-and-it-will-transform-ai/\"\u003estate-of-the-art models\u003c/a\u003e fail to correctly count the number of “r”s in the word strawberry. In another, they incorrectly identify 3.8 as being smaller than 3.1111. These kinds of failures — on tasks that even a young child or basic calculator could solve — expose a mismatch between benchmark-driven progress and real-world robustness, reminding us that intelligence is not just about passing exams, but about reliably navigating everyday logic.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"2048\" height=\"1160\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/unnamed_1d674b.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/unnamed_1d674b.png 2048w, https://venturebeat.com/wp-content/uploads/2025/04/unnamed_1d674b.png?resize=300,170 300w, https://venturebeat.com/wp-content/uploads/2025/04/unnamed_1d674b.png?resize=768,435 768w, https://venturebeat.com/wp-content/uploads/2025/04/unnamed_1d674b.png?resize=800,453 800w, https://venturebeat.com/wp-content/uploads/2025/04/unnamed_1d674b.png?resize=1536,870 1536w, https://venturebeat.com/wp-content/uploads/2025/04/unnamed_1d674b.png?resize=400,227 400w, https://venturebeat.com/wp-content/uploads/2025/04/unnamed_1d674b.png?resize=750,425 750w, https://venturebeat.com/wp-content/uploads/2025/04/unnamed_1d674b.png?resize=578,327 578w, https://venturebeat.com/wp-content/uploads/2025/04/unnamed_1d674b.png?resize=930,527 930w\" sizes=\"(max-width: 2048px) 100vw, 2048px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-the-new-standard-for-measuring-ai-capability\"\u003eThe new standard for measuring AI capability\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs models have advanced, these traditional benchmarks have shown their limitations — GPT-4 with tools achieves only about 15% on more complex, real-world tasks in the \u003ca href=\"https://huggingface.co/gaia-benchmark\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGAIA benchmark\u003c/a\u003e, despite impressive scores on multiple-choice tests.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis disconnect between benchmark performance and practical capability has become increasingly problematic as \u003ca href=\"https://venturebeat.com/ai/from-ai-agent-hype-to-practicality-why-enterprises-must-consider-fit-over-flash/\"\u003eAI systems\u003c/a\u003e move from research environments into business applications. Traditional benchmarks test knowledge recall but miss crucial aspects of intelligence: The ability to gather information, execute code, analyze data and synthesize solutions across multiple domains.\u003c/p\u003e\n\n\n\n\u003cp\u003eGAIA is the needed shift in AI evaluation methodology. Created through collaboration between Meta-FAIR, Meta-GenAI, HuggingFace and AutoGPT teams, the benchmark includes 466 carefully crafted questions across three difficulty levels. These questions test web browsing, multi-modal understanding, code execution, file handling and complex reasoning — capabilities essential for real-world AI applications.\u003c/p\u003e\n\n\n\n\u003cp\u003eLevel 1 questions require approximately 5 steps and one tool for humans to solve. Level 2 questions demand 5 to 10 steps and multiple tools, while Level 3 questions can require up to 50 discrete steps and any number of tools. This structure mirrors the actual complexity of business problems, where solutions rarely come from a single action or tool.\u003c/p\u003e\n\n\n\n\u003cp\u003eBy prioritizing flexibility over complexity, an AI model reached 75% accuracy on GAIA — outperforming industry giants Microsoft’s Magnetic-1 (38%) and Google’s Langfun Agent (49%). Their success stems from using a combination of specialized models for audio-visual understanding and reasoning, with Anthropic’s Sonnet 3.5 as the primary model.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis evolution in AI evaluation reflects a broader shift in the industry: We’re moving from standalone SaaS applications to AI agents that can orchestrate multiple tools and workflows. As businesses increasingly rely on AI systems to handle complex, multi-step tasks, benchmarks like GAIA provide a more meaningful measure of capability than traditional multiple-choice tests.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe future of AI evaluation lies not in isolated knowledge tests but in comprehensive assessments of problem-solving ability. GAIA sets a new standard for measuring AI capability — one that better reflects the challenges and opportunities of real-world AI deployment.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eSri Ambati is the founder and CEO of \u003ca href=\"https://h2o.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eH2O.ai\u003c/a\u003e. \u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-04-13T23:05:00Z",
  "modifiedTime": "2025-04-13T23:04:38Z"
}
