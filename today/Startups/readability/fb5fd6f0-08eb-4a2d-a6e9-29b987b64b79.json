{
  "id": "fb5fd6f0-08eb-4a2d-a6e9-29b987b64b79",
  "title": "ByteDance’s UI-TARS can take over your computer, outperforms GPT-4o and Claude",
  "link": "https://venturebeat.com/ai/bytedances-ui-tars-can-take-over-your-computer-outperforms-gpt-4o-and-claude/",
  "description": "UI-TARS understands graphical user interfaces (GUIs), applies reasoning and takes autonomous, step-by-step action.",
  "author": "Taryn Plumb",
  "published": "Wed, 22 Jan 2025 22:58:46 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI, ML and Deep Learning",
    "Bytedance",
    "category-/News",
    "Claude",
    "Conversational AI",
    "Gemini",
    "Generative AI",
    "gpt-4o",
    "NLP"
  ],
  "byline": "Taryn Plumb",
  "length": 7755,
  "excerpt": "UI-TARS understands graphical user interfaces (GUIs), applies reasoning and takes autonomous, step-by-step action.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More A new AI agent has emerged from the parent company of TikTok to take control of your computer and perform complex workflows. Much like Anthropic’s Computer Use, ByteDance’s new UI-TARS understands graphical user interfaces (GUIs), applies reasoning and takes autonomous, step-by-step action.  Trained on roughly 50B tokens and offered in 7B and 72B parameter versions, the PC/MacOS agents achieves state-of-the-art (SOTA) performance on 10-plus GUI benchmarks across performance, perception, grounding and overall agent capabilities, consistently beating out OpenAI’s GPT-4o, Claude and Google’s Gemini. “Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention,” researchers from ByteDance and Tsinghua University write in a new research paper.  Source: Arxiv How UI-TARS explains its thinking UI-TARS works across desktop, mobile and web applications, using multimodal inputs (text, images, interactions) to understand visual environments. Its UI features two tabs — one to the left showing its step-by-step “thinking,” and a larger one to the right where it pulls up files, websites and apps and automatically takes action.  For example, in a demo video released today, the model is prompted to “Find round trip flights from SEA to NYC on the 5th and return on the 10th next month and filter by price in ascending order.” In response, UI-TARS navigates to the website for Delta Airlines, fills in the “from” and “to” fields, clicks in the relevant dates and sorts and filters by price, explaining each step in its thinking box before taking action.  In another scenario, it is instructed to install an autoDocstring extension in VS Code. Here is its step-by-step thinking as it completes that task:  It first reports that “to proceed with installing the autoDocstring extension in VS Code, I first need to open the VS Code application.”  Recognizing that the VS Code window has not fully loaded, the model notes that “it is necessary to wait for the application to finish initializing before proceeding with the installation of the autoDocstring extension. This ensures that all required functionalities are available for the next steps.”  It then explains that it must access the Extensions view in VS Code, which can be done by clicking on a tab in the left sidebar.  Running into a small glitch, it reasons: “The previous attempt to click on the ‘Extensions’ tab may not have been precise enough, so I will try clicking on it again to ensure the Extensions view opens successfully.” Once in the Extensions view, it types in ‘autoDocstring’ before concluding: “Since the installation process has already been initiated, there is no further action required at this moment. I will wait for the installation to complete and confirm its success once the process is finished.” Outperforming its rivals Across a variety of benchmarks, researchers report that UI-TARS consistently outranked OpenAI’s GPT-4o; Anthropic’s Claude-3.5-Sonnet; Gemini-1.5-Pro and Gemini-2.0; four Qwen models; and numerous academic models. For instance, in VisualWebBench — which measures a model’s ability to ground web elements including webpage quality assurance and optical character recognition — UI-TARS 72B scored 82.8%, outperforming GPT-4o (78.5%) and Claude 3.5 (78.2%).  It also did significantly better on WebSRC benchmarks (understanding of semantic content and layout in web contexts) and ScreenQA-short (comprehension of complex mobile screen layouts and web structure). UI-TARS-7B achieved leading scores of 93.6% on WebSRC, while UI-TARS-72B achieved 88.6% on ScreenQA-short, outperforming Qwen, Gemini, Claude 3.5 and GPT-4o.  “These results demonstrate the superior perception and comprehension capabilities of UI-TARS in web and mobile environments,” the researchers write. “Such perceptual ability lays the foundation for agent tasks, where accurate environmental understanding is crucial for task execution and decision-making.” UI-TARS also showed impressive results in ScreenSpot Pro and ScreenSpot v2 , which assess a model’s ability to understand and localize elements in GUIs. Further, researchers tested its capabilities in planning multi-step actions and low-level tasks in mobile environments, and benchmarked it on OSWorld (which assesses open-ended computer tasks) and AndroidWorld (which scores autonomous agents on 116 programmatic tasks across 20 mobile apps).  Source: Arxiv Source: Arxiv Under the hood To help it take step-by-step actions and recognize what it’s seeing, UI-TARS was trained on a large-scale dataset of screenshots that parsed metadata including element description and type, visual description, bounding boxes (position information), element function and text from various websites, applications and operating systems. This allows the model to provide a comprehensive, detailed description of a screenshot, capturing not only elements but spatial relationships and overall layout.  The model also uses state transition captioning to identify and describe the differences between two consecutive screenshots and determine whether an action — such as a mouse click or keyboard input — has occurred. Meanwhile, set-of-mark (SoM) prompting allows it to overlay distinct marks (letters, numbers) on specific regions of an image.  The model is equipped with both short-term and long-term memory to handle tasks at hand while also retaining historical interactions to improve later decision-making. Researchers trained the model to perform both System 1 (fast, automatic and intuitive) and System 2 (slow and deliberate) reasoning. This allows for multi-step decision-making, “reflection” thinking, milestone recognition and error correction.  Researchers emphasized that it is critical that the model be able to maintain consistent goals and engage in trial and error to hypothesize, test and evaluate potential actions before completing a task. They introduced two types of data to support this: error correction and post-reflection data. For error correction, they identified mistakes and labeled corrective actions; for post-reflection, they simulated recovery steps.  “This strategy ensures that the agent not only learns to avoid errors but also adapts dynamically when they occur,” the researchers write. Clearly, UI-TARS exhibits impressive capabilities, and it’ll be interesting to see its evolving use cases in the increasingly competitive AI agents space. As the researchers note: “Looking ahead, while native agents represent a significant leap forward, the future lies in the integration of active and lifelong learning, where agents autonomously drive their own learning through continuous, real-world interactions.” Researchers point out that Claude Computer Use “performs strongly in web-based tasks but significantly struggles with mobile scenarios, indicating that the GUI operation ability of Claude has not been well transferred to the mobile domain.”  By contrast, “UI-TARS exhibits excellent performance in both website and mobile domain.”  Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/01/a-vector-art-of-a-sophisticated-ai-agent_JBXu7d2gTlyb2aVU-Z7UqQ_VnQU_ayuQuWKl6roKyBHQA-transformed.jpeg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eA new AI agent has emerged from the parent company of TikTok to take control of your computer and perform complex workflows.\u003c/p\u003e\n\n\n\n\u003cp\u003eMuch like Anthropic’s \u003ca href=\"https://venturebeat.com/ai/anthropics-agentic-computer-use-is-giving-people-superpowers/\"\u003eComputer Use\u003c/a\u003e, ByteDance’s new UI-TARS understands graphical user interfaces (GUIs), applies reasoning and takes autonomous, step-by-step action. \u003c/p\u003e\n\n\n\n\u003cp\u003eTrained on roughly 50B tokens and offered in 7B and 72B parameter versions, the PC/MacOS agents achieves state-of-the-art (SOTA) performance on 10-plus GUI benchmarks across performance, perception, grounding and overall agent capabilities, consistently beating out OpenAI’s GPT-4o, Claude and \u003ca href=\"https://venturebeat.com/ai/google-releases-free-gemini-2-0-flash-thinking-model-pressuring-openais-premium-strategy/\"\u003eGoogle’s Gemini\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"586\" height=\"558\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/Image-1-2.png\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/Image-1-2.png 586w, https://venturebeat.com/wp-content/uploads/2025/01/Image-1-2.png?resize=300,286 300w, https://venturebeat.com/wp-content/uploads/2025/01/Image-1-2.png?resize=400,381 400w, https://venturebeat.com/wp-content/uploads/2025/01/Image-1-2.png?resize=578,550 578w\" sizes=\"(max-width: 586px) 100vw, 586px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e“Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention,” researchers from ByteDance and Tsinghua University write in a \u003ca href=\"https://arxiv.org/pdf/2501.12326\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew research paper\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1011\" height=\"357\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/Image-1.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/Image-1.png 1011w, https://venturebeat.com/wp-content/uploads/2025/01/Image-1.png?resize=300,106 300w, https://venturebeat.com/wp-content/uploads/2025/01/Image-1.png?resize=768,271 768w, https://venturebeat.com/wp-content/uploads/2025/01/Image-1.png?resize=800,282 800w, https://venturebeat.com/wp-content/uploads/2025/01/Image-1.png?resize=400,141 400w, https://venturebeat.com/wp-content/uploads/2025/01/Image-1.png?resize=750,265 750w, https://venturebeat.com/wp-content/uploads/2025/01/Image-1.png?resize=578,204 578w, https://venturebeat.com/wp-content/uploads/2025/01/Image-1.png?resize=930,328 930w\" sizes=\"(max-width: 1011px) 100vw, 1011px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eSource: Arxiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-how-ui-tars-explains-its-thinking\"\u003eHow UI-TARS explains its thinking\u003c/h2\u003e\n\n\n\n\u003cp\u003eUI-TARS works across desktop, mobile and web applications, using multimodal inputs (text, images, interactions) to understand visual environments. \u003c/p\u003e\n\n\n\n\u003cp\u003eIts UI features two tabs — one to the left showing its step-by-step “thinking,” and a larger one to the right where it pulls up files, websites and apps and automatically takes action. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, in a demo video released today, the model is prompted to “Find round trip flights from SEA to NYC on the 5th and return on the 10th next month and filter by price in ascending order.” \u003c/p\u003e\n\n\n\n\u003cp\u003eIn response, UI-TARS navigates to the website for Delta Airlines, fills in the “from” and “to” fields, clicks in the relevant dates and sorts and filters by price, explaining each step in its thinking box before taking action. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn another scenario, it is instructed to install an autoDocstring extension in VS Code. Here is its step-by-step thinking as it completes that task: \u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eIt first reports that “to proceed with installing the autoDocstring extension in VS Code, I first need to open the VS Code application.” \u003c/li\u003e\n\n\n\n\u003cli\u003eRecognizing that the VS Code window has not fully loaded, the model notes that “it is necessary to wait for the application to finish initializing before proceeding with the installation of the autoDocstring extension. This ensures that all required functionalities are available for the next steps.” \u003c/li\u003e\n\n\n\n\u003cli\u003eIt then explains that it must access the Extensions view in VS Code, which can be done by clicking on a tab in the left sidebar. \u003c/li\u003e\n\n\n\n\u003cli\u003eRunning into a small glitch, it reasons: “The previous attempt to click on the ‘Extensions’ tab may not have been precise enough, so I will try clicking on it again to ensure the Extensions view opens successfully.”\u003c/li\u003e\n\n\n\n\u003cli\u003eOnce in the Extensions view, it types in ‘autoDocstring’ before concluding: “Since the installation process has already been initiated, there is no further action required at this moment. I will wait for the installation to complete and confirm its success once the process is finished.”\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"488\" height=\"480\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-3-1.png\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-3-1.png 488w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-3-1.png?resize=300,295 300w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-3-1.png?resize=52,52 52w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-3-1.png?resize=400,393 400w\" sizes=\"(max-width: 488px) 100vw, 488px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-outperforming-its-rivals\"\u003eOutperforming its rivals\u003c/h2\u003e\n\n\n\n\u003cp\u003eAcross a variety of benchmarks, researchers report that UI-TARS consistently outranked OpenAI’s GPT-4o; Anthropic’s Claude-3.5-Sonnet; Gemini-1.5-Pro and Gemini-2.0; four \u003ca href=\"https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/\"\u003eQwen models\u003c/a\u003e; and numerous academic models.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor instance, in VisualWebBench — which measures a model’s ability to ground web elements including webpage quality assurance and optical character recognition — UI-TARS 72B scored 82.8%, outperforming GPT-4o (78.5%) and Claude 3.5 (78.2%). \u003c/p\u003e\n\n\n\n\u003cp\u003eIt also did significantly better on WebSRC benchmarks (understanding of semantic content and layout in web contexts) and ScreenQA-short (comprehension of complex mobile screen layouts and web structure). UI-TARS-7B achieved leading scores of 93.6% on WebSRC, while UI-TARS-72B achieved 88.6% on ScreenQA-short, outperforming Qwen, Gemini, Claude 3.5 and GPT-4o. \u003c/p\u003e\n\n\n\n\u003cp\u003e“These results demonstrate the superior perception and comprehension capabilities of UI-TARS in web and mobile environments,” the researchers write. “Such perceptual ability lays the foundation for agent tasks, where accurate environmental understanding is crucial for task execution and decision-making.”\u003c/p\u003e\n\n\n\n\u003cp\u003eUI-TARS also showed impressive results in ScreenSpot Pro and ScreenSpot v2 , which assess a model’s ability to understand and localize elements in GUIs. Further, researchers tested its capabilities in planning multi-step actions and low-level tasks in mobile environments, and benchmarked it on OSWorld (which assesses open-ended computer tasks) and AndroidWorld (which scores autonomous agents on 116 programmatic tasks across 20 mobile apps). \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"967\" height=\"526\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-4.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-4.png 967w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-4.png?resize=300,163 300w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-4.png?resize=768,418 768w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-4.png?resize=800,435 800w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-4.png?resize=400,218 400w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-4.png?resize=750,408 750w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-4.png?resize=578,314 578w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-4.png?resize=930,506 930w\" sizes=\"auto, (max-width: 967px) 100vw, 967px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eSource: Arxiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"915\" height=\"387\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/Image-5.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/Image-5.png 915w, https://venturebeat.com/wp-content/uploads/2025/01/Image-5.png?resize=300,127 300w, https://venturebeat.com/wp-content/uploads/2025/01/Image-5.png?resize=768,325 768w, https://venturebeat.com/wp-content/uploads/2025/01/Image-5.png?resize=800,338 800w, https://venturebeat.com/wp-content/uploads/2025/01/Image-5.png?resize=400,169 400w, https://venturebeat.com/wp-content/uploads/2025/01/Image-5.png?resize=750,317 750w, https://venturebeat.com/wp-content/uploads/2025/01/Image-5.png?resize=578,244 578w\" sizes=\"auto, (max-width: 915px) 100vw, 915px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eSource: Arxiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-under-the-hood\"\u003eUnder the hood\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo help it take step-by-step actions and recognize what it’s seeing, UI-TARS was trained on a large-scale dataset of screenshots that parsed metadata including element description and type, visual description, bounding boxes (position information), element function and text from various websites, applications and operating systems. This allows the model to provide a comprehensive, detailed description of a screenshot, capturing not only elements but spatial relationships and overall layout. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe model also uses state transition captioning to identify and describe the differences between two consecutive screenshots and determine whether an action — such as a mouse click or keyboard input — has occurred. Meanwhile, set-of-mark (SoM) prompting allows it to overlay distinct marks (letters, numbers) on specific regions of an image. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe model is equipped with both short-term and long-term memory to handle tasks at hand while also retaining historical interactions to improve later decision-making. Researchers trained the model to perform both System 1 (fast, automatic and intuitive) and System 2 (slow and deliberate) reasoning. This allows for multi-step decision-making, “reflection” thinking, milestone recognition and error correction. \u003c/p\u003e\n\n\n\n\u003cp\u003eResearchers emphasized that it is critical that the model be able to maintain consistent goals and engage in trial and error to hypothesize, test and evaluate potential actions before completing a task. They introduced two types of data to support this: error correction and post-reflection data. For error correction, they identified mistakes and labeled corrective actions; for post-reflection, they simulated recovery steps. \u003c/p\u003e\n\n\n\n\u003cp\u003e“This strategy ensures that the agent not only learns to avoid errors but also adapts dynamically when they occur,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cp\u003eClearly, UI-TARS exhibits impressive capabilities, and it’ll be interesting to see its evolving use cases in the increasingly competitive AI agents space. As the researchers note: “Looking ahead, while native agents represent a significant leap forward, the future lies in the integration of active and lifelong learning, where agents autonomously drive their own learning through continuous, real-world interactions.”\u003c/p\u003e\n\n\n\n\u003cp\u003eResearchers point out that Claude Computer Use “performs strongly in web-based tasks but significantly struggles with mobile scenarios, indicating that the GUI operation ability of Claude has not been well transferred to the mobile domain.” \u003c/p\u003e\n\n\n\n\u003cp\u003eBy contrast, “UI-TARS exhibits excellent performance in both website and mobile domain.” \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2025-01-22T22:58:46Z",
  "modifiedTime": "2025-01-23T00:22:14Z"
}
