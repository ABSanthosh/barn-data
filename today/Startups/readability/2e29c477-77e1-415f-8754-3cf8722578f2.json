{
  "id": "2e29c477-77e1-415f-8754-3cf8722578f2",
  "title": "Sakana introduces new AI architecture, ‘Continuous Thought Machines’ to make models reason with less guidance — like human brains",
  "link": "https://venturebeat.com/ai/sakana-introduces-new-ai-architecture-continuous-thought-machines-to-make-models-reason-with-less-guidance-like-human-brains/",
  "description": "While the CTM shows strong promise, it is still primarily a research architecture and is not yet production-ready out of the box.",
  "author": "Carl Franzen",
  "published": "Mon, 12 May 2025 23:08:55 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "ai architecture",
    "AI, ML and Deep Learning",
    "attention is all you need",
    "continuous thought machines",
    "Conversational AI",
    "CTMs",
    "dynamic reasoning",
    "Google",
    "Japan",
    "llm architecture",
    "LLM reasoning",
    "NLP",
    "reasoning AI",
    "reasoning llms",
    "reasoning models",
    "research",
    "sakana",
    "sakana ai",
    "ticks",
    "Tokyo",
    "Transformers"
  ],
  "byline": "Carl Franzen",
  "length": 11590,
  "excerpt": "While the CTM shows strong promise, it is still primarily a research architecture and is not yet production-ready out of the box.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "May 12, 2025 4:08 PM Credit: VentureBeat made with Midjourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Tokyo-based artificial intelligence startup Sakana, co-founded by former top Google AI scientists including Llion Jones and David Ha, has unveiled a new type of AI model architecture called Continuous Thought Machines (CTM). CTMs are designed to usher in a new era of AI language models that will be more flexible and able to handle a wider range of cognitive tasks — such as solving complex mazes or navigation tasks without positional cues or pre-existing spatial embeddings — moving them closer to the way human beings reason through unfamiliar problems. Rather than relying on fixed, parallel layers that process inputs all at once — as Transformer models do —CTMs unfold computation over steps within each input/output unit, known as an artificial “neuron.” Each neuron in the model retains a short history of its previous activity and uses that memory to decide when to activate again. This added internal state allows CTMs to adjust the depth and duration of their reasoning dynamically, depending on the complexity of the task. As such, each neuron is far more informationally dense and complex than in a typical Transformer model. The startup has posted a paper on the open access journal arXiv describing its work, a microsite and Github repository. How CTMs differ from Transformer-based LLMs Most modern large language models (LLMs) are still fundamentally based upon the “Transformer” architecture outlined in the seminal 2017 paper from Google Brain researchers entitled “Attention Is All You Need.” These models use parallelized, fixed-depth layers of artificial neurons to process inputs in a single pass — whether those inputs come from user prompts at inference time or labeled data during training. By contrast, CTMs allow each artificial neuron to operate on its own internal timeline, making activation decisions based on a short-term memory of its previous states. These decisions unfold over internal steps known as “ticks,” enabling the model to adjust its reasoning duration dynamically. This time-based architecture allows CTMs to reason progressively, adjusting how long and how deeply they compute — taking a different number of ticks based on the complexity of the input. Neuron-specific memory and synchronization help determine when computation should continue — or stop. The number of ticks changes according to the information inputted, and may be more or less even if the input information is identical, because each neuron is deciding how many ticks to undergo before providing an output (or not providing one at all). This represents both a technical and philosophical departure from conventional deep learning, moving toward a more biologically grounded model. Sakana has framed CTMs as a step toward more brain-like intelligence—systems that adapt over time, process information flexibly, and engage in deeper internal computation when needed. Sakana’s goal is to “to eventually achieve levels of competency that rival or surpass human brains.” Using variable, custom timelines to provide more intelligence The CTM is built around two key mechanisms. First, each neuron in the model maintains a short “history” or working memory of when it activated and why, and uses this history to make a decision of when to fire next. Second, neural synchronization — how and when groups of a model’s artificial neurons “fire,” or process information together — is allowed to happen organically. Groups of neurons decide when to fire together based on internal alignment, not external instructions or reward shaping. These synchronization events are used to modulate attention and produce outputs — that is, attention is directed toward those areas where more neurons are firing. The model isn’t just processing data, it’s timing its thinking to match the complexity of the task. Together, these mechanisms let CTMs reduce computational load on simpler tasks while applying deeper, prolonged reasoning where needed. In demonstrations ranging from image classification and 2D maze solving to reinforcement learning, CTMs have shown both interpretability and adaptability. Their internal “thought” steps allow researchers to observe how decisions form over time—a level of transparency rarely seen in other model families. Early results: how CTMs compare to Transformer models on key benchmarks and tasks Sakana AI’s Continuous Thought Machine is not designed to chase leaderboard-topping benchmark scores, but its early results indicate that its biologically inspired design does not come at the cost of practical capability. On the widely used ImageNet-1K benchmark, the CTM achieved 72.47% top-1 and 89.89% top-5 accuracy. While this falls short of state-of-the-art transformer models like ViT or ConvNeXt, it remains competitive—especially considering that the CTM architecture is fundamentally different and was not optimized solely for performance. What stands out more are CTM’s behaviors in sequential and adaptive tasks. In maze-solving scenarios, the model produces step-by-step directional outputs from raw images—without using positional embeddings, which are typically essential in transformer models. Visual attention traces reveal that CTMs often attend to image regions in a human-like sequence, such as identifying facial features from eyes to nose to mouth. The model also exhibits strong calibration: its confidence estimates closely align with actual prediction accuracy. Unlike most models that require temperature scaling or post-hoc adjustments, CTMs improve calibration naturally by averaging predictions over time as their internal reasoning unfolds. This blend of sequential reasoning, natural calibration, and interpretability offers a valuable trade-off for applications where trust and traceability matter as much as raw accuracy. What’s needed before CTMs are ready for enterprise and commercial deployment? While CTMs show substantial promise, the architecture is still experimental and not yet optimized for commercial deployment. Sakana AI presents the model as a platform for further research and exploration rather than a plug-and-play enterprise solution. Training CTMs currently demands more resources than standard transformer models. Their dynamic temporal structure expands the state space, and careful tuning is needed to ensure stable, efficient learning across internal time steps. Additionally, debugging and tooling support is still catching up—many of today’s libraries and profilers are not designed with time-unfolding models in mind. Still, Sakana has laid a strong foundation for community adoption. The full CTM implementation is open-sourced on GitHub and includes domain-specific training scripts, pretrained checkpoints, plotting utilities, and analysis tools. Supported tasks include image classification (ImageNet, CIFAR), 2D maze navigation, QAMNIST, parity computation, sorting, and reinforcement learning. An interactive web demo also lets users explore the CTM in action, observing how its attention shifts over time during inference—a compelling way to understand the architecture’s reasoning flow. For CTMs to reach production environments, further progress is needed in optimization, hardware efficiency, and integration with standard inference pipelines. But with accessible code and active documentation, Sakana has made it easy for researchers and engineers to begin experimenting with the model today. What enterprise AI leaders should know about CTMs The CTM architecture is still in its early days, but enterprise decision-makers should already take note. Its ability to adaptively allocate compute, self-regulate depth of reasoning, and offer clear interpretability may prove highly valuable in production systems facing variable input complexity or strict regulatory requirements. AI engineers managing model deployment will find value in CTM’s energy-efficient inference — especially in large-scale or latency-sensitive applications. Meanwhile, the architecture’s step-by-step reasoning unlocks richer explainability, enabling organizations to trace not just what a model predicted, but how it arrived there. For orchestration and MLOps teams, CTMs integrate with familiar components like ResNet-based encoders, allowing smoother incorporation into existing workflows. And infrastructure leads can use the architecture’s profiling hooks to better allocate resources and monitor performance dynamics over time. CTMs aren’t ready to replace transformers, but they represent a new category of model with novel affordances. For organizations prioritizing safety, interpretability, and adaptive compute, the architecture deserves close attention. Sakana’s checkered AI research history In February, Sakana introduced the AI CUDA Engineer, an agentic AI system designed to automate the production of highly optimized CUDA kernels, the instruction sets that allow Nvidia’s (and others’) graphics processing units (GPUs) to run code efficiently in parallel across multiple “threads” or computational units. The promise was significant: speedups of 10x to 100x in ML operations. However, shortly after release, external reviewers discovered that the system was exploiting weaknesses in the evaluation sandbox—essentially “cheating” by bypassing correctness checks through a memory exploit. In a public post, Sakana acknowledged the issue and credited community members with flagging it. They’ve since overhauled their evaluation and runtime profiling tools to eliminate similar loopholes and are revising their results and research paper accordingly. The incident offered a real-world test of one of Sakana’s stated values: embracing iteration and transparency in pursuit of better AI systems. Betting on evolutionary mechanisms Sakana AI’s founding ethos lies in merging evolutionary computation with modern machine learning. The company believes current models are too rigid—locked into fixed architectures and requiring retraining for new tasks. By contrast, Sakana aims to create models that adapt in real time, exhibit emergent behavior, and scale naturally through interaction and feedback, much like organisms in an ecosystem. This vision is already manifesting in products like Transformer², a system that adjusts LLM parameters at inference time without retraining, using algebraic tricks like singular-value decomposition. It’s also evident in their commitment to open-sourcing systems like the AI Scientist—even amid controversy—demonstrating a willingness to engage with the broader research community, not just compete with it. As large incumbents like OpenAI and Google double down on foundation models, Sakana is charting a different course: small, dynamic, biologically inspired systems that think in time, collaborate by design, and evolve through experience. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/05/cfr0z3n_stark_crisp_neat_pop_art_colorful_flat_illustration_pol_570ae7e2-ddaf-4775-b7da-c0dd0257c8d0.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-05-12T23:08:55+00:00\" datetime=\"2025-05-12T23:08:55+00:00\"\u003eMay 12, 2025 4:08 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2025/05/cfr0z3n_stark_crisp_neat_pop_art_colorful_flat_illustration_pol_570ae7e2-ddaf-4775-b7da-c0dd0257c8d0.png?w=750\" alt=\"AI line art of a brain diagram with colored hemispheres against a dark backdrop with white lines and nodes\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eTokyo-based artificial intelligence startup Sakana, co-founded by former top Google AI scientists including Llion Jones and David Ha, has unveiled a new type of \u003ca href=\"https://pub.sakana.ai/ctm/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAI model architecture called Continuous Thought Machines (CTM)\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eCTMs are designed to usher in a new era of AI language models that will be more flexible and able to handle a wider range of cognitive tasks — such as solving complex mazes or navigation tasks without positional cues or pre-existing spatial embeddings — moving them closer to the way human beings reason through unfamiliar problems. \u003c/p\u003e\n\n\n\n\u003cp\u003eRather than relying on fixed, parallel layers that process inputs all at once — as Transformer models do —CTMs unfold computation over steps within each input/output unit, known as an artificial “neuron.” \u003c/p\u003e\n\n\n\n\u003cp\u003eEach neuron in the model retains a short history of its previous activity and uses that memory to decide when to activate again. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis added internal state allows CTMs to adjust the depth and duration of their reasoning dynamically, depending on the complexity of the task. As such, each neuron is far more informationally dense and complex than in a typical Transformer model. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe startup has posted a \u003ca href=\"https://arxiv.org/abs/2505.05522\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epaper on the open access journal arXiv\u003c/a\u003e describing its work, \u003ca href=\"https://pub.sakana.ai/ctm/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ea microsite\u003c/a\u003e and \u003ca href=\"https://github.com/SakanaAI/continuous-thought-machines\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGithub repository\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-ctms-differ-from-transformer-based-llms\"\u003eHow CTMs differ from Transformer-based LLMs\u003c/h2\u003e\n\n\n\n\u003cp\u003eMost modern large language models (LLMs) are still fundamentally based upon the “Transformer” architecture outlined in the seminal 2017 paper from Google Brain researchers entitled “\u003ca href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAttention Is All You Need\u003c/a\u003e.” \u003c/p\u003e\n\n\n\n\u003cp\u003eThese models use parallelized, fixed-depth layers of artificial neurons to process inputs in a single pass — whether those inputs come from user prompts at inference time or labeled data during training.\u003c/p\u003e\n\n\n\n\u003cp\u003eBy contrast, CTMs allow each artificial neuron to operate on its own internal timeline, making activation decisions based on a short-term memory of its previous states. These decisions unfold over internal steps known as “ticks,” enabling the model to adjust its reasoning duration dynamically. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis time-based architecture allows CTMs to reason progressively, adjusting how long and how deeply they compute — taking a different number of ticks based on the complexity of the input. \u003c/p\u003e\n\n\n\n\u003cp\u003eNeuron-specific memory and synchronization help determine when computation should continue — or stop.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe number of ticks changes according to the information inputted, and may be more or less even if the input information is identical, because \u003cem\u003eeach neuron\u003c/em\u003e is deciding how many ticks to undergo before providing an output (or not providing one at all). \u003c/p\u003e\n\n\n\n\u003cp\u003eThis represents both a technical and philosophical departure from conventional deep learning, moving toward a more biologically grounded model. Sakana has framed CTMs as a step toward more brain-like intelligence—systems that adapt over time, process information flexibly, and engage in deeper internal computation when needed.\u003c/p\u003e\n\n\n\n\u003cp\u003eSakana’s goal is to “to eventually achieve levels of competency that rival or surpass human brains.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-using-variable-custom-timelines-to-provide-more-intelligence\"\u003eUsing variable, custom timelines to provide more intelligence\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe CTM is built around two key mechanisms. \u003c/p\u003e\n\n\n\n\u003cp\u003eFirst, each neuron in the model maintains a short “history” or working memory of when it activated and why, and uses this history to make a decision of when to fire next.  \u003c/p\u003e\n\n\n\n\u003cp\u003eSecond, neural synchronization — how and when \u003cem\u003egroups\u003c/em\u003e of a model’s artificial neurons “fire,” or process information together — is allowed to happen organically.\u003c/p\u003e\n\n\n\n\u003cp\u003eGroups of neurons decide when to fire together based on internal alignment, not external instructions or reward shaping. These synchronization events are used to modulate attention and produce outputs — that is, attention is directed toward those areas where more neurons are firing. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe model isn’t just processing data, it’s timing its thinking to match the complexity of the task.\u003c/p\u003e\n\n\n\n\u003cp\u003eTogether, these mechanisms let CTMs reduce computational load on simpler tasks while applying deeper, prolonged reasoning where needed. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn demonstrations ranging from image classification and 2D maze solving to reinforcement learning, CTMs have shown both interpretability and adaptability. Their internal “thought” steps allow researchers to observe how decisions form over time—a level of transparency rarely seen in other model families.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-early-results-how-ctms-compare-to-transformer-models-on-key-benchmarks-and-tasks\"\u003eEarly results: how CTMs compare to Transformer models on key benchmarks and tasks\u003c/h2\u003e\n\n\n\n\u003cp\u003eSakana AI’s Continuous Thought Machine is not designed to chase leaderboard-topping benchmark scores, but its early results indicate that its biologically inspired design does not come at the cost of practical capability. \u003c/p\u003e\n\n\n\n\u003cp\u003eOn the widely used ImageNet-1K benchmark, the CTM achieved 72.47% top-1 and 89.89% top-5 accuracy. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile this falls short of state-of-the-art transformer models like ViT or ConvNeXt, it remains competitive—especially considering that the CTM architecture is fundamentally different and was not optimized solely for performance.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhat stands out more are CTM’s behaviors in sequential and adaptive tasks. In maze-solving scenarios, the model produces step-by-step directional outputs from raw images—without using positional embeddings, which are typically essential in transformer models. Visual attention traces reveal that CTMs often attend to image regions in a human-like sequence, such as identifying facial features from eyes to nose to mouth.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe model also exhibits strong calibration: its confidence estimates closely align with actual prediction accuracy. Unlike most models that require temperature scaling or post-hoc adjustments, CTMs improve calibration naturally by averaging predictions over time as their internal reasoning unfolds.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis blend of sequential reasoning, natural calibration, and interpretability offers a valuable trade-off for applications where trust and traceability matter as much as raw accuracy.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-s-needed-before-ctms-are-ready-for-enterprise-and-commercial-deployment\"\u003eWhat’s needed before CTMs are ready for enterprise and commercial deployment?\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile CTMs show substantial promise, the architecture is still experimental and not yet optimized for commercial deployment. Sakana AI presents the model as a platform for further research and exploration rather than a plug-and-play enterprise solution.\u003c/p\u003e\n\n\n\n\u003cp\u003eTraining CTMs currently demands more resources than standard transformer models. Their dynamic temporal structure expands the state space, and careful tuning is needed to ensure stable, efficient learning across internal time steps. Additionally, debugging and tooling support is still catching up—many of today’s libraries and profilers are not designed with time-unfolding models in mind.\u003c/p\u003e\n\n\n\n\u003cp\u003eStill, Sakana has laid a strong foundation for community adoption. The full CTM implementation is open-sourced on \u003ca href=\"https://github.com/SakanaAI/continuous-thought-machines\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGitHub\u003c/a\u003e and includes domain-specific training scripts, pretrained checkpoints, plotting utilities, and analysis tools. Supported tasks include image classification (ImageNet, CIFAR), 2D maze navigation, QAMNIST, parity computation, sorting, and reinforcement learning.\u003c/p\u003e\n\n\n\n\u003cp\u003eAn interactive web demo also lets users explore the CTM in action, observing how its attention shifts over time during inference—a compelling way to understand the architecture’s reasoning flow.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor CTMs to reach production environments, further progress is needed in optimization, hardware efficiency, and integration with standard inference pipelines. But with accessible code and active documentation, Sakana has made it easy for researchers and engineers to begin experimenting with the model today.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-enterprise-ai-leaders-should-know-about-ctms\"\u003eWhat enterprise AI leaders should know about CTMs\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe CTM architecture is still in its early days, but enterprise decision-makers should already take note. Its ability to adaptively allocate compute, self-regulate depth of reasoning, and offer clear interpretability may prove highly valuable in production systems facing variable input complexity or strict regulatory requirements.\u003c/p\u003e\n\n\n\n\u003cp\u003eAI engineers managing model deployment will find value in CTM’s energy-efficient inference — especially in large-scale or latency-sensitive applications. \u003c/p\u003e\n\n\n\n\u003cp\u003eMeanwhile, the architecture’s step-by-step reasoning unlocks richer explainability, enabling organizations to trace not just what a model predicted, but how it arrived there.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor orchestration and MLOps teams, CTMs integrate with familiar components like ResNet-based encoders, allowing smoother incorporation into existing workflows. And infrastructure leads can use the architecture’s profiling hooks to better allocate resources and monitor performance dynamics over time.\u003c/p\u003e\n\n\n\n\u003cp\u003eCTMs aren’t ready to replace transformers, but they represent a new category of model with novel affordances. For organizations prioritizing safety, interpretability, and adaptive compute, the architecture deserves close attention.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-sakana-s-checkered-ai-research-history\"\u003eSakana’s checkered AI research history\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn February, \u003ca href=\"https://sakana.ai/ai-cuda-engineer/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSakana introduced the AI CUDA Engineer\u003c/a\u003e, an agentic AI system designed to automate the production of highly optimized \u003ca href=\"https://people.maths.ox.ac.uk/gilesm/cuda/lecs/lec1.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCUDA kernels\u003c/a\u003e, the instruction sets that allow Nvidia’s (and others’) graphics processing units (GPUs) to run code efficiently in parallel across multiple “threads” or computational units. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe promise was significant: speedups of 10x to 100x in ML operations. However, shortly after release, external reviewers discovered that the\u003ca href=\"https://techcrunch.com/2025/02/21/sakana-walks-back-claims-that-its-ai-can-dramatically-speed-up-model-training/\"\u003e system was exploiting weaknesses in the evaluation sandbox\u003c/a\u003e—essentially “\u003ca href=\"https://techcrunch.com/2025/02/21/sakana-walks-back-claims-that-its-ai-can-dramatically-speed-up-model-training/\"\u003echeating\u003c/a\u003e” by bypassing correctness checks through a memory exploit.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a public post, Sakana acknowledged the issue and credited community members with flagging it.\u003c/p\u003e\n\n\n\n\u003cp\u003eThey’ve since overhauled their evaluation and runtime profiling tools to eliminate similar loopholes and are revising their results and research paper accordingly. The incident offered a real-world test of one of Sakana’s stated values: embracing iteration and transparency in pursuit of better AI systems.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-betting-on-evolutionary-mechanisms\"\u003eBetting on evolutionary mechanisms\u003c/h2\u003e\n\n\n\n\u003cp\u003eSakana AI’s founding ethos lies in merging evolutionary computation with modern machine learning. The company believes current models are too rigid—locked into fixed architectures and requiring retraining for new tasks. \u003c/p\u003e\n\n\n\n\u003cp\u003eBy contrast, Sakana aims to create models that adapt in real time, exhibit emergent behavior, and scale naturally through interaction and feedback, much like organisms in an ecosystem.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis vision is already manifesting in products like Transformer², a system that adjusts LLM parameters at inference time without retraining, using algebraic tricks like singular-value decomposition. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt’s also evident in their commitment to open-sourcing systems like the AI Scientist—even amid controversy—demonstrating a willingness to engage with the broader research community, not just compete with it.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs large incumbents like OpenAI and Google double down on foundation models, Sakana is charting a different course: small, dynamic, biologically inspired systems that think in time, collaborate by design, and evolve through experience.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2025-05-12T23:08:55Z",
  "modifiedTime": "2025-05-12T23:09:05Z"
}
