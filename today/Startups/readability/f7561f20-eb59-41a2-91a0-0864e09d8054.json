{
  "id": "f7561f20-eb59-41a2-91a0-0864e09d8054",
  "title": "How OpenAI’s red team made ChatGPT agent into an AI fortress",
  "link": "https://venturebeat.com/security/openais-red-team-plan-make-chatgpt-agent-an-ai-fortress/",
  "description": "Discover OpenAI's red team blueprint: How 110 coordinated attacks and 7 exploit fixes created ChatGPT Agent's revolutionary 95% security defense system.",
  "author": "Louis Columbus",
  "published": "Fri, 18 Jul 2025 22:13:52 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "Security",
    "AI agent security",
    "AI arms race",
    "AI attack vectors",
    "AI biological threats",
    "AI espionage",
    "AI Red Teaming",
    "AI risk management",
    "AI safety classifiers",
    "AI security baseline",
    "AI security monitoring",
    "AI vulnerability patching",
    "biological information extraction",
    "biosecurity in AI",
    "CBRN safeguards",
    "ChatGPT Agent",
    "ChatGPT Agent security",
    "cloud document vulnerabilities",
    "corporate espionage AI",
    "data exfiltration attacks",
    "enterprise AI security",
    "FAR.AI assessment",
    "Google Drive vulnerabilities",
    "Keren Gu",
    "machine learning security",
    "mission impossible AI",
    "multi-step chain attacks",
    "OpenAI ChatGPT security",
    "OpenAI Red Teaming Network",
    "prompt injection defense",
    "rapid remediation protocol",
    "real-time AI monitoring",
    "red team AI testing",
    "security classifiers",
    "security countermeasures",
    "security exploits",
    "security thresholds",
    "synthetic identity threats",
    "terminal command restrictions",
    "UK AISI AI testing",
    "visual browser exploits"
  ],
  "byline": "Louis Columbus",
  "length": 10907,
  "excerpt": "Discover OpenAI's red team blueprint: How 110 coordinated attacks and 7 exploit fixes created ChatGPT Agent's revolutionary 95% security defense system.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now In case you missed it, OpenAI yesterday debuted a powerful new feature for ChatGPT and with it, a host of new security risks and ramifications. Called the “ChatGPT agent,” this new feature is an optional mode that ChatGPT paying subscribers can engage by clicking “Tools” in the prompt entry box and selecting “agent mode,” at which point, they can ask ChatGPT to log into their email and other web accounts; write and respond to emails; download, modify, and create files; and do a host of other tasks on their behalf, autonomously, much like a real person using a computer with their login credentials. Obviously, this also requires the user to trust the ChatGPT agent not to do anything problematic or nefarious, or to leak their data and sensitive information. It also poses greater risks for a user and their employer than the regular ChatGPT, which can’t log into web accounts or modify files directly. Keren Gu, a member of the Safety Research team at OpenAI, commented on X that “we’ve activated our strongest safeguards for ChatGPT Agent. It’s the first model we’ve classified as High capability in biology \u0026 chemistry under our Preparedness Framework. Here’s why that matters–and what we’re doing to keep it safe.” The AI Impact Series Returns to San Francisco - August 5 The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation. Secure your spot now - space is limited: https://bit.ly/3GuuPLF So how did OpenAI handle all these security issues? The red team’s mission Looking at OpenAI’s ChatGPT agent system card, the “read team” employed by the company to test the feature faced a challenging mission: specifically, 16 PhD security researchers who were given 40 hours to test it out. Through systematic testing, the red team discovered seven universal exploits that could compromise the system, revealing critical vulnerabilities in how AI agents handle real-world interactions. What followed next was extensive security testing, much of it predicated on red teaming. The Red Teaming Network submitted 110 attacks, from prompt injections to biological information extraction attempts. Sixteen exceeded internal risk thresholds. Each finding gave OpenAI engineers the insights they needed to get fixes written and deployed before launch. The results speak for themselves in the published results in the system card. ChatGPT Agent emerged with significant security improvements, including 95% performance against visual browser irrelevant instruction attacks and robust biological and chemical safeguards. Red teams exposed seven universal exploits OpenAI’s Red Teaming Network was comprised 16 researchers with biosafety-relevant PhDs who topgether submitted 110 attack attempts during the testing period. Sixteen exceeded internal risk thresholds, revealing fundamental vulnerabilities in how AI agents handle real-world interactions. But the real breakthrough came from UK AISI’s unprecedented access to ChatGPT Agent’s internal reasoning chains and policy text. Admittedly that’s intelligence regular attackers would never possess. Over four testing rounds, UK AISI forced OpenAI to execute seven universal exploits that had the potential to compromise any conversation: Attack vectors that forced OpenAI’s hand Attack TypeSuccess Rate (Pre-Fix)TargetImpactVisual Browser Hidden Instructions33%Web pagesActive data exfiltrationGoogle Drive Connector ExploitationNot disclosedCloud documentsForced document leaksMulti-Step Chain AttacksVariableCross-site actionsComplete session compromiseBiological Information Extraction16 submissions exceeded thresholdsDangerous knowledgePotential weaponization FAR.AI’s assessment was openly critical of OpenAI’s approach. Despite 40 hours of testing revealing only three partial vulnerabilities, they identified that current safety mechanisms relied heavily on monitoring during reasoning and tool-use processes, which the researchers considered a potential single point of failure if compromised. How red teaming helped turn ChatGPT vulnerabilities into a fortress OpenAI’s response to the red team’s results redefined entire segments of the ChatGPT agent architecture. One of the many initiatives undertaken including building a dual-layer inspection architecture that monitors 100% of production traffic in real-time, achieving these measurable improvements: Security improvements after red team discoveries Defense MetricPrevious ModelsChatGPT AgentImprovementIrrelevant Instructions (Visual Browser)82%95%+13%In-Context Data Exfiltration75%78%+3%Active Data Exfiltration58%67%+9%System ReliabilitySampling-based100% coverageComplete monitoring The architecture works like this: First Tier: A fast classifier with 96% recall flags suspicious content Second Tier: A reasoning model with 84% recall analyzes flagged interactions for actual threats But the technical defenses tell only part of the story. OpenAI made difficult security choices that acknowledge some AI operations require significant restrictions for safe autonomous execution. Based on the vulnerabilities discovered, OpenAI implemented the following countermeasures across their model: Watch Mode Activation: When ChatGPT Agent accesses sensitive contexts like banking or email accounts, the system freezes all activity if users navigate away. This is in direct response to data exfiltration attempts discovered during testing. Memory Features Disabled: Despite being a core functionality, memory is completely disabled at launch to prevent the incremental data leaking attacks red teamers demonstrated. Terminal Restrictions: Network access limited to GET requests only, blocking the command execution vulnerabilities researchers exploited. Rapid Remediation Protocol: A new system that patches vulnerabilities within hours of discovery—developed after red teamers showed how quickly exploits could spread. During pre-launch testing alone, this system identified and resolved 16 critical vulnerabilities that red teamers had discovered. A biological risk wake-up call Red teamers revealed the potential that the ChatGPT Agent could be comprimnised and lead to greater biological risks. Sixteen experienced participants from the Red Teaming Network, each with biosafety-relevant PhDs, attempted to extract dangerous biological information. Their submissions revealed the model could synthesize published literature on modifying and creating biological threats. In response to the red teamers’ findings, OpenAI classified ChatGPT Agent as “High capability” for biological and chemical risks, not because they found definitive evidence of weaponization potential, but as a precautionary measure based on red team findings. This triggered: Always-on safety classifiers scanning 100% of traffic A topical classifier achieving 96% recall for biology-related content A reasoning monitor with 84% recall for weaponization content A bio bug bounty program for ongoing vulnerability discovery What red teams taught OpenAI about AI security The 110 attack submissions revealed patterns that forced fundamental changes in OpenAI’s security philosophy. They include the following: Persistence over power: Attackers don’t need sophisticated exploits, all they need is more time. Red teamers showed how patient, incremental attacks could eventually compromise systems. Trust boundaries are fiction: When your AI agent can access Google Drive, browse the web, and execute code, traditional security perimeters dissolve. Red teamers exploited the gaps between these capabilities. Monitoring isn’t optional: The discovery that sampling-based monitoring missed critical attacks led to the 100% coverage requirement. Speed matters: Traditional patch cycles measured in weeks are worthless against prompt injection attacks that can spread instantly. The rapid remediation protocol patches vulnerabilities within hours. OpenAI is helping to create a new security baseline for Enterprise AI For CISOs evaluating AI deployment, the red team discoveries establish clear requirements: Quantifiable protection: ChatGPT Agent’s 95% defense rate against documented attack vectors sets the industry benchmark. The nuances of the many tests and results defined in the system card explain the context of how they accomplished this and is a must-read for anyone involved with model security. Complete visibility: 100% traffic monitoring isn’t aspirational anymore. OpenAI’s experiences illustrate why it’s mandatory given how easily red teams can hide attacks anywhere. Rapid response: Hours, not weeks, to patch discovered vulnerabilities. Enforced boundaries: Some operations (like memory access during sensitive tasks) must be disabled until proven safe. UK AISI’s testing proved particularly instructive. All seven universal attacks they identified were patched before launch, but their privileged access to internal systems revealed vulnerabilities that would eventually be discoverable by determined adversaries. “This is a pivotal moment for our Preparedness work,” Gu wrote on X. “Before we reached High capability, Preparedness was about analyzing capabilities and planning safeguards. Now, for Agent and future more capable models, Preparedness safeguards have become an operational requirement.” Red teams are core to building safer, more secure AI models The seven universal exploits discovered by researchers and the 110 attacks from OpenAI’s red team network became the crucible that forged ChatGPT Agent. By revealing exactly how AI agents could be weaponized, red teams forced the creation of the first AI system where security isn’t just a feature. It’s the foundation. ChatGPT Agent’s results prove red teaming’s effectiveness: blocking 95% of visual browser attacks, catching 78% of data exfiltration attempts, monitoring every single interaction. In the accelerating AI arms race, the companies that survive and thrive will be those who see their red teams as core architects of the platform that push it to the limits of safety and security. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/07/hero-image.jpg?w=624?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\u003c/em\u003e \u003cem\u003e\u003ca href=\"https://venturebeat.com/newsletters/\"\u003eSubscribe Now\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eIn case you missed it, \u003ca href=\"https://venturebeat.com/ai/openai-unveils-chatgpt-agent-that-gives-chatgpt-its-own-computer-to-autonomously-use-your-email-and-web-apps-download-and-create-files-for-you/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI yesterday debuted a powerful new feature for ChatGPT\u003c/a\u003e and with it, a host of new security risks and ramifications.\u003c/p\u003e\n\n\n\n\u003cp\u003eCalled the “ChatGPT agent,” this new feature is an optional mode that ChatGPT paying subscribers can engage by clicking “Tools” in the prompt entry box and selecting “agent mode,” at which point, they can ask ChatGPT to log into their email and other web accounts; write and respond to emails; download, modify, and create files; and do a host of other tasks on their behalf, autonomously, much like a real person using a computer with their login credentials.\u003c/p\u003e\n\n\n\n\u003cp\u003eObviously, this also requires the user to trust the ChatGPT agent not to do anything problematic or nefarious, or to leak their data and sensitive information. It also poses greater risks for a user and their employer than the regular ChatGPT, which can’t log into web accounts or modify files directly. \u003c/p\u003e\n\n\n\n\u003cp\u003eKeren Gu, a member of the Safety Research team at OpenAI, commented on X that “we’ve activated our strongest safeguards for ChatGPT Agent. It’s the first model we’ve classified as High capability in biology \u0026amp; chemistry under our Preparedness Framework. Here’s why that matters–and what we’re doing to keep it safe.”\u003c/p\u003e\n\n\n\n\u003cdiv id=\"boilerplate_2803147\"\u003e\n\u003chr/\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eThe AI Impact Series Returns to San Francisco - August 5\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\u003c/p\u003e\n\n\n\n\u003cp\u003eSecure your spot now - space is limited: \u003ca href=\"https://bit.ly/3GuuPLF\"\u003ehttps://bit.ly/3GuuPLF\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"590\" height=\"421\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image_2a3039.png\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image_2a3039.png 590w, https://venturebeat.com/wp-content/uploads/2025/07/image_2a3039.png?resize=300,214 300w, https://venturebeat.com/wp-content/uploads/2025/07/image_2a3039.png?resize=400,285 400w, https://venturebeat.com/wp-content/uploads/2025/07/image_2a3039.png?resize=578,412 578w\" sizes=\"(max-width: 590px) 100vw, 590px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eSo how did OpenAI handle all these security issues? \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-red-team-s-mission\"\u003eThe red team’s mission\u003c/h2\u003e\n\n\n\n\u003cp\u003eLooking at OpenAI’s ChatGPT agent \u003ca href=\"https://openai.com/index/chatgpt-agent-system-card/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esystem card\u003c/a\u003e, the “read team” employed by the company to test the feature faced a challenging mission: specifically, 16 PhD security researchers who were given 40 hours to test it out. \u003c/p\u003e\n\n\n\n\u003cp\u003eThrough systematic testing, the red team discovered seven universal exploits that could compromise the system, revealing critical vulnerabilities in how AI agents handle real-world interactions.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhat followed next was extensive security testing, much of it predicated on red teaming. The Red Teaming Network submitted 110 attacks, from prompt injections to biological information extraction attempts. Sixteen exceeded internal risk thresholds. Each finding gave OpenAI engineers the insights they needed to get fixes written and deployed before launch.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe results speak for themselves in the \u003ca href=\"https://openai.com/index/chatgpt-agent-system-card/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epublished results in the system card\u003c/a\u003e. ChatGPT Agent emerged with significant security improvements, including 95% performance against visual browser irrelevant instruction attacks and robust biological and chemical safeguards.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-red-teams-exposed-seven-universal-exploits\"\u003eRed teams exposed seven universal exploits \u003c/h2\u003e\n\n\n\n\u003cp\u003eOpenAI’s Red Teaming Network was comprised 16 researchers with biosafety-relevant PhDs who topgether submitted 110 attack attempts during the testing period. Sixteen exceeded internal risk thresholds, revealing fundamental vulnerabilities in how AI agents handle real-world interactions. But the real breakthrough came from UK AISI’s unprecedented access to ChatGPT Agent’s internal reasoning chains and policy text. Admittedly that’s intelligence regular attackers would never possess.\u003c/p\u003e\n\n\n\n\u003cp\u003eOver four testing rounds, UK AISI forced OpenAI to execute seven universal exploits that had the potential to compromise any conversation:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eAttack vectors that forced OpenAI’s hand\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eAttack Type\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003eSuccess Rate (Pre-Fix)\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003eTarget\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003eImpact\u003c/strong\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eVisual Browser Hidden Instructions\u003c/td\u003e\u003ctd\u003e33%\u003c/td\u003e\u003ctd\u003eWeb pages\u003c/td\u003e\u003ctd\u003eActive data exfiltration\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eGoogle Drive Connector Exploitation\u003c/td\u003e\u003ctd\u003eNot disclosed\u003c/td\u003e\u003ctd\u003eCloud documents\u003c/td\u003e\u003ctd\u003eForced document leaks\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eMulti-Step Chain Attacks\u003c/td\u003e\u003ctd\u003eVariable\u003c/td\u003e\u003ctd\u003eCross-site actions\u003c/td\u003e\u003ctd\u003eComplete session compromise\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eBiological Information Extraction\u003c/td\u003e\u003ctd\u003e16 submissions exceeded thresholds\u003c/td\u003e\u003ctd\u003eDangerous knowledge\u003c/td\u003e\u003ctd\u003ePotential weaponization\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eFAR.AI’s assessment was openly critical of OpenAI’s approach. Despite 40 hours of testing revealing only three partial vulnerabilities, they identified that current safety mechanisms relied heavily on monitoring during reasoning and tool-use processes, which the researchers considered a potential single point of failure if compromised.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-red-teaming-helped-turn-chatgpt-vulnerabilities-into-a-fortress\"\u003eHow red teaming helped turn ChatGPT vulnerabilities into a fortress\u003c/h2\u003e\n\n\n\n\u003cp\u003eOpenAI’s response to the red team’s results redefined entire segments of the ChatGPT agent architecture. One of the many initiatives undertaken including building a dual-layer inspection architecture that monitors 100% of production traffic in real-time, achieving these measurable improvements:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eSecurity improvements after red team discoveries\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eDefense Metric\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003ePrevious Models\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003eChatGPT Agent\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e\u003cstrong\u003eImprovement\u003c/strong\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eIrrelevant Instructions (Visual Browser)\u003c/td\u003e\u003ctd\u003e82%\u003c/td\u003e\u003ctd\u003e95%\u003c/td\u003e\u003ctd\u003e+13%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eIn-Context Data Exfiltration\u003c/td\u003e\u003ctd\u003e75%\u003c/td\u003e\u003ctd\u003e78%\u003c/td\u003e\u003ctd\u003e+3%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eActive Data Exfiltration\u003c/td\u003e\u003ctd\u003e58%\u003c/td\u003e\u003ctd\u003e67%\u003c/td\u003e\u003ctd\u003e+9%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eSystem Reliability\u003c/td\u003e\u003ctd\u003eSampling-based\u003c/td\u003e\u003ctd\u003e100% coverage\u003c/td\u003e\u003ctd\u003eComplete monitoring\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe architecture works like this:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFirst Tier\u003c/strong\u003e: A fast classifier with 96% recall flags suspicious content\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eSecond Tier\u003c/strong\u003e: A reasoning model with 84% recall analyzes flagged interactions for actual threats\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eBut the technical defenses tell only part of the story. OpenAI made difficult security choices that acknowledge some AI operations require significant restrictions for safe autonomous execution.\u003c/p\u003e\n\n\n\n\u003cp\u003eBased on the vulnerabilities discovered, OpenAI implemented the following countermeasures across their model:\u003c/p\u003e\n\n\n\n\u003col start=\"1\"\u003e\n\u003cli\u003e\u003cstrong\u003eWatch Mode Activation\u003c/strong\u003e: When ChatGPT Agent accesses sensitive contexts like banking or email accounts, the system freezes all activity if users navigate away. This is in direct response to data exfiltration attempts discovered during testing.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eMemory Features Disabled\u003c/strong\u003e: Despite being a core functionality, memory is completely disabled at launch to prevent the incremental data leaking attacks red teamers demonstrated.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eTerminal Restrictions\u003c/strong\u003e: Network access limited to GET requests only, blocking the command execution vulnerabilities researchers exploited.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eRapid Remediation Protocol\u003c/strong\u003e: A new system that patches vulnerabilities within hours of discovery—developed after red teamers showed how quickly exploits could spread.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eDuring pre-launch testing alone, this system identified and resolved 16 critical vulnerabilities that red teamers had discovered.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-a-biological-risk-wake-up-call\"\u003eA biological risk wake-up call\u003c/h2\u003e\n\n\n\n\u003cp\u003eRed teamers revealed the potential that the ChatGPT Agent could be comprimnised and lead to greater biological risks. Sixteen experienced participants from the Red Teaming Network, each with biosafety-relevant PhDs, attempted to extract dangerous biological information. Their submissions revealed the model could synthesize published literature on modifying and creating biological threats.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn response to the red teamers’ findings, OpenAI classified ChatGPT Agent as “High capability” for biological and chemical risks, not because they found definitive evidence of weaponization potential, but as a precautionary measure based on red team findings. This triggered:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eAlways-on safety classifiers scanning 100% of traffic\u003c/li\u003e\n\n\n\n\u003cli\u003eA topical classifier achieving 96% recall for biology-related content\u003c/li\u003e\n\n\n\n\u003cli\u003eA reasoning monitor with 84% recall for weaponization content\u003c/li\u003e\n\n\n\n\u003cli\u003eA bio bug bounty program for ongoing vulnerability discovery\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch2 id=\"h-what-red-teams-taught-openai-about-ai-security\"\u003eWhat red teams taught OpenAI about AI security\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe 110 attack submissions revealed patterns that forced fundamental changes in OpenAI’s security philosophy. They include the following: \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePersistence over power\u003c/strong\u003e: Attackers don’t need sophisticated exploits, all they need is more time. Red teamers showed how patient, incremental attacks could eventually compromise systems.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eTrust boundaries are fiction\u003c/strong\u003e: When your AI agent can access Google Drive, browse the web, and execute code, traditional security perimeters dissolve. Red teamers exploited the gaps between these capabilities.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eMonitoring isn’t optional\u003c/strong\u003e: The discovery that sampling-based monitoring missed critical attacks led to the 100% coverage requirement.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eSpeed matters\u003c/strong\u003e: Traditional patch cycles measured in weeks are worthless against prompt injection attacks that can spread instantly. The rapid remediation protocol patches vulnerabilities within hours.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eOpenAI is helping to create a new security baseline for Enterprise AI\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eFor CISOs evaluating AI deployment, the red team discoveries establish clear requirements:\u003c/p\u003e\n\n\n\n\u003col start=\"1\"\u003e\n\u003cli\u003e\u003cstrong\u003eQuantifiable protection\u003c/strong\u003e: ChatGPT Agent’s 95% defense rate against documented attack vectors sets the industry benchmark. The nuances of the many tests and results defined in the system card explain the context of how they accomplished this and is a must-read for anyone involved with model security.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eComplete visibility\u003c/strong\u003e: 100% traffic monitoring isn’t aspirational anymore. OpenAI’s experiences illustrate why it’s mandatory given how easily red teams can hide attacks anywhere.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eRapid response\u003c/strong\u003e: Hours, not weeks, to patch discovered vulnerabilities.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eEnforced boundaries\u003c/strong\u003e: Some operations (like memory access during sensitive tasks) must be disabled until proven safe.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eUK AISI’s testing proved particularly instructive. All seven universal attacks they identified were patched before launch, but their privileged access to internal systems revealed vulnerabilities that would eventually be discoverable by determined adversaries.\u003c/p\u003e\n\n\n\n\u003cp\u003e“This is a pivotal moment for our Preparedness work,” Gu wrote on X. “Before we reached High capability, Preparedness was about analyzing capabilities and planning safeguards. Now, for Agent and future more capable models, Preparedness safeguards have become an operational requirement.”\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"581\" height=\"151\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image_adecd1.png\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image_adecd1.png 581w, https://venturebeat.com/wp-content/uploads/2025/07/image_adecd1.png?resize=300,78 300w, https://venturebeat.com/wp-content/uploads/2025/07/image_adecd1.png?resize=400,104 400w, https://venturebeat.com/wp-content/uploads/2025/07/image_adecd1.png?resize=578,150 578w\" sizes=\"(max-width: 581px) 100vw, 581px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-red-teams-are-core-to-building-safer-more-secure-ai-models\"\u003eRed teams are core to building safer, more secure AI models \u003c/h2\u003e\n\n\n\n\u003cp\u003eThe seven universal exploits discovered by researchers and the 110 attacks from OpenAI’s red team network became the crucible that forged ChatGPT Agent. \u003c/p\u003e\n\n\n\n\u003cp\u003eBy revealing exactly how AI agents could be weaponized, red teams forced the creation of the first AI system where security isn’t just a feature. It’s the foundation.\u003c/p\u003e\n\n\n\n\u003cp\u003eChatGPT Agent’s results prove red teaming’s effectiveness: blocking 95% of visual browser attacks, catching 78% of data exfiltration attempts, monitoring every single interaction. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn the accelerating AI arms race, the companies that survive and thrive will be those who see their red teams as core architects of the platform that push it to the limits of safety and security.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2025-07-18T22:13:52Z",
  "modifiedTime": "2025-07-18T22:29:15Z"
}
