{
  "id": "caa7d7dc-abc2-4b39-93c5-e2b97fb60cf6",
  "title": "Mistral just updated its open source Small model from 3.1 to 3.2: here’s why",
  "link": "https://venturebeat.com/ai/mistral-just-updated-its-open-source-small-model-from-3-1-to-3-2-heres-why/",
  "description": "The fact that it is made by a French startup and compliant with EU rules and regulations such as GDPR and the EU AI Act also helps its appeal",
  "author": "Carl Franzen",
  "published": "Fri, 20 Jun 2025 22:51:45 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "Programming \u0026 Development",
    "AI, ML and Deep Learning",
    "apache 2.0",
    "benchmarks",
    "Conversational AI",
    "data",
    "EU",
    "EU AI Act",
    "eu data act",
    "Europe",
    "European Union",
    "France",
    "function calling",
    "GDPR",
    "instruction following",
    "mistral",
    "Mistral AI",
    "Mistral Small",
    "Mistral Small 3",
    "Mistral Small 3.1",
    "mistral small 3.2",
    "NLP"
  ],
  "byline": "Carl Franzen",
  "length": 6506,
  "excerpt": "The fact that it is made by a French startup and compliant with EU rules and regulations such as GDPR and the EU AI Act also helps its appeal",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy. Learn more French AI darling Mistral is keeping the new releases coming this summer. Just days after announcing its own domestic AI-optimized cloud service Mistral Compute, the well-funded company has released an update to its 24B parameter open source model Mistral Small, jumping from a 3.1 release to 3.2-24B Instruct-2506. The new version builds directly on Mistral Small 3.1, aiming to improve specific behaviors such as instruction following, output stability, and function calling robustness. While overall architectural details remain unchanged, the update introduces targeted refinements that affect both internal evaluations and public benchmarks. According to Mistral AI, Small 3.2 is better at adhering to precise instructions and reduces the likelihood of infinite or repetitive generations — a problem occasionally seen in prior versions when handling long or ambiguous prompts. Similarly, the function calling template has been upgraded to support more reliable tool-use scenarios, particularly in frameworks like vLLM. And at the same time, it could run on a setup with a single Nvidia A100/H100 80GB GPU, drastically opening up the options for businesses with tight compute resources and/or budgets. An updated model after only 3 months Mistral Small 3.1 was announced in March 2025 as a flagship open release in the 24B parameter range. It offered full multimodal capabilities, multilingual understanding, and long-context processing of up to 128K tokens. The model was explicitly positioned against proprietary peers like GPT-4o Mini, Claude 3.5 Haiku, and Gemma 3-it — and, according to Mistral, outperformed them across many tasks. Small 3.1 also emphasized efficient deployment, with claims of running inference at 150 tokens per second and support for on-device use with 32 GB RAM. That release came with both base and instruct checkpoints, offering flexibility for fine-tuning across domains such as legal, medical, and technical fields. In contrast, Small 3.2 focuses on surgical improvements to behavior and reliability. It does not aim to introduce new capabilities or architecture changes. Instead, it acts as a maintenance release: cleaning up edge cases in output generation, tightening instruction compliance, and refining system prompt interactions. Small 3.2 vs. Small 3.1: what changed? Instruction-following benchmarks show a small but measurable improvement. Mistral’s internal accuracy rose from 82.75% in Small 3.1 to 84.78% in Small 3.2. Similarly, performance on external datasets like Wildbench v2 and Arena Hard v2 improved significantly—Wildbench increased by nearly 10 percentage points, while Arena Hard more than doubled, jumping from 19.56% to 43.10%. Internal metrics also suggest reduced output repetition. The rate of infinite generations dropped from 2.11% in Small 3.1 to 1.29% in Small 3.2 — almost a 2× reduction. This makes the model more reliable for developers building applications that require consistent, bounded responses. Performance across text and coding benchmarks presents a more nuanced picture. Small 3.2 showed gains on HumanEval Plus (88.99% to 92.90%), MBPP Pass@5 (74.63% to 78.33%), and SimpleQA. It also modestly improved MMLU Pro and MATH results. Vision benchmarks remain mostly consistent, with slight fluctuations. ChartQA and DocVQA saw marginal gains, while AI2D and Mathvista dropped by less than two percentage points. Average vision performance decreased slightly from 81.39% in Small 3.1 to 81.00% in Small 3.2. This aligns with Mistral’s stated intent: Small 3.2 is not a model overhaul, but a refinement. As such, most benchmarks are within expected variance, and some regressions appear to be trade-offs for targeted improvements elsewhere. However, as AI power user and influencer @chatgpt21 posted on X: “It got worse on MMLU,” meaning the Massive Multitask Language Understanding benchmark, a multidisciplinary test with 57 questions designed to assess broad LLM performance across domains. Indeed, Small 3.2 scored 80.50%, slightly below Small 3.1’s 80.62%. Open source license will make it more appealing to cost-conscious and customized-focused users Both Small 3.1 and 3.2 are available under the Apache 2.0 license and can be accessed via the popular. AI code sharing repository Hugging Face (itself a startup based in France and NYC). Small 3.2 is supported by frameworks like vLLM and Transformers and requires roughly 55 GB of GPU RAM to run in bf16 or fp16 precision. For developers seeking to build or serve applications, system prompts and inference examples are provided in the model repository. While Mistral Small 3.1 is already integrated into platforms like Google Cloud Vertex AI and is scheduled for deployment on NVIDIA NIM and Microsoft Azure, Small 3.2 currently appears limited to self-serve access via Hugging Face and direct deployment. What enterprises should know when considering Mistral Small 3.2 for their use cases Mistral Small 3.2 may not shift competitive positioning in the open-weight model space, but it represents Mistral AI’s commitment to iterative model refinement. With noticeable improvements in reliability and task handling — particularly around instruction precision and tool usage — Small 3.2 offers a cleaner user experience for developers and enterprises building on the Mistral ecosystem. The fact that it is made by a French startup and compliant with EU rules and regulations such as GDPR and the EU AI Act also make it appealing for enterprises working in that part of the world. Still, for those seeking the biggest jumps in benchmark performance, Small 3.1 remains a reference point—especially given that in some cases, such as MMLU, Small 3.2 does not outperform its predecessor. That makes the update more of a stability-focused option than a pure upgrade, depending on the use case. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/06/cfr0z3n_close_up_on_tiny_humanoid_robot_standing_triumphantly_w_9c85edea-1a61-4168-a0fd-f100581b5809.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy. \u003ca href=\"http://vbtransform.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLearn more\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eFrench AI darling Mistral is keeping the new releases coming this summer.\u003c/p\u003e\n\n\n\n\u003cp\u003eJust days after announcing its own \u003ca href=\"https://venturebeat.com/ai/microsoft-backed-mistral-launches-european-ai-cloud-to-compete-with-aws-and-azure/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edomestic AI-optimized cloud service Mistral Compute\u003c/a\u003e, the well-funded company has \u003ca href=\"https://x.com/MistralAI/status/1936093325116781016\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ereleased an update to its 24B parameter open source model Mistral Small\u003c/a\u003e, jumping from a 3.1 release to 3.2-24B Instruct-2506. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe new version builds directly on Mistral Small 3.1, aiming to improve specific behaviors such as instruction following, output stability, and function calling robustness. While overall architectural details remain unchanged, the update introduces targeted refinements that affect both internal evaluations and public benchmarks.\u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Mistral AI, Small 3.2 is better at adhering to precise instructions and reduces the likelihood of infinite or repetitive generations — a problem occasionally seen in prior versions when handling long or ambiguous prompts. \u003c/p\u003e\n\n\n\n\u003cp\u003eSimilarly, the function calling template has been upgraded to support more reliable tool-use scenarios, particularly in frameworks like vLLM.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnd at the same time, it could run on a setup with a single Nvidia A100/H100 80GB GPU, drastically opening up the options for businesses with tight compute resources and/or budgets.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-an-updated-model-after-only-3-months\"\u003eAn updated model after only 3 months\u003c/h2\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://venturebeat.com/ai/mistral-ai-drops-new-open-source-model-that-outperforms-gpt-4o-mini-with-fraction-of-parameters/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMistral Small 3.1 was announced in March 2025\u003c/a\u003e as a flagship open release in the 24B parameter range. It offered full multimodal capabilities, multilingual understanding, and long-context processing of up to 128K tokens.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe model was explicitly positioned against proprietary peers like GPT-4o Mini, Claude 3.5 Haiku, and Gemma 3-it — and, according to Mistral, outperformed them across many tasks.\u003c/p\u003e\n\n\n\n\u003cp\u003eSmall 3.1 also emphasized efficient deployment, with claims of running inference at 150 tokens per second and support for on-device use with 32 GB RAM. \u003c/p\u003e\n\n\n\n\u003cp\u003eThat release came with both base and instruct checkpoints, offering flexibility for fine-tuning across domains such as legal, medical, and technical fields.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn contrast, Small 3.2 focuses on surgical improvements to behavior and reliability. It does not aim to introduce new capabilities or architecture changes. Instead, it acts as a maintenance release: cleaning up edge cases in output generation, tightening instruction compliance, and refining system prompt interactions.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-small-3-2-vs-small-3-1-what-changed\"\u003eSmall 3.2 vs. Small 3.1: what changed?\u003c/h2\u003e\n\n\n\n\u003cp\u003eInstruction-following benchmarks show a small but measurable improvement. Mistral’s internal accuracy rose from 82.75% in Small 3.1 to 84.78% in Small 3.2. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" height=\"486\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg 2402w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg?resize=300,182 300w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg?resize=768,466 768w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg?resize=800,486 800w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg?resize=1536,932 1536w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg?resize=2048,1243 2048w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg?resize=400,243 400w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg?resize=750,455 750w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg?resize=578,351 578w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSJWMAAjaeh-2.jpg?resize=930,565 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eSimilarly, performance on external datasets like Wildbench v2 and Arena Hard v2 improved significantly—Wildbench increased by nearly 10 percentage points, while Arena Hard more than doubled, jumping from 19.56% to 43.10%.\u003c/p\u003e\n\n\n\n\u003cp\u003eInternal metrics also suggest reduced output repetition. The rate of infinite generations dropped from 2.11% in Small 3.1 to 1.29% in Small 3.2 — almost a 2× reduction. This makes the model more reliable for developers building applications that require consistent, bounded responses.\u003c/p\u003e\n\n\n\n\u003cp\u003ePerformance across text and coding benchmarks presents a more nuanced picture. Small 3.2 showed gains on HumanEval Plus (88.99% to 92.90%), MBPP Pass@5 (74.63% to 78.33%), and SimpleQA. It also modestly improved MMLU Pro and MATH results.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" height=\"399\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg 3188w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=300,150 300w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=768,383 768w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=800,399 800w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=1536,766 1536w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=2048,1021 2048w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=100,50 100w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=350,175 350w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=400,199 400w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=750,374 750w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=578,288 578w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=930,464 930w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=1200,600 1200w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSMWAAA6AvG-2.jpg?resize=700,350 700w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eVision benchmarks remain mostly consistent, with slight fluctuations. ChartQA and DocVQA saw marginal gains, while AI2D and Mathvista dropped by less than two percentage points. Average vision performance decreased slightly from 81.39% in Small 3.1 to 81.00% in Small 3.2.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" height=\"454\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg 4096w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg?resize=300,170 300w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg?resize=768,436 768w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg?resize=800,454 800w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg?resize=1536,873 1536w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg?resize=2048,1164 2048w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg?resize=400,227 400w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg?resize=750,426 750w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg?resize=578,328 578w, https://venturebeat.com/wp-content/uploads/2025/06/Gt5ieSIXAAA6rYX-1.jpg?resize=930,528 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThis aligns with Mistral’s stated intent: Small 3.2 is not a model overhaul, but a refinement. As such, most benchmarks are within expected variance, and some regressions appear to be trade-offs for targeted improvements elsewhere.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, as AI power user and influencer \u003ca href=\"https://x.com/chatgpt21/status/1936102511221322188\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e@chatgpt21 posted on X\u003c/a\u003e: “It got worse on MMLU,” meaning the Massive Multitask Language Understanding benchmark, a multidisciplinary test with 57 questions designed to assess broad LLM performance across domains. Indeed, Small 3.2 scored 80.50%, slightly below Small 3.1’s 80.62%. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-open-source-license-will-make-it-more-appealing-to-cost-conscious-and-customized-focused-users\"\u003eOpen source license will make it more appealing to cost-conscious and customized-focused users\u003c/h2\u003e\n\n\n\n\u003cp\u003eBoth Small 3.1 and 3.2 are available under the Apache 2.0 license and can be accessed via the popular. AI code sharing repository \u003ca href=\"https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\u003eHugging Face\u003c/a\u003e (itself a startup based in France and NYC). \u003c/p\u003e\n\n\n\n\u003cp\u003eSmall 3.2 is supported by frameworks like vLLM and Transformers and requires roughly 55 GB of GPU RAM to run in bf16 or fp16 precision. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor developers seeking to build or serve applications, system prompts and inference examples are provided in the model repository.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile Mistral Small 3.1 is already integrated into platforms like Google Cloud Vertex AI and is scheduled for deployment on NVIDIA NIM and Microsoft Azure, Small 3.2 currently appears limited to self-serve access via Hugging Face and direct deployment.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-enterprises-should-know-when-considering-mistral-small-3-2-for-their-use-cases\"\u003eWhat enterprises should know when considering Mistral Small 3.2 for their use cases\u003c/h2\u003e\n\n\n\n\u003cp\u003eMistral Small 3.2 may not shift competitive positioning in the open-weight model space, but it represents Mistral AI’s commitment to iterative model refinement. \u003c/p\u003e\n\n\n\n\u003cp\u003eWith noticeable improvements in reliability and task handling — particularly around instruction precision and tool usage — Small 3.2 offers a cleaner user experience for developers and enterprises building on the Mistral ecosystem.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe fact that it is made by a French startup and compliant with EU rules and regulations such as GDPR and the EU AI Act also make it appealing for enterprises working in that part of the world.\u003c/p\u003e\n\n\n\n\u003cp\u003eStill, for those seeking the biggest jumps in benchmark performance, Small 3.1 remains a reference point—especially given that in some cases, such as MMLU, Small 3.2 does not outperform its predecessor. That makes the update more of a stability-focused option than a pure upgrade, depending on the use case.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-06-20T22:51:45Z",
  "modifiedTime": "2025-06-20T22:53:37Z"
}
