{
  "id": "8a2ce64b-1035-427e-a1c5-afcda30d82b1",
  "title": "Is your AI product actually working? How to develop the right metric system",
  "link": "https://venturebeat.com/ai/is-your-ai-product-actually-working-how-to-develop-the-right-metric-system/",
  "description": "Metrics are critical for determining AI product performance. But where to begin? Here's a framework to apply across various use cases.",
  "author": "Sharanya Rao, Intuit",
  "published": "Sun, 27 Apr 2025 19:15:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "AI research",
    "AI Research and Development",
    "AI, ML and Deep Learning",
    "Generative AI",
    "Intuit",
    "large language models"
  ],
  "byline": "Sharanya Rao, Intuit",
  "length": 6713,
  "excerpt": "Metrics are critical for determining AI product performance. But where to begin? Here's a framework to apply across various use cases.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More In my first stint as a machine learning (ML) product manager, a simple question inspired passionate debates across functions and leaders: How do we know if this product is actually working? The product in question that I managed catered to both internal and external customers. The model enabled internal teams to identify the top issues faced by our customers so that they could prioritize the right set of experiences to fix customer issues. With such a complex web of interdependencies among internal and external customers, choosing the right metrics to capture the impact of the product was critical to steer it towards success. Not tracking whether your product is working well is like landing a plane without any instructions from air traffic control. There is absolutely no way that you can make informed decisions for your customer without knowing what is going right or wrong. Additionally, if you do not actively define the metrics, your team will identify their own back-up metrics. The risk of having multiple flavors of an ‘accuracy’ or ‘quality’ metric is that everyone will develop their own version, leading to a scenario where you might not all be working toward the same outcome. For example, when I reviewed my annual goal and the underlying metric with our engineering team, the immediate feedback was: “But this is a business metric, we already track precision and recall.”  First, identify what you want to know about your AI product Once you do get down to the task of defining the metrics for your product — where to begin? In my experience, the complexity of operating an ML product with multiple customers translates to defining metrics for the model, too. What do I use to measure whether a model is working well? Measuring the outcome of internal teams to prioritize launches based on our models would not be quick enough; measuring whether the customer adopted solutions recommended by our model could risk us drawing conclusions from a very broad adoption metric (what if the customer didn’t adopt the solution because they just wanted to reach a support agent?). Fast-forward to the era of large language models (LLMs) — where we don’t just have a single output from an ML model, we have text answers, images and music as outputs, too. The dimensions of the product that require metrics now rapidly increases — formats, customers, type … the list goes on. Across all my products, when I try to come up with metrics, my first step is to distill what I want to know about its impact on customers into a few key questions. Identifying the right set of questions makes it easier to identify the right set of metrics. Here are a few examples: Did the customer get an output? → metric for coverage How long did it take for the product to provide an output? → metric for latency Did the user like the output? → metrics for customer feedback, customer adoption and retention Once you identify your key questions, the next step is to identify a set of sub-questions for ‘input’ and ‘output’ signals. Output metrics are lagging indicators where you can measure an event that has already happened. Input metrics and leading indicators can be used to identify trends or predict outcomes. See below for ways to add the right sub-questions for lagging and leading indicators to the questions above. Not all questions need to have leading/lagging indicators. Did the customer get an output? → coverage How long did it take for the product to provide an output? → latency Did the user like the output? → customer feedback, customer adoption and retention Did the user indicate that the output is right/wrong? (output) Was the output good/fair? (input) The third and final step is to identify the method to gather metrics. Most metrics are gathered at-scale by new instrumentation via data engineering. However, in some instances (like question 3 above) especially for ML based products, you have the option of manual or automated evaluations that assess the model outputs. While it’s always best to develop automated evaluations, starting with manual evaluations for “was the output good/fair” and creating a rubric for the definitions of good, fair and not good will help you lay the groundwork for a rigorous and tested automated evaluation process, too. Example use cases: AI search, listing descriptions The above framework can be applied to any ML-based product to identify the list of primary metrics for your product. Let’s take search as an example. Question MetricsNature of MetricDid the customer get an output? → Coverage% search sessions with search results shown to customerOutputHow long did it take for the product to provide an output? → LatencyTime taken to display search results for the userOutputDid the user like the output? → Customer feedback, customer adoption and retentionDid the user indicate that the output is right/wrong? (Output) Was the output good/fair? (Input)% of search sessions with ‘thumbs up’ feedback on search results from the customer or % of search sessions with clicks from the customer% of search results marked as ‘good/fair’ for each search term, per quality rubricOutputInput How about a product to generate descriptions for a listing (whether it’s a menu item in Doordash or a product listing on Amazon)? Question MetricsNature of MetricDid the customer get an output? → Coverage% listings with generated descriptionOutputHow long did it take for the product to provide an output? → LatencyTime taken to generate descriptions to the userOutputDid the user like the output? → Customer feedback, customer adoption and retentionDid the user indicate that the output is right/wrong? (Output) Was the output good/fair? (Input)% of listings with generated descriptions that required edits from the technical content team/seller/customer% of listing descriptions marked as ‘good/fair’, per quality rubricOutputInput The approach outlined above is extensible to multiple ML-based products. I hope this framework helps you define the right set of metrics for your ML model. Sharanya Rao is a group product manager at Intuit. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/04/DDM-Robot.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eIn my first stint as a machine learning (ML) product manager, a simple question inspired passionate debates across functions and leaders: How do we know if this product is actually working? The product in question that I managed catered to both internal and external customers. The model enabled internal teams to identify the top issues faced by our customers so that they could prioritize the right set of experiences to fix customer issues. With such a complex web of interdependencies among internal and external customers, choosing the \u003ca href=\"https://venturebeat.com/ai/beyond-arc-agi-gaia-and-the-search-for-a-real-intelligence-benchmark/\"\u003eright metrics\u003c/a\u003e to capture the impact of the product was critical to steer it towards success.\u003c/p\u003e\n\n\n\n\u003cp\u003eNot tracking whether your product is working well is like landing a plane without any instructions from air traffic control. There is absolutely no way that you can make informed decisions for your customer without knowing what is going right or wrong. Additionally, if you do not actively define the metrics, your team will identify their own back-up metrics. The risk of having multiple flavors of an ‘accuracy’ or ‘quality’ metric is that everyone will develop their own version, leading to a scenario where you might not all be working toward the same outcome. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, when I reviewed my annual goal and the underlying metric with our engineering team, the immediate feedback was: “But this is a business metric, we already track precision and recall.” \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-first-identify-what-you-want-to-know-about-your-ai-product\"\u003eFirst, identify what you want to know about your AI product\u003c/h2\u003e\n\n\n\n\u003cp\u003eOnce you do get down to the task of defining the metrics for your product — where to begin? In my experience, the complexity of operating an \u003ca href=\"https://venturebeat.com/ai/2027-agi-forecast-maps-a-24-month-sprint-to-human-level-ai/\"\u003eML product\u003c/a\u003e with multiple customers translates to defining metrics for the model, too. What do I use to measure whether a model is working well? Measuring the outcome of internal teams to prioritize launches based on our models would not be quick enough; measuring whether the customer adopted solutions recommended by our model could risk us drawing conclusions from a very broad adoption metric (what if the customer didn’t adopt the solution because they just wanted to reach a support agent?). \u003c/p\u003e\n\n\n\n\u003cp\u003eFast-forward to the era of \u003ca href=\"https://venturebeat.com/programming-development/the-open-source-advantage-faster-bugs-better-builds-wider-buy-in/\"\u003elarge language models\u003c/a\u003e (LLMs) — where we don’t just have a single output from an ML model, we have text answers, images and music as outputs, too. The dimensions of the product that require metrics now rapidly increases — formats, customers, type … the list goes on.\u003c/p\u003e\n\n\n\n\u003cp\u003eAcross all my products, when I try to come up with metrics, my first step is to distill what I want to know about its impact on customers into a few key questions. Identifying the right set of questions makes it easier to identify the right set of metrics. Here are a few examples: \u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eDid the customer get an output? → metric for coverage\u003c/li\u003e\n\n\n\n\u003cli\u003eHow long did it take for the product to provide an output? → metric for latency\u003c/li\u003e\n\n\n\n\u003cli\u003eDid the user like the output? → metrics for customer feedback, customer adoption and retention\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eOnce you identify your key questions, the next step is to identify a set of sub-questions for ‘input’ and ‘output’ signals. Output metrics are lagging indicators where you can measure an event that has already happened. Input metrics and leading indicators can be used to identify trends or predict outcomes. See below for ways to add the right sub-questions for lagging and leading indicators to the questions above. Not all questions need to have leading/lagging indicators.\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eDid the customer get an output? → coverage\u003c/li\u003e\n\n\n\n\u003cli\u003eHow long did it take for the product to provide an output? → latency\u003c/li\u003e\n\n\n\n\u003cli\u003eDid the user like the output? → customer feedback, customer adoption and retention\n\u003col\u003e\n\u003cli\u003eDid the user indicate that the output is right/wrong? (output)\u003c/li\u003e\n\n\n\n\u003cli\u003eWas the output good/fair? (input)\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eThe third and final step is to identify the method to gather metrics. Most metrics are gathered at-scale by new instrumentation via data engineering. However, in some instances (like question 3 above) especially for ML based products, you have the option of manual or automated evaluations that assess the model outputs. While it’s always best to develop automated evaluations, starting with manual evaluations for “was the output good/fair” and creating a rubric for the definitions of good, fair and not good will help you lay the groundwork for a rigorous and tested automated evaluation process, too.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-example-use-cases-ai-search-listing-descriptions\"\u003eExample use cases: AI search, listing descriptions\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe above framework can be applied to any \u003ca href=\"https://venturebeat.com/ai/bigger-isnt-always-better-examining-the-business-case-for-multi-million-token-llms/\"\u003eML-based product\u003c/a\u003e to identify the list of primary metrics for your product. Let’s take search as an example. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e\u003cstrong\u003eQuestion \u003c/strong\u003e\u003c/th\u003e\u003cth\u003e\u003cstrong\u003eMetrics\u003c/strong\u003e\u003c/th\u003e\u003cth\u003e\u003cstrong\u003eNature of Metric\u003c/strong\u003e\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eDid the customer get an output? → Coverage\u003c/td\u003e\u003ctd\u003e% search sessions with search results shown to customer\u003cbr/\u003e\u003c/td\u003e\u003ctd\u003eOutput\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eHow long did it take for the product to provide an output? → Latency\u003c/td\u003e\u003ctd\u003eTime taken to display search results for the user\u003c/td\u003e\u003ctd\u003eOutput\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eDid the user like the output? → Customer feedback, customer adoption and retention\u003cp\u003eDid the user indicate that the output is right/wrong? (Output) Was the output good/fair? (Input)\u003c/p\u003e\u003c/td\u003e\u003ctd\u003e% of search sessions with ‘thumbs up’ feedback on search results from the customer or % of search sessions with clicks from the customer\u003cp\u003e% of search results marked as ‘good/fair’ for each search term, per quality rubric\u003c/p\u003e\u003c/td\u003e\u003ctd\u003eOutput\u003cp\u003eInput\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eHow about a product to generate descriptions for a listing (whether it’s a menu item in Doordash or a product listing on Amazon)?\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e\u003cstrong\u003eQuestion \u003c/strong\u003e\u003c/th\u003e\u003cth\u003e\u003cstrong\u003eMetrics\u003c/strong\u003e\u003c/th\u003e\u003cth\u003e\u003cstrong\u003eNature of Metric\u003c/strong\u003e\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eDid the customer get an output? → Coverage\u003c/td\u003e\u003ctd\u003e% listings with generated description\u003cbr/\u003e\u003c/td\u003e\u003ctd\u003eOutput\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eHow long did it take for the product to provide an output? → Latency\u003c/td\u003e\u003ctd\u003eTime taken to generate descriptions to the user\u003c/td\u003e\u003ctd\u003eOutput\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eDid the user like the output? → Customer feedback, customer adoption and retention\u003cp\u003eDid the user indicate that the output is right/wrong? (Output) Was the output good/fair? (Input)\u003c/p\u003e\u003c/td\u003e\u003ctd\u003e% of listings with generated descriptions that required edits from the technical content team/seller/customer\u003cp\u003e% of listing descriptions marked as ‘good/fair’, per quality rubric\u003c/p\u003e\u003c/td\u003e\u003ctd\u003eOutput\u003cp\u003eInput\u003c/p\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe approach outlined above is extensible to multiple ML-based products. I hope this framework helps you define the right set of metrics for your ML model. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eSharanya Rao is a group product manager at \u003ca href=\"https://www.intuit.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eIntuit\u003c/a\u003e. \u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-04-27T19:15:00Z",
  "modifiedTime": "2025-04-26T22:37:18Z"
}
