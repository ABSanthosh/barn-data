{
  "id": "84abb72d-d1da-4643-9164-c363f13d93c2",
  "title": "Hugging Face shows how test-time scaling helps small language models punch above their weight",
  "link": "https://venturebeat.com/ai/hugging-face-shows-how-test-time-scaling-helps-small-language-models-punch-above-their-weight/",
  "description": "Given enough time to \"think,\" small language models can beat LLMs at math and coding tasks by generating and verifying multiple answers.",
  "author": "Ben Dickson",
  "published": "Fri, 20 Dec 2024 20:46:10 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI research",
    "AI, ML and Deep Learning",
    "category-/Computers \u0026 Electronics/Programming",
    "category-/Science/Computer Science",
    "Hugging Face",
    "large language models",
    "LLM reasoning",
    "LLMs",
    "openai o1",
    "research",
    "SLMs",
    "small language models"
  ],
  "byline": "Ben Dickson",
  "length": 6486,
  "excerpt": "Given enough time to \"think,\" small language models can beat LLMs at math and coding tasks by generating and verifying multiple answers.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 20, 2024 12:46 PM Image credit: VentureBeat with Ideogram Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More In a new case study, Hugging Face researchers have demonstrated how small language models (SLMs) can be configured to outperform much larger models. Their findings show that a Llama 3 model with 3B parameters can outperform the 70B version of the model in complex math problems. Hugging Face has fully documented the entire process and provides a roadmap for enterprises that want to create their own customized reasoning models. Image source: Hugging Face Scaling test-time compute The work is inspired by OpenAI o1, which uses extra “thinking” to solve complex math, coding and reasoning problems. The key idea behind models like o1 is to scale “test-time compute,” which effectively means using more compute cycles during inference to test and verify different responses and reasoning paths before producing the final answer. Scaling test-time compute is especially useful when there is not enough memory to run a large model.  Since o1 is a private model and OpenAI has remained tight-lipped about its internal workings, researchers have been speculating about how it works and trying to reverse engineer the process. There are already several open alternatives to o1. Hugging Face work is based on a DeepMind study released in August, which investigates the tradeoffs between inference-time and pre-training compute. The study provides comprehensive guidelines on how to balance training and inference compute to get the best results for a fixed budget. In addition to using extra inference-time compute, the success of the technique hinges on two key components: A reward model that evaluates the SLM’s answers, and a search algorithm that optimizes the path it takes to refine its answers. Image source: Hugging Face Different reasoning algorithms The simplest way to use test-time scaling is “majority voting,” in which the same prompt is sent to the model multiple times and the highest-voted is chosen. In simple problems, majority voting can prove useful, but its gains quickly plateau on complex reasoning problems or tasks where errors are consistent across generations. A more advanced reasoning method is “Best-of-N.” In this technique, the SLM generates multiple answers, but instead of majority voting, a reward model is used to evaluate the answers and choose the best one. “Weighted Best-of-N,” a more nuanced version of this method, factors in consistency to choose answers that are both confident and occur more frequently than others. The researchers used a “process reward model” (PRM) that scores the SLM’s response not only on the final answer but also on the multiple stages it goes through to reach it. Their experiments showed that Weighted Best-of-N and PRMs brought the Llama-3.2 1B near the level of Llama-3.2 8B on the difficult MATH-500 benchmark. Image source: Hugging Face Adding search To further improve the model’s performance, the researchers added search algorithms to the model’s reasoning process. Instead of generating the answer in a single pass, they used “beam search,” an algorithm that guides the model’s answer process step by step. At each step, the SLM generates multiple partial answers. The search algorithm uses the reward model to evaluate the answers and chooses a subset that is worth further exploring. The process is repeated until the model exhausts its inference budget or reaches the correct answer. This way, the inference budget can be narrowed to focus on the most promising answers. The researchers found that while beam search improves the model’s performance on complex problems, it tends to underperform other techniques on simple problems. To address this challenge, they added two more elements to their inference strategy. First was Diverse Verifier Tree Search (DVTS), a variant of beam search that ensures that the SLM doesn’t get stuck in false reasoning paths and diversifies its response branches. Secondly, they developed a “compute-optimal scaling strategy,” as suggested in the DeepMind paper, which dynamically chooses the best test-time scaling strategy based on the difficulty of the input problem.  The combination of these techniques enabled Llama-3.2 1B to punch above its weight and outperform the 8B model by a significant margin. They also found that the strategy was scalable, and when applied to Llama-3.2 3B, they were able to outperform the much larger 70B model. Not a perfect solution yet Scaling test-time compute changes the dynamics of model costs. Enterprises now have the ability to choose where to allocate their compute resources. For example, if you are short on memory or can tolerate slower response times, you can use a small model and spend more inference-time cycles to generate more accurate answers. However, test-time scaling also has its limitations. For example, in the experiments carried out by Hugging Face, researchers used a specially trained Llama-3.1-8B model as the PRM, which requires running two models in parallel (even if it is much more resource-efficient than the 70B model). The researchers acknowledge that the holy grail of test-time scaling is to have “self-verification,” where the original model verifies its own answer as opposed to relying on an external verifier. This is an open area of research. The test-time scaling technique presented in this study is also limited to problems where the answer can be clearly evaluated, such as coding and math. Creating reward models and verifiers for subjective tasks such as creative writing and product design requires further research. But what is clear is that test-time scaling has generated a lot of interest and activity and we can expect more tools and techniques to emerge in the coming months. Enterprises will be wise to keep an eye on how the landscape develops. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/tiny-robot.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-20T20:46:10+00:00\" datetime=\"2024-12-20T20:46:10+00:00\"\u003eDecember 20, 2024 12:46 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"421\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/tiny-robot.webp?w=750\" alt=\"Image credit: VentureBeat with Ideogram\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with Ideogram\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eIn a new case study, Hugging Face researchers have demonstrated how \u003ca href=\"https://venturebeat.com/ai/small-model-big-impact-patronus-ai-glider-outperforms-gpt-4-in-key-ai-benchmarks/\"\u003esmall language models\u003c/a\u003e (SLMs) can be configured to outperform much larger models. Their findings show that a Llama 3 model with 3B parameters can outperform the 70B version of the model in complex math problems.\u003c/p\u003e\n\n\n\n\u003cp\u003eHugging Face has \u003ca href=\"https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute\" target=\"_blank\" rel=\"noreferrer noopener\"\u003efully documented\u003c/a\u003e the entire process and provides a roadmap for enterprises that want to create their own customized reasoning models.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1000\" height=\"600\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image_124a1c.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image_124a1c.png 1000w, https://venturebeat.com/wp-content/uploads/2024/12/image_124a1c.png?resize=300,180 300w, https://venturebeat.com/wp-content/uploads/2024/12/image_124a1c.png?resize=768,461 768w, https://venturebeat.com/wp-content/uploads/2024/12/image_124a1c.png?resize=800,480 800w, https://venturebeat.com/wp-content/uploads/2024/12/image_124a1c.png?resize=400,240 400w, https://venturebeat.com/wp-content/uploads/2024/12/image_124a1c.png?resize=750,450 750w, https://venturebeat.com/wp-content/uploads/2024/12/image_124a1c.png?resize=578,347 578w, https://venturebeat.com/wp-content/uploads/2024/12/image_124a1c.png?resize=930,558 930w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eImage source: Hugging Face\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-scaling-test-time-compute\"\u003eScaling test-time compute\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe work is inspired by \u003ca href=\"https://venturebeat.com/programming-development/openai-opens-its-most-powerful-model-o1-up-to-third-party-developers/\"\u003eOpenAI o1\u003c/a\u003e, which uses extra “thinking” to solve complex math, coding and reasoning problems.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe key idea behind models like o1 is to scale “test-time compute,” which effectively means using more compute cycles during inference to test and verify different responses and reasoning paths before producing the final answer. Scaling test-time compute is especially useful when there is not enough memory to run a large model. \u003c/p\u003e\n\n\n\n\u003cp\u003eSince o1 is a private model and OpenAI has remained tight-lipped about its internal workings, researchers have been speculating about how it works and trying to reverse engineer the process. There are already several \u003ca href=\"https://venturebeat.com/ai/heres-how-openai-o1-might-lose-ground-to-open-source-models/\"\u003eopen alternatives to o1\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eHugging Face work is based on a \u003ca href=\"https://venturebeat.com/ai/deepmind-and-uc-berkeley-shows-how-to-make-the-most-of-llm-inference-time-compute/\"\u003eDeepMind study released in August\u003c/a\u003e, which investigates the tradeoffs between inference-time and pre-training compute. The study provides comprehensive guidelines on how to balance training and inference compute to get the best results for a fixed budget.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn addition to using extra inference-time compute, the success of the technique hinges on two key components: A reward model that evaluates the SLM’s answers, and a search algorithm that optimizes the path it takes to refine its answers.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"2922\" height=\"1038\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png 2922w, https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png?resize=300,107 300w, https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png?resize=768,273 768w, https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png?resize=800,284 800w, https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png?resize=1536,546 1536w, https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png?resize=2048,728 2048w, https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png?resize=400,142 400w, https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png?resize=750,266 750w, https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png?resize=578,205 578w, https://venturebeat.com/wp-content/uploads/2024/12/image_2d4457.png?resize=930,330 930w\" sizes=\"(max-width: 2922px) 100vw, 2922px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eImage source: Hugging Face\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-different-reasoning-algorithms\"\u003eDifferent reasoning algorithms\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe simplest way to use test-time scaling is “majority voting,” in which the same prompt is sent to the model multiple times and the highest-voted is chosen. In simple problems, majority voting can prove useful, but its gains quickly plateau on complex reasoning problems or tasks where errors are consistent across generations.\u003c/p\u003e\n\n\n\n\u003cp\u003eA more advanced reasoning method is “Best-of-N.” In this technique, the SLM generates multiple answers, but instead of majority voting, a reward model is used to evaluate the answers and choose the best one. “Weighted Best-of-N,” a more nuanced version of this method, factors in consistency to choose answers that are both confident and occur more frequently than others.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers used a “process reward model” (PRM) that scores the SLM’s response not only on the final answer but also on the multiple stages it goes through to reach it. Their experiments showed that Weighted Best-of-N and PRMs brought the \u003ca href=\"https://venturebeat.com/ai/meta-llama-3-2-vision-models-to-rival-anthropic-openai/\"\u003eLlama-3.2 1B\u003c/a\u003e near the level of Llama-3.2 8B on the difficult MATH-500 benchmark.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1000\" height=\"600\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image_9c3fc4.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image_9c3fc4.png 1000w, https://venturebeat.com/wp-content/uploads/2024/12/image_9c3fc4.png?resize=300,180 300w, https://venturebeat.com/wp-content/uploads/2024/12/image_9c3fc4.png?resize=768,461 768w, https://venturebeat.com/wp-content/uploads/2024/12/image_9c3fc4.png?resize=800,480 800w, https://venturebeat.com/wp-content/uploads/2024/12/image_9c3fc4.png?resize=400,240 400w, https://venturebeat.com/wp-content/uploads/2024/12/image_9c3fc4.png?resize=750,450 750w, https://venturebeat.com/wp-content/uploads/2024/12/image_9c3fc4.png?resize=578,347 578w, https://venturebeat.com/wp-content/uploads/2024/12/image_9c3fc4.png?resize=930,558 930w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eImage source: Hugging Face\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-adding-search\"\u003eAdding search\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo further improve the model’s performance, the researchers added search algorithms to the model’s reasoning process. Instead of generating the answer in a single pass, they used “beam search,” an algorithm that guides the model’s answer process step by step.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt each step, the SLM generates multiple partial answers. The search algorithm uses the reward model to evaluate the answers and chooses a subset that is worth further exploring. The process is repeated until the model exhausts its inference budget or reaches the correct answer. This way, the inference budget can be narrowed to focus on the most promising answers.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers found that while beam search improves the model’s performance on complex problems, it tends to underperform other techniques on simple problems. To address this challenge, they added two more elements to their inference strategy.\u003c/p\u003e\n\n\n\n\u003cp\u003eFirst was Diverse Verifier Tree Search (DVTS), a variant of beam search that ensures that the SLM doesn’t get stuck in false reasoning paths and diversifies its response branches. Secondly, they developed a “compute-optimal scaling strategy,” as suggested in the DeepMind paper, which dynamically chooses the best test-time scaling strategy based on the difficulty of the input problem. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe combination of these techniques enabled Llama-3.2 1B to punch above its weight and outperform the 8B model by a significant margin. They also found that the strategy was scalable, and when applied to Llama-3.2 3B, they were able to outperform the much larger 70B model.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1000\" height=\"600\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/image_0a708f.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/image_0a708f.png 1000w, https://venturebeat.com/wp-content/uploads/2024/12/image_0a708f.png?resize=300,180 300w, https://venturebeat.com/wp-content/uploads/2024/12/image_0a708f.png?resize=768,461 768w, https://venturebeat.com/wp-content/uploads/2024/12/image_0a708f.png?resize=800,480 800w, https://venturebeat.com/wp-content/uploads/2024/12/image_0a708f.png?resize=400,240 400w, https://venturebeat.com/wp-content/uploads/2024/12/image_0a708f.png?resize=750,450 750w, https://venturebeat.com/wp-content/uploads/2024/12/image_0a708f.png?resize=578,347 578w, https://venturebeat.com/wp-content/uploads/2024/12/image_0a708f.png?resize=930,558 930w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-not-a-perfect-solution-yet\"\u003eNot a perfect solution yet\u003c/h2\u003e\n\n\n\n\u003cp\u003eScaling test-time compute changes the dynamics of model costs. Enterprises now have the ability to choose where to allocate their compute resources. For example, if you are short on memory or can tolerate slower response times, you can use a small model and spend more inference-time cycles to generate more accurate answers.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, test-time scaling also has its limitations. For example, in the experiments carried out by Hugging Face, researchers used a specially trained Llama-3.1-8B model as the PRM, which requires running two models in parallel (even if it is much more resource-efficient than the 70B model). The researchers acknowledge that the holy grail of test-time scaling is to have “self-verification,” where the original model verifies its own answer as opposed to relying on an external verifier. This is an open area of research.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe test-time scaling technique presented in this study is also limited to problems where the answer can be clearly evaluated, such as coding and math. Creating reward models and verifiers for subjective tasks such as creative writing and product design requires further research.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut what is clear is that test-time scaling has generated \u003ca href=\"https://venturebeat.com/ai/openai-faces-critical-test-as-chinese-models-close-the-gap-in-ai-leadership/\"\u003ea lot of interest and activity\u003c/a\u003e and we can expect more tools and techniques to emerge in the coming months. Enterprises will be wise to keep an eye on how the landscape develops.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-12-20T20:46:10Z",
  "modifiedTime": "2024-12-20T20:46:17Z"
}
