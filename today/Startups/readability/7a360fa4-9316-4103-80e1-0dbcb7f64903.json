{
  "id": "7a360fa4-9316-4103-80e1-0dbcb7f64903",
  "title": "NoProp: Training Neural Networks without Back-propagation or Forward-propagation",
  "link": "https://arxiv.org/abs/2503.24322",
  "description": "Article URL: https://arxiv.org/abs/2503.24322 Comments URL: https://news.ycombinator.com/item?id=43676837 Points: 11 # Comments: 0",
  "author": "belleville",
  "published": "Mon, 14 Apr 2025 00:03:51 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "[Submitted on 31 Mar 2025]",
  "length": 1884,
  "excerpt": "The canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer below, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, we introduce a new learning method named NoProp, which does not rely on either forward or backwards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. We believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierarchical representations -- at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. We demonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Our results show that NoProp is a viable learning algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gradient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process.",
  "siteName": "arXiv.org",
  "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png",
  "text": "View PDF HTML (experimental) Abstract:The canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer below, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, we introduce a new learning method named NoProp, which does not rely on either forward or backwards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. We believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierarchical representations -- at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. We demonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Our results show that NoProp is a viable learning algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gradient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process. Submission history From: Qinyu Li [view email] [v1] Mon, 31 Mar 2025 17:08:57 UTC (1,263 KB)",
  "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"content-inner\"\u003e\n    \n    \n                \n    \u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2503.24322\"\u003eView PDF\u003c/a\u003e\n    \u003ca href=\"https://arxiv.org/html/2503.24322v1\"\u003eHTML (experimental)\u003c/a\u003e\u003c/p\u003e\u003cblockquote\u003e\n            \u003cspan\u003eAbstract:\u003c/span\u003eThe canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer below, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, we introduce a new learning method named NoProp, which does not rely on either forward or backwards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. We believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierarchical representations -- at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. We demonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Our results show that NoProp is a viable learning algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gradient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process.\n    \u003c/blockquote\u003e\n\n    \n    \n  \u003c/div\u003e\u003cdiv\u003e\n      \u003ch2\u003eSubmission history\u003c/h2\u003e\u003cp\u003e From: Qinyu Li [\u003ca href=\"https://arxiv.org/show-email/42f3dade/2503.24322\" rel=\"nofollow\"\u003eview email\u003c/a\u003e]      \u003cbr/\u003e    \u003cstrong\u003e[v1]\u003c/strong\u003e\n        Mon, 31 Mar 2025 17:08:57 UTC (1,263 KB)\u003cbr/\u003e\n\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": null,
  "modifiedTime": null
}
