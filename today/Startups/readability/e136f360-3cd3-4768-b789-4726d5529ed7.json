{
  "id": "e136f360-3cd3-4768-b789-4726d5529ed7",
  "title": "Liquid AI’s new STAR model architecture outshines Transformer efficiency",
  "link": "https://venturebeat.com/business/liquid-ais-new-star-model-architecture-outshines-transformer-efficiency/",
  "description": "The STAR framework leverages evolutionary algorithms and a numerical encoding system to balance quality and efficiency in AI models.",
  "author": "Carl Franzen",
  "published": "Tue, 03 Dec 2024 00:20:20 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI, ML and Deep Learning",
    "Conversational AI",
    "Liquid",
    "liquid ai",
    "MIT",
    "NLP",
    "Transformers"
  ],
  "byline": "Carl Franzen",
  "length": 5121,
  "excerpt": "The STAR framework leverages evolutionary algorithms and a numerical encoding system to balance quality and efficiency in AI models.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 2, 2024 4:20 PM Credit: VentureBeat mad with ChatGPT Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More As rumors and reports swirl about the difficulty facing top AI companies in developing newer, more powerful large language models (LLMs), the spotlight is increasingly shifting toward alternate architectures to the “Transformer” — the tech underpinning most of the current generative AI boom, introduced by Google researchers in the seminal 2017 paper “Attention Is All You Need.“ As described in that paper and henceforth, a transformer is a deep learning neural network architecture that processes sequential data, such as text or time-series information. Now, MIT-birthed startup Liquid AI has introduced STAR (Synthesis of Tailored Architectures), an innovative framework designed to automate the generation and optimization of AI model architectures. The STAR framework leverages evolutionary algorithms and a numerical encoding system to address the complex challenge of balancing quality and efficiency in deep learning models. According to Liquid AI’s research team, which includes Armin W. Thomas, Rom Parnichkun, Alexander Amini, Stefano Massaroli, and Michael Poli, STAR’s approach represents a shift from traditional architecture design methods. Instead of relying on manual tuning or predefined templates, STAR uses a hierarchical encoding technique—referred to as “STAR genomes”—to explore a vast design space of potential architectures. These genomes enable iterative optimization processes such as recombination and mutation, allowing STAR to synthesize and refine architectures tailored to specific metrics and hardware requirements. 90% cache size reduction versus traditional ML Transformers Liquid AI’s initial focus for STAR has been on autoregressive language modeling, an area where traditional Transformer architectures have long been dominant. In tests conducted during their research, the Liquid AI research team demonstrated STAR’s ability to generate architectures that consistently outperformed highly-optimized Transformer++ and hybrid models. For example, when optimizing for quality and cache size, STAR-evolved architectures achieved cache size reductions of up to 37% compared to hybrid models and 90% compared to Transformers. Despite these efficiency improvements, the STAR-generated models maintained or exceeded the predictive performance of their counterparts. Similarly, when tasked with optimizing for model quality and size, STAR reduced parameter counts by up to 13% while still improving performance on standard benchmarks. The research also highlighted STAR’s ability to scale its designs. A STAR-evolved model scaled from 125 million to 1 billion parameters delivered comparable or superior results to existing Transformer++ and hybrid models, all while significantly reducing inference cache requirements. Re-architecting AI model architecture Liquid AI stated that STAR is rooted in a design theory that incorporates principles from dynamical systems, signal processing, and numerical linear algebra. This foundational approach has enabled the team to develop a versatile search space for computational units, encompassing components such as attention mechanisms, recurrences, and convolutions. One of STAR’s distinguishing features is its modularity, allowing the framework to encode and optimize architectures across multiple hierarchical levels. This capability provides insights into recurring design motifs and enables researchers to identify effective combinations of architectural components. What’s next for STAR? STAR’s ability to synthesize efficient, high-performing architectures has potential applications far beyond language modeling. Liquid AI envisions this framework being used to tackle challenges in various domains where the trade-off between quality and computational efficiency is critical. While Liquid AI has yet to disclose specific plans for commercial deployment or pricing, the research findings signal a significant advancement in the field of automated architecture design. For researchers and developers looking to optimize AI systems, STAR could represent a powerful tool for pushing the boundaries of model performance and efficiency. With its open research approach, Liquid AI has published the full details of STAR in a peer-reviewed paper, encouraging collaboration and further innovation. As the AI landscape continues to evolve, frameworks like STAR are poised to play a key role in shaping the next generation of intelligent systems. STAR might even herald the birth of a new post-Transformer architecture boom — a welcome winter holiday gift for the machine learning and AI research community. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/Liquid-STAR.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-03T00:20:20+00:00\" datetime=\"2024-12-03T00:20:20+00:00\"\u003eDecember 2, 2024 4:20 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"400\" height=\"229\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/Liquid-STAR.png?w=400\" alt=\"vector art, line art, flat art, graphic novel style image of a team of software developers working on laptops and desks and multi monitors in an open field under a starry sky with a constellation forming a giant star\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat mad with ChatGPT\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eAs rumors and reports swirl about the \u003ca href=\"https://www.yahoo.com/tech/ais-meteoric-rise-beginning-slow-013751678.html\"\u003edifficulty facing top AI companies in developing newer, more powerful large language models (LLMs)\u003c/a\u003e, the spotlight is increasingly shifting toward alternate architectures to the “Transformer” — the tech underpinning most of the current generative AI boom, introduced by Google researchers in the seminal 2017 paper “\u003ca href=\"https://arxiv.org/pdf/1706.03762\"\u003eAttention Is All You Need.\u003c/a\u003e“\u003c/p\u003e\n\n\n\n\u003cp\u003eAs described in that paper and henceforth, a transformer is a deep learning neural network architecture that processes sequential data, such as text or time-series information. \u003c/p\u003e\n\n\n\n\u003cp\u003eNow, \u003ca href=\"https://venturebeat.com/ai/mit-spinoff-liquid-debuts-non-transformer-ai-models-and-theyre-already-state-of-the-art/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMIT-birthed startup Liquid AI\u003c/a\u003e has \u003ca href=\"https://www.liquid.ai/research/automated-architecture-synthesis-via-targeted-evolution\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eintroduced STAR (Synthesis of Tailored Architectures)\u003c/a\u003e, an innovative framework designed to automate the generation and optimization of AI model architectures. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe STAR framework leverages evolutionary algorithms and a numerical encoding system to address the complex challenge of balancing quality and efficiency in deep learning models.\u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Liquid AI’s research team, which includes Armin W. Thomas, Rom Parnichkun, Alexander Amini, Stefano Massaroli, and Michael Poli, STAR’s approach represents a shift from traditional architecture design methods. \u003c/p\u003e\n\n\n\n\u003cp\u003eInstead of relying on manual tuning or predefined templates, STAR uses a hierarchical encoding technique—referred to as “STAR genomes”—to explore a vast design space of potential architectures. \u003c/p\u003e\n\n\n\n\u003cp\u003eThese genomes enable iterative optimization processes such as recombination and mutation, allowing STAR to synthesize and refine architectures tailored to specific metrics and hardware requirements.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-90-cache-size-reduction-versus-traditional-ml-transformers\"\u003e90% cache size reduction versus traditional ML Transformers\u003c/h2\u003e\n\n\n\n\u003cp\u003eLiquid AI’s initial focus for STAR has been on autoregressive language modeling, an area where traditional Transformer architectures have long been dominant. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn tests conducted during their research, the Liquid AI research team demonstrated STAR’s ability to generate architectures that consistently outperformed highly-optimized Transformer++ and hybrid models.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, when optimizing for quality and cache size, STAR-evolved architectures achieved cache size reductions of up to 37% compared to hybrid models and 90% compared to Transformers. Despite these efficiency improvements, the STAR-generated models maintained or exceeded the predictive performance of their counterparts. \u003c/p\u003e\n\n\n\n\u003cp\u003eSimilarly, when tasked with optimizing for model quality and size, STAR reduced parameter counts by up to 13% while still improving performance on standard benchmarks.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe research also highlighted STAR’s ability to scale its designs. A STAR-evolved model scaled from 125 million to 1 billion parameters delivered comparable or superior results to existing Transformer++ and hybrid models, all while significantly reducing inference cache requirements.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-re-architecting-ai-model-architecture\"\u003eRe-architecting AI model architecture\u003c/h2\u003e\n\n\n\n\u003cp\u003eLiquid AI stated that STAR is rooted in a design theory that incorporates principles from dynamical systems, signal processing, and numerical linear algebra. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis foundational approach has enabled the team to develop a versatile search space for computational units, encompassing components such as attention mechanisms, recurrences, and convolutions.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne of STAR’s distinguishing features is its modularity, allowing the framework to encode and optimize architectures across multiple hierarchical levels. This capability provides insights into recurring design motifs and enables researchers to identify effective combinations of architectural components.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-s-next-for-star\"\u003eWhat’s next for STAR?\u003c/h2\u003e\n\n\n\n\u003cp\u003eSTAR’s ability to synthesize efficient, high-performing architectures has potential applications far beyond language modeling. Liquid AI envisions this framework being used to tackle challenges in various domains where the trade-off between quality and computational efficiency is critical.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile Liquid AI has yet to disclose specific plans for commercial deployment or pricing, the research findings signal a significant advancement in the field of automated architecture design. For researchers and developers looking to optimize AI systems, STAR could represent a powerful tool for pushing the boundaries of model performance and efficiency.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith its open research approach, Liquid AI has published the \u003ca href=\"https://arxiv.org/pdf/2411.17800\" target=\"_blank\" rel=\"noreferrer noopener\"\u003efull details of STAR in a peer-reviewed paper\u003c/a\u003e, encouraging collaboration and further innovation. As the AI landscape continues to evolve, frameworks like STAR are poised to play a key role in shaping the next generation of intelligent systems. STAR might even herald the birth of a new post-Transformer architecture boom — a welcome winter holiday gift for the machine learning and AI research community. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-12-03T00:20:20Z",
  "modifiedTime": "2024-12-03T00:20:28Z"
}
