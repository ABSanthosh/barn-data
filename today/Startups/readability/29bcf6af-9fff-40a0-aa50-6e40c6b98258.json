{
  "id": "29bcf6af-9fff-40a0-aa50-6e40c6b98258",
  "title": "Anthropic unveils ‘auditing agents’ to test for AI misalignment",
  "link": "https://venturebeat.com/ai/anthropic-unveils-auditing-agents-to-test-for-ai-misalignment/",
  "description": "Anthropic developed its auditing agents while testing Claude Opus 4 for alignment issues.",
  "author": "Emilia David",
  "published": "Thu, 24 Jul 2025 22:15:53 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Agentic AI",
    "AI agent",
    "AI agents",
    "AI alignment",
    "ai alignment auditing",
    "AI, ML and Deep Learning",
    "alignment",
    "Anthropic",
    "claude 4 opus",
    "model alignment"
  ],
  "byline": "Emilia David",
  "length": 6853,
  "excerpt": "Anthropic developed its auditing agents while testing Claude Opus 4 for alignment issues.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "July 24, 2025 3:15 PM Image Credit: VentureBeat via ChatGPT Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now When models attempt to get their way or become overly accommodating to the user, it can mean trouble for enterprises. That is why it’s essential that, in addition to performance evaluations, organizations conduct alignment testing. However, alignment audits often present two major challenges: scalability and validation. Alignment testing requires a significant amount of time for human researchers, and it’s challenging to ensure that the audit has caught everything.  In a paper, Anthropic researchers said they developed auditing agents that achieved “impressive performance at auditing tasks, while also shedding light on their limitations.” The researchers stated that these agents, created during the pre-deployment testing of Claude Opus 4, enhanced alignment validation tests and enabled researchers to conduct multiple parallel audits at scale. Anthropic also released a replication of its audit agents on GitHub.  New Anthropic research: Building and evaluating alignment auditing agents.We developed three AI agents to autonomously complete alignment auditing tasks.In testing, our agents successfully uncovered hidden goals, built safety evaluations, and surfaced concerning behaviors. pic.twitter.com/HMQhMaA4v0— Anthropic (@AnthropicAI) July 24, 2025 “We introduce three agents that autonomously complete alignment auditing tasks. We also introduce three environments that formalize alignment auditing workflows as auditing games, and use them to evaluate our agents,” the researcher said in the paper.  The AI Impact Series Returns to San Francisco - August 5 The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation. Secure your spot now - space is limited: https://bit.ly/3GuuPLF The three agents they explored were: Tool-using investigator agent for open-ended investigation of models using chat, data analysis and interpretability tools Evaluation agent that builds behavioral evaluations and can discriminate between models that have implanted behaviors and those that do not Breadth-first red-teaming agent, which was developed specifically for the Claude 4 alignment assessment, so that it can discover implanted test behaviors “Overall, our agents demonstrate promise across multiple alignment auditing tasks. Additionally, our approach to validating agents with auditing games yields insights into agent capabilities, limitations, and key affordances. With further work, automated auditing could significantly help scale human oversight over AI systems,” the paper said. Auditing agents in action Anthropic said the first environment it developed tests an agent’s ability to complete an alignment audit for an intentionally misaligned model. For this test, the researchers brought the investigator agent to the environment and equipped it with similar tools that human teams use. The researchers instructed the agent to identify the root cause of the misalignment and determine the specific behaviors that were misaligned.  According to the researchers, the investigator agent successfully found the root cause of the issues 10-13% of the time. Still, this performance improved to 42% “when using a super-agent approach that aggregates findings across multiple investigations.” The next step in most alignment audits is typically a behavioral evaluation, which determines whether the model consistently misbehaves in various settings. To test its evaluation agent, the researchers designed 10 models “given system prompt instructions to exhibit a specific behavior, such as excessive deference to the user.” They ran the agent five times per model and saw that the agent correctly finds and flags at least one quirk of the model. However, the agent sometimes failed to identify specific quirks consistently. It had trouble evaluating subtle quirks, such as self-promotion and research-sandbagging, as well as quirks that are difficult to elicit, like the Hardcode Test Cases quirk. The last test and agent concern behavioral red-teaming to find the prompts that elicit “concerning” behaviors. The breadth-first red-teaming agent converses with the target model (in Anthropic’s case, it was with Claude Opus 4), and this chat is then rated for alignment-relevant properties.  The agent identified seven of the ten system quirks, but it also struggled with the same issues as the evaluator agent. Alignment and sycophany problems Alignment became a significant topic in the AI world after users noticed that ChatGPT was becoming overly agreeable. OpenAI rolled back some updates to GPT-4o to address this issue, but it showed that language models and agents can confidently give wrong answers if they decide this is what users want to hear.  To combat this, other methods and benchmarks were developed to curb unwanted behaviors. The Elephant benchmark, developed by researchers from Carnegie Mellon University, the University of Oxford, and Stanford University, aims to measure sycophancy. DarkBench categorizes six issues, such as brand bias, user retention, sycophancy, anthromorphism, harmful content generation, and sneaking. OpenAI also has a method where AI models test themselves for alignment.  Alignment auditing and evaluation continue to evolve, though it is not surprising that some people are not comfortable with it.  Hallucinations auditing HallucinationsGreat work team.— spec (@_opencv_) July 24, 2025 However, Anthropic said that, although these audit agents still need refinement, alignment must be done now.  “As AI systems become more powerful, we need scalable ways to assess their alignment. Human alignment audits take time and are hard to validate,” the company said in an X post.  As AI systems become more powerful, we need scalable ways to assess their alignment.Human alignment audits take time and are hard to validate. Our solution: automating alignment auditing with AI agents.Read more: https://t.co/CqWkQSfBIG— Anthropic (@AnthropicAI) July 24, 2025 Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/03/DALL·E-2025-03-11-09.55.49-A-sleek-minimalist-digital-illustration-representing-Anthropics-AI-coding-agent-Claude.-The-design-features-a-glowing-circuit-in-the-shape-of-a-sty.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-07-24T22:15:53+00:00\" datetime=\"2025-07-24T22:15:53+00:00\"\u003eJuly 24, 2025 3:15 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"429\" src=\"https://venturebeat.com/wp-content/uploads/2025/03/DALL·E-2025-03-11-09.55.49-A-sleek-minimalist-digital-illustration-representing-Anthropics-AI-coding-agent-Claude.-The-design-features-a-glowing-circuit-in-the-shape-of-a-sty.webp?w=750\" alt=\"Anthropic\u0026#39;s Claude is winning the coding agent war\"/\u003e\u003c/p\u003e\u003cdiv\u003e\u003cp\u003e\u003cem\u003eImage Credit: VentureBeat via ChatGPT\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\u003c/em\u003e \u003cem\u003e\u003ca href=\"https://venturebeat.com/newsletters/\"\u003eSubscribe Now\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eWhen models attempt to get their way or become overly accommodating to the user, it can mean trouble for enterprises. That is why it’s essential that, in addition to performance evaluations, \u003ca href=\"https://venturebeat.com/enterprise-analytics/data-everywhere-alignment-nowhere-what-dashboards-are-getting-wrong-and-why-you-need-a-data-product-manager/\"\u003eorganizations conduct alignment testing\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, alignment audits often present two major challenges: scalability and validation. Alignment testing requires a significant amount of time for human researchers, and it’s challenging to ensure that the audit has caught everything. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn \u003ca href=\"https://alignment.anthropic.com/2025/automated-auditing/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ea paper\u003c/a\u003e, \u003ca href=\"https://www.anthropic.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAnthropic\u003c/a\u003e researchers said they developed auditing agents that achieved “impressive performance at auditing tasks, while also shedding light on their limitations.” The researchers stated that these agents, created during the pre-deployment testing of Claude Opus 4, enhanced alignment validation tests and enabled researchers to conduct multiple parallel audits at scale. Anthropic also released a replication of its audit agents on \u003ca href=\"https://github.com/anthropic-experimental/automated-auditing/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGitHub\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cdiv\u003e\n\u003cblockquote data-width=\"500\" data-dnt=\"true\"\u003e\u003cdiv lang=\"en\" dir=\"ltr\"\u003e\u003cp\u003eNew Anthropic research: Building and evaluating alignment auditing agents.\u003c/p\u003e\u003cp\u003eWe developed three AI agents to autonomously complete alignment auditing tasks.\u003c/p\u003e\u003cp\u003eIn testing, our agents successfully uncovered hidden goals, built safety evaluations, and surfaced concerning behaviors. \u003ca href=\"https://t.co/HMQhMaA4v0\"\u003epic.twitter.com/HMQhMaA4v0\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e— Anthropic (@AnthropicAI) \u003ca href=\"https://twitter.com/AnthropicAI/status/1948433493102403876?ref_src=twsrc%5Etfw\"\u003eJuly 24, 2025\u003c/a\u003e\u003c/blockquote\u003e\n\u003c/div\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e“We introduce three agents that autonomously complete alignment auditing tasks. We also introduce three environments that formalize alignment auditing workflows as auditing games, and use them to evaluate our agents,” the researcher said in the paper. \u003c/p\u003e\n\n\n\n\u003cdiv id=\"boilerplate_2803147\"\u003e\n\u003chr/\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eThe AI Impact Series Returns to San Francisco - August 5\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\u003c/p\u003e\n\n\n\n\u003cp\u003eSecure your spot now - space is limited: \u003ca href=\"https://bit.ly/3GuuPLF\"\u003ehttps://bit.ly/3GuuPLF\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eThe three agents they explored were:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eTool-using investigator agent for open-ended investigation of models using chat, data analysis and interpretability tools\u003c/li\u003e\n\n\n\n\u003cli\u003eEvaluation agent that builds behavioral evaluations and can discriminate between models that have implanted behaviors and those that do not\u003c/li\u003e\n\n\n\n\u003cli\u003eBreadth-first red-teaming agent, which was developed specifically for the Claude 4 alignment assessment, so that it can discover implanted test behaviors\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003e“Overall, our agents demonstrate promise across multiple alignment auditing tasks. Additionally, our approach to validating agents with auditing games yields insights into agent capabilities, limitations, and key affordances. With further work, automated auditing could significantly help scale human oversight over AI systems,” the paper said.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-auditing-agents-in-action\"\u003eAuditing agents in action\u003c/h2\u003e\n\n\n\n\u003cp\u003eAnthropic said the first environment it developed tests an agent’s ability to complete an alignment audit for an intentionally misaligned model. For this test, the researchers brought the investigator agent to the environment and equipped it with similar tools that human teams use. The researchers instructed the agent to identify the root cause of the misalignment and determine the specific behaviors that were misaligned. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to the researchers, the investigator agent successfully found the root cause of the issues 10-13% of the time. Still, this performance improved to 42% “when using a super-agent approach that aggregates findings across multiple investigations.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe next step in most alignment audits is typically a behavioral evaluation, which determines whether the model consistently misbehaves in various settings. To test its evaluation agent, the researchers designed 10 models “given system prompt instructions to exhibit a specific behavior, such as excessive deference to the user.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThey ran the agent five times per model and saw that the agent correctly finds and flags at least one quirk of the model. However, the agent sometimes failed to identify specific quirks consistently. It had trouble evaluating subtle quirks, such as self-promotion and research-sandbagging, as well as quirks that are difficult to elicit, like the Hardcode Test Cases quirk.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcW1XtsOKBXA6qJ38w5DENejlWdBHmUizradX0vtmldDWP1XCvKhh5gBgHyTwG_BxHnruDUEbZ_DQk3YAWb-GXcS0biwy3emGjwmpXnyFCQByI9CHlvohfL83Xy0mCqKw9h-kn8Fg?key=08PpMIdS2W6zYhED38G-qQ\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe last test and agent concern behavioral red-teaming to find the prompts that elicit “concerning” behaviors. The breadth-first red-teaming agent converses with the target model (in Anthropic’s case, it was with Claude Opus 4), and this chat is then rated for alignment-relevant properties. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe agent identified seven of the ten system quirks, but it also struggled with the same issues as the evaluator agent.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-alignment-and-sycophany-problems\"\u003eAlignment and sycophany problems\u003c/h2\u003e\n\n\n\n\u003cp\u003eAlignment became a significant topic in the AI world after \u003ca href=\"https://venturebeat.com/ai/ex-openai-ceo-and-power-users-sound-alarm-over-ai-sycophancy-and-flattery-of-users/\"\u003eusers noticed that ChatGPT\u003c/a\u003e was becoming overly agreeable. \u003ca href=\"https://openai.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI\u003c/a\u003e \u003ca href=\"https://venturebeat.com/ai/openai-rolls-back-chatgpts-sycophancy-and-explains-what-went-wrong/\"\u003erolled back some updates\u003c/a\u003e to GPT-4o to address this issue, but it showed that language models and agents can confidently give wrong answers if they decide this is what users want to hear. \u003c/p\u003e\n\n\n\n\u003cp\u003eTo combat this, other methods and benchmarks were developed to curb unwanted behaviors. The \u003ca href=\"https://venturebeat.com/ai/after-gpt-4o-backlash-researchers-benchmark-models-on-moral-endorsement-find-sycophancy-persists-across-the-board/\"\u003eElephant benchmark\u003c/a\u003e, developed by researchers from Carnegie Mellon University, the University of Oxford, and Stanford University, aims to measure sycophancy. \u003ca href=\"https://openreview.net/forum?id=odjMSBSWRt\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDarkBench \u003c/a\u003ecategorizes \u003ca href=\"https://venturebeat.com/ai/darkness-rising-the-hidden-dangers-of-ai-sycophancy-and-dark-patterns/\"\u003esix issues\u003c/a\u003e, such as brand bias, user retention, sycophancy, anthromorphism, harmful content generation, and sneaking. OpenAI also has a method where AI models \u003ca href=\"https://venturebeat.com/ai/ai-models-rank-their-own-safety-in-openais-new-alignment-research/\"\u003etest themselves for alignment\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eAlignment auditing and evaluation continue to evolve, though it is not surprising that some people are not comfortable with it. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cdiv\u003e\n\u003cblockquote data-width=\"500\" data-dnt=\"true\"\u003e\u003cdiv lang=\"en\" dir=\"ltr\"\u003e\u003cp\u003eHallucinations auditing Hallucinations\u003c/p\u003e\u003cp\u003eGreat work team.\u003c/p\u003e\u003c/div\u003e— spec (@_opencv_) \u003ca href=\"https://twitter.com/_opencv_/status/1948434463228395623?ref_src=twsrc%5Etfw\"\u003eJuly 24, 2025\u003c/a\u003e\u003c/blockquote\u003e\n\u003c/div\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eHowever, Anthropic said that, although these audit agents still need refinement, alignment must be done now. \u003c/p\u003e\n\n\n\n\u003cp\u003e“As AI systems become more powerful, we need scalable ways to assess their alignment. Human alignment audits take time and are hard to validate,” the company said in an X post. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cdiv\u003e\n\u003cblockquote data-width=\"500\" data-dnt=\"true\"\u003e\u003cdiv lang=\"en\" dir=\"ltr\"\u003e\u003cp\u003eAs AI systems become more powerful, we need scalable ways to assess their alignment.\u003c/p\u003e\u003cp\u003eHuman alignment audits take time and are hard to validate. \u003c/p\u003e\u003cp\u003eOur solution: automating alignment auditing with AI agents.\u003c/p\u003e\u003cp\u003eRead more: \u003ca href=\"https://t.co/CqWkQSfBIG\"\u003ehttps://t.co/CqWkQSfBIG\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e— Anthropic (@AnthropicAI) \u003ca href=\"https://twitter.com/AnthropicAI/status/1948433497187611022?ref_src=twsrc%5Etfw\"\u003eJuly 24, 2025\u003c/a\u003e\u003c/blockquote\u003e\n\u003c/div\u003e\u003c/figure\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-07-24T22:15:53Z",
  "modifiedTime": "2025-07-24T23:16:07Z"
}
