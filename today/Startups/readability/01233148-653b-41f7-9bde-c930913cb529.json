{
  "id": "01233148-653b-41f7-9bde-c930913cb529",
  "title": "New 1.5B router model achieves 93% accuracy without costly retraining",
  "link": "https://venturebeat.com/ai/new-1-5b-router-model-achieves-93-accuracy-without-costly-retraining/",
  "description": "Katanemo Labs' new LLM routing framework aligns with human preferences and adapts to new models without retraining.",
  "author": "Ben Dickson",
  "published": "Mon, 07 Jul 2025 23:25:31 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI research",
    "AI, ML and Deep Learning",
    "Anthropic",
    "Arch-Router",
    "Google",
    "in-context learning (ICL)",
    "Katanemo Labs",
    "large language models",
    "large language models (LLMs)",
    "LLM router",
    "LLMs",
    "OpenAI",
    "Qwen 2.5",
    "research"
  ],
  "byline": "Ben Dickson",
  "length": 7730,
  "excerpt": "Katanemo Labs' new LLM routing framework aligns with human preferences and adapts to new models without retraining.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "July 7, 2025 4:25 PM Image credit: VentureBeat with ChatGPT Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now Researchers at Katanemo Labs have introduced Arch-Router, a new routing model and framework designed to intelligently map user queries to the most suitable large language model (LLM).  For enterprises building products that rely on multiple LLMs, Arch-Router aims to solve a key challenge: how to direct queries to the best model for the job without relying on rigid logic or costly retraining every time something changes. The challenges of LLM routing As the number of LLMs grows, developers are moving from single-model setups to multi-model systems that use the unique strengths of each model for specific tasks (e.g., code generation, text summarization, or image editing).  LLM routing has emerged as a key technique for building and deploying these systems, acting as a traffic controller that directs each user query to the most appropriate model. Existing routing methods generally fall into two categories: “task-based routing,” where queries are routed based on predefined tasks, and “performance-based routing,” which seeks an optimal balance between cost and performance. However, task-based routing struggles with unclear or shifting user intentions, particularly in multi-turn conversations. Performance-based routing, on the other hand, rigidly prioritizes benchmark scores, often neglects real-world user preferences and adapts poorly to new models unless it undergoes costly fine-tuning. More fundamentally, as the Katanemo Labs researchers note in their paper, “existing routing approaches have limitations in real-world use. They typically optimize for benchmark performance while neglecting human preferences driven by subjective evaluation criteria.”  The researchers highlight the need for routing systems that “align with subjective human preferences, offer more transparency, and remain easily adaptable as models and use cases evolve.” A new framework for preference-aligned routing To address these limitations, the researchers propose a “preference-aligned routing” framework that matches queries to routing policies based on user-defined preferences. In this framework, users define their routing policies in natural language using a “Domain-Action Taxonomy.” This is a two-level hierarchy that reflects how people naturally describe tasks, starting with a general topic (the Domain, such as “legal” or “finance”) and narrowing to a specific task (the Action, such as “summarization” or “code generation”).  Each of these policies is then linked to a preferred model, allowing developers to make routing decisions based on real-world needs rather than just benchmark scores. As the paper states, “This taxonomy serves as a mental model to help users define clear and structured routing policies.” The routing process happens in two stages. First, a preference-aligned router model takes the user query and the full set of policies and selects the most appropriate policy. Second, a mapping function connects that selected policy to its designated LLM.  Because the model selection logic is separated from the policy, models can be added, removed, or swapped simply by editing the routing policies, without any need to retrain or modify the router itself. This decoupling provides the flexibility required for practical deployments, where models and use cases are constantly evolving. Preference-aligned routing framework Source: arXiv The policy selection is powered by Arch-Router, a compact 1.5B parameter language model fine-tuned for preference-aligned routing. Arch-Router receives the user query and the complete set of policy descriptions within its prompt. It then generates the identifier of the best-matching policy.  Since the policies are part of the input, the system can adapt to new or modified routes at inference time through in-context learning and without retraining. This generative approach allows Arch-Router to use its pre-trained knowledge to understand the semantics of both the query and the policies, and to process the entire conversation history at once. A common concern with including extensive policies in a prompt is the potential for increased latency. However, the researchers designed Arch-Router to be highly efficient. “While the length of routing policies can get long, we can easily increase the context window of Arch-Router with minimal impact on latency,” explains Salman Paracha, co-author of the paper and Founder/CEO of Katanemo Labs. He notes that latency is primarily driven by the length of the output, and for Arch-Router, the output is simply the short name of a routing policy, like “image_editing” or “document_creation.” Arch-Router in action To build Arch-Router, the researchers fine-tuned a 1.5B parameter version of the Qwen 2.5 model on a curated dataset of 43,000 examples. They then tested its performance against state-of-the-art proprietary models from OpenAI, Anthropic and Google on four public datasets designed to evaluate conversational AI systems. The results show that Arch-Router achieves the highest overall routing score of 93.17%, surpassing all other models, including top proprietary ones, by an average of 7.71%. The model’s advantage grew with longer conversations, demonstrating its strong ability to track context over multiple turns.  Arch-Router vs other models Source: arXiv In practice, this approach is already being applied in several scenarios, according to Paracha. For example, in open-source coding tools, developers use Arch-Router to direct different stages of their workflow, such as “code design,” “code understanding,” and “code generation,” to the LLMs best suited for each task. Similarly, enterprises can route document creation requests to a model like Claude 3.7 Sonnet while sending image editing tasks to Gemini 2.5 Pro.  The system is also ideal “for personal assistants in various domains, where users have a diversity of tasks from text summarization to factoid queries,” Paracha said, adding that “in those cases, Arch-Router can help developers unify and improve the overall user experience.” This framework is integrated with Arch, Katanemo Labs’ AI-native proxy server for agents, which allows developers to implement sophisticated traffic-shaping rules. For instance, when integrating a new LLM, a team can send a small portion of traffic for a specific routing policy to the new model, verify its performance with internal metrics, and then fully transition traffic with confidence. The company is also working to integrate its tools with evaluation platforms to streamline this process for enterprise developers further. Ultimately, the goal is to move beyond siloed AI implementations. “Arch-Router—and Arch more broadly—helps developers and enterprises move from fragmented LLM implementations to a unified, policy-driven system,” says Paracha. “In scenarios where user tasks are diverse, our framework helps turn that task and LLM fragmentation into a unified experience, making the final product feel seamless to the end user.” Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/07/ChatGPT-Image-Jul-2-2025-09_19_17-AM.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-07-07T23:25:31+00:00\" datetime=\"2025-07-07T23:25:31+00:00\"\u003eJuly 7, 2025 4:25 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"500\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/ChatGPT-Image-Jul-2-2025-09_19_17-AM.png?w=750\" alt=\"Image credit: VentureBeat with ChatGPT\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with ChatGPT\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\u003c/em\u003e \u003cem\u003e\u003ca href=\"https://venturebeat.com/newsletters/\"\u003eSubscribe Now\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eResearchers at \u003ca href=\"https://www.archgw.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eKatanemo Labs \u003c/a\u003ehave introduced \u003ca href=\"https://huggingface.co/katanemo/Arch-Router-1.5B\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eArch-Router\u003c/a\u003e, a new routing model and framework designed to intelligently map user queries to the most suitable large language model (LLM). \u003c/p\u003e\n\n\n\n\u003cp\u003eFor enterprises building products that rely on multiple LLMs, Arch-Router aims to solve a key challenge: how to direct queries to the best model for the job without relying on rigid logic or costly retraining every time something changes.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-challenges-of-llm-routing\"\u003eThe challenges of LLM routing\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs the number of LLMs grows, developers are moving from single-model setups to multi-model systems that use the unique strengths of each model for specific tasks (e.g., code generation, text summarization, or image editing). \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://venturebeat.com/ai/why-accenture-and-martian-see-model-routing-as-key-to-enterprise-ai-success/\"\u003eLLM routing\u003c/a\u003e has emerged as a key technique for building and deploying these systems, acting as a traffic controller that directs each user query to the most appropriate model.\u003c/p\u003e\n\n\n\n\u003cp\u003eExisting routing methods generally fall into two categories: “task-based routing,” where queries are routed based on predefined tasks, and “performance-based routing,” which seeks an optimal balance between cost and performance.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, task-based routing struggles with unclear or shifting user intentions, particularly in multi-turn conversations. Performance-based routing, on the other hand, rigidly prioritizes benchmark scores, often neglects real-world user preferences and adapts poorly to new models unless it undergoes costly fine-tuning.\u003c/p\u003e\n\n\n\n\u003cp\u003eMore fundamentally, as the Katanemo Labs researchers note in their \u003ca href=\"https://arxiv.org/abs/2506.16655\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epaper\u003c/a\u003e, “existing routing approaches have limitations in real-world use. They typically optimize for benchmark performance while neglecting human preferences driven by subjective evaluation criteria.” \u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers highlight the need for routing systems that “align with subjective human preferences, offer more transparency, and remain easily adaptable as models and use cases evolve.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-a-new-framework-for-preference-aligned-routing\"\u003eA new framework for preference-aligned routing\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo address these limitations, the researchers propose a “preference-aligned routing” framework that matches queries to routing policies based on user-defined preferences.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this framework, users define their routing policies in natural language using a “Domain-Action Taxonomy.” This is a two-level hierarchy that reflects how people naturally describe tasks, starting with a general topic (the Domain, such as “legal” or “finance”) and narrowing to a specific task (the Action, such as “summarization” or “code generation”). \u003c/p\u003e\n\n\n\n\u003cp\u003eEach of these policies is then linked to a preferred model, allowing developers to make routing decisions based on real-world needs rather than just benchmark scores. As the paper states, “This taxonomy serves as a mental model to help users define clear and structured routing policies.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe routing process happens in two stages. First, a preference-aligned router model takes the user query and the full set of policies and selects the most appropriate policy. Second, a mapping function connects that selected policy to its designated LLM. \u003c/p\u003e\n\n\n\n\u003cp\u003eBecause the model selection logic is separated from the policy, models can be added, removed, or swapped simply by editing the routing policies, without any need to retrain or modify the router itself. This decoupling provides the flexibility required for practical deployments, where models and use cases are constantly evolving.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" height=\"269\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image.png?w=800\" alt=\"Preference-aligned routing framework (source: arXiv)\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image.png 1600w, https://venturebeat.com/wp-content/uploads/2025/07/image.png?resize=300,101 300w, https://venturebeat.com/wp-content/uploads/2025/07/image.png?resize=768,258 768w, https://venturebeat.com/wp-content/uploads/2025/07/image.png?resize=800,269 800w, https://venturebeat.com/wp-content/uploads/2025/07/image.png?resize=1536,516 1536w, https://venturebeat.com/wp-content/uploads/2025/07/image.png?resize=400,135 400w, https://venturebeat.com/wp-content/uploads/2025/07/image.png?resize=750,252 750w, https://venturebeat.com/wp-content/uploads/2025/07/image.png?resize=578,194 578w, https://venturebeat.com/wp-content/uploads/2025/07/image.png?resize=930,313 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003cfigcaption\u003e\u003cem\u003ePreference-aligned routing framework Source: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe policy selection is powered by Arch-Router, a compact 1.5B parameter language model fine-tuned for preference-aligned routing. Arch-Router receives the user query and the complete set of policy descriptions within its prompt. It then generates the identifier of the best-matching policy. \u003c/p\u003e\n\n\n\n\u003cp\u003eSince the policies are part of the input, the system can adapt to new or modified routes at inference time through \u003ca href=\"https://venturebeat.com/ai/fine-tuning-vs-in-context-learning-new-research-guides-better-llm-customization-for-real-world-tasks/\"\u003ein-context learning\u003c/a\u003e and without retraining. This generative approach allows Arch-Router to use its pre-trained knowledge to understand the semantics of both the query and the policies, and to process the entire conversation history at once.\u003c/p\u003e\n\n\n\n\u003cp\u003eA common concern with including extensive policies in a prompt is the potential for increased latency. However, the researchers designed Arch-Router to be highly efficient. “While the length of routing policies can get long, we can easily increase the context window of Arch-Router with minimal impact on latency,” explains Salman Paracha, co-author of the paper and Founder/CEO of Katanemo Labs. He notes that latency is primarily driven by the length of the output, and for Arch-Router, the output is simply the short name of a routing policy, like “image_editing” or “document_creation.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-arch-router-in-action\"\u003eArch-Router in action\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo build Arch-Router, the researchers fine-tuned a 1.5B parameter version of the \u003ca href=\"https://venturebeat.com/ai/alibaba-new-ai-can-code-in-92-languages-and-its-completely-free/\"\u003eQwen 2.5 model\u003c/a\u003e on a curated dataset of 43,000 examples. They then tested its performance against state-of-the-art proprietary models from OpenAI, Anthropic and Google on four public datasets designed to evaluate conversational AI systems.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe results show that Arch-Router achieves the highest overall routing score of 93.17%, surpassing all other models, including top proprietary ones, by an average of 7.71%. The model’s advantage grew with longer conversations, demonstrating its strong ability to track context over multiple turns. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" height=\"410\" width=\"800\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/image_88212c.png?w=800\" alt=\"Arch-Router vs other models (source: arXiv)\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/07/image_88212c.png 1094w, https://venturebeat.com/wp-content/uploads/2025/07/image_88212c.png?resize=300,154 300w, https://venturebeat.com/wp-content/uploads/2025/07/image_88212c.png?resize=768,393 768w, https://venturebeat.com/wp-content/uploads/2025/07/image_88212c.png?resize=800,410 800w, https://venturebeat.com/wp-content/uploads/2025/07/image_88212c.png?resize=100,50 100w, https://venturebeat.com/wp-content/uploads/2025/07/image_88212c.png?resize=400,205 400w, https://venturebeat.com/wp-content/uploads/2025/07/image_88212c.png?resize=750,384 750w, https://venturebeat.com/wp-content/uploads/2025/07/image_88212c.png?resize=578,296 578w, https://venturebeat.com/wp-content/uploads/2025/07/image_88212c.png?resize=930,476 930w\" sizes=\"(max-width: 800px) 100vw, 800px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eArch-Router vs other models Source: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIn practice, this approach is already being applied in several scenarios, according to Paracha. For example, in open-source coding tools, developers use Arch-Router to direct different stages of their workflow, such as “code design,” “code understanding,” and “code generation,” to the LLMs best suited for each task. Similarly, enterprises can route document creation requests to a model like \u003ca href=\"https://venturebeat.com/ai/anthropics-claude-3-7-sonnet-takes-aim-at-openai-and-deepseek-in-ais-next-big-battle/\"\u003eClaude 3.7 Sonnet\u003c/a\u003e while sending image editing tasks to \u003ca href=\"https://venturebeat.com/ai/meet-the-new-king-of-ai-coding-googles-gemini-2-5-pro-i-o-edition-dethrones-claude-3-7-sonnet/\"\u003eGemini 2.5 Pro\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe system is also ideal “for personal assistants in various domains, where users have a diversity of tasks from text summarization to factoid queries,” Paracha said, adding that “in those cases, Arch-Router can help developers unify and improve the overall user experience.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThis framework is integrated with \u003ca href=\"https://github.com/katanemo/archgw\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eArch\u003c/a\u003e, Katanemo Labs’ AI-native proxy server for agents, which allows developers to implement sophisticated traffic-shaping rules. For instance, when integrating a new LLM, a team can send a small portion of traffic for a specific routing policy to the new model, verify its performance with internal metrics, and then fully transition traffic with confidence. The company is also working to integrate its tools with evaluation platforms to streamline this process for enterprise developers further.\u003c/p\u003e\n\n\n\n\u003cp\u003eUltimately, the goal is to move beyond siloed AI implementations. “Arch-Router—and Arch more broadly—helps developers and enterprises move from fragmented LLM implementations to a unified, policy-driven system,” says Paracha. “In scenarios where user tasks are diverse, our framework helps turn that task and LLM fragmentation into a unified experience, making the final product feel seamless to the end user.”\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-07-07T23:25:31Z",
  "modifiedTime": "2025-07-07T23:25:37Z"
}
