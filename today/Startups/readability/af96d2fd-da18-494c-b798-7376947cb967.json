{
  "id": "af96d2fd-da18-494c-b798-7376947cb967",
  "title": "InstantStyle: Free Lunch Towards Style-Preserving in Text-to-Image Generation",
  "link": "https://github.com/instantX-research/InstantStyle",
  "description": "Article URL: https://github.com/instantX-research/InstantStyle Comments URL: https://news.ycombinator.com/item?id=43286091 Points: 10 # Comments: 2",
  "author": "klaussilveira",
  "published": "Thu, 06 Mar 2025 23:31:13 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "instantX-research",
  "length": 11967,
  "excerpt": "InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation ðŸ”¥ - instantX-research/InstantStyle",
  "siteName": "GitHub",
  "favicon": "https://github.com/fluidicon.png",
  "text": "InstantStyle is a general framework that employs two straightforward yet potent techniques for achieving an effective disentanglement of style and content from reference images. Principle Separating Content from Image. Benefit from the good characterization of CLIP global features, after subtracting the content text fea- tures from the image features, the style and content can be explicitly decoupled. Although simple, this strategy is quite effective in mitigating content leakage. Injecting into Style Blocks Only. Empirically, each layer of a deep network captures different semantic information the key observation in our work is that there exists two specific attention layers handling style. Specifically, we find up blocks.0.attentions.1 and down blocks.2.attentions.1 capture style (color, material, atmosphere) and spatial layout (structure, composition) respectively. Release [2024/07/06] ðŸ”¥ We release CSGO page for content-style composition. Code will be released soon. [2024/07/01] ðŸ”¥ We release InstantStyle-Plus report for content preserving. [2024/04/29] ðŸ”¥ We support InstantStyle natively in diffusers, usage can be found here [2024/04/24] ðŸ”¥ InstantStyle for fast generation, find demos at InstantStyle-SDXL-Lightning and InstantStyle-Hyper-SDXL. [2024/04/24] ðŸ”¥ We support HiDiffusion for generating highres images, find more information here. [2024/04/23] ðŸ”¥ InstantStyle has been natively supported in diffusers, more information can be found here. [2024/04/20] ðŸ”¥ InstantStyle is supported in Mikubill/sd-webui-controlnet. [2024/04/11] ðŸ”¥ We add the experimental distributed inference feature. Check it here. [2024/04/10] ðŸ”¥ We support an online demo on ModelScope. [2024/04/09] ðŸ”¥ We support an online demo on Huggingface. [2024/04/09] ðŸ”¥ We support SDXL-inpainting, more information can be found here. [2024/04/08] ðŸ”¥ InstantStyle is supported in AnyV2V for stylized video-to-video editing, demo can be found here. [2024/04/07] ðŸ”¥ We support image-based stylization, more information can be found here. [2024/04/07] ðŸ”¥ We support an experimental version for SD1.5, more information can be found here. [2024/04/03] ðŸ”¥ InstantStyle is supported in ComfyUI_IPAdapter_plus developed by our co-author. [2024/04/03] ðŸ”¥ We release the technical report. Demos Stylized Synthesis Image-based Stylized Synthesis Comparison with Previous Works Download Follow IP-Adapter to download pre-trained checkpoints from here. git clone https://github.com/InstantStyle/InstantStyle.git cd InstantStyle # download the models git lfs install git clone https://huggingface.co/h94/IP-Adapter mv IP-Adapter/models models mv IP-Adapter/sdxl_models sdxl_models Usage Our method is fully compatible with IP-Adapter. For feature subtraction, it only works for global feature instead of patch features. For SD1.5, you can find a demo at infer_style_sd15.py, but we find that SD1.5 has weaker perception and understanding of style information, thus this demo is experimental only. All block names can be found in attn_blocks.py and attn_blocks_sd15.py for SDXL and SD1.5 respectively. import torch from diffusers import StableDiffusionXLPipeline from PIL import Image from ip_adapter import IPAdapterXL base_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\" image_encoder_path = \"sdxl_models/image_encoder\" ip_ckpt = \"sdxl_models/ip-adapter_sdxl.bin\" device = \"cuda\" # load SDXL pipeline pipe = StableDiffusionXLPipeline.from_pretrained( base_model_path, torch_dtype=torch.float16, add_watermarker=False, ) # reduce memory consumption pipe.enable_vae_tiling() # load ip-adapter # target_blocks=[\"block\"] for original IP-Adapter # target_blocks=[\"up_blocks.0.attentions.1\"] for style blocks only # target_blocks = [\"up_blocks.0.attentions.1\", \"down_blocks.2.attentions.1\"] # for style+layout blocks ip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device, target_blocks=[\"up_blocks.0.attentions.1\"]) image = \"./assets/0.jpg\" image = Image.open(image) image.resize((512, 512)) # generate image variations with only image prompt images = ip_model.generate(pil_image=image, prompt=\"a cat, masterpiece, best quality, high quality\", negative_prompt= \"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\", scale=1.0, guidance_scale=5, num_samples=1, num_inference_steps=30, seed=42, #neg_content_prompt=\"a rabbit\", #neg_content_scale=0.5, ) images[0].save(\"result.png\") Use in diffusers InstantStyle has already been integrated into diffusers (please make sure that you have installed diffusers\u003e=0.28.0.dev0), making the usage significantly simpler. You can now control the per-transformer behavior of each IP-Adapter with the set_ip_adapter_scale() method, using a configuration dictionary as shown below: from diffusers import StableDiffusionXLPipeline from PIL import Image import torch # load SDXL pipeline pipe = StableDiffusionXLPipeline.from_pretrained( \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, add_watermarker=False, ) # load ip-adapter pipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\") pipe.enable_vae_tiling() # configure ip-adapter scales. scale = { \"down\": {\"block_2\": [0.0, 1.0]}, \"up\": {\"block_0\": [0.0, 1.0, 0.0]}, } pipeline.set_ip_adapter_scale(scale) In this example. We set scale=1.0 for IP-Adapter in the second transformer of down-part, block 2, and the second in up-part, block 0. Note that there are 2 transformers in down-part block 2 so the list is of length 2, and so do the up-part block 0. The rest IP-Adapter will have a zero scale which means disable them in all the other layers. With the help of set_ip_adapter_scale(), we can now configure IP-Adapters without a need of reloading them everytime we want to test the IP-Adapter behaviors. # for original IP-Adapter scale = 1.0 pipeline.set_ip_adapter_scale(scale) # for style blocks only scale = { \"up\": {\"block_0\": [0.0, 1.0, 0.0]}, } pipeline.set_ip_adapter_scale(scale) Multiple IP-Adapter images with masks You can also load multiple IP-Adapters, together with multiple IP-Adapter images with masks for more precisely layout control just as that in IP-Adapter do. from diffusers import StableDiffusionXLPipeline from diffusers.image_processor import IPAdapterMaskProcessor from transformers import CLIPVisionModelWithProjection from PIL import Image import torch image_encoder = CLIPVisionModelWithProjection.from_pretrained( \"h94/IP-Adapter\", subfolder=\"models/image_encoder\", torch_dtype=torch.float16 ).to(\"cuda\") pipe = StableDiffusionXLPipeline.from_pretrained( \"RunDiffusion/Juggernaut-XL-v9\", torch_dtype=torch.float16, image_encoder=image_encoder, variant=\"fp16\" ).to(\"cuda\") pipe.load_ip_adapter( [\"ostris/ip-composition-adapter\", \"h94/IP-Adapter\"], subfolder=[\"\", \"sdxl_models\"], weight_name=[ \"ip_plus_composition_sdxl.safetensors\", \"ip-adapter_sdxl_vit-h.safetensors\", ], image_encoder_folder=None, ) scale_1 = { \"down\": [[0.0, 0.0, 1.0]], \"mid\": [[0.0, 0.0, 1.0]], \"up\": {\"block_0\": [[0.0, 0.0, 1.0], [1.0, 1.0, 1.0], [0.0, 0.0, 1.0]], \"block_1\": [[0.0, 0.0, 1.0]]}, } # activate the first IP-Adapter in everywhere in the model, # configure the second one for precise style control to each masked input. pipe.set_ip_adapter_scale([1.0, scale_1]) processor = IPAdapterMaskProcessor() female_mask = Image.open(\"./assets/female_mask.png\") male_mask = Image.open(\"./assets/male_mask.png\") background_mask = Image.open(\"./assets/background_mask.png\") composition_mask = Image.open(\"./assets/composition_mask.png\") mask1 = processor.preprocess([composition_mask], height=1024, width=1024) mask2 = processor.preprocess([female_mask, male_mask, background_mask], height=1024, width=1024) mask2 = mask2.reshape(1, mask2.shape[0], mask2.shape[2], mask2.shape[3]) # output -\u003e (1, 3, 1024, 1024) ip_female_style = Image.open(\"./assets/ip_female_style.png\") ip_male_style = Image.open(\"./assets/ip_male_style.png\") ip_background = Image.open(\"./assets/ip_background.png\") ip_composition_image = Image.open(\"./assets/ip_composition_image.png\") image = pipe( prompt=\"high quality, cinematic photo, cinemascope, 35mm, film grain, highly detailed\", negative_prompt=\"\", ip_adapter_image=[ip_composition_image, [ip_female_style, ip_male_style, ip_background]], cross_attention_kwargs={\"ip_adapter_masks\": [mask1, mask2]}, guidance_scale=6.5, num_inference_steps=25, ).images[0] image High Resolution Generation We employ HiDiffusion to seamlessly generate high-resolution images, you can install via pip install hidiffusion. from hidiffusion import apply_hidiffusion, remove_hidiffusion # reduce memory consumption pipe.enable_vae_tiling() # apply hidiffusion with a single line of code. apply_hidiffusion(pipe) ... # generate image at higher resolution images = ip_model.generate(pil_image=image, prompt=\"a cat, masterpiece, best quality, high quality\", negative_prompt= \"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\", scale=1.0, guidance_scale=5, num_samples=1, num_inference_steps=30, seed=42, height=2048, width=2048 ) Distributed Inference On distributed setups, you can run inference across multiple GPUs with ðŸ¤— Accelerate or PyTorch Distributed, which is useful for generating with multiple prompts in parallel, in case you have limited VRAM on each GPU. More information can be found here. Make sure you have installed diffusers from the source and the lastest accelerate. max_memory = {0:\"10GB\", 1:\"10GB\"} pipe = StableDiffusionXLPipeline.from_pretrained( base_model_path, torch_dtype=torch.float16, add_watermarker=False, device_map=\"balanced\", max_memory=max_memory ) Start a local gradio demo Run the following command: git clone https://github.com/InstantStyle/InstantStyle.git cd ./InstantStyle/gradio_demo/ pip install -r requirements.txt python app.py Resources InstantStyle for WebUI InstantStyle for ComfyUI InstantID Disclaimer The pretrained checkpoints follow the license in IP-Adapter. Users are granted the freedom to create images using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users. Acknowledgements InstantStyle is developed by the InstantX team and is highly built on IP-Adapter, which has been unfairly compared by many other works. We at InstantStyle make IP-Adapter great again. Additionally, we acknowledge Hu Ye for his valuable discussion. Star History Cite If you find InstantStyle useful for your research and applications, please cite us using this BibTeX: @article{wang2024instantstyle, title={InstantStyle-Plus: Style Transfer with Content-Preserving in Text-to-Image Generation}, author={Wang, Haofan and Xing, Peng and Huang, Renyuan and Ai, Hao and Wang, Qixun and Bai, Xu}, journal={arXiv preprint arXiv:2407.00788}, year={2024} } @article{wang2024instantstyle, title={InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation}, author={Wang, Haofan and Wang, Qixun and Bai, Xu and Qin, Zekui and Chen, Anthony}, journal={arXiv preprint arXiv:2404.02733}, year={2024} } For any question, feel free to contact us via haofanwang.ai@gmail.com.",
  "image": "https://opengraph.githubassets.com/b678a98294bdcd2f19b6b63963dd5fdab67fd536bc2f9b6fc46e5baa89d4ea16/instantX-research/InstantStyle",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-hpc=\"true\"\u003e\u003carticle itemprop=\"text\"\u003e\n\u003cp dir=\"auto\"\u003eInstantStyle is a general framework that employs two straightforward yet potent techniques for achieving an effective disentanglement of style and content from reference images.\u003c/p\u003e\n\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/instantX-research/InstantStyle/blob/main/assets/page0.png\"\u003e\u003cimg src=\"https://github.com/instantX-research/InstantStyle/raw/main/assets/page0.png\" width=\"900\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003ePrinciple\u003c/h2\u003e\u003ca id=\"user-content-principle\" aria-label=\"Permalink: Principle\" href=\"#principle\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eSeparating Content from Image. Benefit from the good characterization of CLIP global features, after subtracting the content text fea- tures from the image features, the style and content can be explicitly decoupled. Although simple, this strategy is quite effective in mitigating content leakage.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/instantX-research/InstantStyle/blob/main/assets/subtraction.png\"\u003e\u003cimg src=\"https://github.com/instantX-research/InstantStyle/raw/main/assets/subtraction.png\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eInjecting into Style Blocks Only. Empirically, each layer of a deep network captures different semantic information the key observation in our work is that there exists two specific attention layers handling style. Specifically, we find up blocks.0.attentions.1 and down blocks.2.attentions.1 capture style (color, material, atmosphere) and spatial layout (structure, composition) respectively.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/instantX-research/InstantStyle/blob/main/assets/tree.png\"\u003e\u003cimg src=\"https://github.com/instantX-research/InstantStyle/raw/main/assets/tree.png\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eRelease\u003c/h2\u003e\u003ca id=\"user-content-release\" aria-label=\"Permalink: Release\" href=\"#release\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e[2024/07/06] ðŸ”¥ We release \u003ca href=\"https://github.com/instantX-research/CSGO\"\u003eCSGO\u003c/a\u003e page for content-style composition. Code will be released soon.\u003c/li\u003e\n\u003cli\u003e[2024/07/01] ðŸ”¥ We release \u003ca href=\"https://instantstyle-plus.github.io/\" rel=\"nofollow\"\u003eInstantStyle-Plus\u003c/a\u003e report for content preserving.\u003c/li\u003e\n\u003cli\u003e[2024/04/29] ðŸ”¥ We support InstantStyle natively in diffusers, usage can be found \u003ca href=\"https://github.com/InstantStyle/InstantStyle?tab=readme-ov-file#use-in-diffusers\"\u003ehere\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e[2024/04/24] ðŸ”¥ InstantStyle for fast generation, find demos at \u003ca href=\"https://huggingface.co/spaces/radames/InstantStyle-SDXL-Lightning\" rel=\"nofollow\"\u003eInstantStyle-SDXL-Lightning\u003c/a\u003e and \u003ca href=\"https://huggingface.co/spaces/radames/InstantStyle-Hyper-SDXL\" rel=\"nofollow\"\u003eInstantStyle-Hyper-SDXL\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e[2024/04/24] ðŸ”¥ We support \u003ca href=\"https://github.com/megvii-research/HiDiffusion\"\u003eHiDiffusion\u003c/a\u003e for generating highres images, find more information \u003ca href=\"https://github.com/InstantStyle/InstantStyle/tree/main?tab=readme-ov-file#high-resolution-generation\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e[2024/04/23] ðŸ”¥ InstantStyle has been natively supported in diffusers, more information can be found \u003ca href=\"https://github.com/huggingface/diffusers/pull/7668\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/huggingface/diffusers/pull/7668/hovercard\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e[2024/04/20] ðŸ”¥ InstantStyle is supported in \u003ca href=\"https://github.com/Mikubill/sd-webui-controlnet/discussions/2770\" data-hovercard-type=\"discussion\" data-hovercard-url=\"/Mikubill/sd-webui-controlnet/discussions/2770/hovercard\"\u003eMikubill/sd-webui-controlnet\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e[2024/04/11] ðŸ”¥ We add the experimental distributed inference feature. Check it \u003ca href=\"https://github.com/InstantStyle/InstantStyle?tab=readme-ov-file#distributed-inference\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e[2024/04/10] ðŸ”¥ We support an \u003ca href=\"https://modelscope.cn/studios/instantx/InstantStyle/summary\" rel=\"nofollow\"\u003eonline demo\u003c/a\u003e on ModelScope.\u003c/li\u003e\n\u003cli\u003e[2024/04/09] ðŸ”¥ We support an \u003ca href=\"https://huggingface.co/spaces/InstantX/InstantStyle\" rel=\"nofollow\"\u003eonline demo\u003c/a\u003e on Huggingface.\u003c/li\u003e\n\u003cli\u003e[2024/04/09] ðŸ”¥ We support SDXL-inpainting, more information can be found \u003ca href=\"https://github.com/InstantStyle/InstantStyle/blob/main/infer_style_inpainting.py\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e[2024/04/08] ðŸ”¥ InstantStyle is supported in \u003ca href=\"https://tiger-ai-lab.github.io/AnyV2V/\" rel=\"nofollow\"\u003eAnyV2V\u003c/a\u003e for stylized video-to-video editing, demo can be found \u003ca href=\"https://twitter.com/vinesmsuic/status/1777170927500787782\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e[2024/04/07] ðŸ”¥ We support image-based stylization, more information can be found \u003ca href=\"https://github.com/InstantStyle/InstantStyle/blob/main/infer_style_controlnet.py\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e[2024/04/07] ðŸ”¥ We support an experimental version for SD1.5, more information can be found \u003ca href=\"https://github.com/InstantStyle/InstantStyle/blob/main/infer_style_sd15.py\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e[2024/04/03] ðŸ”¥ InstantStyle is supported in \u003ca href=\"https://github.com/cubiq/ComfyUI_IPAdapter_plus\"\u003eComfyUI_IPAdapter_plus\u003c/a\u003e developed by our co-author.\u003c/li\u003e\n\u003cli\u003e[2024/04/03] ðŸ”¥ We release the \u003ca href=\"https://arxiv.org/abs/2404.02733\" rel=\"nofollow\"\u003etechnical report\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eDemos\u003c/h2\u003e\u003ca id=\"user-content-demos\" aria-label=\"Permalink: Demos\" href=\"#demos\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eStylized Synthesis\u003c/h3\u003e\u003ca id=\"user-content-stylized-synthesis\" aria-label=\"Permalink: Stylized Synthesis\" href=\"#stylized-synthesis\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/instantX-research/InstantStyle/blob/main/assets/example1.png\"\u003e\u003cimg src=\"https://github.com/instantX-research/InstantStyle/raw/main/assets/example1.png\"/\u003e\u003c/a\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/instantX-research/InstantStyle/blob/main/assets/example2.png\"\u003e\u003cimg src=\"https://github.com/instantX-research/InstantStyle/raw/main/assets/example2.png\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eImage-based Stylized Synthesis\u003c/h3\u003e\u003ca id=\"user-content-image-based-stylized-synthesis\" aria-label=\"Permalink: Image-based Stylized Synthesis\" href=\"#image-based-stylized-synthesis\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/instantX-research/InstantStyle/blob/main/assets/example3.png\"\u003e\u003cimg src=\"https://github.com/instantX-research/InstantStyle/raw/main/assets/example3.png\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eComparison with Previous Works\u003c/h3\u003e\u003ca id=\"user-content-comparison-with-previous-works\" aria-label=\"Permalink: Comparison with Previous Works\" href=\"#comparison-with-previous-works\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/instantX-research/InstantStyle/blob/main/assets/comparison.png\"\u003e\u003cimg src=\"https://github.com/instantX-research/InstantStyle/raw/main/assets/comparison.png\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eDownload\u003c/h2\u003e\u003ca id=\"user-content-download\" aria-label=\"Permalink: Download\" href=\"#download\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eFollow \u003ca href=\"https://github.com/tencent-ailab/IP-Adapter?tab=readme-ov-file#download-models\"\u003eIP-Adapter\u003c/a\u003e to download pre-trained checkpoints from \u003ca href=\"https://huggingface.co/h94/IP-Adapter\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv data-snippet-clipboard-copy-content=\"git clone https://github.com/InstantStyle/InstantStyle.git\ncd InstantStyle\n\n# download the models\ngit lfs install\ngit clone https://huggingface.co/h94/IP-Adapter\nmv IP-Adapter/models models\nmv IP-Adapter/sdxl_models sdxl_models\"\u003e\u003cpre\u003e\u003ccode\u003egit clone https://github.com/InstantStyle/InstantStyle.git\ncd InstantStyle\n\n# download the models\ngit lfs install\ngit clone https://huggingface.co/h94/IP-Adapter\nmv IP-Adapter/models models\nmv IP-Adapter/sdxl_models sdxl_models\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eUsage\u003c/h2\u003e\u003ca id=\"user-content-usage\" aria-label=\"Permalink: Usage\" href=\"#usage\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eOur method is fully compatible with \u003ca href=\"https://github.com/tencent-ailab/IP-Adapter\"\u003eIP-Adapter\u003c/a\u003e. For feature subtraction, it only works for global feature instead of patch features. For SD1.5, you can find a demo at \u003ca href=\"https://github.com/InstantStyle/InstantStyle/blob/main/infer_style_sd15.py\"\u003einfer_style_sd15.py\u003c/a\u003e, but we find that SD1.5 has weaker perception and understanding of style information, thus this demo is experimental only. All block names can be found in \u003ca href=\"https://github.com/InstantStyle/InstantStyle/blob/main/attn_blocks.py\"\u003eattn_blocks.py\u003c/a\u003e and \u003ca href=\"https://github.com/InstantStyle/InstantStyle/blob/main/attn_blocks_sd15.py\"\u003eattn_blocks_sd15.py\u003c/a\u003e for SDXL and SD1.5 respectively.\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"import torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom PIL import Image\n\nfrom ip_adapter import IPAdapterXL\n\nbase_model_path = \u0026#34;stabilityai/stable-diffusion-xl-base-1.0\u0026#34;\nimage_encoder_path = \u0026#34;sdxl_models/image_encoder\u0026#34;\nip_ckpt = \u0026#34;sdxl_models/ip-adapter_sdxl.bin\u0026#34;\ndevice = \u0026#34;cuda\u0026#34;\n\n# load SDXL pipeline\npipe = StableDiffusionXLPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    add_watermarker=False,\n)\n\n# reduce memory consumption\npipe.enable_vae_tiling()\n\n# load ip-adapter\n# target_blocks=[\u0026#34;block\u0026#34;] for original IP-Adapter\n# target_blocks=[\u0026#34;up_blocks.0.attentions.1\u0026#34;] for style blocks only\n# target_blocks = [\u0026#34;up_blocks.0.attentions.1\u0026#34;, \u0026#34;down_blocks.2.attentions.1\u0026#34;] # for style+layout blocks\nip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device, target_blocks=[\u0026#34;up_blocks.0.attentions.1\u0026#34;])\n\nimage = \u0026#34;./assets/0.jpg\u0026#34;\nimage = Image.open(image)\nimage.resize((512, 512))\n\n# generate image variations with only image prompt\nimages = ip_model.generate(pil_image=image,\n                            prompt=\u0026#34;a cat, masterpiece, best quality, high quality\u0026#34;,\n                            negative_prompt= \u0026#34;text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\u0026#34;,\n                            scale=1.0,\n                            guidance_scale=5,\n                            num_samples=1,\n                            num_inference_steps=30, \n                            seed=42,\n                            #neg_content_prompt=\u0026#34;a rabbit\u0026#34;,\n                            #neg_content_scale=0.5,\n                          )\n\nimages[0].save(\u0026#34;result.png\u0026#34;)\"\u003e\u003cpre\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003etorch\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ediffusers\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eStableDiffusionXLPipeline\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ePIL\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e\n\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003eip_adapter\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eIPAdapterXL\u003c/span\u003e\n\n\u003cspan\u003ebase_model_path\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;stabilityai/stable-diffusion-xl-base-1.0\u0026#34;\u003c/span\u003e\n\u003cspan\u003eimage_encoder_path\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;sdxl_models/image_encoder\u0026#34;\u003c/span\u003e\n\u003cspan\u003eip_ckpt\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;sdxl_models/ip-adapter_sdxl.bin\u0026#34;\u003c/span\u003e\n\u003cspan\u003edevice\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e\n\n\u003cspan\u003e# load SDXL pipeline\u003c/span\u003e\n\u003cspan\u003epipe\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eStableDiffusionXLPipeline\u003c/span\u003e.\u003cspan\u003efrom_pretrained\u003c/span\u003e(\n    \u003cspan\u003ebase_model_path\u003c/span\u003e,\n    \u003cspan\u003etorch_dtype\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003etorch\u003c/span\u003e.\u003cspan\u003efloat16\u003c/span\u003e,\n    \u003cspan\u003eadd_watermarker\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e,\n)\n\n\u003cspan\u003e# reduce memory consumption\u003c/span\u003e\n\u003cspan\u003epipe\u003c/span\u003e.\u003cspan\u003eenable_vae_tiling\u003c/span\u003e()\n\n\u003cspan\u003e# load ip-adapter\u003c/span\u003e\n\u003cspan\u003e# target_blocks=[\u0026#34;block\u0026#34;] for original IP-Adapter\u003c/span\u003e\n\u003cspan\u003e# target_blocks=[\u0026#34;up_blocks.0.attentions.1\u0026#34;] for style blocks only\u003c/span\u003e\n\u003cspan\u003e# target_blocks = [\u0026#34;up_blocks.0.attentions.1\u0026#34;, \u0026#34;down_blocks.2.attentions.1\u0026#34;] # for style+layout blocks\u003c/span\u003e\n\u003cspan\u003eip_model\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eIPAdapterXL\u003c/span\u003e(\u003cspan\u003epipe\u003c/span\u003e, \u003cspan\u003eimage_encoder_path\u003c/span\u003e, \u003cspan\u003eip_ckpt\u003c/span\u003e, \u003cspan\u003edevice\u003c/span\u003e, \u003cspan\u003etarget_blocks\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e[\u003cspan\u003e\u0026#34;up_blocks.0.attentions.1\u0026#34;\u003c/span\u003e])\n\n\u003cspan\u003eimage\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;./assets/0.jpg\u0026#34;\u003c/span\u003e\n\u003cspan\u003eimage\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e.\u003cspan\u003eopen\u003c/span\u003e(\u003cspan\u003eimage\u003c/span\u003e)\n\u003cspan\u003eimage\u003c/span\u003e.\u003cspan\u003eresize\u003c/span\u003e((\u003cspan\u003e512\u003c/span\u003e, \u003cspan\u003e512\u003c/span\u003e))\n\n\u003cspan\u003e# generate image variations with only image prompt\u003c/span\u003e\n\u003cspan\u003eimages\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eip_model\u003c/span\u003e.\u003cspan\u003egenerate\u003c/span\u003e(\u003cspan\u003epil_image\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eimage\u003c/span\u003e,\n                            \u003cspan\u003eprompt\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;a cat, masterpiece, best quality, high quality\u0026#34;\u003c/span\u003e,\n                            \u003cspan\u003enegative_prompt\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\u0026#34;\u003c/span\u003e,\n                            \u003cspan\u003escale\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1.0\u003c/span\u003e,\n                            \u003cspan\u003eguidance_scale\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e5\u003c/span\u003e,\n                            \u003cspan\u003enum_samples\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e,\n                            \u003cspan\u003enum_inference_steps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e30\u003c/span\u003e, \n                            \u003cspan\u003eseed\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e42\u003c/span\u003e,\n                            \u003cspan\u003e#neg_content_prompt=\u0026#34;a rabbit\u0026#34;,\u003c/span\u003e\n                            \u003cspan\u003e#neg_content_scale=0.5,\u003c/span\u003e\n                          )\n\n\u003cspan\u003eimages\u003c/span\u003e[\u003cspan\u003e0\u003c/span\u003e].\u003cspan\u003esave\u003c/span\u003e(\u003cspan\u003e\u0026#34;result.png\u0026#34;\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eUse in diffusers\u003c/h2\u003e\u003ca id=\"user-content-use-in-diffusers\" aria-label=\"Permalink: Use in diffusers\" href=\"#use-in-diffusers\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eInstantStyle has already been integrated into \u003ca href=\"https://huggingface.co/docs/diffusers/main/en/using-diffusers/ip_adapter#style--layout-control\" rel=\"nofollow\"\u003ediffusers\u003c/a\u003e (please make sure that you have installed diffusers\u0026gt;=0.28.0.dev0), making the usage significantly simpler. You can now control the per-transformer behavior of each IP-Adapter with the set_ip_adapter_scale() method, using a configuration dictionary as shown below:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"from diffusers import StableDiffusionXLPipeline\nfrom PIL import Image\nimport torch\n\n# load SDXL pipeline\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \u0026#34;stabilityai/stable-diffusion-xl-base-1.0\u0026#34;,\n    torch_dtype=torch.float16,\n    add_watermarker=False,\n)\n\n# load ip-adapter\npipe.load_ip_adapter(\u0026#34;h94/IP-Adapter\u0026#34;, subfolder=\u0026#34;sdxl_models\u0026#34;, weight_name=\u0026#34;ip-adapter_sdxl.bin\u0026#34;)\npipe.enable_vae_tiling()\n\n# configure ip-adapter scales.\nscale = {\n    \u0026#34;down\u0026#34;: {\u0026#34;block_2\u0026#34;: [0.0, 1.0]},\n    \u0026#34;up\u0026#34;: {\u0026#34;block_0\u0026#34;: [0.0, 1.0, 0.0]},\n}\npipeline.set_ip_adapter_scale(scale)\"\u003e\u003cpre\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ediffusers\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eStableDiffusionXLPipeline\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ePIL\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e\n\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003etorch\u003c/span\u003e\n\n\u003cspan\u003e# load SDXL pipeline\u003c/span\u003e\n\u003cspan\u003epipe\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eStableDiffusionXLPipeline\u003c/span\u003e.\u003cspan\u003efrom_pretrained\u003c/span\u003e(\n    \u003cspan\u003e\u0026#34;stabilityai/stable-diffusion-xl-base-1.0\u0026#34;\u003c/span\u003e,\n    \u003cspan\u003etorch_dtype\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003etorch\u003c/span\u003e.\u003cspan\u003efloat16\u003c/span\u003e,\n    \u003cspan\u003eadd_watermarker\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e,\n)\n\n\u003cspan\u003e# load ip-adapter\u003c/span\u003e\n\u003cspan\u003epipe\u003c/span\u003e.\u003cspan\u003eload_ip_adapter\u003c/span\u003e(\u003cspan\u003e\u0026#34;h94/IP-Adapter\u0026#34;\u003c/span\u003e, \u003cspan\u003esubfolder\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;sdxl_models\u0026#34;\u003c/span\u003e, \u003cspan\u003eweight_name\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;ip-adapter_sdxl.bin\u0026#34;\u003c/span\u003e)\n\u003cspan\u003epipe\u003c/span\u003e.\u003cspan\u003eenable_vae_tiling\u003c/span\u003e()\n\n\u003cspan\u003e# configure ip-adapter scales.\u003c/span\u003e\n\u003cspan\u003escale\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e {\n    \u003cspan\u003e\u0026#34;down\u0026#34;\u003c/span\u003e: {\u003cspan\u003e\u0026#34;block_2\u0026#34;\u003c/span\u003e: [\u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e1.0\u003c/span\u003e]},\n    \u003cspan\u003e\u0026#34;up\u0026#34;\u003c/span\u003e: {\u003cspan\u003e\u0026#34;block_0\u0026#34;\u003c/span\u003e: [\u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e1.0\u003c/span\u003e, \u003cspan\u003e0.0\u003c/span\u003e]},\n}\n\u003cspan\u003epipeline\u003c/span\u003e.\u003cspan\u003eset_ip_adapter_scale\u003c/span\u003e(\u003cspan\u003escale\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eIn this example. We set \u003ccode\u003escale=1.0\u003c/code\u003e for IP-Adapter in the second transformer of down-part, block 2, and the second in up-part, block 0. Note that there are 2 transformers in down-part block 2 so the list is of length 2, and so do the up-part block 0. The rest IP-Adapter will have a zero scale which means disable them in all the other layers.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWith the help of \u003ccode\u003eset_ip_adapter_scale()\u003c/code\u003e, we can now configure IP-Adapters without a need of reloading them everytime we want to test the IP-Adapter behaviors.\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"# for original IP-Adapter\nscale = 1.0\npipeline.set_ip_adapter_scale(scale)\n\n# for style blocks only\nscale = {\n    \u0026#34;up\u0026#34;: {\u0026#34;block_0\u0026#34;: [0.0, 1.0, 0.0]},\n}\npipeline.set_ip_adapter_scale(scale)\"\u003e\u003cpre\u003e\u003cspan\u003e# for original IP-Adapter\u003c/span\u003e\n\u003cspan\u003escale\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e1.0\u003c/span\u003e\n\u003cspan\u003epipeline\u003c/span\u003e.\u003cspan\u003eset_ip_adapter_scale\u003c/span\u003e(\u003cspan\u003escale\u003c/span\u003e)\n\n\u003cspan\u003e# for style blocks only\u003c/span\u003e\n\u003cspan\u003escale\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e {\n    \u003cspan\u003e\u0026#34;up\u0026#34;\u003c/span\u003e: {\u003cspan\u003e\u0026#34;block_0\u0026#34;\u003c/span\u003e: [\u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e1.0\u003c/span\u003e, \u003cspan\u003e0.0\u003c/span\u003e]},\n}\n\u003cspan\u003epipeline\u003c/span\u003e.\u003cspan\u003eset_ip_adapter_scale\u003c/span\u003e(\u003cspan\u003escale\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eMultiple IP-Adapter images with masks\u003c/h3\u003e\u003ca id=\"user-content-multiple-ip-adapter-images-with-masks\" aria-label=\"Permalink: Multiple IP-Adapter images with masks\" href=\"#multiple-ip-adapter-images-with-masks\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eYou can also load multiple IP-Adapters, together with multiple IP-Adapter images with masks for more precisely layout control just as that in \u003ca href=\"https://huggingface.co/docs/diffusers/main/en/using-diffusers/ip_adapter#ip-adapter-masking\" rel=\"nofollow\"\u003eIP-Adapter\u003c/a\u003e do.\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"from diffusers import StableDiffusionXLPipeline\nfrom diffusers.image_processor import IPAdapterMaskProcessor\nfrom transformers import CLIPVisionModelWithProjection\nfrom PIL import Image\nimport torch\n\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(\n    \u0026#34;h94/IP-Adapter\u0026#34;, subfolder=\u0026#34;models/image_encoder\u0026#34;, torch_dtype=torch.float16\n).to(\u0026#34;cuda\u0026#34;)\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \u0026#34;RunDiffusion/Juggernaut-XL-v9\u0026#34;, torch_dtype=torch.float16, image_encoder=image_encoder, variant=\u0026#34;fp16\u0026#34;\n).to(\u0026#34;cuda\u0026#34;)\n\npipe.load_ip_adapter(\n    [\u0026#34;ostris/ip-composition-adapter\u0026#34;, \u0026#34;h94/IP-Adapter\u0026#34;],\n    subfolder=[\u0026#34;\u0026#34;, \u0026#34;sdxl_models\u0026#34;],\n    weight_name=[\n        \u0026#34;ip_plus_composition_sdxl.safetensors\u0026#34;,\n        \u0026#34;ip-adapter_sdxl_vit-h.safetensors\u0026#34;,\n    ],\n    image_encoder_folder=None,\n)\n\nscale_1 = {\n    \u0026#34;down\u0026#34;: [[0.0, 0.0, 1.0]],\n    \u0026#34;mid\u0026#34;: [[0.0, 0.0, 1.0]],\n    \u0026#34;up\u0026#34;: {\u0026#34;block_0\u0026#34;: [[0.0, 0.0, 1.0], [1.0, 1.0, 1.0], [0.0, 0.0, 1.0]], \u0026#34;block_1\u0026#34;: [[0.0, 0.0, 1.0]]},\n}\n# activate the first IP-Adapter in everywhere in the model,\n# configure the second one for precise style control to each masked input.\npipe.set_ip_adapter_scale([1.0, scale_1])\n\nprocessor = IPAdapterMaskProcessor()\nfemale_mask = Image.open(\u0026#34;./assets/female_mask.png\u0026#34;)\nmale_mask = Image.open(\u0026#34;./assets/male_mask.png\u0026#34;)\nbackground_mask = Image.open(\u0026#34;./assets/background_mask.png\u0026#34;)\ncomposition_mask = Image.open(\u0026#34;./assets/composition_mask.png\u0026#34;)\nmask1 = processor.preprocess([composition_mask], height=1024, width=1024)\nmask2 = processor.preprocess([female_mask, male_mask, background_mask], height=1024, width=1024)\nmask2 = mask2.reshape(1, mask2.shape[0], mask2.shape[2], mask2.shape[3])   # output -\u0026gt; (1, 3, 1024, 1024)\n\nip_female_style = Image.open(\u0026#34;./assets/ip_female_style.png\u0026#34;)\nip_male_style = Image.open(\u0026#34;./assets/ip_male_style.png\u0026#34;)\nip_background = Image.open(\u0026#34;./assets/ip_background.png\u0026#34;)\nip_composition_image = Image.open(\u0026#34;./assets/ip_composition_image.png\u0026#34;)\n\nimage = pipe(\n    prompt=\u0026#34;high quality, cinematic photo, cinemascope, 35mm, film grain, highly detailed\u0026#34;,\n    negative_prompt=\u0026#34;\u0026#34;,\n    ip_adapter_image=[ip_composition_image, [ip_female_style, ip_male_style, ip_background]],\n    cross_attention_kwargs={\u0026#34;ip_adapter_masks\u0026#34;: [mask1, mask2]},\n    guidance_scale=6.5,\n    num_inference_steps=25,\n).images[0]\nimage\n\"\u003e\u003cpre\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ediffusers\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eStableDiffusionXLPipeline\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ediffusers\u003c/span\u003e.\u003cspan\u003eimage_processor\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eIPAdapterMaskProcessor\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003etransformers\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eCLIPVisionModelWithProjection\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ePIL\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e\n\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003etorch\u003c/span\u003e\n\n\u003cspan\u003eimage_encoder\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eCLIPVisionModelWithProjection\u003c/span\u003e.\u003cspan\u003efrom_pretrained\u003c/span\u003e(\n    \u003cspan\u003e\u0026#34;h94/IP-Adapter\u0026#34;\u003c/span\u003e, \u003cspan\u003esubfolder\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;models/image_encoder\u0026#34;\u003c/span\u003e, \u003cspan\u003etorch_dtype\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003etorch\u003c/span\u003e.\u003cspan\u003efloat16\u003c/span\u003e\n).\u003cspan\u003eto\u003c/span\u003e(\u003cspan\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e)\n\n\u003cspan\u003epipe\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eStableDiffusionXLPipeline\u003c/span\u003e.\u003cspan\u003efrom_pretrained\u003c/span\u003e(\n    \u003cspan\u003e\u0026#34;RunDiffusion/Juggernaut-XL-v9\u0026#34;\u003c/span\u003e, \u003cspan\u003etorch_dtype\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003etorch\u003c/span\u003e.\u003cspan\u003efloat16\u003c/span\u003e, \u003cspan\u003eimage_encoder\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eimage_encoder\u003c/span\u003e, \u003cspan\u003evariant\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;fp16\u0026#34;\u003c/span\u003e\n).\u003cspan\u003eto\u003c/span\u003e(\u003cspan\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e)\n\n\u003cspan\u003epipe\u003c/span\u003e.\u003cspan\u003eload_ip_adapter\u003c/span\u003e(\n    [\u003cspan\u003e\u0026#34;ostris/ip-composition-adapter\u0026#34;\u003c/span\u003e, \u003cspan\u003e\u0026#34;h94/IP-Adapter\u0026#34;\u003c/span\u003e],\n    \u003cspan\u003esubfolder\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e[\u003cspan\u003e\u0026#34;\u0026#34;\u003c/span\u003e, \u003cspan\u003e\u0026#34;sdxl_models\u0026#34;\u003c/span\u003e],\n    \u003cspan\u003eweight_name\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e[\n        \u003cspan\u003e\u0026#34;ip_plus_composition_sdxl.safetensors\u0026#34;\u003c/span\u003e,\n        \u003cspan\u003e\u0026#34;ip-adapter_sdxl_vit-h.safetensors\u0026#34;\u003c/span\u003e,\n    ],\n    \u003cspan\u003eimage_encoder_folder\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eNone\u003c/span\u003e,\n)\n\n\u003cspan\u003escale_1\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e {\n    \u003cspan\u003e\u0026#34;down\u0026#34;\u003c/span\u003e: [[\u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e1.0\u003c/span\u003e]],\n    \u003cspan\u003e\u0026#34;mid\u0026#34;\u003c/span\u003e: [[\u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e1.0\u003c/span\u003e]],\n    \u003cspan\u003e\u0026#34;up\u0026#34;\u003c/span\u003e: {\u003cspan\u003e\u0026#34;block_0\u0026#34;\u003c/span\u003e: [[\u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e1.0\u003c/span\u003e], [\u003cspan\u003e1.0\u003c/span\u003e, \u003cspan\u003e1.0\u003c/span\u003e, \u003cspan\u003e1.0\u003c/span\u003e], [\u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e1.0\u003c/span\u003e]], \u003cspan\u003e\u0026#34;block_1\u0026#34;\u003c/span\u003e: [[\u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e0.0\u003c/span\u003e, \u003cspan\u003e1.0\u003c/span\u003e]]},\n}\n\u003cspan\u003e# activate the first IP-Adapter in everywhere in the model,\u003c/span\u003e\n\u003cspan\u003e# configure the second one for precise style control to each masked input.\u003c/span\u003e\n\u003cspan\u003epipe\u003c/span\u003e.\u003cspan\u003eset_ip_adapter_scale\u003c/span\u003e([\u003cspan\u003e1.0\u003c/span\u003e, \u003cspan\u003escale_1\u003c/span\u003e])\n\n\u003cspan\u003eprocessor\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eIPAdapterMaskProcessor\u003c/span\u003e()\n\u003cspan\u003efemale_mask\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e.\u003cspan\u003eopen\u003c/span\u003e(\u003cspan\u003e\u0026#34;./assets/female_mask.png\u0026#34;\u003c/span\u003e)\n\u003cspan\u003emale_mask\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e.\u003cspan\u003eopen\u003c/span\u003e(\u003cspan\u003e\u0026#34;./assets/male_mask.png\u0026#34;\u003c/span\u003e)\n\u003cspan\u003ebackground_mask\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e.\u003cspan\u003eopen\u003c/span\u003e(\u003cspan\u003e\u0026#34;./assets/background_mask.png\u0026#34;\u003c/span\u003e)\n\u003cspan\u003ecomposition_mask\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e.\u003cspan\u003eopen\u003c/span\u003e(\u003cspan\u003e\u0026#34;./assets/composition_mask.png\u0026#34;\u003c/span\u003e)\n\u003cspan\u003emask1\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eprocessor\u003c/span\u003e.\u003cspan\u003epreprocess\u003c/span\u003e([\u003cspan\u003ecomposition_mask\u003c/span\u003e], \u003cspan\u003eheight\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1024\u003c/span\u003e, \u003cspan\u003ewidth\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1024\u003c/span\u003e)\n\u003cspan\u003emask2\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eprocessor\u003c/span\u003e.\u003cspan\u003epreprocess\u003c/span\u003e([\u003cspan\u003efemale_mask\u003c/span\u003e, \u003cspan\u003emale_mask\u003c/span\u003e, \u003cspan\u003ebackground_mask\u003c/span\u003e], \u003cspan\u003eheight\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1024\u003c/span\u003e, \u003cspan\u003ewidth\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1024\u003c/span\u003e)\n\u003cspan\u003emask2\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003emask2\u003c/span\u003e.\u003cspan\u003ereshape\u003c/span\u003e(\u003cspan\u003e1\u003c/span\u003e, \u003cspan\u003emask2\u003c/span\u003e.\u003cspan\u003eshape\u003c/span\u003e[\u003cspan\u003e0\u003c/span\u003e], \u003cspan\u003emask2\u003c/span\u003e.\u003cspan\u003eshape\u003c/span\u003e[\u003cspan\u003e2\u003c/span\u003e], \u003cspan\u003emask2\u003c/span\u003e.\u003cspan\u003eshape\u003c/span\u003e[\u003cspan\u003e3\u003c/span\u003e])   \u003cspan\u003e# output -\u0026gt; (1, 3, 1024, 1024)\u003c/span\u003e\n\n\u003cspan\u003eip_female_style\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e.\u003cspan\u003eopen\u003c/span\u003e(\u003cspan\u003e\u0026#34;./assets/ip_female_style.png\u0026#34;\u003c/span\u003e)\n\u003cspan\u003eip_male_style\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e.\u003cspan\u003eopen\u003c/span\u003e(\u003cspan\u003e\u0026#34;./assets/ip_male_style.png\u0026#34;\u003c/span\u003e)\n\u003cspan\u003eip_background\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e.\u003cspan\u003eopen\u003c/span\u003e(\u003cspan\u003e\u0026#34;./assets/ip_background.png\u0026#34;\u003c/span\u003e)\n\u003cspan\u003eip_composition_image\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e.\u003cspan\u003eopen\u003c/span\u003e(\u003cspan\u003e\u0026#34;./assets/ip_composition_image.png\u0026#34;\u003c/span\u003e)\n\n\u003cspan\u003eimage\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003epipe\u003c/span\u003e(\n    \u003cspan\u003eprompt\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;high quality, cinematic photo, cinemascope, 35mm, film grain, highly detailed\u0026#34;\u003c/span\u003e,\n    \u003cspan\u003enegative_prompt\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026#34;\u003c/span\u003e,\n    \u003cspan\u003eip_adapter_image\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e[\u003cspan\u003eip_composition_image\u003c/span\u003e, [\u003cspan\u003eip_female_style\u003c/span\u003e, \u003cspan\u003eip_male_style\u003c/span\u003e, \u003cspan\u003eip_background\u003c/span\u003e]],\n    \u003cspan\u003ecross_attention_kwargs\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e{\u003cspan\u003e\u0026#34;ip_adapter_masks\u0026#34;\u003c/span\u003e: [\u003cspan\u003emask1\u003c/span\u003e, \u003cspan\u003emask2\u003c/span\u003e]},\n    \u003cspan\u003eguidance_scale\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e6.5\u003c/span\u003e,\n    \u003cspan\u003enum_inference_steps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e25\u003c/span\u003e,\n).\u003cspan\u003eimages\u003c/span\u003e[\u003cspan\u003e0\u003c/span\u003e]\n\u003cspan\u003eimage\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\n  \u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/instantX-research/InstantStyle/blob/main/assets/multi_instantstyle.png\"\u003e\u003cimg src=\"https://github.com/instantX-research/InstantStyle/raw/main/assets/multi_instantstyle.png\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eHigh Resolution Generation\u003c/h2\u003e\u003ca id=\"user-content-high-resolution-generation\" aria-label=\"Permalink: High Resolution Generation\" href=\"#high-resolution-generation\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWe employ \u003ca href=\"https://github.com/megvii-research/HiDiffusion\"\u003eHiDiffusion\u003c/a\u003e to seamlessly generate high-resolution images, you can install via \u003ccode\u003epip install hidiffusion\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"from hidiffusion import apply_hidiffusion, remove_hidiffusion\n\n# reduce memory consumption\npipe.enable_vae_tiling()\n\n# apply hidiffusion with a single line of code.\napply_hidiffusion(pipe)\n\n...\n\n# generate image at higher resolution\nimages = ip_model.generate(pil_image=image,\n                           prompt=\u0026#34;a cat, masterpiece, best quality, high quality\u0026#34;,\n                           negative_prompt= \u0026#34;text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\u0026#34;,\n                           scale=1.0,\n                           guidance_scale=5,\n                           num_samples=1,\n                           num_inference_steps=30, \n                           seed=42,\n                           height=2048,\n                           width=2048\n                          )\"\u003e\u003cpre\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ehidiffusion\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eapply_hidiffusion\u003c/span\u003e, \u003cspan\u003eremove_hidiffusion\u003c/span\u003e\n\n\u003cspan\u003e# reduce memory consumption\u003c/span\u003e\n\u003cspan\u003epipe\u003c/span\u003e.\u003cspan\u003eenable_vae_tiling\u003c/span\u003e()\n\n\u003cspan\u003e# apply hidiffusion with a single line of code.\u003c/span\u003e\n\u003cspan\u003eapply_hidiffusion\u003c/span\u003e(\u003cspan\u003epipe\u003c/span\u003e)\n\n...\n\n\u003cspan\u003e# generate image at higher resolution\u003c/span\u003e\n\u003cspan\u003eimages\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eip_model\u003c/span\u003e.\u003cspan\u003egenerate\u003c/span\u003e(\u003cspan\u003epil_image\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eimage\u003c/span\u003e,\n                           \u003cspan\u003eprompt\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;a cat, masterpiece, best quality, high quality\u0026#34;\u003c/span\u003e,\n                           \u003cspan\u003enegative_prompt\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\u0026#34;\u003c/span\u003e,\n                           \u003cspan\u003escale\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1.0\u003c/span\u003e,\n                           \u003cspan\u003eguidance_scale\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e5\u003c/span\u003e,\n                           \u003cspan\u003enum_samples\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e,\n                           \u003cspan\u003enum_inference_steps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e30\u003c/span\u003e, \n                           \u003cspan\u003eseed\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e42\u003c/span\u003e,\n                           \u003cspan\u003eheight\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e,\n                           \u003cspan\u003ewidth\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\n                          )\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eDistributed Inference\u003c/h2\u003e\u003ca id=\"user-content-distributed-inference\" aria-label=\"Permalink: Distributed Inference\" href=\"#distributed-inference\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eOn distributed setups, you can run inference across multiple GPUs with ðŸ¤— Accelerate or PyTorch Distributed, which is useful for generating with multiple prompts in parallel, in case you have limited VRAM on each GPU. More information can be found \u003ca href=\"https://huggingface.co/docs/diffusers/main/en/training/distributed_inference#device-placement\" rel=\"nofollow\"\u003ehere\u003c/a\u003e. Make sure you have installed diffusers from the source and the lastest accelerate.\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"max_memory = {0:\u0026#34;10GB\u0026#34;, 1:\u0026#34;10GB\u0026#34;}\npipe = StableDiffusionXLPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    add_watermarker=False,\n    device_map=\u0026#34;balanced\u0026#34;,\n    max_memory=max_memory\n)\"\u003e\u003cpre\u003e\u003cspan\u003emax_memory\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e {\u003cspan\u003e0\u003c/span\u003e:\u003cspan\u003e\u0026#34;10GB\u0026#34;\u003c/span\u003e, \u003cspan\u003e1\u003c/span\u003e:\u003cspan\u003e\u0026#34;10GB\u0026#34;\u003c/span\u003e}\n\u003cspan\u003epipe\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eStableDiffusionXLPipeline\u003c/span\u003e.\u003cspan\u003efrom_pretrained\u003c/span\u003e(\n    \u003cspan\u003ebase_model_path\u003c/span\u003e,\n    \u003cspan\u003etorch_dtype\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003etorch\u003c/span\u003e.\u003cspan\u003efloat16\u003c/span\u003e,\n    \u003cspan\u003eadd_watermarker\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e,\n    \u003cspan\u003edevice_map\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;balanced\u0026#34;\u003c/span\u003e,\n    \u003cspan\u003emax_memory\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003emax_memory\u003c/span\u003e\n)\u003c/pre\u003e\u003c/div\u003e\n\u003cdiv dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eStart a local gradio demo \u003ca href=\"https://github.com/gradio-app/gradio\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0aad2cc35d9b929ec7344ece3bbe7884c9618b501d29c029fffc324932e3f50d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67726164696f2d6170702f67726164696f\" data-canonical-src=\"https://img.shields.io/github/stars/gradio-app/gradio\"/\u003e\u003c/a\u003e\u003c/h2\u003e\u003ca id=\"user-content-start-a-local-gradio-demo-\" aria-label=\"Permalink: Start a local gradio demo \" href=\"#start-a-local-gradio-demo-\"\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eRun the following command:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"git clone https://github.com/InstantStyle/InstantStyle.git\ncd ./InstantStyle/gradio_demo/\npip install -r requirements.txt\npython app.py\"\u003e\u003cpre\u003egit clone https://github.com/InstantStyle/InstantStyle.git\n\u003cspan\u003ecd\u003c/span\u003e ./InstantStyle/gradio_demo/\npip install -r requirements.txt\npython app.py\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eResources\u003c/h2\u003e\u003ca id=\"user-content-resources\" aria-label=\"Permalink: Resources\" href=\"#resources\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Mikubill/sd-webui-controlnet/discussions/2770\" data-hovercard-type=\"discussion\" data-hovercard-url=\"/Mikubill/sd-webui-controlnet/discussions/2770/hovercard\"\u003eInstantStyle for WebUI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/cubiq/ComfyUI_IPAdapter_plus\"\u003eInstantStyle for ComfyUI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/InstantID/InstantID\"\u003eInstantID\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eDisclaimer\u003c/h2\u003e\u003ca id=\"user-content-disclaimer\" aria-label=\"Permalink: Disclaimer\" href=\"#disclaimer\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe pretrained checkpoints follow the license in \u003ca href=\"https://github.com/tencent-ailab/IP-Adapter?tab=readme-ov-file#download-models\"\u003eIP-Adapter\u003c/a\u003e. Users are granted the freedom to create images using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eAcknowledgements\u003c/h2\u003e\u003ca id=\"user-content-acknowledgements\" aria-label=\"Permalink: Acknowledgements\" href=\"#acknowledgements\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eInstantStyle is developed by the InstantX team and is highly built on \u003ca href=\"https://github.com/tencent-ailab/IP-Adapter\"\u003eIP-Adapter\u003c/a\u003e, which has been unfairly compared by many other works. We at InstantStyle make IP-Adapter great again. Additionally, we acknowledge \u003ca href=\"https://github.com/xiaohu2015\"\u003eHu Ye\u003c/a\u003e for his valuable discussion.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eStar History\u003c/h2\u003e\u003ca id=\"user-content-star-history\" aria-label=\"Permalink: Star History\" href=\"#star-history\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://star-history.com/#InstantStyle/InstantStyle\u0026amp;Date\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f8212abdcc55af180b804f88de05964bca76851bb8e9f91c36bb5a8351a552d9/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d496e7374616e745374796c652f496e7374616e745374796c6526747970653d44617465\" alt=\"Star History Chart\" data-canonical-src=\"https://api.star-history.com/svg?repos=InstantStyle/InstantStyle\u0026amp;type=Date\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eCite\u003c/h2\u003e\u003ca id=\"user-content-cite\" aria-label=\"Permalink: Cite\" href=\"#cite\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eIf you find InstantStyle useful for your research and applications, please cite us using this BibTeX:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{wang2024instantstyle,\n  title={InstantStyle-Plus: Style Transfer with Content-Preserving in Text-to-Image Generation},\n  author={Wang, Haofan and Xing, Peng and Huang, Renyuan and Ai, Hao and Wang, Qixun and Bai, Xu},\n  journal={arXiv preprint arXiv:2407.00788},\n  year={2024}\n}\n\n@article{wang2024instantstyle,\n  title={InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation},\n  author={Wang, Haofan and Wang, Qixun and Bai, Xu and Qin, Zekui and Chen, Anthony},\n  journal={arXiv preprint arXiv:2404.02733},\n  year={2024}\n}\"\u003e\u003cpre\u003e\u003cspan\u003e@article\u003c/span\u003e{\u003cspan\u003ewang2024instantstyle\u003c/span\u003e,\n  \u003cspan\u003etitle\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003eInstantStyle-Plus: Style Transfer with Content-Preserving in Text-to-Image Generation\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan\u003eauthor\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003eWang, Haofan and Xing, Peng and Huang, Renyuan and Ai, Hao and Wang, Qixun and Bai, Xu\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan\u003ejournal\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003earXiv preprint arXiv:2407.00788\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan\u003eyear\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003e2024\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e\n}\n\n\u003cspan\u003e@article\u003c/span\u003e{\u003cspan\u003ewang2024instantstyle\u003c/span\u003e,\n  \u003cspan\u003etitle\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003eInstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan\u003eauthor\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003eWang, Haofan and Wang, Qixun and Bai, Xu and Qin, Zekui and Chen, Anthony\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan\u003ejournal\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003earXiv preprint arXiv:2404.02733\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan\u003eyear\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e{\u003c/span\u003e2024\u003cspan\u003e}\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eFor any question, feel free to contact us via \u003ca href=\"mailto:haofanwang.ai@gmail.com\"\u003ehaofanwang.ai@gmail.com\u003c/a\u003e.\u003c/p\u003e\n\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": null,
  "modifiedTime": null
}
