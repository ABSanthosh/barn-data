{
  "id": "3d9fa5dd-7b50-4f26-b3dc-15799d1d442b",
  "title": "Meta Rolls Back Hate Speech Rules",
  "link": "https://www.inc.com/associated-press/meta-rolls-back-hate-speech-rules/91105068",
  "description": "Advocates say Meta’s decision to loosen content moderation could lead to serious harms.",
  "author": "Associated Press",
  "published": "Wed, 08 Jan 2025 19:21:20 -0500",
  "source": "https://www.inc.com/rss/",
  "categories": [
    "Technology"
  ],
  "byline": "Associated Press",
  "length": 3573,
  "excerpt": "Advocates say Meta's decision to loosen content moderation could lead to serious harms.",
  "siteName": "Inc",
  "favicon": "https://www.inc.com/_public/icons/apple-icon.png",
  "text": "It wasn’t just fact-checking that Meta scrapped from its platforms as it prepares for the second Trump administration. The social media giant has also loosened its rules around hate speech and abuse — again following the lead of Elon Musk’s X — specifically when it comes to sexual orientation and gender identity as well as immigration status.The changes are worrying advocates for vulnerable groups, who say Meta’s decision to scale back content moderation could lead to real-word harms. Meta CEO Mark Zuckerberg said Tuesday that the company will “remove restrictions on topics like immigration and gender that are out of touch with mainstream discourse,” citing “recent elections” as a catalyst.For instance, Meta has added the following to its rules — called community standards — that users are asked to abide by:“We do allow allegations of mental illness or abnormality when based on gender or sexual orientation, given political and religious discourse about transgenderism and homosexuality and common non-serious usage of words like ‘weird.'” In other words, it is now permitted to call gay people mentally ill on Facebook, Threads and Instagram. Other slurs and what Meta calls “harmful stereotypes historically linked to intimidation” — such as Blackface and Holocaust denial — are still prohibited.The Menlo Park, California-based company also removed a sentence from its “policy rationale” explaining why it bans certain hateful conduct. The now-deleted sentence said that hate speech “creates an environment of intimidation and exclusion, and in some cases may promote offline violence.”“The policy change is a tactic to earn favor with the incoming administration while also reducing business costs related to content moderation,” said Ben Leiner, a lecturer at the University of Virginia’s Darden School of Business who studies political and technology trends. “This decision will lead to real-world harm, not only in the United States where there has been an uptick in hate speech and disinformation on social media platforms, but also abroad where disinformation on Facebook has accelerated ethnic conflict in places like Myanmar.”Meta, in fact, acknowledged in 2018 that it didn’t do enough to prevent its platform from being used to “incite offline violence” in Myanmar, fueling communal hatred and violence against the country’s Muslim Rohingya minority.Arturo Béjar, a former engineering director at Meta known for his expertise on curbing online harassment, said while most of the attention has gone to the company’s fact-checking announcement Tuesday, he is more worried about the changes to Meta’s harmful content policies.That’s because instead of proactively enforcing rules against things like self-harm, bullying and harassment, Meta will now rely on user reports before it takes any action. The company said it plans to focus its automated systems on “tackling illegal and high-severity violations, like terrorism, child sexual exploitation, drugs, fraud and scams.”Béjar said that’s even though “Meta knows that by the time a report is submitted and reviewed the content will have done most of its harm.”“I shudder to think what these changes will mean for our youth, Meta is abdicating their responsibility to safety, and we won’t the impact of these changes because Meta refuses to be transparent about the harms teenagers experience, and they go to extraordinary lengths to dilute or stop legislation that could help,” he said.Copyright 2025. All rights reserved. This material may not be published, broadcast, rewritten or redistributed.",
  "image": "https://img-cdn.inc.com/image/upload/f_webp,q_auto,c_fit,w_1024,h_1024/vip/2025/01/GettyImages-1446235728.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eIt wasn’t just \u003ca href=\"https://www.inc.com/associated-press/meta-replaces-fact-checking-with-x-style-community-notes/91104053\"\u003efact-checking\u003c/a\u003e that \u003ca href=\"https://www.inc.com/ben-sherry/meta-deleted-its-controversial-ai-profiles-heres-why/91102577\"\u003eMeta\u003c/a\u003e scrapped from its platforms as it prepares for the second Trump administration. The social media giant has also loosened its rules around hate speech and abuse — again following the lead of Elon Musk’s X — specifically when it comes to sexual orientation and gender identity as well as immigration status.\u003c/p\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\u003cp\u003eThe changes are worrying advocates for vulnerable groups, who say Meta’s decision to scale back content moderation could lead to real-word harms. Meta CEO Mark Zuckerberg said Tuesday that the company will “remove restrictions on topics like immigration and gender that are out of touch with mainstream discourse,” citing “recent elections” as a catalyst.\u003c/p\u003e\u003cp\u003eFor instance, Meta has added the following to its rules — called community standards — that users are asked to abide by:\u003cbr/\u003e“We do allow allegations of mental illness or abnormality when based on gender or sexual orientation, given political and religious discourse about transgenderism and homosexuality and common non-serious usage of words like ‘weird.\u0026#39;” In other words, it is now permitted to call gay people mentally ill on Facebook, Threads and Instagram. Other slurs and what Meta calls “harmful stereotypes historically linked to intimidation” — such as Blackface and Holocaust denial — are still prohibited.\u003c/p\u003e\u003cp\u003eThe Menlo Park, California-based company also removed a sentence from its “policy rationale” explaining why it bans certain hateful conduct. The now-deleted sentence said that hate speech “creates an environment of intimidation and exclusion, and in some cases may promote offline violence.”\u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\u003cp\u003e“The policy change is a tactic to earn favor with the incoming administration while also reducing business costs related to content moderation,” said Ben Leiner, a lecturer at the University of Virginia’s Darden School of Business who studies political and technology trends. “This decision will lead to real-world harm, not only in the United States where there has been an uptick in hate speech and disinformation on social media platforms, but also abroad where disinformation on Facebook has accelerated ethnic conflict in places like Myanmar.”\u003c/p\u003e\u003cp\u003eMeta, in fact, acknowledged in 2018 that it didn’t do enough to prevent its platform from being used to “incite offline violence” in Myanmar, fueling communal hatred and violence against the country’s Muslim Rohingya minority.\u003c/p\u003e\u003cp\u003eArturo Béjar, a former engineering director at Meta known for his expertise on curbing online harassment, said while most of the attention has gone to the company’s fact-checking announcement Tuesday, he is more worried about the changes to Meta’s harmful content policies.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"content-chunk\"\u003e\u003cp\u003eThat’s because instead of proactively enforcing rules against things like self-harm, bullying and harassment, Meta will now rely on user reports before it takes any action. The company said it plans to focus its automated systems on “tackling illegal and high-severity violations, like terrorism, child sexual exploitation, drugs, fraud and scams.”\u003c/p\u003e\u003cp\u003eBéjar said that’s even though “Meta knows that by the time a report is submitted and reviewed the content will have done most of its harm.”\u003c/p\u003e\u003cp\u003e“I shudder to think what these changes will mean for our youth, Meta is abdicating their responsibility to safety, and we won’t the impact of these changes because Meta refuses to be transparent about the harms teenagers experience, and they go to extraordinary lengths to dilute or stop legislation that could help,” he said.\u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cem\u003eCopyright 2025. All rights reserved. This material may not be published, broadcast, rewritten or redistributed.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-01-09T00:21:20Z",
  "modifiedTime": "2025-01-09T00:21:23Z"
}
