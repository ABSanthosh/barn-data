{
  "id": "447daab2-2343-44bb-99f4-5f14c8b43299",
  "title": "Meta proposes new scalable memory layers that improve knowledge, reduce hallucinations",
  "link": "https://venturebeat.com/ai/meta-proposes-new-scalable-memory-layers-that-improve-knowledge-reduce-hallucinations/",
  "description": "According to Meta, memory layers may be the the answer to LLM hallucinations as they don't require huge compute resources at inference time.",
  "author": "Ben Dickson",
  "published": "Tue, 07 Jan 2025 21:45:05 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI research",
    "AI, ML and Deep Learning",
    "category-/Computers \u0026 Electronics",
    "deep learning",
    "large language models",
    "large language models (LLMs)",
    "LLMs",
    "Meta AI research",
    "mixture of experts"
  ],
  "byline": "Ben Dickson",
  "length": 5840,
  "excerpt": "According to Meta, memory layers may be the the answer to LLM hallucinations as they don't require huge compute resources at inference time.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "January 7, 2025 1:45 PM Image credit: VentureBeat generated with Ideogram Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More As enterprises continue to adopt large language models (LLMs) in various applications, one of the key challenges they face is improving the factual knowledge of models and reducing hallucinations. In a new paper, researchers at Meta AI propose “scalable memory layers,” which could be one of several possible solutions to this problem. Scalable memory layers add more parameters to LLMs to increase their learning capacity without requiring additional compute resources. The architecture is useful for applications where you can spare extra memory for factual knowledge but also want the inference speed of nimbler models. Dense and memory layers Traditional language models use “dense layers” to encode vast amounts of information in their parameters. In dense layers, all parameters are used at their full capacity and are mostly activated at the same time during inference. Dense layers can learn complex functions, and increasing their requires additional computational and energy resources.  In contrast, for simple factual knowledge, much simpler layers with associative memory architectures would be more efficient and interpretable. This is what memory layers do. They use simple sparse activations and key-value lookup mechanisms to encode and retrieve knowledge. Sparse layers take up more memory than dense layers but only use a small portion of the parameters at once, which makes them much more compute-efficient. Memory layers have existed for several years but are rarely used in modern deep learning architectures. They are not optimized for current hardware accelerators.  Current frontier LLMs usually use some form of “mixture of experts” (MoE) architecture, which uses a mechanism vaguely similar to memory layers. MoE models are composed of many smaller expert components that specialize in specific tasks. At inference time, a routing mechanism determines which expert becomes activated based on the input sequence. PEER, an architecture recently developed by Google DeepMind, extends MoE to millions of experts, providing more granular control over the parameters that become activated during inference. Upgrading memory layers Memory layers are light on compute but heavy on memory, which presents specific challenges for current hardware and software frameworks. In their paper, the Meta researchers propose several modifications that solve these challenges and make it possible to use them at scale. Memory layers can store knowledge in parallel across several GPUs without slowing down the model (source: arXiv) First, the researchers configured the memory layers for parallelization, distributing them across several GPUs to store millions of key-value pairs without changing other layers in the model. They also implemented a special CUDA kernel for handling high-memory bandwidth operations. And, they developed a parameter-sharing mechanism that supports a single set of memory parameters across multiple memory layers within a model. This means that the keys and values used for lookups are shared across layers. These modifications make it possible to implement memory layers within LLMs without slowing down the model. “Memory layers with their sparse activations nicely complement dense networks, providing increased capacity for knowledge acquisition while being light on compute,” the researchers write. “They can be efficiently scaled, and provide practitioners with an attractive new direction to trade-off memory with compute.” To test memory layers, the researchers modified Llama models by replacing one or more dense layers with a shared memory layer. They compared the memory-enhanced models against the dense LLMs as well as MoE and PEER models on several tasks, including factual question answering, scientific and common-sense world knowledge and coding. A 1.3B memory model (solid line) trained on 1 trillion tokens approaches the performance of a 7B model (dashed line) on factual question-answering tasks as it is given more memory parameters (source: arxiv) Their findings show that memory models improve significantly over dense baselines and compete with models that use 2X to 4X more compute. They also match the performance of MoE models that have the same compute budget and parameter count. The model’s performance is especially notable on tasks that require factual knowledge. For example, on factual question-answering, a memory model with 1.3 billion parameters approaches the performance of Llama-2-7B, which has been trained on twice as many tokens and 10X more compute.  Moreover, the researchers found that the benefits of memory models remain consistent with model size as they scaled their experiments from 134 million to 8 billion parameters. “Given these findings, we strongly advocate that memory layers should be integrated into all next generation AI architectures,” the researchers write, while adding that there is still a lot more room for improvement. “In particular, we hope that new learning methods can be developed to push the effectiveness of these layers even further, enabling less forgetting, fewer hallucinations and continual learning.” Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/01/robot-with-knowledge.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-01-07T21:45:05+00:00\" datetime=\"2025-01-07T21:45:05+00:00\"\u003eJanuary 7, 2025 1:45 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"421\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/robot-with-knowledge.webp?w=750\" alt=\"Image credit: VentureBeat generated with Ideogram\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat generated with Ideogram\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eAs enterprises continue to adopt large language models (LLMs) in various applications, one of the key challenges they face is improving the factual knowledge of models and reducing hallucinations. In a new paper, researchers at \u003ca href=\"https://ai.meta.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMeta AI\u003c/a\u003e propose “\u003ca href=\"https://arxiv.org/abs/2412.09764\" target=\"_blank\" rel=\"noreferrer noopener\"\u003escalable memory layers\u003c/a\u003e,” which could be one of several possible solutions to this problem.\u003c/p\u003e\n\n\n\n\u003cp\u003eScalable memory layers add more parameters to LLMs to increase their learning capacity without requiring additional compute resources. The architecture is useful for applications where you can spare extra memory for factual knowledge but also want the inference speed of nimbler models.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-dense-and-memory-layers\"\u003eDense and memory layers\u003c/h2\u003e\n\n\n\n\u003cp\u003eTraditional \u003ca href=\"https://venturebeat.com/ai/nvidias-ai-agent-play-is-here-with-new-models-orchestration-blueprints/\"\u003elanguage models\u003c/a\u003e use “dense layers” to encode vast amounts of information in their parameters. In dense layers, all parameters are used at their full capacity and are mostly activated at the same time during inference. Dense layers can learn complex functions, and increasing their requires additional computational and energy resources. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn contrast, for simple factual knowledge, much simpler layers with associative memory architectures would be more efficient and interpretable. This is what memory layers do. They use simple sparse activations and key-value lookup mechanisms to encode and retrieve knowledge. Sparse layers take up more memory than dense layers but only use a small portion of the parameters at once, which makes them much more compute-efficient.\u003c/p\u003e\n\n\n\n\u003cp\u003eMemory layers have existed for several years but are rarely used in modern deep learning architectures. They are not optimized for current hardware accelerators. \u003c/p\u003e\n\n\n\n\u003cp\u003eCurrent frontier LLMs usually use some form of “\u003ca href=\"https://venturebeat.com/ai/mistral-ai-drops-new-mixture-of-experts-model-with-a-torrent-link/\"\u003emixture of experts\u003c/a\u003e” (MoE) architecture, which uses a mechanism vaguely similar to memory layers. MoE models are composed of many smaller expert components that specialize in specific tasks. At inference time, a routing mechanism determines which expert becomes activated based on the input sequence. \u003ca href=\"https://venturebeat.com/ai/deepminds-peer-scales-language-models-with-millions-of-tiny-experts/\"\u003ePEER\u003c/a\u003e, an architecture recently developed by Google DeepMind, extends MoE to millions of experts, providing more granular control over the parameters that become activated during inference.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-upgrading-memory-layers\"\u003eUpgrading memory layers\u003c/h2\u003e\n\n\n\n\u003cp\u003eMemory layers are light on compute but heavy on memory, which presents specific challenges for current hardware and software frameworks. In their paper, the Meta researchers propose several modifications that solve these challenges and make it possible to use them at scale.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1200\" height=\"595\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png?w=800\" alt=\"Memory layers\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png 1200w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png?resize=300,149 300w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png?resize=768,381 768w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png?resize=800,397 800w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png?resize=100,50 100w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png?resize=350,175 350w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png?resize=400,198 400w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png?resize=750,372 750w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png?resize=578,287 578w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-layers.png?resize=930,461 930w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eMemory layers can store knowledge in parallel across several GPUs without slowing down the model (source: arXiv)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eFirst, the researchers configured the memory layers for parallelization, distributing them across several GPUs to store millions of key-value pairs without changing other layers in the model. They also implemented a special CUDA kernel for handling high-memory bandwidth operations. And, they developed a parameter-sharing mechanism that supports a single set of memory parameters across multiple memory layers within a model. This means that the keys and values used for lookups are shared across layers.\u003c/p\u003e\n\n\n\n\u003cp\u003eThese modifications make it possible to implement memory layers within LLMs without slowing down the model.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Memory layers with their sparse activations nicely complement dense networks, providing increased capacity for knowledge acquisition while being light on compute,” the researchers write. “They can be efficiently scaled, and provide practitioners with an attractive new direction to trade-off memory with compute.”\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cp\u003eTo test memory layers, the researchers modified \u003ca href=\"https://venturebeat.com/ai/meta-launches-open-source-llama-3-3-shrinking-powerful-bigger-model-into-smaller-size/\"\u003eLlama models\u003c/a\u003e by replacing one or more dense layers with a shared memory layer. They compared the memory-enhanced models against the dense LLMs as well as MoE and PEER models on several tasks, including factual question answering, scientific and common-sense world knowledge and coding.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1160\" height=\"980\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/Memory-model-vs-dense-layers.png?w=710\" alt=\"Memory model vs dense layers\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/Memory-model-vs-dense-layers.png 1160w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-model-vs-dense-layers.png?resize=300,253 300w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-model-vs-dense-layers.png?resize=768,649 768w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-model-vs-dense-layers.png?resize=710,600 710w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-model-vs-dense-layers.png?resize=400,338 400w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-model-vs-dense-layers.png?resize=750,634 750w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-model-vs-dense-layers.png?resize=578,488 578w, https://venturebeat.com/wp-content/uploads/2025/01/Memory-model-vs-dense-layers.png?resize=930,786 930w\" sizes=\"(max-width: 1160px) 100vw, 1160px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eA 1.3B memory model (solid line) trained on 1 trillion tokens approaches the performance of a 7B model (dashed line) on factual question-answering tasks as it is given more memory parameters (source: arxiv)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eTheir findings show that memory models improve significantly over dense baselines and compete with models that use 2X to 4X more compute. They also match the performance of MoE models that have the same compute budget and parameter count. The model’s performance is especially notable on tasks that require factual knowledge. For example, on factual question-answering, a memory model with 1.3 billion parameters approaches the performance of Llama-2-7B, which has been trained on twice as many tokens and 10X more compute. \u003c/p\u003e\n\n\n\n\u003cp\u003eMoreover, the researchers found that the benefits of memory models remain consistent with model size as they scaled their experiments from 134 million to 8 billion parameters.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Given these findings, we strongly advocate that memory layers should be integrated into all next generation AI architectures,” the researchers write, while adding that there is still a lot more room for improvement. “In particular, we hope that new learning methods can be developed to push the effectiveness of these layers even further, enabling less forgetting, fewer hallucinations and continual learning.”\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-01-07T21:45:05Z",
  "modifiedTime": "2025-01-07T21:45:11Z"
}
