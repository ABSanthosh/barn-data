{
  "id": "4452c1f4-552d-4cba-beef-07cc4e92a27d",
  "title": "Defending SOCs Under Siege: Battling Adversarial AI Attacks",
  "link": "https://venturebeat.com/security/defending-socs-battling-adversarial-attacks/",
  "description": "With 77% of enterprises victimized by adversarial AI, the question isn't if your Security Operations Center (SOC) will be targeted—it's when.",
  "author": "Louis Columbus",
  "published": "Mon, 09 Dec 2024 20:07:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "Security",
    "adversarial training",
    "AI cybersecurity",
    "AI risk management",
    "AI safety",
    "AI-generated threats",
    "category-/Law \u0026 Government/Public Safety/Law Enforcement",
    "cybersecurity",
    "Generative AI",
    "machine learning (ML)",
    "Supply chain security"
  ],
  "byline": "Louis Columbus",
  "length": 13536,
  "excerpt": "With 77% of enterprises victimized by adversarial AI, the question isn't if your Security Operations Center (SOC) will be targeted—it's when.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "December 9, 2024 12:07 PM Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More With 77% of enterprises already victimized by adversarial AI attacks and eCrime actors achieving a record breakout time of just 2 minutes and 7 seconds, the question isn’t if your Security Operations Center (SOC) will be targeted — it’s when. As cloud intrusions soared by 75% in the past year, and two in five enterprises suffered AI-related security breaches, every SOC leader needs to confront a brutal truth: Your defenses must either evolve as fast as the attackers’ tradecraft or risk being overrun by relentless, resourceful adversaries who pivot in seconds to succeed with a breach. Combining generative AI (gen AI), social engineering, interactive intrusion campaigns and an all-out assault on cloud vulnerabilities and identities, attackers are executing a playbook that seeks to capitalize on every SOC weakness they can find. CrowdStrike’s 2024 Global Threat Report finds that nation-state attackers are taking identity-based and social engineering attacks to a new level of intensity. Nation-states have long used machine learning to craft phishing and social engineering campaigns. Now, the focus is on pirating authentication tools and systems including API keys and one-time passwords (OTPs). “What we’re seeing is that the threat actors have really been focused on…taking a legitimate identity. Logging in as a legitimate user. And then laying low, staying under the radar by living off the land by using legitimate tools,” Adam Meyers, senior vice president counter adversary operations at CrowdStrike, told VentureBeat during a recent briefing.  Cybercrime gangs and nation-state cyberwar teams continue sharpening their tradecraft to launch AI-based attacks aimed at undermining the foundation of identity and access management (IAM) trust. By exploiting fake identities generated through deepfake voice, image and video data, these attacks aim to breach IAM systems and create chaos in a targeted organization. The Gartner figure below shows why SOC teams need to be prepared now for adversarial AI attacks, which most often take the form of fake identity attacks. Source: Gartner 2025 Planning Guide for Identity and Access Management. Published on October 14, 2024. Document ID: G00815708. Scoping the adversarial AI threat landscape going into 2025 “As gen AI continues to evolve, so must the understanding of its implications for cybersecurity,”  Bob Grazioli, CIO and senior vice president of Ivanti, recently told VentureBeat. “Undoubtedly, gen AI equips cybersecurity professionals with powerful tools, but it also provides attackers with advanced capabilities. To counter this, new strategies are needed to prevent malicious AI from becoming a dominant threat. This report helps equip organizations with the insights needed to stay ahead of advanced threats and safeguard their digital assets effectively,” Grazioli said. A recent Gartner survey revealed that 73% of enterprises have hundreds or thousands of AI models deployed, while 41% reported AI-related security incidents. According to HiddenLayer, seven in 10 companies have experienced AI-related breaches, with 60% linked to insider threats and 27% involving external attacks targeting AI infrastructure. Nir Zuk, CTO of Palo Alto Networks, framed it starkly in an interview with VentureBeat earlier this year: Machine learning assumes adversaries are already inside, and this demands real-time responsiveness to stealthy attacks. Researchers at Carnegie Mellon University recently published “Current State of LLM Risks and AI Guardrails,” a paper that explains the vulnerabilities of large language models (LLMs) in critical applications. It highlights risks such as bias, data poisoning and non-reproducibility. With security leaders and SOC teams increasingly collaborating on new model safety measures, the guidelines advocated by these researchers need to be part of SOC teams’ training and ongoing development. These guidelines include deploying layered protection models that integrate retrieval-augmented generation (RAG) and situational awareness tools to counter adversarial exploitation. SOC teams also carry the support burden for new gen AI applications, including the rapidly growing use of agentic AI. Researchers from the University of California, Davis recently published “Security of AI Agents,” a study examining the security challenges SOC teams face as AI agents execute real-world tasks. Threats including data integrity breaches and model pollution, where adversarial inputs may compromise the agent’s decisions and actions, are deconstructed and analyzed. To counter these risks, the researchers propose defenses such as having SOC teams initiate and manage sandboxing — limiting the agent’s operational scope — and encrypted workflows that protect sensitive interactions, creating a controlled environment to contain potential exploits. Why SOCs are targets of adversarial AI Dealing with alert fatigue, turnover of key staff, incomplete and inconsistent data on threats, and systems designed to protect perimeters and not identities, SOC teams are at a disadvantage against attackers’ growing AI arsenals. SOC leaders in financial services, insurance and manufacturing tell VentureBeat, under the condition of anonymity, that their companies are under siege, with a high number of high-risk alerts coming in every day. The techniques below focus on ways AI models can be compromised such that, once breached, they provide sensitive data and can be used to pivot to other systems and assets within the enterprise. Attackers’ tactics focus on establishing a foothold that leads to deeper network penetration. Data Poisoning: Attackers introduce malicious data into a model’s training set to degrade performance or control predictions. According to a Gartner report from 2023, nearly 30% of AI-enabled organizations, particularly those in finance and healthcare, have experienced such attacks. Backdoor attacks embed specific triggers in training data, causing models to behave incorrectly when these triggers appear in real-world inputs. A 2023 MIT study highlights the growing risk of such attacks as AI adoption grows, making defense strategies such as adversarial training increasingly important. Evasion Attacks: These attacks alter input data in order to mispredict. Slight image distortions can confuse models into misclassifying objects. A popular evasion method, the Fast Gradient Sign Method (FGSM), uses adversarial noise to trick models. Evasion attacks in the autonomous vehicle industry have caused safety concerns, with altered stop signs misinterpreted as yield signs. A 2019 study found that a small sticker on a stop sign misled a self-driving car into thinking it was a speed limit sign. Tencent’s Keen Security Lab used road stickers to trick a Tesla Model S’s autopilot system. These stickers steered the car into the wrong lane, showing how small, carefully crafted input changes can be dangerous. Adversarial attacks on critical systems like autonomous vehicles are real-world threats. Exploiting API vulnerabilities: Model-stealing and other adversarial attacks are highly effective against public APIs and are essential for obtaining AI model outputs. Many businesses are susceptible to exploitation because they lack strong API security, as was mentioned at BlackHat 2022. Vendors, including Checkmarx and Traceable AI, are automating API discovery and ending malicious bots to mitigate these risks. API security must be strengthened to preserve the integrity of AI models and safeguard sensitive data. Model Integrity and Adversarial Training: Without adversarial training, machine learning models can be manipulated. However, researchers say that while adversarial training improves robustness it requires longer training times and may trade accuracy for resilience. Although flawed, it is an essential defense against adversarial attacks. Researchers have also found that poor machine identity management in hybrid cloud environments increases the risk of adversarial attacks on machine learning models. Model Inversion: This type of attack allows adversaries to infer sensitive data from a model’s outputs, posing significant risks when trained on confidential data like health or financial records. Hackers query the model and use the responses to reverse-engineer training data. In 2023, Gartner warned, “The misuse of model inversion can lead to significant privacy violations, especially in healthcare and financial sectors, where adversaries can extract patient or customer information from AI systems.” Model Stealing: Repeated API queries can be used to replicate model functionality. These queries help the attacker create a surrogate model that behaves like the original. AI Security states, “AI models are often targeted through API queries to reverse-engineer their functionality, posing significant risks to proprietary systems, especially in sectors like finance, healthcare and autonomous vehicles.” These attacks are increasing as AI is used more, raising concerns about IP and trade secrets in AI models. Reinforcing SOC defenses through AI model hardening and supply chain security SOC teams need to think holistically about how a seemingly isolated breach of AL/ML models could quickly escalate into an enterprise-wide cyberattack. SOC leaders need to take the initiative and identify which security and risk management frameworks are the most complementary to their company’s business model. Great starting points are the NIST AI Risk Management Framework and the NIST AI Risk Management Framework and Playbook. VentureBeat is seeing that the following steps are delivering results by reinforcing defenses while also enhancing model reliability — two critical steps to securing a company’s infrastructure against adversarial AI attacks: Commit to continually hardening model architectures. Deploy gatekeeper layers to filter out malicious prompts and tie models to verified data sources. Address potential weak points at the pretraining stage so your models withstand even the most advanced adversarial tactics. Never stop strengthing data integrity and provenance: Never assume all data is trustworthy. Validate its origins, quality and integrity through rigorous checks and adversarial input testing. By ensuring only clean, reliable data enters the pipeline, SOCs can do their part to maintain the accuracy and credibility of outputs. Integrate adversarial validation and red-teaming: Don’t wait for attackers to find your blind spots. Continually pressure-test models against known and emerging threats. Use red teams to uncover hidden vulnerabilities, challenge assumptions and drive immediate remediation — ensuring defenses evolve in lockstep with attacker strategies. Enhance threat intelligence integration: SOC leaders need to support devops teams and help keep models in sync with current risks. SOC leaders need to provide devops teams with a steady stream of updated threat intelligence and simulate real-world attacker tactics using red-teaming. Increase and keep enforcing supply chain transparency: Identify and neutralize threats before they take root in codebases or pipelines. Regularly audit repositories, dependencies and CI/CD workflows. Treat every component as a potential risk, and use red-teaming to expose hidden gaps — fostering a secure, transparent supply chain. Employ privacy-preserving techniques and secure collaboration: Leverage techniques like federated learning and homomorphic encryption to let stakeholders contribute without revealing confidential information. This approach broadens AI expertise without increasing exposure. Implement session management, sandboxing, and zero trust starting with microsegmentation: Lock down access and movement across your network by segmenting sessions, isolating risky operations in sandboxed environments and strictly enforcing zero-trust principles. Under zero trust, no user, device or process is inherently trusted without verification. These measures curb lateral movement, containing threats at their point of origin. They safeguard system integrity, availability and confidentiality. In general, they have proven effective in stopping advanced adversarial AI attacks. Conclusion “CISO and CIO alignment will be critical in 2025,” Grazioli told VentureBeat. “Executives need to consolidate resources — budgets, personnel, data and technology — to enhance an organization’s security posture. A lack of data accessibility and visibility undermines AI investments. To address this, data silos between departments such as the CIO and CISO must be eliminated.” “In the coming year, we will need to view AI as an employee rather than a tool,” Grazioli noted. “For instance, prompt engineers must now anticipate the types of questions that would typically be asked of AI, highlighting how ingrained AI has become in everyday business activities. To ensure accuracy, AI will need to be trained and evaluated just like any other employee.” VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/Fortifying-SOC-operations-against-adversarial-AI-attacks-hero-1.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-12-09T20:07:00+00:00\" datetime=\"2024-12-09T20:07:00+00:00\"\u003eDecember 9, 2024 12:07 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"427\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/Fortifying-SOC-operations-against-adversarial-AI-attacks-hero-1.jpg?w=750\" alt=\"Fortifying SOC operations against adversarial AI attacks\"/\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eWith \u003ca href=\"https://hiddenlayer.com/threatreport2024/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e77%\u003c/a\u003e of enterprises already victimized by adversarial AI attacks and eCrime actors achieving a record breakout time of just \u003ca href=\"https://www.crowdstrike.com/en-us/global-threat-report/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e2 minutes and 7 seconds\u003c/a\u003e, the question isn’t if your Security Operations Center (SOC) will be targeted — it’s when.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs cloud intrusions soared by \u003ca href=\"https://www.crowdstrike.com/en-us/global-threat-report/\"\u003e75% in the past\u003c/a\u003e\u003ca href=\"https://www.crowdstrike.com/en-us/global-threat-report/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e \u003c/a\u003e\u003ca href=\"https://www.crowdstrike.com/en-us/global-threat-report/\"\u003eyear\u003c/a\u003e, and \u003ca href=\"https://higherlogicdownload.s3.amazonaws.com/ISACA/9c800c88-34d8-4fca-8bd2-5d46fd639822/UploadedImages/Audit_of_Machine_Learning_Reslience.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etwo in five enterprises suffered AI-related security breaches\u003c/a\u003e, every SOC leader needs to confront a brutal truth: Your defenses must either evolve as fast as the attackers’ tradecraft or risk being overrun by relentless, resourceful adversaries who pivot in seconds to succeed with a breach.\u003c/p\u003e\n\n\n\n\u003cp\u003eCombining generative AI (gen AI), social engineering, interactive intrusion campaigns and an all-out assault on cloud vulnerabilities and identities, attackers are executing a playbook that seeks to capitalize on every SOC weakness they can find. \u003ca href=\"https://www.crowdstrike.com/en-us/global-threat-report/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCrowdStrike’s 2024 Global Threat Report\u003c/a\u003e finds that nation-state attackers are taking identity-based and social engineering attacks to a new level of intensity. Nation-states have long used machine learning to craft phishing and social engineering campaigns. Now, the focus is on pirating authentication tools and systems including API keys and one-time passwords (OTPs).\u003c/p\u003e\n\n\n\n\u003cp\u003e“What we’re seeing is that the threat actors have really been focused on…taking a legitimate identity. Logging in as a legitimate user. And then laying low, staying under the radar by living off the land by using legitimate tools,” Adam Meyers, senior vice president counter adversary operations at CrowdStrike, told VentureBeat during a recent briefing. \u003c/p\u003e\n\n\n\n\u003cp\u003eCybercrime gangs and nation-state cyberwar teams continue sharpening their tradecraft to launch AI-based attacks aimed at undermining the foundation of identity and access management (IAM) trust. By exploiting fake identities generated through deepfake voice, image and video data, these attacks aim to breach IAM systems and create chaos in a targeted organization.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe \u003ca href=\"https://www.gartner.com/document-reader/document/5823247?ref=solrResearch\u0026amp;refval=442710420\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGartner\u003c/a\u003e figure below shows why SOC teams need to be prepared now for adversarial AI attacks, which most often take the form of fake identity attacks.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1280\" height=\"818\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/Figure_1_GenAI-Borne_Attack_on_the_IAM_Trust_Foundation.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/12/Figure_1_GenAI-Borne_Attack_on_the_IAM_Trust_Foundation.png 1280w, https://venturebeat.com/wp-content/uploads/2024/12/Figure_1_GenAI-Borne_Attack_on_the_IAM_Trust_Foundation.png?resize=300,192 300w, https://venturebeat.com/wp-content/uploads/2024/12/Figure_1_GenAI-Borne_Attack_on_the_IAM_Trust_Foundation.png?resize=768,491 768w, https://venturebeat.com/wp-content/uploads/2024/12/Figure_1_GenAI-Borne_Attack_on_the_IAM_Trust_Foundation.png?resize=800,511 800w, https://venturebeat.com/wp-content/uploads/2024/12/Figure_1_GenAI-Borne_Attack_on_the_IAM_Trust_Foundation.png?resize=400,256 400w, https://venturebeat.com/wp-content/uploads/2024/12/Figure_1_GenAI-Borne_Attack_on_the_IAM_Trust_Foundation.png?resize=750,479 750w, https://venturebeat.com/wp-content/uploads/2024/12/Figure_1_GenAI-Borne_Attack_on_the_IAM_Trust_Foundation.png?resize=578,369 578w, https://venturebeat.com/wp-content/uploads/2024/12/Figure_1_GenAI-Borne_Attack_on_the_IAM_Trust_Foundation.png?resize=930,594 930w\" sizes=\"(max-width: 1280px) 100vw, 1280px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eSource: Gartner 2025 Planning Guide for Identity and Access Management. Published on October 14, 2024. Document ID: G00815708.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-scoping-the-adversarial-ai-threat-landscape-going-into-2025\"\u003e\u003cstrong\u003eScoping the adversarial AI threat landscape going into 2025\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003e“As gen AI continues to evolve, so must the understanding of its implications for \u003ca href=\"https://venturebeat.com/category/security/\"\u003ecybersecurity\u003c/a\u003e,”  Bob Grazioli, CIO and senior vice president of \u003ca href=\"https://www.ivanti.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eIvanti\u003c/a\u003e, recently told VentureBeat.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Undoubtedly, gen AI equips cybersecurity professionals with powerful tools, but it also provides attackers with advanced capabilities. To counter this, new strategies are needed to prevent malicious AI from becoming a dominant threat. This report helps equip organizations with the insights needed to stay ahead of advanced threats and safeguard their digital assets effectively,” Grazioli said.\u003c/p\u003e\n\n\n\n\u003cp\u003eA recent Gartner survey revealed that \u003ca href=\"https://venturebeat.com/ai/new-gartner-survey-only-half-of-ai-models-make-it-into-production/?utm_source=chatgpt.com\"\u003e73%\u003c/a\u003e of enterprises have hundreds or thousands of AI models deployed, while 41% reported AI-related security incidents. According to \u003ca href=\"https://hiddenlayer.com/threatreport2024/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHiddenLayer\u003c/a\u003e, seven in 10 companies have experienced AI-related breaches, with 60% linked to insider threats and 27% involving external attacks targeting AI infrastructure.\u003c/p\u003e\n\n\n\n\u003cp\u003eNir Zuk, CTO of \u003ca href=\"https://www.paloaltonetworks.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePalo Alto Networks\u003c/a\u003e, framed it starkly in an \u003ca href=\"https://venturebeat.com/security/palo-alto-networks-cto-on-why-machine-learning-is-revolutionizing-soc-performance/\u0026#39;\"\u003einterview with VentureBeat\u003c/a\u003e earlier this year: Machine learning assumes adversaries are already inside, and this demands real-time responsiveness to stealthy attacks.\u003c/p\u003e\n\n\n\n\u003cp\u003eResearchers at \u003ca href=\"https://www.cmu.edu/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCarnegie Mellon University\u003c/a\u003e recently published “\u003ca href=\"https://arxiv.org/abs/2406.12934\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCurrent State of LLM Risks and AI Guardrails\u003c/a\u003e,” a paper that explains the vulnerabilities of large language models (LLMs) in critical applications. It highlights risks such as bias, data poisoning and non-reproducibility. With security leaders and SOC teams increasingly collaborating on new model safety measures, the guidelines advocated by these researchers need to be part of SOC teams’ training and ongoing development. These guidelines include deploying layered protection models that integrate retrieval-augmented generation (RAG) and situational awareness tools to counter adversarial exploitation.\u003c/p\u003e\n\n\n\n\u003cp\u003eSOC teams also carry the support burden for new gen AI applications, including the rapidly growing use of agentic AI. Researchers from the \u003ca href=\"https://www.ucdavis.edu/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eUniversity of California, Davis\u003c/a\u003e recently published “\u003ca href=\"https://arxiv.org/html/2406.08689v2\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSecurity of AI Agents\u003c/a\u003e,” a study examining the security challenges SOC teams face as AI agents execute real-world tasks. Threats including data integrity breaches and model pollution, where adversarial inputs may compromise the agent’s decisions and actions, are deconstructed and analyzed. To counter these risks, the researchers propose defenses such as having SOC teams initiate and manage sandboxing — limiting the agent’s operational scope — and encrypted workflows that protect sensitive interactions, creating a controlled environment to contain potential exploits.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-why-socs-are-targets-of-adversarial-ai\"\u003e\u003cstrong\u003eWhy SOCs are targets of adversarial AI\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eDealing with alert fatigue, turnover of key staff, incomplete and inconsistent data on threats, and systems designed to protect perimeters and not identities, SOC teams are at a disadvantage against attackers’ growing AI arsenals.\u003c/p\u003e\n\n\n\n\u003cp\u003eSOC leaders in financial services, insurance and manufacturing tell VentureBeat, under the condition of anonymity, that their companies are under siege, with a high number of high-risk alerts coming in every day.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe techniques below focus on ways AI models can be compromised such that, once breached, they provide sensitive data and can be used to pivot to other systems and assets within the enterprise. Attackers’ tactics focus on establishing a foothold that leads to deeper network penetration.\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eData Poisoning: \u003c/strong\u003eAttackers introduce malicious data into a model’s training set to degrade performance or control predictions. According to a Gartner report from 2023, nearly 30% of AI-enabled organizations, particularly those in finance and healthcare, have experienced such attacks. Backdoor attacks embed specific triggers in training data, causing models to behave incorrectly when these triggers appear in real-world inputs. A 2023 \u003ca href=\"https://www.csail.mit.edu/news/global-ai-adoption-outpacing-risk-understanding-warns-mit-csail\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMIT study\u003c/a\u003e highlights the growing risk of such attacks as AI adoption grows, making defense strategies such as adversarial training increasingly important.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEvasion Attacks: \u003c/strong\u003eThese attacks alter input data in order to mispredict. Slight image distortions can confuse models into misclassifying objects. A popular evasion method, the Fast Gradient Sign Method (FGSM), uses adversarial noise to trick models. Evasion attacks in the autonomous vehicle industry have caused safety concerns, with altered stop signs misinterpreted as yield signs. A 2019 study found that a small sticker on a stop sign misled a self-driving car into thinking it was a speed limit sign. \u003ca href=\"https://keenlab.tencent.com/en/whitepapers/Experimental_Security_Research_of_Tesla_Autopilot.pdf?utm_campaign=the_algorithm.unpaid.engagement\u0026amp;utm_source=hs_email\u0026amp;utm_medium=email\u0026amp;utm_content=71373464\u0026amp;_hsenc=p2ANqtz--JBcpulYc-GW10QtUBBH_VTXHIiaAgdM-w3SdhQ1uop_m2MwFNQK8b-uDQ6hEgwH-08IpeSACOY432EYgtoku-uYAOZA\u0026amp;_hsmi=71373464\"\u003eTencent’s Keen Security Lab\u003c/a\u003e used road stickers to trick a Tesla Model S’s autopilot system. These stickers steered the car into the wrong lane, showing how small, carefully crafted input changes can be dangerous. Adversarial attacks on critical systems like autonomous vehicles are real-world threats.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eExploiting API vulnerabilities:\u003c/strong\u003e Model-stealing and other adversarial attacks are highly effective against public APIs and are essential for obtaining AI model outputs. Many businesses are susceptible to exploitation because they lack strong API security, as was mentioned at BlackHat 2022. Vendors, including Checkmarx and Traceable AI, are automating API discovery and ending malicious bots to mitigate these risks. API security must be strengthened to preserve the integrity of AI models and safeguard sensitive data.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eModel Integrity and Adversarial Training: \u003c/strong\u003eWithout adversarial training, machine learning models can be manipulated. However, researchers say that while adversarial training improves robustness it requires longer training times and may trade accuracy for resilience. Although flawed, it is an essential defense against adversarial attacks. Researchers have also found that poor machine identity management in hybrid cloud environments increases the risk of adversarial attacks on machine learning models.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eModel Inversion: \u003c/strong\u003eThis type of attack allows adversaries to infer sensitive data from a model’s outputs, posing significant risks when trained on confidential data like health or financial records. Hackers query the model and use the responses to reverse-engineer training data. In 2023, \u003ca href=\"https://www.gartner.com/document/code/773522\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGartner warned\u003c/a\u003e, “The misuse of model inversion can lead to significant privacy violations, especially in healthcare and financial sectors, where adversaries can extract patient or customer information from AI systems.”\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eModel Stealing: \u003c/strong\u003eRepeated API queries can be used to replicate model functionality. These queries help the attacker create a surrogate model that behaves like the original. AI Security \u003ca href=\"https://www.bleepingcomputer.com/news/security/researchers-find-you-can-trick-self-driving-cars-by-defacing-street-signs/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003estates\u003c/a\u003e, “AI models are often targeted through API queries to reverse-engineer their functionality, posing significant risks to proprietary systems, especially in sectors like finance, healthcare and autonomous vehicles.” These attacks are increasing as AI is used more, raising concerns about IP and trade secrets in AI models.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch2 id=\"h-reinforcing-soc-defenses-through-ai-model-hardening-and-supply-chain-security\"\u003e\u003cstrong\u003eReinforcing SOC defenses through AI model hardening and supply chain security\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSOC teams need to think holistically about how a seemingly isolated breach of AL/ML models could quickly escalate into an enterprise-wide cyberattack. SOC leaders need to take the initiative and identify which security and risk management frameworks are the most complementary to their company’s business model. Great starting points are the \u003ca href=\"https://www.nist.gov/itl/ai-risk-management-framework\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eNIST AI Risk Management Framework\u003c/a\u003e and the \u003ca href=\"https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eNIST AI Risk Management Framework and Playbook\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eVentureBeat is seeing that the following steps are delivering results by reinforcing defenses while also enhancing model reliability — two critical steps to securing a company’s infrastructure against adversarial AI attacks:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCommit to continually hardening model architectures. \u003c/strong\u003eDeploy gatekeeper layers to filter out malicious prompts and tie models to verified data sources. Address potential weak points at the pretraining stage so your models withstand even the most advanced adversarial tactics.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eNever stop strengthing data integrity and provenance: \u003c/strong\u003eNever assume all data is trustworthy. Validate its origins, quality and integrity through rigorous checks and adversarial input testing. By ensuring only clean, reliable data enters the pipeline, SOCs can do their part to maintain the accuracy and credibility of outputs.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eIntegrate adversarial validation and red-teaming:\u003c/strong\u003e Don’t wait for attackers to find your blind spots. Continually pressure-test models against known and emerging threats. Use red teams to uncover hidden vulnerabilities, challenge assumptions and drive immediate remediation — ensuring defenses evolve in lockstep with attacker strategies.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eEnhance threat intelligence integration: \u003c/strong\u003eSOC leaders need to support devops teams and help keep models in sync with current risks. SOC leaders need to provide devops teams with a steady stream of updated threat intelligence and simulate real-world attacker tactics using red-teaming.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eIncrease and keep enforcing supply chain transparency:\u003c/strong\u003e Identify and neutralize threats before they take root in codebases or pipelines. Regularly audit repositories, dependencies and CI/CD workflows. Treat every component as a potential risk, and use red-teaming to expose hidden gaps — fostering a secure, transparent supply chain.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eEmploy privacy-preserving techniques and secure collaboration:\u003c/strong\u003e Leverage techniques like federated learning and homomorphic encryption to let stakeholders contribute without revealing confidential information. This approach broadens AI expertise without increasing exposure.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eImplement session management, sandboxing, and zero trust starting with microsegmentation:\u003c/strong\u003e Lock down access and movement across your network by segmenting sessions, isolating risky operations in sandboxed environments and strictly enforcing zero-trust principles. Under zero trust, no user, device or process is inherently trusted without verification. These measures curb lateral movement, containing threats at their point of origin. They safeguard system integrity, availability and confidentiality. In general, they have proven effective in stopping advanced adversarial AI attacks.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-conclusion\"\u003e\u003cstrong\u003eConclusion \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003e“CISO and CIO alignment will be critical in 2025,” Grazioli told VentureBeat. “Executives need to consolidate resources — budgets, personnel, data and technology — to enhance an organization’s security posture. A lack of data accessibility and visibility undermines AI investments. To address this, data silos between departments such as the CIO and CISO must be eliminated.”\u003c/p\u003e\n\n\n\n\u003cp\u003e“In the coming year, we will need to view AI as an employee rather than a tool,” Grazioli noted. “For instance, prompt engineers must now anticipate the types of questions that would typically be asked of AI, highlighting how ingrained AI has become in everyday business activities. To ensure accuracy, AI will need to be trained and evaluated just like any other employee.”\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2024-12-09T20:07:00Z",
  "modifiedTime": "2024-12-09T18:50:03Z"
}
