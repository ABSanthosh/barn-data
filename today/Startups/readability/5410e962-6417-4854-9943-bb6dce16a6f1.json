{
  "id": "5410e962-6417-4854-9943-bb6dce16a6f1",
  "title": "MiniMax unveils its own open source LLM with industry-leading 4M token context",
  "link": "https://venturebeat.com/ai/minimax-unveils-its-own-open-source-llm-with-industry-leading-4m-token-context/",
  "description": "The LLM, MiniMax-Text-o1, is of particular note for enabling up to 4 million tokens in its context window — equivalent to a small library.",
  "author": "Carl Franzen",
  "published": "Tue, 14 Jan 2025 23:46:27 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Agentic AI",
    "AI agents",
    "AI, ML and Deep Learning",
    "attention",
    "attention is all you need",
    "Conversational AI",
    "hailuo",
    "LLMs",
    "MiniMax",
    "NLP",
    "Singapore"
  ],
  "byline": "Carl Franzen",
  "length": 5565,
  "excerpt": "LLM MiniMax-Text-o1 is of particular note for enabling up to 4 million tokens in its context window — equivalent to a small library.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "January 14, 2025 3:46 PM Credit: VentureBeat made with Midjourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More MiniMax is perhaps today best known here in the U.S. as the Singaporean company behind Hailuo, a realistic, high-resolution generative AI video model that competes with Runway, OpenAI’s Sora and Luma AI’s Dream Machine. But the company has far more tricks up its sleeve: Today, for instance, it announced the release and open-sourcing of the MiniMax-01 series, a new family of models built to handle ultra-long contexts and enhance AI agent development. The series includes MiniMax-Text-01, a foundation large language model (LLM), and MiniMax-VL-01, a visual multi-modal model. A massive context window MiniMax-Text-o1, is of particular note for enabling up to 4 million tokens in its context window — equivalent to a small library’s worth of books. The context window is how much information the LLM can handle in one input/output exchange, with words and concepts represented as numerical “tokens,” the LLM’s own internal mathematical abstraction of the data it was trained on. And, while Google previously led the pack with its Gemini 1.5 Pro model and 2 million token context window, MiniMax remarkably doubled that. As MiniMax posted on its official X account today: “MiniMax-01 efficiently processes up to 4M tokens — 20 to 32 times the capacity of other leading models. We believe MiniMax-01 is poised to support the anticipated surge in agent-related applications in the coming year, as agents increasingly require extended context handling capabilities and sustained memory.” The models are available now for download on Hugging Face and Github under a custom MiniMax license, for users to try directly on Hailuo AI Chat (a ChatGPT/Gemini/Claude competitor), and through MiniMax’s application programming interface (API), where third-party developers can link their own unique apps to them. MiniMax is offering APIs for text and multi-modal processing at competitive rates: $0.2 per 1 million input tokens $1.1 per 1 million output tokens For comparison, OpenAI’s GPT-4o costs $2.50 per 1 million input tokens through its API, a staggering 12.5X more expensive. MiniMax has also integrated a mixture of experts (MoE) framework with 32 experts to optimize scalability. This design balances computational and memory efficiency while maintaining competitive performance on key benchmarks. Striking new ground with Lightning Attention Architecture At the heart of MiniMax-01 is a Lightning Attention mechanism, an innovative alternative to transformer architecture. This design significantly reduces computational complexity. The models consist of 456 billion parameters, with 45.9 billion activated per inference. Unlike earlier architectures, Lightning Attention employs a mix of linear and traditional SoftMax layers, achieving near-linear complexity for long inputs. SoftMax, for those like myself who are new to the concept, are the transformation of input numerals into probabilities adding up to 1, so that the LLM can approximate which meaning of the input is likeliest. MiniMax has rebuilt its training and inference frameworks to support the Lightning Attention architecture. Key improvements include: MoE all-to-all communication optimization: Reduces inter-GPU communication overhead. Varlen ring attention: Minimizes computational waste for long-sequence processing. Efficient kernel implementations: Tailored CUDA kernels improve Lightning Attention performance. These advancements make MiniMax-01 models accessible for real-world applications, while maintaining affordability. Performance and Benchmarks On mainstream text and multi-modal benchmarks, MiniMax-01 rivals top-tier models like GPT-4 and Claude-3.5, with especially strong results on long-context evaluations. Notably, MiniMax-Text-01 achieved 100% accuracy on the Needle-In-A-Haystack task with a 4-million-token context. The models also demonstrate minimal performance degradation as input length increases. MiniMax plans regular updates to expand the models’ capabilities, including code and multi-modal enhancements. The company views open-sourcing as a step toward building foundational AI capabilities for the evolving AI agent landscape. With 2025 predicted to be a transformative year for AI agents, the need for sustained memory and efficient inter-agent communication is increasing. MiniMax’s innovations are designed to meet these challenges. Open to collaboration MiniMax invites developers and researchers to explore the capabilities of MiniMax-01. Beyond open-sourcing, its team welcomes technical suggestions and collaboration inquiries at model@minimaxi.com. With its commitment to cost-effective and scalable AI, MiniMax positions itself as a key player in shaping the AI agent era. The MiniMax-01 series offers an exciting opportunity for developers to push the boundaries of what long-context AI can achieve. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/01/cfr0z3n_a_stunned_progammer_watches_as_an_entire_library_of_b_01f1129f-3f24-4bb6-93a3-d735ac143f70_1.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-01-14T23:46:27+00:00\" datetime=\"2025-01-14T23:46:27+00:00\"\u003eJanuary 14, 2025 3:46 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/cfr0z3n_a_stunned_progammer_watches_as_an_entire_library_of_b_01f1129f-3f24-4bb6-93a3-d735ac143f70_1.png?w=750\" alt=\"Flat illustration of a human figure watching as colorful square books fly into an oversized computer monitor\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eMiniMax is perhaps today best known here in the U.S. as the Singaporean company behind \u003ca href=\"https://venturebeat.com/ai/hailuo-gets-feature-competitive-launching-image-to-video-ai-generation-capability/\"\u003eHailuo\u003c/a\u003e, a realistic, high-resolution generative AI video model that competes with \u003ca href=\"https://venturebeat.com/ai/runway-inks-deal-with-lionsgate-in-first-team-up-for-ai-provider-and-major-movie-studio/\"\u003eRunway\u003c/a\u003e, \u003ca href=\"https://venturebeat.com/ai/open-ai-sora-launches/\"\u003eOpenAI’s Sora\u003c/a\u003e and\u003ca href=\"https://venturebeat.com/ai/luma-ai-debuts-dream-machine-for-realistic-video-generation-heating-up-ai-media-race/\"\u003e Luma AI’s Dream Machine.\u003c/a\u003e \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the company has far more tricks up its sleeve: Today, for instance, it announced the release and open-sourcing of the \u003ca href=\"https://minimaxi.com/en/news/minimax-01-series-2\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMiniMax-01 series\u003c/a\u003e, a new family of models built to handle ultra-long contexts and enhance AI agent development.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe series includes MiniMax-Text-01, a foundation large language model (LLM), and MiniMax-VL-01, a visual multi-modal model.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-a-massive-context-window\"\u003eA massive context window\u003c/h2\u003e\n\n\n\n\u003cp\u003eMiniMax-Text-o1, is of particular note for enabling up to 4 million tokens in its context window — equivalent to a \u003ca href=\"https://simple.ai/p/tokens-and-context-windows\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esmall library’s worth of books\u003c/a\u003e. The context window is how much information the LLM can handle in \u003ca href=\"https://zapier.com/blog/context-window/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eone input/output exchange\u003c/a\u003e, with words and concepts represented as numerical “tokens,” the LLM’s own internal mathematical abstraction of the data it was trained on.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnd, while Google previously led the pack with its Gemini 1.5 Pro model and \u003ca href=\"https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e2 million token context window\u003c/a\u003e, MiniMax remarkably doubled that. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs MiniMax \u003ca href=\"https://x.com/MiniMax__AI/status/1879226391352549451\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eposted on its official X account today\u003c/a\u003e: “MiniMax-01 efficiently processes up to 4M tokens — 20 to 32 times the capacity of other leading models. We believe MiniMax-01 is poised to support the anticipated surge in agent-related applications in the coming year, as agents increasingly require extended context handling capabilities and sustained memory.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe models are available now for download on \u003ca href=\"https://www.hailuo.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHugging Face\u003c/a\u003e and \u003ca href=\"https://github.com/MiniMax-AI/MiniMax-01\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGithub\u003c/a\u003e under a \u003ca href=\"https://github.com/MiniMax-AI/MiniMax-01/blob/main/LICENSE\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecustom MiniMax license\u003c/a\u003e, for users to try directly on \u003ca href=\"https://www.hailuo.ai/\"\u003eHailuo AI Chat\u003c/a\u003e (a ChatGPT/Gemini/Claude competitor), and through MiniMax’s \u003ca href=\"https://intl.minimaxi.com/document/platform%20introduction?key=66701c8e1d57f38758d58198\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eapplication programming interface (API)\u003c/a\u003e, where third-party developers can link their own unique apps to them.\u003c/p\u003e\n\n\n\n\u003cp\u003eMiniMax is offering APIs for text and multi-modal processing at competitive rates:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$0.2 per 1 million input tokens\u003c/strong\u003e\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003e$1.1 per 1 million output tokens\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eFor comparison, OpenAI’s GPT-4o costs \u003ca href=\"https://openai.com/api/pricing/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e$2.50 per 1 million input tokens\u003c/a\u003e through its API, a staggering 12.5X more expensive.\u003c/p\u003e\n\n\n\n\u003cp\u003eMiniMax has also integrated a mixture of experts (MoE) framework with 32 experts to optimize scalability. This design balances computational and memory efficiency while maintaining competitive performance on key benchmarks.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-striking-new-ground-with-lightning-attention-architecture\"\u003eStriking new ground with Lightning Attention Architecture \u003c/h2\u003e\n\n\n\n\u003cp\u003eAt the heart of MiniMax-01 is a Lightning Attention mechanism, an innovative alternative to transformer architecture.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis design significantly reduces computational complexity. The models consist of 456 billion parameters, with 45.9 billion activated per inference. \u003c/p\u003e\n\n\n\n\u003cp\u003eUnlike earlier architectures, Lightning Attention employs a mix of linear and traditional SoftMax layers, achieving near-linear complexity for long inputs. \u003ca href=\"https://medium.com/@sue_nlp/what-is-the-softmax-function-used-in-deep-learning-illustrated-in-an-easy-to-understand-way-8b937fe13d49\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSoftMax\u003c/a\u003e, for those like myself who are new to the concept, are the transformation of input numerals into probabilities adding up to 1, so that the LLM can approximate which meaning of the input is likeliest.\u003c/p\u003e\n\n\n\n\u003cp\u003eMiniMax has rebuilt its training and inference frameworks to support the Lightning Attention architecture. Key improvements include:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMoE all-to-all communication optimization\u003c/strong\u003e: Reduces inter-GPU communication overhead.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eVarlen ring attention\u003c/strong\u003e: Minimizes computational waste for long-sequence processing.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eEfficient kernel implementations\u003c/strong\u003e: Tailored CUDA kernels improve Lightning Attention performance.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eThese advancements make MiniMax-01 models accessible for real-world applications, while maintaining affordability.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-performance-and-benchmarks\"\u003ePerformance and Benchmarks\u003c/h2\u003e\n\n\n\n\u003cp\u003eOn mainstream text and multi-modal benchmarks, MiniMax-01 rivals top-tier models like GPT-4 and Claude-3.5, with especially strong results on long-context evaluations. Notably, MiniMax-Text-01 achieved 100% accuracy on the \u003ca href=\"https://github.com/gkamradt/LLMTest_NeedleInAHaystack\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eNeedle-In-A-Haystack task\u003c/a\u003e with a 4-million-token context.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe models also demonstrate minimal performance degradation as input length increases.\u003c/p\u003e\n\n\n\n\u003cp\u003eMiniMax plans regular updates to expand the models’ capabilities, including code and multi-modal enhancements.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe company views open-sourcing as a step toward building foundational AI capabilities for the evolving AI agent landscape. \u003c/p\u003e\n\n\n\n\u003cp\u003eWith 2025 predicted to be a transformative year for AI agents, the need for sustained memory and efficient inter-agent communication is increasing. MiniMax’s innovations are designed to meet these challenges.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-open-to-collaboration\"\u003eOpen to collaboration\u003c/h2\u003e\n\n\n\n\u003cp\u003eMiniMax invites developers and researchers to explore the capabilities of MiniMax-01. Beyond open-sourcing, its team welcomes technical suggestions and collaboration inquiries at \u003ca href=\"mailto:model@minimaxi.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emodel@minimaxi.com\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith its commitment to cost-effective and scalable AI, MiniMax positions itself as a key player in shaping the AI agent era. The MiniMax-01 series offers an exciting opportunity for developers to push the boundaries of what long-context AI can achieve.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-01-14T23:46:27Z",
  "modifiedTime": "2025-01-15T00:09:33Z"
}
