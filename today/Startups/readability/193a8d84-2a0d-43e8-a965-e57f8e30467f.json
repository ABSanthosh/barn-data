{
  "id": "193a8d84-2a0d-43e8-a965-e57f8e30467f",
  "title": "CubeCL: GPU Kernels in Rust for CUDA, ROCm, and WGPU",
  "link": "https://github.com/tracel-ai/cubecl",
  "description": "Article URL: https://github.com/tracel-ai/cubecl Comments URL: https://news.ycombinator.com/item?id=43777731 Points: 22 # Comments: 2",
  "author": "ashvardanian",
  "published": "Wed, 23 Apr 2025 23:19:32 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "tracel-ai",
  "length": 11792,
  "excerpt": "Multi-platform high-performance compute language extension for Rust. - tracel-ai/cubecl",
  "siteName": "GitHub",
  "favicon": "https://github.githubassets.com/assets/apple-touch-icon-180x180-a80b8e11abe2.png",
  "text": "TL;DR With CubeCL, you can program your GPU using Rust, taking advantage of zero-cost abstractions to develop maintainable, flexible, and efficient compute kernels. CubeCL currently fully supports functions, generics, and structs, with partial support for traits, methods and type inference. As the project evolves, we anticipate even broader support for Rust language primitives, all while maintaining optimal performance. Example Simply annotate functions with the cube attribute to indicate that they should run on the GPU. use cubecl::prelude::*; #[cube(launch_unchecked)] /// A [Line] represents a contiguous series of elements where SIMD operations may be available. /// The runtime will automatically use SIMD instructions when possible for improved performance. fn gelu_array\u003cF: Float\u003e(input: \u0026Array\u003cLine\u003cF\u003e\u003e, output: \u0026mut Array\u003cLine\u003cF\u003e\u003e) { if ABSOLUTE_POS \u003c input.len() { output[ABSOLUTE_POS] = gelu_scalar(input[ABSOLUTE_POS]); } } #[cube] fn gelu_scalar\u003cF: Float\u003e(x: Line\u003cF\u003e) -\u003e Line\u003cF\u003e { // Execute the sqrt function at comptime. let sqrt2 = F::new(comptime!(2.0f32.sqrt())); let tmp = x / Line::new(sqrt2); x * (Line::erf(tmp) + 1.0) / 2.0 } You can then launch the kernel using the autogenerated gelu_array::launch_unchecked function. pub fn launch\u003cR: Runtime\u003e(device: \u0026R::Device) { let client = R::client(device); let input = \u0026[-1., 0., 1., 5.]; let vectorization = 4; let output_handle = client.empty(input.len() * core::mem::size_of::\u003cf32\u003e()); let input_handle = client.create(f32::as_bytes(input)); unsafe { gelu_array::launch_unchecked::\u003cf32, R\u003e( \u0026client, CubeCount::Static(1, 1, 1), CubeDim::new(input.len() as u32 / vectorization, 1, 1), ArrayArg::from_raw_parts::\u003cf32\u003e(\u0026input_handle, input.len(), vectorization as u8), ArrayArg::from_raw_parts::\u003cf32\u003e(\u0026output_handle, input.len(), vectorization as u8), ) }; let bytes = client.read_one(output_handle.binding()); let output = f32::from_bytes(\u0026bytes); // Should be [-0.1587, 0.0000, 0.8413, 5.0000] println!(\"Executed gelu with runtime {:?} =\u003e {output:?}\", R::name()); } To see it in action, run the working GELU example with the following command: cargo run --example gelu --features cuda # cuda runtime cargo run --example gelu --features wgpu # wgpu runtime Runtime We support the following GPU runtimes: WGPU for cross-platform GPU support (Vulkan, Metal, DirectX, WebGPU) CUDA for NVIDIA GPU support ROCm/HIP for AMD GPU support (WIP) We also plan to develop an optimized JIT CPU runtime with SIMD instructions, leveraging Cranelift. Motivation The goal of CubeCL is to ease the pain of writing highly optimized compute kernels that are portable across hardware. There is currently no adequate solution when you want optimal performance while still being multi-platform. You either have to write custom kernels for different hardware, often with different languages such as CUDA, Metal, or ROCm. To fix this, we created a Just-in-Time compiler with three core features: automatic vectorization, comptime, and autotune! These features are extremely useful for anyone writing high-performance kernels, even when portability is not a concern. They improve code composability, reusability, testability, and maintainability, all while staying optimal. CubeCL also ships with a memory management strategy optimized for throughput with heavy buffer reuse to avoid allocations. Our goal extends beyond providing an optimized compute language; we aim to develop an ecosystem of high-performance and scientific computing in Rust. To achieve this, we're developing linear algebra components that you can integrate into your own kernels. We currently have an highly optimized matrix multiplication module, leveraging Tensor Cores on NVIDIA hardware where available, while gracefully falling back to basic instructions on other platforms. While there's room for improvement, particularly in using custom instructions from newer NVIDIA GPUs, our implementation already delivers impressive performance. This is just the beginning. We plan to include more utilities such as convolutions, random number generation, fast Fourier transforms, and other essential algorithms. We are a small team also building Burn, so don't hesitate to contribute and port algorithms; it can help more than you would imagine! How it works CubeCL leverages Rust's proc macro system in a unique two-step process: Parsing: The proc macro parses the GPU kernel code using the syn crate. Expansion: Instead of immediately generating an Intermediate Representation (IR), the macro generates a new Rust function. The generated function, semantically similar to the original, is responsible for creating the IR when called. This approach differs from traditional compilers, which typically generate IR directly after parsing. Our method enables several key features: Comptime: By not transforming the original code, it becomes remarkably easy to integrate compile-time optimizations. Automatic Vectorization: By simply vectorizing the inputs of a CubeCL function, we can determine the vectorization factor of each intermediate variable during the expansion. Rust Integration: The generated code remains valid Rust code, allowing it to be bundled without any dependency on the specific runtime. Design CubeCL is designed around - you guessed it - Cubes! More specifically, it's based on cuboids, because not all axes are the same size. Since all compute APIs need to map to the hardware, which are tiles that can be accessed using a 3D representation, our topology can easily be mapped to concepts from other APIs. CubeCL - Topology A cube is composed of units, so a 3x3x3 cube has 27 units that can be accessed by their positions along the x, y, and z axes. Similarly, a hyper-cube is composed of cubes, just as a cube is composed of units. Each cube in the hyper-cube can be accessed by its position relative to the hyper-cube along the x, y, and z axes. Hence, a hyper-cube of 3x3x3 will have 27 cubes. In this example, the total number of working units would be 27 x 27 = 729. Topology Equivalence ðŸ‘‡ Since all topology variables are constant within the kernel entry point, we chose to use the Rust constant syntax with capital letters. Often when creating kernels, we don't always care about the relative position of a unit within a cube along each axis, but often we only care about its position in general. Therefore, each kind of variable also has its own axis-independent variable, which is often not present in other languages. CubeCL CUDA WebGPU Metal CUBE_COUNT N/A N/A N/A CUBE_COUNT_X gridDim.x num_workgroups.x threadgroups_per_grid.x CUBE_COUNT_Y gridDim.y num_workgroups.y threadgroups_per_grid.y CUBE_COUNT_Z gridDim.z num_workgroups.z threadgroups_per_grid.z CUBE_POS N/A N/A N/A CUBE_POS_X blockIdx.x workgroup_id.x threadgroup_position_in_grid.x CUBE_POS_Y blockIdx.y workgroup_id.y threadgroup_position_in_grid.y CUBE_POS_Z blockIdx.z workgroup_id.z threadgroup_position_in_grid.z CUBE_DIM N/A N/A N/A CUBE_DIM_X blockDim.x workgroup_size.x threads_per_threadgroup.x CUBE_DIM_Y blockDim.y workgroup_size.y threads_per_threadgroup.y CUBE_DIM_Z blockDim.z workgroup_size.z threads_per_threadgroup.z UNIT_POS N/A local_invocation_index thread_index_in_threadgroup UNIT_POS_X threadIdx.x local_invocation_id.x thread_position_in_threadgroup.x UNIT_POS_Y threadIdx.y local_invocation_id.y thread_position_in_threadgroup.y UNIT_POS_Z threadIdx.z local_invocation_id.z thread_position_in_threadgroup.z PLANE_POS N/A subgroup_id simdgroup_index_in_threadgroup PLANE_DIM warpSize subgroup_size threads_per_simdgroup UNIT_POS_PLANE N/A subgroup_invocation_id thread_index_in_simdgroup ABSOLUTE_POS N/A N/A N/A ABSOLUTE_POS_X N/A global_id.x thread_position_in_grid.x ABSOLUTE_POS_Y N/A global_id.y thread_position_in_grid.y ABSOLUTE_POS_Z N/A global_id.z thread_position_in_grid.z Special Features Automatic Vectorization High-performance kernels should rely on SIMD instructions whenever possible, but doing so can quickly get pretty complicated! With CubeCL, you can specify the vectorization factor of each input variable when launching a kernel. Inside the kernel code, you still use only one type, which is dynamically vectorized and supports automatic broadcasting. The runtimes are able to compile kernels and have all the necessary information to use the best instruction! However, since the algorithmic behavior may depend on the vectorization factor, CubeCL allows you to access it directly in the kernel when needed, without any performance loss, using the comptime system! Comptime CubeCL isn't just a new compute language: though it feels like you are writing GPU kernels, you are, in fact, writing compiler plugins that you can fully customize! Comptime is a way to modify the compiler IR at runtime when compiling a kernel for the first time. This enables lots of optimizations and flexibility without having to write many separate variants of the same kernels to ensure maximal performance. Feature Description Instruction Specialization Not all instructions are available on all hardware, but when a specialized one exists, it should be enabled with a simple if statement. Automatic Vectorization When you can use SIMD instructions, you should! But since not all hardware supports the same vectorization factors, it can be injected at runtime! Loop Unrolling You may want multiple flavors of the same kernel, with loop unrolling for only a certain range of values. This can be configured easily with Comptime. Shape Specialization For deep learning kernels, it's often crucial to rely on different kernels for different input sizes; you can do it by passing the shape information as Comptime values. Compile Time Calculation In general, you can calculate a constant using Rust runtime properties and inject it into a kernel during its compilation, to avoid recalculating it during each execution. Autotuning Autotuning drastically simplifies kernel selection by running small benchmarks at runtime to figure out the best kernels with the best configurations to run on the current hardware; an essential feature for portability. This feature combines gracefully with comptime to test the effect of different comptime values on performance; sometimes it can be surprising! Even if the benchmarks may add some overhead when running the application for the first time, the information gets cached on the device and will be reused. It is usually a no-brainer trade-off for throughput-oriented programs such as deep learning models. You can even ship the autotune cache with your program, reducing cold start time when you have more control over the deployment target. Resource For now we don't have a lot of resources to learn, but you can look at the linear algebra library to see how CubeCL can be used. If you have any questions or want to contribute, don't hesitate to join the Discord. Disclaimer \u0026 History CubeCL is currently in alpha. While CubeCL is used in Burn, there are still a lot of rough edges; it isn't refined yet. The project started as a WebGPU-only backend for Burn. As we optimized it, we realized that we needed an intermediate representation (IR) that could be optimized then compiled to WGSL. Having an IR made it easy to support another compilation target, so we made a CUDA runtime. However, writing kernels directly in that IR wasn't easy, so we created a Rust frontend using the syn crate. Navigating the differences between CUDA and WebGPU, while leveraging both platforms, forced us to come up with general concepts that worked everywhere. Hence, CubeCL was born!",
  "image": "https://opengraph.githubassets.com/2a70e2553df2491a7a41024c6881271a8166cdd32e6ea075a8ae0ba311cadb90/tracel-ai/cubecl",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-hpc=\"true\"\u003e\u003carticle itemprop=\"text\"\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eTL;DR\u003c/h2\u003e\u003ca id=\"user-content-tldr\" aria-label=\"Permalink: TL;DR\" href=\"#tldr\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWith CubeCL, you can program your GPU using Rust, taking advantage of zero-cost abstractions to develop maintainable, flexible, and efficient compute kernels.\nCubeCL currently fully supports functions, generics, and structs, with partial support for traits, methods and type inference.\nAs the project evolves, we anticipate even broader support for Rust language primitives, all while maintaining optimal performance.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eExample\u003c/h3\u003e\u003ca id=\"user-content-example\" aria-label=\"Permalink: Example\" href=\"#example\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eSimply annotate functions with the \u003ccode\u003ecube\u003c/code\u003e attribute to indicate that they should run on the GPU.\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"use cubecl::prelude::*;\n\n#[cube(launch_unchecked)]\n/// A [Line] represents a contiguous series of elements where SIMD operations may be available.\n/// The runtime will automatically use SIMD instructions when possible for improved performance.\nfn gelu_array\u0026lt;F: Float\u0026gt;(input: \u0026amp;Array\u0026lt;Line\u0026lt;F\u0026gt;\u0026gt;, output: \u0026amp;mut Array\u0026lt;Line\u0026lt;F\u0026gt;\u0026gt;) {\n    if ABSOLUTE_POS \u0026lt; input.len() {\n        output[ABSOLUTE_POS] = gelu_scalar(input[ABSOLUTE_POS]);\n    }\n}\n\n#[cube]\nfn gelu_scalar\u0026lt;F: Float\u0026gt;(x: Line\u0026lt;F\u0026gt;) -\u0026gt; Line\u0026lt;F\u0026gt; {\n    // Execute the sqrt function at comptime.\n    let sqrt2 = F::new(comptime!(2.0f32.sqrt()));\n    let tmp = x / Line::new(sqrt2);\n\n    x * (Line::erf(tmp) + 1.0) / 2.0\n}\"\u003e\u003cpre\u003e\u003cspan\u003euse\u003c/span\u003e cubecl\u003cspan\u003e::\u003c/span\u003eprelude\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n\n\u003cspan\u003e#\u003cspan\u003e[\u003c/span\u003ecube\u003cspan\u003e(\u003c/span\u003elaunch_unchecked\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003c/span\u003e\n\u003cspan\u003e/// A [Line] represents a contiguous series of elements where SIMD operations may be available.\u003c/span\u003e\n\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e/// The runtime will automatically use SIMD instructions when possible for improved performance.\u003c/span\u003e\n\u003cspan\u003e\u003c/span\u003e\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003egelu_array\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eF\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eFloat\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003einput\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eArray\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eLine\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eF\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eoutput\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003emut\u003c/span\u003e \u003cspan\u003eArray\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eLine\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eF\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n    \u003cspan\u003eif\u003c/span\u003e \u003cspan\u003eABSOLUTE_POS\u003c/span\u003e \u0026lt; input\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n        output\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eABSOLUTE_POS\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e = \u003cspan\u003egelu_scalar\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003einput\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003eABSOLUTE_POS\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n    \u003cspan\u003e}\u003c/span\u003e\n\u003cspan\u003e}\u003c/span\u003e\n\n\u003cspan\u003e#\u003cspan\u003e[\u003c/span\u003ecube\u003cspan\u003e]\u003c/span\u003e\u003c/span\u003e\n\u003cspan\u003efn\u003c/span\u003e \u003cspan\u003egelu_scalar\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eF\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eFloat\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ex\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eLine\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eF\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e -\u0026gt; \u003cspan\u003eLine\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eF\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n    \u003cspan\u003e// Execute the sqrt function at comptime.\u003c/span\u003e\n    \u003cspan\u003elet\u003c/span\u003e sqrt2 = \u003cspan\u003eF\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003enew\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ecomptime\u003c/span\u003e\u003cspan\u003e!\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e2.0f32\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003esqrt\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n    \u003cspan\u003elet\u003c/span\u003e tmp = x / \u003cspan\u003eLine\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003enew\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003esqrt2\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n\n    x \u003cspan\u003e*\u003c/span\u003e \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eLine\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003eerf\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003etmp\u003cspan\u003e)\u003c/span\u003e + \u003cspan\u003e1.0\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e / \u003cspan\u003e2.0\u003c/span\u003e\n\u003cspan\u003e}\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eYou can then launch the kernel using the autogenerated \u003ccode\u003egelu_array::launch_unchecked\u003c/code\u003e function.\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"pub fn launch\u0026lt;R: Runtime\u0026gt;(device: \u0026amp;R::Device) {\n    let client = R::client(device);\n    let input = \u0026amp;[-1., 0., 1., 5.];\n    let vectorization = 4;\n    let output_handle = client.empty(input.len() * core::mem::size_of::\u0026lt;f32\u0026gt;());\n    let input_handle = client.create(f32::as_bytes(input));\n\n    unsafe {\n        gelu_array::launch_unchecked::\u0026lt;f32, R\u0026gt;(\n            \u0026amp;client,\n            CubeCount::Static(1, 1, 1),\n            CubeDim::new(input.len() as u32 / vectorization, 1, 1),\n            ArrayArg::from_raw_parts::\u0026lt;f32\u0026gt;(\u0026amp;input_handle, input.len(), vectorization as u8),\n            ArrayArg::from_raw_parts::\u0026lt;f32\u0026gt;(\u0026amp;output_handle, input.len(), vectorization as u8),\n        )\n    };\n\n    let bytes = client.read_one(output_handle.binding());\n    let output = f32::from_bytes(\u0026amp;bytes);\n\n    // Should be [-0.1587,  0.0000,  0.8413,  5.0000]\n    println!(\u0026#34;Executed gelu with runtime {:?} =\u0026gt; {output:?}\u0026#34;, R::name());\n}\"\u003e\u003cpre\u003e\u003cspan\u003epub\u003c/span\u003e \u003cspan\u003efn\u003c/span\u003e \u003cspan\u003elaunch\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eR\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eRuntime\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edevice\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003eR\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003eDevice\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n    \u003cspan\u003elet\u003c/span\u003e client = \u003cspan\u003eR\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003eclient\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003edevice\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n    \u003cspan\u003elet\u003c/span\u003e input = \u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e-\u003cspan\u003e1.\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e0.\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e1.\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e5.\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n    \u003cspan\u003elet\u003c/span\u003e vectorization = \u003cspan\u003e4\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n    \u003cspan\u003elet\u003c/span\u003e output_handle = client\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eempty\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003einput\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003e*\u003c/span\u003e core\u003cspan\u003e::\u003c/span\u003emem\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003esize_of\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003ef32\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n    \u003cspan\u003elet\u003c/span\u003e input_handle = client\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ecreate\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003ef32\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003eas_bytes\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003einput\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n\n    \u003cspan\u003eunsafe\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n        gelu_array\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003elaunch_unchecked\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003ef32\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eR\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n            \u003cspan\u003e\u0026amp;\u003c/span\u003eclient\u003cspan\u003e,\u003c/span\u003e\n            \u003cspan\u003eCubeCount\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003eStatic\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n            \u003cspan\u003eCubeDim\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003enew\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003einput\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003eas\u003c/span\u003e \u003cspan\u003eu32\u003c/span\u003e / vectorization\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n            \u003cspan\u003eArrayArg\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003efrom_raw_parts\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003ef32\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003einput_handle\u003cspan\u003e,\u003c/span\u003e input\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e vectorization \u003cspan\u003eas\u003c/span\u003e \u003cspan\u003eu8\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n            \u003cspan\u003eArrayArg\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003efrom_raw_parts\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003ef32\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003eoutput_handle\u003cspan\u003e,\u003c/span\u003e input\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elen\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e vectorization \u003cspan\u003eas\u003c/span\u003e \u003cspan\u003eu8\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n\n    \u003cspan\u003elet\u003c/span\u003e bytes = client\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eread_one\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003eoutput_handle\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ebinding\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n    \u003cspan\u003elet\u003c/span\u003e output = f32\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003efrom_bytes\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003ebytes\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n\n    \u003cspan\u003e// Should be [-0.1587,  0.0000,  0.8413,  5.0000]\u003c/span\u003e\n    \u003cspan\u003eprintln\u003c/span\u003e\u003cspan\u003e!\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;Executed gelu with runtime {:?} =\u0026gt; {output:?}\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eR\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003ename\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n\u003cspan\u003e}\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eTo see it in action, run the working GELU example with the following command:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"cargo run --example gelu --features cuda # cuda runtime\ncargo run --example gelu --features wgpu # wgpu runtime\"\u003e\u003cpre\u003ecargo run --example gelu --features cuda \u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e cuda runtime\u003c/span\u003e\ncargo run --example gelu --features wgpu \u003cspan\u003e\u003cspan\u003e#\u003c/span\u003e wgpu runtime\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eRuntime\u003c/h2\u003e\u003ca id=\"user-content-runtime\" aria-label=\"Permalink: Runtime\" href=\"#runtime\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWe support the following GPU runtimes:\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/gfx-rs/wgpu\"\u003eWGPU\u003c/a\u003e for cross-platform GPU support (Vulkan, Metal, DirectX, WebGPU)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://developer.nvidia.com/cuda-toolkit\" rel=\"nofollow\"\u003eCUDA\u003c/a\u003e for NVIDIA GPU support\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.amd.com/en/products/software/rocm.html\" rel=\"nofollow\"\u003eROCm/HIP\u003c/a\u003e for AMD GPU support (WIP)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003eWe also plan to develop an optimized JIT CPU runtime with SIMD instructions, leveraging \u003ca href=\"https://cranelift.dev\" rel=\"nofollow\"\u003eCranelift\u003c/a\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eMotivation\u003c/h2\u003e\u003ca id=\"user-content-motivation\" aria-label=\"Permalink: Motivation\" href=\"#motivation\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe goal of CubeCL is to ease the pain of writing highly optimized compute kernels that are portable across hardware.\nThere is currently no adequate solution when you want optimal performance while still being multi-platform.\nYou either have to write custom kernels for different hardware, often with different languages such as CUDA, Metal, or ROCm.\nTo fix this, we created a Just-in-Time compiler with three core features: \u003cstrong\u003eautomatic vectorization\u003c/strong\u003e, \u003cstrong\u003ecomptime\u003c/strong\u003e, and \u003cstrong\u003eautotune\u003c/strong\u003e!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThese features are extremely useful for anyone writing high-performance kernels, even when portability is not a concern.\nThey improve code composability, reusability, testability, and maintainability, all while staying optimal.\nCubeCL also ships with a memory management strategy optimized for throughput with heavy buffer reuse to avoid allocations.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eOur goal extends beyond providing an optimized compute language; we aim to develop an ecosystem of high-performance and scientific computing in Rust.\nTo achieve this, we\u0026#39;re developing linear algebra components that you can integrate into your own kernels.\nWe currently have an highly optimized matrix multiplication module, leveraging Tensor Cores on NVIDIA hardware where available, while gracefully falling back to basic instructions on other platforms.\nWhile there\u0026#39;s room for improvement, particularly in using custom instructions from newer NVIDIA GPUs, our implementation already delivers impressive performance.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThis is just the beginning.\nWe plan to include more utilities such as convolutions, random number generation, fast Fourier transforms, and other essential algorithms.\nWe are a small team also building \u003ca href=\"https://burn.dev\" rel=\"nofollow\"\u003eBurn\u003c/a\u003e, so don\u0026#39;t hesitate to contribute and port algorithms; it can help more than you would imagine!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eHow it works\u003c/h2\u003e\u003ca id=\"user-content-how-it-works\" aria-label=\"Permalink: How it works\" href=\"#how-it-works\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eCubeCL leverages Rust\u0026#39;s proc macro system in a unique two-step process:\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003eParsing: The proc macro parses the GPU kernel code using the syn crate.\u003c/li\u003e\n\u003cli\u003eExpansion: Instead of immediately generating an Intermediate Representation (IR), the macro generates a new Rust function.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"auto\"\u003eThe generated function, semantically similar to the original, is responsible for creating the IR when called.\nThis approach differs from traditional compilers, which typically generate IR directly after parsing.\nOur method enables several key features:\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eComptime\u003c/strong\u003e: By not transforming the original code, it becomes remarkably easy to integrate compile-time optimizations.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAutomatic Vectorization\u003c/strong\u003e: By simply vectorizing the inputs of a CubeCL function, we can determine the vectorization factor of each intermediate variable during the expansion.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRust Integration\u003c/strong\u003e: The generated code remains valid Rust code, allowing it to be bundled without any dependency on the specific runtime.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eDesign\u003c/h2\u003e\u003ca id=\"user-content-design\" aria-label=\"Permalink: Design\" href=\"#design\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eCubeCL is designed around - you guessed it - Cubes! More specifically, it\u0026#39;s based on cuboids, because not all axes are the same size.\nSince all compute APIs need to map to the hardware, which are tiles that can be accessed using a 3D representation, our topology can easily be mapped to concepts from other APIs.\u003c/p\u003e\n\u003cdiv dir=\"auto\"\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eCubeCL - Topology\u003c/h3\u003e\u003ca id=\"user-content-cubecl---topology\" aria-label=\"Permalink: CubeCL - Topology\" href=\"#cubecl---topology\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/tracel-ai/cubecl/blob/main/assets/cubecl.drawio.svg\"\u003e\u003cimg src=\"https://github.com/tracel-ai/cubecl/raw/main/assets/cubecl.drawio.svg\" width=\"100%\"/\u003e\u003c/a\u003e\n\u003cbr/\u003e\n\u003c/p\u003e\u003c/div\u003e\n\n\u003cp dir=\"auto\"\u003e\u003cem\u003eA cube is composed of units, so a 3x3x3 cube has 27 units that can be accessed by their positions along the x, y, and z axes.\nSimilarly, a hyper-cube is composed of cubes, just as a cube is composed of units.\nEach cube in the hyper-cube can be accessed by its position relative to the hyper-cube along the x, y, and z axes.\nHence, a hyper-cube of 3x3x3 will have 27 cubes.\nIn this example, the total number of working units would be 27 x 27 = 729.\u003c/em\u003e\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eTopology Equivalence ðŸ‘‡\u003c/summary\u003e\n\n\u003cp dir=\"auto\"\u003eSince all topology variables are constant within the kernel entry point, we chose to use the Rust constant syntax with capital letters.\nOften when creating kernels, we don\u0026#39;t always care about the relative position of a unit within a cube along each axis, but often we only care about its position in general.\nTherefore, each kind of variable also has its own axis-independent variable, which is often not present in other languages.\u003c/p\u003e\n\u003cbr/\u003e\n\u003cmarkdown-accessiblity-table\u003e\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eCubeCL\u003c/th\u003e\n\u003cth\u003eCUDA\u003c/th\u003e\n\u003cth\u003eWebGPU\u003c/th\u003e\n\u003cth\u003eMetal\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_COUNT\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_COUNT_X\u003c/td\u003e\n\u003ctd\u003egridDim.x\u003c/td\u003e\n\u003ctd\u003enum_workgroups.x\u003c/td\u003e\n\u003ctd\u003ethreadgroups_per_grid.x\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_COUNT_Y\u003c/td\u003e\n\u003ctd\u003egridDim.y\u003c/td\u003e\n\u003ctd\u003enum_workgroups.y\u003c/td\u003e\n\u003ctd\u003ethreadgroups_per_grid.y\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_COUNT_Z\u003c/td\u003e\n\u003ctd\u003egridDim.z\u003c/td\u003e\n\u003ctd\u003enum_workgroups.z\u003c/td\u003e\n\u003ctd\u003ethreadgroups_per_grid.z\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_POS\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_POS_X\u003c/td\u003e\n\u003ctd\u003eblockIdx.x\u003c/td\u003e\n\u003ctd\u003eworkgroup_id.x\u003c/td\u003e\n\u003ctd\u003ethreadgroup_position_in_grid.x\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_POS_Y\u003c/td\u003e\n\u003ctd\u003eblockIdx.y\u003c/td\u003e\n\u003ctd\u003eworkgroup_id.y\u003c/td\u003e\n\u003ctd\u003ethreadgroup_position_in_grid.y\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_POS_Z\u003c/td\u003e\n\u003ctd\u003eblockIdx.z\u003c/td\u003e\n\u003ctd\u003eworkgroup_id.z\u003c/td\u003e\n\u003ctd\u003ethreadgroup_position_in_grid.z\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_DIM\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_DIM_X\u003c/td\u003e\n\u003ctd\u003eblockDim.x\u003c/td\u003e\n\u003ctd\u003eworkgroup_size.x\u003c/td\u003e\n\u003ctd\u003ethreads_per_threadgroup.x\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_DIM_Y\u003c/td\u003e\n\u003ctd\u003eblockDim.y\u003c/td\u003e\n\u003ctd\u003eworkgroup_size.y\u003c/td\u003e\n\u003ctd\u003ethreads_per_threadgroup.y\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCUBE_DIM_Z\u003c/td\u003e\n\u003ctd\u003eblockDim.z\u003c/td\u003e\n\u003ctd\u003eworkgroup_size.z\u003c/td\u003e\n\u003ctd\u003ethreads_per_threadgroup.z\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUNIT_POS\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003elocal_invocation_index\u003c/td\u003e\n\u003ctd\u003ethread_index_in_threadgroup\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUNIT_POS_X\u003c/td\u003e\n\u003ctd\u003ethreadIdx.x\u003c/td\u003e\n\u003ctd\u003elocal_invocation_id.x\u003c/td\u003e\n\u003ctd\u003ethread_position_in_threadgroup.x\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUNIT_POS_Y\u003c/td\u003e\n\u003ctd\u003ethreadIdx.y\u003c/td\u003e\n\u003ctd\u003elocal_invocation_id.y\u003c/td\u003e\n\u003ctd\u003ethread_position_in_threadgroup.y\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUNIT_POS_Z\u003c/td\u003e\n\u003ctd\u003ethreadIdx.z\u003c/td\u003e\n\u003ctd\u003elocal_invocation_id.z\u003c/td\u003e\n\u003ctd\u003ethread_position_in_threadgroup.z\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ePLANE_POS\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003esubgroup_id\u003c/td\u003e\n\u003ctd\u003esimdgroup_index_in_threadgroup\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ePLANE_DIM\u003c/td\u003e\n\u003ctd\u003ewarpSize\u003c/td\u003e\n\u003ctd\u003esubgroup_size\u003c/td\u003e\n\u003ctd\u003ethreads_per_simdgroup\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUNIT_POS_PLANE\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003esubgroup_invocation_id\u003c/td\u003e\n\u003ctd\u003ethread_index_in_simdgroup\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eABSOLUTE_POS\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eABSOLUTE_POS_X\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eglobal_id.x\u003c/td\u003e\n\u003ctd\u003ethread_position_in_grid.x\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eABSOLUTE_POS_Y\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eglobal_id.y\u003c/td\u003e\n\u003ctd\u003ethread_position_in_grid.y\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eABSOLUTE_POS_Z\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003eglobal_id.z\u003c/td\u003e\n\u003ctd\u003ethread_position_in_grid.z\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\u003c/markdown-accessiblity-table\u003e\n\u003c/details\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eSpecial Features\u003c/h2\u003e\u003ca id=\"user-content-special-features\" aria-label=\"Permalink: Special Features\" href=\"#special-features\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eAutomatic Vectorization\u003c/h3\u003e\u003ca id=\"user-content-automatic-vectorization\" aria-label=\"Permalink: Automatic Vectorization\" href=\"#automatic-vectorization\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eHigh-performance kernels should rely on SIMD instructions whenever possible, but doing so can quickly get pretty complicated!\nWith CubeCL, you can specify the vectorization factor of each input variable when launching a kernel.\nInside the kernel code, you still use only one type, which is dynamically vectorized and supports automatic broadcasting.\nThe runtimes are able to compile kernels and have all the necessary information to use the best instruction!\nHowever, since the algorithmic behavior may depend on the vectorization factor, CubeCL allows you to access it directly in the kernel when needed, without any performance loss, using the comptime system!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eComptime\u003c/h3\u003e\u003ca id=\"user-content-comptime\" aria-label=\"Permalink: Comptime\" href=\"#comptime\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eCubeCL isn\u0026#39;t just a new compute language: though it feels like you are writing GPU kernels, you are, in fact, writing compiler plugins that you can fully customize!\nComptime is a way to modify the compiler IR at runtime when compiling a kernel for the first time.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThis enables lots of optimizations and flexibility without having to write many separate variants of the same kernels to ensure maximal performance.\u003c/p\u003e\n\u003cmarkdown-accessiblity-table\u003e\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFeature\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eInstruction Specialization\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eNot all instructions are available on all hardware, but when a specialized one exists, it should be enabled with a simple if statement.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eAutomatic Vectorization\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eWhen you can use SIMD instructions, you should! But since not all hardware supports the same vectorization factors, it can be injected at runtime!\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eLoop Unrolling\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eYou may want multiple flavors of the same kernel, with loop unrolling for only a certain range of values. This can be configured easily with Comptime.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eShape Specialization\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eFor deep learning kernels, it\u0026#39;s often crucial to rely on different kernels for different input sizes; you can do it by passing the shape information as Comptime values.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eCompile Time Calculation\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eIn general, you can calculate a constant using Rust runtime properties and inject it into a kernel during its compilation, to avoid recalculating it during each execution.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\u003c/markdown-accessiblity-table\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eAutotuning\u003c/h3\u003e\u003ca id=\"user-content-autotuning\" aria-label=\"Permalink: Autotuning\" href=\"#autotuning\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eAutotuning drastically simplifies kernel selection by running small benchmarks at runtime to figure out the best kernels with the best configurations to run on the current hardware; an essential feature for portability.\nThis feature combines gracefully with comptime to test the effect of different comptime values on performance; sometimes it can be surprising!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eEven if the benchmarks may add some overhead when running the application for the first time, the information gets cached on the device and will be reused.\nIt is usually a no-brainer trade-off for throughput-oriented programs such as deep learning models.\nYou can even ship the autotune cache with your program, reducing cold start time when you have more control over the deployment target.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eResource\u003c/h2\u003e\u003ca id=\"user-content-resource\" aria-label=\"Permalink: Resource\" href=\"#resource\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eFor now we don\u0026#39;t have a lot of resources to learn, but you can look at the \u003ca href=\"https://github.com/tracel-ai/cubecl/blob/main/crates/cubecl-linalg/README.md\"\u003elinear algebra library\u003c/a\u003e to see how CubeCL can be used.\nIf you have any questions or want to contribute, don\u0026#39;t hesitate to join the \u003ca href=\"https://discord.gg/KSBSPhAUCc\" rel=\"nofollow\"\u003eDiscord\u003c/a\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eDisclaimer \u0026amp; History\u003c/h2\u003e\u003ca id=\"user-content-disclaimer--history\" aria-label=\"Permalink: Disclaimer \u0026amp; History\" href=\"#disclaimer--history\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eCubeCL is currently in \u003cstrong\u003ealpha\u003c/strong\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWhile CubeCL is used in \u003ca href=\"https://burn.dev\" rel=\"nofollow\"\u003eBurn\u003c/a\u003e, there are still a lot of rough edges; it isn\u0026#39;t refined yet.\nThe project started as a WebGPU-only backend for Burn.\nAs we optimized it, we realized that we needed an intermediate representation (IR) that could be optimized then compiled to WGSL.\nHaving an IR made it easy to support another compilation target, so we made a CUDA runtime.\nHowever, writing kernels directly in that IR wasn\u0026#39;t easy, so we created a Rust frontend using the \u003ca href=\"https://github.com/dtolnay/syn\"\u003esyn\u003c/a\u003e crate.\nNavigating the differences between CUDA and WebGPU, while leveraging both platforms, forced us to come up with general concepts that worked everywhere.\nHence, CubeCL was born!\u003c/p\u003e\n\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": null,
  "modifiedTime": null
}
