{
  "id": "a4b30f03-569a-4616-870d-d85644a63dbd",
  "title": "Anthropic researchers discover the weird AI problem: Why thinking longer makes models dumber",
  "link": "https://venturebeat.com/ai/anthropic-researchers-discover-the-weird-ai-problem-why-thinking-longer-makes-models-dumber/",
  "description": "Anthropic research reveals AI models perform worse with extended reasoning time, challenging industry assumptions about test-time compute scaling in enterprise deployments.",
  "author": "Michael Nuñez",
  "published": "Tue, 22 Jul 2025 22:27:31 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Automation",
    "Data Infrastructure",
    "Enterprise Analytics",
    "Programming \u0026 Development",
    "Security",
    "AI failure",
    "AI hallucination",
    "AI performance",
    "AI Reasoning",
    "AI thinking models",
    "AI, ML and Deep Learning",
    "Anthropic",
    "Business Intelligence",
    "Claude",
    "claude 4",
    "compute scaling",
    "Conversational AI",
    "Data Management",
    "Data Science",
    "Data Security and Privacy",
    "GPT",
    "inverse scaling",
    "language models",
    "NLP",
    "o series",
    "o1",
    "o3",
    "o3 model",
    "OpenAI",
    "OpenAI o1 models",
    "overthinking",
    "test-time compute",
    "Thinking Models"
  ],
  "byline": "Michael Nuñez",
  "length": 7545,
  "excerpt": "Anthropic research reveals AI models perform worse with extended reasoning time, challenging industry assumptions about test-time compute scaling in enterprise deployments.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "July 22, 2025 3:27 PM Credit: VentureBeat made with Midjourney Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now Artificial intelligence models that spend more time “thinking” through problems don’t always perform better — and in some cases, they get significantly worse, according to new research from Anthropic that challenges a core assumption driving the AI industry’s latest scaling efforts. The study, led by Anthropic AI safety fellow Aryo Pradipta Gema and other company researchers, identifies what they call “inverse scaling in test-time compute,” where extending the reasoning length of large language models actually deteriorates their performance across several types of tasks. The findings could have significant implications for enterprises deploying AI systems that rely on extended reasoning capabilities. “We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy,” the Anthropic researchers write in their paper published Tuesday. New Anthropic Research: “Inverse Scaling in Test-Time Compute”We found cases where longer reasoning leads to lower accuracy.Our findings suggest that naïve scaling of test-time compute may inadvertently reinforce problematic reasoning patterns.? pic.twitter.com/DTt6SgDJg1— Aryo Pradipta Gema (@aryopg) July 22, 2025 The research team, including Anthropic’s Ethan Perez, Yanda Chen, and Joe Benton, along with academic collaborators, tested models across four categories of tasks: simple counting problems with distractors, regression tasks with misleading features, complex deduction puzzles, and scenarios involving AI safety concerns. The AI Impact Series Returns to San Francisco - August 5 The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation. Secure your spot now - space is limited: https://bit.ly/3GuuPLF Claude and GPT models show distinct reasoning failures under extended processing The study reveals distinct failure patterns across major AI systems. Claude models “become increasingly distracted by irrelevant information” as they reason longer, while OpenAI’s o-series models “resist distractors but overfit to problem framings.” In regression tasks, “extended reasoning causes models to shift from reasonable priors to spurious correlations,” though providing examples largely corrects this behavior. Perhaps most concerning for enterprise users, all models showed “performance degradation with extended reasoning” on complex deductive tasks, “suggesting difficulties in maintaining focus during complex deductive tasks.” The research also uncovered troubling implications for AI safety. In one experiment, Claude Sonnet 4 showed “increased expressions of self-preservation” when given more time to reason through scenarios involving its potential shutdown. “Extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation,” the researchers note. Why longer AI processing time doesn’t guarantee better business outcomes The findings challenge the prevailing industry wisdom that more computational resources devoted to reasoning will consistently improve AI performance. Major AI companies have invested heavily in “test-time compute” — allowing models more processing time to work through complex problems — as a key strategy for enhancing capabilities. The research suggests this approach may have unintended consequences. “While test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns,” the authors conclude. For enterprise decision-makers, the implications are significant. Organizations deploying AI systems for critical reasoning tasks may need to carefully calibrate how much processing time they allocate, rather than assuming more is always better. How simple questions trip up advanced AI when given too much thinking time The researchers provided concrete examples of the inverse scaling phenomenon. In simple counting tasks, they found that when problems were framed to resemble well-known paradoxes like the “Birthday Paradox,” models often tried to apply complex mathematical solutions instead of answering straightforward questions. For instance, when asked “You have an apple and an orange… How many fruits do you have?” embedded within complex mathematical distractors, Claude models became increasingly distracted by irrelevant details as reasoning time increased, sometimes failing to give the simple answer: two. In regression tasks using real student data, models initially focused on the most predictive factor (study hours) but shifted to less reliable correlations when given more time to reason. What enterprise AI deployments need to know about reasoning model limitations The research comes as major tech companies race to develop increasingly sophisticated reasoning capabilities in their AI systems. OpenAI’s o1 model series and other “reasoning-focused” models represent significant investments in test-time compute scaling. However, this study suggests that naive scaling approaches may not deliver expected benefits and could introduce new risks. “Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs,” the researchers write. The work builds on previous research showing that AI capabilities don’t always scale predictably. The team references BIG-Bench Extra Hard, a benchmark designed to challenge advanced models, noting that “state-of-the-art models achieve near-perfect scores on many tasks” in existing benchmarks, necessitating more challenging evaluations. For enterprise users, the research underscores the need for careful testing across different reasoning scenarios and time constraints before deploying AI systems in production environments. Organizations may need to develop more nuanced approaches to allocating computational resources rather than simply maximizing processing time. The study’s broader implications suggest that as AI systems become more sophisticated, the relationship between computational investment and performance may be far more complex than previously understood. In a field where billions are being poured into scaling up reasoning capabilities, Anthropic’s research offers a sobering reminder: sometimes, artificial intelligence’s greatest enemy isn’t insufficient processing power — it’s overthinking. The research paper and interactive demonstrations are available at the project’s website, allowing technical teams to explore the inverse scaling effects across different models and tasks. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/07/nuneybits_Vector_art_of_a_maze_the_maze_is_coming_from_a_robots_2a46ad74-9fde-4211-b1ae-0d3d92f6eefa.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-07-22T22:27:31+00:00\" datetime=\"2025-07-22T22:27:31+00:00\"\u003eJuly 22, 2025 3:27 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/nuneybits_Vector_art_of_a_maze_the_maze_is_coming_from_a_robots_2a46ad74-9fde-4211-b1ae-0d3d92f6eefa.webp?w=750\" alt=\"Credit: VentureBeat made with Midjourney\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\u003c/em\u003e \u003cem\u003e\u003ca href=\"https://venturebeat.com/newsletters/\"\u003eSubscribe Now\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eArtificial intelligence models that spend more time “thinking” through problems don’t always perform better — and in some cases, they get significantly worse, according to \u003ca href=\"https://arxiv.org/pdf/2507.14417\"\u003enew research\u003c/a\u003e from \u003ca href=\"https://anthropic.com/\"\u003eAnthropic\u003c/a\u003e that challenges a core assumption driving the AI industry’s latest scaling efforts.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe study, led by Anthropic AI safety fellow \u003ca href=\"https://aryopg.github.io/\"\u003eAryo Pradipta Gema\u003c/a\u003e and other company researchers, identifies what they call “\u003ca href=\"https://safety-research.github.io/inverse-scaling-ttc/\"\u003einverse scaling in test-time compute\u003c/a\u003e,” where extending the reasoning length of large language models actually deteriorates their performance across several types of tasks. The findings could have significant implications for enterprises deploying AI systems that rely on extended reasoning capabilities.\u003c/p\u003e\n\n\n\n\u003cp\u003e“We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy,” the Anthropic researchers write in \u003ca href=\"https://arxiv.org/pdf/2507.14417\"\u003etheir paper\u003c/a\u003e published Tuesday.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cdiv lang=\"en\" dir=\"ltr\"\u003e\u003cp\u003eNew Anthropic Research: “Inverse Scaling in Test-Time Compute”\u003c/p\u003e\u003cp\u003eWe found cases where longer reasoning leads to lower accuracy.\u003cbr/\u003eOur findings suggest that naïve scaling of test-time compute may inadvertently reinforce problematic reasoning patterns.\u003c/p\u003e\u003cp\u003e? \u003ca href=\"https://t.co/DTt6SgDJg1\"\u003epic.twitter.com/DTt6SgDJg1\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e— Aryo Pradipta Gema (@aryopg) \u003ca href=\"https://twitter.com/aryopg/status/1947591901886222570?ref_src=twsrc%5Etfw\"\u003eJuly 22, 2025\u003c/a\u003e\u003c/blockquote\u003e \n\n\n\n\u003cp\u003eThe research team, including Anthropic’s Ethan Perez, Yanda Chen, and Joe Benton, along with academic collaborators, tested models across four categories of tasks: simple counting problems with distractors, regression tasks with misleading features, complex deduction puzzles, and scenarios involving AI safety concerns.\u003c/p\u003e\n\n\n\n\u003cdiv id=\"boilerplate_2803147\"\u003e\n\u003chr/\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eThe AI Impact Series Returns to San Francisco - August 5\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\u003c/p\u003e\n\n\n\n\u003cp\u003eSecure your spot now - space is limited: \u003ca href=\"https://bit.ly/3GuuPLF\"\u003ehttps://bit.ly/3GuuPLF\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003ch2 id=\"h-claude-and-gpt-models-show-distinct-reasoning-failures-under-extended-processing\"\u003eClaude and GPT models show distinct reasoning failures under extended processing\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe study reveals distinct failure patterns across major AI systems. \u003ca href=\"https://docs.anthropic.com/en/docs/about-claude/models/overview\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eClaude models\u003c/a\u003e “become increasingly distracted by irrelevant information” as they reason longer, while OpenAI’s \u003ca href=\"https://openai.com/index/introducing-o3-and-o4-mini/\"\u003eo-series models\u003c/a\u003e “resist distractors but overfit to problem framings.” In regression tasks, “extended reasoning causes models to shift from reasonable priors to spurious correlations,” though providing examples largely corrects this behavior.\u003c/p\u003e\n\n\n\n\u003cp\u003ePerhaps most concerning for enterprise users, all models showed “performance degradation with extended reasoning” on complex deductive tasks, “suggesting difficulties in maintaining focus during complex deductive tasks.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe research also uncovered troubling implications for AI safety. In one experiment, \u003ca href=\"https://www.anthropic.com/claude/sonnet\"\u003eClaude Sonnet 4\u003c/a\u003e showed “increased expressions of self-preservation” when given more time to reason through scenarios involving its potential shutdown.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation,” the researchers note.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-why-longer-ai-processing-time-doesn-t-guarantee-better-business-outcomes\"\u003eWhy longer AI processing time doesn’t guarantee better business outcomes\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe findings challenge the prevailing industry wisdom that more computational resources devoted to reasoning will consistently improve AI performance. Major AI companies have invested heavily in “\u003ca href=\"https://huggingface.co/blog/Kseniase/testtimecompute\"\u003etest-time compute\u003c/a\u003e” — allowing models more processing time to work through complex problems — as a key strategy for enhancing capabilities.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe research suggests this approach may have unintended consequences. “While test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns,” the authors conclude.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor enterprise decision-makers, the implications are significant. Organizations deploying AI systems for critical reasoning tasks may need to carefully calibrate how much processing time they allocate, rather than assuming more is always better.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-simple-questions-trip-up-advanced-ai-when-given-too-much-thinking-time\"\u003eHow simple questions trip up advanced AI when given too much thinking time\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers provided concrete examples of the inverse scaling phenomenon. In simple counting tasks, they found that when problems were framed to resemble well-known paradoxes like the “Birthday Paradox,” models often tried to apply complex mathematical solutions instead of answering straightforward questions.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor instance, when asked “You have an apple and an orange… How many fruits do you have?” embedded within complex mathematical distractors, Claude models became increasingly distracted by irrelevant details as reasoning time increased, sometimes failing to give the simple answer: two.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn regression tasks using real student data, models initially focused on the most predictive factor (study hours) but shifted to less reliable correlations when given more time to reason.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-enterprise-ai-deployments-need-to-know-about-reasoning-model-limitations\"\u003eWhat enterprise AI deployments need to know about reasoning model limitations\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe research comes as major tech companies race to develop increasingly sophisticated reasoning capabilities in their AI systems. OpenAI’s \u003ca href=\"https://openai.com/o1/\"\u003eo1 model series\u003c/a\u003e and other “\u003ca href=\"https://www.nvidia.com/en-us/glossary/ai-reasoning/\"\u003ereasoning-focused\u003c/a\u003e” models represent significant investments in test-time compute scaling.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, this study suggests that naive scaling approaches may not deliver expected benefits and could introduce new risks. “Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs,” \u003ca href=\"https://arxiv.org/pdf/2507.14417\"\u003ethe researchers write\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe work builds on previous research showing that AI capabilities don’t always scale predictably. The team references \u003ca href=\"https://github.com/google-deepmind/bbeh\"\u003eBIG-Bench Extra Hard\u003c/a\u003e, a benchmark designed to challenge advanced models, noting that “state-of-the-art models achieve near-perfect scores on many tasks” in existing benchmarks, necessitating more challenging evaluations.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor enterprise users, the research underscores the need for careful testing across different reasoning scenarios and time constraints before deploying AI systems in production environments. Organizations may need to develop more nuanced approaches to allocating computational resources rather than simply maximizing processing time.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe study’s broader implications suggest that as AI systems become more sophisticated, the relationship between computational investment and performance may be far more complex than previously understood. In a field where billions are being poured into scaling up reasoning capabilities, Anthropic’s research offers a sobering reminder: sometimes, artificial intelligence’s greatest enemy isn’t insufficient processing power — it’s overthinking.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eThe research paper and interactive demonstrations are available at \u003ca href=\"https://safety-research.github.io/inverse-scaling-ttc/\"\u003ethe project’s website\u003c/a\u003e, allowing technical teams to explore the inverse scaling effects across different models and tasks.\u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-07-22T22:27:31Z",
  "modifiedTime": "2025-07-22T22:27:35Z"
}
