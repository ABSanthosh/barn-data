{
  "id": "5492df9a-6ffc-43bc-aabe-4f4eaf6ca3ed",
  "title": "New technique makes RAG systems much better at retrieving the right documents",
  "link": "https://venturebeat.com/ai/new-technique-makes-rag-systems-much-better-at-retrieving-the-right-documents/",
  "description": "By adding knowledge of surrounding documents to document embeddings, you can make embedding models aware of the context of their applications.",
  "author": "Ben Dickson",
  "published": "Wed, 09 Oct 2024 23:17:18 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI research",
    "AI, ML and Deep Learning",
    "category-/Science/Computer Science",
    "Cornell Tech",
    "cornell university",
    "embedding models",
    "embeddings",
    "large language models",
    "LLMs",
    "research",
    "retrieval augmented generation",
    "Retrieval-augmented generation (RAG)"
  ],
  "byline": "Ben Dickson",
  "length": 6212,
  "excerpt": "By adding knowledge of surrounding documents to document embeddings, you can make embedding models aware of the context of their applications.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "October 9, 2024 4:17 PM Image credit: VentureBeat with DALL-E 3 Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Retrieval-augmented generation (RAG) has become a popular method for grounding large language models (LLMs) in external knowledge. RAG systems typically use an embedding model to encode documents in a knowledge corpus and select those that are most relevant to the user’s query. However, standard retrieval methods often fail to account for context-specific details that can make a big difference in application-specific datasets. In a new paper, researchers at Cornell University introduce “contextual document embeddings,” a technique that improves the performance of embedding models by making them aware of the context in which documents are retrieved. The limitations of bi-encoders The most common approach for document retrieval in RAG is to use “bi-encoders,” where an embedding model creates a fixed representation of each document and stores it in a vector database. During inference, the embedding of the query is calculated and compared to the stored embeddings to find the most relevant documents. Bi-encoders have become a popular choice for document retrieval in RAG systems due to their efficiency and scalability. However, bi-encoders often struggle with nuanced, application-specific datasets because they are trained on generic data. In fact, when it comes to specialized knowledge corpora, they can fall short of classic statistical methods such as BM25 in certain tasks. “Our project started with the study of BM25, an old-school algorithm for text retrieval,” John (Jack) Morris, a doctoral student at Cornell Tech and co-author of the paper, told VentureBeat. “We performed a little analysis and saw that the more out-of-domain the dataset is, the more BM25 outperforms neural networks.” BM25 achieves its flexibility by calculating the weight of each word in the context of the corpus it is indexing. For example, if a word appears in many documents in the knowledge corpus, its weight will be reduced, even if it is an important keyword in other contexts. This allows BM25 to adapt to the specific characteristics of different datasets. “Traditional neural network-based dense retrieval models can’t do this because they just set weights once, based on the training data,” Morris said. “We tried to design an approach that could fix this.” Contextual document embeddings Contextual document embeddings Credit: arXiv The Cornell researchers propose two complementary methods to improve the performance of bi-encoders by adding the notion of context to document embeddings. “If you think about retrieval as a ‘competition’ between documents to see which is most relevant to a given search query, we use ‘context’ to inform the encoder about the other documents that will be in the competition,” Morris said. The first method modifies the training process of the embedding model. The researchers use a technique that groups similar documents before training the embedding model. They then use contrastive learning to train the encoder on distinguishing documents within each cluster.  Contrastive learning is an unsupervised technique where the model is trained to tell the difference between positive and negative examples. By being forced to distinguish between similar documents, the model becomes more sensitive to subtle differences that are important in specific contexts. The second method modifies the architecture of the bi-encoder. The researchers augment the encoder with a mechanism that gives it access to the corpus during the embedding process. This allows the encoder to take into account the context of the document when generating its embedding. The augmented architecture works in two stages. First, it calculates a shared embedding for the cluster to which the document belongs. Then, it combines this shared embedding with the document’s unique features to create a contextualized embedding. This approach enables the model to capture both the general context of the document’s cluster and the specific details that make it unique. The output is still an embedding of the same size as a regular bi-encoder, so it does not require any changes to the retrieval process. The impact of contextual document embeddings The researchers evaluated their method on various benchmarks and found that it consistently outperformed standard bi-encoders of similar sizes, especially in out-of-domain settings where the training and test datasets are significantly different. “Our model should be useful for any domain that’s materially different from the training data, and can be thought of as a cheap replacement for finetuning domain-specific embedding models,” Morris said. The contextual embeddings can be used to improve the performance of RAG systems in different domains. For example, if all of your documents share a structure or context, a normal embedding model would waste space in its embeddings by storing this redundant structure or information.  “Contextual embeddings, on the other hand, can see from the surrounding context that this shared information isn’t useful, and throw it away before deciding exactly what to store in the embedding,” Morris said. The researchers have released a small version of their contextual document embedding model (cde-small-v1). It can be used as a drop-in replacement for popular open-source tools such as HuggingFace and SentenceTransformers to create custom embeddings for different applications. Morris says that contextual embeddings are not limited to text-based models can be extended to other modalities, such as text-to-image architectures. There is also room to improve them with more advanced clustering algorithms and evaluate the effectiveness of the technique at larger scales. VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/10/Robot-poring-over-documents.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-10-09T23:17:18+00:00\" datetime=\"2024-10-09T23:17:18+00:00\"\u003eOctober 9, 2024 4:17 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"422\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/Robot-poring-over-documents.jpg?w=750\" alt=\"Robot poring over documents\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with DALL-E 3\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eRetrieval-augmented generation (\u003ca href=\"https://venturebeat.com/ai/from-gen-ai-1-5-to-2-0-moving-from-rag-to-agent-systems/\"\u003eRAG\u003c/a\u003e) has become a popular method for grounding large language models (LLMs) in external knowledge. RAG systems typically use an \u003ca href=\"https://venturebeat.com/ai/cohere-launches-embed-v3-for-enterprise-llm-applications/\"\u003eembedding model\u003c/a\u003e to encode documents in a knowledge corpus and select those that are most relevant to the user’s query.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, standard retrieval methods often fail to account for context-specific details that can make a big difference in application-specific datasets. In a new paper, researchers at \u003ca href=\"https://www.cornell.edu/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCornell University\u003c/a\u003e introduce “\u003ca href=\"https://arxiv.org/abs/2410.02525\" target=\"_blank\" rel=\"noreferrer noopener\"\u003econtextual document embeddings\u003c/a\u003e,” a technique that improves the performance of embedding models by making them aware of the context in which documents are retrieved.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-limitations-of-bi-encoders\"\u003eThe limitations of bi-encoders\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe most common approach for document retrieval in RAG is to use “bi-encoders,” where an embedding model creates a fixed representation of each document and stores it in a \u003ca href=\"https://venturebeat.com/ai/vector-database-company-qdrant-wants-rag-to-be-more-cost-effective/\"\u003evector database\u003c/a\u003e. During inference, the embedding of the query is calculated and compared to the stored embeddings to find the most relevant documents.\u003c/p\u003e\n\n\n\n\u003cp\u003eBi-encoders have become a popular choice for document retrieval in RAG systems due to their efficiency and scalability. However, bi-encoders often struggle with nuanced, application-specific datasets because they are trained on generic data. In fact, when it comes to specialized knowledge corpora, they can fall short of classic statistical methods such as \u003ca href=\"https://en.wikipedia.org/wiki/Okapi_BM25\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBM25\u003c/a\u003e in certain tasks.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Our project started with the study of BM25, an old-school algorithm for text retrieval,” John (Jack) Morris, a doctoral student at Cornell Tech and co-author of the paper, told VentureBeat. “We performed a little analysis and saw that the more out-of-domain the dataset is, the more BM25 outperforms neural networks.”\u003c/p\u003e\n\n\n\n\u003cp\u003eBM25 achieves its flexibility by calculating the weight of each word in the context of the corpus it is indexing. For example, if a word appears in many documents in the knowledge corpus, its weight will be reduced, even if it is an important keyword in other contexts. This allows BM25 to adapt to the specific characteristics of different datasets.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Traditional neural network-based dense retrieval models can’t do this because they just set weights once, based on the training data,” Morris said. “We tried to design an approach that could fix this.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-contextual-document-embeddings\"\u003eContextual document embeddings\u003c/h2\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1400\" height=\"572\" src=\"https://venturebeat.com/wp-content/uploads/2024/10/Contextual-document-embeddings.jpg?w=800\" alt=\"Contextual document embeddings\" srcset=\"https://venturebeat.com/wp-content/uploads/2024/10/Contextual-document-embeddings.jpg 1400w, https://venturebeat.com/wp-content/uploads/2024/10/Contextual-document-embeddings.jpg?resize=300,123 300w, https://venturebeat.com/wp-content/uploads/2024/10/Contextual-document-embeddings.jpg?resize=768,314 768w, https://venturebeat.com/wp-content/uploads/2024/10/Contextual-document-embeddings.jpg?resize=800,327 800w, https://venturebeat.com/wp-content/uploads/2024/10/Contextual-document-embeddings.jpg?resize=400,163 400w, https://venturebeat.com/wp-content/uploads/2024/10/Contextual-document-embeddings.jpg?resize=750,306 750w, https://venturebeat.com/wp-content/uploads/2024/10/Contextual-document-embeddings.jpg?resize=578,236 578w, https://venturebeat.com/wp-content/uploads/2024/10/Contextual-document-embeddings.jpg?resize=930,380 930w\" sizes=\"(max-width: 1400px) 100vw, 1400px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eContextual document embeddings Credit: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eThe Cornell researchers propose two complementary methods to improve the performance of bi-encoders by adding the notion of context to document embeddings.\u003c/p\u003e\n\n\n\n\u003cp\u003e“If you think about retrieval as a ‘competition’ between documents to see which is most relevant to a given search query, we use ‘context’ to inform the encoder about the other documents that will be in the competition,” Morris said.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe first method modifies the training process of the embedding model. The researchers use a technique that groups similar documents before training the embedding model. They then use \u003ca href=\"https://venturebeat.com/ai/why-snowflake-is-backing-embedding-startup-voyage-ai-to-improve-enterprise-rag/\"\u003econtrastive learning\u003c/a\u003e to train the encoder on distinguishing documents within each cluster. \u003c/p\u003e\n\n\n\n\u003cp\u003eContrastive learning is an unsupervised technique where the model is trained to tell the difference between positive and negative examples. By being forced to distinguish between similar documents, the model becomes more sensitive to subtle differences that are important in specific contexts.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe second method modifies the architecture of the bi-encoder. The researchers augment the encoder with a mechanism that gives it access to the corpus during the embedding process. This allows the encoder to take into account the context of the document when generating its embedding.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe augmented architecture works in two stages. First, it calculates a shared embedding for the cluster to which the document belongs. Then, it combines this shared embedding with the document’s unique features to create a contextualized embedding.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis approach enables the model to capture both the general context of the document’s cluster and the specific details that make it unique. The output is still an embedding of the same size as a regular bi-encoder, so it does not require any changes to the retrieval process.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-impact-of-contextual-document-embeddings\"\u003eThe impact of contextual document embeddings\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe researchers evaluated their method on various benchmarks and found that it consistently outperformed standard bi-encoders of similar sizes, especially in out-of-domain settings where the training and test datasets are significantly different.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Our model should be useful for any domain that’s materially different from the training data, and can be thought of as a cheap replacement for finetuning domain-specific embedding models,” Morris said.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe contextual embeddings can be used to improve the performance of RAG systems in different domains. For example, if all of your documents share a structure or context, a normal embedding model would waste space in its embeddings by storing this redundant structure or information. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Contextual embeddings, on the other hand, can see from the surrounding context that this shared information isn’t useful, and throw it away before deciding exactly what to store in the embedding,” Morris said.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers have released a small version of their contextual document embedding model (\u003ca href=\"https://huggingface.co/jxm/cde-small-v1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecde-small-v1\u003c/a\u003e). It can be used as a drop-in replacement for popular open-source tools such as HuggingFace and SentenceTransformers to create custom embeddings for different applications.\u003c/p\u003e\n\n\n\n\u003cp\u003eMorris says that contextual embeddings are not limited to text-based models can be extended to other modalities, such as text-to-image architectures. There is also room to improve them with more advanced clustering algorithms and evaluate the effectiveness of the technique at larger scales.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-10-09T23:17:18Z",
  "modifiedTime": "2024-10-09T23:47:53Z"
}
