{
  "id": "0fbe955f-db4f-4ad7-bd66-2f9d0437d5e5",
  "title": "Valkey Turns One: How the Community Fork Left Redis in the Dust",
  "link": "https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/",
  "description": "Article URL: https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/ Comments URL: https://news.ycombinator.com/item?id=44140379 Points: 24 # Comments: 2",
  "author": "cebert",
  "published": "Fri, 30 May 2025 22:24:22 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "Khawaja Shams",
  "length": 11763,
  "excerpt": "Valkey is not only thriving, but now outperforming Redis 8.0 in real world benchmarks.",
  "siteName": "Momento",
  "favicon": "https://www.gomomento.com/wp-content/uploads/2024/06/cropped-favicon-green-192x192.png",
  "text": "A year ago, Redis Inc (formerly Garantia Data) made a controversial move that disrupted the open source ecosystem: it closed the source of Redis. As I wrote at the time, it was a trust-breaking decision that could have shattered the community. But instead of splintering, the community responded with purpose. Out of that disruption came Valkey, a fork that took a shot at keeping the community alive. A Return, A Reversal As part of efforts to rebuild trust with the community, Redis Inc brought back Salvatore Sanfilippo (aka Antirez), the original creator of Redis. I am genuinely excited about his return because it is already impactful. He’s following through on his promise of contributing new features and performance optimizations to Redis. More profoundly, Redis 8.0 has been open-sourced again. Redis acknowledged that adopting SSPL strained their bond with the community, questioning contributions from others in the same breath. How do you keep innovating and investing in OSS projects when cloud providers reap the profits and control the infrastructure without proportional contributions back to the projects that they exploit? The disheartening move from Redis Inc catalyzed an unprecedented wave of collaboration and contributions. Valkey became the test of the community resolve to keep itself together. One year later, Valkey hasn’t just survived – it’s thriving! The Async I/O Threading model contribution from AWS unlocked 3x+ throughput by fundamentally changing how I/O threads work inside Redis. But how do these and other contributions compare to Redis 8? Can we hit 1M RPS out of an 8 VCPU instance (c8g.2xl) on either Valkey 8.1 or Redis 8.0 (with 1KB items, 3M items in the key space, and ~500 connections)? It’s time for a bake off! Valkey 8.1 vs Redis 8.0: Can the Fork Outrun the Source? The punchline: we could not sustain 1M RPS on an 8 VCPU instance with either Valkey or Redis 8.0, but we got really close!! On a full-tuned c8g.2xl (8 VCPU), Valkey 8.1.1 pushed to 999.8K RPS on SETs with .8ms p99 latency. Redis 8.0 got as high as 729.4 RPS on SETs with .99ms p99 latencies. On each iteration, we tested 50M SETs followed by 50M GETs. We varied connection counts to optimize the maximum throughput for each system. Valkey achieved higher throughput and lower latency across both reads and writes (37% higher on SET and 16% higher on GET), alongside 30% faster p99 latencies for SET and 60%+ faster on GET. Threading the multi-threading needle If I had a penny for every time heard, “but Redis /Valkey is single threaded….” Antirez’s emphasis on a shared nothing architecture has been foundational for Redis. Nevertheless, as early as 2020, Redis added support for I/O threads. Unfortunately, they did not offer drastic improvement until recently. If you have previously tried and discarded I/O threads, it is time to evaluate again! On Valkey, we see SET throughput going from 239K RPS without I/O threads to 678K with 6 threads. Meanwhile, p99 latencies dropped from 1.68ms to 0.93ms despite doing nearly 3x the throughput! Similarly, Redis went from 235K RPS without I/O threads to 563K RPS with 6 I/O threads. P99s for Redis also dropped around 40% from 1.35ms to 0.84ms. Two key takeaways emerged: With two threads, gains were modest (~20%). The impact only really surfaced at three threads and beyond. Redis and Valkey were neck-and-neck until the fourth thread. After that, Valkey pulled away sharply. SET Performance on Valkey with IO/Threads \u0026 256 Connections: SET Performance on Redis with IO/Threads \u0026 256 Connections: Pushing Valkey Throughput Further In the previous section, we saw that Valkey could hit 678K RPS on SETs with 6 threads and 256 connections. If we up the connections to 400, the throughput goes up to 832K RPS. How did we get the additional 167K RPS? We used Rezolus, our favorite Linux performance telemetry agent, to get deep insights into the system under stress. You can see in the charts below that overall CPU utilization is around 80% and unevenly distributed across the 8 cores. Diving deeper, this is driven by hardware interrupts from network queues across all 8 cores. Interrupts are bad because they disrupt a hard working Valkey thread to yield to handle network packets. What if we could avoid the context switching on our c8g.2xl with 8 cores? Running close to a million RPS requires considerable packet processing horsepower. Luckily, since a lot of work happens at the Nitro level on EC2 instances, two allocated cores to IRQs is all you need (if you let them focus). Pinning the IRQs to two cores is pretty straightforward. sudo ethtool -L ens34 combined 2 # reduce to 2 IRQs grep ens34 /proc/interrupts # ours were on 99 and 100 echo 1 | sudo tee /proc/irq/99/smp_affinity # pin 99 to core 1 echo 2 | sudo tee /proc/irq/100/smp_affinity # pin 100 to core 2 But how do we let these threads focus? and how do we avoid Redis / Valkey threads contending for the same cores? We pin Redis/Valkey to cores 3-8, giving their IO-Threads better isolation while also allowing the IRQs to focus. We used the --cpuset-cpus Docker flag to set these CPU assignments, making sure that Redis and Valkey process stayed pinned to the intended cores throughout the test. This reduces cross-core contention and improves cache locality, both of which are critical for minimizing tail latencies at high throughput. Ideal core allocation can vary in multi-tenant environments or mixed workloads, but in this benchmark it provided clean isolation between system and application workloads. Redis: docker run --network=\"host\" --rm \\ --cpuset-cpus=\"2-7\" redis:8.0 \\ --save \"\" --appendonly no \\ --io-threads 6 \\ --protected-mode no --maxmemory 10gb Valkey: docker run --network=\"host\" --rm \\ --cpuset-cpus=\"2-7\" valkey/valkey:8.1.1 \\ --save \"\" --appendonly no --io-threads 6 \\ --protected-mode no --maxmemory 10gb Let’s see what Rezolus has to say about the new setup with IRQs pinned to the first 2 cores and Valkey pinned to the remaining 6 cores. First, we observed meaningfully higher CPU Utilization. Second, looking at the bottom chart (SoftIRQ), we see that it is now limited to only the first two cores. Third, the Valkey cores are running red hot, whereas we previously saw a much more scattered distribution one usage across cores. While this setup is ideal for this benchmark, optimal IRQ tuning depends heavily on NIC architecture and the concurrency model of your application. The extra 20% CPU utilization is what buys us the extra 167K RPS (from 832K RPS to 999.8K RPS). Try it Yourself (And Know Before You Go) These benchmarks are hopefully a beginning and not the end. Our hope is that this sets up both Valkey and Redis communities to continue the performance improvement journey. We also recognize that many folks may want to reproduce the benchmark in their own environments, incorporating their workflow specifics. Below, we outline some key instructions that you can use to reproduce this in your own AWS account within an hour. Instance Types: We used AWS Graviton4-based c8g instances, launched in September 2024. The c8g.2xlarge server node provides 8 vCPUs (costing roughly $250/month in us-east-1), while the c8g.8xlarge load generator offers 32 vCPUs. This provided enough CPU headroom to cleanly isolate the benchmark workload, IRQ handling, and Redis/Valkey processing. The same c8g.2xl instance was used to run valkey and redis (one at a time). The same load gen node was run each time. Valkey and Redis were restarted right before each test to ensure fairness. Placement Groups: We used EC2 placement groups (cluster mode) to ensure minimal network jitter and low-latency communication between the client and server nodes. Placement groups offer extremely tight latencies by reducing the number of hops between your EC2 instances. This has the upside on higher throughput, fewer interruptions, and lower latencies – but it has some shared fate / blast radius implications that are worth considering before deploying them in your production environment. Core Pinning. To see the highest throughput and lowest latencies, consider core pinning and reducing the IRQs. See section above for specific instructions we used on our 8 core instance. It is also important to apply similar techniques on your test nodes. Vary the connections. Connection count is a surprisingly crucial variable for both Redis and Valkey. In fact, latencies rise steeply as you approach 1024 connections on both of them. For example, going from 400 to 1024 connections, Valkey’s SET throughput dropped from 999.9K RPS with to 969K RPS and p99 latencies doubled from .8ms to 1.6ms (at 2048 conns, p99 latencies triple). Going from 384 connections to 1024, Redis throughput drops from 729.4K RPS to 668K RPS, and p99 latencies more than double from .99ms to 2.5ms. Lower throughput with higher latencies? You get why connection count tuning is so crucial here. Key Space. If you want the best numbers, use smaller values and a really small key space (-r 10000). This will help you get everything from L3 cache. To make this test slightly more real world, we used 1KB items (-d 1024) and a key space of 3Million (-r 3000000). Multi-Thread the Benchmark App. To get the maximum throughput out of valkey-benchmark, make sure to turn on multi-threading on the benchmark tool as well. The --threads 6 flag tells valkey-benchmark to run in multi-threaded mode. Benchmark command: docker run --network=\"host\" --rm --cpuset-cpus=\"2-7\" \\ valkey/valkey:8.0.1 valkey-benchmark \\ -h 172.31.4.92 -p 6379 -t SET,GET -n 100000000 -c 256 \\ -r 3000000 --threads 6 -d 1024 A Final Caveat: Benchmarking is imprecise in nature We made every effort to make this benchmark resemble more real world workflows, but you can always do better. Valkey-bench is not perfect (nothing is). We have a wishlist of improvements (and so does the Valkey project). First, today, it simply pushes as much load as the server is able to handle instead of targeting a particular TPS. The real world rarely modulates its throughput based on your latency. Second, once it can target specific load, it would become closer to real world if it could modulate the load to show spikes and troughs as opposed to running a consistent throughput profile. Lastly, it’s rare to see 100% GETs or 100% SETs in a Key Value cache workflow. We’d love to provide a SET:GET ratio to see how the system reacts. At Momento, we typically do our testing using rpc-perf. It is written entirely in rust, handles more real world scenarios (like the three feature requests above), and pairs incredibly well with Rezolus. Regardless, even rpc-perf is a synthetic benchmark and even though it gives you more degrees of freedom to simulate production workflows, the results should not be interpreted as generally applicable to every workflow. Small variables make huge differences – and simulations are no match for production. Final Thoughts: Performance Is a Practice Valkey has not only kept pace – it’s setting it. Meanwhile, the performance ceiling keeps rising. But getting there isn’t automatic. It requires expertise across systems, infrastructure, and workload behavior. At Momento, we help teams achieve this kind of performance every day. Whether you’re running Valkey, Redis, or evaluating your options – we’re here to help you scale with confidence. Want help tuning your real-time infrastructure? Let’s talk. Special thanks to Yao and Brian from IOP Systems for providing the tools, including rpc-perf and rezolus, as well as insights for this benchmark.",
  "image": "https://www.gomomento.com/wp-content/uploads/2025/05/valkey-v-redis-bannerimg-nologo.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"content\"\u003e\n  \n  \u003carticle\u003e\n    \u003cdiv\u003e\n        \n\u003cp\u003eA year ago, Redis Inc (formerly Garantia Data) made a controversial move that disrupted the open source ecosystem: it closed the source of Redis. \u003ca href=\"https://www.gomomento.com/blog/rip-redis-how-garantia-data-pulled-off-the-biggest-heist-in-open-source-history/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAs I wrote at the time\u003c/a\u003e, it was a trust-breaking decision that could have shattered the community.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut instead of splintering, the community responded with purpose. Out of that disruption came \u003ca href=\"https://valkey.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eValkey\u003c/a\u003e, a fork that took a shot at keeping the community alive.\u003c/p\u003e\n\n\n\n\u003ch2\u003eA Return, A Reversal\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs part of efforts to rebuild trust with the community, Redis Inc \u003ca href=\"https://redis.io/blog/welcome-back-to-redis-antirez/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ebrought back\u003c/a\u003e Salvatore Sanfilippo (aka Antirez), the original creator of Redis. I am genuinely excited about his return because it is already impactful. He’s following through on his promise of contributing new features and performance optimizations to Redis. More profoundly, \u003ca href=\"https://redis.io/blog/agplv3/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRedis 8.0 has been open-sourced again\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eRedis acknowledged that adopting \u003ca href=\"https://en.wikipedia.org/wiki/Server_Side_Public_License\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSSPL\u003c/a\u003e strained their bond with the community, questioning contributions from others in the same breath.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\n\u003cfigure\u003e\u003cblockquote\u003e\u003cp\u003eHow do you keep innovating and investing in OSS projects when cloud providers reap the profits and control the infrastructure without proportional contributions back to the projects that they exploit?\u003c/p\u003e\u003c/blockquote\u003e\u003c/figure\u003e\n\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eThe disheartening move from Redis Inc catalyzed an unprecedented wave of collaboration and contributions. Valkey became the test of the community resolve to keep itself together. One year later, Valkey hasn’t just survived – \u003ca href=\"https://www.linkedin.com/posts/kshams_valkey-rocks-the-most-remarkable-thing-about-activity-7318683448506793985-_qJE\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eit’s thriving\u003c/a\u003e! The Async I/O Threading model \u003ca href=\"https://github.com/valkey-io/valkey/pull/758\" target=\"_blank\" rel=\"noreferrer noopener\"\u003econtribution\u003c/a\u003e from AWS unlocked 3x+ throughput by fundamentally changing how I/O threads work inside Redis.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut how do these and other contributions compare to Redis 8? Can we hit 1M RPS out of an 8 VCPU instance (c8g.2xl) on either Valkey 8.1 or Redis 8.0 (with 1KB items, 3M items in the key space, and ~500 connections)? It’s time for a bake off!\u003c/p\u003e\n\n\n\n\u003ch2\u003eValkey 8.1 vs Redis 8.0: Can the Fork Outrun the Source?\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe punchline: we could not sustain 1M RPS on an 8 VCPU instance with either Valkey or Redis 8.0, but we got really close!!\u003c/p\u003e\n\n\n\n\u003cp\u003eOn a full-tuned c8g.2xl (8 VCPU), Valkey 8.1.1 pushed to 999.8K RPS on SETs with .8ms p99 latency. Redis 8.0 got as high as 729.4 RPS on SETs with .99ms p99 latencies. On each iteration, we tested 50M SETs followed by 50M GETs. We varied connection counts to optimize the maximum throughput for each system.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cblockquote\u003e\u003cp\u003e\u003cmark\u003eValkey achieved higher throughput and lower latency across both reads and writes (37% higher on SET and 16% higher on GET), alongside 30% faster p99 latencies for SET and 60%+ faster on GET.\u003c/mark\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"512\" src=\"https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png\" alt=\"\" srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1.png 1200w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" data-old-src=\"data:image/svg+xml,%3Csvg%20xmlns=\u0026#39;http://www.w3.org/2000/svg\u0026#39;%20viewBox=\u0026#39;0%200%201024%20512\u0026#39;%3E%3C/svg%3E\" data-lazy-srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1.png 1200w\" data-lazy-src=\"https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"341\" src=\"https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png\" alt=\"Valkey vs Redis table with SET and GET command\" srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-300x100.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-768x256.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-18x6.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2.png 1200w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" data-old-src=\"data:image/svg+xml,%3Csvg%20xmlns=\u0026#39;http://www.w3.org/2000/svg\u0026#39;%20viewBox=\u0026#39;0%200%201024%20341\u0026#39;%3E%3C/svg%3E\" data-lazy-srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-300x100.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-768x256.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-18x6.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2.png 1200w\" data-lazy-src=\"https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eThreading the multi-threading needle\u003c/h2\u003e\n\n\n\n\u003cp\u003eIf I had a penny for every time heard, “but Redis /Valkey is single threaded….”\u003c/p\u003e\n\n\n\n\u003cp\u003eAntirez’s emphasis on a shared nothing architecture has been foundational for Redis. Nevertheless, as early as 2020, Redis added support for I/O threads. Unfortunately, they did not offer drastic improvement until recently. If you have previously tried and discarded I/O threads, it is time to evaluate again!\u003c/p\u003e\n\n\n\n\u003cp\u003eOn Valkey, we see SET throughput going from 239K RPS without I/O threads to 678K with 6 threads. Meanwhile, p99 latencies dropped from 1.68ms to 0.93ms \u003cstrong\u003edespite doing nearly 3x the throughput\u003c/strong\u003e! Similarly, Redis went from 235K RPS without I/O threads to 563K RPS with 6 I/O threads. P99s for Redis also dropped around 40% from 1.35ms to 0.84ms.\u003c/p\u003e\n\n\n\n\u003cp\u003eTwo key takeaways emerged:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eWith two threads, gains were modest (~20%). The impact only really surfaced at three threads and beyond.\u003c/li\u003e\n\n\n\n\u003cli\u003eRedis and Valkey were neck-and-neck until the fourth thread. After that, Valkey pulled away sharply.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"512\" src=\"https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png\" alt=\"\" srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2.png 1200w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" data-old-src=\"data:image/svg+xml,%3Csvg%20xmlns=\u0026#39;http://www.w3.org/2000/svg\u0026#39;%20viewBox=\u0026#39;0%200%201024%20512\u0026#39;%3E%3C/svg%3E\" data-lazy-srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2.png 1200w\" data-lazy-src=\"https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch3\u003eSET Performance on Valkey with IO/Threads \u0026amp; 256 Connections:\u003c/h3\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"700\" height=\"500\" src=\"https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png\" alt=\"\" srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-18x12.png 18w\" sizes=\"(max-width: 700px) 100vw, 700px\" data-old-src=\"data:image/svg+xml,%3Csvg%20xmlns=\u0026#39;http://www.w3.org/2000/svg\u0026#39;%20viewBox=\u0026#39;0%200%20700%20500\u0026#39;%3E%3C/svg%3E\" data-lazy-srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-18x12.png 18w\" data-lazy-src=\"https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch3\u003eSET Performance on Redis with IO/Threads \u0026amp; 256 Connections:\u003c/h3\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"700\" height=\"500\" src=\"https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png\" alt=\"\" srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-18x12.png 18w\" sizes=\"(max-width: 700px) 100vw, 700px\" data-old-src=\"data:image/svg+xml,%3Csvg%20xmlns=\u0026#39;http://www.w3.org/2000/svg\u0026#39;%20viewBox=\u0026#39;0%200%20700%20500\u0026#39;%3E%3C/svg%3E\" data-lazy-srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-18x12.png 18w\" data-lazy-src=\"https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003ePushing Valkey Throughput Further\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the previous section, we saw that Valkey could hit 678K RPS on SETs with 6 threads and 256 connections. If we up the connections to 400, the throughput goes up to 832K RPS. How did we get the additional 167K RPS?\u003c/p\u003e\n\n\n\n\u003cp\u003eWe used \u003ca href=\"https://github.com/iopsystems/rezolus\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRezolus\u003c/a\u003e, our favorite Linux performance telemetry agent, to get deep insights into the system under stress. You can see in the charts below that overall CPU utilization is around 80% and unevenly distributed across the 8 cores.\u003c/p\u003e\n\n\n\n\u003cp\u003eDiving deeper, this is driven by hardware interrupts from network queues across all 8 cores. Interrupts are bad because they disrupt a hard working Valkey thread to yield to handle network packets.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"835\" src=\"https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png\" alt=\"CPU Usage chart\" srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-300x245.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-768x627.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-15x12.png 15w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1.png 1445w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" data-old-src=\"data:image/svg+xml,%3Csvg%20xmlns=\u0026#39;http://www.w3.org/2000/svg\u0026#39;%20viewBox=\u0026#39;0%200%201024%20835\u0026#39;%3E%3C/svg%3E\" data-lazy-srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-300x245.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-768x627.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-15x12.png 15w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1.png 1445w\" data-lazy-src=\"https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhat if we could avoid the context switching on our \u003ccode\u003e\u003csup\u003ec8g.2xl\u003c/sup\u003e\u003c/code\u003e with 8 cores? Running close to a million RPS requires considerable packet processing horsepower. Luckily, since a lot of work happens at the Nitro level on EC2 instances, two allocated cores to IRQs is all you need (if you let them focus). Pinning the IRQs to two cores is pretty straightforward.\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003esudo ethtool -L ens34 combined 2 # reduce to 2 IRQs\ngrep ens34 /proc/interrupts # ours were on 99 and 100\necho 1 | sudo tee /proc/irq/99/smp_affinity # pin 99 to core 1\necho 2 | sudo tee /proc/irq/100/smp_affinity # pin 100 to core 2\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eBut how do we let these threads focus? and how do we avoid Redis / Valkey threads contending for the same cores? We pin Redis/Valkey to cores 3-8, giving their IO-Threads better isolation while also allowing the IRQs to focus. We used the \u003ccode\u003e\u003csup\u003e--cpuset-cpus\u003c/sup\u003e\u003c/code\u003e Docker flag to set these CPU assignments, making sure that Redis and Valkey process stayed pinned to the intended cores throughout the test. This reduces cross-core contention and improves cache locality, \u003cstrong\u003eboth of which are critical for minimizing tail latencies at high throughput\u003c/strong\u003e. Ideal core allocation can vary in multi-tenant environments or mixed workloads, but in this benchmark it provided clean isolation between system and application workloads.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eRedis:\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003edocker run --network=\u0026#34;host\u0026#34; --rm \\\n  --cpuset-cpus=\u0026#34;2-7\u0026#34; redis:8.0 \\\n  --save \u0026#34;\u0026#34; --appendonly no \\\n  --io-threads 6  \\\n  --protected-mode no --maxmemory 10gb\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eValkey:\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003edocker run --network=\u0026#34;host\u0026#34; --rm \\\n  --cpuset-cpus=\u0026#34;2-7\u0026#34; valkey/valkey:8.1.1 \\\n  --save \u0026#34;\u0026#34; --appendonly no --io-threads 6 \\\n  --protected-mode no --maxmemory 10gb\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eLet’s see what Rezolus has to say about the new setup with IRQs pinned to the first 2 cores and Valkey pinned to the remaining 6 cores. First, we observed meaningfully higher CPU Utilization. Second, looking at the bottom chart (SoftIRQ), we see that it is now limited to only the first two cores. Third, the Valkey cores are running red hot, whereas we previously saw a much more scattered distribution one usage across cores. \u003cstrong\u003eWhile this setup is ideal for this benchmark, optimal IRQ tuning depends heavily on NIC architecture and the concurrency model of your application.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"720\" height=\"603\" src=\"https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png\" alt=\"CPU Chart 2\" srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png 720w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-300x251.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-14x12.png 14w\" sizes=\"(max-width: 720px) 100vw, 720px\" data-old-src=\"data:image/svg+xml,%3Csvg%20xmlns=\u0026#39;http://www.w3.org/2000/svg\u0026#39;%20viewBox=\u0026#39;0%200%20720%20603\u0026#39;%3E%3C/svg%3E\" data-lazy-srcset=\"https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png 720w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-300x251.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-14x12.png 14w\" data-lazy-src=\"https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe extra 20% CPU utilization is what buys us the extra 167K RPS (from 832K RPS to 999.8K RPS).\u003c/p\u003e\n\n\n\n\u003ch2\u003eTry it Yourself (And Know Before You Go)\u003c/h2\u003e\n\n\n\n\u003cp\u003eThese benchmarks are hopefully a beginning and not the end. Our hope is that this sets up both Valkey and Redis communities to continue the performance improvement journey. We also recognize that many folks may want to reproduce the benchmark in their own environments, incorporating their workflow specifics. Below, we outline some key instructions that you can use to reproduce this in your own AWS account within an hour.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eInstance Types:\u003c/strong\u003e We used AWS Graviton4-based \u003ccode\u003e\u003csup\u003ec8g\u003c/sup\u003e\u003c/code\u003e instances, launched in September 2024. The \u003csup\u003e\u003ccode\u003ec8g.2xlarge\u003c/code\u003e\u003c/sup\u003e server node provides 8 vCPUs (costing roughly $250/month in us-east-1), while the \u003ccode\u003e\u003csup\u003ec8g.8xlarge\u003c/sup\u003e\u003c/code\u003e load generator offers 32 vCPUs. This provided enough CPU headroom to cleanly isolate the benchmark workload, IRQ handling, and Redis/Valkey processing. The same c8g.2xl instance was used to run valkey and redis (one at a time). The same load gen node was run each time. Valkey and Redis were restarted right before each test to ensure fairness.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePlacement Groups:\u003c/strong\u003e We used EC2 placement groups (cluster mode) to ensure minimal network jitter and low-latency communication between the client and server nodes. Placement groups offer extremely tight latencies by reducing the number of hops between your EC2 instances. This has the upside on higher throughput, fewer interruptions, and lower latencies – but it has some shared fate / blast radius implications that are worth considering before deploying them in your production environment.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCore Pinning.\u003c/strong\u003e To see the highest throughput and lowest latencies, consider core pinning and reducing the IRQs. See section above for specific instructions we used on our 8 core instance. It is also important to apply similar techniques on your test nodes.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eVary the connections.\u003c/strong\u003e Connection count is a surprisingly crucial variable for both Redis and Valkey. In fact, latencies rise steeply as you approach 1024 connections on both of them. For example, going from 400 to 1024 connections, Valkey’s SET throughput dropped from 999.9K RPS with to 969K RPS and p99 latencies doubled from .8ms to 1.6ms (at 2048 conns, p99 latencies triple). Going from 384 connections to 1024, Redis throughput drops from 729.4K RPS to 668K RPS, and p99 latencies more than double from .99ms to 2.5ms. Lower throughput with higher latencies? You get why connection count tuning is so crucial here.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eKey Space.\u003c/strong\u003e If you want the best numbers, \u003ca href=\"https://www.linkedin.com/posts/yaoyue-thinkingfish_my-performance-rant-of-the-day-if-you-are-activity-7326350824261980161-qB6h/?utm_source=share\u0026amp;utm_medium=member_desktop\u0026amp;rcm=ACoAAAANW9wBFDc3gQ3Jwp6YswZ_BARGfJvyJQQ\" target=\"_blank\" rel=\"noreferrer noopener\"\u003euse smaller values and a really small key space\u003c/a\u003e (\u003csup\u003e\u003ccode\u003e-r 10000\u003c/code\u003e\u003c/sup\u003e). This will help you get everything from L3 cache. To make this test slightly more real world, we used 1KB items (\u003ccode\u003e\u003csub\u003e\u003csup\u003e-d 1024\u003c/sup\u003e\u003c/sub\u003e\u003c/code\u003e) and a key space of 3Million (\u003ccode\u003e\u003csup\u003e-r 3000000\u003c/sup\u003e\u003c/code\u003e).\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eMulti-Thread the Benchmark App.\u003c/strong\u003e To get the maximum throughput out of valkey-benchmark, make sure to turn on multi-threading on the benchmark tool as well. The \u003ccode\u003e\u003csup\u003e--threads 6\u003c/sup\u003e\u003c/code\u003e flag tells valkey-benchmark to run in multi-threaded mode.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eBenchmark command:\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003edocker run --network=\u0026#34;host\u0026#34; --rm --cpuset-cpus=\u0026#34;2-7\u0026#34; \\\nvalkey/valkey:8.0.1 valkey-benchmark \\\n-h 172.31.4.92 -p 6379 -t SET,GET -n 100000000 -c 256 \\\n-r 3000000 --threads 6 -d 1024\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003ch2\u003eA Final Caveat: Benchmarking is imprecise in nature\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe made every effort to make this benchmark resemble more real world workflows, but you can always do better. Valkey-bench is not perfect (nothing is). We have a wishlist of improvements (and so \u003ca href=\"https://github.com/valkey-io/valkey/issues/900\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edoes\u003c/a\u003e the Valkey project).\u003c/p\u003e\n\n\n\n\u003cp\u003eFirst, today, it simply pushes as much load as the server is able to handle instead of targeting a particular TPS. The real world rarely modulates its throughput based on your latency. Second, once it can target specific load, it would become closer to real world if it could modulate the load to show spikes and troughs as opposed to running a consistent throughput profile. Lastly, it’s rare to see 100% GETs or 100% SETs in a Key Value cache workflow. We’d love to provide a SET:GET ratio to see how the system reacts.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt Momento, we typically do our testing using \u003ca href=\"https://github.com/iopsystems/rpc-perf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003erpc-perf\u003c/a\u003e. It is written entirely in rust, handles more real world scenarios (like the three feature requests above), and pairs incredibly well with Rezolus. Regardless, even rpc-perf is a synthetic benchmark and even though it gives you more degrees of freedom to simulate production workflows, the results should not be interpreted as generally applicable to every workflow. Small variables make huge differences – and simulations are no match for production.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFinal Thoughts: Performance Is a Practice\u003c/h2\u003e\n\n\n\n\u003cp\u003eValkey has not only kept pace – it’s setting it. Meanwhile, the performance ceiling keeps rising. But getting there isn’t automatic. It requires expertise across systems, infrastructure, and workload behavior.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt Momento, we help teams achieve this kind of performance every day. Whether you’re running Valkey, Redis, or evaluating your options – we’re here to help you scale with confidence.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eWant help tuning your real-time infrastructure? \u003ca href=\"https://gomomento.com/contact-us\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eLet’s talk.\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\n\n\n\n\u003chr/\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eSpecial thanks to Yao and Brian from \u003ca href=\"https://iop.systems/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eIOP Systems\u003c/a\u003e for providing the tools, including rpc-perf and rezolus, as well as insights for this benchmark.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\n        \n      \u003c/div\u003e\n\n  \u003c/article\u003e\n  \n\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2025-05-07T20:38:34Z",
  "modifiedTime": "2025-05-09T20:15:44Z"
}
