{
  "id": "1d9e2214-1f14-4a8e-921f-1994a293e9bc",
  "title": "WonderHuman: 3D avatars from single-view video",
  "link": "https://arxiv.org/abs/2502.01045",
  "description": "Article URL: https://arxiv.org/abs/2502.01045 Comments URL: https://news.ycombinator.com/item?id=43109466 Points: 9 # Comments: 2",
  "author": "jinqueeny",
  "published": "Thu, 20 Feb 2025 00:05:39 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "[Submitted on 3 Feb 2025]",
  "length": 1631,
  "excerpt": "In this paper, we present WonderHuman to reconstruct dynamic human avatars from a monocular video for high-fidelity novel view synthesis. Previous dynamic human avatar reconstruction methods typically require the input video to have full coverage of the observed human body. However, in daily practice, one typically has access to limited viewpoints, such as monocular front-view videos, making it a cumbersome task for previous methods to reconstruct the unseen parts of the human avatar. To tackle the issue, we present WonderHuman, which leverages 2D generative diffusion model priors to achieve high-quality, photorealistic reconstructions of dynamic human avatars from monocular videos, including accurate rendering of unseen body parts. Our approach introduces a Dual-Space Optimization technique, applying Score Distillation Sampling (SDS) in both canonical and observation spaces to ensure visual consistency and enhance realism in dynamic human reconstruction. Additionally, we present a View Selection strategy and Pose Feature Injection to enforce the consistency between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity in the reconstructed avatar. In the experiments, our method achieves SOTA performance in producing photorealistic renderings from the given monocular video, particularly for those challenging unseen parts. The project page and source code can be found at https://wyiguanw.github.io/WonderHuman/.",
  "siteName": "arXiv.org",
  "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png",
  "text": "View PDF HTML (experimental) Abstract:In this paper, we present WonderHuman to reconstruct dynamic human avatars from a monocular video for high-fidelity novel view synthesis. Previous dynamic human avatar reconstruction methods typically require the input video to have full coverage of the observed human body. However, in daily practice, one typically has access to limited viewpoints, such as monocular front-view videos, making it a cumbersome task for previous methods to reconstruct the unseen parts of the human avatar. To tackle the issue, we present WonderHuman, which leverages 2D generative diffusion model priors to achieve high-quality, photorealistic reconstructions of dynamic human avatars from monocular videos, including accurate rendering of unseen body parts. Our approach introduces a Dual-Space Optimization technique, applying Score Distillation Sampling (SDS) in both canonical and observation spaces to ensure visual consistency and enhance realism in dynamic human reconstruction. Additionally, we present a View Selection strategy and Pose Feature Injection to enforce the consistency between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity in the reconstructed avatar. In the experiments, our method achieves SOTA performance in producing photorealistic renderings from the given monocular video, particularly for those challenging unseen parts. The project page and source code can be found at this https URL. Submission history From: Zilong Wang [view email] [v1] Mon, 3 Feb 2025 04:43:41 UTC (7,479 KB)",
  "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"content-inner\"\u003e\n    \n    \n                \n    \u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2502.01045\"\u003eView PDF\u003c/a\u003e\n    \u003ca href=\"https://arxiv.org/html/2502.01045v1\"\u003eHTML (experimental)\u003c/a\u003e\u003c/p\u003e\u003cblockquote\u003e\n            \u003cspan\u003eAbstract:\u003c/span\u003eIn this paper, we present WonderHuman to reconstruct dynamic human avatars from a monocular video for high-fidelity novel view synthesis. Previous dynamic human avatar reconstruction methods typically require the input video to have full coverage of the observed human body. However, in daily practice, one typically has access to limited viewpoints, such as monocular front-view videos, making it a cumbersome task for previous methods to reconstruct the unseen parts of the human avatar. To tackle the issue, we present WonderHuman, which leverages 2D generative diffusion model priors to achieve high-quality, photorealistic reconstructions of dynamic human avatars from monocular videos, including accurate rendering of unseen body parts. Our approach introduces a Dual-Space Optimization technique, applying Score Distillation Sampling (SDS) in both canonical and observation spaces to ensure visual consistency and enhance realism in dynamic human reconstruction. Additionally, we present a View Selection strategy and Pose Feature Injection to enforce the consistency between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity in the reconstructed avatar. In the experiments, our method achieves SOTA performance in producing photorealistic renderings from the given monocular video, particularly for those challenging unseen parts. The project page and source code can be found at \u003ca href=\"https://wyiguanw.github.io/WonderHuman/\" rel=\"external noopener nofollow\"\u003ethis https URL\u003c/a\u003e.\n    \u003c/blockquote\u003e\n\n    \n    \n  \u003c/div\u003e\u003cdiv\u003e\n      \u003ch2\u003eSubmission history\u003c/h2\u003e\u003cp\u003e From: Zilong Wang [\u003ca href=\"https://arxiv.org/show-email/a4db723d/2502.01045\" rel=\"nofollow\"\u003eview email\u003c/a\u003e]      \u003cbr/\u003e    \u003cstrong\u003e[v1]\u003c/strong\u003e\n        Mon, 3 Feb 2025 04:43:41 UTC (7,479 KB)\u003cbr/\u003e\n\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "2 min read",
  "publishedTime": null,
  "modifiedTime": null
}
