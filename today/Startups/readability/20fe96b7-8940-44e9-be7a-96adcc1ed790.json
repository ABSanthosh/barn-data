{
  "id": "20fe96b7-8940-44e9-be7a-96adcc1ed790",
  "title": "DeepSeek jolts AI industry: Why AI’s next leap may not come from more data, but more compute at inference",
  "link": "https://venturebeat.com/ai/deepseek-jolts-ai-industry-why-ais-next-leap-may-not-come-from-more-data-but-more-compute-at-inference/",
  "description": "To contextualize DeepSeek’s disruption, let's consider the broader shift in AI being driven by the scarcity of training data.",
  "author": "Pashootan Vaezipoor, Georgian",
  "published": "Sat, 05 Apr 2025 19:15:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "AI training",
    "AI, ML and Deep Learning",
    "category-/News",
    "Deepseek",
    "Generative AI",
    "large language models",
    "test-time compute"
  ],
  "byline": "Pashootan Vaezipoor, Georgian",
  "length": 9268,
  "excerpt": "To contextualize DeepSeek’s disruption, let's consider the broader shift in AI being driven by the scarcity of training data.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More The AI landscape continues to evolve at a rapid pace, with recent developments challenging established paradigms. Early in 2025, Chinese AI lab DeepSeek unveiled a new model that sent shockwaves through the AI industry and resulted in a 17% drop in Nvidia’s stock, along with other stocks related to AI data center demand. This market reaction was widely reported to stem from DeepSeek’s apparent ability to deliver high-performance models at a fraction of the cost of rivals in the U.S., sparking discussion about the implications for AI data centers.  To contextualize DeepSeek’s disruption, we think it’s useful to consider a broader shift in the AI landscape being driven by the scarcity of additional training data. Because the major AI labs have now already trained their models on much of the available public data on the internet, data scarcity is slowing further improvements in pre-training. As a result, model providers are looking to “test-time compute” (TTC) where reasoning models (such as Open AI’s “o” series of models) “think” before responding to a question at inference time, as an alternative method to improve overall model performance. The current thinking is that TTC may exhibit scaling-law improvements similar to those that once propelled pre-training, potentially enabling the next wave of transformative AI advancements. These developments indicate two significant shifts: First, labs operating on smaller (reported) budgets are now capable of releasing state-of-the-art models. The second shift is the focus on TTC as the next potential driver of AI progress. Below we unpack both of these trends and the potential implications for the competitive landscape and broader AI market. Implications for the AI industry We believe that the shift towards TTC and the increased competition among reasoning models may have a number of implications for the wider AI landscape across hardware, cloud platforms, foundation models and enterprise software.  1. Hardware (GPUs, dedicated chips and compute infrastructure) From massive training clusters to on-demand “test-time” spikes: In our view, the shift towards TTC may have implications for the type of hardware resources that AI companies require and how they are managed. Rather than investing in increasingly larger GPU clusters dedicated to training workloads, AI companies may instead increase their investment in inference capabilities to support growing TTC needs. While AI companies will likely still require large numbers of GPUs to handle inference workloads, the differences between training workloads and inference workloads may impact how those chips are configured and used. Specifically, since inference workloads tend to be more dynamic (and “spikey”), capacity planning may become more complex than it is for batch-oriented training workloads.  Rise of inference-optimized hardware: We believe that the shift in focus towards TTC is likely to increase opportunities for alternative AI hardware that specializes in low-latency inference-time compute. For example, we may see more demand for GPU alternatives such as application specific integrated circuits (ASICs) for inference. As access to TTC becomes more important than training capacity, the dominance of general-purpose GPUs, which are used for both training and inference, may decline. This shift could benefit specialized inference chip providers.  2. Cloud platforms: Hyperscalers (AWS, Azure, GCP) and cloud compute Quality of service (QoS) becomes a key differentiator: One issue preventing AI adoption in the enterprise, in addition to concerns around model accuracy, is the unreliability of inference APIs. Problems associated with unreliable API inference include fluctuating response times, rate limiting and difficulty handling concurrent requests and adapting to API endpoint changes. Increased TTC may further exacerbate these problems. In these circumstances, a cloud provider able to provide models with QoS assurances that address these challenges would, in our view, have a significant advantage. Increased cloud spend despite efficiency gains: Rather than reducing demand for AI hardware, it is possible that more efficient approaches to large language model (LLM) training and inference may follow the Jevons Paradox, a historical observation where improved efficiency drives higher overall consumption. In this case, efficient inference models may encourage more AI developers to leverage reasoning models, which, in turn, increases demand for compute. We believe that recent model advances may lead to increased demand for cloud AI compute for both model inference and smaller, specialized model training. 3. Foundation model providers (OpenAI, Anthropic, Cohere, DeepSeek, Mistral) Impact on pre-trained models: If new players like DeepSeek can compete with frontier AI labs at a fraction of the reported costs, proprietary pre-trained models may become less defensible as a moat. We can also expect further innovations in TTC for transformer models and, as DeepSeek has demonstrated, those innovations can come from sources outside of the more established AI labs.    4. Enterprise AI adoption and SaaS (application layer) Security and privacy concerns: Given DeepSeek’s origins in China, there is likely to be ongoing scrutiny of the firm’s products from a security and privacy perspective. In particular, the firm’s China-based API and chatbot offerings are unlikely to be widely used by enterprise AI customers in the U.S., Canada or other Western countries. Many companies are reportedly moving to block the use of DeepSeek’s website and applications. We expect that DeepSeek’s models will face scrutiny even when they are hosted by third parties in the U.S. and other Western data centers which may limit enterprise adoption of the models. Researchers are already pointing to examples of security concerns around jail breaking, bias and harmful content generation. Given consumer attention, we may see experimentation and evaluation of DeepSeek’s models in the enterprise, but it is unlikely that enterprise buyers will move away from incumbents due to these concerns. Vertical specialization gains traction: In the past, vertical applications that use foundation models mainly focused on creating workflows designed for specific business needs. Techniques such as retrieval-augmented generation (RAG), model routing, function calling and guardrails have played an important role in adapting generalized models for these specialized use cases. While these strategies have led to notable successes, there has been persistent concern that significant improvements to the underlying models could render these applications obsolete. As Sam Altman cautioned, a major breakthrough in model capabilities could “steamroll” application-layer innovations that are built as wrappers around foundation models. However, if advancements in train-time compute are indeed plateauing, the threat of rapid displacement diminishes. In a world where gains in model performance come from TTC optimizations, new opportunities may open up for application-layer players. Innovations in domain-specific post-training algorithms — such as structured prompt optimization, latency-aware reasoning strategies and efficient sampling techniques — may provide significant performance improvements within targeted verticals. Any performance improvement would be especially relevant in the context of reasoning-focused models like OpenAI’s GPT-4o and DeepSeek-R1, which often exhibit multi-second response times. In real-time applications, reducing latency and improving the quality of inference within a given domain could provide a competitive advantage. As a result, application-layer companies with domain expertise may play a pivotal role in optimizing inference efficiency and fine-tuning outputs. DeepSeek demonstrates a declining emphasis on ever-increasing amounts of pre-training as the sole driver of model quality. Instead, the development underscores the growing importance of TTC. While the direct adoption of DeepSeek models in enterprise software applications remains uncertain due to ongoing scrutiny, their impact on driving improvements in other existing models is becoming clearer. We believe that DeepSeek’s advancements have prompted established AI labs to incorporate similar techniques into their engineering and research processes, supplementing their existing hardware advantages. The resulting reduction in model costs, as predicted, appears to be contributing to increased model usage, aligning with the principles of Jevons Paradox. Pashootan Vaezipoor is technical lead at Georgian. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/04/Datatransformer.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eThe AI landscape continues to evolve at a rapid pace, with recent developments challenging established paradigms. Early in 2025, Chinese AI lab DeepSeek unveiled a new model that sent shockwaves \u003ca href=\"https://venturebeat.com/ai/why-everyone-in-ai-is-freaking-out-about-deepseek/\"\u003ethrough the AI industry\u003c/a\u003e and resulted in a \u003ca href=\"https://www.wsj.com/livecoverage/stock-market-today-dow-sp500-nasdaq-live-01-27-2025\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e17%\u003c/a\u003e drop in Nvidia’s stock, \u003ca href=\"https://www.reuters.com/business/energy/us-power-stocks-plummet-deepseek-raises-data-center-demand-doubts-2025-01-27/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ealong with\u003c/a\u003e other stocks related to AI data center demand. This market reaction was widely reported to stem from DeepSeek’s apparent ability to deliver high-performance models at a fraction of the cost of rivals in the U.S., sparking discussion about the \u003ca href=\"https://www.datacenterknowledge.com/ai-data-centers/deepseek-s-ai-breakthrough-signals-major-shifts-for-data-centers\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eimplications for AI data centers\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eTo contextualize DeepSeek’s disruption, we think it’s useful to consider a broader shift in the AI landscape being driven by the scarcity of additional training data. Because the major AI labs have now already trained their models on much of the available public data on the internet, data scarcity is \u003ca href=\"https://www.youtube.com/watch?v=1yvBqasHLZs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eslowing further improvements in pre-training\u003c/a\u003e. As a result, model providers are looking to “test-time compute” (TTC) where reasoning models (such as Open AI’s “o” series of models) “think” before responding to a question at inference time, as an alternative method to improve overall model performance. The current thinking is that TTC may exhibit scaling-law improvements similar to those that once propelled pre-training, potentially enabling the next wave of transformative AI advancements.\u003c/p\u003e\n\n\n\n\u003cp\u003eThese developments indicate two significant shifts: First, labs operating on smaller (reported) budgets are now capable of releasing state-of-the-art models. The second shift is the focus on TTC as the next potential driver of AI progress. Below we unpack both of these trends and the potential implications for the competitive landscape and broader AI market.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-implications-for-the-ai-industry\"\u003eImplications for the AI industry\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe believe that the shift towards TTC and the increased competition among reasoning models may have a number of implications for the wider \u003ca href=\"https://venturebeat.com/ai/i-asked-an-ai-swarm-to-fill-out-a-march-madness-bracket-heres-what-happened/\"\u003eAI landscape\u003c/a\u003e across hardware, cloud platforms, foundation models and enterprise software. \u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-1-hardware-gpus-dedicated-chips-and-compute-infrastructure\"\u003e1. Hardware (GPUs, dedicated chips and compute infrastructure)\u003c/h3\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrom massive training clusters to on-demand “test-time” spikes:\u003c/strong\u003e In our view, the shift towards TTC may have implications for the type of hardware resources that AI companies require and how they are managed. Rather than investing in increasingly larger GPU clusters dedicated to training workloads, AI companies may instead increase their investment in inference capabilities to support growing TTC needs. While AI companies will likely still require large numbers of GPUs to handle inference workloads, the differences between \u003ca href=\"https://embeddedcomputing.com/technology/ai-machine-learning/ai-logic-devices-worload-acceleration/why-gpus-are-great-for-training-but-not-for-inferencing\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etraining workloads\u003c/a\u003e and inference workloads may impact how those chips are configured and used. Specifically, since inference workloads tend to be more \u003ca href=\"https://arxiv.org/abs/2407.14843?utm_source=chatgpt.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edynamic (and “spikey”)\u003c/a\u003e, capacity planning may become more complex than it is for batch-oriented training workloads. \u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eRise of inference-optimized hardware:\u003c/strong\u003e We believe that the shift in focus towards TTC is likely to increase opportunities for alternative AI hardware that specializes in low-latency inference-time compute. For example, we may see more demand for GPU alternatives such as application specific integrated circuits \u003ca href=\"https://www.aiacceleratorinstitute.com/improving-ai-inference-performance-with-hardware-accelerators/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e(ASICs) for inference\u003c/a\u003e. As access to TTC becomes more important than training capacity, the dominance of \u003ca href=\"https://www.wsj.com/articles/as-ai-matures-chip-industry-will-look-beyond-gpus-amd-chief-says-fec6776a\" target=\"_blank\" rel=\"noreferrer noopener\"\u003egeneral-purpose GPUs,\u003c/a\u003e which are used for both training and inference, may decline. This shift could benefit specialized inference chip providers. \u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch3 id=\"h-2-cloud-platforms-hyperscalers-aws-azure-gcp-and-cloud-compute\"\u003e2. Cloud platforms: Hyperscalers (AWS, Azure, GCP) and cloud compute\u003c/h3\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eQuality of service (QoS) becomes a key differentiator:\u003c/strong\u003e One issue preventing AI adoption in the enterprise, in addition to concerns around model accuracy, is the unreliability of inference APIs. Problems associated with unreliable API inference include \u003ca href=\"https://community.openai.com/t/concurrency-rate-limiting-a-10-000-issue/907411\" target=\"_blank\" rel=\"noreferrer noopener\"\u003efluctuating response times\u003c/a\u003e, \u003ca href=\"https://stackoverflow.com/questions/79266024/are-there-inconsistencies-with-anthropic-limits\"\u003erate limiting\u003c/a\u003e and difficulty \u003ca href=\"https://community.openai.com/t/concurrent-request-restriction/1062443\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehandling concurrent requests\u003c/a\u003e and \u003ca href=\"https://github.com/xtekky/gpt4free/issues/2481\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eadapting to API endpoint changes\u003c/a\u003e. Increased TTC may further exacerbate these problems. In these circumstances, a cloud provider able to provide models with QoS assurances that address these challenges would, in our view, have a significant advantage.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eIncreased cloud spend despite efficiency gains:\u003c/strong\u003e Rather than reducing demand for AI hardware, it is possible that more efficient approaches to large language model (LLM) training and inference may follow the Jevons Paradox, a historical observation where improved efficiency drives higher overall consumption. In this case, efficient inference models may encourage more AI developers to leverage reasoning models, which, in turn, increases demand for compute. We believe that recent model advances may lead to increased demand for cloud AI compute for both model inference and smaller, specialized model training.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch3 id=\"h-3-foundation-model-providers-openai-anthropic-cohere-deepseek-mistral\"\u003e3. Foundation model providers (OpenAI, Anthropic, Cohere, DeepSeek, Mistral)\u003c/h3\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImpact on pre-trained models:\u003c/strong\u003e If new players like DeepSeek can compete with \u003ca href=\"https://venturebeat.com/ai/why-businesses-judge-ai-like-humans-and-what-that-means-for-adoption/\"\u003efrontier AI labs\u003c/a\u003e at a fraction of the reported costs, proprietary pre-trained models may become less defensible as a moat. We can also expect further innovations in TTC for transformer models and, as DeepSeek has demonstrated, those innovations can come from sources outside of the more established AI labs.   \u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch3 id=\"h-4-enterprise-ai-adoption-and-saas-application-layer\"\u003e4. Enterprise AI adoption and SaaS (application layer)\u003c/h3\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSecurity and privacy concerns: \u003c/strong\u003eGiven DeepSeek’s origins in China, there is likely to be ongoing \u003ca href=\"https://www.euronews.com/next/2025/01/31/harmful-and-toxic-output-deepseek-has-major-security-and-safety-gaps-study-warns\" target=\"_blank\" rel=\"noreferrer noopener\"\u003escrutiny\u003c/a\u003e of the firm’s products from a security and privacy perspective. In particular, the firm’s China-based API and chatbot offerings are unlikely to be widely used by enterprise AI customers in the U.S., Canada or other Western countries. Many companies are reportedly \u003ca href=\"https://financialpost.com/news/deepseek-ai-restricted-hundreds-companies\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emoving to block\u003c/a\u003e the use of DeepSeek’s website and applications. We expect that DeepSeek’s models will face scrutiny even when they are hosted by \u003ca href=\"https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethird parties\u003c/a\u003e in the U.S. and other Western data centers which may limit enterprise adoption of the models. Researchers are already pointing to examples of security concerns around \u003ca href=\"https://unit42.paloaltonetworks.com/jailbreaking-deepseek-three-techniques/?utm_source=chatgpt.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ejail breaking\u003c/a\u003e, \u003ca href=\"http://cdn.prod.website-files.com/6690a78074d86ca0ad978007/679bc2e71b48e423c0ff7e60_1%20RedTeaming_DeepSeek_Jan29_2025%20(1).pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ebias and harmful content generation\u003c/a\u003e. Given \u003ca href=\"https://www.cnbc.com/2025/01/27/chinas-deepseek-ai-tops-chatgpt-app-store-what-you-should-know.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003econsumer attention\u003c/a\u003e, we may see experimentation and evaluation of DeepSeek’s models in the enterprise, but it is unlikely that enterprise buyers will move away from incumbents due to these concerns.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eVertical specialization gains traction:\u003c/strong\u003e In the past, vertical applications that use foundation models mainly focused on creating workflows designed for specific business needs. Techniques such as retrieval-augmented generation (RAG), model routing, function calling and guardrails have played an important role in adapting generalized models for these specialized use cases. While these strategies have led to notable successes, there has been persistent concern that significant improvements to the underlying models could render these applications obsolete. As Sam Altman cautioned, a major breakthrough in model capabilities could “\u003ca href=\"https://fortune.com/2024/08/13/sam-altman-openai-will-steamroll-ai-startups-i-run-one-not-worried/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esteamroll” application-layer innovations\u003c/a\u003e that are built as wrappers around foundation models.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eHowever, if advancements in train-time compute are indeed plateauing, the threat of rapid displacement diminishes. In a world where gains in model performance come from TTC optimizations, new opportunities may open up for application-layer players. Innovations in domain-specific post-training algorithms — such as \u003ca href=\"https://arxiv.org/html/2406.10504v1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003estructured prompt optimization\u003c/a\u003e, \u003ca href=\"https://arxiv.org/html/2406.06461v3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elatency-aware reasoning strategies\u003c/a\u003e and efficient sampling techniques — may provide significant performance improvements within targeted verticals. \u003c/p\u003e\n\n\n\n\u003cp\u003eAny performance improvement would be especially relevant in the context of reasoning-focused models like OpenAI’s GPT-4o and DeepSeek-R1, which often exhibit multi-second response times. In real-time applications, reducing latency and improving the quality of inference within a given domain could provide a competitive advantage. As a result, application-layer companies with domain expertise may play a pivotal role in optimizing inference efficiency and fine-tuning outputs.\u003c/p\u003e\n\n\n\n\u003cp\u003eDeepSeek demonstrates a declining emphasis on ever-increasing amounts of pre-training as the sole driver of model quality. Instead, the development underscores the growing importance of TTC. While the direct adoption of DeepSeek models in enterprise software applications remains uncertain due to ongoing scrutiny, their impact on driving improvements in other existing models is becoming clearer. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe believe that DeepSeek’s advancements have prompted established AI labs to incorporate similar techniques into their engineering and research processes, supplementing their existing hardware advantages. The resulting reduction in model costs, as predicted, appears to be contributing to increased model usage, aligning with the principles of Jevons Paradox.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003ePashootan Vaezipoor is technical lead at Georgian.\u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-04-05T19:15:00Z",
  "modifiedTime": "2025-04-04T22:10:08Z"
}
