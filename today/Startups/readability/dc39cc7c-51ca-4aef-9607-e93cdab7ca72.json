{
  "id": "dc39cc7c-51ca-4aef-9607-e93cdab7ca72",
  "title": "You can now fine-tune your enterprise’s own version of OpenAI’s o4-mini reasoning model with reinforcement learning",
  "link": "https://venturebeat.com/ai/you-can-now-fine-tune-your-enterprises-own-version-of-openais-o4-mini-reasoning-model-with-reinforcement-learning/",
  "description": "For organizations with clearly defined problems and verifiable answers, RFT offers a compelling way to align models.",
  "author": "Carl Franzen",
  "published": "Fri, 09 May 2025 00:07:32 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "Programming \u0026 Development",
    "AI, ML and Deep Learning",
    "ChatGPT",
    "Conversational AI",
    "developer tools",
    "developers",
    "fine tuning",
    "grader",
    "LLMs",
    "multimodal ai",
    "multimodal large language model",
    "multimodal LLMs",
    "multimodal models",
    "NLP",
    "o4-mini",
    "o4-mini AI model",
    "OpenAI",
    "reinforcement fine tuning",
    "RFT"
  ],
  "byline": "Carl Franzen",
  "length": 7684,
  "excerpt": "For organizations with clearly defined problems and verifiable answers, RFT offers a compelling way to align models.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "May 8, 2025 5:07 PM Credit: VentureBeat made with Midjourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More OpenAI today announced on its developer-focused account on the social network X that third-party software developers outside the company can now access reinforcement fine-tuning (RFT) for its new o4-mini language reasoning model, enabling them to customize a new, private version of it based on their enterprise’s unique products, internal terminology, goals, employees, processes, and more. Essentially, this capability lets developers take the model available to the general public and tweak it to better fit their needs using OpenAI’s platform dashboard. Then, they can deploy it through OpenAI’s application programming interface (API), another part of its developer platform, and connect it to their internal employee computers, databases, and applications. Once deployed, if an employee or leader at the company wants to use it through a custom internal chatbot or custom OpenAI GPT to pull up private, proprietary company knowledge; or to answer specific questions about company products and policies; or generate new communications and collateral in the company’s voice, they can do so more easily with their RFT version of the model. However, one cautionary note: research has shown that fine-tuned models may be more prone to jailbreaks and hallucinations, so proceed cautiously! This launch expands the company’s model optimization tools beyond supervised fine-tuning (SFT) and introduces more flexible control for complex, domain-specific tasks. Additionally, OpenAI announced that supervised fine-tuning is now supported for its GPT-4.1 nano model, the company’s most affordable and fastest offering to date. How does Reinforcement Fine-Tuning (RFT) help organizations and enterprises? RFT creates a new version of OpenAI’s o4-mini reasoning model that is automatically adapted to the user’s goals, or those of their enterprise/organization. It does so by applying a feedback loop during training, which developers at large enterprises (or even independent developers working on their own) can now initiate relatively simply, easily, and affordably through OpenAI’s online developer platform. Instead of training on a set of questions with fixed correct answers — which is what traditional supervised learning does — RFT uses a grader model to score multiple candidate responses per prompt. The training algorithm then adjusts model weights so that high-scoring outputs become more likely. This structure allows customers to align models with nuanced objectives such as an enterprise’s “house style” of communication and terminology, safety rules, factual accuracy, or internal policy compliance. To perform RFT, users need to: Define a grading function or use OpenAI model-based graders. Upload a dataset with prompts and validation splits. Configure a training job via API or the fine-tuning dashboard. Monitor progress, review checkpoints, and iterate on data or grading logic. RFT currently supports only o-series reasoning models and is available for the o4-mini model. Early enterprise use cases On its platform, OpenAI highlighted several early customers who have adopted RFT across diverse industries: Accordance AI used RFT to fine-tune a model for complex tax analysis tasks, achieving a 39% improvement in accuracy and outperforming all leading models on tax reasoning benchmarks. Ambience Healthcare applied RFT to ICD-10 medical code assignment, raising model performance by 12 points over physician baselines on a gold-panel dataset. Harvey used RFT for legal document analysis, improving citation extraction F1 scores by 20% and matching GPT-4o in accuracy while achieving faster inference. Runloop fine-tuned models for generating Stripe API code snippets, using syntax-aware graders and AST validation logic, achieving a 12% improvement. Milo applied RFT to scheduling tasks, boosting correctness in high-complexity situations by 25 points. SafetyKit used RFT to enforce nuanced content moderation policies and increased model F1 from 86% to 90% in production. ChipStack, Thomson Reuters, and other partners also demonstrated performance gains in structured data generation, legal comparison tasks, and verification workflows. These cases often shared characteristics: clear task definitions, structured output formats, and reliable evaluation criteria—all essential for effective reinforcement fine-tuning. RFT is available now to verified organizations. OpenAI is offering a 50% discount to teams that choose to share their training datasets with OpenAI to help improve future models. Interested developers can get started using OpenAI’s RFT documentation and dashboard. Pricing and billing structure Unlike supervised or preference fine-tuning, which is billed per token, RFT is billed based on time spent actively training. Specifically: $100 per hour of core training time (wall-clock time during model rollouts, grading, updates, and validation). Time is prorated by the second, rounded to two decimal places (so 1.8 hours of training would cost the customer $180). Charges apply only to work that modifies the model. Queues, safety checks, and idle setup phases are not billed. If the user employs OpenAI models as graders (e.g., GPT-4.1), the inference tokens consumed during grading are billed separately at OpenAI’s standard API rates. Otherwise, the company can use outside models, including open source ones, as graders. Here is an example cost breakdown: ScenarioBillable TimeCost4 hours training4 hours$4001.75 hours (prorated)1.75 hours$1752 hours training + 1 hour lost (due to failure)2 hours$200 This pricing model provides transparency and rewards efficient job design. To control costs, OpenAI encourages teams to: Use lightweight or efficient graders where possible. Avoid overly frequent validation unless necessary. Start with smaller datasets or shorter runs to calibrate expectations. Monitor training with API or dashboard tools and pause as needed. OpenAI uses a billing method called “captured forward progress,” meaning users are only billed for model training steps that were successfully completed and retained. So should your organization invest in RFTing a custom version of OpenAI’s o4-mini or not? Reinforcement fine-tuning introduces a more expressive and controllable method for adapting language models to real-world use cases. With support for structured outputs, code-based and model-based graders, and full API control, RFT enables a new level of customization in model deployment. OpenAI’s rollout emphasizes thoughtful task design and robust evaluation as keys to success. Developers interested in exploring this method can access documentation and examples via OpenAI’s fine-tuning dashboard. For organizations with clearly defined problems and verifiable answers, RFT offers a compelling way to align models with operational or compliance goals — without building RL infrastructure from scratch. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/05/cfr0z3n_sleek_robots_in_uniforms_adjust_dials_on_a_vintage_swit_0554196c-8d1b-40c5-bc71-3324d595cbfe.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-05-09T00:07:32+00:00\" datetime=\"2025-05-09T00:07:32+00:00\"\u003eMay 8, 2025 5:07 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2025/05/cfr0z3n_sleek_robots_in_uniforms_adjust_dials_on_a_vintage_swit_0554196c-8d1b-40c5-bc71-3324d595cbfe.png?w=750\" alt=\"Two blocky blue humanoid robots in suits stare at and one of them manipulates red and blue dials on a large rectangular switchboard against a dark deep blue backdrop\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eOpenAI today announced on its \u003ca href=\"https://x.com/OpenAIDevs/status/1920531856426143825\"\u003edeveloper-focused account on the social network X\u003c/a\u003e that third-party software developers outside the company can now access reinforcement fine-tuning (RFT) for its new \u003ca href=\"https://venturebeat.com/ai/openai-launches-o3-and-o4-mini-ai-models-that-think-with-images-and-use-tools-autonomously/\"\u003eo4-mini language reasoning model\u003c/a\u003e, enabling them to customize a new, private version of it based on their enterprise’s unique products, internal terminology, goals, employees, processes, and more. \u003c/p\u003e\n\n\n\n\u003cp\u003eEssentially, this capability lets developers take the model available to the general public and tweak it to better fit their needs using \u003ca href=\"https://platform.openai.com/finetune\"\u003eOpenAI’s platform dashboard\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eThen, they can deploy it through OpenAI’s application programming interface (API), another part of its developer platform, and connect it to their internal employee computers, databases, and applications.\u003c/p\u003e\n\n\n\n\u003cp\u003eOnce deployed, if an employee or leader at the company wants to use it through a custom internal chatbot or \u003ca href=\"https://community.openai.com/t/how-to-use-an-own-fine-tuned-model-in-customgpt/793627\"\u003ecustom OpenAI GPT\u003c/a\u003e to pull up private, proprietary company knowledge; or to answer specific questions about company products and policies; or generate new communications and collateral in the company’s voice, they can do so more easily with their RFT version of the model. \u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, one cautionary note: research has shown that \u003ca href=\"https://venturebeat.com/ai/cisco-warns-fine-tuning-turns-llms-into-threat-vectorsstructure/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003efine-tuned models may be more prone to jailbreaks and hallucinations\u003c/a\u003e, so proceed cautiously!\u003c/p\u003e\n\n\n\n\u003cp\u003eThis launch expands the company’s model optimization tools beyond supervised fine-tuning (SFT) and introduces more flexible control for complex, domain-specific tasks. \u003c/p\u003e\n\n\n\n\u003cp\u003eAdditionally, OpenAI announced that supervised fine-tuning is now supported for its GPT-4.1 nano model, the company’s most affordable and fastest offering to date.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-does-reinforcement-fine-tuning-rft-help-organizations-and-enterprises\"\u003eHow does Reinforcement Fine-Tuning (RFT) help organizations and enterprises?\u003c/h2\u003e\n\n\n\n\u003cp\u003eRFT creates a new version of OpenAI’s o4-mini reasoning model that is automatically adapted to the user’s goals, or those of their enterprise/organization. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt does so by applying a feedback loop during training, which developers at large enterprises (or even independent developers working on their own) can now initiate relatively simply, easily, and affordably through \u003ca href=\"https://platform.openai.com/docs/guides/reinforcement-fine-tuning\"\u003eOpenAI’s online developer platform\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eInstead of training on a set of questions with fixed correct answers — which is what traditional supervised learning does — RFT uses a grader model to score multiple candidate responses per prompt.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe training algorithm then adjusts model weights so that high-scoring outputs become more likely. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis structure allows customers to align models with nuanced objectives such as an enterprise’s “house style” of communication and terminology, safety rules, factual accuracy, or internal policy compliance.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo perform RFT, users need to:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eDefine a grading function or use OpenAI model-based graders.\u003c/li\u003e\n\n\n\n\u003cli\u003eUpload a dataset with prompts and validation splits.\u003c/li\u003e\n\n\n\n\u003cli\u003eConfigure a training job via API or the fine-tuning dashboard.\u003c/li\u003e\n\n\n\n\u003cli\u003eMonitor progress, review checkpoints, and iterate on data or grading logic.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eRFT currently supports only o-series reasoning models and is available for the o4-mini model.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-early-enterprise-use-cases\"\u003eEarly enterprise use cases\u003c/h2\u003e\n\n\n\n\u003cp\u003eOn its platform, \u003ca href=\"https://platform.openai.com/docs/guides/rft-use-cases\"\u003eOpenAI highlighted several early customers\u003c/a\u003e who have adopted RFT across diverse industries:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAccordance AI\u003c/strong\u003e used RFT to fine-tune a model for complex tax analysis tasks, achieving a 39% improvement in accuracy and outperforming all leading models on tax reasoning benchmarks.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eAmbience Healthcare\u003c/strong\u003e applied RFT to ICD-10 medical code assignment, raising model performance by 12 points over physician baselines on a gold-panel dataset.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eHarvey\u003c/strong\u003e used RFT for legal document analysis, improving citation extraction F1 scores by 20% and matching GPT-4o in accuracy while achieving faster inference.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eRunloop\u003c/strong\u003e fine-tuned models for generating Stripe API code snippets, using syntax-aware graders and AST validation logic, achieving a 12% improvement.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eMilo\u003c/strong\u003e applied RFT to scheduling tasks, boosting correctness in high-complexity situations by 25 points.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eSafetyKit\u003c/strong\u003e used RFT to enforce nuanced content moderation policies and increased model F1 from 86% to 90% in production.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eChipStack\u003c/strong\u003e, \u003cstrong\u003eThomson Reuters\u003c/strong\u003e, and other partners also demonstrated performance gains in structured data generation, legal comparison tasks, and verification workflows.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eThese cases often shared characteristics: clear task definitions, structured output formats, and reliable evaluation criteria—all essential for effective reinforcement fine-tuning.\u003c/p\u003e\n\n\n\n\u003cp\u003eRFT is available now to verified organizations. OpenAI is offering a 50% discount to teams that choose to share their training datasets with OpenAI to help improve future models. Interested developers can get started using \u003ca href=\"https://platform.openai.com/docs/guides/reinforcement-fine-tuning\"\u003eOpenAI’s RFT documentation\u003c/a\u003e and \u003ca href=\"https://platform.openai.com/finetune\"\u003edashboard\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-pricing-and-billing-structure\"\u003ePricing and billing structure\u003c/h2\u003e\n\n\n\n\u003cp\u003eUnlike supervised or preference fine-tuning, which is billed per token, RFT is billed based on time spent actively training. Specifically:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e$100 per hour of core training time (wall-clock time during model rollouts, grading, updates, and validation).\u003c/li\u003e\n\n\n\n\u003cli\u003eTime is prorated by the second, rounded to two decimal places (so 1.8 hours of training would cost the customer $180).\u003c/li\u003e\n\n\n\n\u003cli\u003eCharges apply only to work that modifies the model. Queues, safety checks, and idle setup phases are not billed.\u003c/li\u003e\n\n\n\n\u003cli\u003eIf the user employs OpenAI models as graders (e.g., GPT-4.1), the inference tokens consumed during grading are billed separately at OpenAI’s standard API rates. Otherwise, the company can use outside models, including open source ones, as graders.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eHere is an example cost breakdown:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e\u003cstrong\u003eScenario\u003c/strong\u003e\u003c/th\u003e\u003cth\u003e\u003cstrong\u003eBillable Time\u003c/strong\u003e\u003c/th\u003e\u003cth\u003e\u003cstrong\u003eCost\u003c/strong\u003e\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e4 hours training\u003c/td\u003e\u003ctd\u003e4 hours\u003c/td\u003e\u003ctd\u003e$400\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e1.75 hours (prorated)\u003c/td\u003e\u003ctd\u003e1.75 hours\u003c/td\u003e\u003ctd\u003e$175\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e2 hours training + 1 hour lost (due to failure)\u003c/td\u003e\u003ctd\u003e2 hours\u003c/td\u003e\u003ctd\u003e$200\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThis pricing model provides transparency and rewards efficient job design. To control costs, OpenAI encourages teams to:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eUse lightweight or efficient graders where possible.\u003c/li\u003e\n\n\n\n\u003cli\u003eAvoid overly frequent validation unless necessary.\u003c/li\u003e\n\n\n\n\u003cli\u003eStart with smaller datasets or shorter runs to calibrate expectations.\u003c/li\u003e\n\n\n\n\u003cli\u003eMonitor training with API or dashboard tools and pause as needed.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eOpenAI uses a billing method called “captured forward progress,” meaning users are only billed for model training steps that were successfully completed and retained.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-so-should-your-organization-invest-in-rfting-a-custom-version-of-openai-s-o4-mini-or-not\"\u003eSo should your organization invest in RFTing a custom version of OpenAI’s o4-mini or not?\u003c/h2\u003e\n\n\n\n\u003cp\u003eReinforcement fine-tuning introduces a more expressive and controllable method for adapting language models to real-world use cases. \u003c/p\u003e\n\n\n\n\u003cp\u003eWith support for structured outputs, code-based and model-based graders, and full API control, RFT enables a new level of customization in model deployment. OpenAI’s rollout emphasizes thoughtful task design and robust evaluation as keys to success.\u003c/p\u003e\n\n\n\n\u003cp\u003eDevelopers interested in exploring this method can access documentation and examples via OpenAI’s fine-tuning dashboard. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor organizations with clearly defined problems and verifiable answers, RFT offers a compelling way to align models with operational or compliance goals — without building RL infrastructure from scratch.\u003c/p\u003e\n\n\n\n\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-05-09T00:07:32Z",
  "modifiedTime": "2025-05-09T00:07:50Z"
}
