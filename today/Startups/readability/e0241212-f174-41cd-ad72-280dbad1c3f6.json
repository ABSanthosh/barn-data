{
  "id": "e0241212-f174-41cd-ad72-280dbad1c3f6",
  "title": "SWiRL: The business case for AI that thinks like your best problem-solvers",
  "link": "https://venturebeat.com/ai/swirl-the-business-case-for-ai-that-thinks-like-your-best-problem-solvers/",
  "description": "Training LLMs on trajectories of reasoning and tool use makes them superior at multi-step reasoning tasks.",
  "author": "Ben Dickson",
  "published": "Tue, 22 Apr 2025 23:53:59 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "Data Infrastructure",
    "AI agents",
    "AI research",
    "AI tool use",
    "AI, ML and Deep Learning",
    "data",
    "DeepMind",
    "large language models",
    "LLM reasoning",
    "LLMs",
    "reasoning models",
    "reinforcement learning from human feedback (RLHF)",
    "research",
    "Stanford University",
    "Step-Wise Reinforcement Learning (SWiRL)",
    "swirl"
  ],
  "byline": "Ben Dickson",
  "length": 9493,
  "excerpt": "Training LLMs on trajectories of reasoning and tool use makes them superior at multi-step reasoning tasks.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "April 22, 2025 4:53 PM Image credit: VentureBeat with Ideogram Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Researchers from Stanford University and Google DeepMind have unveiled Step-Wise Reinforcement Learning (SWiRL), a technique designed to enhance the ability of large language models (LLMs) to tackle complex tasks requiring multi-step reasoning and tool use.  As the interest in AI agents and LLM tool use continues to increase, this technique could offer substantial benefits for enterprises looking to integrate reasoning models into their applications and workflows. The challenge of multi-step problems Real-world enterprise applications often involve multi-step processes. For example, planning a complex marketing campaign may involve market research, internal data analysis, budget calculation and reviewing customer support tickets. This requires online searches, access to internal databases and running code. Traditional reinforcement learning (RL) methods used to fine-tune LLMs, such as Reinforcement Learning from Human Feedback (RLHF) or RL from AI Feedback (RLAIF), typically focus on optimizing models for single-step reasoning tasks.  The lead authors of the SWiRL paper, Anna Goldie, research scientist at Google DeepMind, and Azalia Mirhosseini, assistant professor of computer science at Stanford University, believe that current LLM training methods are not suited for the multi-step reasoning tasks that real-world applications require. “LLMs trained via traditional methods typically struggle with multi-step planning and tool integration, meaning that they have difficulty performing tasks that require retrieving and synthesizing documents from multiple sources (e.g., writing a business report) or multiple steps of reasoning and arithmetic calculation (e.g., preparing a financial summary),” they told VentureBeat. Step-Wise Reinforcement Learning (SWiRL) SWiRL tackles this multi-step challenge through a combination of synthetic data generation and a specialized RL approach that trains models on entire sequences of actions.  As the researchers state in their paper, “Our goal is to teach the model how to decompose complex problems into a sequence of more manageable subtasks, when to call the tool, how to formulate a call to the tool, when to use the results of these queries to answer the question, and how to effectively synthesize its findings.” SWiRL employs a two-stage methodology. First, it generates and filters large amounts of multi-step reasoning and tool-use data. Second, it uses a step-wise RL algorithm to optimize a base LLM using these generated trajectories.  “This approach has the key practical advantage that we can quickly generate large volumes of multi-step training data via parallel calls to avoid throttling the training process with slow tool use execution,” the paper notes. “In addition, this offline process enables greater reproducibility due to having a fixed dataset.” Generating training data SWiRL data generation process Credit: arXiv The first stage involves creating the synthetic data SWiRL learns from. An LLM is given access to a relevant tool, like a search engine or a calculator. The model is then prompted iteratively to generate a “trajectory,” a sequence of steps to solve a given problem. At each step, the model can generate internal reasoning (its “chain of thought“), call a tool, or produce the final answer. If it calls a tool, the query is extracted, executed (e.g., a search is performed), and the result is fed back into the model’s context for the next step. This continues until the model provides a final answer. Each complete trajectory, from the initial prompt to the final answer, is then broken down into multiple overlapping sub-trajectories. Each sub-trajectory represents the process up to a specific action, providing a granular view of the model’s step-by-step reasoning. Using this method, the team compiled large datasets based on questions from multi-hop question-answering (HotPotQA) and math problem-solving (GSM8K) benchmarks, generating tens of thousands of trajectories. The researchers explored four different data filtering strategies: no filtering, filtering based solely on the correctness of the final answer (outcome filtering), filtering based on the judged reasonableness of each individual step (process filtering) and filtering based on both process and outcome. Many standard approaches, such as Supervised Fine-Tuning (SFT), rely heavily on “golden labels” (perfect, predefined correct answers) and often discard data that does not lead to the correct final answer. Recent popular RL approaches, such as the one used in DeepSeek-R1, also use outcome-based rewards to train the model. In contrast, SWiRL achieved its best results using process-filtered data. This means the data included trajectories where each reasoning step or tool call was deemed logical given the previous context, even if the final answer turned out to be wrong.  The researchers found that SWiRL can “learn even from trajectories that end in incorrect final answers. In fact, we achieve our best results by including process-filtered data, regardless of the correctness of the outcome.”  Training LLMs with SWiRL SWiRL training process Credit:arXiv In the second stage, SWiRL uses reinforcement learning to train a base LLM on the generated synthetic trajectories. At every step within a trajectory, the model is optimized to predict the next appropriate action (an intermediate reasoning step, a tool call, or the final answer) based on the preceding context. The LLM receives feedback at each step by a separate generative reward model, which assesses the model’s generated action given the context up to that point.  “Our granular, step-by-step finetuning paradigm enables the model to learn both local decision-making (next-step prediction) and global trajectory optimization (final response generation) while being guided by immediate feedback on the soundness of each prediction,” the researchers write. SWiRL during inference Credit: arXiv At inference time, a SWiRL-trained model works in the same iterative fashion. It receives a prompt and generates text in response. If it outputs a tool call (such as a search query or a mathematical expression), the system parses it, executes the tool, and feeds the result back into the model’s context window. The model then continues generating, potentially making more tool calls, until it outputs a final answer or reaches a pre-set limit on the number of steps. “By training the model to take reasonable steps at each moment in time (and to do so in a coherent and potentially more explainable way), we address a core weakness of traditional LLMs, namely their brittleness in the face of complex, multi-step tasks, where the probability of success decays exponentially with path length,” Goldie and Mirhoseini said. “Useful and robust Enterprise AI will inevitably need to integrate a wide variety of different tools, chaining them together into complex sequences.” SWiRL in action The Stanford and Google DeepMind team evaluated SWiRL across several challenging multi-step question-answering and mathematical reasoning tasks. Compared to baseline models, SWiRL demonstrated significant relative accuracy improvements, ranging from 11% to over 21% on datasets like GSM8K, HotPotQA, MuSiQue and BeerQA. The experiments confirmed that training a Gemma 2-27B model with SWiRL on process-filtered data yielded the best results, outperforming models trained on outcome-filtered data or using traditional SFT. This suggests SWiRL learns the underlying reasoning process more effectively, rather than just memorizing paths to correct answers, which aids performance on unseen problems. More importantly, SWiRL exhibited strong generalization capabilities. For example, training a model using SWiRL on text-based question-answering examples improved its performance on math reasoning tasks, even though the model wasn’t explicitly trained on math problems.  This transferability across different tasks and tool types is highly valuable as there is an explosion of agentic applications for language models, and methods that generalize across datasets and tasks will be easier, cheaper and faster to adapt to new environments. “SWiRL’s generalization seems quite robust in the domains that we explored, but it would be interesting to test this in other areas such as coding,” Goldie and Mirhoseini said. “Our findings suggest that an enterprise AI model trained on one core task using SWiRL would likely exhibit significant performance improvements on other, seemingly unrelated tasks without task-specific fine-tuning. SWiRL generalizes better when applied to larger (i.e. more powerful) models, indicating that this technique may be even more effective in the future as baseline capabilities grow.” Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/04/robot-solving-problem.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-04-22T23:53:59+00:00\" datetime=\"2025-04-22T23:53:59+00:00\"\u003eApril 22, 2025 4:53 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"421\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/robot-solving-problem.webp?w=750\" alt=\"Image credit: VentureBeat with Ideogram\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eImage credit: VentureBeat with Ideogram\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eResearchers from \u003ca href=\"https://www.stanford.edu/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eStanford University\u003c/a\u003e and \u003ca href=\"https://deepmind.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle DeepMind\u003c/a\u003e have unveiled \u003ca href=\"https://arxiv.org/abs/2504.04736\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eStep-Wise Reinforcement Learning\u003c/a\u003e (SWiRL), a technique designed to enhance the ability of large language models (LLMs) to tackle complex tasks requiring multi-step reasoning and tool use. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs the interest in AI agents and \u003ca href=\"https://venturebeat.com/ai/octotools-stanfords-open-source-framework-optimizes-llm-reasoning-through-modular-tool-orchestration/\"\u003eLLM tool use\u003c/a\u003e continues to increase, this technique could offer substantial benefits for enterprises looking to integrate reasoning models into their applications and workflows.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-challenge-of-multi-step-problems\"\u003eThe challenge of multi-step problems\u003c/h2\u003e\n\n\n\n\u003cp\u003eReal-world enterprise applications often involve multi-step processes. For example, planning a complex marketing campaign may involve market research, internal data analysis, budget calculation and reviewing customer support tickets. This requires online searches, access to internal databases and running code.\u003c/p\u003e\n\n\n\n\u003cp\u003eTraditional reinforcement learning (RL) methods used to fine-tune LLMs, such as Reinforcement Learning from Human Feedback (\u003ca href=\"https://bdtechtalks.com/2023/01/16/what-is-rlhf/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRLHF\u003c/a\u003e) or RL from AI Feedback (\u003ca href=\"https://www.datacamp.com/blog/rlaif-reinforcement-learning-from-ai-feedback\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRLAIF\u003c/a\u003e), typically focus on optimizing models for single-step reasoning tasks. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe lead authors of the SWiRL paper, Anna Goldie, research scientist at Google DeepMind, and Azalia Mirhosseini, assistant professor of computer science at Stanford University, believe that current LLM training methods are not suited for the multi-step reasoning tasks that real-world applications require.\u003c/p\u003e\n\n\n\n\u003cp\u003e“LLMs trained via traditional methods typically struggle with multi-step planning and tool integration, meaning that they have difficulty performing tasks that require retrieving and synthesizing documents from multiple sources (e.g., writing a business report) or multiple steps of reasoning and arithmetic calculation (e.g., preparing a financial summary),” they told VentureBeat.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-step-wise-reinforcement-learning-swirl\"\u003eStep-Wise Reinforcement Learning (SWiRL)\u003c/h2\u003e\n\n\n\n\u003cp\u003eSWiRL tackles this multi-step challenge through a combination of synthetic data generation and a specialized RL approach that trains models on entire sequences of actions. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs the researchers state in \u003ca href=\"https://arxiv.org/abs/2504.04736\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etheir paper\u003c/a\u003e, “Our goal is to teach the model how to decompose complex problems into a sequence of more manageable subtasks, when to call the tool, how to formulate a call to the tool, when to use the results of these queries to answer the question, and how to effectively synthesize its findings.”\u003c/p\u003e\n\n\n\n\u003cp\u003eSWiRL employs a two-stage methodology. First, it generates and filters large amounts of multi-step reasoning and tool-use data. Second, it uses a step-wise RL algorithm to optimize a base LLM using these generated trajectories. \u003c/p\u003e\n\n\n\n\u003cp\u003e“This approach has the key practical advantage that we can quickly generate large volumes of multi-step training data via parallel calls to avoid throttling the training process with slow tool use execution,” the paper notes. “In addition, this offline process enables greater reproducibility due to having a fixed dataset.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-generating-training-data\"\u003eGenerating training data\u003c/h2\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"974\" height=\"516\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_9db185.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_9db185.png 974w, https://venturebeat.com/wp-content/uploads/2025/04/image_9db185.png?resize=300,159 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_9db185.png?resize=768,407 768w, https://venturebeat.com/wp-content/uploads/2025/04/image_9db185.png?resize=800,424 800w, https://venturebeat.com/wp-content/uploads/2025/04/image_9db185.png?resize=400,212 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_9db185.png?resize=750,397 750w, https://venturebeat.com/wp-content/uploads/2025/04/image_9db185.png?resize=578,306 578w, https://venturebeat.com/wp-content/uploads/2025/04/image_9db185.png?resize=930,493 930w\" sizes=\"(max-width: 974px) 100vw, 974px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eSWiRL data generation process Credit: arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe first stage involves creating the synthetic data SWiRL learns from. An LLM is given access to a relevant tool, like a search engine or a calculator. The model is then prompted iteratively to generate a “trajectory,” a sequence of steps to solve a given problem. At each step, the model can generate internal reasoning (its “\u003ca href=\"https://venturebeat.com/ai/not-every-ai-prompt-deserves-multiple-seconds-of-thinking-how-meta-is-teaching-models-to-prioritize/\"\u003echain of thought\u003c/a\u003e“), call a tool, or produce the final answer. If it calls a tool, the query is extracted, executed (e.g., a search is performed), and the result is fed back into the model’s context for the next step. This continues until the model provides a final answer.\u003c/p\u003e\n\n\n\n\u003cp\u003eEach complete trajectory, from the initial prompt to the final answer, is then broken down into multiple overlapping sub-trajectories. Each sub-trajectory represents the process up to a specific action, providing a granular view of the model’s step-by-step reasoning. Using this method, the team compiled large datasets based on questions from multi-hop question-answering (HotPotQA) and math problem-solving (GSM8K) benchmarks, generating tens of thousands of trajectories.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers explored four different data filtering strategies: no filtering, filtering based solely on the correctness of the final answer (outcome filtering), filtering based on the judged reasonableness of each individual step (process filtering) and filtering based on both process and outcome.\u003c/p\u003e\n\n\n\n\u003cp\u003eMany standard approaches, such as Supervised Fine-Tuning (SFT), rely heavily on “golden labels” (perfect, predefined correct answers) and often discard data that does not lead to the correct final answer. Recent popular RL approaches, such as the one used in \u003ca href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/\"\u003eDeepSeek-R1\u003c/a\u003e, also use outcome-based rewards to train the model.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn contrast, SWiRL achieved its best results using process-filtered data. This means the data included trajectories where each reasoning step or tool call was deemed logical given the previous context, even if the final answer turned out to be wrong. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe researchers found that SWiRL can “learn even from trajectories that end in incorrect final answers. In fact, we achieve our best results by including process-filtered data, regardless of the correctness of the outcome.” \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-training-llms-with-swirl\"\u003eTraining LLMs with SWiRL\u003c/h2\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1218\" height=\"472\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_d6f9b2.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_d6f9b2.png 1218w, https://venturebeat.com/wp-content/uploads/2025/04/image_d6f9b2.png?resize=300,116 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_d6f9b2.png?resize=768,298 768w, https://venturebeat.com/wp-content/uploads/2025/04/image_d6f9b2.png?resize=800,310 800w, https://venturebeat.com/wp-content/uploads/2025/04/image_d6f9b2.png?resize=400,155 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_d6f9b2.png?resize=750,291 750w, https://venturebeat.com/wp-content/uploads/2025/04/image_d6f9b2.png?resize=578,224 578w, https://venturebeat.com/wp-content/uploads/2025/04/image_d6f9b2.png?resize=930,360 930w\" sizes=\"(max-width: 1218px) 100vw, 1218px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eSWiRL training process Credit:arXiv\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIn the second stage, SWiRL uses reinforcement learning to train a base LLM on the generated synthetic trajectories. At every step within a trajectory, the model is optimized to predict the next appropriate action (an intermediate reasoning step, a tool call, or the final answer) based on the preceding context.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe LLM receives feedback at each step by a separate \u003ca href=\"https://venturebeat.com/ai/deepminds-genrm-improves-llm-accuracy-by-having-models-verify-their-own-outputs/\"\u003egenerative reward model\u003c/a\u003e, which assesses the model’s generated action given the context up to that point. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Our granular, step-by-step finetuning paradigm enables the model to learn both local decision-making (next-step prediction) and global trajectory optimization (final response generation) while being guided by immediate feedback on the soundness of each prediction,” the researchers write.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1210\" height=\"574\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_9f1960.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_9f1960.png 1210w, https://venturebeat.com/wp-content/uploads/2025/04/image_9f1960.png?resize=300,142 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_9f1960.png?resize=768,364 768w, https://venturebeat.com/wp-content/uploads/2025/04/image_9f1960.png?resize=800,380 800w, https://venturebeat.com/wp-content/uploads/2025/04/image_9f1960.png?resize=400,190 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_9f1960.png?resize=750,356 750w, https://venturebeat.com/wp-content/uploads/2025/04/image_9f1960.png?resize=578,274 578w, https://venturebeat.com/wp-content/uploads/2025/04/image_9f1960.png?resize=930,441 930w\" sizes=\"(max-width: 1210px) 100vw, 1210px\"/\u003e\u003cfigcaption\u003eSWiRL during inference Credit: arXiv\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAt inference time, a SWiRL-trained model works in the same iterative fashion. It receives a prompt and generates text in response. If it outputs a tool call (such as a search query or a mathematical expression), the system parses it, executes the tool, and feeds the result back into the model’s context window. The model then continues generating, potentially making more tool calls, until it outputs a final answer or reaches a pre-set limit on the number of steps.\u003c/p\u003e\n\n\n\n\u003cp\u003e“By training the model to take reasonable steps at each moment in time (and to do so in a coherent and potentially more explainable way), we address a core weakness of traditional LLMs, namely their brittleness in the face of complex, multi-step tasks, where the probability of success decays exponentially with path length,” Goldie and Mirhoseini said. “Useful and robust Enterprise AI will inevitably need to integrate a wide variety of different tools, chaining them together into complex sequences.”\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-swirl-in-action\"\u003eSWiRL in action\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Stanford and Google DeepMind team evaluated SWiRL across several challenging multi-step question-answering and mathematical reasoning tasks. Compared to baseline models, SWiRL demonstrated significant relative accuracy improvements, ranging from 11% to over 21% on datasets like GSM8K, HotPotQA, MuSiQue and BeerQA.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe experiments confirmed that training a Gemma 2-27B model with SWiRL on process-filtered data yielded the best results, outperforming models trained on outcome-filtered data or using traditional SFT. This suggests SWiRL learns the underlying reasoning process more effectively, rather than just memorizing paths to correct answers, which aids performance on unseen problems.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1004\" height=\"536\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image_4a2c4b.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/image_4a2c4b.png 1004w, https://venturebeat.com/wp-content/uploads/2025/04/image_4a2c4b.png?resize=300,160 300w, https://venturebeat.com/wp-content/uploads/2025/04/image_4a2c4b.png?resize=768,410 768w, https://venturebeat.com/wp-content/uploads/2025/04/image_4a2c4b.png?resize=800,427 800w, https://venturebeat.com/wp-content/uploads/2025/04/image_4a2c4b.png?resize=400,214 400w, https://venturebeat.com/wp-content/uploads/2025/04/image_4a2c4b.png?resize=750,400 750w, https://venturebeat.com/wp-content/uploads/2025/04/image_4a2c4b.png?resize=578,309 578w, https://venturebeat.com/wp-content/uploads/2025/04/image_4a2c4b.png?resize=930,496 930w\" sizes=\"auto, (max-width: 1004px) 100vw, 1004px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eMore importantly, SWiRL exhibited strong generalization capabilities. For example, training a model using SWiRL on text-based question-answering examples improved its performance on math reasoning tasks, even though the model wasn’t explicitly trained on math problems. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis transferability across different tasks and tool types is highly valuable as there is an explosion of agentic applications for language models, and methods that generalize across datasets and tasks will be easier, cheaper and faster to adapt to new environments.\u003c/p\u003e\n\n\n\n\u003cp\u003e“SWiRL’s generalization seems quite robust in the domains that we explored, but it would be interesting to test this in other areas such as coding,” Goldie and Mirhoseini said. “Our findings suggest that an enterprise AI model trained on one core task using SWiRL would likely exhibit significant performance improvements on other, seemingly unrelated tasks without task-specific fine-tuning. SWiRL generalizes better when applied to larger (i.e. more powerful) models, indicating that this technique may be even more effective in the future as baseline capabilities grow.”\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-04-22T23:53:59Z",
  "modifiedTime": "2025-04-22T23:54:10Z"
}
