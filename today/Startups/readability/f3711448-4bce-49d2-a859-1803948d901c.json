{
  "id": "f3711448-4bce-49d2-a859-1803948d901c",
  "title": "Your AI models are failing in production—Here’s how to fix model selection",
  "link": "https://venturebeat.com/ai/your-ai-models-are-failing-in-production-heres-how-to-fix-model-selection/",
  "description": "The Allen Institute of AI updated its reward model evaluation RewardBench to better reflect real-life scenarios for enterprises.",
  "author": "Emilia David",
  "published": "Tue, 03 Jun 2025 23:47:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "AI model evaluation",
    "AI, ML and Deep Learning",
    "Allen Institute for Artificial Intelligence",
    "benchmarks",
    "model evaluation",
    "reinforcement learning from human feedback (RLHF)",
    "reward models (RMs)"
  ],
  "byline": "Emilia David",
  "length": 5724,
  "excerpt": "The Allen Institute of AI updated its reward model evaluation RewardBench to better reflect real-life scenarios for enterprises.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "June 3, 2025 4:47 PM Credit: VentureBeat, generated with MidJourney Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Enterprises need to know if the models that power their applications and agents work in real-life scenarios. This type of evaluation can sometimes be complex because it is hard to predict specific scenarios. A revamped version of the RewardBench benchmark looks to give organizations a better idea of a model’s real-life performance.  The Allen Institute of AI (Ai2) launched RewardBench 2, an updated version of its reward model benchmark, RewardBench, which they claim provides a more holistic view of model performance and assesses how models align with an enterprise’s goals and standards.  Ai2 built RewardBench with classification tasks that measure correlations through inference-time compute and downstream training. RewardBench mainly deals with reward models (RM), which can act as judges and evaluate LLM outputs. RMs assign a score or a “reward” that guides reinforcement learning with human feedback (RHLF). RewardBench 2 is here! We took a long time to learn from our first reward model evaluation tool to make one that is substantially harder and more correlated with both downstream RLHF and inference-time scaling. pic.twitter.com/NGetvNrOQV— Ai2 (@allen_ai) June 2, 2025 Nathan Lambert, a senior research scientist at Ai2, told VentureBeat that the first RewardBench worked as intended when it was launched. Still, the model environment rapidly evolved, and so should its benchmarks.  “As reward models became more advanced and use cases more nuanced, we quickly recognized with the community that the first version didn’t fully capture the complexity of real-world human preferences,” he said.  Lambert added that with RewardBench 2, “we set out to improve both the breadth and depth of evaluation—incorporating more diverse, challenging prompts and refining the methodology to reflect better how humans actually judge AI outputs in practice.” He said the second version uses unseen human prompts, has a more challenging scoring setup and new domains.  Using evaluations for models that evaluate While reward models test how well models work, it’s also important that RMs align with company values; otherwise, the fine-tuning and reinforcement learning process can reinforce bad behavior, such as hallucinations, reduce generalization, and score harmful responses too high. RewardBench 2 covers six different domains: factuality, precise instruction following, math, safety, focus and ties. “Enterprises should use RewardBench 2 in two different ways depending on their application. If they’re performing RLHF themselves, they should adopt the best practices and datasets from leading models in their own pipelines because reward models need on-policy training recipes (i.e. reward models that mirror the model they’re trying to train with RL). For inference time scaling or data filtering, RewardBench 2 has shown that they can select the best model for their domain and see correlated performance,” Lambert said.  Lambert noted that benchmarks like RewardBench offer users a way to evaluate the models they’re choosing based on the “dimensions that matter most to them, rather than relying on a narrow one-size-fits-all score.” He said the idea of performance, which many evaluation methods claim to assess, is very subjective because a good response from a model highly depends on the context and goals of the user. At the same time, human preferences get very nuanced.  Ai 2 released the first version of RewardBench in March 2024. At the time, the company said it was the first benchmark and leaderboard for reward models. Since then, several methods for benchmarking and improving RM have emerged. Researchers at Meta’s FAIR came out with reWordBench. DeepSeek released a new technique called Self-Principled Critique Tuning for smarter and scalable RM.  Super excited that our second reward model evaluation is out. It's substantially harder, much cleaner, and well correlated with downstream PPO/BoN sampling. Happy hillclimbing!Huge congrats to @saumyamalik44 who lead the project with a total commitment to excellence. https://t.co/c0b6rHTXY5— Nathan Lambert (@natolambert) June 2, 2025 How models performed Since RewardBench 2 is an updated version of RewardBench, Ai2 tested both existing and newly trained models to see if they continue to rank high. These included a variety of models, such as versions of Gemini, Claude, GPT-4.1, and Llama-3.1, along with datasets and models like Qwen, Skywork, and its own Tulu.  The company found that larger reward models perform best on the benchmark because their base models are stronger. Overall, the strongest-performing models are variants of Llama-3.1 Instruct. In terms of focus and safety, Skywork data “is particularly helpful,” and Tulu did well on factuality.  Ai2 said that while they believe RewardBench 2 “is a step forward in broad, multi-domain accuracy-based evaluation” for reward models, they cautioned that model evaluation should be mainly used as a guide to pick models that work best with an enterprise’s needs.  Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/06/crimedy7_illustration_of_a_robot_rewarding_another_robot_abstra_11fd0825-4ec3-4e18-80c7-dd371d901a25.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-06-03T23:47:00+00:00\" datetime=\"2025-06-03T23:47:00+00:00\"\u003eJune 3, 2025 4:47 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2025/06/crimedy7_illustration_of_a_robot_rewarding_another_robot_abstra_11fd0825-4ec3-4e18-80c7-dd371d901a25.png?w=750\" alt=\"Credit: VentureBeat, generated with MidJourney\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat, generated with MidJourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eEnterprises need to know if the models that power their applications and agents work in real-life scenarios. This \u003ca href=\"https://venturebeat.com/ai/the-rag-reality-check-new-open-source-framework-lets-enterprises-scientifically-measure-ai-performance/\"\u003etype of evaluation\u003c/a\u003e can sometimes \u003ca href=\"https://venturebeat.com/ai/ai-agent-benchmarks-are-misleading-study-warns/\"\u003ebe complex\u003c/a\u003e because it is hard to predict specific scenarios. A revamped version of the RewardBench benchmark looks to give organizations a better idea of a model’s real-life performance. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe \u003ca href=\"https://allenai.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAllen Institute of AI (Ai2)\u003c/a\u003e launched RewardBench 2, an updated version of its reward model benchmark, RewardBench, which they claim provides a more holistic view of model performance and assesses how models align with an enterprise’s goals and standards. \u003c/p\u003e\n\n\n\n\u003cp\u003eAi2 built RewardBench with classification tasks that measure correlations through inference-time compute and downstream training. RewardBench mainly deals with reward models (RM), which can act as judges and evaluate LLM outputs. RMs assign a score or a “reward” that guides reinforcement learning with human feedback (RHLF).\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cdiv\u003e\n\u003cblockquote data-width=\"500\" data-dnt=\"true\"\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eRewardBench 2 is here! We took a long time to learn from our first reward model evaluation tool to make one that is substantially harder and more correlated with both downstream RLHF and inference-time scaling. \u003ca href=\"https://t.co/NGetvNrOQV\"\u003epic.twitter.com/NGetvNrOQV\u003c/a\u003e\u003c/p\u003e— Ai2 (@allen_ai) \u003ca href=\"https://twitter.com/allen_ai/status/1929576050352111909?ref_src=twsrc%5Etfw\"\u003eJune 2, 2025\u003c/a\u003e\u003c/blockquote\u003e\n\u003c/div\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eNathan Lambert, a senior research scientist at Ai2, told VentureBeat that the first RewardBench worked as intended when it was launched. Still, the model environment rapidly evolved, and so should its benchmarks. \u003c/p\u003e\n\n\n\n\u003cp\u003e“As reward models became more advanced and use cases more nuanced, we quickly recognized with the community that the first version didn’t fully capture the complexity of real-world human preferences,” he said. \u003c/p\u003e\n\n\n\n\u003cp\u003eLambert added that with RewardBench 2, “we set out to improve both the breadth and depth of evaluation—incorporating more diverse, challenging prompts and refining the methodology to reflect better how humans actually judge AI outputs in practice.” He said the second version uses unseen human prompts, has a more challenging scoring setup and new domains. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-using-evaluations-for-models-that-evaluate\"\u003eUsing evaluations for models that evaluate\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile reward models test how well models work, it’s also important that RMs align with company values; otherwise, the fine-tuning and reinforcement learning process can reinforce bad behavior, such as hallucinations, reduce generalization, and score harmful responses too high.\u003c/p\u003e\n\n\n\n\u003cp\u003eRewardBench 2 covers six different domains: factuality, precise instruction following, math, safety, focus and ties.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Enterprises should use RewardBench 2 in two different ways depending on their application. If they’re performing RLHF themselves, they should adopt the best practices and datasets from leading models in their own pipelines because reward models need on-policy training recipes (i.e. reward models that mirror the model they’re trying to train with RL). For inference time scaling or data filtering, RewardBench 2 has shown that they can select the best model for their domain and see correlated performance,” Lambert said. \u003c/p\u003e\n\n\n\n\u003cp\u003eLambert noted that benchmarks like RewardBench offer users a way to evaluate the models they’re choosing based on the “dimensions that matter most to them, rather than relying on a narrow one-size-fits-all score.” He said the idea of performance, which many evaluation methods claim to assess, is very subjective because a good response from a model highly depends on the context and goals of the user. At the same time, human preferences get very nuanced. \u003c/p\u003e\n\n\n\n\u003cp\u003eAi 2 released the first version of \u003ca href=\"https://allenai.org/blog/rewardbench-the-first-benchmark-leaderboard-for-reward-models-used-in-rlhf-1d4d7d04a90b\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRewardBench in March 2024\u003c/a\u003e. At the time, the company said it was the first benchmark and leaderboard for reward models. Since then, several methods for benchmarking and improving RM have emerged. Researchers at \u003ca href=\"https://ai.meta.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMeta\u003c/a\u003e’s FAIR came out with \u003ca href=\"https://ai.meta.com/research/publications/rewordbench-benchmarking-and-improving-the-robustness-of-reward-models-with-transformed-inputs/\"\u003ereWordBench\u003c/a\u003e. \u003ca href=\"https://deepseek.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDeepSeek\u003c/a\u003e released a \u003ca href=\"https://venturebeat.com/ai/deepseek-unveils-new-technique-for-smarter-scalable-ai-reward-models/\"\u003enew technique called Self-Principled Critique Tuning\u003c/a\u003e for smarter and scalable RM. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cdiv\u003e\n\u003cblockquote data-width=\"500\" data-dnt=\"true\"\u003e\u003cdiv lang=\"en\" dir=\"ltr\"\u003e\u003cp\u003eSuper excited that our second reward model evaluation is out. It\u0026#39;s substantially harder, much cleaner, and well correlated with downstream PPO/BoN sampling. \u003c/p\u003e\u003cp\u003eHappy hillclimbing!\u003c/p\u003e\u003cp\u003eHuge congrats to \u003ca href=\"https://twitter.com/saumyamalik44?ref_src=twsrc%5Etfw\"\u003e@saumyamalik44\u003c/a\u003e who lead the project with a total commitment to excellence. \u003ca href=\"https://t.co/c0b6rHTXY5\"\u003ehttps://t.co/c0b6rHTXY5\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e— Nathan Lambert (@natolambert) \u003ca href=\"https://twitter.com/natolambert/status/1929577037896777891?ref_src=twsrc%5Etfw\"\u003eJune 2, 2025\u003c/a\u003e\u003c/blockquote\u003e\n\u003c/div\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-how-models-performed\"\u003eHow models performed\u003c/h2\u003e\n\n\n\n\u003cp\u003eSince RewardBench 2 is an updated version of RewardBench, Ai2 tested both existing and newly trained models to see if they continue to rank high. These included a variety of models, such as versions of Gemini, Claude, GPT-4.1, and Llama-3.1, along with datasets and models like Qwen, Skywork, and \u003ca href=\"https://venturebeat.com/ai/ai2-releases-tulu-3-a-fully-open-source-model-that-bests-deepseek-v3-gpt-4o-with-novel-post-training-approach/\"\u003eits own Tulu\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe company found that larger reward models perform best on the benchmark because their base models are stronger. Overall, the strongest-performing models are variants of Llama-3.1 Instruct. In terms of focus and safety, Skywork data “is particularly helpful,” and Tulu did well on factuality. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd5nQSE4_DgDBXRJzMDnmYFdTuJuDmUjj0EqRptD3S-g3wqXAS7EQ9MVVsAcbLPqZmcgX9H4r4SDkchee3AlbK6vz9mGp8dvv3s4DjcONCRUG6uLnaZHLhSB96CND_LIV7ml-pjvQ?key=NN_7kK_imh5HExYdE2x5cg\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAi2 said that while they believe RewardBench 2 “is a step forward in broad, multi-domain accuracy-based evaluation” for reward models, they cautioned that model evaluation should be mainly used as a guide to pick models that work best with an enterprise’s needs. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-06-03T23:47:00Z",
  "modifiedTime": "2025-06-03T23:47:14Z"
}
