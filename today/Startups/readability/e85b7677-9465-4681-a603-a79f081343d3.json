{
  "id": "e85b7677-9465-4681-a603-a79f081343d3",
  "title": "Google DeepMind makes AI history with gold medal win at world’s toughest math competition",
  "link": "https://venturebeat.com/ai/google-deepmind-makes-ai-history-with-gold-medal-win-at-worlds-toughest-math-competition/",
  "description": "Google DeepMind's Gemini AI won a gold medal at the International Mathematical Olympiad by solving complex math problems using natural language, marking a breakthrough in AI reasoning and human-level performance.",
  "author": "Michael Nuñez",
  "published": "Mon, 21 Jul 2025 22:33:34 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Automation",
    "Business",
    "Data Infrastructure",
    "Enterprise Analytics",
    "Programming \u0026 Development",
    "Security",
    "ai",
    "AI breakthrough",
    "AI competition",
    "AI Reasoning",
    "AI, ML and Deep Learning",
    "artificial intelligence",
    "Breakthrough",
    "Business Intelligence",
    "competition",
    "Conversational AI",
    "Data Management",
    "Data Science",
    "Data Security and Privacy",
    "DeepMind Gemini",
    "Gemini",
    "Gemini AI",
    "gold medal",
    "Google Deepmind",
    "Human-level AI",
    "International Mathematical Olympiad",
    "machine learning",
    "Math Olympiad",
    "natural language",
    "NLP",
    "OpenAI",
    "Tech rivalry"
  ],
  "byline": "Michael Nuñez",
  "length": 11621,
  "excerpt": "Google DeepMind's Gemini AI won a gold medal at the International Mathematical Olympiad by solving complex math problems using natural language, marking a breakthrough in AI reasoning and human-level performance.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "July 21, 2025 3:33 PM Credit: VentureBeat made with Midjourney Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now Google DeepMind announced Monday that an advanced version of its Gemini artificial intelligence model has officially achieved gold medal-level performance at the International Mathematical Olympiad, solving five of six exceptionally difficult problems and earning recognition as the first AI system to receive official gold-level grading from competition organizers. The victory advances the field of AI reasoning and puts Google ahead in the intensifying battle between tech giants building next-generation artificial intelligence. More importantly, it demonstrates that AI can now tackle complex mathematical problems using natural language understanding rather than requiring specialized programming languages. “Official results are in — Gemini achieved gold-medal level in the International Mathematical Olympiad!” Demis Hassabis, CEO of Google DeepMind, wrote on social media platform X Monday morning. “An advanced version was able to solve 5 out of 6 problems. Incredible progress.” Official results are in – Gemini achieved gold-medal level in the International Mathematical Olympiad! ? An advanced version was able to solve 5 out of 6 problems. Incredible progress – huge congrats to @lmthang and the team! https://t.co/pp9bXF7rVj— Demis Hassabis (@demishassabis) July 21, 2025 The International Mathematical Olympiad, held annually since 1959, is widely considered the world’s most prestigious mathematics competition for pre-university students. Each participating country sends six elite young mathematicians to compete in solving six exceptionally challenging problems spanning algebra, combinatorics, geometry, and number theory. Only about 8% of human participants typically earn gold medals. The AI Impact Series Returns to San Francisco - August 5 The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation. Secure your spot now - space is limited: https://bit.ly/3GuuPLF How Google DeepMind’s Gemini Deep Think cracked math’s toughest problems Google’s latest success far exceeds its 2024 performance, when the company’s combined AlphaProof and AlphaGeometry systems earned silver medal status by solving four of six problems. That earlier system required human experts to first translate natural language problems into domain-specific programming languages and then interpret the AI’s mathematical output. This year’s breakthrough came through Gemini Deep Think, an enhanced reasoning system that employs what researchers call “parallel thinking.” Unlike traditional AI models that follow a single chain of reasoning, Deep Think simultaneously explores multiple possible solutions before arriving at a final answer. “Our model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions,” Hassabis explained in a follow-up post on the social media site X, emphasizing that the system completed its work within the competition’s standard 4.5-hour time limit. We achieved this year’s impressive result using an advanced version of Gemini Deep Think (an enhanced reasoning mode for complex problems). Our model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions –…— Demis Hassabis (@demishassabis) July 21, 2025 The model achieved 35 out of a possible 42 points, comfortably exceeding the gold medal threshold. According to IMO President Prof. Dr. Gregor Dolinar, the solutions were “astonishing in many respects” and found to be “clear, precise and most of them easy to follow” by competition graders. OpenAI faces backlash for bypassing official competition rules The announcement comes amid growing tension in the AI industry over competitive practices and transparency. Google DeepMind’s measured approach to releasing its results has drawn praise from the AI community, particularly in contrast to rival OpenAI’s handling of similar achievements. “We didn’t announce on Friday because we respected the IMO Board’s original request that all AI labs share their results only after the official results had been verified by independent experts \u0026 the students had rightly received the acclamation they deserved,” Hassabis wrote, appearing to reference OpenAI’s earlier announcement of its own olympiad performance. Btw as an aside, we didn’t announce on Friday because we respected the IMO Board's original request that all AI labs share their results only after the official results had been verified by independent experts \u0026 the students had rightly received the acclamation they deserved— Demis Hassabis (@demishassabis) July 21, 2025 Social media users were quick to note the distinction. “You see? OpenAI ignored the IMO request. Shame. No class. Straight up disrespect,” wrote one user. “Google DeepMind acted with integrity, aligned with humanity.” The criticism stems from OpenAI’s decision to announce its own mathematical olympiad results without participating in the official IMO evaluation process. Instead, OpenAI had a panel of former IMO participants grade its AI’s performance, a approach that some in the community view as lacking credibility. “OpenAI is quite possibly the worst company on the planet right now,” wrote one critic, while others suggested the company needs to “take things seriously” and “be more credible.” You see?OpenAI ignored the IMO request. Shame. No class. Straight up disrespect. Google DeepMind acted with integrity, aligned with humanity. TRVTHNUKE pic.twitter.com/8LAOak6XUE— NIK (@ns123abc) July 21, 2025 Inside the training methods that powered Gemini’s mathematical mastery Google DeepMind’s success appears to stem from novel training techniques that go beyond traditional approaches. The team used advanced reinforcement learning methods designed to leverage multi-step reasoning, problem-solving, and theorem-proving data. The model was also provided access to a curated collection of high-quality mathematical solutions and received specific guidance on approaching IMO-style problems. The technical achievement impressed AI researchers who noted its broader implications. “Not just solving math… but understanding language-described problems and applying abstract logic to novel cases,” wrote AI observer Elyss Wren. “This isn’t rote memory — this is emergent cognition in motion.” Ethan Mollick, a professor at the Wharton School who studies AI, emphasized the significance of using a general-purpose model rather than specialized tools. “Increasing evidence of the ability of LLMs to generalize to novel problem solving,” he wrote, highlighting how this differs from previous approaches that required specialized mathematical software. It wasn't just OpenAI.Google also used a general purpose model to solve the very hard math problems of the International Math Olympiad in plain language. Last year they used specialized tool useIncreasing evidence of the ability of LLMs to generalize to novel problem solving https://t.co/Ve72fFmx2b— Ethan Mollick (@emollick) July 21, 2025 The model demonstrated particularly impressive reasoning in one problem where many human competitors applied graduate-level mathematical concepts. According to DeepMind researcher Junehyuk Jung, Gemini “made a brilliant observation and used only elementary number theory to create a self-contained proof,” finding a more elegant solution than many human participants. What Google DeepMind’s victory means for the $200 billion AI race The breakthrough comes at a critical moment in the AI industry, where companies are racing to demonstrate superior reasoning capabilities. The success has immediate practical implications: Google plans to make a version of this Deep Think model available to mathematicians for testing before rolling it out to Google AI Ultra subscribers, who pay $250 monthly for access to the company’s most advanced AI models. The timing also highlights the intensifying competition between major AI laboratories. While Google celebrated its methodical, officially-verified approach, the controversy surrounding OpenAI’s announcement reflects broader tensions about transparency and credibility in AI development. This competitive dynamic extends beyond just mathematical reasoning. Recent weeks have seen various AI companies announce breakthrough capabilities, though not all have been received positively. Elon Musk’s xAI recently launched Grok 4, which the company claimed was the “smartest AI in the world,” though leaderboard scores showed it trailing behind models from Google and OpenAI. Additionally, Grok has faced criticism for controversial features including sexualized AI companions and episodes of generating antisemitic content. The dawn of AI that thinks like humans—with real-world consequences The mathematical olympiad victory goes beyond competitive bragging rights. Gemini’s performance demonstrates that AI systems can now match human-level reasoning in complex tasks requiring creativity, abstract thinking, and the ability to synthesize insights across multiple domains. “This is a significant advance over last year’s breakthrough result,” the DeepMind team noted in their technical announcement. The progression from requiring specialized formal languages to operating entirely in natural language suggests that AI systems are becoming more intuitive and accessible. For businesses, this development signals that AI may soon tackle complex analytical problems across various industries without requiring specialized programming or domain expertise. The ability to reason through intricate challenges using everyday language could democratize sophisticated analytical capabilities across organizations. However, questions persist about whether these reasoning capabilities will translate effectively to messier real-world challenges. The mathematical olympiad provides well-defined problems with clear success criteria — a far cry from the ambiguous, multifaceted decisions that define most business and scientific endeavors. Google DeepMind plans to return to next year’s competition “in search of a perfect score.” The company believes AI systems combining natural language fluency with rigorous reasoning “will become invaluable tools for mathematicians, scientists, engineers, and researchers, helping us advance human knowledge on the path to AGI.” But perhaps the most telling detail emerged from the competition itself: when faced with the contest’s most difficult problem, Gemini started from an incorrect hypothesis and never recovered. Only five human students solved that problem correctly. In the end, it seems, even gold medal-winning AI still has something to learn from teenage mathematicians. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/07/nuneybits_Vector_art_of_robot_winning_medal_5be7ef30-62b2-4f25-bd3c-9480201df4b7.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-07-21T22:33:34+00:00\" datetime=\"2025-07-21T22:33:34+00:00\"\u003eJuly 21, 2025 3:33 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"420\" src=\"https://venturebeat.com/wp-content/uploads/2025/07/nuneybits_Vector_art_of_robot_winning_medal_5be7ef30-62b2-4f25-bd3c-9480201df4b7.webp?w=750\" alt=\"Credit: VentureBeat made with Midjourney\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Midjourney\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\u003c/em\u003e \u003cem\u003e\u003ca href=\"https://venturebeat.com/newsletters/\"\u003eSubscribe Now\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://deepmind.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle DeepMind\u003c/a\u003e announced Monday that an advanced version of its Gemini artificial intelligence model has officially achieved \u003ca href=\"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003egold medal-level performance\u003c/a\u003e at the \u003ca href=\"https://www.imo-official.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eInternational Mathematical Olympiad\u003c/a\u003e, solving five of six exceptionally difficult problems and earning recognition as the first AI system to receive official gold-level grading from competition organizers.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe victory advances the field of AI reasoning and puts Google ahead in the intensifying battle between tech giants building next-generation artificial intelligence. More importantly, it demonstrates that AI can now tackle complex mathematical problems using natural language understanding rather than requiring specialized programming languages.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Official results are in — Gemini achieved gold-medal level in the International Mathematical Olympiad!” \u003ca href=\"https://x.com/demishassabis/status/1947337615054671882\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDemis Hassabis\u003c/a\u003e, CEO of Google DeepMind, wrote on social media platform X Monday morning. “An advanced version was able to solve 5 out of 6 problems. Incredible progress.”\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eOfficial results are in – Gemini achieved gold-medal level in the International Mathematical Olympiad! ? An advanced version was able to solve 5 out of 6 problems. Incredible progress – huge congrats to \u003ca href=\"https://twitter.com/lmthang?ref_src=twsrc%5Etfw\"\u003e@lmthang\u003c/a\u003e and the team! \u003ca href=\"https://t.co/pp9bXF7rVj\"\u003ehttps://t.co/pp9bXF7rVj\u003c/a\u003e\u003c/p\u003e— Demis Hassabis (@demishassabis) \u003ca href=\"https://twitter.com/demishassabis/status/1947337615054671882?ref_src=twsrc%5Etfw\"\u003eJuly 21, 2025\u003c/a\u003e\u003c/blockquote\u003e \n\n\n\n\u003cp\u003eThe \u003ca href=\"https://www.imo-official.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eInternational Mathematical Olympiad\u003c/a\u003e, held annually since 1959, is widely considered the world’s most prestigious mathematics competition for pre-university students. Each participating country sends six elite young mathematicians to compete in solving six exceptionally challenging problems spanning algebra, combinatorics, geometry, and number theory. Only about 8% of human participants typically earn gold medals.\u003c/p\u003e\n\n\n\n\u003cdiv id=\"boilerplate_2803147\"\u003e\n\u003chr/\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eThe AI Impact Series Returns to San Francisco - August 5\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\u003c/p\u003e\n\n\n\n\u003cp\u003eSecure your spot now - space is limited: \u003ca href=\"https://bit.ly/3GuuPLF\"\u003ehttps://bit.ly/3GuuPLF\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003ch2 id=\"h-how-google-deepmind-s-gemini-deep-think-cracked-math-s-toughest-problems\"\u003eHow Google DeepMind’s Gemini Deep Think cracked math’s toughest problems\u003c/h2\u003e\n\n\n\n\u003cp\u003eGoogle’s latest success far exceeds its 2024 performance, when the company’s combined \u003ca href=\"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAlphaProof\u003c/a\u003e and \u003ca href=\"https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAlphaGeometry\u003c/a\u003e systems earned silver medal status by solving four of six problems. That earlier system required human experts to first translate natural language problems into domain-specific programming languages and then interpret the AI’s mathematical output.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis year’s breakthrough came through \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#deep-think\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGemini Deep Think\u003c/a\u003e, an enhanced reasoning system that employs what researchers call “\u003ca href=\"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eparallel thinking\u003c/a\u003e.” Unlike traditional AI models that follow a single chain of reasoning, Deep Think simultaneously explores multiple possible solutions before arriving at a final answer.\u003c/p\u003e\n\n\n\n\u003cp\u003e“Our model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions,” \u003ca href=\"https://x.com/demishassabis/status/1947337617231778168\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHassabis explained\u003c/a\u003e in a follow-up post on the social media site X, emphasizing that the system completed its work within the competition’s standard 4.5-hour time limit.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eWe achieved this year’s impressive result using an advanced version of Gemini Deep Think (an enhanced reasoning mode for complex problems). Our model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions –…\u003c/p\u003e— Demis Hassabis (@demishassabis) \u003ca href=\"https://twitter.com/demishassabis/status/1947337617231778168?ref_src=twsrc%5Etfw\"\u003eJuly 21, 2025\u003c/a\u003e\u003c/blockquote\u003e \n\n\n\n\u003cp\u003eThe model achieved 35 out of a possible 42 points, comfortably exceeding the gold medal threshold. According to IMO President Prof. Dr. Gregor Dolinar, the solutions were “\u003ca href=\"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eastonishing in many respects\u003c/a\u003e” and found to be “clear, precise and most of them easy to follow” by competition graders.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-openai-faces-backlash-for-bypassing-official-competition-rules\"\u003eOpenAI faces backlash for bypassing official competition rules\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe announcement comes amid growing tension in the AI industry over competitive practices and transparency. Google DeepMind’s measured approach to releasing its results has drawn praise from the AI community, particularly in contrast to rival OpenAI’s handling of similar achievements.\u003c/p\u003e\n\n\n\n\u003cp\u003e“We didn’t announce on Friday because we respected the IMO Board’s original request that all AI labs share their results only after the official results had been verified by independent experts \u0026amp; the students had rightly received the acclamation they deserved,” \u003ca href=\"https://x.com/demishassabis/status/1947337618787615175\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHassabis wrote\u003c/a\u003e, appearing to reference OpenAI’s earlier announcement of its own olympiad performance.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eBtw as an aside, we didn’t announce on Friday because we respected the IMO Board\u0026#39;s original request that all AI labs share their results only after the official results had been verified by independent experts \u0026amp; the students had rightly received the acclamation they deserved\u003c/p\u003e— Demis Hassabis (@demishassabis) \u003ca href=\"https://twitter.com/demishassabis/status/1947337618787615175?ref_src=twsrc%5Etfw\"\u003eJuly 21, 2025\u003c/a\u003e\u003c/blockquote\u003e \n\n\n\n\u003cp\u003eSocial media users were quick to note the distinction. “You see? OpenAI ignored the IMO request. Shame. No class. Straight up disrespect,” \u003ca href=\"https://x.com/ns123abc/status/1947347617131680232\"\u003ewrote one user\u003c/a\u003e. “Google DeepMind acted with integrity, aligned with humanity.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe criticism stems from OpenAI’s decision to announce its own mathematical olympiad results without participating in the official IMO evaluation process. Instead, OpenAI had a panel of former IMO participants grade its AI’s performance, a approach that some in the community view as lacking credibility.\u003c/p\u003e\n\n\n\n\u003cp\u003e“OpenAI is quite possibly the worst company on the planet right now,” wrote one critic, while others suggested the company needs to “take things seriously” and “be more credible.”\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cdiv lang=\"en\" dir=\"ltr\"\u003e\u003cp\u003eYou see?\u003c/p\u003e\u003cp\u003eOpenAI ignored the IMO request. Shame. No class. Straight up disrespect. \u003c/p\u003e\u003cp\u003eGoogle DeepMind acted with integrity, aligned with humanity. \u003c/p\u003e\u003cp\u003eTRVTHNUKE \u003ca href=\"https://t.co/8LAOak6XUE\"\u003epic.twitter.com/8LAOak6XUE\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e— NIK (@ns123abc) \u003ca href=\"https://twitter.com/ns123abc/status/1947347617131680232?ref_src=twsrc%5Etfw\"\u003eJuly 21, 2025\u003c/a\u003e\u003c/blockquote\u003e \n\n\n\n\u003ch2 id=\"h-inside-the-training-methods-that-powered-gemini-s-mathematical-mastery\"\u003eInside the training methods that powered Gemini’s mathematical mastery\u003c/h2\u003e\n\n\n\n\u003cp\u003eGoogle DeepMind’s success appears to stem from novel training techniques that go beyond traditional approaches. The team used advanced reinforcement learning methods designed to leverage multi-step reasoning, problem-solving, and theorem-proving data. The model was also provided access to a curated collection of high-quality mathematical solutions and received specific guidance on approaching IMO-style problems.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe technical achievement impressed AI researchers who noted its broader implications. “Not just solving math… but understanding language-described problems and applying abstract logic to novel cases,” wrote AI observer \u003ca href=\"https://x.com/ElyssWren/status/1947359071696245033\"\u003eElyss Wren\u003c/a\u003e. “This isn’t rote memory — this is emergent cognition in motion.”\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://x.com/emollick/status/1947356382581137867\"\u003eEthan Mollick\u003c/a\u003e, a professor at the Wharton School who studies AI, emphasized the significance of using a general-purpose model rather than specialized tools. “Increasing evidence of the ability of LLMs to generalize to novel problem solving,” he wrote, highlighting how this differs from previous approaches that required specialized mathematical software.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cdiv lang=\"en\" dir=\"ltr\"\u003e\u003cp\u003eIt wasn\u0026#39;t just OpenAI.\u003c/p\u003e\u003cp\u003eGoogle also used a general purpose model to solve the very hard math problems of the International Math Olympiad in plain language. Last year they used specialized tool use\u003c/p\u003e\u003cp\u003eIncreasing evidence of the ability of LLMs to generalize to novel problem solving \u003ca href=\"https://t.co/Ve72fFmx2b\"\u003ehttps://t.co/Ve72fFmx2b\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e— Ethan Mollick (@emollick) \u003ca href=\"https://twitter.com/emollick/status/1947356382581137867?ref_src=twsrc%5Etfw\"\u003eJuly 21, 2025\u003c/a\u003e\u003c/blockquote\u003e \n\n\n\n\u003cp\u003eThe model demonstrated particularly impressive reasoning in one problem where many human competitors applied graduate-level mathematical concepts. According to DeepMind researcher Junehyuk Jung, Gemini “made a brilliant observation and used only elementary number theory to create a self-contained proof,” finding a more elegant solution than many human participants.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-google-deepmind-s-victory-means-for-the-200-billion-ai-race\"\u003eWhat Google DeepMind’s victory means for the $200 billion AI race\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe breakthrough comes at a critical moment in the AI industry, where companies are racing to demonstrate superior reasoning capabilities. The success has immediate practical implications: Google plans to make a version of this \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#deep-think\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDeep Think model\u003c/a\u003e available to mathematicians for testing before rolling it out to Google AI Ultra subscribers, who pay $250 monthly for access to the company’s most advanced AI models.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe timing also highlights the intensifying competition between major AI laboratories. While Google celebrated its methodical, officially-verified approach, the controversy surrounding OpenAI’s announcement reflects broader tensions about transparency and credibility in AI development.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis competitive dynamic extends beyond just mathematical reasoning. Recent weeks have seen various AI companies announce breakthrough capabilities, though not all have been received positively. Elon Musk’s xAI recently launched \u003ca href=\"https://x.ai/news/grok-4\"\u003eGrok 4\u003c/a\u003e, which the company claimed was the “smartest AI in the world,” though \u003ca href=\"https://futurism.com/grok-4-ai-leaderboard\"\u003eleaderboard scores showed it trailing\u003c/a\u003e behind models from Google and OpenAI. Additionally, Grok has faced criticism for controversial features including \u003ca href=\"https://www.rollingstone.com/culture/culture-news/grok-pornographic-anime-companion-department-of-defense-1235385034/\"\u003esexualized AI companions\u003c/a\u003e and episodes of generating \u003ca href=\"https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content\"\u003eantisemitic content\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-dawn-of-ai-that-thinks-like-humans-with-real-world-consequences\"\u003eThe dawn of AI that thinks like humans—with real-world consequences\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe mathematical olympiad victory goes beyond competitive bragging rights. Gemini’s performance demonstrates that AI systems can now match human-level reasoning in complex tasks requiring creativity, abstract thinking, and the ability to synthesize insights across multiple domains.\u003c/p\u003e\n\n\n\n\u003cp\u003e“This is a significant advance over last year’s breakthrough result,” the\u003ca href=\"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\"\u003e DeepMind team noted\u003c/a\u003e in their technical announcement. The progression from requiring specialized formal languages to operating entirely in natural language suggests that AI systems are becoming more intuitive and accessible.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor businesses, this development signals that AI may soon tackle complex analytical problems across various industries without requiring specialized programming or domain expertise. The ability to reason through intricate challenges using everyday language could democratize sophisticated analytical capabilities across organizations.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, questions persist about whether these reasoning capabilities will translate effectively to messier real-world challenges. The mathematical olympiad provides well-defined problems with clear success criteria — a far cry from the ambiguous, multifaceted decisions that define most business and scientific endeavors.\u003c/p\u003e\n\n\n\n\u003cp\u003eGoogle DeepMind plans to return to next year’s competition “\u003ca href=\"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\"\u003ein search of a perfect score\u003c/a\u003e.” The company believes AI systems combining natural language fluency with rigorous reasoning “will become invaluable tools for mathematicians, scientists, engineers, and researchers, helping us advance human knowledge on the path to AGI.”\u003c/p\u003e\n\n\n\n\u003cp\u003eBut perhaps the most telling detail emerged from the competition itself: when faced with the contest’s most difficult problem, Gemini started from an incorrect hypothesis and never recovered. Only five human students solved that problem correctly. In the end, it seems, even gold medal-winning AI still has something to learn from teenage mathematicians.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2025-07-21T22:33:34Z",
  "modifiedTime": "2025-07-21T22:33:38Z"
}
