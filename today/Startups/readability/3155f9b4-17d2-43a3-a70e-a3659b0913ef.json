{
  "id": "3155f9b4-17d2-43a3-a70e-a3659b0913ef",
  "title": "OpenAI responds to DeepSeek competition with detailed reasoning traces for o3-mini",
  "link": "https://venturebeat.com/ai/openai-responds-to-deepseek-competition-with-detailed-reasoning-traces-for-o3-mini/",
  "description": "By showing a more detailed version of the chain of thought of o3-mini, OpenAI is closing the gap with DeepSeek-R1.",
  "author": "Ben Dickson",
  "published": "Fri, 07 Feb 2025 21:46:56 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI, ML and Deep Learning",
    "category-/News",
    "chain of thought reasoning",
    "Conversational AI",
    "Deepseek R1",
    "Generative AI",
    "large language models",
    "LLM reasoning",
    "o3-mini",
    "OpenAI",
    "openai o3-mini",
    "reasoning models"
  ],
  "byline": "Ben Dickson",
  "length": 5192,
  "excerpt": "By showing a more detailed version of the chain of thought of o3-mini, OpenAI is closing the gap with DeepSeek-R1.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "February 7, 2025 1:46 PM Image Credit: VentureBeat via ChatGPT Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More OpenAI is now showing more details of the reasoning process of o3-mini, its latest reasoning model. The change was announced on OpenAI’s X account and comes as the AI lab is under increased pressure by DeepSeek-R1, a rival open model that fully displays its reasoning tokens. Models like o3 and R1 undergo a lengthy “chain of thought” (CoT) process in which they generate extra tokens to break down the problem, reason about and test different answers and reach a final solution. Previously, OpenAI’s reasoning models hid their chain of thought and only produced a high-level overview of reasoning steps. This made it difficult for users and developers to understand the model’s reasoning logic and change their instructions and prompts to steer it in the right direction.  OpenAI considered chain of thought a competitive advantage and hid it to prevent rivals from copying to train their models. But with R1 and other open models showing their full reasoning trace, the lack of transparency becomes a disadvantage for OpenAI. The new version of o3-mini shows a more detailed version of CoT. Although we still don’t see the raw tokens, it provides much more clarity on the reasoning process. Why it matters for applications In our previous experiments on o1 and R1, we found that o1 was slightly better at solving data analysis and reasoning problems. However, one of the key limitations was that there was no way to figure out why the model made mistakes — and it often made mistakes when faced with messy real-world data obtained from the web. On the other hand, R1’s chain of thought enabled us to troubleshoot the problems and change our prompts to improve reasoning. For example, in one of our experiments, both models failed to provide the correct answer. But thanks to R1’s detailed chain of thought, we were able to find out that the problem was not with the model itself but with the retrieval stage that gathered information from the web. In other experiments, R1’s chain of thought was able to provide us with hints when it failed to parse the information we provided it, while o1 only gave us a very rough overview of how it was formulating its response. We tested the new o3-mini model on a variant of a previous experiment we ran with o1. We provided the model with a text file containing prices of various stocks from January 2024 through January 2025. The file was noisy and unformatted, a mixture of plain text and HTML elements. We then asked the model to calculate the value of a portfolio that invested $140 in the Magnificent 7 stocks on the first day of each month from January 2024 to January 2025, distributed evenly across all stocks (we used the term “Mag 7” in the prompt to make it a bit more challenging). o3-mini’s CoT was really helpful this time. First, the model reasoned about what the Mag 7 was, filtered the data to only keep the relevant stocks (to make the problem challenging, we added a few non–Mag 7 stocks to the data), calculated the monthly amount to invest in each stock, and made the final calculations to provide the correct answer (the portfolio would be worth around $2,200 at the latest time registered in the data we provided to the model). It will take a lot more testing to see the limits of the new chain of thought, since OpenAI is still hiding a lot of details. But in our vibe checks, it seems that the new format is much more useful. What it means for OpenAI When DeepSeek-R1 was released, it had three clear advantages over OpenAI’s reasoning models: It was open, cheap and transparent. Since then, OpenAI has managed to shorten the gap. While o1 costs $60 per million output tokens, o3-mini costs just $4.40, while outperforming o1 on many reasoning benchmarks. R1 costs around $7 and $8 per million tokens on U.S. providers. (DeepSeek offers R1 at $2.19 per million tokens on its own servers, but many organizations will not be able to use it because it is hosted in China.) With the new change to the CoT output, OpenAI has managed to somewhat work around the transparency problem. It remains to be seen what OpenAI will do about open sourcing its models. Since its release, R1 has already been adapted, forked and hosted by many different labs and companies potentially making it the preferred reasoning model for enterprises. OpenAI CEO Sam Altman recently admitted that he was “on the wrong side of history” in open source debate. We’ll have to see how this realization will manifest itself in OpenAI’s future releases. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2024/12/DALL·E-2024-12-29-07.53.04-A-clean-and-modern-vector-illustration-representing-artificial-intelligence-breakthroughs.-The-design-features-an-abstract-humanoid-AI-figure-with-int.webp?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-02-07T21:46:56+00:00\" datetime=\"2025-02-07T21:46:56+00:00\"\u003eFebruary 7, 2025 1:46 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"429\" src=\"https://venturebeat.com/wp-content/uploads/2024/12/DALL·E-2024-12-29-07.53.04-A-clean-and-modern-vector-illustration-representing-artificial-intelligence-breakthroughs.-The-design-features-an-abstract-humanoid-AI-figure-with-int.webp?w=750\" alt=\"\"/\u003e\u003c/p\u003e\u003cdiv\u003e\u003cp\u003e\u003cem\u003eImage Credit: VentureBeat via ChatGPT\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eOpenAI is now showing more details of the reasoning process of o3-mini, its latest reasoning model. The change was announced on \u003ca href=\"https://x.com/OpenAI/status/1887616278661112259\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI’s X account\u003c/a\u003e and comes as the AI lab is \u003ca href=\"https://venturebeat.com/ai/tech-leaders-respond-to-the-rapid-rise-of-deepseek/\"\u003eunder increased pressure\u003c/a\u003e by DeepSeek-R1, a rival open model that fully displays its reasoning tokens.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1190\" height=\"1496\" src=\"https://venturebeat.com/wp-content/uploads/2025/02/image_ce3f8f.png?w=477\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/02/image_ce3f8f.png 1190w, https://venturebeat.com/wp-content/uploads/2025/02/image_ce3f8f.png?resize=300,377 300w, https://venturebeat.com/wp-content/uploads/2025/02/image_ce3f8f.png?resize=768,965 768w, https://venturebeat.com/wp-content/uploads/2025/02/image_ce3f8f.png?resize=477,600 477w, https://venturebeat.com/wp-content/uploads/2025/02/image_ce3f8f.png?resize=400,503 400w, https://venturebeat.com/wp-content/uploads/2025/02/image_ce3f8f.png?resize=750,943 750w, https://venturebeat.com/wp-content/uploads/2025/02/image_ce3f8f.png?resize=578,727 578w, https://venturebeat.com/wp-content/uploads/2025/02/image_ce3f8f.png?resize=930,1169 930w\" sizes=\"(max-width: 1190px) 100vw, 1190px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eModels like o3 and R1 undergo a lengthy “chain of thought” (CoT) process in which they generate extra tokens to break down the problem, reason about and test different answers and reach a final solution. Previously, OpenAI’s reasoning models hid their chain of thought and only produced a high-level overview of reasoning steps. This made it difficult for users and developers to understand the model’s reasoning logic and change their instructions and prompts to steer it in the right direction. \u003c/p\u003e\n\n\n\n\u003cp\u003eOpenAI considered chain of thought a competitive advantage and hid it to prevent rivals from copying to train their models. But with R1 and other open models \u003ca href=\"https://venturebeat.com/ai/heres-how-openai-o1-might-lose-ground-to-open-source-models/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eshowing their full reasoning trace\u003c/a\u003e, the lack of transparency becomes a disadvantage for OpenAI.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe new version of o3-mini shows a more detailed version of CoT. Although we still don’t see the raw tokens, it provides much more clarity on the reasoning process.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1180\" height=\"1692\" src=\"https://venturebeat.com/wp-content/uploads/2025/02/image_264b8a.png?w=418\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/02/image_264b8a.png 1180w, https://venturebeat.com/wp-content/uploads/2025/02/image_264b8a.png?resize=279,400 279w, https://venturebeat.com/wp-content/uploads/2025/02/image_264b8a.png?resize=768,1101 768w, https://venturebeat.com/wp-content/uploads/2025/02/image_264b8a.png?resize=418,600 418w, https://venturebeat.com/wp-content/uploads/2025/02/image_264b8a.png?resize=1071,1536 1071w, https://venturebeat.com/wp-content/uploads/2025/02/image_264b8a.png?resize=400,574 400w, https://venturebeat.com/wp-content/uploads/2025/02/image_264b8a.png?resize=750,1075 750w, https://venturebeat.com/wp-content/uploads/2025/02/image_264b8a.png?resize=578,829 578w, https://venturebeat.com/wp-content/uploads/2025/02/image_264b8a.png?resize=930,1334 930w\" sizes=\"(max-width: 1180px) 100vw, 1180px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-why-it-matters-for-applications\"\u003eWhy it matters for applications\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn our \u003ca href=\"https://venturebeat.com/ai/beyond-benchmarks-how-deepseek-r1-and-o1-perform-on-real-world-tasks/\"\u003eprevious experiments\u003c/a\u003e on o1 and R1, we found that o1 was slightly better at solving data analysis and reasoning problems. However, one of the key limitations was that there was no way to figure out why the model made mistakes — and it often made mistakes when faced with messy real-world data obtained from the web. On the other hand, R1’s chain of thought enabled us to troubleshoot the problems and change our prompts to improve reasoning.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, in one of our experiments, both models failed to provide the correct answer. But thanks to R1’s detailed chain of thought, we were able to find out that the problem was not with the model itself but with the retrieval stage that gathered information from the web. In other experiments, R1’s chain of thought was able to provide us with hints when it failed to parse the information we provided it, while o1 only gave us a very rough overview of how it was formulating its response.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe tested the new o3-mini model on a variant of a previous experiment we ran with o1. We provided the model with a text file containing prices of various stocks from January 2024 through January 2025. The file was noisy and unformatted, a mixture of plain text and HTML elements. We then asked the model to calculate the value of a portfolio that invested $140 in the Magnificent 7 stocks on the first day of each month from January 2024 to January 2025, distributed evenly across all stocks (we used the term “Mag 7” in the prompt to make it a bit more challenging).\u003c/p\u003e\n\n\n\n\u003cp\u003eo3-mini’s CoT was really helpful this time. First, the model reasoned about what the Mag 7 was, filtered the data to only keep the relevant stocks (to make the problem challenging, we added a few non–Mag 7 stocks to the data), calculated the monthly amount to invest in each stock, and made the final calculations to provide the correct answer (the portfolio would be worth around $2,200 at the latest time registered in the data we provided to the model).\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1566\" height=\"1170\" src=\"https://venturebeat.com/wp-content/uploads/2025/02/image_133321.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/02/image_133321.png 1566w, https://venturebeat.com/wp-content/uploads/2025/02/image_133321.png?resize=300,224 300w, https://venturebeat.com/wp-content/uploads/2025/02/image_133321.png?resize=768,574 768w, https://venturebeat.com/wp-content/uploads/2025/02/image_133321.png?resize=800,598 800w, https://venturebeat.com/wp-content/uploads/2025/02/image_133321.png?resize=1536,1148 1536w, https://venturebeat.com/wp-content/uploads/2025/02/image_133321.png?resize=400,299 400w, https://venturebeat.com/wp-content/uploads/2025/02/image_133321.png?resize=750,560 750w, https://venturebeat.com/wp-content/uploads/2025/02/image_133321.png?resize=578,432 578w, https://venturebeat.com/wp-content/uploads/2025/02/image_133321.png?resize=930,695 930w\" sizes=\"(max-width: 1566px) 100vw, 1566px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIt will take a lot more testing to see the limits of the new chain of thought, since OpenAI is still hiding a lot of details. But in our vibe checks, it seems that the new format is much more useful.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-it-means-for-openai\"\u003eWhat it means for OpenAI\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen DeepSeek-R1 was released, it had three clear advantages over OpenAI’s reasoning models: It was open, cheap and transparent.\u003c/p\u003e\n\n\n\n\u003cp\u003eSince then, OpenAI has managed to shorten the gap. While o1 costs $60 per million output tokens, o3-mini costs just $4.40, while outperforming o1 on many reasoning benchmarks. R1 costs around $7 and $8 per million tokens on U.S. providers. (DeepSeek offers R1 at $2.19 per million tokens on its own servers, but many organizations will not be able to use it because it is hosted in China.)\u003c/p\u003e\n\n\n\n\u003cp\u003eWith the new change to the CoT output, OpenAI has managed to somewhat work around the transparency problem.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt remains to be seen what OpenAI will do about open sourcing its models. Since its release, R1 has already been adapted, forked and hosted by many different labs and companies potentially making it the preferred reasoning model for enterprises. OpenAI CEO Sam Altman recently admitted that he was “\u003ca href=\"https://venturebeat.com/ai/sam-altman-admits-openai-was-on-the-wrong-side-of-history-in-open-source-debate/\"\u003eon the wrong side of history\u003c/a\u003e” in open source debate. We’ll have to see how this realization will manifest itself in OpenAI’s future releases.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-02-07T21:46:56Z",
  "modifiedTime": "2025-02-07T21:47:48Z"
}
