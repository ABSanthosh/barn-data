{
  "id": "b417d5e5-1239-4d85-b439-ab55e7e460f9",
  "title": "Cohere adds vision to its RAG search capabilities",
  "link": "https://venturebeat.com/ai/cohere-adds-vision-to-its-rag-search-capabilities/",
  "description": "Cohere enhances its embeddings model Embed 3 to now be multimodal, letting enterprises search images on their databases.",
  "author": "Emilia David",
  "published": "Tue, 22 Oct 2024 22:40:26 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Data Infrastructure",
    "AI, ML and Deep Learning",
    "category-/Science/Computer Science",
    "ChatGPT",
    "Cohere",
    "Cohere AI",
    "data",
    "Data Labelling",
    "database",
    "embedding models",
    "embeddings",
    "enterprise RAG",
    "gen AI",
    "Google",
    "image search",
    "images",
    "multimodal models",
    "open source models",
    "OpenAI",
    "retrieval augmented generation",
    "Retrieval-augmented generation (RAG)",
    "Search"
  ],
  "byline": "Emilia David",
  "length": 4475,
  "excerpt": "Cohere enhances its embeddings model Embed 3 to now be multimodal, letting enterprises search images on their databases.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "October 22, 2024 3:40 PM A person's hands type on a laptop displaying a futuristic search interfaces with red and blue holographic elements.Image Credit: VentureBeat made with OpenAI DALL-E 3 via ChatGPT Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Cohere has added multimodal embeddings to its search model, allowing users to deploy images to RAG-style enterprise search.  Embed 3, which emerged last year, uses embedding models that transform data into numerical representations. Embeddings have become crucial in retrieval augmented generation (RAG) because enterprises can make embeddings of their documents that the model can then compare to get the information requested by the prompt.  Your search can see now.We're excited to release fully multimodal embeddings for folks to start building with! pic.twitter.com/Zdj70B07zJ— Aidan Gomez (@aidangomez) October 22, 2024 The new multimodal version can generate embeddings in both images and texts. Cohere claims Embed 3 is “now the most generally capable multimodal embedding model on the market.” Aidan Gonzales, Cohere co-founder and CEO, posted a graph on X showing performance improvements in image search with Embed 3.  The image-search performance of the model across a range of categories is quite compelling. Substantial lifts across nearly all categories considered. pic.twitter.com/6oZ3M6u0V0— Aidan Gomez (@aidangomez) October 22, 2024 “This advancement enables enterprises to unlock real value from their vast amount of data stored in images,” Cohere said in a blog post. “Businesses can now build systems that accurately and quickly search important multimodal assets such as complex reports, product catalogs and design files to boost workforce productivity.” Cohere said a more multimodal focus expands the volume of data enterprises can access through an RAG search. Many organizations often limit RAG searches to structured and unstructured text despite having multiple file formats in their data libraries. Customers can now bring in more charts, graphs, product images, and design templates.  Performance improvements Cohere said encoders in Embed 3 “share a unified latent space,” allowing users to include both images and text in a database. Some methods of image embedding often require maintaining a separate database for images and text. The company said this method leads to better-mixed modality searches.  According to the company, “Other models tend to cluster text and image data into separate areas, which leads to weak search results that are biased toward text-only data. Embed 3, on the other hand, prioritizes the meaning behind the data without biasing towards a specific modality.” Embed 3 is available in more than 100 languages.  Cohere said multimodal Embed 3 is now available on its platform and Amazon SageMaker.  Playing catch up Many consumers are fast becoming familiar with multimodal search, thanks to the introduction of image-based search in platforms like Google and chat interfaces like ChatGPT. As individual users get used to looking for information from pictures, it makes sense that they would want to get the same experience in their working life.  Enterprises have begun seeing this benefit, too, as other companies that offer embedding models provide some multimodal options. Some model developers, like Google and OpenAI, offer some type of multimodal embedding. Other open-source models can also facilitate embeddings for images and other modalities. The fight is now on the multimodal embeddings model that can perform at the speed, accuracy and security enterprises demand.  Cohere, which was founded by some of the researchers responsible for the Transformer model (Gomez is one of the writers of the famous “Attention is all you need” paper), has struggled to be top of mind for many in the enterprise space. It updated its APIs in September to allow customers to switch from competitor models to Cohere models easily. At the time, Cohere had said the move was to align itself with industry standards where customers often toggle between models.  VB Daily Stay in the know! Get the latest news in your inbox daily By subscribing, you agree to VentureBeat's Terms of Service. Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2023/11/DALL·E-2023-11-29-14.14.42-An-image-focused-solely-on-a-laptop-with-hands-typing-on-it-showcasing-a-colorful-and-intricate-futuristic-search-interface-on-the-screen.-The-laptop.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2024-10-22T22:40:26+00:00\" datetime=\"2024-10-22T22:40:26+00:00\"\u003eOctober 22, 2024 3:40 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"429\" src=\"https://venturebeat.com/wp-content/uploads/2023/11/DALL·E-2023-11-29-14.14.42-An-image-focused-solely-on-a-laptop-with-hands-typing-on-it-showcasing-a-colorful-and-intricate-futuristic-search-interface-on-the-screen.-The-laptop.png?w=750\" alt=\"A person\u0026#39;s hands type on a laptop displaying a futuristic search interfaces with red and blue holographic elements.\"/\u003e\u003c/p\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eA person\u0026#39;s hands type on a laptop displaying a futuristic search interfaces with red and blue holographic elements.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eImage Credit: VentureBeat made with OpenAI DALL-E 3 via ChatGPT\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://cohere.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCohere\u003c/a\u003e has added multimodal embeddings to its search model, allowing users to deploy images to RAG-style enterprise search. \u003c/p\u003e\n\n\n\n\u003cp\u003eEmbed 3, \u003ca href=\"https://venturebeat.com/ai/cohere-launches-embed-v3-for-enterprise-llm-applications/\"\u003ewhich emerged last year\u003c/a\u003e, uses embedding models that transform data into numerical representations. \u003ca href=\"https://venturebeat.com/ai/beyond-chatbots-the-wide-world-of-embeddings/\"\u003eEmbeddings have become crucial\u003c/a\u003e in retrieval augmented generation (RAG) because enterprises can make embeddings of their documents that the model can then compare to get the information requested by the prompt. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cdiv lang=\"en\" dir=\"ltr\"\u003e\u003cp\u003eYour search can see now.\u003c/p\u003e\u003cp\u003eWe\u0026#39;re excited to release fully multimodal embeddings for folks to start building with! \u003ca href=\"https://t.co/Zdj70B07zJ\"\u003epic.twitter.com/Zdj70B07zJ\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e— Aidan Gomez (@aidangomez) \u003ca href=\"https://twitter.com/aidangomez/status/1848756300122828857?ref_src=twsrc%5Etfw\"\u003eOctober 22, 2024\u003c/a\u003e\u003c/blockquote\u003e \n\n\n\n\u003cp\u003eThe new multimodal version can generate embeddings in both images and texts. Cohere claims Embed 3 is “now the most generally capable multimodal embedding model on the market.” Aidan Gonzales, Cohere co-founder and CEO, posted a graph on X showing performance improvements in image search with Embed 3. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eThe image-search performance of the model across a range of categories is quite compelling. Substantial lifts across nearly all categories considered. \u003ca href=\"https://t.co/6oZ3M6u0V0\"\u003epic.twitter.com/6oZ3M6u0V0\u003c/a\u003e\u003c/p\u003e— Aidan Gomez (@aidangomez) \u003ca href=\"https://twitter.com/aidangomez/status/1848756302396223500?ref_src=twsrc%5Etfw\"\u003eOctober 22, 2024\u003c/a\u003e\u003c/blockquote\u003e \n\n\n\n\u003cp\u003e“This advancement enables enterprises to unlock real value from their vast amount of data stored in images,” Cohere said in \u003ca href=\"https://cohere.com/blog/multimodal-embed-3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ea blog post\u003c/a\u003e. “Businesses can now build systems that accurately and quickly search important multimodal assets such as complex reports, product catalogs and design files to boost workforce productivity.”\u003c/p\u003e\n\n\n\n\u003cp\u003eCohere said a more multimodal focus expands the volume of data enterprises can access through an RAG search. Many organizations often limit \u003ca href=\"https://venturebeat.com/ai/from-gen-ai-1-5-to-2-0-moving-from-rag-to-agent-systems/\"\u003eRAG searches\u003c/a\u003e to structured and unstructured text despite having multiple file formats in their data libraries. Customers can now bring in more charts, graphs, product images, and design templates. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-performance-improvements\"\u003ePerformance improvements\u003c/h2\u003e\n\n\n\n\u003cp\u003eCohere said encoders in Embed 3 “share a unified latent space,” allowing users to include both images and text in a database. Some methods of image embedding often require maintaining a separate database for images and text. The company said this method leads to better-mixed modality searches. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to the company, “Other models tend to cluster text and image data into separate areas, which leads to weak search results that are biased toward text-only data. Embed 3, on the other hand, prioritizes the meaning behind the data without biasing towards a specific modality.”\u003c/p\u003e\n\n\n\n\u003cp\u003eEmbed 3 is available in more than 100 languages. \u003c/p\u003e\n\n\n\n\u003cp\u003eCohere said multimodal Embed 3 is now available on its platform and Amazon SageMaker. \u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-playing-catch-up\"\u003ePlaying catch up\u003c/h2\u003e\n\n\n\n\u003cp\u003eMany consumers are fast becoming familiar with multimodal search, thanks to the introduction of image-based search in platforms like Google and chat interfaces like ChatGPT. As individual users get used to looking for information from pictures, it makes sense that they would want to get the same experience in their working life. \u003c/p\u003e\n\n\n\n\u003cp\u003eEnterprises have begun seeing this benefit, too, as other companies that offer embedding models provide some multimodal options. Some model developers, like \u003ca href=\"https://google.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle\u003c/a\u003e and \u003ca href=\"https://openai.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpenAI\u003c/a\u003e, offer some type of multimodal embedding. Other open-source models can also facilitate embeddings for images and other modalities. The fight is now on the multimodal embeddings model that can perform at the speed, accuracy and security enterprises demand. \u003c/p\u003e\n\n\n\n\u003cp\u003eCohere, which was founded by some of the researchers responsible for the Transformer model (Gomez is one of the writers of the famous “Attention is all you need” paper), has struggled to be top of mind for many in the enterprise space. It \u003ca href=\"https://venturebeat.com/ai/cohere-updates-apis-to-make-it-easier-for-devs-to-switch-from-other-models/\"\u003eupdated its APIs in September\u003c/a\u003e to allow customers to switch from competitor models to Cohere models easily. At the time, Cohere had said the move was to align itself with industry standards where customers often toggle between models. \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eVB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eStay in the know! Get the latest news in your inbox daily\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eBy subscribing, you agree to VentureBeat\u0026#39;s \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003eTerms of Service.\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-10-22T22:40:26Z",
  "modifiedTime": "2024-10-22T22:40:36Z"
}
