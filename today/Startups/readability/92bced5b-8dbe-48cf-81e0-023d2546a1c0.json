{
  "id": "92bced5b-8dbe-48cf-81e0-023d2546a1c0",
  "title": "Google DeepMind researchers introduce new benchmark to improve LLM factuality, reduce hallucinations",
  "link": "https://venturebeat.com/ai/google-deepmind-researchers-introduce-new-benchmark-to-improve-llm-factuality-reduce-hallucinations/",
  "description": "Based on a new benchmark, Google DeepMind found Gemini 2.0 Flash to be the most factual LLM, with a score of 83.6%.",
  "author": "Taryn Plumb",
  "published": "Fri, 10 Jan 2025 22:05:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI research",
    "AI, ML and Deep Learning",
    "category-/Computers \u0026 Electronics",
    "category-/Science/Computer Science",
    "Conversational AI",
    "DeepMind",
    "Generative AI",
    "Google Deepmind",
    "Google DeepMind Researchers",
    "hallucinations",
    "large language models",
    "LLM hallucinations",
    "LLMs",
    "NLP"
  ],
  "byline": "Taryn Plumb",
  "length": 6136,
  "excerpt": "Based on a new benchmark, Google DeepMind found Gemini 2.0 Flash to be the most factual LLM, with a score of 83.6%.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Hallucinations, or factually inaccurate responses, continue to plague large language models (LLMs). Models falter particularly when they are given more complex tasks and when users are looking for specific and highly detailed responses.  It’s a challenge data scientists have struggled to overcome, and now, researchers from Google DeepMind say they have come a step closer to achieving true factuality in foundation models. They have introduced FACTS Grounding, a benchmark that evaluates LLMs’ ability to generate factually accurate responses based on long-form documents. Models are also judged on whether their responses are detailed enough to provide useful, relevant answers to prompts.  Along with the new benchmark, the researchers have released a FACTS leaderboard to the Kaggle data science community.  As of this week, Gemini 2.0 Flash topped the leaderboard, with a factuality score of 83.6%. Others in the top 9 include Google’s Gemini 1.0 Flash and Gemini 1.5 Pro; Anthropic’s Clade 3.5 Sonnet and Claude 3.5 Haiku; and OpenAI’s GPT-4o, 4o-mini, o1-mini and o1-preview. These all ranked above 61.7% in terms of accuracy. The researchers say the leaderboard will be actively maintained and continually updated to include new models and their different iterations.  “We believe that this benchmark fills a gap in evaluating a wider variety of model behaviors pertaining to factuality, in comparison to benchmarks that focus on narrower use cases…such as summarization alone,” the researchers write in a technical paper published this week. Weeding out inaccurate responses Ensuring factual accuracy in LLM responses is difficult because of modeling (architecture, training and inference) and measuring (evaluation methodologies, data and metrics) factors. Typically, researchers point out, pre-training focuses on predicting the next token given previous tokens.  “While this objective may teach models salient world knowledge, it does not directly optimize the model towards the various factuality scenarios, instead encouraging the model to generate generally plausible text,” the researchers write.  To address this, the FACTS dataset incorporates 1,719 examples — 860 public and 859 private — each requiring long-form responses based on context in provided documents. Each example includes:  A system prompt (system_instruction) with general directives and the order to only answer based on provided context; A task (user_request) that includes a specific question to be answered;  A long document (context_document) with necessary information.  To succeed and be labeled “accurate,” the model must process the long-form document and create a subsequent long-form response that is both comprehensive and fully attributable to the document. Responses are labeled “inaccurate” if the model’s claims are not directly supported by the document and not highly relevant or useful.  For example, a user may ask a model to summarize the main reasons why a company’s revenue decreased in Q3, and provide it with detailed information including a company’s annual financial report discussing quarterly earnings, expenses, planned investments and market analysis.  If a model then, say, returned: “The company faced challenges in Q3 that impacted its revenue,” it would be deemed inaccurate.  “The response avoids specifying any reasons, such as market trends, increased competition or operational setbacks, which would likely be in the document,” the researchers point out. “It doesn’t demonstrate an attempt to engage with or extract relevant details.”  By contrast, if a user prompted, “What are some tips on saving money?” and provided a compilation of categorized money-saving tips for college students, a correct response would be highly detailed: “Utilize free activities on campus, buy items in bulk and cook at home. Also, set spending goals, avoid credit cards and conserve resources.”  DeepMind uses LLMs to judge LLMs To allow for diverse inputs, researchers included documents of varying lengths, up to 32,000 tokens (or the equivalent of 20,000 words). These cover areas including finance, technology, retail, medicine and law. User requests are also broad, including Q\u0026A generation, requests for summarization and rewriting.  Each example is judged in two phases. First, responses are evaluated for eligibility: If they don’t satisfy user requests, they are disqualified. Second, responses must be hallucination-free and fully grounded in the documents provided. These factuality scores are calculated by three different LLM judges — specifically Gemini 1.5 Pro, GPT-4o and Claude 3.5 Sonnet — that determine individual scores based on the percentage of accurate model outputs. Subsequently, the final factuality determination is based on an average of the three judges’ scores. Researchers point out that models are often biased towards other members of their model family — at a mean increase of around 3.23% — so the combination of different judges was critical to help ensure responses were indeed factual. Ultimately, the researchers emphasize that factuality and grounding are key factors to the future success and usefulness of LLMs. “We believe that comprehensive benchmarking methods, coupled with continuous research and development, will continue to improve AI systems,” they write.  However, they also concede: “We are mindful that benchmarks can be quickly overtaken by progress, so this launch of our FACTS Grounding benchmark and leaderboard is just the beginning.”  Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/01/a-medium-shot-of-a-sophisticated-ai-robo_z7e8_hz3QaqLCZpO3cV2tw_AJOkXZ8wSti6QVF-s9_LZg-transformed.jpeg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://venturebeat.com/ai/meta-proposes-new-scalable-memory-layers-that-improve-knowledge-reduce-hallucinations/\"\u003eHallucinations\u003c/a\u003e, or factually inaccurate responses, continue to plague large language models (LLMs). Models falter particularly when they are given more complex tasks and when users are looking for specific and highly detailed responses. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt’s a challenge data scientists have struggled to overcome, and now, researchers from \u003ca href=\"https://deepmind.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle DeepMind\u003c/a\u003e say they have come a step closer to achieving true factuality in foundation models. They have introduced FACTS Grounding, a benchmark that evaluates LLMs’ ability to generate factually accurate responses based on long-form documents. Models are also judged on whether their responses are detailed enough to provide useful, relevant answers to prompts. \u003c/p\u003e\n\n\n\n\u003cp\u003eAlong with the new benchmark, the researchers have released a \u003ca href=\"http://www.kaggle.com/facts-leaderboard\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eFACTS leaderboard\u003c/a\u003e to the Kaggle data science community. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs of this week, Gemini 2.0 Flash topped the leaderboard, with a factuality score of 83.6%. Others in the top 9 include Google’s Gemini 1.0 Flash and Gemini 1.5 Pro; Anthropic’s Clade 3.5 Sonnet and Claude 3.5 Haiku; and OpenAI’s GPT-4o, 4o-mini, o1-mini and o1-preview. These all ranked above 61.7% in terms of accuracy.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"1491\" height=\"781\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-118.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-118.png 1491w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-118.png?resize=300,157 300w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-118.png?resize=768,402 768w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-118.png?resize=800,419 800w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-118.png?resize=400,210 400w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-118.png?resize=750,393 750w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-118.png?resize=578,303 578w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-118.png?resize=930,487 930w\" sizes=\"(max-width: 1491px) 100vw, 1491px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe researchers say the leaderboard will be actively maintained and continually updated to include new models and their different iterations. \u003c/p\u003e\n\n\n\n\u003cp\u003e“We believe that this benchmark fills a gap in evaluating a wider variety of model behaviors pertaining to factuality, in comparison to benchmarks that focus on narrower use cases…such as summarization alone,” the researchers write in a \u003ca href=\"https://arxiv.org/pdf/2501.03200\"\u003etechnical paper\u003c/a\u003e published this week.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-weeding-out-inaccurate-responses\"\u003eWeeding out inaccurate responses\u003c/h2\u003e\n\n\n\n\u003cp\u003eEnsuring \u003ca href=\"https://venturebeat.com/ai/learn-how-ge-healthcare-used-aws-to-build-a-new-ai-model-that-interprets-mris/\"\u003efactual accuracy\u003c/a\u003e in LLM responses is difficult because of modeling (architecture, training and inference) and measuring (evaluation methodologies, data and metrics) factors. Typically, researchers point out, pre-training focuses on predicting the next token given previous tokens. \u003c/p\u003e\n\n\n\n\u003cp\u003e“While this objective may teach models salient world knowledge, it does not directly optimize the model towards the various factuality scenarios, instead encouraging the model to generate generally \u003cem\u003eplausible \u003c/em\u003etext,” the researchers write. \u003c/p\u003e\n\n\n\n\u003cp\u003eTo address this, the FACTS dataset incorporates 1,719 examples — 860 public and 859 private — each requiring long-form responses based on context in provided documents. Each example includes: \u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eA system prompt (system_instruction) with general directives and the order to only answer based on provided context;\u003c/li\u003e\n\n\n\n\u003cli\u003eA task (user_request) that includes a specific question to be answered; \u003c/li\u003e\n\n\n\n\u003cli\u003eA long document (context_document) with necessary information. \u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eTo succeed and be labeled “accurate,” \u003ca href=\"https://venturebeat.com/ai/what-ai-vendor-should-you-choose-here-are-the-top-7-openai-still-leads/\"\u003ethe model\u003c/a\u003e must process the long-form document and create a subsequent long-form response that is both comprehensive and fully attributable to the document. Responses are labeled “inaccurate” if the model’s claims are not directly supported by the document and not highly relevant or useful. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, a user may ask a model to summarize the main reasons why a company’s revenue decreased in Q3, and provide it with detailed information including a company’s annual financial report discussing quarterly earnings, expenses, planned investments and market analysis. \u003c/p\u003e\n\n\n\n\u003cp\u003eIf a model then, say, returned: “The company faced challenges in Q3 that impacted its revenue,” it would be deemed inaccurate. \u003c/p\u003e\n\n\n\n\u003cp\u003e“The response avoids specifying any reasons, such as market trends, increased competition or operational setbacks, which would likely be in the document,” the researchers point out. “It doesn’t demonstrate an attempt to engage with or extract relevant details.” \u003c/p\u003e\n\n\n\n\u003cp\u003eBy contrast, if a user prompted, “What are some tips on saving money?” and provided a compilation of categorized money-saving tips for college students, a correct response would be highly detailed: “Utilize free activities on campus, buy items in bulk and cook at home. Also, set spending goals, avoid credit cards and conserve resources.” \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"769\" height=\"431\" src=\"https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-120.png\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-120.png 769w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-120.png?resize=300,168 300w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-120.png?resize=400,224 400w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-120.png?resize=750,420 750w, https://venturebeat.com/wp-content/uploads/2025/01/Screenshot-120.png?resize=578,324 578w\" sizes=\"(max-width: 769px) 100vw, 769px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-deepmind-uses-llms-to-judge-llms\"\u003eDeepMind uses LLMs to judge LLMs\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo allow for diverse inputs, researchers included documents of varying lengths, up to 32,000 tokens (or the equivalent of 20,000 words). These cover areas including finance, technology, retail, medicine and law. User requests are also broad, including Q\u0026amp;A generation, requests for summarization and rewriting. \u003c/p\u003e\n\n\n\n\u003cp\u003eEach example is judged in two phases. First, responses are evaluated for eligibility: If they don’t satisfy user requests, they are disqualified. Second, responses must be hallucination-free and fully grounded in the documents provided.\u003c/p\u003e\n\n\n\n\u003cp\u003eThese factuality scores are calculated by three different LLM judges — specifically Gemini 1.5 Pro, GPT-4o and Claude 3.5 Sonnet — that determine individual scores based on the percentage of accurate model outputs. Subsequently, the final factuality determination is based on an average of the three judges’ scores.\u003c/p\u003e\n\n\n\n\u003cp\u003eResearchers point out that models are often biased towards other members of their model family — at a mean increase of around 3.23% — so the combination of different judges was critical to help ensure responses were indeed factual.\u003c/p\u003e\n\n\n\n\u003cp\u003eUltimately, the researchers emphasize that factuality and grounding are key factors to the future success and usefulness of LLMs. “We believe that comprehensive benchmarking methods, coupled with continuous research and development, will continue to improve AI systems,” they write. \u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, they also concede: “We are mindful that benchmarks can be quickly overtaken by progress, so this launch of our FACTS Grounding benchmark and leaderboard is just the beginning.” \u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-01-10T22:05:00Z",
  "modifiedTime": "2025-01-10T22:11:27Z"
}
