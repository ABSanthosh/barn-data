{
  "id": "149f9b9c-f1f9-4577-8156-ef9fffe59c1f",
  "title": "DeepSeek's multi-head latent attention and other KV cache tricks",
  "link": "https://www.pyspur.dev/blog/multi-head-latent-attention-kv-cache-paper-list",
  "description": "Article URL: https://www.pyspur.dev/blog/multi-head-latent-attention-kv-cache-paper-list Comments URL: https://news.ycombinator.com/item?id=42858741 Points: 137 # Comments: 17",
  "author": "t55",
  "published": "Tue, 28 Jan 2025 22:11:36 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "PySpur",
  "length": 16648,
  "excerpt": "How a Key-Value (KV) cache reduces Transformer inference time by trading memory for computation",
  "siteName": "",
  "favicon": "",
  "text": "January 21, 2025 (1w ago)•Overview: Introduction: We'll explore how Key-Value (KV) caches make language models like ChatGPT and DeepSeek faster at generating text, by making a clever trade-off between memory usage and computation time. MLA and other Tricks: We'll then look at 11 recent research papers, including DeepSeek's Multi-head Latent Attention (MLA), that build upon this basic idea to make LLM inference even more time-efficient. Understanding the Problem: Why Text Generation is Slow Let's start with a simple analogy. Imagine you're writing a story, and for each new word you write, you need to re-read the entire story so far to maintain consistency. The longer your story gets, the more time you spend re-reading. This is exactly what large language models face during text generation! The Basic Building Block: Self-Attention At the heart of modern language models is a mechanism called self-attention. For a sequence of nn tokens (think of tokens as roughly corresponding to words), each token needs to \"look at\" or \"attend to\" all other tokens to understand the context. This looking-at-everything process has a computational cost that grows with the sequence length: For nn tokens, each token needs to look at all nn tokens This means the cost is proportional to n×n=n2n \\times n = n^2 In mathematical notation, we write this as O(n2)O(n^2) complexity The Real Problem: Generating Text One Token at a Time When a language model generates text, it does so one token at a time, and this is where things get computationally expensive: First token: Look at 1 token (cost: O(12)O(1^2)) Second token: Look at 2 tokens (cost: O(22)O(2^2)) Third token: Look at 3 tokens (cost: O(32)O(3^2)) And so on until the nn-th token: Look at nn tokens (cost: O(n2)O(n^2)) If we add up all these costs for generating a sequence of length nn, we get: O(12+22+32+⋯+n2)≈O(n3)O(1^2 + 2^2 + 3^2 + \\dots + n^2) \\approx O(n^3) This O(n3)O(n^3) cost means that as your text gets longer, the generation time grows extremely quickly. For example, generating a sequence twice as long takes roughly eight times as long! Clearly, we need a better approach. The Solution: Key-Value (KV) Cache The key insight behind KV caching is that we're doing a lot of redundant work. When generating each new token, we're recomputing things for all previous tokens that we've already processed before. Let's see how we can fix this. What is a Key-Value Cache? Think of a KV cache like a smart notepad where we write down important information about each token the first time we see it. For each token, we compute and store two things: A key (kk): Think of this as an addressing mechanism - it helps determine how relevant this token is to future tokens A value (vv): Think of this as the actual information that gets used when this token is found to be relevant Mathematically, we compute these as: Key: k=xWKk = x W_K (where xx is the token and WKW_K is a learned transformation) Value: v=xWVv = x W_V (where WVW_V is another learned transformation) When generating a new token, we use its query (computed similarly to keys) to find relevant information in our cache by comparing it with all stored keys. The matching values are then used to help generate the token. How the KV Cache Makes Things Faster With a KV cache, the process becomes much more efficient: When we see a new token, we only need to compute its key and value once For all future tokens, we can just look up these pre-computed values from our cache This means each new token only needs to do a small amount of new work, instead of redoing all previous computations The trade-off is clear: We use more memory to store all the keys and values. For a model with: LL layers HH attention heads Sequence length nn Key/value dimension dkd_k The total memory cost is L×H×n×dk×2L \\times H \\times n \\times d_k \\times 2 values (the factor of 2 accounts for both keys and values). This grows linearly with sequence length (O(n)O(n)), but the constant factors can be substantial for large models. But in return, we reduce the computation cost from O(n3)O(n^3) to O(n2)O(n^2) To understand why it's O(n2)O(n^2), let's look at the cost at each step: Step 1: Process 1 token → cost O(1)O(1) Step 2: Process 1 new token + look at 1 cached token → cost O(2)O(2) Step 3: Process 1 new token + look at 2 cached tokens → cost O(3)O(3) And so on... Adding these up: O(1+2+3+⋯+n)=O(n2)O(1 + 2 + 3 + \\dots + n) = O(n^2) This is a dramatic improvement over O(n3)O(n^3)! While we still have to do the fundamental work of looking at all previous tokens (O(n2)O(n^2)), we avoid the costly recomputation at each step. The Memory Challenge: Why We Need Better Solutions While KV cache is a powerful optimization, it comes with a significant memory cost. Let's look at a concrete example using a modern large language model like Llama3 70B with: L=80L = 80 layers H=64H = 64 attention heads B=8B = 8 batch size of 8 sequences dk=128d_k = 128 key/value dimension 16-bit precision The memory required for a batch of 8 sequences of 1000 tokens each would be: L×H×B×n×dk×2×2 bytes=80×64×8×1000×128×2×2 bytes=20.97GBL \\times H \\times B \\times n \\times d_k \\times 2 \\times 2 \\text{ bytes} = 80 \\times 64 \\times 8 \\times 1000 \\times 128 \\times 2 \\times 2 \\text{ bytes} = 20.97\\text{GB} This substantial memory usage creates several challenges: Scales linearly with sequence length Multiplies with batch size for parallel processing Limits the maximum context length we can handle Constrains deployment on memory-limited devices These challenges have sparked a wave of innovation in the research community, leading to various techniques for optimizing KV cache usage. Let's explore these cutting-edge solutions. Can we improve over naive KV caches? The following papers represent key innovations in KV cache optimization. We'll explore them through three main approaches: token selection, post-hoc compression techniques, and architectural redesigns. Token Selection and Pruning Approaches 1) Heavy-Hitter Oracle (H2O) H2O introduces the concept of identifying and preserving important tokens in the KV cache: Heavy-Hitter Tokens: H2O identifies tokens with the highest accumulated attention scores during generation, following a power-law distribution. These tokens are critical for model functionality and are prioritized in the cache. Dynamic Submodular Eviction: The method frames cache management as an optimization problem with a submodular objective function F(S)F(S) that quantifies the importance of a token set SS: F(S)=∑i∈SAiF(S) = \\sum_{i \\in S} A_{i} where AiA_i is the accumulated attention score for token ii. The cache StS_t is updated by: St=argmaxS⊆St−1∪{i},∣S∣≤k F(S)S_t = \\text{argmax}_{S \\subseteq S_{t-1} \\cup \\{i\\}, |S| \\leq k} \\, F(S) ensuring that at most one token is evicted per step. This greedy algorithm is computationally efficient and guarantees near-optimal performance under submodular constraints. Results: Achieves 5× reduction in KV cache size with negligible accuracy loss and up to 29× throughput improvement. 2) StreamLLM The authors observe the phenomenon of Attention Sinks: Initial tokens that act as natural attention anchors during decoding Without these attention sink tokens, the performance of naive window attention drops Based on that observation, they introduce a Rolling Cache for recent context with retained initial tokens, enabling infinite-length sequence processing. They show that these sink tokens can also be trained; serving as dedicated attention anchors, reducing reliance on multiple initial tokens. 3) Value-Aware Token Pruning (VATP) VATP extends H2O's token importance concept by considering both attention patterns and value vector properties: Importance Scoring: Combines attention scores with value vector information: Ikt=Skt⋅∥vk∥1,Skt=∑k≤j≤taj,kI_k^t = S_k^t \\cdot \\|v_k\\|_1, \\quad S_k^t = \\sum_{k \\leq j \\leq t} a_{j,k} where SktS_k^t is the accumulated attention score and ∥vk∥1\\|v_k\\|_1 is the value vector's L1 norm. Token Pruning: Tokens are ranked by IktI_k^t, and those with the lowest scores are pruned, while attention sink tokens (e.g., start or newline tokens) are preserved to prevent performance degradation. Performance and Efficiency: Outperforms baselines like H2O and Scissorhands in 12–14 out of 16 LongBench tasks. Achieves effective 50% compression with minimal performance loss. Introduces negligible computational overhead and is compatible with FlashAttention when integrated with Scissorhands. Post-hoc Compression Techniques These methods compress or optimize the KV cache while preserving the standard transformer architecture. 4) Adaptive KV Compression (FastGen) FastGen introduces adaptive compression based on attention patterns observed at run-time: Attention Profiling: during prompt encoding, FastGen identifies attention patterns and selects compression policies C∗C^* that minimize memory cost while preserving attention recovery: C∗=arg⁡min⁡C∈CCacheMemoryCost(C)s.t.∥A−softmax(QKCT)∥≤1−T.C^* = \\arg\\min_{C \\in \\mathcal{C}} \\text{CacheMemoryCost}(C) \\quad \\text{s.t.} \\quad \\|A - \\text{softmax}(QK_C^T)\\| \\leq 1 - T. Adaptive Compression Policies: Compression strategies include: Special Tokens (CspecialC_{\\text{special}}): Retain only special tokens. Locality (ClocalC_{\\text{local}}): Evict tokens beyond a relative distance rlr_l. Frequency (CfrequentC_{\\text{frequent}}): Keep tokens with high cumulative attention scores (rfr_f). Hybrid Policies combine strategies, starting with CspecialC_{\\text{special}}, and applies them adaptively to each head: C={Cspecial,Cspecial+Cpunct,…,Cfull}.\\mathcal{C} = \\{C_{\\text{special}}, C_{\\text{special}} + C_{\\text{punct}}, \\ldots, C_{\\text{full}}\\}. Token Generation: During decoding, pre-selected compression policies manage the KV cache efficiently: KCi,VCi=f(K,V,Ci).K_{C_i}, V_{C_i} = f(K, V, C_i). 5) Dynamic Memory Compression (DMC) DMC introduces adaptive token merging: Decision Mechanism: At time tt, predicts merge decisions αt\\alpha_t and weights ωt\\omega_t: αt=⌊sigmoid(kt[0])⌉,ωt=sigmoid(qt[0]).\\alpha_t = \\lfloor \\text{sigmoid}(k_t[0]) \\rceil, \\quad \\omega_t = \\text{sigmoid}(q_t[0]). Weighted Merging: When αt=1\\alpha_t = 1, merges current and previous entries: k′=ωtkt+zt−1kt−1ωt+zt−1,v′=ωtvt+zt−1vt−1ωt+zt−1,k' = \\frac{\\omega_t k_t + z_{t-1} k_{t-1}}{\\omega_t + z_{t-1}}, \\quad v' = \\frac{\\omega_t v_t + z_{t-1} v_{t-1}}{\\omega_t + z_{t-1}}, where zt=zt−1+ωtz_t = z_{t-1} + \\omega_t accumulates importance weights. Training: Uses a Gumbel-Sigmoid relaxation for αt\\alpha_t to allow end-to-end training with gradient descent: αt∼Gumbel-Sigmoid(kt[0],τ),\\alpha_t \\sim \\text{Gumbel-Sigmoid}(k_t[0], \\tau), where τ\\tau is a temperature parameter. Optimizes a combined objective: L=LLM+λmax⁡(0,nCR−∑tαt),\\mathcal{L} = \\mathcal{L}_{\\text{LM}} + \\lambda \\max\\left(0, \\frac{n}{\\text{CR}} - \\sum_{t} \\alpha_t \\right), where LLM\\mathcal{L}_{\\text{LM}} is the language modeling loss, and the second term encourages the model to match a target compression ratio (CR). Results: Up to 8× compression with maintained performance. 6) L2L_2 Norm-Based Compression This paper presents a surprising observation: A clear correlation between the L2L_2 norm and the attention scores over cached KV pairs, where a low L2L_2 norm of a key embedding usually leads to a high attention score during decoding. Consequently, they introduce a simple but effective compression objective: Norm-Based Selection: For a set of cached keys K={k1,k2,…,kn}K = \\{k_1, k_2, \\dots, k_n\\}, computes and sorts key norms: ∥ki∥2=∑j=1dki,j2\\|k_i\\|_2 = \\sqrt{\\sum_{j=1}^d k_{i,j}^2} Sorting and Selection: To compress the KV cache, sort all keys by their L2 norm values: Ksorted=Sort({∥k1∥2,∥k2∥2,…,∥kn∥2})K_{\\text{sorted}} = \\text{Sort}\\big(\\{\\|k_1\\|_2, \\|k_2\\|_2, \\dots, \\|k_n\\|_2\\}\\big) Retain the top-mm keys with lowest norms, where m=⌊c⋅n⌋m = \\lfloor c \\cdot n \\rfloor and cc is the compression ratio. Compressed Cache: The compressed key-value cache consists of: Kcompressed={ki∣∥ki∥2∈Ksorted[1:m]},Vcompressed={vi∣ki∈Kcompressed}K_{\\text{compressed}} = \\{k_i \\mid \\|k_i\\|_2 \\in K_{\\text{sorted}}[1:m]\\}, \\quad V_{\\text{compressed}} = \\{v_i \\mid k_i \\in K_{\\text{compressed}}\\} Due to its simplicity, this approach maintains compatibility with FlashAttention. Architectural Redesigns These approaches change the Transformers architecture to handle KV caches more efficiently, often incorporating compression directly into the architecture. 7) Multi-Query Attention (MQA) Key Idea: MQA reduces the KV cache size by sharing a single key-value head across all query heads, replacing the traditional Multi-Head Attention (MHA): K=XWK,V=XWV,K = XW_K, \\quad V = XW_V, where KK and VV are the shared key and value projections. Benefits: Reduces the KV cache size by a factor of HH (the number of attention heads), significantly lowering memory bandwidth overhead. Trade-Off: While MQA is faster, it often suffers from quality degradation, especially in tasks requiring diverse attention patterns. 8) Group-Query Attention (GQA) Key Idea: GQA interpolates between full multi-head attention and MQA to offering a scalable trade-off between inference speed and model quality. It divides query heads into GG groups, where each group shares a single key-value head: Kgroup=1∣G∣∑h∈GKh,Vgroup=1∣G∣∑h∈GVhK_{\\text{group}} = \\frac{1}{|G|} \\sum_{h \\in G} K_h, \\quad V_{\\text{group}} = \\frac{1}{|G|} \\sum_{h \\in G} V_h GQA-1: Equivalent to MQA (G=1G = 1 ). GQA-HH : Equivalent to MHA (G=HG = H ). Uptraining: GQA can be introduced to existing pre-trained models through fine-tuning: First, convert MHA checkpoints to GQA by mean pooling key and value heads into groups Then fine-tune (\"uptrain\") the model briefly to adapt to the new attention pattern This adaptation process requires only 5% of the original pre-training compute, making it very efficient The resulting model maintains quality while gaining GQA's memory benefits 9) Multi-head Latent Attention (MLA) DeepSeek's Multi-Head Latent Attention (MLA) takes a novel approach to reducing KV cache overhead. While MQA and GQA achieve this through head-sharing, MLA instead employs a low-rank latent compression technique that maintains the benefits of multiple attention heads. MLA reduces KV cache size by compressing keys and values into low-dimensional latent vectors before reconstruction. It down-project key-value embeddings into a compressed latent space: cKV,t=WDKVht,kC=WUKcKV,t,vC=WUVcKV,tc_{\\text{KV}, t} = W_{\\text{DKV}} h_t, \\quad k_C = W_{\\text{UK}} c_{\\text{KV}, t}, \\quad v_C = W_{\\text{UV}} c_{\\text{KV}, t} where WDKVW_{\\text{DKV}} is the down-projection matrix, and WUKW_{\\text{UK}}, WUVW_{\\text{UV}} are up-projection matrices for keys and values. It retains per-head flexibility through compressed representations, unlike MQA's complete head sharing. It introduces Rotary Positional Embeddings (RoPE) for decoupling position-aware keys: kR=RoPE(WKRht),kt=[kC;kR]k_R = \\text{RoPE}(W_{KR} h_t), \\quad k_t = [k_C; k_R] This reduces KV cache storage further by caching only compressed latent vectors cKVc_{\\text{KV}} and positional keys kRk_R. 10) SnapKV SnapKV introduces an Observation Window: Uses end-of-prompt tokens to identify attention patterns: C=∑i=0LobsWobs[:,i,:],I=Topk(C,k)C = \\sum_{i=0}^{L_{\\text{obs}}} W_{\\text{obs}}[:, i, :], \\quad I = \\text{Top}_k(C, k) where WobsW_{\\text{obs}} represents the attention weights, and kk is determined by the compression rate. Compression: Clusters features around the selected positions using a pooling layer to preserve context completeness. 11) You Only Cache Once (YOCO) YOCO modifies the transformer architecture for caching: Global Cache: Uses a decoder-decoder design with a single shared KV cache. Complexity Reduction: Reduces memory from O(N×L)O(N \\times L) to O(N+L)O(N + L), where NN is sequence length and LL is the number of layers. Efficient Attention: The self-decoder employs sliding-window attention or gated retention, enabling constant memory usage (O(C)O(C), where CC is a small window size). Conclusion Key-Value caching techniques are central to scaling and optimizing Transformer-based models for real-world use. Innovations like dynamic eviction, compression, and structured approximations continue to push the boundaries on what is possible in long-context or resource-constrained scenarios. KV caching remains a lively research area, offering both theoretical insights and practical improvements. PS: This blog post is mostly AI-generated using a PySpur workflow with minor human edits.",
  "image": "http://localhost:3000/blog/kv-cache/mla.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection id=\"blog\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"DeepSeek\u0026#39;s Multi-Head Latent Attention and Other KV Cache Tricks\" loading=\"lazy\" width=\"1920\" height=\"1080\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://www.pyspur.dev/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png\u0026amp;w=1920\u0026amp;q=75\u0026amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 1x, https://www.pyspur.dev/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png\u0026amp;w=3840\u0026amp;q=75\u0026amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 2x\" src=\"https://www.pyspur.dev/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png\u0026amp;w=3840\u0026amp;q=75\u0026amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP\"/\u003e\u003c/p\u003e\u003cdiv\u003e\u003cp\u003e\u003ctime datetime=\"2025-01-21\"\u003eJanuary 21, 2025 (1w ago)\u003c/time\u003e\u003cspan\u003e•\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://twitter.com/jeankaddour\"\u003e\u003cimg alt=\"Jean Kaddour\" loading=\"lazy\" width=\"40\" height=\"40\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://www.pyspur.dev/_next/image?url=%2Fauthor.jpg\u0026amp;w=48\u0026amp;q=75\u0026amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 1x, https://www.pyspur.dev/_next/image?url=%2Fauthor.jpg\u0026amp;w=96\u0026amp;q=75\u0026amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 2x\" src=\"https://www.pyspur.dev/_next/image?url=%2Fauthor.jpg\u0026amp;w=96\u0026amp;q=75\u0026amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP\"/\u003e\u003c/a\u003e\u003c/div\u003e\u003carticle\u003e\u003cp\u003e\u003cstrong\u003eOverview\u003c/strong\u003e:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e: We\u0026#39;ll explore how Key-Value (KV) caches make language models like ChatGPT and DeepSeek faster at generating text, by making a clever trade-off between memory usage and computation time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMLA and other Tricks\u003c/strong\u003e: We\u0026#39;ll then look at 11 recent research papers, including \u003cstrong\u003eDeepSeek\u0026#39;s Multi-head Latent Attention (MLA)\u003c/strong\u003e, that build upon this basic idea to make LLM inference even more time-efficient.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr/\u003e\n\u003ch2\u003eUnderstanding the Problem: Why Text Generation is Slow\u003c/h2\u003e\n\u003cp\u003eLet\u0026#39;s start with a simple analogy. Imagine you\u0026#39;re writing a story, and for each new word you write, you need to re-read the entire story so far to maintain consistency. The longer your story gets, the more time you spend re-reading. This is exactly what large language models face during text generation!\u003c/p\u003e\n\u003ch3\u003eThe Basic Building Block: Self-Attention\u003c/h3\u003e\n\u003cp\u003eAt the heart of modern language models is a mechanism called \u003cstrong\u003eself-attention\u003c/strong\u003e. For a sequence of \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e tokens (think of tokens as roughly corresponding to words), each token needs to \u0026#34;look at\u0026#34; or \u0026#34;attend to\u0026#34; all other tokens to understand the context.\u003c/p\u003e\n\u003cp\u003eThis looking-at-everything process has a computational cost that grows with the sequence length:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e tokens, each token needs to look at all \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e tokens\u003c/li\u003e\n\u003cli\u003eThis means the cost is proportional to \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en \\times n = n^2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eIn mathematical notation, we write this as \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(n^2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e complexity\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eThe Real Problem: Generating Text One Token at a Time\u003c/h3\u003e\n\u003cp\u003eWhen a language model generates text, it does so one token at a time, and this is where things get computationally expensive:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eFirst token\u003c/strong\u003e: Look at 1 token (cost: \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(1^2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSecond token\u003c/strong\u003e: Look at 2 tokens (cost: \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(2^2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThird token\u003c/strong\u003e: Look at 3 tokens (cost: \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmn\u003e3\u003c/mn\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(3^2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/li\u003e\n\u003cli\u003eAnd so on until the \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e-th token: Look at \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e tokens (cost: \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(n^2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIf we add up all these costs for generating a sequence of length \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e, we get:\u003c/p\u003e\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsup\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsup\u003e\u003cmn\u003e3\u003c/mn\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmo\u003e⋯\u003c/mo\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e≈\u003c/mo\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(1^2 + 2^2 + 3^2 + \\dots + n^2) \\approx O(n^3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cp\u003eThis \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(n^3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e cost means that as your text gets longer, the generation time grows \u003cstrong\u003eextremely quickly\u003c/strong\u003e. For example, generating a sequence twice as long takes roughly \u003cstrong\u003eeight times\u003c/strong\u003e as long! Clearly, we need a better approach.\u003c/p\u003e\n\u003chr/\u003e\n\u003ch2\u003eThe Solution: Key-Value (KV) Cache\u003c/h2\u003e\n\u003cp\u003eThe key insight behind KV caching is that we\u0026#39;re doing a lot of redundant work. When generating each new token, we\u0026#39;re recomputing things for all previous tokens that we\u0026#39;ve already processed before. Let\u0026#39;s see how we can fix this.\u003c/p\u003e\n\u003ch3\u003eWhat is a Key-Value Cache?\u003c/h3\u003e\n\u003cp\u003eThink of a KV cache like a smart notepad where we write down important information about each token the first time we see it. For each token, we compute and store two things:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eA \u003cstrong\u003ekey\u003c/strong\u003e (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ek\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e): Think of this as an addressing mechanism - it helps determine how relevant this token is to future tokens\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003evalue\u003c/strong\u003e (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ev\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ev\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e): Think of this as the actual information that gets used when this token is found to be relevant\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMathematically, we compute these as:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKey: \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003eK\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ek = x W_K\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e (where \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ex\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ex\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is the token and \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003eK\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eW_K\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is a learned transformation)\u003c/li\u003e\n\u003cli\u003eValue: \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003eV\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ev = x W_V\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e (where \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003eV\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eW_V\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is another learned transformation)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhen generating a new token, we use its query (computed similarly to keys) to find relevant information in our cache by comparing it with all stored keys. The matching values are then used to help generate the token.\u003c/p\u003e\n\u003ch3\u003eHow the KV Cache Makes Things Faster\u003c/h3\u003e\n\u003cp\u003eWith a KV cache, the process becomes much more efficient:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWhen we see a new token, we only need to compute its key and value \u003cstrong\u003eonce\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eFor all future tokens, we can just look up these pre-computed values from our cache\u003c/li\u003e\n\u003cli\u003eThis means each new token only needs to do a small amount of new work, instead of redoing all previous computations\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe trade-off is clear:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe use more memory to store all the keys and values. For a model with:\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e layers\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eH\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eH\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e attention heads\u003c/li\u003e\n\u003cli\u003eSequence length \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eKey/value dimension \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ed_k\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\nThe total memory cost is \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003eH\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL \\times H \\times n \\times d_k \\times 2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e values (the factor of 2 accounts for both keys and values).\nThis grows linearly with sequence length (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(n)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e), but the constant factors can be substantial for large models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBut in return, we reduce the computation cost from \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(n^3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e to \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(n^2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo understand why it\u0026#39;s \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(n^2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e, let\u0026#39;s look at the cost at each step:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eStep 1\u003c/strong\u003e: Process 1 token → cost \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(1)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStep 2\u003c/strong\u003e: Process 1 new token + look at 1 cached token → cost \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStep 3\u003c/strong\u003e: Process 1 new token + look at 2 cached tokens → cost \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e3\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eAnd so on...\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAdding these up:\u003c/p\u003e\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmn\u003e3\u003c/mn\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmo\u003e⋯\u003c/mo\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(1 + 2 + 3 + \\dots + n) = O(n^2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cp\u003eThis is a dramatic improvement over \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(n^3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e! While we still have to do the fundamental work of looking at all previous tokens (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(n^2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e), we avoid the costly recomputation at each step.\u003c/p\u003e\n\u003chr/\u003e\n\u003ch2\u003eThe Memory Challenge: Why We Need Better Solutions\u003c/h2\u003e\n\u003cp\u003eWhile KV cache is a powerful optimization, it comes with a significant memory cost. Let\u0026#39;s look at a concrete example using a modern large language model like Llama3 70B with:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e80\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL = 80\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e layers\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eH\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e64\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eH = 64\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e attention heads\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eB\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e8\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eB = 8\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e batch size of 8 sequences\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e128\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ed_k = 128\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e key/value dimension\u003c/li\u003e\n\u003cli\u003e16-bit precision\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe memory required for a batch of 8 sequences of 1000 tokens each would be:\u003c/p\u003e\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003eH\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003eB\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmtext\u003e bytes\u003c/mtext\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e80\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e64\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e8\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e1000\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e128\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmtext\u003e bytes\u003c/mtext\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e20.97\u003c/mn\u003e\u003cmtext\u003eGB\u003c/mtext\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL \\times H \\times B \\times n \\times d_k \\times 2 \\times 2 \\text{ bytes} = 80 \\times 64 \\times 8 \\times 1000 \\times 128 \\times 2 \\times 2 \\text{ bytes} = 20.97\\text{GB}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cp\u003eThis substantial memory usage creates several challenges:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eScales linearly\u003c/strong\u003e with sequence length\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMultiplies\u003c/strong\u003e with batch size for parallel processing\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLimits\u003c/strong\u003e the maximum context length we can handle\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConstrains\u003c/strong\u003e deployment on memory-limited devices\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThese challenges have sparked a wave of innovation in the research community, leading to various techniques for optimizing KV cache usage. Let\u0026#39;s explore these cutting-edge solutions.\u003c/p\u003e\n\u003ch2\u003eCan we improve over naive KV caches?\u003c/h2\u003e\n\u003cp\u003eThe following papers represent key innovations in KV cache optimization. We\u0026#39;ll explore them through three main approaches: token selection, post-hoc compression techniques, and architectural redesigns.\u003c/p\u003e\n\u003ch2\u003eToken Selection and Pruning Approaches\u003c/h2\u003e\n\u003ch3\u003e1) \u003ca href=\"https://arxiv.org/abs/2306.14048\"\u003eHeavy-Hitter Oracle (H2O)\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/h2o_alg.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003eH2O introduces the concept of identifying and preserving important tokens in the KV cache:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHeavy-Hitter Tokens\u003c/strong\u003e: H2O identifies tokens with the highest accumulated attention scores during generation, following a power-law distribution. These tokens are critical for model functionality and are prioritized in the cache.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDynamic Submodular Eviction\u003c/strong\u003e: The method frames cache management as an optimization problem with a submodular objective function \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eF\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eF(S)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e that quantifies the importance of a token set \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eS\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eF\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmunder\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e∈\u003c/mo\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/mrow\u003e\u003c/munder\u003e\u003cmsub\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eF(S) = \\sum_{i \\in S} A_{i}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nwhere \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eA_i\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is the accumulated attention score for token \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ei\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e. The cache \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eS_t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is updated by:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmtext\u003eargmax\u003c/mtext\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e⊆\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e∪\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e{\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e}\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmo\u003e≤\u003c/mo\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003eF\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eS_t = \\text{argmax}_{S \\subseteq S_{t-1} \\cup \\{i\\}, |S| \\leq k} \\, F(S)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nensuring that at most one token is evicted per step. This greedy algorithm is computationally efficient and guarantees near-optimal performance under submodular constraints.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eResults\u003c/strong\u003e: Achieves \u003cstrong\u003e5× reduction\u003c/strong\u003e in KV cache size with negligible accuracy loss and up to \u003cstrong\u003e29×\u003c/strong\u003e throughput improvement.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e2) \u003ca href=\"https://arxiv.org/abs/2309.17453\"\u003eStreamLLM\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/streamingLLM.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe authors observe the phenomenon of \u003cstrong\u003eAttention Sinks\u003c/strong\u003e: Initial tokens that act as natural attention anchors during decoding\n\u003cul\u003e\n\u003cli\u003eWithout these attention sink tokens, the performance of naive window attention drops\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBased on that observation, they introduce a \u003cstrong\u003eRolling Cache\u003c/strong\u003e for recent context with retained initial tokens, enabling infinite-length sequence processing.\u003c/li\u003e\n\u003cli\u003eThey show that these sink tokens can also be \u003cstrong\u003etrained\u003c/strong\u003e; serving as dedicated attention anchors, reducing reliance on multiple initial tokens.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3) \u003ca href=\"https://arxiv.org/abs/2406.12335\"\u003eValue-Aware Token Pruning (VATP)\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/vatp.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003eVATP extends H2O\u0026#39;s token importance concept by considering both attention patterns and value vector properties:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImportance Scoring\u003c/strong\u003e: Combines attention scores with value vector information:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsubsup\u003e\u003cmi\u003eI\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msubsup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsubsup\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msubsup\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmsubsup\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msubsup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmunder\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmo\u003e≤\u003c/mo\u003e\u003cmi\u003ej\u003c/mi\u003e\u003cmo\u003e≤\u003c/mo\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/mrow\u003e\u003c/munder\u003e\u003cmsub\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003ej\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eI_k^t = S_k^t \\cdot \\|v_k\\|_1, \\quad S_k^t = \\sum_{k \\leq j \\leq t} a_{j,k}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nwhere \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsubsup\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msubsup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eS_k^t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is the accumulated attention score and \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\|v_k\\|_1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is the value vector\u0026#39;s L1 norm.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eToken Pruning\u003c/strong\u003e: Tokens are ranked by \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsubsup\u003e\u003cmi\u003eI\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msubsup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eI_k^t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e, and those with the lowest scores are pruned, while \u003cstrong\u003eattention sink tokens\u003c/strong\u003e (e.g., start or newline tokens) are preserved to prevent performance degradation.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePerformance and Efficiency\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eOutperforms baselines like H2O and Scissorhands in 12–14 out of 16 LongBench tasks.\u003c/li\u003e\n\u003cli\u003eAchieves effective \u003cstrong\u003e50% compression\u003c/strong\u003e with minimal performance loss.\u003c/li\u003e\n\u003cli\u003eIntroduces negligible computational overhead and is compatible with FlashAttention when integrated with Scissorhands.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePost-hoc Compression Techniques\u003c/h2\u003e\n\u003cp\u003eThese methods compress or optimize the KV cache while preserving the standard transformer architecture.\u003c/p\u003e\n\u003ch3\u003e4) \u003ca href=\"https://arxiv.org/pdf/2310.01801\"\u003eAdaptive KV Compression (FastGen)\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/fastgen_1.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003eFastGen introduces adaptive compression based on attention patterns observed at run-time:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAttention Profiling\u003c/strong\u003e: during prompt encoding, FastGen identifies attention patterns and selects compression policies \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC^*\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e that minimize memory cost while preserving attention recovery:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003c/msup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003earg\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmunder\u003e\u003cmrow\u003e\u003cmi\u003emin\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmo\u003e∈\u003c/mo\u003e\u003cmi mathvariant=\"script\"\u003eC\u003c/mi\u003e\u003c/mrow\u003e\u003c/munder\u003e\u003cmtext\u003eCacheMemoryCost\u003c/mtext\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmtext\u003es.t.\u003c/mtext\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmtext\u003esoftmax\u003c/mtext\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003cmsubsup\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msubsup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmo\u003e≤\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC^* = \\arg\\min_{C \\in \\mathcal{C}} \\text{CacheMemoryCost}(C) \\quad \\text{s.t.} \\quad \\|A - \\text{softmax}(QK_C^T)\\| \\leq 1 - T.\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAdaptive Compression Policies\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eCompression strategies include:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpecial Tokens\u003c/strong\u003e (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmtext\u003especial\u003c/mtext\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC_{\\text{special}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e): Retain only special tokens.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLocality\u003c/strong\u003e (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmtext\u003elocal\u003c/mtext\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC_{\\text{local}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e): Evict tokens beyond a relative distance \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003er_l\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFrequency\u003c/strong\u003e (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmtext\u003efrequent\u003c/mtext\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC_{\\text{frequent}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e): Keep tokens with high cumulative attention scores (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003ef\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003er_f\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHybrid Policies\u003c/strong\u003e combine strategies, starting with \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmtext\u003especial\u003c/mtext\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC_{\\text{special}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e, and applies them adaptively to each head:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"script\"\u003eC\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e{\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmtext\u003especial\u003c/mtext\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmtext\u003especial\u003c/mtext\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmtext\u003epunct\u003c/mtext\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmo\u003e…\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmtext\u003efull\u003c/mtext\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e}\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathcal{C} = \\{C_{\\text{special}}, C_{\\text{special}} + C_{\\text{punct}}, \\ldots, C_{\\text{full}}\\}.\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e\u003cstrong\u003eToken Generation\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eDuring decoding, pre-selected compression policies manage the KV cache efficiently:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003ef\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK_{C_i}, V_{C_i} = f(K, V, C_i).\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e5) \u003ca href=\"https://arxiv.org/pdf/2403.09636\"\u003eDynamic Memory Compression (DMC)\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/dmc.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003eDMC introduces adaptive token merging:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDecision Mechanism\u003c/strong\u003e: At time \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003et\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e, predicts merge decisions \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha_t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e and weights \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eω\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\omega_t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e⌊\u003c/mo\u003e\u003cmtext\u003esigmoid\u003c/mtext\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e0\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e⌉\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmsub\u003e\u003cmi\u003eω\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmtext\u003esigmoid\u003c/mtext\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eq\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e0\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha_t = \\lfloor \\text{sigmoid}(k_t[0]) \\rceil, \\quad \\omega_t = \\text{sigmoid}(q_t[0]).\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWeighted Merging\u003c/strong\u003e: When \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha_t = 1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e, merges current and previous entries:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003e′\u003c/mo\u003e\u003c/msup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eω\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eω\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmsup\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003e′\u003c/mo\u003e\u003c/msup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eω\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eω\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ek\u0026#39; = \\frac{\\omega_t k_t + z_{t-1} k_{t-1}}{\\omega_t + z_{t-1}}, \\quad v\u0026#39; = \\frac{\\omega_t v_t + z_{t-1} v_{t-1}}{\\omega_t + z_{t-1}},\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nwhere \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eω\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ez_t = z_{t-1} + \\omega_t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e accumulates importance weights.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eUses a Gumbel-Sigmoid relaxation for \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha_t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e to allow end-to-end training with gradient descent:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e∼\u003c/mo\u003e\u003cmtext\u003eGumbel-Sigmoid\u003c/mtext\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e0\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003eτ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha_t \\sim \\text{Gumbel-Sigmoid}(k_t[0], \\tau),\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nwhere \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eτ\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\tau\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is a temperature parameter.\u003c/li\u003e\n\u003cli\u003eOptimizes a combined objective:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"script\"\u003eL\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi mathvariant=\"script\"\u003eL\u003c/mi\u003e\u003cmtext\u003eLM\u003c/mtext\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eλ\u003c/mi\u003e\u003cmi\u003emax\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmrow\u003e\u003cmo fence=\"true\"\u003e(\u003c/mo\u003e\u003cmn\u003e0\u003c/mn\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmfrac\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmtext\u003eCR\u003c/mtext\u003e\u003c/mfrac\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmunder\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/munder\u003e\u003cmsub\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo fence=\"true\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathcal{L} = \\mathcal{L}_{\\text{LM}} + \\lambda \\max\\left(0, \\frac{n}{\\text{CR}} - \\sum_{t} \\alpha_t \\right),\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nwhere \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi mathvariant=\"script\"\u003eL\u003c/mi\u003e\u003cmtext\u003eLM\u003c/mtext\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathcal{L}_{\\text{LM}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is the language modeling loss, and the second term encourages the model to match a target compression ratio (CR).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eResults\u003c/strong\u003e: Up to \u003cstrong\u003e8× compression\u003c/strong\u003e with maintained performance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e6) \u003ca href=\"https://arxiv.org/pdf/2406.11430\"\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL_2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e Norm-Based Compression\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/l2.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003eThis paper presents a surprising observation: A clear correlation between the \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL_2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e norm and the attention scores over cached KV pairs, where a low \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL_2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e norm of a key embedding usually leads to a high attention score during decoding. Consequently, they introduce a simple but effective compression objective:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNorm-Based Selection\u003c/strong\u003e: For a set of cached keys \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e{\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmo\u003e…\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e}\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK = \\{k_1, k_2, \\dots, k_n\\}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e, computes and sorts key norms:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsqrt\u003e\u003cmrow\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ej\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003ed\u003c/mi\u003e\u003c/munderover\u003e\u003cmsubsup\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003ej\u003c/mi\u003e\u003c/mrow\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msubsup\u003e\u003c/mrow\u003e\u003c/msqrt\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\|k_i\\|_2 = \\sqrt{\\sum_{j=1}^d k_{i,j}^2}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSorting and Selection\u003c/strong\u003e: To compress the KV cache, sort all keys by their L2 norm values:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmtext\u003esorted\u003c/mtext\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmtext\u003eSort\u003c/mtext\u003e\u003cmo fence=\"false\" stretchy=\"true\" minsize=\"1.2em\" maxsize=\"1.2em\"\u003e(\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e{\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmo\u003e…\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e}\u003c/mo\u003e\u003cmo fence=\"false\" stretchy=\"true\" minsize=\"1.2em\" maxsize=\"1.2em\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK_{\\text{sorted}} = \\text{Sort}\\big(\\{\\|k_1\\|_2, \\|k_2\\|_2, \\dots, \\|k_n\\|_2\\}\\big)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nRetain the top-\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e keys with lowest norms, where \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e⌊\u003c/mo\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e⌋\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em = \\lfloor c \\cdot n \\rfloor\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e and \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ec\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ec\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is the compression ratio.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCompressed Cache\u003c/strong\u003e: The compressed key-value cache consists of:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmtext\u003ecompressed\u003c/mtext\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e{\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e∣\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi mathvariant=\"normal\"\u003e∥\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003cmo\u003e∈\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmtext\u003esorted\u003c/mtext\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e}\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmsub\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmtext\u003ecompressed\u003c/mtext\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e{\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e∣\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e∈\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmtext\u003ecompressed\u003c/mtext\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e}\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK_{\\text{compressed}} = \\{k_i \\mid \\|k_i\\|_2 \\in K_{\\text{sorted}}[1:m]\\}, \\quad V_{\\text{compressed}} = \\{v_i \\mid k_i \\in K_{\\text{compressed}}\\}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003c/li\u003e\n\u003cli\u003eDue to its simplicity, this approach maintains compatibility with \u003cstrong\u003eFlashAttention\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eArchitectural Redesigns\u003c/h2\u003e\n\u003cp\u003eThese approaches change the Transformers architecture to handle KV caches more efficiently, often incorporating compression directly into the architecture.\u003c/p\u003e\n\u003ch3\u003e7) \u003ca href=\"https://arxiv.org/pdf/2305.13245\"\u003eMulti-Query Attention (MQA)\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/mqa.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eKey Idea\u003c/strong\u003e: MQA reduces the KV cache size by sharing a \u003cstrong\u003esingle key-value head\u003c/strong\u003e across all query heads, replacing the traditional Multi-Head Attention (MHA):\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003eK\u003c/mi\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003eV\u003c/mi\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK = XW_K, \\quad V = XW_V,\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nwhere \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eK\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK \u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e and \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eV\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eV \u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e are the shared key and value projections.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBenefits\u003c/strong\u003e: Reduces the KV cache size by a factor of \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eH\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eH \u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e (the number of attention heads), significantly lowering memory bandwidth overhead.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTrade-Off\u003c/strong\u003e: While MQA is faster, it often suffers from \u003cstrong\u003equality degradation\u003c/strong\u003e, especially in tasks requiring diverse attention patterns.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e8) \u003ca href=\"https://arxiv.org/abs/2305.13245\"\u003eGroup-Query Attention (GQA)\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/gqa.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eKey Idea\u003c/strong\u003e: GQA interpolates between full multi-head attention and MQA to offering a scalable trade-off between inference speed and model quality. It divides query heads into \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eG\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eG \u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e groups, where each group shares a single key-value head:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmtext\u003egroup\u003c/mtext\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmi\u003eG\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003cmunder\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmo\u003e∈\u003c/mo\u003e\u003cmi\u003eG\u003c/mi\u003e\u003c/mrow\u003e\u003c/munder\u003e\u003cmsub\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmsub\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmtext\u003egroup\u003c/mtext\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmi\u003eG\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003cmunder\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmo\u003e∈\u003c/mo\u003e\u003cmi\u003eG\u003c/mi\u003e\u003c/mrow\u003e\u003c/munder\u003e\u003cmsub\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK_{\\text{group}} = \\frac{1}{|G|} \\sum_{h \\in G} K_h, \\quad V_{\\text{group}} = \\frac{1}{|G|} \\sum_{h \\in G} V_h\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGQA-1\u003c/strong\u003e: Equivalent to MQA (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eG\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eG = 1 \u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGQA-\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eH\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eH \u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/strong\u003e: Equivalent to MHA (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eG\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eH\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eG = H \u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUptraining\u003c/strong\u003e: GQA can be introduced to existing pre-trained models through fine-tuning:\n\u003cul\u003e\n\u003cli\u003eFirst, convert MHA checkpoints to GQA by \u003cstrong\u003emean pooling\u003c/strong\u003e key and value heads into groups\u003c/li\u003e\n\u003cli\u003eThen fine-tune (\u0026#34;uptrain\u0026#34;) the model briefly to adapt to the new attention pattern\u003c/li\u003e\n\u003cli\u003eThis adaptation process requires only \u003cstrong\u003e5% of the original pre-training compute\u003c/strong\u003e, making it very efficient\u003c/li\u003e\n\u003cli\u003eThe resulting model maintains quality while gaining GQA\u0026#39;s memory benefits\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e9) \u003ca href=\"https://arxiv.org/abs/2405.04434\"\u003eMulti-head Latent Attention (MLA)\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/mla.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003eDeepSeek\u0026#39;s \u003cstrong\u003eMulti-Head Latent Attention (MLA)\u003c/strong\u003e takes a novel approach to reducing KV cache overhead. While MQA and GQA achieve this through head-sharing, MLA instead employs a \u003cstrong\u003elow-rank latent compression\u003c/strong\u003e technique that maintains the benefits of multiple attention heads.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMLA reduces KV cache size by compressing keys and values into low-dimensional latent vectors before reconstruction.\u003c/li\u003e\n\u003cli\u003eIt down-project key-value embeddings into a compressed latent space:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmrow\u003e\u003cmtext\u003eKV\u003c/mtext\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmtext\u003eDKV\u003c/mtext\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003eC\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmtext\u003eUK\u003c/mtext\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmrow\u003e\u003cmtext\u003eKV\u003c/mtext\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003eC\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmtext\u003eUV\u003c/mtext\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmrow\u003e\u003cmtext\u003eKV\u003c/mtext\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ec_{\\text{KV}, t} = W_{\\text{DKV}} h_t, \\quad k_C = W_{\\text{UK}} c_{\\text{KV}, t}, \\quad v_C = W_{\\text{UV}} c_{\\text{KV}, t}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nwhere \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmtext\u003eDKV\u003c/mtext\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eW_{\\text{DKV}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is the down-projection matrix, and \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmtext\u003eUK\u003c/mtext\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eW_{\\text{UK}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e, \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmtext\u003eUV\u003c/mtext\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eW_{\\text{UV}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e are up-projection matrices for keys and values.\u003c/li\u003e\n\u003cli\u003eIt retains per-head flexibility through compressed representations, unlike MQA\u0026#39;s complete head sharing.\u003c/li\u003e\n\u003cli\u003eIt introduces \u003cstrong\u003eRotary Positional Embeddings (RoPE)\u003c/strong\u003e for decoupling position-aware keys:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003eR\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmtext\u003eRoPE\u003c/mtext\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmi\u003eR\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmsub\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003eC\u003c/mi\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e;\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003eR\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ek_R = \\text{RoPE}(W_{KR} h_t), \\quad k_t = [k_C; k_R]\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nThis reduces KV cache storage further by caching only compressed latent vectors \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmtext\u003eKV\u003c/mtext\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ec_{\\text{KV}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e and positional keys \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003eR\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ek_R\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e10) \u003ca href=\"https://arxiv.org/pdf/2404.14469\"\u003eSnapKV\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/snapKV.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSnapKV introduces an \u003cstrong\u003eObservation Window\u003c/strong\u003e: Uses end-of-prompt tokens to identify attention patterns:\n\u003cspan\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/mrow\u003e\u003cmsub\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmtext\u003eobs\u003c/mtext\u003e\u003c/msub\u003e\u003c/munderover\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmtext\u003eobs\u003c/mtext\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi\u003eI\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmtext\u003eTop\u003c/mtext\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC = \\sum_{i=0}^{L_{\\text{obs}}} W_{\\text{obs}}[:, i, :], \\quad I = \\text{Top}_k(C, k)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nwhere \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmtext\u003eobs\u003c/mtext\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eW_{\\text{obs}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e represents the attention weights, and \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ek\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is determined by the compression rate.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCompression\u003c/strong\u003e: Clusters features around the selected positions using a pooling layer to preserve context completeness.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e11) \u003ca href=\"https://arxiv.org/pdf/2405.05254\"\u003eYou Only Cache Once (YOCO)\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://www.pyspur.dev/blog/kv-cache/yoco_2.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003eYOCO modifies the transformer architecture for caching:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal Cache\u003c/strong\u003e: Uses a decoder-decoder design with a single shared KV cache.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eComplexity Reduction\u003c/strong\u003e: Reduces memory from \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eN\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(N \\times L)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e to \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eN\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(N + L)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e, where \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eN\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eN\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is sequence length and \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is the number of layers.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEfficient Attention\u003c/strong\u003e: The self-decoder employs \u003cstrong\u003esliding-window attention\u003c/strong\u003e or \u003cstrong\u003egated retention\u003c/strong\u003e, enabling constant memory usage (\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(C)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e, where \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eC\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e is a small window size).\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr/\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eKey-Value caching techniques are central to scaling and optimizing Transformer-based models for real-world use. Innovations like dynamic eviction, compression, and structured approximations continue to push the boundaries on what is possible in long-context or resource-constrained scenarios. KV caching remains a lively research area, offering both theoretical insights and practical improvements.\u003c/p\u003e\n\u003cp\u003ePS: This blog post is mostly AI-generated using a PySpur workflow with minor human edits.\u003c/p\u003e\u003c/article\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e",
  "readingTime": "18 min read",
  "publishedTime": "2025-01-21T00:00:00Z",
  "modifiedTime": null
}
