{
  "id": "bf9d2ad2-03bb-4af8-b080-3b21feb97ea0",
  "title": "Alibaba launches open source Qwen3 model that surpasses OpenAI o1 and DeepSeek R1",
  "link": "https://venturebeat.com/ai/alibaba-launches-open-source-qwen3-model-that-surpasses-openai-o1-and-deepseek-r1/",
  "description": "Qwen3’s open-weight release under an accessible license marks an important milestone, lowering barriers for developers and organizations.",
  "author": "Carl Franzen",
  "published": "Mon, 28 Apr 2025 23:56:06 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "Business",
    "AI, ML and Deep Learning",
    "alibaba",
    "alibaba qwen",
    "China",
    "Conversational AI",
    "large language models (LLMs)",
    "LLMs",
    "NLP",
    "Qwen",
    "qwen3"
  ],
  "byline": "Carl Franzen",
  "length": 7994,
  "excerpt": "Qwen3’s open-weight release under an accessible license marks an important milestone, lowering barriers for developers and organizations.",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "April 28, 2025 4:56 PM Credit: VentureBeat made with Qwen Chat Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Chinese e-commerce and web giant Alibaba’s Qwen team has officially launched a new series of open source AI large language multimodal models known as Qwen3 that appear to be among the state-of-the-art for open models, and approach performance of proprietary models from the likes of OpenAI and Google. The Qwen3 series features two “mixture-of-experts” models and six dense models for a total of eight (!) new models. The “mixture-of-experts” approach involves having several different specialty model types combined into one, with only those relevant models to the task at hand being activated when needed in the internal settings of the model (known as parameters). It was popularized by open source French AI startup Mistral. According to the team, the 235-billion parameter version of Qwen3 codenamed A22B outperforms DeepSeek’s open source R1 and OpenAI’s proprietary o1 on key third-party benchmarks including ArenaHard (with 500 user questions in software engineering and math) and nears the performance of the new, proprietary Google Gemini 2.5-Pro. Overall, the benchmark data positions Qwen3-235B-A22B as one of the most powerful publicly available models, achieving parity or superiority relative to major industry offerings. Hybrid (reasoning) theory The Qwen3 models are trained to provide so-called “hybrid reasoning” or “dynamic reasoning” capabilities, allowing users to toggle between fast, accurate responses and more time-consuming and compute-intensive reasoning steps (similar to OpenAI’s “o” series) for more difficult queries in science, math, engineering and other specialized fields. This is an approach pioneered by Nous Research and other AI startups and research collectives. With Qwen3, users can engage the more intensive “Thinking Mode” using the button marked as such on the Qwen Chat website or by embedding specific prompts like /think or /no_think when deploying the model locally or through the API, allowing for flexible use depending on the task complexity. Users can now access and deploy these models across platforms like Hugging Face, ModelScope, Kaggle, and GitHub, as well as interact with them directly via the Qwen Chat web interface and mobile applications. The release includes both Mixture of Experts (MoE) and dense models, all available under the Apache 2.0 open-source license. In my brief usage of the Qwen Chat website so far, it was able to generate imagery relatively rapidly and with decent prompt adherence — especially when incorporating text into the image natively while matching the style. However, it often prompted me to log in and was subject to the usual Chinese content restrictions (such as prohibiting prompts or responses related to the Tiananmen Square protests). In addition to the MoE offerings, Qwen3 includes dense models at different scales: Qwen3-32B, Qwen3-14B, Qwen3-8B, Qwen3-4B, Qwen3-1.7B, and Qwen3-0.6B. These models vary in size and architecture, offering users options to fit diverse needs and computational budgets. The Qwen3 models also significantly expand multilingual support, now covering 119 languages and dialects across major language families. This broadens the models’ potential applications globally, facilitating research and deployment in a wide range of linguistic contexts. Model training and architecture In terms of model training, Qwen3 represents a substantial step up from its predecessor, Qwen2.5. The pretraining dataset doubled in size to approximately 36 trillion tokens. The data sources include web crawls, PDF-like document extractions, and synthetic content generated using previous Qwen models focused on math and coding. The training pipeline consisted of a three-stage pretraining process followed by a four-stage post-training refinement to enable the hybrid thinking and non-thinking capabilities. The training improvements allow the dense base models of Qwen3 to match or exceed the performance of much larger Qwen2.5 models. Deployment options are versatile. Users can integrate Qwen3 models using frameworks such as SGLang and vLLM, both of which offer OpenAI-compatible endpoints. For local usage, options like Ollama, LMStudio, MLX, llama.cpp, and KTransformers are recommended. Additionally, users interested in the models’ agentic capabilities are encouraged to explore the Qwen-Agent toolkit, which simplifies tool-calling operations. Junyang Lin, a member of the Qwen team, commented on X that building Qwen3 involved addressing critical but less glamorous technical challenges such as scaling reinforcement learning stably, balancing multi-domain data, and expanding multilingual performance without quality sacrifice. Lin also indicated that the team is transitioning focus toward training agents capable of long-horizon reasoning for real-world tasks. What it means for enterprise decision-makers Engineering teams can point existing OpenAI-compatible endpoints to the new model in hours instead of weeks. The MoE checkpoints (235 B parameters with 22 B active, and 30 B with 3 B active) deliver GPT-4-class reasoning at roughly the GPU memory cost of a 20–30 B dense model. Official LoRA and QLoRA hooks allow private fine-tuning without sending proprietary data to a third-party vendor. Dense variants from 0.6 B to 32 B make it easy to prototype on laptops and scale to multi-GPU clusters without rewriting prompts. Running the weights on-premises means all prompts and outputs can be logged and inspected. MoE sparsity reduces the number of active parameters per call, cutting the inference attack surface. The Apache-2.0 license removes usage-based legal hurdles, though organizations should still review export-control and governance implications of using a model trained by a China-based vendor. Yet at the same time, it also offers a viable alternative to other Chinese players including DeepSeek, Tencent, and ByteDance — as well as the myriad and growing number of North American models such as the aforementioned OpenAI, Google, Microsoft, Anthropic, Amazon, Meta and others. The permissive Apache 2.0 license — which allows for unlimited commercial usage — is also a big advantage over other open source players like Meta, whose licenses are more restrictive. It indicates furthermore that the race between AI providers to offer ever-more powerful and accessible models continues to remain highly competitive, and savvy organizations looking to cut costs should attempt to remain flexible and open to evaluating said new models for their AI agents and workflows. Looking ahead The Qwen team positions Qwen3 not just as an incremental improvement but as a significant step toward future goals in Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), AI significantly smarter than humans. Plans for Qwen’s next phase include scaling data and model size further, extending context lengths, broadening modality support, and enhancing reinforcement learning with environmental feedback mechanisms. As the landscape of large-scale AI research continues to evolve, Qwen3’s open-weight release under an accessible license marks another important milestone, lowering barriers for researchers, developers, and organizations aiming to innovate with state-of-the-art LLMs. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/04/029ebd88-b24c-4396-80ff-2a47b3477cc6.png?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-04-28T23:56:06+00:00\" datetime=\"2025-04-28T23:56:06+00:00\"\u003eApril 28, 2025 4:56 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"422\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/029ebd88-b24c-4396-80ff-2a47b3477cc6.png?w=750\" alt=\"Numerous small humanoid robots assemble mechanical components in a crowded factory below a large Q3 sign on the wall\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eCredit: VentureBeat made with Qwen Chat\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eChinese e-commerce and web giant \u003ca href=\"https://qwenlm.github.io/blog/qwen3/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAlibaba’s Qwen team has officially launched\u003c/a\u003e a new series of open source AI large language multimodal models known as Qwen3 that appear to be among the state-of-the-art for open models, and approach performance of proprietary models from the likes of OpenAI and Google. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe Qwen3 series features two “mixture-of-experts” models and six dense models for a total of eight (!) new models. The “mixture-of-experts” approach involves having several different specialty model types combined into one, with only those relevant models to the task at hand being activated when needed in the internal settings of the model (known as parameters). It was \u003ca href=\"https://venturebeat.com/ai/mistral-ai-drops-new-mixture-of-experts-model-with-a-torrent-link/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epopularized by open source French AI startup Mistral\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to the team, the 235-billion parameter version of Qwen3 codenamed A22B outperforms DeepSeek’s open source R1 and OpenAI’s proprietary o1 on key third-party benchmarks including ArenaHard (with 500 user questions in software engineering and math) and nears the performance of the new, proprietary \u003ca href=\"https://venturebeat.com/ai/googles-gemini-2-5-pro-is-the-smartest-model-youre-not-using-and-4-reasons-it-matters-for-enterprise-ai/\"\u003eGoogle Gemini 2.5-Pro\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" width=\"3413\" height=\"1920\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg 3413w, https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg?resize=300,169 300w, https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg?resize=768,432 768w, https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg?resize=800,450 800w, https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg?resize=1536,864 1536w, https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg?resize=2048,1152 2048w, https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg?resize=400,225 400w, https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg?resize=750,422 750w, https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg?resize=578,325 578w, https://venturebeat.com/wp-content/uploads/2025/04/Gppj9_kbEAAkO9U.jpg?resize=930,523 930w\" sizes=\"(max-width: 3413px) 100vw, 3413px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eOverall, the benchmark data positions Qwen3-235B-A22B as one of the most powerful publicly available models, achieving parity or superiority relative to major industry offerings.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-hybrid-reasoning-theory\"\u003eHybrid (reasoning) theory\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Qwen3 models are trained to provide so-called “hybrid reasoning” or “dynamic reasoning” capabilities, allowing users to toggle between fast, accurate responses and more time-consuming and compute-intensive reasoning steps (similar to OpenAI’s “o” series) for more difficult queries in science, math, engineering and other specialized fields. This is \u003ca href=\"https://venturebeat.com/ai/personalized-unrestricted-ai-lab-nous-research-launches-first-toggle-on-reasoning-model-deephermes-3/\"\u003ean approach pioneered by Nous Research\u003c/a\u003e and other AI startups and research collectives. \u003c/p\u003e\n\n\n\n\u003cp\u003eWith Qwen3, users can engage the more intensive “Thinking Mode” using the button marked as such on the Qwen Chat website or by embedding specific prompts like \u003ccode\u003e/think\u003c/code\u003e or \u003ccode\u003e/no_think\u003c/code\u003e when deploying the model locally or through the API, allowing for flexible use depending on the task complexity.\u003c/p\u003e\n\n\n\n\u003cp\u003eUsers can now access and deploy these models across platforms like Hugging Face, ModelScope, Kaggle, and GitHub, as well as interact with them directly via the \u003ca href=\"https://chat.qwen.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eQwen Chat web interface\u003c/a\u003e and mobile applications. The release includes both Mixture of Experts (MoE) and dense models, all available under the Apache 2.0 open-source license. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn my brief usage of the Qwen Chat website so far, it was able to generate imagery relatively rapidly and with decent prompt adherence — especially when incorporating text into the image natively while matching the style. However, it often prompted me to log in and was subject to the usual Chinese content restrictions (such as prohibiting prompts or responses related to the Tiananmen Square protests).\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"2302\" height=\"770\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44%E2%80%AFPM.png?w=800\" alt=\"\" srcset=\"https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44 PM.png 2302w, https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44 PM.png?resize=300,100 300w, https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44 PM.png?resize=768,257 768w, https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44 PM.png?resize=800,268 800w, https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44 PM.png?resize=1536,514 1536w, https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44 PM.png?resize=2048,685 2048w, https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44 PM.png?resize=400,134 400w, https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44 PM.png?resize=750,251 750w, https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44 PM.png?resize=578,193 578w, https://venturebeat.com/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-6.31.44 PM.png?resize=930,311 930w\" sizes=\"(max-width: 2302px) 100vw, 2302px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIn addition to the MoE offerings, Qwen3 includes dense models at different scales: Qwen3-32B, Qwen3-14B, Qwen3-8B, Qwen3-4B, Qwen3-1.7B, and Qwen3-0.6B. \u003c/p\u003e\n\n\n\n\u003cp\u003eThese models vary in size and architecture, offering users options to fit diverse needs and computational budgets.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe Qwen3 models also significantly expand multilingual support, now covering 119 languages and dialects across major language families. This broadens the models’ potential applications globally, facilitating research and deployment in a wide range of linguistic contexts.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-model-training-and-architecture\"\u003eModel training and architecture\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn terms of model training, Qwen3 represents a substantial step up from its predecessor, Qwen2.5. The pretraining dataset doubled in size to approximately 36 trillion tokens. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe data sources include web crawls, PDF-like document extractions, and synthetic content generated using previous Qwen models focused on math and coding.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe training pipeline consisted of a three-stage pretraining process followed by a four-stage post-training refinement to enable the hybrid thinking and non-thinking capabilities. The training improvements allow the dense base models of Qwen3 to match or exceed the performance of much larger Qwen2.5 models.\u003c/p\u003e\n\n\n\n\u003cp\u003eDeployment options are versatile. Users can integrate Qwen3 models using frameworks such as SGLang and vLLM, both of which offer OpenAI-compatible endpoints. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor local usage, options like Ollama, LMStudio, MLX, llama.cpp, and KTransformers are recommended. Additionally, users interested in the models’ agentic capabilities are encouraged to explore the Qwen-Agent toolkit, which simplifies tool-calling operations.\u003c/p\u003e\n\n\n\n\u003cp\u003eJunyang Lin, a member of the Qwen team, \u003ca href=\"https://x.com/JustinLin610/status/1916965026977747313\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecommented on X\u003c/a\u003e that building Qwen3 involved addressing critical but less glamorous technical challenges such as scaling reinforcement learning stably, balancing multi-domain data, and expanding multilingual performance without quality sacrifice. \u003c/p\u003e\n\n\n\n\u003cp\u003eLin also indicated that the team is transitioning focus toward training agents capable of long-horizon reasoning for real-world tasks.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-what-it-means-for-enterprise-decision-makers\"\u003eWhat it means for enterprise decision-makers\u003c/h2\u003e\n\n\n\n\u003cp\u003eEngineering teams can point existing OpenAI-compatible endpoints to the new model in hours instead of weeks. The MoE checkpoints (235 B parameters with 22 B active, and 30 B with 3 B active) deliver GPT-4-class reasoning at roughly the GPU memory cost of a 20–30 B dense model. \u003c/p\u003e\n\n\n\n\u003cp\u003eOfficial LoRA and QLoRA hooks allow private fine-tuning without sending proprietary data to a third-party vendor.\u003c/p\u003e\n\n\n\n\u003cp\u003eDense variants from 0.6 B to 32 B make it easy to prototype on laptops and scale to multi-GPU clusters without rewriting prompts.\u003c/p\u003e\n\n\n\n\u003cp\u003eRunning the weights on-premises means all prompts and outputs can be logged and inspected. MoE sparsity reduces the number of active parameters per call, cutting the inference attack surface. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe Apache-2.0 license removes usage-based legal hurdles, though organizations should still review export-control and governance implications of using a model trained by a China-based vendor.\u003c/p\u003e\n\n\n\n\u003cp\u003eYet at the same time, it also offers a viable alternative to other Chinese players including DeepSeek, Tencent, and ByteDance — as well as the myriad and growing number of North American models such as the aforementioned OpenAI, Google, Microsoft, Anthropic, Amazon, Meta and others. The permissive Apache 2.0 license — which allows for unlimited commercial usage — is also a big advantage over other open source players like Meta, whose licenses are more restrictive. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt indicates furthermore that the race between AI providers to offer ever-more powerful and accessible models continues to remain highly competitive, and savvy organizations looking to cut costs should attempt to remain flexible and open to evaluating said new models for their AI agents and workflows.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-looking-ahead\"\u003eLooking ahead\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Qwen team positions Qwen3 not just as an incremental improvement but as a significant step toward future goals in Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), AI significantly smarter than humans. \u003c/p\u003e\n\n\n\n\u003cp\u003ePlans for Qwen’s next phase include scaling data and model size further, extending context lengths, broadening modality support, and enhancing reinforcement learning with environmental feedback mechanisms.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs the landscape of large-scale AI research continues to evolve, Qwen3’s open-weight release under an accessible license marks another important milestone, lowering barriers for researchers, developers, and organizations aiming to innovate with state-of-the-art LLMs.\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2025-04-28T23:56:06Z",
  "modifiedTime": "2025-04-28T23:56:20Z"
}
