{
  "id": "6ceed041-6011-4df6-b418-a482b4d616e4",
  "title": "Bigger isn’t always better: Examining the business case for multi-million token LLMs",
  "link": "https://venturebeat.com/ai/bigger-isnt-always-better-examining-the-business-case-for-multi-million-token-llms/",
  "description": "Are we unlocking new frontiers in AI reasoning, or simply stretching the limits of token memory without meaningful improvements?",
  "author": "Rahul Raja, LinkedIn",
  "published": "Sat, 12 Apr 2025 19:30:00 +0000",
  "source": "https://feeds.feedburner.com/venturebeat/SZYF",
  "categories": [
    "AI",
    "DataDecisionMakers",
    "AI, ML and Deep Learning",
    "category-/Computers \u0026 Electronics",
    "Generative AI",
    "large language models",
    "NLP",
    "Retrieval-augmented generation (RAG)"
  ],
  "byline": "Rahul Raja, LinkedIn",
  "length": 10051,
  "excerpt": "Are we unlocking new frontiers in AI reasoning, or simply stretching the limits of token memory without meaningful improvements?",
  "siteName": "VentureBeat",
  "favicon": "",
  "text": "April 12, 2025 12:30 PM Author generated with Dall-E Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More The race to expand large language models (LLMs) beyond the million-token threshold has ignited a fierce debate in the AI community. Models like MiniMax-Text-01 boast 4-million-token capacity, and Gemini 1.5 Pro can process up to 2 million tokens simultaneously. They now promise game-changing applications and can analyze entire codebases, legal contracts or research papers in a single inference call. At the core of this discussion is context length — the amount of text an AI model can process and also remember at once. A longer context window allows a machine learning (ML) model to handle much more information in a single request and reduces the need for chunking documents into sub-documents or splitting conversations. For context, a model with a 4-million-token capacity could digest 10,000 pages of books in one go. In theory, this should mean better comprehension and more sophisticated reasoning. But do these massive context windows translate to real-world business value? As enterprises weigh the costs of scaling infrastructure against potential gains in productivity and accuracy, the question remains: Are we unlocking new frontiers in AI reasoning, or simply stretching the limits of token memory without meaningful improvements? This article examines the technical and economic trade-offs, benchmarking challenges and evolving enterprise workflows shaping the future of large-context LLMs. The rise of large context window models: Hype or real value? Why AI companies are racing to expand context lengths AI leaders like OpenAI, Google DeepMind and MiniMax are in an arms race to expand context length, which equates to the amount of text an AI model can process in one go. The promise? deeper comprehension, fewer hallucinations and more seamless interactions. For enterprises, this means AI that can analyze entire contracts, debug large codebases or summarize lengthy reports without breaking context. The hope is that eliminating workarounds like chunking or retrieval-augmented generation (RAG) could make AI workflows smoother and more efficient. Solving the ‘needle-in-a-haystack’ problem The needle-in-a-haystack problem refers to AI’s difficulty identifying critical information (needle) hidden within massive datasets (haystack). LLMs often miss key details, leading to inefficiencies in: Search and knowledge retrieval: AI assistants struggle to extract the most relevant facts from vast document repositories. Legal and compliance: Lawyers need to track clause dependencies across lengthy contracts. Enterprise analytics: Financial analysts risk missing crucial insights buried in reports. Larger context windows help models retain more information and potentially reduce hallucinations. They help in improving accuracy and also enable: Cross-document compliance checks: A single 256K-token prompt can analyze an entire policy manual against new legislation. Medical literature synthesis: Researchers use 128K+ token windows to compare drug trial results across decades of studies. Software development: Debugging improves when AI can scan millions of lines of code without losing dependencies. Financial research: Analysts can analyze full earnings reports and market data in one query. Customer support: Chatbots with longer memory deliver more context-aware interactions. Increasing the context window also helps the model better reference relevant details and reduces the likelihood of generating incorrect or fabricated information. A 2024 Stanford study found that 128K-token models reduced hallucination rates by 18% compared to RAG systems when analyzing merger agreements. However, early adopters have reported some challenges: JPMorgan Chase’s research demonstrates how models perform poorly on approximately 75% of their context, with performance on complex financial tasks collapsing to near-zero beyond 32K tokens. Models still broadly struggle with long-range recall, often prioritizing recent data over deeper insights. This raises questions: Does a 4-million-token window truly enhance reasoning, or is it just a costly expansion of memory? How much of this vast input does the model actually use? And do the benefits outweigh the rising computational costs? Cost vs. performance: RAG vs. large prompts: Which option wins? The economic trade-offs of using RAG RAG combines the power of LLMs with a retrieval system to fetch relevant information from an external database or document store. This allows the model to generate responses based on both pre-existing knowledge and dynamically retrieved data. As companies adopt AI for complex tasks, they face a key decision: Use massive prompts with large context windows, or rely on RAG to fetch relevant information dynamically. Large prompts: Models with large token windows process everything in a single pass and reduce the need for maintaining external retrieval systems and capturing cross-document insights. However, this approach is computationally expensive, with higher inference costs and memory requirements. RAG: Instead of processing the entire document at once, RAG retrieves only the most relevant portions before generating a response. This reduces token usage and costs, making it more scalable for real-world applications. Comparing AI inference costs: Multi-step retrieval vs. large single prompts While large prompts simplify workflows, they require more GPU power and memory, making them costly at scale. RAG-based approaches, despite requiring multiple retrieval steps, often reduce overall token consumption, leading to lower inference costs without sacrificing accuracy. For most enterprises, the best approach depends on the use case: Need deep analysis of documents? Large context models may work better. Need scalable, cost-efficient AI for dynamic queries? RAG is likely the smarter choice. A large context window is valuable when: The full text must be analyzed at once (ex: contract reviews, code audits). Minimizing retrieval errors is critical (ex: regulatory compliance). Latency is less of a concern than accuracy (ex: strategic research). Per Google research, stock prediction models using 128K-token windows analyzing 10 years of earnings transcripts outperformed RAG by 29%. On the other hand, GitHub Copilot’s internal testing showed that 2.3x faster task completion versus RAG for monorepo migrations. Breaking down the diminishing returns The limits of large context models: Latency, costs and usability While large context models offer impressive capabilities, there are limits to how much extra context is truly beneficial. As context windows expand, three key factors come into play: Latency: The more tokens a model processes, the slower the inference. Larger context windows can lead to significant delays, especially when real-time responses are needed. Costs: With every additional token processed, computational costs rise. Scaling up infrastructure to handle these larger models can become prohibitively expensive, especially for enterprises with high-volume workloads. Usability: As context grows, the model’s ability to effectively “focus” on the most relevant information diminishes. This can lead to inefficient processing where less relevant data impacts the model’s performance, resulting in diminishing returns for both accuracy and efficiency. Google’s Infini-attention technique seeks to offset these trade-offs by storing compressed representations of arbitrary-length context with bounded memory. However, compression leads to information loss, and models struggle to balance immediate and historical information. This leads to performance degradations and cost increases compared to traditional RAG. The context window arms race needs direction While 4M-token models are impressive, enterprises should use them as specialized tools rather than universal solutions. The future lies in hybrid systems that adaptively choose between RAG and large prompts. Enterprises should choose between large context models and RAG based on reasoning complexity, cost and latency. Large context windows are ideal for tasks requiring deep understanding, while RAG is more cost-effective and efficient for simpler, factual tasks. Enterprises should set clear cost limits, like $0.50 per task, as large models can become expensive. Additionally, large prompts are better suited for offline tasks, whereas RAG systems excel in real-time applications requiring fast responses. Emerging innovations like GraphRAG can further enhance these adaptive systems by integrating knowledge graphs with traditional vector retrieval methods that better capture complex relationships, improving nuanced reasoning and answer precision by up to 35% compared to vector-only approaches. Recent implementations by companies like Lettria have demonstrated dramatic improvements in accuracy from 50% with traditional RAG to more than 80% using GraphRAG within hybrid retrieval systems. As Yuri Kuratov warns: “Expanding context without improving reasoning is like building wider highways for cars that can’t steer.” The future of AI lies in models that truly understand relationships across any context size. Rahul Raja is a staff software engineer at LinkedIn. Advitya Gemawat is a machine learning (ML) engineer at Microsoft. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.",
  "image": "https://venturebeat.com/wp-content/uploads/2025/04/image1.jpg?w=1024?w=1200\u0026strip=all",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\n\t\t\u003csection\u003e\n\t\t\t\n\t\t\t\u003cp\u003e\u003ctime title=\"2025-04-12T19:30:00+00:00\" datetime=\"2025-04-12T19:30:00+00:00\"\u003eApril 12, 2025 12:30 PM\u003c/time\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/section\u003e\n\t\t\u003cdiv\u003e\n\t\t\t\t\t\u003cp\u003e\u003cimg width=\"750\" height=\"429\" src=\"https://venturebeat.com/wp-content/uploads/2025/04/image1.jpg?w=750\" alt=\"Author generated with Dall-E\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eAuthor generated with Dall-E\u003c/span\u003e\u003c/p\u003e\t\t\u003c/div\u003e\n\t\u003c/div\u003e\u003cdiv id=\"primary\"\u003e\n\n\t\t\u003carticle id=\"content\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cdiv id=\"boilerplate_2682874\"\u003e\n\u003cp\u003e\u003cem\u003eJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. \u003ca href=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite\u0026amp;utm_medium=desktopNav\"\u003eLearn More\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003chr/\u003e\n\u003c/div\u003e\u003cp\u003eThe race to expand large language models (LLMs) beyond the million-token threshold has ignited a fierce debate in the AI community. Models like \u003ca href=\"https://arxiv.org/abs/2501.08313\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMiniMax-Text-01\u003c/a\u003e boast 4-million-token capacity, and \u003ca href=\"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGemini 1.5 Pro\u003c/a\u003e can process up to 2 million tokens simultaneously. They now promise game-changing applications and can analyze entire codebases, legal contracts or research papers in a single inference call.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt the core of this discussion is context length — the amount of text an AI model can process and also \u003cem\u003eremember\u003c/em\u003e at once. A longer context window allows a \u003ca href=\"https://venturebeat.com/ai/mips-to-exaflops-in-just-40-years-compute-power-is-exploding-and-it-will-transform-ai/\"\u003emachine learning (ML) model\u003c/a\u003e to handle much more information in a single request and reduces the need for chunking documents into sub-documents or splitting conversations. For context, a model with a 4-million-token capacity could digest 10,000 pages of books in one go.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn theory, this should mean better comprehension and more sophisticated reasoning. But do these massive context windows translate to real-world business value?\u003c/p\u003e\n\n\n\n\u003cp\u003eAs enterprises weigh the costs of scaling infrastructure against potential gains in productivity and accuracy, the question remains: Are we unlocking new frontiers in AI reasoning, or simply stretching the limits of token memory without meaningful improvements? This article examines the technical and economic trade-offs, benchmarking challenges and evolving enterprise workflows shaping the future of \u003ca href=\"https://venturebeat.com/ai/from-ai-agent-hype-to-practicality-why-enterprises-must-consider-fit-over-flash/\"\u003elarge-context LLMs\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-rise-of-large-context-window-models-hype-or-real-value\"\u003eThe rise of large context window models: Hype or real value?\u003c/h2\u003e\n\n\n\n\u003ch3 id=\"h-why-ai-companies-are-racing-to-expand-context-lengths\"\u003eWhy AI companies are racing to expand context lengths\u003c/h3\u003e\n\n\n\n\u003cp\u003eAI leaders like OpenAI, Google DeepMind and MiniMax are in an arms race to expand context length, which equates to the amount of text an AI model can process in one go. The promise? deeper comprehension, fewer hallucinations and more seamless interactions.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor enterprises, this means AI that can analyze entire contracts, debug large codebases or summarize lengthy reports without breaking context. The hope is that eliminating workarounds like chunking or retrieval-augmented generation (RAG) could make AI workflows smoother and more efficient.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"h-solving-the-needle-in-a-haystack-problem\"\u003eSolving the ‘needle-in-a-haystack’ problem\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe needle-in-a-haystack problem refers to AI’s difficulty identifying critical information (needle) hidden within massive datasets (haystack). LLMs often miss key details, leading to inefficiencies in:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eSearch and knowledge retrieval: AI assistants struggle to extract the most relevant facts from vast document repositories.\u003c/li\u003e\n\n\n\n\u003cli\u003eLegal and compliance: Lawyers need to track clause dependencies across lengthy contracts.\u003c/li\u003e\n\n\n\n\u003cli\u003eEnterprise analytics: Financial analysts risk missing crucial insights buried in reports.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eLarger context windows help models retain more information and potentially reduce hallucinations. They help in improving accuracy and also enable:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eCross-document compliance checks: \u003ca href=\"https://www.ai21.com/blog/7-long-context-trends-in-the-enterprise\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eA single 256K-token prompt\u003c/a\u003e can analyze an entire policy manual against new legislation.\u003c/li\u003e\n\n\n\n\u003cli\u003eMedical literature synthesis: Researchers \u003ca href=\"https://www.ai21.com/blog/7-long-context-trends-in-the-enterprise\" target=\"_blank\" rel=\"noreferrer noopener\"\u003euse 128K+ token\u003c/a\u003e windows to compare drug trial results across decades of studies.\u003c/li\u003e\n\n\n\n\u003cli\u003eSoftware development: Debugging improves when AI can scan millions of lines of code without losing dependencies.\u003c/li\u003e\n\n\n\n\u003cli\u003eFinancial research: Analysts can analyze full earnings reports and market data in one query.\u003c/li\u003e\n\n\n\n\u003cli\u003eCustomer support: Chatbots with longer memory deliver more context-aware interactions.\u003cbr/\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eIncreasing the context window also helps the model better reference relevant details and reduces the likelihood of generating incorrect or fabricated information. \u003ca href=\"https://arxiv.org/html/2501.12372v3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eA 2024 Stanford study\u003c/a\u003e found that 128K-token models reduced hallucination rates by 18% compared to RAG systems when analyzing merger agreements.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, early adopters have reported some challenges: \u003ca href=\"https://aclanthology.org/2024.emnlp-industry.88.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eJPMorgan Chase’s research\u003c/a\u003e demonstrates how models perform poorly on approximately 75% of their context, with performance on complex financial tasks collapsing to near-zero beyond 32K tokens. Models still broadly struggle with long-range recall, often prioritizing recent data over deeper insights.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis raises questions: Does a 4-million-token window truly enhance reasoning, or is it just a costly expansion of memory? How much of this vast input does the model actually use? And do the benefits outweigh the rising computational costs?\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-cost-vs-performance-rag-vs-large-prompts-which-option-wins\"\u003eCost vs. performance: RAG vs. large prompts: Which option wins?\u003c/h2\u003e\n\n\n\n\u003ch3 id=\"h-the-economic-trade-offs-of-using-rag\"\u003eThe economic trade-offs of using RAG\u003c/h3\u003e\n\n\n\n\u003cp\u003eRAG combines the power of LLMs with a retrieval system to fetch relevant information from an external database or document store. This allows the model to generate responses based on both pre-existing knowledge and dynamically retrieved data.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs companies adopt \u003ca href=\"https://venturebeat.com/ai/deepseek-jolts-ai-industry-why-ais-next-leap-may-not-come-from-more-data-but-more-compute-at-inference/\"\u003eAI for complex tasks\u003c/a\u003e, they face a key decision: Use massive prompts with large context windows, or rely on RAG to fetch relevant information dynamically.\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eLarge prompts: Models with large token windows process everything in a single pass and reduce the need for maintaining external retrieval systems and capturing cross-document insights. However, this approach is computationally expensive, with higher inference costs and memory requirements.\u003c/li\u003e\n\n\n\n\u003cli\u003eRAG: Instead of processing the entire document at once, RAG retrieves only the most relevant portions before generating a response. This reduces token usage and costs, making it more scalable for real-world applications.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch3 id=\"h-comparing-ai-inference-costs-multi-step-retrieval-vs-large-single-prompts\"\u003eComparing AI inference costs: Multi-step retrieval vs. large single prompts\u003c/h3\u003e\n\n\n\n\u003cp\u003eWhile large prompts simplify workflows, they require more GPU power and memory, making them costly at scale. RAG-based approaches, despite requiring multiple retrieval steps, often reduce overall token consumption, leading to lower inference costs without sacrificing accuracy.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor most enterprises, the best approach depends on the use case:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eNeed deep analysis of documents? Large context models may work better.\u003c/li\u003e\n\n\n\n\u003cli\u003eNeed scalable, cost-efficient AI for dynamic queries? RAG is likely the smarter choice.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eA large context window is valuable when:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eThe full text must be analyzed at once (ex: contract reviews, code audits).\u003c/li\u003e\n\n\n\n\u003cli\u003eMinimizing retrieval errors is critical (ex: regulatory compliance).\u003c/li\u003e\n\n\n\n\u003cli\u003eLatency is less of a concern than accuracy (ex: strategic research).\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003ePer Google research, stock prediction models using 128K-token windows analyzing 10 years of earnings transcripts \u003ca href=\"https://arxiv.org/html/2501.12372v3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eoutperformed RAG\u003c/a\u003e by 29%. On the other hand, GitHub Copilot’s internal testing showed that \u003ca href=\"https://codingscape.com/blog/llms-with-largest-context-windows\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e2.3x faster task\u003c/a\u003e completion versus RAG for monorepo migrations.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-breaking-down-the-diminishing-returns\"\u003eBreaking down the diminishing returns\u003c/h2\u003e\n\n\n\n\u003ch3 id=\"h-the-limits-of-large-context-models-latency-costs-and-usability\"\u003eThe limits of large context models: Latency, costs and usability\u003c/h3\u003e\n\n\n\n\u003cp\u003eWhile large context models offer impressive capabilities, there are limits to how much extra context is truly beneficial. As context windows expand, three key factors come into play:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eLatency: The more tokens a model processes, the slower the inference. Larger context windows can lead to significant delays, especially when real-time responses are needed.\u003c/li\u003e\n\n\n\n\u003cli\u003eCosts: With every additional token processed, computational costs rise. Scaling up infrastructure to handle these larger models can become prohibitively expensive, especially for enterprises with high-volume workloads.\u003c/li\u003e\n\n\n\n\u003cli\u003eUsability: As context grows, the model’s ability to effectively “focus” on the most relevant information diminishes. This can lead to inefficient processing where less relevant data impacts the model’s performance, resulting in diminishing returns for both accuracy and efficiency.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eGoogle’s \u003ca href=\"https://huggingface.co/blog/infini-attention\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eInfini-attention technique\u003c/a\u003e seeks to offset these trade-offs by storing compressed representations of arbitrary-length context with bounded memory. However, compression leads to information loss, and models struggle to balance immediate and historical information. This leads to  performance degradations and cost increases compared to traditional RAG.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-context-window-arms-race-needs-direction\"\u003eThe context window arms race needs direction\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile 4M-token models are impressive, enterprises should use them as specialized tools rather than universal solutions. The future lies in hybrid systems that adaptively choose between RAG and large prompts.\u003c/p\u003e\n\n\n\n\u003cp\u003eEnterprises should choose between large context models and RAG based on reasoning complexity, cost and latency. Large context windows are ideal for tasks requiring deep understanding, while RAG is more cost-effective and efficient for simpler, factual tasks. Enterprises should set clear cost limits, like $0.50 per task, as large models can become expensive. Additionally, large prompts are better suited for offline tasks, whereas RAG systems excel in real-time applications requiring fast responses.\u003c/p\u003e\n\n\n\n\u003cp\u003eEmerging innovations like \u003ca href=\"https://aws.amazon.com/blogs/machine-learning/improving-retrieval-augmented-generation-accuracy-with-graphrag/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGraphRAG\u003c/a\u003e can further enhance these adaptive systems by integrating knowledge graphs with traditional vector retrieval methods that better capture complex relationships, improving nuanced reasoning and answer precision by up to 35% compared to vector-only approaches. Recent implementations by companies like Lettria have demonstrated dramatic improvements in accuracy from 50% with traditional RAG to more than 80% using GraphRAG within hybrid retrieval systems.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs \u003ca href=\"https://openreview.net/forum?id=u7m2CG84BQ#discussion\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eYuri Kuratov warns\u003c/a\u003e: “\u003cem\u003eExpanding context without improving reasoning is like building wider highways for cars that can’t steer.\u003c/em\u003e” The future of AI lies in models that truly understand relationships across any context size.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eRahul Raja is a staff software engineer at LinkedIn.\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eAdvitya Gemawat is a machine learning (ML) engineer at Microsoft.\u003c/em\u003e\u003c/p\u003e\n\u003cdiv id=\"boilerplate_2660155\"\u003e\n\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cp\u003e\u003cstrong\u003eDaily insights on business use cases with VB Daily\u003c/strong\u003e\u003c/p\u003e\n\t\t\t\t\u003cp\u003eIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\u003c/p\u003e\n\t\t\t\t\n\t\t\t\t\u003cp\u003eRead our \u003ca href=\"https://venturebeat.com/terms-of-service/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\u003cp id=\"boilerplateNewsletterConfirmation\"\u003e\n\t\t\t\t\tThanks for subscribing. Check out more \u003ca href=\"https://venturebeat.com/newsletters/\"\u003eVB newsletters here\u003c/a\u003e.\n\t\t\t\t\u003c/p\u003e\n\t\t\t\t\u003cp\u003eAn error occured.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003cp\u003e\u003cimg src=\"https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png\" alt=\"\"/\u003e\n\t\t\t\t\u003c/p\u003e\n\t\t\t\n\t\t\u003c/div\u003e\t\t\t\u003c/div\u003e\n\n\t\t\t\t\t\t\t\n\t\t\t\n\t\t\u003c/article\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-04-12T19:30:00Z",
  "modifiedTime": "2025-04-12T19:32:04Z"
}
