{
  "id": "d9084371-acb2-4680-b2b3-dddc8854efaf",
  "title": "You need much less memory than time",
  "link": "https://blog.computationalcomplexity.org/2025/02/you-need-much-less-memory-than-time.html",
  "description": "Article URL: https://blog.computationalcomplexity.org/2025/02/you-need-much-less-memory-than-time.html Comments URL: https://news.ycombinator.com/item?id=44212855 Points: 55 # Comments: 6",
  "author": "jonbaer",
  "published": "Sat, 07 Jun 2025 21:38:48 +0000",
  "source": "https://hnrss.org/frontpage",
  "categories": null,
  "byline": "",
  "length": 3626,
  "excerpt": "Just as I was complaining that we haven't seen many surprising breakthroughs in complexity recently, we get an earthquake of a result to st...",
  "siteName": "",
  "favicon": "",
  "text": "Just as I was complaining that we haven't seen many surprising breakthroughs in complexity recently, we get an earthquake of a result to start the year, showing that all algorithms can be simulated using considerable less memory than the time of the original algorithm. You can reuse space (memory) but you can't reuse time, and this new result from Ryan Williams in an upcoming STOC paper provides the first stark difference.DTIME(\\(t(n)\\)) \\(\\subseteq\\) DSPACE(\\(\\sqrt{t(n)\\log t(n)}\\))This is a vast improvement on the previous best known simulation, the classic 1977 Hopcroft-Paul-Valiant paper showingDTIME(\\(t(n)\\)) \\(\\subseteq\\) DSPACE(\\(t(n)/\\log t(n)\\))only slightly lower than the trivial \\(t(n)\\) bound. Williams gets a huge near quadratic improvement that will go down as a true classic complexity theorem. Note that the space simulation does not maintain the time bound.Williams' proof relies on a space-efficient tree evaluation algorithm by James Cook and Ian Mertz from last year's STOC conference. Cook and Mertz's algorithm builds on earlier work on catalytic computing, highlighted in a recent Quanta article. Let me give an highly overly simplified view of the combined proof.A \\(t(n)\\) time Turing machine uses at most that much space on its tapes. Split the tapes into \\(\\sqrt{t(n)}\\) segments of size \\(\\sqrt{t(n)}\\). Using the fact that it takes \\(\\sqrt{t(n)}\\) time to cross an entire segment, Williams with some clever tricks models acceptance of the Turing machines as a circuit of bounded degree and depth \\(\\sqrt{t(n)}\\), where the wires carry the contents of the size \\(\\sqrt{t(n)}\\) segments at various times in the computation. Williams then applies the tree evaluation algorithm of Cook and Mertz. Cook and Mertz use finite fields to encode these segments as a combination of registers of size \\(\\log t(n)\\) and show how to compute the value of each node of the tree using only \\(\\sqrt{t(n)}\\) space for the local computation plus needing to only remember a constant number of registers while reusing the rest of the space when recursively computing the tree. It's pretty magical how they manage to make it all work. It's worth going through the proof yourself. I recommend Sections 3.1 and Footnote 6 in Williams' paper (a slightly weaker space bound but much simpler) and Sections 2-4 of the Cook-Mertz paper. Oded Goldreich has an alternative exposition of the Cook-Mertz algorithm and proof.Williams' theorem works for multitape Turing machines and oblivious random-access machines, where the queries to the memory are fixed in advance. He shows how to use this result to compute the output a circuit of size \\(s\\) using nearly \\(\\sqrt{s}\\) space. Fully general random access machines remains open, as does nondeterministic and other models of computation (random, quantum, etc).In 1986 my advisor Mike Sipser gave the first hardness vs randomness result, showing roughly that if there were problems that took time \\(2^n\\) but could not be solved in space \\(2^{.99n}\\) on multi-tape Turing machines then RP = P. Williams' theorem kills this assumption though we've developed weaker assumptions since. Moving forward, can we push Williams' result to get a simulation in space \\(n^\\epsilon\\) for \\(\\epsilon\u003c1/2\\). A simulation for all \\(\\epsilon\u003e0\\) would separate P from PSPACE. Even a slight improvement would have applications for alternating time. Maybe try to use the Cook-Mertz techniques directly in the Turing machine simulation instead of going through computation trees.Read sections 4 and 5 of Williams' paper for some further consequences and challenges for further improvements.",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"post-body-3222385872113044921\" itemprop=\"description articleBody\"\u003e\n\u003cp\u003eJust as I was \u003ca href=\"https://blog.computationalcomplexity.org/2025/02/research-then-and-now.html\"\u003ecomplaining\u003c/a\u003e that we haven\u0026#39;t seen many surprising breakthroughs in complexity recently, we get an earthquake of a result to start the year, showing that all algorithms can be simulated using considerable less memory than the time of the original algorithm. You can reuse space (memory) but you can\u0026#39;t reuse time, and this new result from Ryan Williams in an upcoming \u003ca href=\"https://eccc.weizmann.ac.il/report/2025/017/\"\u003eSTOC paper\u003c/a\u003e provides the first stark difference.\u003c/p\u003e\u003cp\u003eDTIME(\\(t(n)\\)) \\(\\subseteq\\) DSPACE(\\(\\sqrt{t(n)\\log t(n)}\\))\u003c/p\u003e\u003cp\u003eThis is a vast improvement on the previous best known simulation, the classic 1977 \u003ca href=\"https://doi.org/10.1145/322003.322015\"\u003eHopcroft-Paul-Valiant paper\u003c/a\u003e showing\u003c/p\u003e\u003cp\u003eDTIME(\\(t(n)\\)) \\(\\subseteq\\) DSPACE(\\(t(n)/\\log t(n)\\))\u003c/p\u003e\u003cp\u003eonly slightly lower than the trivial \\(t(n)\\) bound. Williams gets a huge near quadratic improvement that will go down as a true classic complexity theorem. Note that the space simulation does not maintain the time bound.\u003c/p\u003e\u003cp\u003eWilliams\u0026#39; proof relies on a \u003ca href=\"https://doi.org/10.1145/3618260.3649664\"\u003espace-efficient tree evaluation algorithm\u003c/a\u003e by James Cook and Ian Mertz from last year\u0026#39;s STOC conference. Cook and Mertz\u0026#39;s algorithm builds on earlier work on catalytic computing, highlighted in a \u003ca href=\"https://www.quantamagazine.org/catalytic-computing-taps-the-full-power-of-a-full-hard-drive-20250218/\"\u003erecent Quanta article\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eLet me give an highly overly simplified view of the combined proof.\u003c/p\u003e\u003cp\u003eA \\(t(n)\\) time Turing machine uses at most that much space on its tapes. Split the tapes into \\(\\sqrt{t(n)}\\) segments of size \\(\\sqrt{t(n)}\\). Using the fact that it takes \\(\\sqrt{t(n)}\\) time to cross an entire segment, Williams with some clever tricks models acceptance of the Turing machines as a circuit of bounded degree and depth\u003cb\u003e \u003c/b\u003e\\(\\sqrt{t(n)}\\), where the wires carry the contents of the size \\(\\sqrt{t(n)}\\) segments at various times in the computation. \u003c/p\u003e\u003cp\u003eWilliams then applies the tree evaluation algorithm of Cook and Mertz. Cook and Mertz use finite fields to encode these segments as a combination of registers of size \\(\\log t(n)\\) and show how to compute the value of each node of the tree using only \\(\\sqrt{t(n)}\\) space for the local computation plus needing to only remember a constant number of registers while reusing the rest of the space when recursively computing the tree. It\u0026#39;s pretty magical how they manage to make it all work. \u003c/p\u003e\u003cp\u003eIt\u0026#39;s worth going through the proof yourself. I recommend Sections 3.1 and Footnote 6 in Williams\u0026#39; paper (a slightly weaker space bound but much simpler) and Sections 2-4 of the Cook-Mertz paper. Oded Goldreich has an \u003ca href=\"https://www.wisdom.weizmann.ac.il/~oded/VO/tree-eval.pdf\"\u003ealternative exposition\u003c/a\u003e of the Cook-Mertz algorithm and proof.\u003c/p\u003e\u003cp\u003eWilliams\u0026#39; theorem works for multitape Turing machines and oblivious random-access machines, where the queries to the memory are fixed in advance. He shows how to use this result to compute the output a circuit of size \\(s\\) using nearly \\(\\sqrt{s}\\) space. Fully general random access machines remains open, as does nondeterministic and other models of computation (random, quantum, etc).\u003c/p\u003e\u003cp\u003eIn 1986 my advisor Mike Sipser gave the \u003ca href=\"https://doi.org/10.1016/0022-0000(88)90035-9\"\u003efirst hardness vs randomness result\u003c/a\u003e, showing roughly that if there were problems that took time \\(2^n\\) but could not be solved in space \\(2^{.99n}\\) on multi-tape Turing machines then RP = P. Williams\u0026#39; theorem kills this assumption though we\u0026#39;ve developed \u003ca href=\"https://blog.computationalcomplexity.org/2006/03/uniform-derandomization-assumptions.html\"\u003eweaker assumptions\u003c/a\u003e since. \u003c/p\u003e\u003cp\u003eMoving forward, can we push Williams\u0026#39; result to get a simulation in space \\(n^\\epsilon\\) for \\(\\epsilon\u0026lt;1/2\\). A simulation for all \\(\\epsilon\u0026gt;0\\) would separate P from PSPACE. Even a slight improvement would have applications for alternating time. Maybe try to use the Cook-Mertz techniques directly in the Turing machine simulation instead of going through computation trees.\u003c/p\u003e\u003cp\u003eRead sections 4 and 5 of Williams\u0026#39; paper for some further consequences and challenges for further improvements. \u003c/p\u003e\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": null,
  "modifiedTime": null
}
