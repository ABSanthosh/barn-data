{
  "id": "ee5c49c4-a296-4473-a0f5-5d5266074b4e",
  "title": "Llamafile v0.8.14: a new UI, performance gains, and more",
  "link": "https://hacks.mozilla.org/2024/10/llamafile-v0-8-14-a-new-ui-performance-gains-and-more/",
  "description": "Discover the latest release of Llamafile 0.8.14, an open-source AI tool by Mozilla Builders. With a new command-line chat interface, enhanced performance, and support for powerful models, Llamafile makes it easy to run large language models (LLMs) on your own hardware. Learn more about the updates and how to get involved with this cutting-edge project. The post Llamafile v0.8.14: a new UI, performance gains, and more appeared first on Mozilla Hacks - the Web developer blog.",
  "author": "Stephen Hood",
  "published": "Wed, 16 Oct 2024 13:32:30 +0000",
  "source": "https://hacks.mozilla.org/feed/",
  "categories": [
    "Featured Article",
    "llamafile",
    "open source"
  ],
  "byline": "By Stephen Hood",
  "length": 4762,
  "excerpt": "Introducing Llamafile 0.8.14, Mozilla's open-source AI tool with a new chat interface, faster performance and support for powerful models.",
  "siteName": "Mozilla Hacks – the Web developer blog",
  "favicon": "",
  "text": "We’ve just released Llamafile 0.8.14, the latest version of our popular open source AI tool. A Mozilla Builders project, Llamafile turns model weights into fast, convenient executables that run on most computers, making it easy for anyone to get the most out of open LLMs using the hardware they already have. New chat interface The key feature of this new release is our colorful new command line chat interface. When you launch a Llamafile we now automatically open this new chat UI for you, right there in the terminal. This new interface is fast, easy to use, and an all around simpler experience than the Web-based interface we previously launched by default. (That interface, which our project inherits from the upstream llama.cpp project, is still available and supports a range of features, including image uploads. Simply point your browser at port 8080 on localhost). Other recent improvements This new chat UI is just the tip of the iceberg. In the months since our last blog post here, lead developer Justine Tunney has been busy shipping a slew of new releases, each of which have moved the project forward in important ways. Here are just a few of the highlights: Llamafiler: We’re building our own clean sheet OpenAI-compatible API server, called Llamafiler. This new server will be more reliable, stable, and most of all faster than the one it replaces. We’ve already shipped the embeddings endpoint, which runs three times as fast as the one in llama.cpp. Justine is currently working on the completions endpoint, at which point Llamafiler will become the default API server for Llamafile. Performance improvements: With the help of open source contributors like k-quant inventor @Kawrakow Llamafile has enjoyed a series of dramatic speed boosts over the last few months. In particular, pre-fill (prompt evaluation) speed has improved dramatically on a variety of architectures: Intel Core i9 went from 100 tokens/second to 400 (4x). AMD Threadripper went from 300 tokens/second to 2,400 (8x). Even the modest Raspberry Pi 5 jumped from 8 tokens/second to 80 (10x!). When combined with the new high-speed embedding server described above, Llamafile has become one of the fastest ways to run complex local AI applications that use methods like retrieval augmented generation (RAG). Support for powerful new models: Llamafile continues to keep pace with progress in open LLMs, adding support for dozens of new models and architectures, ranging in size from 405 billion parameters all the way down to 1 billion. Here are just a few of the new Llamafiles available for download on Hugging Face: Llama 3.2 1B and 3B: offering extremely impressive performance and quality for their small size. (Here’s a video from our own Mike Heavers showing it in action.) Llama 3.1 405B: a true “frontier model” that’s possible to run at home with sufficient system RAM. OLMo 7B: from our friends at the Allen Institute, OLMo is one of the first truly open and transparent models available. TriLM: a new “1.58 bit” tiny model that is optimized for CPU inference and points to a near future where matrix multiplication might no longer rule the day. Whisperfile, speech-to-text in a single file: Thanks to contributions from community member @cjpais, we’ve created Whisperfile, which does for whisper.cpp what Llamafile did for llama.cpp: that is, turns it into a multi-platform executable that runs nearly everywhere. Whisperfile thus makes it easy to use OpenAI’s Whisper technology to efficiently convert speech into text, no matter which kind of hardware you have. Get involved Our goal is for Llamafile to become a rock-solid foundation for building sophisticated locally-running AI applications. Justine’s work on the new Llamafiler server is a big part of that equation, but so is the ongoing work of supporting new models and optimizing inference performance for as many users as possible. We’re proud and grateful that some of the project’s biggest breakthroughs in these areas, and others, have come from the community, with contributors like @Kawrakow, @cjpais, @mofosyne, and @Djip007 routinely leaving their mark. We invite you to join them, and us. We welcome issues and PRs in our GitHub repo. And we welcome you to become a member of Mozilla’s AI Discord server, which has a dedicated channel just for Llamafile where you can get direct access to the project team. Hope to see you there! Stephen leads open source AI projects (including llamafile) in Mozilla Builders. He previously managed social bookmarking pioneer del.icio.us; co-founded Storium, Blockboard, and FairSpin; and worked on Yahoo Search and BEA WebLogic. More articles by Stephen Hood…",
  "image": "https://hacks.mozilla.org/wp-content/uploads/2024/10/llamafile_8.1.14_release_image.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle role=\"article\"\u003e\n    \u003cp\u003e\u003cspan\u003eWe’ve just released\u003c/span\u003e \u003ca href=\"https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8.14\"\u003e\u003cb\u003eLlamafile 0.8.14\u003c/b\u003e\u003c/a\u003e\u003cspan\u003e, the latest version of our popular open source AI tool. A \u003c/span\u003e\u003ca href=\"https://future.mozilla.org/builders/\"\u003e\u003cspan\u003eMozilla Builders project\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, Llamafile turns model weights into fast, convenient executables that run on most computers, making it easy for anyone to get the most out of open LLMs using the hardware they already have.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eNew chat interface\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe key feature of this new release is \u003c/span\u003e\u003cb\u003eour colorful new command line chat interface\u003c/b\u003e\u003cspan\u003e. When you launch a Llamafile we now automatically open this new chat UI for you, right there in the terminal. This new interface is fast, easy to use, and an all around simpler experience than the Web-based interface we previously launched by default. (That interface, which our project inherits from the upstream llama.cpp project, is still available and supports a range of features, including image uploads. \u003c/span\u003e\u003cspan\u003eSimply point your browser at port 8080 on localhost).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" src=\"https://hacks.mozilla.org/wp-content/uploads/2024/10/llamafile_8.1.14_release_image.png\" alt=\"llamafile\" width=\"1007\" height=\"790\" srcset=\"https://hacks.mozilla.org/wp-content/uploads/2024/10/llamafile_8.1.14_release_image.png 1007w, https://hacks.mozilla.org/wp-content/uploads/2024/10/llamafile_8.1.14_release_image-250x196.png 250w, https://hacks.mozilla.org/wp-content/uploads/2024/10/llamafile_8.1.14_release_image-500x392.png 500w, https://hacks.mozilla.org/wp-content/uploads/2024/10/llamafile_8.1.14_release_image-768x603.png 768w\" sizes=\"(max-width: 1007px) 100vw, 1007px\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eOther recent improvements\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThis new chat UI is just the tip of the iceberg. In the months since our last blog post here, lead developer \u003c/span\u003e\u003ca href=\"https://justine.lol/\"\u003e\u003cspan\u003eJustine Tunney\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e has been busy shipping a slew of new releases, each of which have moved the project forward in important ways. Here are just a few of the highlights:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eLlamafiler\u003c/b\u003e\u003cspan\u003e: We’re building our own clean sheet OpenAI-compatible API server, called \u003c/span\u003e\u003ci\u003e\u003cspan\u003eLlamafiler\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e. This new server will be more reliable, stable, and most of all \u003c/span\u003e\u003ci\u003e\u003cspan\u003efaster\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e than the one it replaces. We’ve already shipped the embeddings endpoint, which runs \u003c/span\u003e\u003ci\u003e\u003cspan\u003ethree times as fast\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e as the one in llama.cpp. Justine is currently working on the completions endpoint, at which point Llamafiler will become the default API server for Llamafile.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003ePerformance improvements\u003c/b\u003e\u003cspan\u003e: With the help of open source contributors like k-quant inventor \u003c/span\u003e\u003ca href=\"https://github.com/Kawrakow\"\u003e\u003cspan\u003e@Kawrakow\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e Llamafile has enjoyed a series of dramatic speed boosts over the last few months. In particular, pre-fill (prompt evaluation) speed has improved dramatically on a variety of architectures:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eIntel Core i9 went from 100 tokens/second to 400 (4x).\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAMD Threadripper went from 300 tokens/second to 2,400 (8x).\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eEven the modest Raspberry Pi 5 jumped from 8 tokens/second to 80 (10x!).\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eWhen combined with the new high-speed embedding server described above, Llamafile has become one of the fastest ways to run complex local AI applications that use methods like retrieval augmented generation (RAG).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eSupport for powerful new models\u003c/b\u003e\u003cspan\u003e: Llamafile continues to keep pace with progress in open LLMs, adding support for dozens of new models and architectures, ranging in size from 405 billion parameters all the way down to 1 billion. Here are just a few of the new Llamafiles \u003c/span\u003e\u003ca href=\"https://huggingface.co/Mozilla\"\u003e\u003cspan\u003eavailable for download on Hugging Face\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eLlama 3.2 \u003c/span\u003e\u003ca href=\"https://huggingface.co/Mozilla/Llama-3.2-1B-Instruct-llamafile\"\u003e\u003cspan\u003e1B\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://huggingface.co/Mozilla/Llama-3.2-3B-Instruct-llamafile\"\u003e\u003cspan\u003e3B\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e: offering extremely impressive performance and quality for their small size. (Here’s \u003c/span\u003e\u003ca href=\"https://www.youtube.com/watch?v=Lqh7egmfy4o\"\u003e\u003cspan\u003ea video\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e from our own Mike Heavers showing it in action.)\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eLlama 3.1 \u003c/span\u003e\u003ca href=\"https://huggingface.co/Mozilla/Meta-Llama-3.1-405B-llamafile\"\u003e\u003cspan\u003e405B\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e: a true “frontier model” that’s possible to run \u003c/span\u003e\u003ci\u003e\u003cspan\u003eat home\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e with sufficient system RAM.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003ca href=\"https://huggingface.co/Mozilla/OLMo-7B-0424-llamafile\"\u003e\u003cspan\u003eOLMo 7B\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e: from our friends at the \u003c/span\u003e\u003ca href=\"https://alleninstitute.org/\"\u003e\u003cspan\u003eAllen Institute\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, OLMo is one of the first truly open and transparent models available.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003ca href=\"https://huggingface.co/Mozilla/TriLM-llamafile\"\u003e\u003cspan\u003eTriLM\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e: a new “1.58 bit” tiny model that is optimized for CPU inference and points to a near future where matrix multiplication might no longer rule the day.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cb\u003eWhisperfile, speech-to-text in a single file\u003c/b\u003e\u003cspan\u003e: Thanks to contributions from community member \u003c/span\u003e\u003ca href=\"https://github.com/cjpais\"\u003e\u003cspan\u003e@cjpais\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, we’ve created \u003c/span\u003e\u003ca href=\"https://huggingface.co/Mozilla/whisperfile\"\u003e\u003cspan\u003eWhisperfile\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, which does for whisper.cpp what Llamafile did for llama.cpp: that is, turns it into a multi-platform executable that runs nearly everywhere. Whisperfile thus makes it easy to use OpenAI’s Whisper technology to efficiently convert speech into text, no matter which kind of hardware you have.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eGet involved\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eOur goal is for Llamafile to become a rock-solid foundation for building sophisticated locally-running AI applications. Justine’s work on the new Llamafiler server is a big part of that equation, but so is the ongoing work of supporting new models and optimizing inference performance for as many users as possible. We’re proud and grateful that some of the project’s biggest breakthroughs in these areas, and others, have come from the community, with contributors like \u003c/span\u003e\u003ca href=\"https://github.com/Kawrakow\"\u003e\u003cspan\u003e@Kawrakow\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, \u003c/span\u003e\u003ca href=\"https://github.com/cjpais\"\u003e\u003cspan\u003e@cjpais\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, \u003c/span\u003e\u003ca href=\"https://github.com/mofosyne\"\u003e\u003cspan\u003e@mofosyne\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, and \u003c/span\u003e\u003ca href=\"https://github.com/djip007\"\u003e\u003cspan\u003e@Djip007\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e routinely leaving their mark.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe invite you to join them, and us. We welcome issues and PRs in \u003c/span\u003e\u003ca href=\"https://github.com/Mozilla-Ocho/llamafile\"\u003e\u003cspan\u003eour GitHub repo\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. And we welcome you to become a member of Mozilla’s AI Discord server, which has \u003c/span\u003e\u003ca href=\"https://discord.gg/gbR6vJH9gu\"\u003e\u003cspan\u003ea dedicated channel just for Llamafile\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e where you can get direct access to the project team. Hope to see you there!\u003c/span\u003e\u003c/p\u003e\n\n    \u003csection\u003e\n                                \n                      \u003cp\u003eStephen leads open source AI projects (including llamafile) in Mozilla Builders. He previously managed social bookmarking pioneer del.icio.us; co-founded Storium, Blockboard, and FairSpin; and worked on Yahoo Search and BEA WebLogic.\u003c/p\u003e\n                                \u003cp\u003e\u003ca href=\"https://hacks.mozilla.org/author/slangtonhoodmozilla-com/\"\u003eMore articles by Stephen Hood…\u003c/a\u003e\u003c/p\u003e\n                  \u003c/section\u003e\n  \u003c/article\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
