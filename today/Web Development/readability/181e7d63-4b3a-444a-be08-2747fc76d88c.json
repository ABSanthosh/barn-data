{
  "id": "181e7d63-4b3a-444a-be08-2747fc76d88c",
  "title": "Watch our AI talks at I/O 2025",
  "link": "https://developer.chrome.com/blog/ai-io25?hl=en",
  "description": "At Google I/O 2025, we shared what we've been working on, the future of AI on the web, and demonstrated how our partners are making use of client-side AI.",
  "author": "",
  "published": "Thu, 22 May 2025 07:00:00 GMT",
  "source": "https://developer.chrome.com/static/blog/feed.xml",
  "categories": null,
  "byline": "X",
  "length": 9144,
  "excerpt": "At Google I/O 2025, we shared what we've been working on, the future of AI on the web, and demonstrated how our partners are making use of client-side AI.",
  "siteName": "Chrome for Developers",
  "favicon": "https://www.gstatic.com/devrel-devsite/prod/v6dc4611c4232bd02b2b914c4948f523846f90835f230654af18f87f75fe9f73c/chrome/images/favicon.png",
  "text": "Skip to main content Watch our AI talks at I/O 2025 Stay organized with collections Save and categorize content based on your preferences. Published: May 22, 2025 AI is transforming how web developers are building websites and web applications. At Google I/O 2025, we shared what we've been working on over the last year, demonstrated how our partners are making use of AI on the web, and announced new built-in AI APIs. Did you miss the event? Good news, you can now watch the talks on-demand! Practical built-in AI with Gemini Nano in Chrome Our core mission is to make Chrome and the web smarter for all developers and all users. In this talk, Thomas Steiner shares updates to built-in AI, practical use cases, and a look at our future. Built-in AI runs client-side models in the browser, which has several advantages: Private: Sensitive user data remains on the device, never needing to leave the browser. Offline: Applications can access AI capabilities, even without an internet connection. Performant: Thanks to hardware acceleration, these APIs deliver excellent performance. Take a look at code samples for each of the built-in AI APIs, get an update on their status, and see what companies are implementing this technology. Multimodal APIs We are working on brand new multimodal APIs. This means you can ask Gemini Nano about what it \"sees\" in visual content or \"hears\" in audio content. For example, get suggestions for alternative text on uploaded images on a blog platform, that users can refine and tweak. Or, you could ask Gemini Nano to write descriptions or transcriptions for podcasts. Hybrid AI One challenge developers face with client-side AI is that not all platforms and browsers meet the hardware requirements to run a model on-device. Gemini and Firebase partnered to build the Firebase Web SDK so that when client-side implementations are unavailable, you can fallback to Gemini Nano on a server. Working with you We're so glad to have worked with so many developers on built-in AI APIs. Our efforts aren't possible without you. Early Preview Program: More than 16,000 developers have joined the EPP, testing new APIs, discovering new use cases, and providing feedback to build better AI for the web. Hackathons: We've hosted two hackathons, and you built some incredible websites and Extensions. Your work isn't over. Keep sharing your feedback, testing the new built-in APIs, and we'll keep iterating. You can even help standardize these APIs by joining the W3C's Web Machine Learning Community Group. The future of Chrome Extensions with Gemini in your browser The number of AI-powered Extensions has doubled in the last two years. In fact, 10% of all Extensions installed from the Chrome Web Store use AI. In this talk, Sebastian Benz gives practical examples for why Chrome Extensions and Gemini are such a powerful combination. Examples range from how you can make the browser more helpful by extracting and processing data from websites on the client using Chrome's newly launched prompt API. Over demonstrating the potential of new multimodal capabilities of Chrome's prompting API in Chrome Extensions to make audio and images more accessible to users. To taking a look at the future of browsing by explaining how Google DeepMind's Project Mariner uses Chrome Extensions and the latest Gemini Cloud APIs to build a full-blown browser agent. Explore the potential of using Gemini in the cloud or in the browser in Chrome Extensions to build new browsing experiences and make the browser more helpful. Web AI use cases and strategies in the real world Yuriko Hirota and Swetha Gopalakrishnan highlighted real-world examples of companies using AI on the web to improve their business and user experience.Whether their solution uses client-side models, server-side, or a hybrid solution, what matters is the exciting new functions and features that you make available to your users, right now. BILIBILI made their video streams more engaging with a new feature: bullet-screen comments. They offer real-time user comments in the video, rendered behind the speaker. To do so, they use image segmentation, a well-understood machine learning concept. As a result, session duration increased by 30%! Tokopedia reduced friction in their seller verification process using a face detection model, to assess the quality of photos uploaded. As a result, they reduced manual approvals by almost 70%. Vision Nanny, a web platform for children with Cerebral Visual Impairment (CVI), provides AI-powered vision stimulation activities. They use multiple MediaPipe libraries, including the hand landmark detection model, which locates key points of the hands in an image, video, or in real-time. A pilot with 50 children demonstrated that Vision Nanny delivered responses 5x faster than manual visual stimulation activities. Therapists reported saving an average of three hours per session by removing manual setup. Google Meet has several features enabled by AI, from improving lighting to reducing blur and fuzzy videos. The biggest challenge is that these features need to work in real-time. That's where WebAssembly (Wasm) comes in, to tap into the full power of a computer's CPU and enable real-time video processing. These are just a few real-world examples of AI happening on the web. Several other companies experimented with the built-in AI APIs, some of which shared their work in case studies. Client-side Web AI agents to build smarter future user experiences Jason Mayes walked through the future of the internet: Web AI Agents. The web has an agentic future, bringing AI capabilities directly to the browser, to perform useful work on your behalf, beyond the capabilities of large language models (LLMs). With a client-side approach, there's enhanced privacy, reduced latency, and potential significant cost savings. Agents allow you to upgrade your existing website, to perform tasks autonomously for a user, dynamically selecting and using exposed tools–potentially in a loop–allowing the agent to complete potentially complex or multi-step tasks. Agents can: Plan and divide sub-tasks, handling more complex problems through multi-step planning to break down the task into logical steps to complete. Select the best tools, whether it's functions, API usage, or datastore access to augmented language model's base knowledge, then perform actions on the outside world. Retain context-based memory, based on prior outputs from the agent or external tools. Short-term memory acts like a FIFO buffer of context history up to the context window size of the model, versus long-term memory where a vector database can be used to store information to recall as needed from prior conversation sessions or other data sources entirely. Web AI agents are designed to integrate into existing web technologies in JavaScript. Ultimately, it's important that we continue accelerating our hardware to best run models in the browser. Looking towards the future, technology like WebNN will play a key role in optimizing model execution across CPUs, GPUs, and NPUs. With the trend towards smaller LLMs and continued advancement, this will only grow to be more powerful in the future. Consider using a hybrid approach, combining on-device processing with strategic cloud calls, so you can create intelligent, responsive, and personalized user experiences in the browser right now. Soon, your return from investing in Web AI approach should pay off as devices become more capable at running LLMs. Catch up on Google I/O 2025 We've released all of the talks for Google I/O 2025, with a playlist dedicated to web developers. Watch even more on io.google/2025. Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-22 UTC. [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-05-22 UTC.\"],[],[]]",
  "image": "https://developer.chrome.com/static/blog/ai-io25/image/cover.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\n    \u003cdevsite-progress type=\"indeterminate\" id=\"app-progress\"\u003e\u003c/devsite-progress\u003e\n  \n    \u003ca href=\"#main-content\"\u003e\n      \n      Skip to main content\n    \u003c/a\u003e\n    \u003csection\u003e\n      \u003cdevsite-cookie-notification-bar\u003e\u003c/devsite-cookie-notification-bar\u003e\u003cdevsite-header role=\"banner\"\u003e\n  \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\u003c/devsite-header\u003e\n      \n      \u003csection id=\"gc-wrapper\"\u003e\n        \u003cmain role=\"main\" id=\"main-content\" has-sidebar=\"\"\u003e\n          \n          \u003cdevsite-content\u003e\n            \n              \n\n\n\n\n\n\n\n\n\n\n\n\n\u003carticle\u003e\n  \n  \n  \n  \n  \n\n  \n  \n    \u003ch2 tabindex=\"-1\"\u003e\n      Watch our AI talks at I/O 2025\n      \u003cp data-nosnippet=\"\"\u003e\u003cdevsite-feature-tooltip ack-key=\"AckCollectionsBookmarkTooltipDismiss\" analytics-category=\"Site-Wide Custom Events\" analytics-action-show=\"Callout Profile displayed\" analytics-action-close=\"Callout Profile dismissed\" analytics-label=\"Create Collection Callout\" dismiss-button=\"true\" id=\"devsite-collections-dropdown\" dismiss-button-text=\"Dismiss\" close-button-text=\"Got it\"\u003e\n\n    \n    \u003cdevsite-bookmark\u003e\u003c/devsite-bookmark\u003e\n\n    \u003cspan slot=\"popout-heading\"\u003e\n      \n      Stay organized with collections\n    \u003c/span\u003e\n    \u003cspan slot=\"popout-contents\"\u003e\n      \n      Save and categorize content based on your preferences.\n    \u003c/span\u003e\n  \u003c/devsite-feature-tooltip\u003e\u003c/p\u003e\n  \n    \u003c/h2\u003e\n  \n  \n\n  \u003cdevsite-toc depth=\"2\" devsite-toc-embedded=\"\"\u003e\n  \u003c/devsite-toc\u003e\n  \n    \n  \n\n  \u003cdiv\u003e\n\n  \n    \n\n\n\n\n\u003cdiv translate=\"no\"\u003e\n        \n          \u003cp\u003e\u003cimg alt=\"Alexandra Klepper\" src=\"https://web.dev/images/authors/alexandraklepper.jpg\" decoding=\"async\" height=\"64\" loading=\"lazy\" width=\"64\"/\u003e\u003c/p\u003e\n      \u003c/div\u003e\n\n\u003cp\u003e\n  Published: May 22, 2025\n\u003c/p\u003e\n\n\n\u003cp\u003eAI is transforming how web developers are building websites and web\napplications. At Google I/O 2025, we shared what we\u0026#39;ve been working on over\nthe last year, demonstrated how our partners are making use of AI on the web,\nand \u003ca href=\"https://developer.chrome.com/blog/ai-api-updates-io25\"\u003eannounced new built-in AI APIs\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eDid you miss the event? Good news, you can now watch the talks on-demand!\u003c/p\u003e\n\n\u003ch2 id=\"practical_built-in_ai_with_gemini_nano_in_chrome\" data-text=\"Practical built-in AI with Gemini Nano in Chrome\" tabindex=\"-1\"\u003ePractical built-in AI with Gemini Nano in Chrome\u003c/h2\u003e\n\n\u003cdiv translate=\"no\"\u003e\n        \n          \u003cp\u003e\u003cimg alt=\"Thomas Steiner\" src=\"https://web.dev/images/authors/thomassteiner.jpg\" decoding=\"async\" height=\"64\" loading=\"lazy\" width=\"64\"/\u003e\u003c/p\u003e\n      \u003c/div\u003e\n\n\u003cp\u003eOur core mission is to make Chrome and the web smarter for all developers and\nall users. In this talk, Thomas Steiner shares updates to\n\u003ca href=\"https://developer.chrome.com/docs/ai/built-in\"\u003ebuilt-in AI\u003c/a\u003e, practical use cases, and a look at our future.\u003c/p\u003e\n\n\n\n\u003cp\u003eBuilt-in AI runs client-side models in the browser, which has several advantages:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePrivate\u003c/strong\u003e: Sensitive user data remains on the device, never needing to leave the browser.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOffline\u003c/strong\u003e: Applications can access AI capabilities, even without an internet connection.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePerformant\u003c/strong\u003e: Thanks to hardware acceleration, these APIs deliver excellent performance.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eTake a look at code samples for each of the \u003ca href=\"https://developer.chrome.com/docs/ai/built-in-apis\"\u003ebuilt-in AI APIs\u003c/a\u003e, get an update on their status, and see what companies are implementing this technology.\u003c/p\u003e\n\n\u003ch3 id=\"multimodal_apis\" data-text=\"Multimodal APIs\" tabindex=\"-1\"\u003eMultimodal APIs\u003c/h3\u003e\n\n\u003cp\u003eWe are working on \u003ca href=\"https://developer.chrome.com/blog/ai-api-updates-io25#prompt_api_multimodal\"\u003ebrand new multimodal APIs\u003c/a\u003e. This means you can ask Gemini Nano about what it \u0026#34;sees\u0026#34; in visual content or \u0026#34;hears\u0026#34; in audio content. For example, get suggestions for alternative text on uploaded images on a blog platform, that users can refine and tweak. Or, you could ask Gemini Nano to write descriptions or transcriptions for podcasts.\u003c/p\u003e\n\n\u003ch3 id=\"hybrid_ai\" data-text=\"Hybrid AI\" tabindex=\"-1\"\u003eHybrid AI\u003c/h3\u003e\n\n\u003cp\u003eOne challenge developers face with client-side AI is that not all platforms and\nbrowsers meet the hardware requirements to run a model on-device. Gemini and\nFirebase partnered to build the \u003ca href=\"https://developer.chrome.com/docs/ai/firebase-ai-logic\"\u003eFirebase Web SDK\u003c/a\u003e\nso that when client-side implementations are unavailable, you can fallback to\nGemini Nano on a server.\u003c/p\u003e\n\n\u003ch3 id=\"working_with_you\" data-text=\"Working with you\" tabindex=\"-1\"\u003eWorking with you\u003c/h3\u003e\n\n\u003cp\u003eWe\u0026#39;re so glad to have worked with so many developers on built-in AI APIs. Our efforts aren\u0026#39;t possible without you.\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEarly Preview Program\u003c/strong\u003e: More than 16,000 developers have joined the \u003ca href=\"https://developer.chrome.com/docs/ai/join-epp\"\u003eEPP\u003c/a\u003e, testing new APIs, discovering new use cases, and providing feedback to build better AI for the web.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHackathons\u003c/strong\u003e: We\u0026#39;ve hosted two hackathons, and \u003ca href=\"https://developer.chrome.com/blog/ai-challenge-winners\"\u003eyou built some incredible websites and Extensions\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eYour work isn\u0026#39;t over. Keep sharing your feedback, testing the new built-in APIs, and we\u0026#39;ll keep iterating. You can even help standardize these APIs by joining the W3C\u0026#39;s \u003ca href=\"https://www.w3.org/groups/cg/webmachinelearning/\"\u003eWeb Machine Learning Community Group\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"the_future_of_chrome_extensions_with_gemini_in_your_browser\" data-text=\"The future of Chrome Extensions with Gemini in your browser\" tabindex=\"-1\"\u003eThe future of Chrome Extensions with Gemini in your browser\u003c/h2\u003e\n\n\u003cdiv translate=\"no\"\u003e\n        \n          \u003cp\u003e\u003cimg alt=\"Sebastian Benz\" src=\"https://web.dev/images/authors/sebastianbenz.jpg\" decoding=\"async\" height=\"64\" loading=\"lazy\" width=\"64\"/\u003e\u003c/p\u003e\n      \u003c/div\u003e\n\n\u003cp\u003eThe number of AI-powered Extensions has doubled in the last two years. In fact, 10% of all Extensions installed from the Chrome Web Store use AI. In this talk, Sebastian Benz gives practical examples for why Chrome Extensions and Gemini are such a powerful combination.\u003c/p\u003e\n\n\n\n\u003cp\u003eExamples range from how you can make the browser more helpful by extracting and processing data from websites on the client using Chrome\u0026#39;s newly launched prompt API.\u003c/p\u003e\n\n\u003cp\u003eOver demonstrating the potential of new multimodal capabilities of Chrome\u0026#39;s prompting API in Chrome Extensions to make audio and images more accessible to users. \u003c/p\u003e\n\n\u003cp\u003eTo taking a look at the future of browsing by explaining how Google DeepMind\u0026#39;s \u003ca href=\"https://deepmind.google/technologies/project-mariner/\"\u003eProject Mariner\u003c/a\u003e uses Chrome Extensions and the latest Gemini Cloud APIs to build a full-blown browser agent.\u003c/p\u003e\n\n\u003cp\u003eExplore the potential of using Gemini in the cloud or \u003ca href=\"https://developer.chrome.com/docs/ai/built-in-apis\"\u003ein the browser\u003c/a\u003e in \u003ca href=\"https://developer.chrome.com/docs/extensions/ai\"\u003eChrome Extensions\u003c/a\u003e to build new browsing experiences and make the browser more helpful.\u003c/p\u003e\n\n\u003ch2 id=\"web_ai_use_cases_and_strategies_in_the_real_world\" data-text=\"Web AI use cases and strategies in the real world\" tabindex=\"-1\"\u003eWeb AI use cases and strategies in the real world\u003c/h2\u003e\n\n\u003cdiv translate=\"no\"\u003e\n  \n    \n    \n      \u003cdiv\u003e\n        \n          \u003cp\u003e\u003cimg alt=\"Yuriko Hirota\" src=\"https://web.dev/images/authors/yuriko-hirota.jpg\" decoding=\"async\" height=\"64\" loading=\"lazy\" width=\"64\"/\u003e\u003c/p\u003e\n      \u003c/div\u003e\n    \n  \n    \n    \n      \u003cdiv\u003e\n        \n          \u003cp\u003e\u003cimg alt=\"Swetha Gopalakrishnan\" src=\"https://web.dev/images/authors/swethagopalakrishnan.jpg\" decoding=\"async\" height=\"64\" loading=\"lazy\" width=\"64\"/\u003e\u003c/p\u003e\n      \u003c/div\u003e\n    \n  \n\u003c/div\u003e\n\n\u003cp\u003eYuriko Hirota and Swetha Gopalakrishnan highlighted real-world examples of\ncompanies using AI on the web to improve their business and user experience.Whether their solution uses client-side models, server-side, or a hybrid\nsolution, what matters is the exciting new functions and features that you\nmake available to your users, right now.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://web.dev/case-studies/bilibili-web-ai-improvements\"\u003eBILIBILI\u003c/a\u003e made their\nvideo streams more engaging with a new feature: \u003cem\u003ebullet-screen comments\u003c/em\u003e. They\noffer real-time user comments in the video, rendered behind the speaker. To do\nso, they use image segmentation, a well-understood machine learning concept. As\na result, session duration increased by 30%!\n\u003ca href=\"https://web.dev/case-studies/tokopedia-ml\"\u003eTokopedia\u003c/a\u003e reduced friction in\ntheir seller verification process using a face detection model, to assess the\nquality of photos uploaded. As a result, they reduced manual approvals by\nalmost 70%.\u003c/p\u003e\n\n\u003cp\u003eVision Nanny, a web platform for children with Cerebral Visual Impairment (CVI),\nprovides AI-powered vision stimulation activities. They use multiple MediaPipe\nlibraries, including the hand landmark detection model, which locates key points\nof the hands in an image, video, or in real-time. A pilot with 50 children\ndemonstrated that Vision Nanny delivered responses 5x faster than manual visual\nstimulation activities. Therapists reported saving an average of three hours per\nsession by removing manual setup.\u003c/p\u003e\n\n\u003cp\u003eGoogle Meet has several features enabled by AI, from improving lighting to\nreducing blur and fuzzy videos. The biggest challenge is that these features\nneed to work in real-time. That\u0026#39;s where\n\u003ca href=\"https://developer.chrome.com/blog/io24-webassembly-webgpu-1#webassembly\"\u003eWebAssembly (Wasm)\u003c/a\u003e comes in,\nto tap into the full power of a computer\u0026#39;s CPU and enable real-time video\nprocessing.\u003c/p\u003e\n\n\u003cp\u003eThese are just a few real-world examples of AI happening on the web. Several\nother companies experimented with the built-in AI APIs, some of which shared\ntheir work in \u003ca href=\"https://developer.chrome.com/docs/ai#case-studies\"\u003ecase studies\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"client-side_web_ai_agents_to_build_smarter_future_user_experiences\" data-text=\"Client-side Web AI agents to build smarter future user experiences\" tabindex=\"-1\"\u003eClient-side Web AI agents to build smarter future user experiences\u003c/h2\u003e\n\n\u003cdiv translate=\"no\"\u003e\n        \n          \u003cp\u003e\u003cimg alt=\"Jason Mayes\" src=\"https://web.dev/images/authors/jasonmayes.jpg\" decoding=\"async\" height=\"64\" loading=\"lazy\" width=\"64\"/\u003e\u003c/p\u003e\n      \u003c/div\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.linkedin.com/in/webai\"\u003eJason Mayes\u003c/a\u003e walked through the future of\nthe internet: Web AI Agents. The web has an agentic future, bringing AI\ncapabilities directly to the browser, to perform useful work on your behalf,\nbeyond the capabilities of large language models (LLMs).\u003c/p\u003e\n\n\n\n\u003cp\u003eWith a client-side approach, there\u0026#39;s enhanced privacy, reduced latency, and\npotential significant cost savings. Agents allow you to upgrade your existing\nwebsite, to perform tasks autonomously for a user, dynamically selecting and\nusing exposed tools–potentially in a loop–allowing the agent to complete\npotentially complex or multi-step tasks.\u003c/p\u003e\n\n\u003cp\u003eAgents can:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePlan and divide sub-tasks\u003c/strong\u003e, handling more complex problems through\nmulti-step planning to break down the task into logical steps to complete.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelect the best tools\u003c/strong\u003e, whether it\u0026#39;s functions, API usage, or datastore\naccess to augmented language model\u0026#39;s base knowledge, then perform actions\non the outside world.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRetain context-based memory\u003c/strong\u003e, based on prior outputs from the agent or\nexternal tools. Short-term memory acts like a FIFO buffer of context history\nup to the context window size of the model, versus long-term memory where a\nvector database can be used to store information to recall as needed from\nprior conversation sessions or other data sources entirely.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWeb AI agents are designed to integrate into existing web technologies in\nJavaScript. Ultimately, it\u0026#39;s important that we continue accelerating our\nhardware to best run models in the browser. Looking towards the future,\ntechnology like WebNN will play a key role in optimizing model execution across\nCPUs, GPUs, and NPUs. With the trend towards smaller LLMs and continued\nadvancement, this will only grow to be more powerful in the future.\u003c/p\u003e\n\n\u003cp\u003eConsider using a hybrid approach, combining on-device processing with strategic\ncloud calls, so you can create intelligent, responsive, and personalized user\nexperiences in the browser right now. Soon, your return from investing in \n Web AI approach should pay off as devices become more capable at running LLMs.\u003c/p\u003e\n\n\u003ch2 id=\"catch_up_on_google_io_2025\" data-text=\"Catch up on Google I/O 2025\" tabindex=\"-1\"\u003eCatch up on Google I/O 2025\u003c/h2\u003e\n\n\u003cp\u003eWe\u0026#39;ve released all of the talks for Google I/O 2025, with a playlist dedicated\nto \u003ca href=\"https://www.youtube.com/watch?v=8iIvAMZ-XYU\u0026amp;list=PLNYkxOF6rcIDf2yTHfwShSCwVxaUuGk-v\u0026amp;index=1\"\u003eweb developers\u003c/a\u003e.\nWatch even more on \u003ca href=\"https://io.google/2025/explore/?focus_areas=web\"\u003eio.google/2025\u003c/a\u003e.\u003c/p\u003e\n\n  \n\n  \n\u003c/div\u003e\n\n  \n\n  \n    \n    \n      \n    \u003cdevsite-thumb-rating position=\"footer\"\u003e\n    \u003c/devsite-thumb-rating\u003e\n  \n       \n    \n    \n  \n\n  \n  \n\u003c/article\u003e\n\n\n\u003cdevsite-content-footer\u003e\n  \u003cp\u003eExcept as otherwise noted, the content of this page is licensed under the \u003ca href=\"https://creativecommons.org/licenses/by/4.0/\"\u003eCreative Commons Attribution 4.0 License\u003c/a\u003e, and code samples are licensed under the \u003ca href=\"https://www.apache.org/licenses/LICENSE-2.0\"\u003eApache 2.0 License\u003c/a\u003e. For details, see the \u003ca href=\"https://developers.google.com/site-policies\"\u003eGoogle Developers Site Policies\u003c/a\u003e. Java is a registered trademark of Oracle and/or its affiliates.\u003c/p\u003e\n  \u003cp\u003eLast updated 2025-05-22 UTC.\u003c/p\u003e\n\u003c/devsite-content-footer\u003e\n\n\n\u003cdevsite-notification\u003e\n\u003c/devsite-notification\u003e\n\n\n  \n\u003cp\u003e\n  \n  \n    \u003ctemplate\u003e\n      [[[\u0026#34;Easy to understand\u0026#34;,\u0026#34;easyToUnderstand\u0026#34;,\u0026#34;thumb-up\u0026#34;],[\u0026#34;Solved my problem\u0026#34;,\u0026#34;solvedMyProblem\u0026#34;,\u0026#34;thumb-up\u0026#34;],[\u0026#34;Other\u0026#34;,\u0026#34;otherUp\u0026#34;,\u0026#34;thumb-up\u0026#34;]],[[\u0026#34;Missing the information I need\u0026#34;,\u0026#34;missingTheInformationINeed\u0026#34;,\u0026#34;thumb-down\u0026#34;],[\u0026#34;Too complicated / too many steps\u0026#34;,\u0026#34;tooComplicatedTooManySteps\u0026#34;,\u0026#34;thumb-down\u0026#34;],[\u0026#34;Out of date\u0026#34;,\u0026#34;outOfDate\u0026#34;,\u0026#34;thumb-down\u0026#34;],[\u0026#34;Samples / code issue\u0026#34;,\u0026#34;samplesCodeIssue\u0026#34;,\u0026#34;thumb-down\u0026#34;],[\u0026#34;Other\u0026#34;,\u0026#34;otherDown\u0026#34;,\u0026#34;thumb-down\u0026#34;]],[\u0026#34;Last updated 2025-05-22 UTC.\u0026#34;],[],[]]\n    \u003c/template\u003e\n  \n\u003c/p\u003e\n            \n          \u003c/devsite-content\u003e\n        \u003c/main\u003e\n        \n        \n        \n        \u003cdevsite-panel\u003e\n          \n        \u003c/devsite-panel\u003e\n        \n      \u003c/section\u003e\u003c/section\u003e\n    \u003cdevsite-sitemask\u003e\u003c/devsite-sitemask\u003e\n    \u003cdevsite-snackbar\u003e\u003c/devsite-snackbar\u003e\n    \u003cdevsite-tooltip\u003e\u003c/devsite-tooltip\u003e\n    \u003cdevsite-heading-link\u003e\u003c/devsite-heading-link\u003e\n    \u003cdevsite-analytics\u003e\n      \n        \n\n      \n    \u003c/devsite-analytics\u003e\n    \n      \u003cdevsite-badger\u003e\u003c/devsite-badger\u003e\n    \n    \n    \n\n\n    \u003cdevsite-a11y-announce\u003e\u003c/devsite-a11y-announce\u003e\n  \n\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": null,
  "modifiedTime": null
}
