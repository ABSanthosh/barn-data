{
  "id": "d692e3b2-ab85-4c8e-aa19-539f187ba93f",
  "title": "Llamafile’s progress, four months in",
  "link": "https://hacks.mozilla.org/2024/04/llamafiles-progress-four-months-in/",
  "description": "When Mozilla’s Innovation group first launched the llamafile project late last year, we were thrilled by the immediate positive response from open source AI developers. It’s become one of Mozilla’s top three most-favorited repositories on GitHub, attracting a number of contributors, some excellent PRs, and a growing community on our Discord server. The post Llamafile’s progress, four months in appeared first on Mozilla Hacks - the Web developer blog.",
  "author": "Stephen Hood",
  "published": "Thu, 25 Apr 2024 15:34:08 +0000",
  "source": "https://hacks.mozilla.org/feed/",
  "categories": [
    "Developer Tools",
    "Featured Article",
    "Firefox",
    "discord",
    "firefox",
    "hugging face",
    "llamafile",
    "openAI",
    "raspberrypi"
  ],
  "byline": "By Stephen Hood",
  "length": 9614,
  "excerpt": "Mozilla’s Innovation group launched the llamafile project last year and it has become one of Mozilla’s most-favorited repositories on GitHub.",
  "siteName": "Mozilla Hacks – the Web developer blog",
  "favicon": "",
  "text": "When Mozilla’s Innovation group first launched the llamafile project late last year, we were thrilled by the immediate positive response from open source AI developers. It’s become one of Mozilla’s top three most-favorited repositories on GitHub, attracting a number of contributors, some excellent PRs, and a growing community on our Discord server. Through it all, lead developer and project visionary Justine Tunney has remained hard at work on a wide variety of fundamental improvements to the project. Just last night, Justine shipped the v0.8 release of llamafile, which includes not only support for the very latest open models, but also a number of big performance improvements for CPU inference. As a result of Justine’s work, today llamafile is both the easiest and fastest way to run a wide range of open large language models on your own hardware. See for yourself: with llamafile, you can run Meta’s just-released LLaMA 3 model–which rivals the very best models available in its size class–on an everyday Macbook. How did we do it? To explain that, let’s take a step back and tell you about everything that’s changed since v0.1. tinyBLAS: democratizing GPU support for NVIDIA and AMD llamafile is built atop the now-legendary llama.cpp project. llama.cpp supports GPU-accelerated inference for NVIDIA processors via the cuBLAS linear algebra library, but that requires users to install NVIDIA’s CUDA SDK. We felt uncomfortable with that fact, because it conflicts with our project goal of building a fully open-source and transparent AI stack that anyone can run on commodity hardware. And besides, getting CUDA set up correctly can be a bear on some systems. There had to be a better way. With the community’s help (here’s looking at you, @ahgamut and @mrdomino!), we created our own solution: it’s called tinyBLAS, and it’s llamafile’s brand-new and highly efficient linear algebra library. tinyBLAS makes NVIDIA acceleration simple and seamless for llamafile users. On Windows, you don’t even need to install CUDA at all; all you need is the display driver you’ve probably already installed. But tinyBLAS is about more than just NVIDIA: it supports AMD GPUs, as well. This is no small feat. While AMD commands a respectable 20% of today’s GPU market, poor software and driver support have historically made them a secondary player in the machine learning space. That’s a shame, given that AMD’s GPUs offer high performance, are price competitive, and are widely available. One of llamafile’s goals is to democratize access to open source AI technology, and that means getting AMD a seat at the table. That’s exactly what we’ve done: with llamafile’s tinyBLAS, you can now easily make full use of your AMD GPU to accelerate local inference. And, as with CUDA, if you’re a Windows user you don’t even have to install AMD’s ROCm SDK. All of this means that, for many users, llamafile will automatically use your GPU right out of the box, with little to no effort on your part. CPU performance gains for faster local AI Here at Mozilla, we are keenly interested in the promise of “local AI,” in which AI models and applications run directly on end-user hardware instead of in the cloud. Local AI is exciting because it opens up the possibility of more user control over these systems and greater privacy and security for users. But many consumer devices lack the high-end GPUs that are often required for inference tasks. llama.cpp has been a game-changer in this regard because it makes local inference both possible and usably performant on CPUs instead of just GPUs.  Justine’s recent work on llamafile has now pushed the state of the art even further. As documented in her detailed blog post on the subject, by writing 84 new matrix multiplication kernels she was able to increase llamafile’s prompt evaluation performance by an astonishing 10x compared to our previous release. This is a substantial and impactful step forward in the quest to make local AI viable on consumer hardware. This work is also a great example of our commitment to the open source AI community. After completing this work we immediately submitted a PR to upstream these performance improvements to llama.cpp. This was just the latest of a number of enhancements we’ve contributed back to llama.cpp, a practice we plan to continue. Raspberry Pi performance gains Speaking of consumer hardware, there are few examples that are both more interesting and more humble than the beloved Raspberry Pi. For a bargain basement price, you get a full-featured computer running Linux with plenty of computing power for typical desktop uses. It’s an impressive package, but historically it hasn’t been considered a viable platform for AI applications. Not any more. llamafile has now been optimized for the latest model (the Raspberry Pi 5), and the result is that a number of small LLMs–such as Rocket-3B (download), TinyLLaMA-1.5B (download), and Phi-2 (download)–run at usable speeds on one of the least expensive computers available today. We’ve seen prompt evaluation speeds of up to 80 tokens/sec in some cases! Keeping up with the latest models The pace of progress in the open model space has been stunningly fast. Over the past few months, hundreds of models have been released or updated via fine-tuning. Along the way, there has been a clear trend of ever-increasing model performance and ever-smaller model sizes. The llama.cpp project has been doing an excellent job of keeping up with all of these new models, frequently rolling-out support for new architectures and model features within days of their release. For our part we’ve been keeping llamafile closely synced with llama.cpp so that we can support all the same models. Given the complexity of both projects, this has been no small feat, so we’re lucky to have Justine on the case. Today, you can today use the very latest and most capable open models with llamafile thanks to her hard work. For example, we were able to roll-out llamafiles for Meta’s newest LLaMA 3 models–8B-Instruct and 70B-Instruct–within a day of their release. With yesterday’s 0.8 release, llamafile can also run Grok, Mixtral 8x22B, and Command-R. Creating your own llamafiles Since the day that llamafile shipped people have wanted to create their own llamafiles. Previously, this required a number of steps, but today you can do it with a single command, e.g.: llamafile-convert [model.gguf] In just moments, this will produce a “model.llamafile” file that is ready for immediate use. Our thanks to community member @chan1012 for contributing this helpful improvement. In a related development, Hugging Face recently added official support for llamafile within their model hub. This means you can now search and filter Hugging Face specifically for llamafiles created and distributed by other people in the open source community. OpenAI-compatible API server Since it’s built on top of llama.cpp, llamafile inherits that project’s server component, which provides OpenAI-compatible API endpoints. This enables developers who are building on top of OpenAI to switch to using open models instead. At Mozilla we very much want to support this kind of future: one where open-source AI is a viable alternative to centralized, closed, commercial offerings. While open models do not yet fully rival the capabilities of closed models, they’re making rapid progress. We believe that making it easier to pivot existing code over to executing against open models will increase demand and further fuel this progress. Over the past few months, we’ve invested effort in extending these endpoints, both to increase functionality and improve compatibility. Today, llamafile can serve as a drop-in replacement for OpenAI in a wide variety of use cases. We want to further extend our API server’s capabilities, and we’re eager to hear what developers want and need. What’s holding you back from using open models? What features, capabilities, or tools do you need? Let us know! Integrations with other open source AI projects Finally, it’s been a delight to see llamafile adopted by independent developers and integrated into leading open source AI projects (like Open Interpreter). Kudos in particular to our own Kate Silverstein who landed PRs that add llamafile support to LangChain and LlamaIndex (with AutoGPT coming soon). If you’re a maintainer or contributor to an open source AI project that you feel would benefit from llamafile integration, let us know how we can help. Join us! The llamafile project is just getting started, and it’s also only the first step in a major new initiative on Mozilla’s part to contribute to and participate in the open source AI community. We’ll have more to share about that soon, but for now: I invite you to join us on the llamafile project! The best place to connect with both the llamafile team at Mozilla and the overall llamafile community is over at our Discord server, which has a dedicated channel just for llamafile. And of course, your enhancement requests, issues, and PRs are always welcome over at our GitHub repo. I hope you’ll join us. The next few months are going to be even more interesting and unexpected than the last, both for llamafile and for open source AI itself. Stephen leads open source AI projects (including llamafile) in Mozilla's Innovation group. He previously managed social bookmarking pioneer del.icio.us; co-founded Storium, Blockboard, and FairSpin; and worked on Yahoo Search and BEA WebLogic. More articles by Stephen Hood…",
  "image": "https://hacks.mozilla.org/wp-content/uploads/2023/11/image-3.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle role=\"article\"\u003e\n    \u003cp\u003e\u003cspan\u003eWhen Mozilla’s Innovation group \u003c/span\u003e\u003ca href=\"https://hacks.mozilla.org/2023/11/introducing-llamafile/\"\u003e\u003cspan\u003efirst launched\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e the \u003c/span\u003e\u003ca href=\"https://github.com/Mozilla-Ocho/llamafile\"\u003e\u003cspan\u003ellamafile project\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e late last year, we were thrilled by the immediate positive response from open source AI developers. It’s become one of Mozilla’s top three most-favorited repositories on GitHub, attracting a number of contributors, some excellent PRs, and a growing community on our \u003c/span\u003e\u003ca href=\"https://discord.gg/YuMNeuKStr\"\u003e\u003cspan\u003eDiscord server\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eThrough it all, lead developer and project visionary \u003ca href=\"https://github.com/jart\"\u003eJustine Tunney\u003c/a\u003e has remained hard at work on a wide variety of fundamental improvements to the project. Just last night, Justine \u003ca href=\"https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8\"\u003eshipped the v0.8 release of llamafile\u003c/a\u003e, which includes not only support for the very latest open models, but also a number of big performance improvements for CPU inference.\u003c/p\u003e\n\u003cp\u003eAs a result of Justine’s work, today llamafile is both the easiest \u003ci\u003eand fastest\u003c/i\u003e way to run a wide range of open large language models on your own hardware. See for yourself: with llamafile, you can run Meta’s just-released \u003ca href=\"https://huggingface.co/jartine/Meta-Llama-3-8B-Instruct-llamafile\"\u003eLLaMA 3 model\u003c/a\u003e–which rivals the very best models available in its size class–on an everyday Macbook.\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHow did we do it? To explain that, let’s take a step back and tell you about everything that’s changed since v0.1.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003etinyBLAS: democratizing GPU support for NVIDIA \u003c/b\u003e\u003cb\u003e\u003ci\u003eand\u003c/i\u003e\u003c/b\u003e\u003cb\u003e AMD\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ellamafile is built atop the now-legendary \u003c/span\u003e\u003ca href=\"https://github.com/ggerganov/llama.cpp\"\u003e\u003cspan\u003ellama.cpp\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e project. llama.cpp supports GPU-accelerated inference for NVIDIA processors via the cuBLAS linear algebra library, but that requires users to install NVIDIA’s CUDA SDK. We felt uncomfortable with that fact, because it conflicts with our project goal of building a fully open-source and transparent AI stack that anyone can run on commodity hardware. And besides, getting CUDA set up correctly can be a bear on some systems. There had to be a better way.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith the community’s help (here’s looking at you, \u003c/span\u003e\u003ca href=\"https://ahgamut.github.io/\"\u003e\u003cspan\u003e@ahgamut\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://github.com/mrdomino\"\u003e\u003cspan\u003e@mrdomino\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e!), we created our own solution: it’s called tinyBLAS, and it’s llamafile’s brand-new and highly efficient linear algebra library. tinyBLAS makes NVIDIA acceleration simple and seamless for llamafile users. On Windows, you don’t even need to install CUDA at all; all you need is the display driver you’ve probably already installed.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBut tinyBLAS is about more than just NVIDIA: it supports AMD GPUs, as well. This is no small feat. While AMD commands a respectable 20% of today’s GPU market, poor software and driver support have historically made them a secondary player in the machine learning space. That’s a shame, given that AMD’s GPUs offer high performance, are price competitive, and are widely available. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOne of llamafile’s goals is to democratize access to open source AI technology, and that means getting AMD a seat at the table. That’s exactly what we’ve done: with llamafile’s tinyBLAS, you can now easily make full use of your AMD GPU to accelerate local inference. And, as with CUDA, if you’re a Windows user you don’t even have to install AMD’s ROCm SDK.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAll of this means that, for many users, llamafile will automatically use your GPU right out of the box, with little to no effort on your part.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eCPU performance gains for faster local AI\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHere at Mozilla, we are keenly interested in the promise of “local AI,” in which AI models and applications run directly on end-user hardware instead of in the cloud. Local AI is exciting because it opens up the possibility of more user control over these systems and greater privacy and security for users.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBut many consumer devices lack the high-end GPUs that are often required for inference tasks. llama.cpp has been a game-changer in this regard because it makes local inference both possible and usably performant on CPUs instead of just GPUs. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eJustine’s recent work on llamafile has now pushed the state of the art even further. As documented in \u003c/span\u003e\u003ca href=\"http://justine.lol/matmul/\"\u003e\u003cspan\u003eher detailed blog post\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e on the subject, by writing 84 new matrix multiplication kernels she was able to increase llamafile’s prompt evaluation performance by an astonishing 10x compared to our previous release. This is a substantial and impactful step forward in the quest to make local AI viable on consumer hardware.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis work is also a great example of our commitment to the open source AI community. After completing this work we immediately \u003c/span\u003e\u003ca href=\"https://github.com/ggerganov/llama.cpp/pull/6414\"\u003e\u003cspan\u003esubmitted a PR\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to upstream these performance improvements to llama.cpp. This was just the latest of a number of enhancements we’ve contributed back to llama.cpp, a practice we plan to continue.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eRaspberry Pi performance gains\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSpeaking of consumer hardware, there are few examples that are both more interesting and more humble than the beloved Raspberry Pi. For a bargain basement price, you get a full-featured computer running Linux with plenty of computing power for typical desktop uses. It’s an impressive package, but historically it hasn’t been considered a viable platform for AI applications.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eNot any more. llamafile has now been optimized for the latest model (the Raspberry Pi 5), and the result is that a number of small LLMs–such as Rocket-3B (\u003c/span\u003e\u003ca href=\"https://huggingface.co/jartine/rocket-3B-llamafile/resolve/main/rocket-3b.Q5_K_M.llamafile?download=true\"\u003e\u003cspan\u003edownload\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e), TinyLLaMA-1.5B (\u003c/span\u003e\u003ca href=\"https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile?download=true\"\u003e\u003cspan\u003edownload\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e), and Phi-2 (\u003c/span\u003e\u003ca href=\"https://huggingface.co/jartine/phi-2-llamafile/resolve/main/phi-2.Q5_K_M.llamafile?download=true\"\u003e\u003cspan\u003edownload\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e)–run at usable speeds on one of the least expensive computers available today. We’ve seen prompt evaluation speeds of \u003c/span\u003e\u003ca href=\"https://twitter.com/JustineTunney/status/1776440470152867930\"\u003e\u003cspan\u003eup to 80 tokens/sec\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e in some cases!\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eKeeping up with the latest models\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe pace of progress in the open model space has been \u003c/span\u003e\u003ca href=\"https://twitter.com/maximelabonne/status/1779123021480865807\"\u003e\u003cspan\u003estunningly fast\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. Over the past few months, hundreds of models have been released or updated via fine-tuning. Along the way, there has been a clear trend of ever-increasing model performance and ever-smaller model sizes. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe llama.cpp project has been doing an excellent job of keeping up with all of these new models, frequently rolling-out support for new architectures and model features within days of their release.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor our part we’ve been keeping llamafile closely synced with llama.cpp so that we can support all the same models. Given the complexity of both projects, this has been no small feat, so we’re lucky to have Justine on the case. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eToday, you can today use the very latest and most capable open models with llamafile thanks to her hard work. For example, we were able to roll-out llamafiles for Meta’s newest LLaMA 3 models–\u003c/span\u003e\u003ca href=\"https://huggingface.co/jartine/Meta-Llama-3-8B-Instruct-llamafile\"\u003e\u003cspan\u003e8B-Instruct\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://huggingface.co/jartine/Meta-Llama-3-70B-Instruct-llamafile\"\u003e\u003cspan\u003e70B-Instruct\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e–within a day of their release. With yesterday’s 0.8 release, llamafile can also run Grok, Mixtral 8x22B, and Command-R.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eCreating your own llamafiles\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSince the day that llamafile shipped people have wanted to create their own llamafiles. Previously, this required a number of steps, but today you can do it with a single command, e.g.:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003ccode\u003ellamafile-convert [model.gguf]\u003c/code\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn just moments, this will produce a “model.llamafile” file that is ready for immediate use. Our thanks to community member \u003c/span\u003e\u003ca href=\"https://github.com/chand1012\"\u003e\u003cspan\u003e@chan1012\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e for contributing this helpful improvement.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn a related development, Hugging Face recently added official support for llamafile within their model hub. This means you can now \u003c/span\u003e\u003ca href=\"https://huggingface.co/models?library=llamafile\u0026amp;sort=trending\"\u003e\u003cspan\u003esearch and filter\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e Hugging Face specifically for llamafiles created and distributed by other people in the open source community.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eOpenAI-compatible API server\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSince it’s built on top of llama.cpp, llamafile inherits that project’s server component, which provides OpenAI-compatible API endpoints. This enables developers who are building on top of OpenAI to switch to using open models instead. At Mozilla we very much want to support this kind of future: one where open-source AI is a viable alternative to centralized, closed, commercial offerings. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhile open models do not yet fully rival the capabilities of closed models, they’re making rapid progress. We believe that making it easier to pivot existing code over to executing against open models will increase demand and further fuel this progress.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOver the past few months, we’ve invested effort in extending these endpoints, both to increase functionality and improve compatibility. Today, llamafile can serve as a drop-in replacement for OpenAI in a wide variety of use cases. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe want to further extend our API server’s capabilities, and we’re eager to hear what developers want and need. What’s holding you back from using open models? What features, capabilities, or tools do you need? \u003c/span\u003e\u003ca href=\"https://discord.gg/YTgM42NZEr\"\u003e\u003cspan\u003eLet us know\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e!\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eIntegrations with other open source AI projects\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFinally, it’s been a delight to see llamafile adopted by independent developers and integrated into leading open source AI projects (like \u003c/span\u003e\u003ca href=\"https://github.com/OpenInterpreter/open-interpreter\"\u003e\u003cspan\u003eOpen Interpreter\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e). Kudos in particular to our own \u003c/span\u003e\u003ca href=\"https://github.com/k8si\"\u003e\u003cspan\u003eKate Silverstein\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e who landed PRs that add llamafile support to \u003c/span\u003e\u003ca href=\"https://github.com/langchain-ai/langchain\"\u003e\u003cspan\u003eLangChain\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://github.com/run-llama/llama_index\"\u003e\u003cspan\u003eLlamaIndex\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e (with \u003c/span\u003e\u003ca href=\"https://github.com/Significant-Gravitas/AutoGPT\"\u003e\u003cspan\u003eAutoGPT\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e coming soon).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIf you’re a maintainer or contributor to an open source AI project that you feel would benefit from llamafile integration, \u003c/span\u003e\u003ca href=\"https://discord.gg/YTgM42NZEr\"\u003e\u003cspan\u003elet us know how we can help\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eJoin us!\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe llamafile project is just getting started, and it’s also only the first step in a major new initiative on Mozilla’s part to contribute to and participate in the open source AI community. We’ll have more to share about that soon, but for now: I invite you to join us on the llamafile project!\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe best place to connect with both the llamafile team at Mozilla and the overall llamafile community is over at our Discord server, which has \u003c/span\u003e\u003ca href=\"https://discord.gg/YuMNeuKStr\"\u003e\u003cspan\u003ea dedicated channel just for llamafile\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. And of course, your enhancement requests, issues, and PRs are always welcome over at our \u003c/span\u003e\u003ca href=\"https://github.com/Mozilla-Ocho/llamafile\"\u003e\u003cspan\u003eGitHub repo\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eI hope you’ll join us. The next few months are going to be even more interesting and unexpected than the last, both for llamafile and for open source AI itself.\u003c/span\u003e\u003c/p\u003e\n\n    \u003csection\u003e\n                                \n                      \u003cp\u003eStephen leads open source AI projects (including llamafile) in Mozilla\u0026#39;s Innovation group. He previously managed social bookmarking pioneer del.icio.us; co-founded Storium, Blockboard, and FairSpin; and worked on Yahoo Search and BEA WebLogic.\u003c/p\u003e\n                                \u003cp\u003e\u003ca href=\"https://hacks.mozilla.org/author/slangtonhoodmozilla-com/\"\u003eMore articles by Stephen Hood…\u003c/a\u003e\u003c/p\u003e\n                  \u003c/section\u003e\n  \u003c/article\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": null,
  "modifiedTime": null
}
