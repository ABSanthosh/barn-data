{
  "id": "ee55c2a6-2ac5-4b82-8d34-25272bfb7ded",
  "title": "WordPress Multi-Multisite: A Case Study",
  "link": "https://css-tricks.com/wordpress-multi-multisite-a-case-study/",
  "description": "What's it look like to create a dashboard within the WordPress admin for analyzing Google Analytics data across 900 blogs across 25 multisite instances? It involves designing a user-friendly interface, leveraging the WordPress REST API, implementing a plugin for data retrieval, and addressing challenges like rate limits and authentication. WordPress Multi-Multisite: A Case Study originally published on CSS-Tricks, which is part of the DigitalOcean family. You should get the newsletter.",
  "author": "Scott Fennell",
  "published": "Wed, 27 Nov 2024 16:09:14 +0000",
  "source": "https://css-tricks.com/feed/",
  "categories": [
    "Articles",
    "WordPress"
  ],
  "byline": "Scott Fennell",
  "length": 25448,
  "excerpt": "What's it look like to create a dashboard within the WordPress admin for analyzing Google Analytics data across 900 blogs across 25 multisite instances? It involves designing a user-friendly interface, leveraging the WordPress REST API, implementing a plugin for data retrieval, and addressing challenges like rate limits and authentication.",
  "siteName": "CSS-Tricks",
  "favicon": "https://i0.wp.com/css-tricks.com/wp-content/uploads/2021/07/star.png?fit=180%2C180\u0026ssl=1",
  "text": "The mission: Provide a dashboard within the WordPress admin area for browsing Google Analytics data for all your blogs. The catch? You’ve got about 900 live blogs, spread across about 25 WordPress multisite instances. Some instances have just one blog, others have as many as 250. In other words, what you need is to compress a data set that normally takes a very long time to compile into a single user-friendly screen. The implementation details are entirely up to you, but the final result should look like this Figma comp: Design courtesy of the incomparable Brian Biddle. I want to walk you through my approach and some of the interesting challenges I faced coming up with it, as well as the occasional nitty-gritty detail in between. I’ll cover topics like the WordPress REST API, choosing between a JavaScript or PHP approach, rate/time limits in production web environments, security, custom database design — and even a touch of AI. But first, a little orientation. Let’s define some terms We’re about to cover a lot of ground, so it’s worth spending a couple of moments reviewing some key terms we’ll be using throughout this post. What is WordPress multisite? WordPress Multisite is a feature of WordPress core — no plugins required — whereby you can run multiple blogs (or websites, or stores, or what have you) from a single WordPress installation. All the blogs share the same WordPress core files, wp-content folder, and MySQL database. However, each blog gets its own folder within wp-content/uploads for its uploaded media, and its own set of database tables for its posts, categories, options, etc. Users can be members of some or all blogs within the multisite installation. What is WordPress multi-multisite? It’s just a nickname for managing multiple instances of WordPress multisite. It can get messy to have different customers share one multisite instance, so I prefer to break it up so that each customer has their own multisite, but they can have many blogs within their multisite. So that’s different from a “Network of Networks”? It’s apparently possible to run multiple instances of WordPress multisite against the same WordPress core installation. I’ve never looked into this, but I recall hearing about it over the years. I’ve heard the term “Network of Networks” and I like it, but that is not the scenario I’m covering in this article. Why do you keep saying “blogs”? Do people still blog? You betcha! And people read them, too. You’re reading one right now. Hence, the need for a robust analytics solution. But this article could just as easily be about any sort of WordPress site. I happen to be dealing with blogs, and the word “blog” is a concise way to express “a subsite within a WordPress multisite instance”. One more thing: In this article, I’ll use the term dashboard site to refer to the site from which I observe the compiled analytics data. I’ll use the term client sites to refer to the 25 multisites I pull data from. My implementation My strategy was to write one WordPress plugin that is installed on all 25 client sites, as well as on the dashboard site. The plugin serves two purposes: Expose data at API endpoints of the client sites Scrape the data from the client sites from the dashboard site, cache it in the database, and display it in a dashboard. The WordPress REST API is the Backbone The WordPress REST API is my favorite part of WordPress. Out of the box, WordPress exposes default WordPress stuff like posts, authors, comments, media files, etc., via the WordPress REST API. You can see an example of this by navigating to /wp-json from any WordPress site, including CSS-Tricks. Here’s the REST API root for the WordPress Developer Resources site: The root URL for the WordPress REST API exposes structured JSON data, such as this example from the WordPress Developer Resources website. What’s so great about this? WordPress ships with everything developers need to extend the WordPress REST API and publish custom endpoints. Exposing data via an API endpoint is a fantastic way to share it with other websites that need to consume it, and that’s exactly what I did: Open the code \u003c?php [...] function register(\\WP_REST_Server $server) { $endpoints = $this-\u003eget(); foreach ($endpoints as $endpoint_slug =\u003e $endpoint) { register_rest_route( $endpoint['namespace'], $endpoint['route'], $endpoint['args'] ); } } function get() { $version = 'v1'; return array( 'empty_db' =\u003e array( 'namespace' =\u003e 'LXB_DBA/' . $version, 'route' =\u003e '/empty_db', 'args' =\u003e array( 'methods' =\u003e array( 'DELETE' ), 'callback' =\u003e array($this, 'empty_db_cb'), 'permission_callback' =\u003e array( $this, 'is_admin' ), ), ), 'get_blogs' =\u003e array( 'namespace' =\u003e 'LXB_DBA/' . $version, 'route' =\u003e '/get_blogs', 'args' =\u003e array( 'methods' =\u003e array('GET', 'OPTIONS'), 'callback' =\u003e array($this, 'get_blogs_cb'), 'permission_callback' =\u003e array($this, 'is_dba'), ), ), 'insert_blogs' =\u003e array( 'namespace' =\u003e 'LXB_DBA/' . $version, 'route' =\u003e '/insert_blogs', 'args' =\u003e array( 'methods' =\u003e array( 'POST' ), 'callback' =\u003e array($this, 'insert_blogs_cb'), 'permission_callback' =\u003e array( $this, 'is_admin' ), ), ), 'get_blogs_from_db' =\u003e array( 'namespace' =\u003e 'LXB_DBA/' . $version, 'route' =\u003e '/get_blogs_from_db', 'args' =\u003e array( 'methods' =\u003e array( 'GET' ), 'callback' =\u003e array($this, 'get_blogs_from_db_cb'), 'permission_callback' =\u003e array($this, 'is_admin'), ), ), 'get_blog_details' =\u003e array( 'namespace' =\u003e 'LXB_DBA/' . $version, 'route' =\u003e '/get_blog_details', 'args' =\u003e array( 'methods' =\u003e array( 'GET' ), 'callback' =\u003e array($this, 'get_blog_details_cb'), 'permission_callback' =\u003e array($this, 'is_dba'), ), ), 'update_blogs' =\u003e array( 'namespace' =\u003e 'LXB_DBA/' . $version, 'route' =\u003e '/update_blogs', 'args' =\u003e array( 'methods' =\u003e array( 'PATCH' ), 'callback' =\u003e array($this, 'update_blogs_cb'), 'permission_callback' =\u003e array($this, 'is_admin'), ), ), ); } We don’t need to get into every endpoint’s details, but I want to highlight one thing. First, I provided a function that returns all my endpoints in an array. Next, I wrote a function to loop through the array and register each array member as a WordPress REST API endpoint. Rather than doing both steps in one function, this decoupling allows me to easily retrieve the array of endpoints in other parts of my plugin to do other interesting things with them, such as exposing them to JavaScript. More on that shortly. Once registered, the custom API endpoints are observable in an ordinary web browser like in the example above, or via purpose-built tools for API work, such as Postman: PHP vs. JavaScript I tend to prefer writing applications in PHP whenever possible, as opposed to JavaScript, and executing logic on the server, as nature intended, rather than in the browser. So, what would that look like on this project? On the dashboard site, upon some event, such as the user clicking a “refresh data” button or perhaps a cron job, the server would make an HTTP request to each of the 25 multisite installs. Each multisite install would query all of its blogs and consolidate its analytics data into one response per multisite. Unfortunately, this strategy falls apart for a couple of reasons: PHP operates synchronously, meaning you wait for one line of code to execute before moving to the next. This means that we’d be waiting for all 25 multisites to respond in series. That’s sub-optimal. My production environment has a max execution limit of 60 seconds, and some of my multisites contain hundreds of blogs. Querying their analytics data takes a second or two per blog. Damn. I had no choice but to swallow hard and commit to writing the application logic in JavaScript. Not my favorite, but an eerily elegant solution for this case: Due to the asynchronous nature of JavaScript, it pings all 25 Multisites at once. The endpoint on each Multisite returns a list of all the blogs on that Multisite. The JavaScript compiles that list of blogs and (sort of) pings all 900 at once. All 900 blogs take about one-to-two seconds to respond concurrently. Holy cow, it just went from this: ( 1 second per Multisite * 25 installs ) + ( 1 second per blog * 900 blogs ) = roughly 925 seconds to scrape all the data. To this: 1 second for all the Multisites at once + 1 second for all 900 blogs at once = roughly 2 seconds to scrape all the data. That is, in theory. In practice, two factors enforce a delay: Browsers have a limit as to how many concurrent HTTP requests they will allow, both per domain and regardless of domain. I’m having trouble finding documentation on what those limits are. Based on observing the network panel in Chrome while working on this, I’d say it’s about 50-100. Web hosts have a limit on how many requests they can handle within a given period, both per IP address and overall. I was frequently getting a “429; Too Many Requests” response from my production environment, so I introduced a delay of 150 milliseconds between requests. They still operate concurrently, it’s just that they’re forced to wait 150ms per blog. Maybe “stagger” is a better word than “wait” in this context: Open the code async function getBlogsDetails(blogs) { let promises = []; // Iterate and set timeouts to stagger requests by 100ms each blogs.forEach((blog, index) =\u003e { if (typeof blog.url === 'undefined') { return; } let id = blog.id; const url = blog.url + '/' + blogDetailsEnpointPath + '?uncache=' + getRandomInt(); // Create a promise that resolves after 150ms delay per blog index const delayedPromise = new Promise(resolve =\u003e { setTimeout(async () =\u003e { try { const blogResult = await fetchBlogDetails(url, id); if( typeof blogResult.urls == 'undefined' ) { console.error( url, id, blogResult ); } else if( ! blogResult.urls ) { console.error( blogResult ); } else if( blogResult.urls.length == 0 ) { console.error( blogResult ); } else { console.log( blogResult ); } resolve(blogResult); } catch (error) { console.error(`Error fetching details for blog ID ${id}:`, error); resolve(null); // Resolve with null to handle errors gracefully } }, index * 150); // Offset each request by 100ms }); promises.push(delayedPromise); }); // Wait for all requests to complete const blogsResults = await Promise.all(promises); // Filter out any null results in case of caught errors return blogsResults.filter(result =\u003e result !== null); } With these limitations factored in, I found that it takes about 170 seconds to scrape all 900 blogs. This is acceptable because I cache the results, meaning the user only has to wait once at the start of each work session. The result of all this madness — this incredible barrage of Ajax calls, is just plain fun to watch: PHP and JavaScript: Connecting the dots I registered my endpoints in PHP and called them in JavaScript. Merging these two worlds is often an annoying and bug-prone part of any project. To make it as easy as possible, I use wp_localize_script(): \u003c?php [...] class Enqueue { function __construct() { add_action( 'admin_enqueue_scripts', array( $this, 'lexblog_network_analytics_script' ), 10 ); add_action( 'admin_enqueue_scripts', array( $this, 'lexblog_network_analytics_localize' ), 11 ); } function lexblog_network_analytics_script() { wp_register_script( 'lexblog_network_analytics_script', LXB_DBA_URL . '/js/lexblog_network_analytics.js', array( 'jquery', 'jquery-ui-autocomplete' ), false, false ); } function lexblog_network_analytics_localize() { $a = new LexblogNetworkAnalytics; $data = $a -\u003e get_localization_data(); $slug = $a -\u003e get_slug(); wp_localize_script( 'lexblog_network_analytics_script', $slug, $data ); } // etc. } In that script, I’m telling WordPress two things: Load my JavaScript file. When you do, take my endpoint URLs, bundle them up as JSON, and inject them into the HTML document as a global variable for my JavaScript to read. This is leveraging the point I noted earlier where I took care to provide a convenient function for defining the endpoint URLs, which other functions can then invoke without fear of causing any side effects. Here’s how that ended up looking: The JSON and its associated JavaScript file, where I pass information from PHP to JavaScript using wp_localize_script(). Auth: Fort Knox or Sandbox? We need to talk about authentication. To what degree do these endpoints need to be protected by server-side logic? Although exposing analytics data is not nearly as sensitive as, say, user passwords, I’d prefer to keep things reasonably locked up. Also, since some of these endpoints perform a lot of database queries and Google Analytics API calls, it’d be weird to sit here and be vulnerable to weirdos who might want to overload my database or Google Analytics rate limits. That’s why I registered an application password on each of the 25 client sites. Using an app password in php is quite simple. You can authenticate the HTTP requests just like any basic authentication scheme. I’m using JavaScript, so I had to localize them first, as described in the previous section. With that in place, I was able to append these credentials when making an Ajax call: async function fetchBlogsOfInstall(url, id) { let install = lexblog_network_analytics.installs[id]; let pw = install.pw; let user = install.user; // Create a Basic Auth token let token = btoa(`${user}:${pw}`); let auth = { 'Authorization': `Basic ${token}` }; try { let data = await $.ajax({ url: url, method: 'GET', dataType: 'json', headers: auth }); return data; } catch (error) { console.error('Request failed:', error); return []; } } That file uses this cool function called btoa() for turning the raw username and password combo into basic authentication. The part where we say, “Oh Right, CORS.” Whenever I have a project where Ajax calls are flying around all over the place, working reasonably well in my local environment, I always have a brief moment of panic when I try it on a real website, only to get errors like this: Oh. Right. CORS. Most reasonably secure websites do not allow other websites to make arbitrary Ajax requests. In this project, I absolutely do need the Dashboard Site to make many Ajax calls to the 25 client sites, so I have to tell the client sites to allow CORS: \u003c?php // ... function __construct() { add_action( 'rest_api_init', array( $this, 'maybe_add_cors_headers' ), 10 ); } function maybe_add_cors_headers() { // Only allow CORS for the endpoints that pertain to this plugin. if( $this-\u003eis_dba() ) { add_filter( 'rest_pre_serve_request', array( $this, 'send_cors_headers' ), 10, 2 ); } } function is_dba() { $url = $this-\u003eget_current_url(); $ep_urls = $this-\u003eget_endpoint_urls(); $out = in_array( $url, $ep_urls ); return $out; } function send_cors_headers( $served, $result ) { // Only allow CORS from the dashboard site. $dashboard_site_url = $this-\u003eget_dashboard_site_url(); header( \"Access-Control-Allow-Origin: $dashboard_site_url\" ); header( 'Access-Control-Allow-Headers: Origin, X-Requested-With, Content-Type, Accept, Authorization' ); header( 'Access-Control-Allow-Methods: GET, OPTIONS' ); return $served; } [...] } You’ll note that I’m following the principle of least privilege by taking steps to only allow CORS where it’s necessary. Auth, Part 2: I’ve been known to auth myself I authenticated an Ajax call from the dashboard site to the client sites. I registered some logic on all the client sites to allow the request to pass CORS. But then, back on the dashboard site, I had to get that response from the browser to the server. The answer, again, was to make an Ajax call to the WordPress REST API endpoint for storing the data. But since this was an actual database write, not merely a read, it was more important than ever to authenticate. I did this by requiring that the current user be logged into WordPress and possess sufficient privileges. But how would the browser know about this? In PHP, when registering our endpoints, we provide a permissions callback to make sure the current user is an admin: \u003c?php // ... function get() { $version = 'v1'; return array( 'update_blogs' =\u003e array( 'namespace' =\u003e 'LXB_DBA/' . $version, 'route' =\u003e '/update_blogs', 'args' =\u003e array( 'methods' =\u003e array( 'PATCH' ), 'callback' =\u003e array( $this, 'update_blogs_cb' ), 'permission_callback' =\u003e array( $this, 'is_admin' ), ), ), // ... ); } function is_admin() { $out = current_user_can( 'update_core' ); return $out; } JavaScript can use this — it’s able to identify the current user — because, once again, that data is localized. The current user is represented by their nonce: async function insertBlog( data ) { let url = lexblog_network_analytics.endpoint_urls.insert_blog; try { await $.ajax({ url: url, method: 'POST', dataType: 'json', data: data, headers: { 'X-WP-Nonce': getNonce() } }); } catch (error) { console.error('Failed to store blogs:', error); } } function getNonce() { if( typeof wpApiSettings.nonce == 'undefined' ) { return false; } return wpApiSettings.nonce; } The wpApiSettings.nonce global variable is automatically present in all WordPress admin screens. I didn’t have to localize that. WordPress core did it for me. Cache is King Compressing the Google Analytics data from 900 domains into a three-minute loading .gif is decent, but it would be totally unacceptable to have to wait for that long multiple times per work session. Therefore I cache the results of all 25 client sites in the database of the dashboard site. I’ve written before about using the WordPress Transients API for caching data, and I could have used it on this project. However, something about the tremendous volume of data and the complexity implied within the Figma design made me consider a different approach. I like the saying, “The wider the base, the higher the peak,” and it applies here. Given that the user needs to query and sort the data by date, author, and metadata, I think stashing everything into a single database cell — which is what a transient is — would feel a little claustrophobic. Instead, I dialed up E.F. Codd and used a relational database model via custom tables: In the Dashboard Site, I created seven custom database tables, including one relational table, to cache the data from the 25 client sites, as shown in the image. It’s been years since I’ve paged through Larry Ullman’s career-defining (as in, my career) books on database design, but I came into this project with a general idea of what a good architecture would look like. As for the specific details — things like column types — I foresaw a lot of Stack Overflow time in my future. Fortunately, LLMs love MySQL and I was able to scaffold out my requirements using DocBlocks and let Sam Altman fill in the blanks: Open the code \u003c?php /** * Provides the SQL code for creating the Blogs table. It has columns for: * - ID: The ID for the blog. This should just autoincrement and is the primary key. * - name: The name of the blog. Required. * - slug: A machine-friendly version of the blog name. Required. * - url: The url of the blog. Required. * - mapped_domain: The vanity domain name of the blog. Optional. * - install: The name of the Multisite install where this blog was scraped from. Required. * - registered: The date on which this blog began publishing posts. Optional. * - firm_id: The ID of the firm that publishes this blog. This will be used as a foreign key to relate to the Firms table. Optional. * - practice_area_id: The ID of the firm that publishes this blog. This will be used as a foreign key to relate to the PracticeAreas table. Optional. * - amlaw: Either a 0 or a 1, to indicate if the blog comes from an AmLaw firm. Required. * - subscriber_count: The number of email subscribers for this blog. Optional. * - day_view_count: The number of views for this blog today. Optional. * - week_view_count: The number of views for this blog this week. Optional. * - month_view_count: The number of views for this blog this month. Optional. * - year_view_count: The number of views for this blog this year. Optional. * * @return string The SQL for generating the blogs table. */ function get_blogs_table_sql() { $slug = 'blogs'; $out = \"CREATE TABLE {$this-\u003eget_prefix()}_$slug ( id BIGINT NOT NULL AUTO_INCREMENT, slug VARCHAR(255) NOT NULL, name VARCHAR(255) NOT NULL, url VARCHAR(255) NOT NULL UNIQUE, /* adding unique constraint */ mapped_domain VARCHAR(255) UNIQUE, install VARCHAR(255) NOT NULL, registered DATE DEFAULT NULL, firm_id BIGINT, practice_area_id BIGINT, amlaw TINYINT NOT NULL, subscriber_count BIGINT, day_view_count BIGINT, week_view_count BIGINT, month_view_count BIGINT, year_view_count BIGINT, PRIMARY KEY (id), FOREIGN KEY (firm_id) REFERENCES {$this-\u003eget_prefix()}_firms(id), FOREIGN KEY (practice_area_id) REFERENCES {$this-\u003eget_prefix()}_practice_areas(id) ) DEFAULT CHARSET=utf8mb4;\"; return $out; } In that file, I quickly wrote a DocBlock for each function, and let the OpenAI playground spit out the SQL. I tested the result and suggested some rigorous type-checking for values that should always be formatted as numbers or dates, but that was the only adjustment I had to make. I think that’s the correct use of AI at this moment: You come in with a strong idea of what the result should be, AI fills in the details, and you debate with it until the details reflect what you mostly already knew. How it’s going I’ve implemented most of the user stories now. Certainly enough to release an MVP and begin gathering whatever insights this data might have for us: It’s working! One interesting data point thus far: Although all the blogs are on the topic of legal matters (they are lawyer blogs, after all), blogs that cover topics with a more general appeal seem to drive more traffic. Blogs about the law as it pertains to food, cruise ships, germs, and cannabis, for example. Furthermore, the largest law firms on our network don’t seem to have much of a foothold there. Smaller firms are doing a better job of connecting with a wider audience. I’m positive that other insights will emerge as we work more deeply with this. Regrets? I’ve had a few. This project probably would have been a nice opportunity to apply a modern JavaScript framework, or just no framework at all. I like React and I can imagine how cool it would be to have this application be driven by the various changes in state rather than… drumroll… a couple thousand lines of jQuery! I like jQuery’s ajax() method, and I like the jQueryUI autocomplete component. Also, there’s less of a performance concern here than on a public-facing front-end. Since this screen is in the WordPress admin area, I’m not concerned about Google admonishing me for using an extra library. And I’m just faster with jQuery. Use whatever you want. I also think it would be interesting to put AWS to work here and see what could be done through Lambda functions. Maybe I could get Lambda to make all 25 plus 900 requests concurrently with no worries about browser limitations. Heck, maybe I could get it to cycle through IP addresses and sidestep the 429 rate limit as well. And what about cron? Cron could do a lot of work for us here. It could compile the data on each of the 25 client sites ahead of time, meaning that the initial three-minute refresh time goes away. Writing an application in cron, initially, I think is fine. Coming back six months later to debug something is another matter. Not my favorite. I might revisit this later on, but for now, the cron-free implementation meets the MVP goal. I have not provided a line-by-line tutorial here, or even a working repo for you to download, and that level of detail was never my intention. I wanted to share high-level strategy decisions that might be of interest to fellow Multi-Multisite people. Have you faced a similar challenge? I’d love to hear about it in the comments!",
  "image": "https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/wp-multisite.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n          \n          \n\u003cp\u003e\u003cstrong\u003eThe mission:\u003c/strong\u003e Provide a dashboard within the WordPress admin area for browsing Google Analytics data for all your blogs.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eThe catch?\u003c/strong\u003e You’ve got about 900 live blogs, spread across about 25 WordPress multisite instances. Some instances have just one blog, others have as many as 250. In other words, what you need is to compress a data set that normally takes a very long time to compile into a single user-friendly screen.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe implementation details are entirely up to you, but the final result should look like this Figma comp:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" data-recalc-dims=\"1\" fetchpriority=\"high\" decoding=\"async\" width=\"1600\" height=\"1260\" src=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-2.png?resize=1600%2C1260\u0026amp;ssl=1\" alt=\"\" srcset=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-2.png?w=1600\u0026amp;ssl=1 1600w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-2.png?resize=300%2C236\u0026amp;ssl=1 300w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-2.png?resize=1024%2C806\u0026amp;ssl=1 1024w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-2.png?resize=768%2C605\u0026amp;ssl=1 768w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-2.png?resize=1536%2C1210\u0026amp;ssl=1 1536w\" sizes=\"(min-width: 735px) 864px, 96vw\"/\u003e\u003cfigcaption\u003eDesign courtesy of the incomparable \u003ca href=\"https://www.biddlebrain.com\" rel=\"noopener\"\u003eBrian Biddle\u003c/a\u003e.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eI want to walk you through my approach and some of the interesting challenges I faced coming up with it, as well as the occasional nitty-gritty detail in between. I’ll cover topics like the WordPress REST API, choosing between a JavaScript or PHP approach, rate/time limits in production web environments, security, custom database design — and even a touch of AI. But first, a little orientation.\u003c/p\u003e\n\n\n\u003ch3 id=\"let-s-define-some-terms\"\u003eLet’s define some terms\u003c/h3\u003e\n\n\n\u003cp\u003eWe’re about to cover a lot of ground, so it’s worth spending a couple of moments reviewing some key terms we’ll be using throughout this post.\u003c/p\u003e\n\n\n\u003ch4 id=\"what-is-wordpress-multisite-\"\u003eWhat is WordPress multisite?\u003c/h4\u003e\n\n\n\u003cp\u003e\u003ca href=\"https://developer.wordpress.org/advanced-administration/multisite/\" rel=\"noopener\"\u003eWordPress Multisite\u003c/a\u003e is a feature of WordPress core — no plugins required — whereby you can run multiple blogs (or websites, or stores, or what have you) from a single WordPress installation. All the blogs share the same WordPress core files, wp-content folder, and MySQL database. However, each blog gets its own folder within wp-content/uploads for its uploaded media, and its own set of database tables for its posts, categories, options, etc. Users can be members of some or all blogs within the multisite installation.\u003c/p\u003e\n\n\n\u003ch4 id=\"what-is-wordpress-multi-multisite-\"\u003eWhat is WordPress \u003cem\u003emulti-\u003c/em\u003emultisite?\u003c/h4\u003e\n\n\n\u003cp\u003eIt’s just a nickname for managing multiple instances of WordPress multisite. It can get messy to have different customers share one multisite instance, so I prefer to break it up so that each customer has their own multisite, but they can have many blogs within their multisite.\u003c/p\u003e\n\n\n\u003ch4 id=\"so-that-s-different-from-a-network-of-networks-\"\u003eSo that’s different from a “Network of Networks”?\u003c/h4\u003e\n\n\n\u003cp\u003eIt’s apparently possible to run multiple instances of WordPress multisite against the same WordPress core installation. I’ve never looked into this, but I recall hearing about it over the years. I’ve heard the term “Network of Networks” and I like it, but that is not the scenario I’m covering in this article.\u003c/p\u003e\n\n\n\u003ch4 id=\"why-do-you-keep-saying-blogs-do-people-still-blog-\"\u003eWhy do you keep saying “blogs”? Do people still blog?\u003c/h4\u003e\n\n\n\u003cp\u003eYou betcha! And people read them, too. You’re reading one right now. Hence, the need for a robust analytics solution. But this article could just as easily be about any sort of WordPress site. I happen to be dealing with blogs, and the word “blog” is a concise way to express “a subsite within a WordPress multisite instance”.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne more thing: In this article, I’ll use the term \u003cstrong\u003edashboard site\u003c/strong\u003e to refer to the site from which I observe the compiled analytics data. I’ll use the term \u003cstrong\u003eclient sites\u003c/strong\u003e to refer to the 25 multisites I pull data from.\u003c/p\u003e\n\n\n\u003ch3 id=\"my-implementation\"\u003eMy implementation\u003c/h3\u003e\n\n\n\u003cp\u003eMy strategy was to write one WordPress plugin that is installed on all 25 client sites, as well as on the dashboard site. The plugin serves two purposes:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eExpose data at API endpoints of the client sites\u003c/li\u003e\n\n\n\n\u003cli\u003eScrape the data from the client sites from the dashboard site, cache it in the database, and display it in a dashboard.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ch4 id=\"the-wordpress-rest-api-is-the-backbone\"\u003eThe WordPress REST API is the Backbone\u003c/h4\u003e\n\n\n\u003cp\u003eThe WordPress REST API is my favorite part of WordPress. Out of the box, WordPress exposes default WordPress \u003cem\u003estuff\u003c/em\u003e like posts, authors, comments, media files, etc., via the WordPress REST API. You can see an example of this by navigating to \u003ccode\u003e/wp-json\u003c/code\u003e from any WordPress site, \u003ca href=\"https://css-tricks.com/wp-json\"\u003eincluding CSS-Tricks\u003c/a\u003e. Here’s the REST API root for the \u003ca href=\"https://developer.wordpress.org\" rel=\"noopener\"\u003eWordPress Developer Resources site\u003c/a\u003e:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" data-recalc-dims=\"1\" decoding=\"async\" width=\"1536\" height=\"1296\" src=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed.png?resize=1536%2C1296\u0026amp;ssl=1\" alt=\"\" srcset=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed.png?w=1536\u0026amp;ssl=1 1536w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed.png?resize=300%2C253\u0026amp;ssl=1 300w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed.png?resize=1024%2C864\u0026amp;ssl=1 1024w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed.png?resize=768%2C648\u0026amp;ssl=1 768w\" sizes=\"(min-width: 735px) 864px, 96vw\"/\u003e\u003cfigcaption\u003eThe root URL for the WordPress REST API exposes structured JSON data, such as this example from the WordPress Developer Resources website.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhat’s so great about this? WordPress ships with everything developers need to extend the WordPress REST API and publish \u003ca href=\"https://developer.wordpress.org/rest-api/extending-the-rest-api/adding-custom-endpoints/\" rel=\"noopener\"\u003ecustom endpoints\u003c/a\u003e. Exposing data via an API endpoint is a fantastic way to share it with other websites that need to consume it, and that’s exactly what I did:\u003c/p\u003e\n\n\n\n\u003cdetails\u003e\n  \u003csummary\u003e\n          Open the code      \u003c/summary\u003e\n  \n\n\u003cpre rel=\"PHP\" data-line=\"\"\u003e\u003ccode markup=\"tt\"\u003e\n\u0026lt;?php\n\n[...]\n\nfunction register(\\WP_REST_Server $server) {\n  $endpoints = $this-\u0026gt;get();\n\n  foreach ($endpoints as $endpoint_slug =\u0026gt; $endpoint) {\n    register_rest_route(\n      $endpoint[\u0026#39;namespace\u0026#39;],\n      $endpoint[\u0026#39;route\u0026#39;],\n      $endpoint[\u0026#39;args\u0026#39;]\n    );\n  }\n}\n\nfunction get() {\n\n  $version = \u0026#39;v1\u0026#39;;\n\n  return array(\n      \n    \u0026#39;empty_db\u0026#39; =\u0026gt; array(\n      \u0026#39;namespace\u0026#39; =\u0026gt; \u0026#39;LXB_DBA/\u0026#39; . $version,\n      \u0026#39;route\u0026#39;     =\u0026gt; \u0026#39;/empty_db\u0026#39;,\n      \u0026#39;args\u0026#39;      =\u0026gt; array(\n        \u0026#39;methods\u0026#39; =\u0026gt; array( \u0026#39;DELETE\u0026#39; ),\n        \u0026#39;callback\u0026#39; =\u0026gt; array($this, \u0026#39;empty_db_cb\u0026#39;),\n        \u0026#39;permission_callback\u0026#39; =\u0026gt; array( $this, \u0026#39;is_admin\u0026#39; ),\n      ),\n    ),\n\n    \u0026#39;get_blogs\u0026#39; =\u0026gt; array(\n      \u0026#39;namespace\u0026#39; =\u0026gt; \u0026#39;LXB_DBA/\u0026#39; . $version,\n      \u0026#39;route\u0026#39;     =\u0026gt; \u0026#39;/get_blogs\u0026#39;,\n      \u0026#39;args\u0026#39;      =\u0026gt; array(\n        \u0026#39;methods\u0026#39; =\u0026gt; array(\u0026#39;GET\u0026#39;, \u0026#39;OPTIONS\u0026#39;),\n        \u0026#39;callback\u0026#39; =\u0026gt; array($this, \u0026#39;get_blogs_cb\u0026#39;),\n        \u0026#39;permission_callback\u0026#39; =\u0026gt; array($this, \u0026#39;is_dba\u0026#39;),\n      ),\n    ),\n\n    \u0026#39;insert_blogs\u0026#39; =\u0026gt; array(\n      \u0026#39;namespace\u0026#39; =\u0026gt; \u0026#39;LXB_DBA/\u0026#39; . $version,\n      \u0026#39;route\u0026#39;     =\u0026gt; \u0026#39;/insert_blogs\u0026#39;,\n      \u0026#39;args\u0026#39;      =\u0026gt; array(\n        \u0026#39;methods\u0026#39; =\u0026gt; array( \u0026#39;POST\u0026#39; ),\n        \u0026#39;callback\u0026#39; =\u0026gt; array($this, \u0026#39;insert_blogs_cb\u0026#39;),\n        \u0026#39;permission_callback\u0026#39; =\u0026gt; array( $this, \u0026#39;is_admin\u0026#39; ),\n      ),\n    ),\n\n    \u0026#39;get_blogs_from_db\u0026#39; =\u0026gt; array(\n      \u0026#39;namespace\u0026#39; =\u0026gt; \u0026#39;LXB_DBA/\u0026#39; . $version,\n      \u0026#39;route\u0026#39;     =\u0026gt; \u0026#39;/get_blogs_from_db\u0026#39;,\n      \u0026#39;args\u0026#39;      =\u0026gt; array(\n        \u0026#39;methods\u0026#39; =\u0026gt; array( \u0026#39;GET\u0026#39; ),\n        \u0026#39;callback\u0026#39; =\u0026gt; array($this, \u0026#39;get_blogs_from_db_cb\u0026#39;),\n        \u0026#39;permission_callback\u0026#39; =\u0026gt; array($this, \u0026#39;is_admin\u0026#39;),\n      ),\n    ),  \n\n    \u0026#39;get_blog_details\u0026#39; =\u0026gt; array(\n      \u0026#39;namespace\u0026#39; =\u0026gt; \u0026#39;LXB_DBA/\u0026#39; . $version,\n      \u0026#39;route\u0026#39;     =\u0026gt; \u0026#39;/get_blog_details\u0026#39;,\n      \u0026#39;args\u0026#39;      =\u0026gt; array(\n        \u0026#39;methods\u0026#39; =\u0026gt; array( \u0026#39;GET\u0026#39; ),\n        \u0026#39;callback\u0026#39; =\u0026gt; array($this, \u0026#39;get_blog_details_cb\u0026#39;),\n        \u0026#39;permission_callback\u0026#39; =\u0026gt; array($this, \u0026#39;is_dba\u0026#39;),\n      ),\n    ),   \n\n    \u0026#39;update_blogs\u0026#39; =\u0026gt; array(\n      \u0026#39;namespace\u0026#39; =\u0026gt; \u0026#39;LXB_DBA/\u0026#39; . $version,\n      \u0026#39;route\u0026#39;     =\u0026gt; \u0026#39;/update_blogs\u0026#39;,\n      \u0026#39;args\u0026#39;      =\u0026gt; array(\n        \u0026#39;methods\u0026#39; =\u0026gt; array( \u0026#39;PATCH\u0026#39; ),\n        \u0026#39;callback\u0026#39; =\u0026gt; array($this, \u0026#39;update_blogs_cb\u0026#39;),\n        \u0026#39;permission_callback\u0026#39; =\u0026gt; array($this, \u0026#39;is_admin\u0026#39;),\n      ),\n    ),     \n\n  );\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\u003c/details\u003e\n\n\n\u003cp\u003eWe don’t need to get into every endpoint’s details, but I want to highlight one thing. First, I provided a function that returns all my endpoints in an array. Next, I wrote a function to loop through the array and register each array member as a WordPress REST API endpoint. Rather than doing both steps in one function, this decoupling allows me to easily retrieve the array of endpoints in other parts of my plugin to do other interesting things with them, such as exposing them to JavaScript. More on that shortly.\u003c/p\u003e\n\n\n\n\u003cp\u003eOnce registered, the custom API endpoints are observable in an ordinary web browser like in the example above, or via purpose-built tools for API work, such as \u003ca href=\"https://www.postman.com/\" rel=\"noopener\"\u003ePostman\u003c/a\u003e:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" width=\"1490\" height=\"1600\" src=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-3.png?resize=1490%2C1600\u0026amp;ssl=1\" alt=\"JSON output.\" srcset=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-3.png?w=1490\u0026amp;ssl=1 1490w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-3.png?resize=279%2C300\u0026amp;ssl=1 279w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-3.png?resize=954%2C1024\u0026amp;ssl=1 954w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-3.png?resize=768%2C825\u0026amp;ssl=1 768w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-3.png?resize=1430%2C1536\u0026amp;ssl=1 1430w\" sizes=\"auto, (min-width: 735px) 864px, 96vw\"/\u003e\u003c/figure\u003e\n\n\n\u003ch4 id=\"php-vs-javascript\"\u003ePHP vs. JavaScript\u003c/h4\u003e\n\n\n\u003cp\u003eI tend to prefer writing applications in PHP whenever possible, as opposed to JavaScript, and executing logic on the server, as nature intended, rather than in the browser. So, what would that look like on this project?\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eOn the dashboard site, upon some event, such as the user clicking a “refresh data” button or perhaps a cron job, the server would make an HTTP request to each of the 25 multisite installs.\u003c/li\u003e\n\n\n\n\u003cli\u003eEach multisite install would query all of its blogs and consolidate its analytics data into one response per multisite.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eUnfortunately, this strategy falls apart for a couple of reasons:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003ePHP operates synchronously, meaning you wait for one line of code to execute before moving to the next. This means that we’d be waiting for all 25 multisites to respond in series. That’s sub-optimal.\u003c/li\u003e\n\n\n\n\u003cli\u003eMy production environment has a max execution limit of 60 seconds, and some of my multisites contain hundreds of blogs. Querying their analytics data takes a second or two per blog.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eDamn. I had no choice but to swallow hard and commit to writing the application logic in JavaScript. Not my favorite, but an eerily elegant solution for this case:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eDue to the asynchronous nature of JavaScript, it pings all 25 Multisites at once.\u003c/li\u003e\n\n\n\n\u003cli\u003eThe endpoint on each Multisite returns a list of all the blogs on that Multisite.\u003c/li\u003e\n\n\n\n\u003cli\u003eThe JavaScript compiles that list of blogs and (sort of) pings all 900 at once.\u003c/li\u003e\n\n\n\n\u003cli\u003eAll 900 blogs take about one-to-two seconds to respond concurrently.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eHoly cow, it just went from this:\u003c/p\u003e\n\n\n\n\u003cpre rel=\"\" data-line=\"\"\u003e\u003ccode markup=\"tt\"\u003e( 1 second per Multisite * 25 installs ) + ( 1 second per blog * 900 blogs ) = roughly 925 seconds to scrape all the data.\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eTo this:\u003c/p\u003e\n\n\n\n\u003cpre rel=\"\" data-line=\"\"\u003e\u003ccode markup=\"tt\"\u003e1 second for all the Multisites at once + 1 second for all 900 blogs at once = roughly 2 seconds to scrape all the data.\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eThat is, in theory. In practice, two factors enforce a delay:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eBrowsers have a limit as to how many concurrent HTTP requests they will allow, both per domain and regardless of domain. I’m having trouble finding documentation on what those limits are. Based on observing the network panel in Chrome while working on this, I’d say it’s about 50-100.\u003c/li\u003e\n\n\n\n\u003cli\u003eWeb hosts have a limit on how many requests they can handle within a given period, both per IP address and overall. I was frequently getting a \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\" rel=\"noopener\"\u003e“429; Too Many Requests” response\u003c/a\u003e from my production environment, so I introduced a delay of 150 milliseconds between requests. They still operate concurrently, it’s just that they’re forced to wait 150ms per blog. Maybe “stagger” is a better word than “wait” in this context:\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cdetails\u003e\n  \u003csummary\u003e\n          Open the code      \u003c/summary\u003e\n  \n\n\u003cpre rel=\"JavaScript\" data-line=\"\"\u003e\u003ccode markup=\"tt\"\u003easync function getBlogsDetails(blogs) {\n  let promises = [];\n\n  // Iterate and set timeouts to stagger requests by 100ms each\n  blogs.forEach((blog, index) =\u0026gt; {\n    if (typeof blog.url === \u0026#39;undefined\u0026#39;) {\n      return;\n    }\n\n    let id = blog.id;\n    const url = blog.url + \u0026#39;/\u0026#39; + blogDetailsEnpointPath + \u0026#39;?uncache=\u0026#39; + getRandomInt();\n\n    // Create a promise that resolves after 150ms delay per blog index\n    const delayedPromise = new Promise(resolve =\u0026gt; {\n      setTimeout(async () =\u0026gt; {\n        try {\n          const blogResult = await fetchBlogDetails(url, id);\n                \n          if( typeof blogResult.urls == \u0026#39;undefined\u0026#39; ) {\n            console.error( url, id, blogResult );\n\n          } else if( ! blogResult.urls ) {\n            console.error( blogResult );\n                \n                \n          } else if( blogResult.urls.length == 0 ) {\n            console.error( blogResult );\n                \n          } else {\n            console.log( blogResult );\n          }\n                \n          resolve(blogResult);\n        } catch (error) {\n          console.error(`Error fetching details for blog ID ${id}:`, error);\n          resolve(null); // Resolve with null to handle errors gracefully\n      }\n    }, index * 150); // Offset each request by 100ms\n  });\n\n  promises.push(delayedPromise);\n});\n\n  // Wait for all requests to complete\n  const blogsResults = await Promise.all(promises);\n\n  // Filter out any null results in case of caught errors\n  return blogsResults.filter(result =\u0026gt; result !== null);\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\u003c/details\u003e\n\n\n\u003cp\u003eWith these limitations factored in, I found that it takes about 170 seconds to scrape all 900 blogs. This is acceptable because I cache the results, meaning the user only has to wait once at the start of each work session.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe result of all this madness — this incredible barrage of Ajax calls, is just plain fun to watch:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cvideo controls=\"\" src=\"https://css-tricks.com/wp-content/plugins/breeze/assets/images/placeholder.mp4\" data-breeze=\"https://css-tricks.com/wp-content/uploads/2024/11/2b6LXlN.mp4\" playsinline=\"\"\u003e\u003c/video\u003e\u003c/figure\u003e\n\n\n\u003ch4 id=\"php-and-javascript-connecting-the-dots\"\u003ePHP and JavaScript: Connecting the dots\u003c/h4\u003e\n\n\n\u003cp\u003eI registered my endpoints in PHP and called them in JavaScript. Merging these two worlds is often an annoying and bug-prone part of any project. To make it as easy as possible, I use \u003ccode\u003e\u003ca href=\"https://pippinsplugins.com/use-wp_localize_script-it-is-awesome/\" rel=\"noopener\"\u003ewp_localize_script()\u003c/a\u003e\u003c/code\u003e:\u003c/p\u003e\n\n\n\n\u003cpre rel=\"PHP\" data-line=\"\"\u003e\u003ccode markup=\"tt\"\u003e\u0026lt;?php\n\n[...]\n\nclass Enqueue {\n\n  function __construct() {\n    add_action( \u0026#39;admin_enqueue_scripts\u0026#39;, array( $this, \u0026#39;lexblog_network_analytics_script\u0026#39; ), 10 );\n    add_action( \u0026#39;admin_enqueue_scripts\u0026#39;, array( $this, \u0026#39;lexblog_network_analytics_localize\u0026#39; ), 11 );\n  }\n\n  function lexblog_network_analytics_script() {\n    wp_register_script( \u0026#39;lexblog_network_analytics_script\u0026#39;, LXB_DBA_URL . \u0026#39;/js/lexblog_network_analytics.js\u0026#39;, array( \u0026#39;jquery\u0026#39;, \u0026#39;jquery-ui-autocomplete\u0026#39; ), false, false );\n  }\n\n  function lexblog_network_analytics_localize() {\n    $a = new LexblogNetworkAnalytics;\n    $data = $a -\u0026gt; get_localization_data();\n    $slug = $a -\u0026gt; get_slug();\n\n    wp_localize_script( \u0026#39;lexblog_network_analytics_script\u0026#39;, $slug, $data );\n\n  }\n\n  // etc.              \n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eIn that script, I’m telling WordPress two things:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eLoad my JavaScript file.\u003c/li\u003e\n\n\n\n\u003cli\u003eWhen you do, take my endpoint URLs, bundle them up as JSON, and inject them into the HTML document as a global variable for my JavaScript to read. This is leveraging the point I noted earlier where I took care to provide a convenient function for defining the endpoint URLs, which other functions can then invoke without fear of causing any side effects.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eHere’s how that ended up looking:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" width=\"1600\" height=\"654\" src=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-4.png?resize=1600%2C654\u0026amp;ssl=1\" alt=\"\" srcset=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-4.png?w=1600\u0026amp;ssl=1 1600w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-4.png?resize=300%2C123\u0026amp;ssl=1 300w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-4.png?resize=1024%2C419\u0026amp;ssl=1 1024w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-4.png?resize=768%2C314\u0026amp;ssl=1 768w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-4.png?resize=1536%2C628\u0026amp;ssl=1 1536w\" sizes=\"auto, (min-width: 735px) 864px, 96vw\"/\u003e\u003cfigcaption\u003eThe JSON and its associated JavaScript file, where I pass information from PHP to JavaScript using \u003ccode\u003ewp_localize_script()\u003c/code\u003e.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\u003ch3 id=\"auth-fort-knox-or-sandbox-\"\u003eAuth: Fort Knox or Sandbox?\u003c/h3\u003e\n\n\n\u003cp\u003eWe need to talk about authentication. To what degree do these endpoints need to be protected by server-side logic? Although exposing analytics data is not nearly as sensitive as, say, user passwords, I’d prefer to keep things reasonably locked up. Also, since some of these endpoints perform a lot of database queries and Google Analytics API calls, it’d be weird to sit here and be vulnerable to weirdos who might want to overload my database or Google Analytics rate limits.\u003c/p\u003e\n\n\n\n\u003cp\u003eThat’s why I registered an \u003ca href=\"https://make.wordpress.org/core/2020/11/05/application-passwords-integration-guide/\" rel=\"noopener\"\u003eapplication password\u003c/a\u003e on each of the 25 client sites. Using an app password in php is quite simple. You can \u003ca href=\"https://developer.wordpress.org/apis/making-http-requests/authentication/\" rel=\"noopener\"\u003eauthenticate the HTTP requests\u003c/a\u003e just like any basic authentication scheme.\u003c/p\u003e\n\n\n\n\u003cp\u003eI’m using JavaScript, so I had to localize them first, as described in the previous section. With that in place, I was able to append these credentials when making an Ajax call:\u003c/p\u003e\n\n\n\n\u003cpre rel=\"JavaScript\" data-line=\"\"\u003e\u003ccode markup=\"tt\"\u003easync function fetchBlogsOfInstall(url, id) {\n  let install = lexblog_network_analytics.installs[id];\n  let pw = install.pw;\n  let user = install.user;\n\n  // Create a Basic Auth token\n  let token = btoa(`${user}:${pw}`);\n  let auth = {\n      \u0026#39;Authorization\u0026#39;: `Basic ${token}`\n  };\n\n  try {\n    let data = await $.ajax({\n        url: url,\n        method: \u0026#39;GET\u0026#39;,\n        dataType: \u0026#39;json\u0026#39;,\n        headers: auth\n    });\n\n    return data;\n\n  } catch (error) {\n    console.error(\u0026#39;Request failed:\u0026#39;, error);\n    return [];\n  }\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eThat file uses this cool function called \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/Window/btoa\" rel=\"noopener\"\u003e\u003ccode\u003ebtoa()\u003c/code\u003e\u003c/a\u003e for turning the raw username and password combo into basic authentication.\u003c/p\u003e\n\n\n\u003ch3 id=\"the-part-where-we-say-oh-right-cors-\"\u003eThe part where we say, “Oh Right, CORS.”\u003c/h3\u003e\n\n\n\u003cp\u003eWhenever I have a project where Ajax calls are flying around all over the place, working reasonably well in my local environment, I always have a brief moment of panic when I try it on a real website, only to get errors like this:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" width=\"1330\" height=\"146\" src=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-5.png?resize=1330%2C146\u0026amp;ssl=1\" alt=\"CORS console error.\" srcset=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-5.png?w=1330\u0026amp;ssl=1 1330w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-5.png?resize=300%2C33\u0026amp;ssl=1 300w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-5.png?resize=1024%2C112\u0026amp;ssl=1 1024w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-5.png?resize=768%2C84\u0026amp;ssl=1 768w\" sizes=\"auto, (min-width: 735px) 864px, 96vw\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eOh. Right. \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS\" rel=\"noopener\"\u003eCORS\u003c/a\u003e. Most reasonably secure websites do not allow other websites to make arbitrary Ajax requests. In this project, I absolutely do need the Dashboard Site to make many Ajax calls to the 25 client sites, so I have to tell the client sites to allow CORS:\u003c/p\u003e\n\n\n\n\u003cpre rel=\"PHP\" data-line=\"\"\u003e\u003ccode markup=\"tt\"\u003e\u0026lt;?php\n\n  // ...\n\n  function __construct() {\n  add_action( \u0026#39;rest_api_init\u0026#39;, array( $this, \u0026#39;maybe_add_cors_headers\u0026#39; ), 10 );\n}\n\nfunction maybe_add_cors_headers() {   \n  // Only allow CORS for the endpoints that pertain to this plugin.\n  if( $this-\u0026gt;is_dba() ) {\n      add_filter( \u0026#39;rest_pre_serve_request\u0026#39;, array( $this, \u0026#39;send_cors_headers\u0026#39; ), 10, 2 );\n  }\n}\n\nfunction is_dba() {\n  $url = $this-\u0026gt;get_current_url();\n  $ep_urls = $this-\u0026gt;get_endpoint_urls();\n  $out = in_array( $url, $ep_urls );\n          \n  return $out;\n          \n}\n\nfunction send_cors_headers( $served, $result ) {\n\n          // Only allow CORS from the dashboard site.\n  $dashboard_site_url = $this-\u0026gt;get_dashboard_site_url();\n\n  header( \u0026#34;Access-Control-Allow-Origin: $dashboard_site_url\u0026#34; );\n  header( \u0026#39;Access-Control-Allow-Headers: Origin, X-Requested-With, Content-Type, Accept, Authorization\u0026#39; );\n  header( \u0026#39;Access-Control-Allow-Methods: GET, OPTIONS\u0026#39; );\n  return $served;\n  \n}\n\n  [...]\n\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eYou’ll note that I’m following the \u003ca href=\"https://en.wikipedia.org/wiki/Principle_of_least_privilege\" rel=\"noopener\"\u003eprinciple of least privilege\u003c/a\u003e by taking steps to only allow CORS where it’s necessary.\u003c/p\u003e\n\n\n\u003ch3 id=\"auth-part-2-i-ve-been-known-to-auth-myself\"\u003eAuth, Part 2: I’ve been known to auth myself\u003c/h3\u003e\n\n\n\u003cp\u003eI authenticated an Ajax call from the dashboard site to the client sites. I registered some logic on all the client sites to allow the request to pass CORS. But then, back on the dashboard site, I had to get that response from the browser to the server.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe answer, again, was to make an Ajax call to the WordPress REST API endpoint for storing the data. But since this was an actual database write, not merely a read, it was more important than ever to authenticate. I did this by requiring that the current user be logged into WordPress and possess sufficient privileges. But how would the browser know about this?\u003c/p\u003e\n\n\n\n\u003cp\u003eIn PHP, when registering our endpoints, we provide a \u003ca href=\"https://developer.wordpress.org/rest-api/extending-the-rest-api/adding-custom-endpoints/#permissions-callback\" rel=\"noopener\"\u003epermissions callback\u003c/a\u003e to make sure the current user is an admin:\u003c/p\u003e\n\n\n\n\u003cpre rel=\"PHP\" data-line=\"\"\u003e\u003ccode markup=\"tt\"\u003e\u0026lt;?php\n\n// ...\n\nfunction get() {\n  $version = \u0026#39;v1\u0026#39;;\n  return array(\n\n    \u0026#39;update_blogs\u0026#39; =\u0026gt; array(\n      \u0026#39;namespace\u0026#39; =\u0026gt; \u0026#39;LXB_DBA/\u0026#39; . $version,\n      \u0026#39;route\u0026#39;     =\u0026gt; \u0026#39;/update_blogs\u0026#39;,\n      \u0026#39;args\u0026#39;      =\u0026gt; array(\n        \u0026#39;methods\u0026#39; =\u0026gt; array( \u0026#39;PATCH\u0026#39; ),\n        \u0026#39;callback\u0026#39; =\u0026gt; array( $this, \u0026#39;update_blogs_cb\u0026#39; ),\n        \u0026#39;permission_callback\u0026#39; =\u0026gt; array( $this, \u0026#39;is_admin\u0026#39; ),\n        ),\n      ),\n      // ...\n    );           \n  }\n                      \nfunction is_admin() {\n    $out = current_user_can( \u0026#39;update_core\u0026#39; );\n    return $out;\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eJavaScript can use this — it’s able to identify the current user — because, once again, that data is localized. The current user is represented by their \u003ca href=\"https://codex.wordpress.org/WordPress_Nonces\" rel=\"noopener\"\u003enonce\u003c/a\u003e:\u003c/p\u003e\n\n\n\n\u003cpre rel=\"JavaScript\" data-line=\"\"\u003e\u003ccode markup=\"tt\"\u003easync function insertBlog( data ) {\n    \n  let url = lexblog_network_analytics.endpoint_urls.insert_blog;\n\n  try {\n    await $.ajax({\n      url: url,\n      method: \u0026#39;POST\u0026#39;,\n      dataType: \u0026#39;json\u0026#39;,\n      data: data,\n      headers: {\n        \u0026#39;X-WP-Nonce\u0026#39;: getNonce()\n      }\n    });\n  } catch (error) {\n    console.error(\u0026#39;Failed to store blogs:\u0026#39;, error);\n  }\n}\n\nfunction getNonce() {\n  if( typeof wpApiSettings.nonce == \u0026#39;undefined\u0026#39; ) { return false; }\n  return wpApiSettings.nonce;\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eThe \u003ccode\u003ewpApiSettings.nonce\u003c/code\u003e global variable is automatically present in all WordPress admin screens. I didn’t have to localize that. WordPress core did it for me.\u003c/p\u003e\n\n\n\u003ch3 id=\"cache-is-king\"\u003eCache is King\u003c/h3\u003e\n\n\n\u003cp\u003eCompressing the Google Analytics data from 900 domains into a three-minute loading \u003ccode\u003e.gif\u003c/code\u003e is decent, but it would be totally unacceptable to have to wait for that long multiple times per work session. Therefore I cache the results of all 25 client sites in the database of the dashboard site.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://css-tricks.com/the-deal-with-wordpress-transients/\"\u003eI’ve written before about using the WordPress Transients API\u003c/a\u003e for caching data, and I could have used it on this project. However, something about the tremendous volume of data and the complexity implied within the Figma design made me consider a different approach. I like the saying, “The wider the base, the higher the peak,” and it applies here. Given that the user needs to query and sort the data by date, author, and metadata, I think stashing everything into a single database cell — which is what a transient is — would feel a little claustrophobic. Instead, I dialed up \u003ca href=\"https://en.wikipedia.org/wiki/Edgar_F._Codd\" rel=\"noopener\"\u003eE.F. Codd\u003c/a\u003e and used a relational database model via custom tables:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" width=\"1600\" height=\"612\" src=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-6.png?resize=1600%2C612\u0026amp;ssl=1\" alt=\"\" srcset=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-6.png?w=1600\u0026amp;ssl=1 1600w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-6.png?resize=300%2C115\u0026amp;ssl=1 300w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-6.png?resize=1024%2C392\u0026amp;ssl=1 1024w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-6.png?resize=768%2C294\u0026amp;ssl=1 768w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/unnamed-6.png?resize=1536%2C588\u0026amp;ssl=1 1536w\" sizes=\"auto, (min-width: 735px) 864px, 96vw\"/\u003e\u003cfigcaption\u003eIn the Dashboard Site, I created seven custom database tables, including one relational table, to cache the data from the 25 client sites, as shown in the image.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIt’s been years since I’ve paged through \u003ca href=\"https://larryullman.com/books/\" rel=\"noopener\"\u003eLarry Ullman’s career-defining (as in, my career) books\u003c/a\u003e on database design, but I came into this project with a general idea of what a good architecture would look like. As for the specific details — things like column types — I foresaw a lot of Stack Overflow time in my future. Fortunately, LLMs love MySQL and I was able to scaffold out my requirements using \u003ca href=\"https://docs.phpdoc.org/guide/guides/docblocks.html\" rel=\"noopener\"\u003eDocBlocks\u003c/a\u003e and let \u003ca href=\"https://blog.samaltman.com\" rel=\"noopener\"\u003eSam Altman\u003c/a\u003e fill in the blanks:\u003c/p\u003e\n\n\n\n\u003cdetails\u003e\n  \u003csummary\u003e\n          Open the code      \u003c/summary\u003e\n  \n\n\u003cpre rel=\"PHP\" data-line=\"\"\u003e\u003ccode markup=\"tt\"\u003e\u0026lt;?php \n\n/**\n* Provides the SQL code for creating the Blogs table.  It has columns for:\n* - ID: The ID for the blog.  This should just autoincrement and is the primary key.\n* - name: The name of the blog.  Required.\n* - slug: A machine-friendly version of the blog name.  Required.\n* - url:  The url of the blog.  Required.\n* - mapped_domain: The vanity domain name of the blog.  Optional.\n* - install: The name of the Multisite install where this blog was scraped from.  Required.\n* - registered:  The date on which this blog began publishing posts.  Optional.\n* - firm_id:  The ID of the firm that publishes this blog.  This will be used as a foreign key to relate to the Firms table.  Optional.\n* - practice_area_id:  The ID of the firm that publishes this blog.  This will be used as a foreign key to relate to the PracticeAreas table.  Optional.\n* - amlaw:  Either a 0 or a 1, to indicate if the blog comes from an AmLaw firm.  Required.\n* - subscriber_count:  The number of email subscribers for this blog.  Optional.\n* - day_view_count:  The number of views for this blog today.  Optional.\n* - week_view_count:  The number of views for this blog this week.  Optional.\n* - month_view_count:  The number of views for this blog this month.  Optional.\n* - year_view_count:  The number of views for this blog this year.  Optional.\n* \n* @return string The SQL for generating the blogs table.\n*/\nfunction get_blogs_table_sql() {\n  $slug = \u0026#39;blogs\u0026#39;;\n  $out = \u0026#34;CREATE TABLE {$this-\u0026gt;get_prefix()}_$slug (\n      id BIGINT NOT NULL AUTO_INCREMENT,\n      slug VARCHAR(255) NOT NULL,\n      name VARCHAR(255) NOT NULL,\n      url VARCHAR(255) NOT NULL UNIQUE, /* adding unique constraint */\n      mapped_domain VARCHAR(255) UNIQUE,\n      install VARCHAR(255) NOT NULL,\n      registered DATE DEFAULT NULL,\n      firm_id BIGINT,\n      practice_area_id BIGINT,\n      amlaw TINYINT NOT NULL,\n      subscriber_count BIGINT,\n      day_view_count BIGINT,\n      week_view_count BIGINT,\n      month_view_count BIGINT,\n      year_view_count BIGINT,\n      PRIMARY KEY (id),\n      FOREIGN KEY (firm_id) REFERENCES {$this-\u0026gt;get_prefix()}_firms(id),\n      FOREIGN KEY (practice_area_id) REFERENCES {$this-\u0026gt;get_prefix()}_practice_areas(id)\n  ) DEFAULT CHARSET=utf8mb4;\u0026#34;;\n  return $out;\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\u003c/details\u003e\n\n\n\u003cp\u003eIn that file, I quickly wrote a DocBlock for each function, and let the OpenAI playground spit out the SQL. I tested the result and suggested some rigorous type-checking for values that should always be formatted as numbers or dates, but that was the only adjustment I had to make. I think that’s the correct use of AI at this moment: You come in with a strong idea of what the result should be, AI fills in the details, and you debate with it until the details reflect what you mostly already knew.\u003c/p\u003e\n\n\n\u003ch3 id=\"how-it-s-going\"\u003eHow it’s going\u003c/h3\u003e\n\n\n\u003cp\u003eI’ve implemented most of the user stories now. Certainly enough to release an MVP and begin gathering whatever insights this data might have for us:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"600\" src=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/s_F8AFC3711E9801FBBF7AF5D4AE0AC9233CF517C4FDA437AED5D82E8DC300B19E_1732282344562_Screenshot2024-11-22at8.30.29AM.png?resize=1024%2C600\u0026amp;ssl=1\" alt=\"Screenshot of the final dashboard which looks similar to the Figma mockups from earlier.\" srcset=\"https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/s_F8AFC3711E9801FBBF7AF5D4AE0AC9233CF517C4FDA437AED5D82E8DC300B19E_1732282344562_Screenshot2024-11-22at8.30.29AM.png?resize=1024%2C600\u0026amp;ssl=1 1024w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/s_F8AFC3711E9801FBBF7AF5D4AE0AC9233CF517C4FDA437AED5D82E8DC300B19E_1732282344562_Screenshot2024-11-22at8.30.29AM.png?resize=300%2C176\u0026amp;ssl=1 300w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/s_F8AFC3711E9801FBBF7AF5D4AE0AC9233CF517C4FDA437AED5D82E8DC300B19E_1732282344562_Screenshot2024-11-22at8.30.29AM.png?resize=768%2C450\u0026amp;ssl=1 768w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/s_F8AFC3711E9801FBBF7AF5D4AE0AC9233CF517C4FDA437AED5D82E8DC300B19E_1732282344562_Screenshot2024-11-22at8.30.29AM.png?resize=1536%2C900\u0026amp;ssl=1 1536w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/s_F8AFC3711E9801FBBF7AF5D4AE0AC9233CF517C4FDA437AED5D82E8DC300B19E_1732282344562_Screenshot2024-11-22at8.30.29AM.png?resize=2048%2C1201\u0026amp;ssl=1 2048w, https://i0.wp.com/css-tricks.com/wp-content/uploads/2024/11/s_F8AFC3711E9801FBBF7AF5D4AE0AC9233CF517C4FDA437AED5D82E8DC300B19E_1732282344562_Screenshot2024-11-22at8.30.29AM.png?w=3000\u0026amp;ssl=1 3000w\" sizes=\"auto, (min-width: 735px) 864px, 96vw\"/\u003e\u003cfigcaption\u003eIt’s working!\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eOne interesting data point thus far: Although all the blogs are on the topic of legal matters (they are lawyer blogs, after all), blogs that cover topics with a more general appeal seem to drive more traffic. Blogs about the law as it pertains to food, cruise ships, germs, and cannabis, for example. Furthermore, the largest law firms on our network don’t seem to have much of a foothold there. Smaller firms are doing a better job of connecting with a wider audience. I’m positive that other insights will emerge as we work more deeply with this.\u003c/p\u003e\n\n\n\u003ch3 id=\"regrets-i-ve-had-a-few-\"\u003eRegrets? I’ve had a few.\u003c/h3\u003e\n\n\n\u003cp\u003eThis project probably would have been a nice opportunity to apply a modern JavaScript framework, or just no framework at all. I like React and I can imagine how cool it would be to have this application be driven by the various changes in state rather than… \u003cem\u003edrumroll\u003c/em\u003e… \u003cstrong\u003ea couple thousand lines of jQuery!\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eI like jQuery’s \u003ccode\u003eajax()\u003c/code\u003e method, and I like the \u003ca href=\"https://jqueryui.com/autocomplete/\" rel=\"noopener\"\u003ejQueryUI autocomplete component\u003c/a\u003e. Also, there’s less of a performance concern here than on a public-facing front-end. Since this screen is in the WordPress admin area, I’m not concerned about Google admonishing me for using an extra library. And I’m just faster with jQuery. Use whatever you want.\u003c/p\u003e\n\n\n\n\u003cp\u003eI also think it would be interesting to put AWS to work here and see what could be done through Lambda functions. Maybe I could get Lambda to make all 25 plus 900 requests concurrently with no worries about browser limitations. Heck, maybe I could get it to cycle through IP addresses and sidestep the 429 rate limit as well.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://developer.wordpress.org/plugins/cron/\" rel=\"noopener\"\u003eAnd what about cron?\u003c/a\u003e Cron could do a lot of work for us here. It could compile the data on each of the 25 client sites ahead of time, meaning that the initial three-minute refresh time goes away. Writing an application in cron, initially, I think is fine. Coming back six months later to debug something is another matter. Not my favorite. I might revisit this later on, but for now, the cron-free implementation meets the MVP goal.\u003c/p\u003e\n\n\n\n\u003cp\u003eI have not provided a line-by-line tutorial here, or even a working repo for you to download, and that level of detail was never my intention. I wanted to share high-level strategy decisions that might be of interest to fellow Multi-Multisite people. Have you faced a similar challenge? I’d love to hear about it in the comments!\u003c/p\u003e\n\n          \n        \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "27 min read",
  "publishedTime": "2024-11-27T09:09:14-07:00",
  "modifiedTime": "2024-11-27T09:09:20-07:00"
}
