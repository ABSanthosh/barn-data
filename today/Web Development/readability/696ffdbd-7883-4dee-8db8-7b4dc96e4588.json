{
  "id": "696ffdbd-7883-4dee-8db8-7b4dc96e4588",
  "title": "Experimenting with local alt text generation in Firefox Nightly",
  "link": "https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly/",
  "description": "Firefox 130 will introduce an experimental new capability to automatically generate alt-text for images using a fully private on-device AI model. The feature will be available as part of Firefox’s built-in PDF editor, and our end goal is to make it available in general browsing for users with screen readers. The post Experimenting with local alt text generation in Firefox Nightly appeared first on Mozilla Hacks - the Web developer blog.",
  "author": "Tarek Ziadé",
  "published": "Fri, 31 May 2024 16:43:46 +0000",
  "source": "https://hacks.mozilla.org/feed/",
  "categories": [
    "Artificial Intelligence",
    "Feature",
    "Featured Article",
    "Firefox",
    "Inference",
    "Machine Learning",
    "AI",
    "alt text",
    "artificial intelligence",
    "firefox",
    "Firefox Nightly",
    "pdf"
  ],
  "byline": "By Tarek Ziadé",
  "length": 12540,
  "excerpt": "Firefox 130 will feature an on-device AI model that automatically generates alt-text for images, integrated into its built-in PDF editor.",
  "siteName": "Mozilla Hacks – the Web developer blog",
  "favicon": "",
  "text": "As discussed on Mozilla Connect, Firefox 130 will introduce an experimental new capability to automatically generate alt-text for images using a fully private on-device AI model. The feature will be available as part of Firefox’s built-in PDF editor, and our end goal is to make it available in general browsing for users with screen readers. Why alt text? Web pages have a fundamentally simple structure, with semantics that allow the browser to interpret the same content differently for different people based on their own needs and preferences. This is a big part of what we think makes the Web special, and what enables the browser to act as a user agent, responsible for making the Web work for people. This is particularly useful for assistive technology such as screen readers, which are able to work alongside browser features to reduce obstacles for people to access and exchange information. For static web pages, this generally can be accomplished with very little interaction from the site, and this access has been enormously beneficial to many people. But even for a simple static page there are certain types of information, like alternative text for images, that must be provided by the author to provide an understandable experience for people using assistive technology (as required by the spec). Unfortunately, many authors don’t do this: the Web Almanac reported in 2022 that nearly half of images were missing alt text. Until recently it’s not been feasible for the browser to infer reasonably high quality alt text for images, without sending potentially sensitive data to a remote server. However, latest developments in AI have enabled this type of image analysis to happen efficiently, even on a CPU. We are adding a feature within the PDF editor in Firefox Nightly to validate this approach. As we develop it further and learn from the deployment, our goal is to offer it for users who’d like to use it when browsing to help them better understand images which would otherwise be inaccessible. Generating alt text with small open source models We are using Transformer-based machine learning models to describe images. These models are getting good at describing the contents of the image, yet are compact enough to operate on devices with limited resources. While can’t outperform a large language model like GPT-4 Turbo with Vision, or LLaVA, they are sufficiently accurate to provide valuable insights on-device across a diversity of hardware. Model architectures like BLIP or even VIT that were trained on datasets like COCO (Common Object In Context) or Flickr30k are good at identifying objects in an image. When combined with a text decoder like OpenAI’s GPT-2, they can produce alternative text with 200M or fewer parameters. Once quantized, these models can be under 200MB on disk, and run in a couple of seconds on a laptop – a big reduction compared to the gigabytes and resources an LLM requires. Example Output The image below (pulled from the COCO dataset) is described by: FIREFOX – our 182M parameters model using a Distilled version of GPT-2 alongside a Vision Transformer (ViT) image encoder. BASELINE MODEL – a slightly bigger ViT+GPT-2 model HUMAN TEXT – the description provided by the dataset annotator. Both small models lose accuracy compared to the description provided by a person, and the baseline model is confused by the hands position. The Firefox model is doing slightly better in that case, and captures what is important. What matters can be suggestive in any case. Notice how the person did not write about the office settings or the cherries on the cake, and specified that the candles were long. If we run the same image on a model like GPT-4o, the results are extremely detailed: The image depicts a group of people gathered around a cake with lit candles. The focus is on the cake, which has a red jelly topping and a couple of cherries. There are several lit candles in the foreground. In the background, there is a woman smiling, wearing a gray turtleneck sweater, and a few other people can be seen, likely in an office or indoor setting. The image conveys a celebratory atmosphere, possibly a birthday or a special occasion. But such level of detail in alt text is overwhelming and doesn’t prioritize the most important information. Brevity is not the only goal, but it’s a helpful starting point, and pithy accuracy in a first draft allows content creators to focus their edits on missing context and details. So if we ask the LLM for a one-sentence description, we get: A group of people in an office celebrates with a lit birthday cake in the foreground and a smiling woman in the background. This has more detail than our small model, but can’t be run locally without sending your image to a server. Small is beautiful Running inference locally with small models offers many advantages: Privacy: All operations are contained within the device, ensuring data privacy. We won’t have access to your images, PDF content, generated captions, or final captions. Your data will not be used to train the model. Resource Efficiency: Small models eliminate the need for high-powered GPUs in the cloud, reducing resource consumption and making it more environmentally friendly. Increased Transparency: In-house management of models allows for direct oversight of the training datasets, offering more transparency compared to some large language models (LLMs). Carbon Footprint Monitoring: Training models in-house facilitates precise tracking of CO2 emissions using tools such as CodeCarbon. Ease of Improvement: Since retraining can be completed in less than a day on a single piece of hardware, it allows for frequent updates and enhancements of the model. Integrating Local Inference into Firefox Extending the Translations inference architecture Firefox Translations uses the Bergamot project powered by the Marian C++  inference runtime. The runtime is compiled into WASM, and there’s a model file for each translation task. For example, if you run Firefox in French and visit an English page, Firefox will ask if you want to translate it to French and download the English-to-French model (~20MiB) alongside the inference runtime. This is a one-shot download: translations will happen completely offline once those files are on disk. The WASM runtime and models are both stored in the Firefox Remote Settings service, which allows us to distribute them at scale and manage versions. The inference task runs in a separate process, which prevents the browser or one of its tabs from crashing if the inference runtime crashes. ONNX and Transformers.js We’ve decided to embed the ONNX runtime in Firefox Nightly along with the Transformers.js library to extend the translation architecture to perform different inference work. Like Bergamot, the ONNX runtime has a WASM distribution and can run directly into the browser. The ONNX project has recently introduced WebGPU support, which will eventually be activated in Firefox Nightly for this feature. Transformers.js provides a Javascript layer on top of the ONNX inference runtime, making it easy to add inference for a huge list of model architectures. The API mimics the very popular Python library. It does all the tedious work of preparing the data that is passed to the runtime and converting the output back to a usable result. It also deals with downloading models from Hugging Face and caching them. From the project’s documentation, this is how you can run a sentiment analysis model on a text: import { pipeline } from '@xenova/transformers'; // Allocate a pipeline for sentiment-analysis let pipe = await pipeline('sentiment-analysis'); let out = await pipe('I love transformers!'); // [{'label': 'POSITIVE', 'score': 0.999817686}] Using Transformers.js gives us confidence when trying out a new model with ONNX. If its architecture is listed in the Transformers.js documentation, that’s a good indication it will work for us. To vendor it into Firefox Nightly, we’ve slightly changed its release to distribute ONNX separately from Transformers.js, dropped Node.js-related pieces, and fixed those annoying eval() calls the ONNX library ships with. You can find the build script here which was used to populate that vendor directory. From there, we reused the Translation architecture to run the ONNX runtime inside its own process, and have Transformers.js run with a custom model cache system. Model caching The Transformers.js project can use local and remote models and has a caching mechanism using the browser cache. Since we are running inference in an isolated web worker, we don’t want to provide access to the file system or store models inside the browser cache. We also don’t want to use Hugging Face as the model hub in Firefox, and want to serve model files from our own servers. Since Transformers.js provides a callback for a custom cache, we have implemented a specific model caching layer that downloads files from our own servers and caches them in IndexedDB. As the project grows, we anticipate the browser will store more models, which can take up significant space on disk. We plan to add an interface in Firefox to manage downloaded models so our users can list them and remove some if needed. Fine-tuning a ViT + GPT-2 model Ankur Kumar released a popular model on Hugging Face to generate alt text for images and blogged about it. This model was also published as ONNX weights by Joshua Lochner so it could be used in Transformers.js, see https://huggingface.co/Xenova/vit-gpt2-image-captioning The model is doing a good job – even if in some cases we had better results with https://huggingface.co/microsoft/git-base-coco – But the GIT architecture is not yet supported in ONNX converters, and with less than 200M params, most of the accuracy is obtained by focusing on good training data. So we have picked ViT for our first model. Ankur used the google/vit-base-patch16-224-in21k image encoder and the GPT-2 text decoder and fine-tuned them using the COCO dataset, which is a dataset of over 120k labeled images. In order to reduce the model size and speed it up a little bit, we’ve decided to replace GPT-2 with DistilGPT-2 — which is 2 times faster and 33% smaller according to its documentation. Using that model in Transformers.js gave good results (see the training code at GitHub – mozilla/distilvit: image-to-text model for PDF.js). We further improved the model for our use case with an updated training dataset and some supervised learning to simplify the output and mitigate some of the biases common in image to text models. Alt text generation in PDF.js Firefox is able to add an image in a PDF using our popular open source pdf.js library: Starting in Firefox 130, we will automatically generate an alt text and let the user validate it. So every time an image is added, we get an array of pixels we pass to the ML engine and a few seconds after, we get a string corresponding to a description of this image (see the code). The first time the user adds an image, they’ll have to wait a bit for downloading the model (which can take up to a few minutes depending on your connection) but the subsequent uses will be much faster since the model will be stored locally. In the future, we want to be able to provide an alt text for any existing image in PDFs, except images which just contain text (it’s usually the case for PDFs containing scanned books). Next steps Our alt text generator is far from perfect, but we want to take an iterative approach and improve it in the open. The inference engine has already landed in Firefox Nightly as a new ml component along with an initial documentation page. We are currently working on improving the image-to-text datasets and model with what we’ve described in this blog post, which will be continuously updated on our Hugging Face page. The code that produces the model lives in Github https://github.com/mozilla/distilvit and the web application we’re building for our team to improve the model is located at https://github.com/mozilla/checkvite. We want to make sure the models and datasets we build, and all the code used, are made available to the community. Once the alt text feature in PDF.js has matured and proven to work well, we hope to make the feature available in general browsing for users with screen readers. Senior Staff Machine Learning Engineer working on Firefox \u0026 Python expert. More articles by Tarek Ziadé…",
  "image": "https://hacks.mozilla.org/wp-content/uploads/2024/05/Screenshot-2024-05-28-at-18.39.17.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle role=\"article\"\u003e\n    \u003cp\u003e\u003ca href=\"https://connect.mozilla.org/t5/discussions/here-s-what-we-re-working-on-in-firefox/td-p/57694\"\u003e\u003ci\u003eAs discussed on Mozilla Connect\u003c/i\u003e\u003c/a\u003e\u003ci\u003e, Firefox 130 will introduce an experimental new capability to automatically generate alt-text for images using a fully private on-device AI model. The feature will be available as part of Firefox’s built-in PDF editor, and our end goal is to make it available in general browsing for users with screen readers.\u003c/i\u003e\u003c/p\u003e\n\u003ch2\u003eWhy alt text?\u003c/h2\u003e\n\u003cp\u003eWeb pages have a fundamentally simple structure, with semantics that allow the browser to interpret the same content differently for different people based on their own needs and preferences. This is a big part of what we think \u003ca href=\"https://www.mozilla.org/en-US/about/webvision/full/#agency\"\u003emakes the Web special\u003c/a\u003e, and what enables the browser to act as a user agent, responsible for making the Web work for people.\u003c/p\u003e\n\u003cp\u003eThis is particularly useful for assistive technology such as screen readers, which are able to work alongside browser features to reduce obstacles for people to access and exchange information. For static web pages, this generally can be accomplished with very little interaction from the site, and this access has been enormously beneficial to many people.\u003c/p\u003e\n\u003cp\u003eBut even for a simple static page there are certain types of information, like \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLImageElement/alt\"\u003ealternative text for images\u003c/a\u003e, that must be provided by the author to provide an understandable experience for people using assistive technology (as \u003ca href=\"https://html.spec.whatwg.org/multipage/images.html#alt\"\u003erequired by the spec\u003c/a\u003e). Unfortunately, many authors don’t do this: the Web Almanac \u003ca href=\"https://almanac.httparchive.org/en/2022/seo#fig-27\"\u003ereported\u003c/a\u003e in 2022 that nearly half of images were missing alt text.\u003c/p\u003e\n\u003cp\u003eUntil recently it’s not been feasible for the browser to infer reasonably high quality alt text for images, without sending potentially sensitive data to a remote server. However, latest developments in AI have enabled this type of image analysis to happen efficiently, even on a CPU.\u003c/p\u003e\n\u003cp\u003eWe are adding a feature within the PDF editor in Firefox Nightly to validate this approach. As we develop it further and learn from the deployment, our goal is to offer it for users who’d like to use it when browsing to help them better understand images which would otherwise be inaccessible.\u003c/p\u003e\n\u003ch2\u003eGenerating alt text with small open source models\u003c/h2\u003e\n\u003cp\u003eWe are using Transformer-based machine learning models to describe images. These models are getting good at describing the contents of the image, yet are compact enough to operate on devices with limited resources. While can’t outperform a large language model like \u003ca href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision\"\u003eGPT-4 Turbo with Vision\u003c/a\u003e, or \u003ca href=\"https://llava-vl.github.io/\"\u003eLLaVA\u003c/a\u003e, they are sufficiently accurate to provide valuable insights on-device across a diversity of hardware.\u003c/p\u003e\n\u003cp\u003eModel architectures like \u003ca href=\"https://huggingface.co/models?other=blip\"\u003eBLIP\u003c/a\u003e or even \u003ca href=\"https://en.wikipedia.org/wiki/Vision_transformer\"\u003eVIT\u003c/a\u003e that were trained on datasets like \u003ca href=\"https://cocodataset.org/#home\"\u003eCOCO\u003c/a\u003e (Common Object In Context) or \u003ca href=\"https://shannon.cs.illinois.edu/DenotationGraph/\"\u003eFlickr30k\u003c/a\u003e are good at identifying objects in an image. When combined with a text decoder like OpenAI’s \u003ca href=\"https://en.wikipedia.org/wiki/GPT-2\"\u003eGPT-2\u003c/a\u003e, they can produce alternative text with 200M or fewer parameters. Once quantized, these models can be under 200MB on disk, and run in a couple of seconds on a laptop – a big reduction compared to the gigabytes and resources an LLM requires.\u003c/p\u003e\n\u003ch3\u003eExample Output\u003c/h3\u003e\n\u003cp\u003eThe image below (pulled from the COCO dataset) is described by:\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eFIREFOX\u003c/b\u003e – our 182M parameters model using a Distilled version of GPT-2 alongside a Vision Transformer (ViT) image encoder.\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eBASELINE MODEL\u003c/b\u003e – a slightly bigger ViT+GPT-2 model\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eHUMAN TEXT\u003c/b\u003e – the description provided by the dataset annotator.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cimg fetchpriority=\"high\" decoding=\"async\" src=\"https://hacks.mozilla.org/wp-content/uploads/2024/05/Screenshot-2024-05-27-at-17.43.26-250x454.png\" alt=\"A person is standing in front of a cake with candles.\" width=\"250\" height=\"454\" srcset=\"https://hacks.mozilla.org/wp-content/uploads/2024/05/Screenshot-2024-05-27-at-17.43.26-250x454.png 250w, https://hacks.mozilla.org/wp-content/uploads/2024/05/Screenshot-2024-05-27-at-17.43.26-500x908.png 500w, https://hacks.mozilla.org/wp-content/uploads/2024/05/Screenshot-2024-05-27-at-17.43.26.png 764w\" sizes=\"(max-width: 250px) 100vw, 250px\"/\u003e\u003c/p\u003e\n\u003cp\u003eBoth small models lose accuracy compared to the description provided by a person, and the baseline model is confused by the hands position. The Firefox model is doing slightly better in that case, and captures what is important.\u003c/p\u003e\n\u003cp\u003eWhat matters can be suggestive in any case. Notice how the person did not write about the office settings or the cherries on the cake, and specified that the candles were long.\u003c/p\u003e\n\u003cp\u003eIf we run the same image on a model like \u003ca href=\"https://openai.com/index/hello-gpt-4o/\"\u003eGPT-4o\u003c/a\u003e, the results are extremely detailed:\u003c/p\u003e\n\u003cp\u003e\u003ci\u003eThe image depicts a group of people gathered around a cake with lit candles. The focus is on the cake, which has a red jelly topping and a couple of cherries. There are several lit candles in the foreground. In the background, there is a woman smiling, wearing a gray turtleneck sweater, and a few other people can be seen, likely in an office or indoor setting. The image conveys a celebratory atmosphere, possibly a birthday or a special occasion.\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eBut such level of detail in alt text is overwhelming and doesn’t prioritize the most important information. Brevity is not the only goal, but it’s a helpful starting point, and pithy accuracy in a first draft allows content creators to focus their edits on missing context and details.\u003c/p\u003e\n\u003cp\u003eSo if we ask the LLM for a one-sentence description, we get:\u003c/p\u003e\n\u003cp\u003e\u003ci\u003eA group of people in an office celebrates with a lit birthday cake in the foreground and a smiling woman in the background.\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThis has more detail than our small model, but can’t be run locally without sending your image to a server.\u003c/p\u003e\n\u003ch3\u003eSmall is beautiful\u003c/h3\u003e\n\u003cp\u003eRunning inference locally with small models offers many advantages:\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003ePrivacy\u003c/b\u003e: All operations are contained within the device, ensuring data privacy. We won’t have access to your images, PDF content, generated captions, or final captions. Your data will not be used to train the model.\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eResource Efficiency\u003c/b\u003e: Small models eliminate the need for high-powered GPUs in the cloud, reducing resource consumption and making it more environmentally friendly.\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eIncreased Transparency\u003c/b\u003e: In-house management of models allows for direct oversight of the training datasets, offering more transparency compared to some large language models (LLMs).\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eCarbon Footprint Monitoring\u003c/b\u003e: Training models in-house facilitates precise tracking of CO2 emissions using tools such as \u003ca href=\"http://codecarbon.io/\"\u003eCodeCarbon\u003c/a\u003e.\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eEase of Improvement\u003c/b\u003e: Since retraining can be completed in less than a day on a single piece of hardware, it allows for frequent updates and enhancements of the model.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eIntegrating Local Inference into Firefox\u003c/h2\u003e\n\u003ch2\u003eExtending the Translations inference architecture\u003c/h2\u003e\n\u003cp\u003eFirefox Translations uses the \u003ca href=\"https://browser.mt/\"\u003eBergamot\u003c/a\u003e project powered by the \u003ca href=\"https://aclanthology.org/P18-4020/\"\u003eMarian C++\u003c/a\u003e  inference runtime. The runtime is compiled into WASM, and there’s a model file for each translation task.\u003c/p\u003e\n\u003cp\u003eFor example, if you run Firefox in French and visit an English page, Firefox will ask if you want to translate it to French and download the English-to-French model (~20MiB) alongside the inference runtime. This is a one-shot download: translations will happen completely offline once those files are on disk.\u003c/p\u003e\n\u003cp\u003eThe WASM runtime and models are both stored in the \u003ca href=\"https://remote-settings.readthedocs.io/en/latest/\"\u003eFirefox Remote Settings\u003c/a\u003e service, which allows us to distribute them at scale and manage versions.\u003c/p\u003e\n\u003cp\u003eThe inference task runs in a separate process, which prevents the browser or one of its tabs from crashing if the inference runtime crashes.\u003c/p\u003e\n\u003ch3\u003eONNX and Transformers.js\u003c/h3\u003e\n\u003cp\u003eWe’ve decided to embed the \u003ca href=\"https://onnxruntime.ai/\"\u003eONNX runtime\u003c/a\u003e in Firefox Nightly along with the \u003ca href=\"https://huggingface.co/docs/transformers.js/index\"\u003eTransformers.js\u003c/a\u003e library to extend the translation architecture to perform different inference work.\u003c/p\u003e\n\u003cp\u003eLike Bergamot, the ONNX runtime has a WASM distribution and can run directly into the browser. The ONNX project has recently introduced WebGPU support, which will eventually be activated in Firefox Nightly for this feature.\u003c/p\u003e\n\u003cp\u003eTransformers.js provides a Javascript layer on top of the ONNX inference runtime, making it easy to add inference for a huge list of model architectures. The API mimics the very popular \u003ca href=\"https://huggingface.co/docs/transformers/en/index\"\u003ePython library\u003c/a\u003e. It does all the tedious work of preparing the data that is passed to the runtime and converting the output back to a usable result. It also deals with downloading models from Hugging Face and caching them.\u003c/p\u003e\n\u003cp\u003eFrom the project’s documentation, this is how you can run a sentiment analysis model on a text:\u003c/p\u003e\n\u003cpre\u003eimport { pipeline } from \u0026#39;@xenova/transformers\u0026#39;;\n\n// Allocate a pipeline for sentiment-analysis\nlet pipe = await pipeline(\u0026#39;sentiment-analysis\u0026#39;);\nlet out = await pipe(\u0026#39;I love transformers!\u0026#39;);\n\n// [{\u0026#39;label\u0026#39;: \u0026#39;POSITIVE\u0026#39;, \u0026#39;score\u0026#39;: 0.999817686}]\u003c/pre\u003e\n\u003cp\u003eUsing Transformers.js gives us confidence when trying out a new model with ONNX. If its architecture is listed in the Transformers.js documentation, that’s a good indication it will work for us.\u003c/p\u003e\n\u003cp\u003eTo vendor it into Firefox Nightly, we’ve slightly changed its release to distribute ONNX separately from Transformers.js, dropped Node.js-related pieces, and fixed those annoying eval() calls the ONNX library ships with. You can find the build script \u003ca href=\"https://hg.mozilla.org/mozilla-central/file/tip/toolkit/components/ml/vendor/build.sh\"\u003ehere\u003c/a\u003e which was used to populate that vendor directory.\u003c/p\u003e\n\u003cp\u003eFrom there, we reused the Translation architecture to run the ONNX runtime inside its own process, and have Transformers.js run with a custom model cache system.\u003c/p\u003e\n\u003ch3\u003eModel caching\u003c/h3\u003e\n\u003cp\u003eThe Transformers.js project can use local and remote models and has a caching mechanism using the browser cache. Since we are running inference in an isolated web worker, we don’t want to provide access to the file system or store models inside the browser cache. We also don’t want to use Hugging Face as the model hub in Firefox, and want to serve model files from our own servers.\u003c/p\u003e\n\u003cp\u003eSince Transformers.js provides a callback for a custom cache, we have implemented a \u003ca href=\"https://hg.mozilla.org/mozilla-central/file/tip/toolkit/components/ml/content/ModelHub.sys.mjs\"\u003especific model caching layer\u003c/a\u003e that downloads files from our own servers and caches them in IndexedDB.\u003c/p\u003e\n\u003cp\u003eAs the project grows, we anticipate the browser will store more models, which can take up significant space on disk. We plan to add an interface in Firefox to manage downloaded models so our users can list them and remove some if needed.\u003c/p\u003e\n\u003ch3\u003eFine-tuning a ViT + GPT-2 model\u003c/h3\u003e\n\u003cp\u003eAnkur Kumar released a\u003ca href=\"https://huggingface.co/nlpconnect/vit-gpt2-image-captioning\"\u003e popular model\u003c/a\u003e on Hugging Face to generate alt text for images and\u003ca href=\"https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/\"\u003e blogged about it\u003c/a\u003e. This model was also published as ONNX weights by Joshua Lochner so it could be used in Transformers.js, see\u003ca href=\"https://huggingface.co/Xenova/vit-gpt2-image-captioning\"\u003e https://huggingface.co/Xenova/vit-gpt2-image-captioning\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe model is doing a good job – even if in some cases we had better results with\u003ca href=\"https://huggingface.co/microsoft/git-base-coco\"\u003e https://huggingface.co/microsoft/git-base-coco\u003c/a\u003e – But the GIT architecture is not yet supported in ONNX converters, and with less than 200M params, most of the accuracy is obtained by focusing on good training data. So we have picked ViT for our first model.\u003c/p\u003e\n\u003cp\u003eAnkur used the \u003ca href=\"https://huggingface.co/google/vit-base-patch16-224-in21k\"\u003egoogle/vit-base-patch16-224-in21k\u003c/a\u003e image encoder and the GPT-2 text decoder and fine-tuned them using the COCO dataset, which is a dataset of over 120k labeled images.\u003c/p\u003e\n\u003cp\u003eIn order to reduce the model size and speed it up a little bit, we’ve decided to replace GPT-2 with \u003ca href=\"https://huggingface.co/distilbert/distilgpt2\"\u003eDistilGPT-2\u003c/a\u003e — which is 2 times faster and 33% smaller according to its documentation.\u003c/p\u003e\n\u003cp\u003eUsing that model in Transformers.js gave good results (see the training code at \u003ca href=\"https://github.com/mozilla/distilvit\"\u003eGitHub – mozilla/distilvit: image-to-text model for PDF.js\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eWe further improved the model for our use case with an \u003ca href=\"https://huggingface.co/datasets/Mozilla/flickr30k-transformed-captions\"\u003eupdated training dataset\u003c/a\u003e and some \u003ca href=\"https://huggingface.co/datasets/Mozilla/alt-text-validation\"\u003esupervised learning\u003c/a\u003e to simplify the output and mitigate some of the biases common in image to text models.\u003c/p\u003e\n\u003ch3\u003eAlt text generation in PDF.js\u003c/h3\u003e\n\u003cp\u003eFirefox is able to add an image in a PDF using our \u003ca href=\"http://pdf.js\"\u003epopular open source pdf.js library\u003c/a\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://hacks.mozilla.org/wp-content/uploads/2024/05/Screenshot-2024-05-28-at-18.39.17-250x223.png\" alt=\"A screenshot of the PDF.js alt text modal window\" width=\"250\" height=\"223\" srcset=\"https://hacks.mozilla.org/wp-content/uploads/2024/05/Screenshot-2024-05-28-at-18.39.17-250x223.png 250w, https://hacks.mozilla.org/wp-content/uploads/2024/05/Screenshot-2024-05-28-at-18.39.17-500x445.png 500w, https://hacks.mozilla.org/wp-content/uploads/2024/05/Screenshot-2024-05-28-at-18.39.17.png 658w\" sizes=\"(max-width: 250px) 100vw, 250px\"/\u003e\u003c/p\u003e\n\u003cp\u003eStarting in Firefox 130, we will automatically generate an alt text and let the user validate it. So every time an image is added, we get an array of pixels we pass to the ML engine and a few seconds after, we get a string corresponding to a description of this image (see the \u003ca href=\"https://github.com/mozilla/pdf.js/blob/d79aaee62a27c25774100d545a420020b8769717/src/display/editor/stamp.js#L430-L464\"\u003ecode\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eThe first time the user adds an image, they’ll have to wait a bit for downloading the model (which can take up to a few minutes depending on your connection) but the subsequent uses will be much faster since the model will be stored locally.\u003c/p\u003e\n\u003cp\u003eIn the future, we want to be able to provide an alt text for any existing image in PDFs, except images which just contain text (it’s usually the case for PDFs containing scanned books).\u003c/p\u003e\n\u003ch2\u003eNext steps\u003c/h2\u003e\n\u003cp\u003eOur alt text generator is far from perfect, but we want to take an iterative approach and improve it in the open. The inference engine has already landed in Firefox Nightly as a new \u003ca href=\"https://hg.mozilla.org/mozilla-central/file/tip/toolkit/components/ml\"\u003eml component\u003c/a\u003e along with an\u003ca href=\"https://firefox-source-docs.mozilla.org/toolkit/components/ml/\"\u003e initial documentation page\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe are currently working on improving the image-to-text datasets and model with what we’ve described in this blog post, which will be continuously updated on our \u003ca href=\"https://huggingface.co/Mozilla\"\u003eHugging Face\u003c/a\u003e page.\u003c/p\u003e\n\u003cp\u003eThe code that produces the model lives in Github \u003ca href=\"https://github.com/mozilla/distilvit\"\u003ehttps://github.com/mozilla/distilvit\u003c/a\u003e and the web application we’re building for our team to improve the model is located at \u003ca href=\"https://github.com/mozilla/checkvite\"\u003ehttps://github.com/mozilla/checkvite\u003c/a\u003e. We want to make sure the models and datasets we build, and all the code used, are made available to the community.\u003c/p\u003e\n\u003cp\u003eOnce the alt text feature in PDF.js has matured and proven to work well, we hope to make the feature available in general browsing for users with screen readers.\u003c/p\u003e\n    \u003csection\u003e\n                                \n                      \u003cp\u003eSenior Staff Machine Learning Engineer working on Firefox \u0026amp; Python expert.\u003c/p\u003e\n                                \u003cp\u003e\u003ca href=\"https://hacks.mozilla.org/author/tziademozilla-com/\"\u003eMore articles by Tarek Ziadé…\u003c/a\u003e\u003c/p\u003e\n                  \u003c/section\u003e\n  \u003c/article\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": null,
  "modifiedTime": null
}
