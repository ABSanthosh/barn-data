{
  "id": "34d0408a-003e-47b5-9efe-6b42918644d7",
  "title": "2024: A year of extraordinary progress and advancement in AI",
  "link": "https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/",
  "description": "As we move into 2025, we’re looking back at the astonishing progress in AI in 2024.",
  "author": "Jeff DeanChief Scientist",
  "published": "Thu, 23 Jan 2025 16:00:00 +0000",
  "source": "https://www.blog.google/rss/",
  "categories": [
    "Research",
    "Google DeepMind",
    "AI"
  ],
  "byline": "Demis Hassabis",
  "length": 26423,
  "excerpt": "As we move into 2025, we’re looking back at the astonishing progress in AI in 2024.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png?version=pr20250121-1756",
  "text": "Jan 23, 2025 [[read-time]] min read A look back on a year of breakthroughs, progress and extraordinary accomplishments. Bullet points This article summarizes Google's AI advancements in 2024, highlighting their commitment to responsible development. Google released Gemini 2.0, a powerful AI model designed for the \"agentic era,\" and integrated it into various products. They made significant progress in generative AI, releasing updates to Imagen, Veo, and MusicFX, empowering creativity. Google also advanced robotics, hardware, and computing, with breakthroughs in quantum computing and chip design. They explored AI's potential in science, biology, and mathematics, with notable achievements in protein structure prediction and geometry. Summaries were generated by Google AI. Generative AI is experimental. As we move into 2025, we wanted to take a moment to recognize the astonishing progress of the last year. From new Gemini models built for the agentic era and empowering creativity, to an AI system that designs novel, high-strength protein binders, AI–enabled neuroscience and even landmark advances in quantum computing, we’ve been boldly and responsibly advancing the frontiers of artificial intelligence and all the ways it can benefit humanity.As we and our colleagues wrote two years ago in an essay titled Why we focus on AI:“Our approach to developing and harnessing the potential of AI is grounded in our founding mission — to organize the world’s information and make it universally accessible and useful — and it is shaped by our commitment to improve the lives of as many people as possible.”This remains as true today as it was when we first wrote it.In this 2024 Year-in-Review post, we look back on a year's worth of extraordinary progress in AI, made possible by the many incredible teams across Google, that helped deliver on that mission and commitment — progress that sets the stage for more to come this year.Relentless innovation in models, products and technologies2024 was a year of experimenting, fast shipping, and putting our latest technologies in the hands of developers.In December 2024, we released the first models in our Gemini 2.0 experimental series — AI models designed for the agentic era. First out of the gate was Gemini 2.0 Flash, our workhorse model, followed by prototypes from the frontiers of our agentic research including: an updated Project Astra, which explores the capabilities of a universal AI assistant; Project Mariner, an early prototype capable of taking actions in Chrome as an experimental extension; and Jules, an AI-powered code agent. We're looking forward to bringing Gemini 2.0’s powerful capabilities to our flagship products — in Search, we’ve already started testing in AI Overviews, which are now used by over a billion people to ask new types of questions. We also released Deep Research, a new agentic feature in Gemini Advanced that saves people hours of research work by creating and executing multi-step plans for finding answers to complicated questions; and introduced Gemini 2.0 Flash Thinking Experimental, an experimental model that explicitly shows its thoughts.These advances followed swift progress earlier in the year, from incorporating Gemini’s capabilities into more Google products to the release of Gemini 1.5 Pro and Gemini 1.5 Flash — a model optimized for speed and efficiency. 1.5 Flash’s compact size made it more cost-efficient to serve, and in 2024 it became our most popular model for developers.And we improved and updated AI Studio, which provides a host of resources for developers. It is now available as a progressive web app (PWA) that can be installed on desktop, iOS and Android.Notably, it’s been exciting to see the public reception to several new features for NotebookLM, such as Audio Overviews, which can take uploaded source material and produce a “deep dive” discussion between two AI hosts. Your browser does not support the audio element. NotebookLM Audio OverviewIn this Audio Overview, two AI hosts dive into the world of NotebookLM updates. More natural and intuitive handling of speech input and output remains at the core of several of our products: Gemini Live, Project Astra, Journey Voices and YouTube’s auto dubbing.Continuing our long history of contributing innovations to the open community — such as with Transformers, TensorFlow, BERT, T5, JAX, AlphaFold and AlphaCode — we released two new models from Gemma, our state-of-the-art open model built from the same research and technology used to create the Gemini models. Gemma outperformed similarly sized open models on capabilities like question answering, reasoning, math / science and coding. And we released Gemma Scope, which provides tools that help researchers understand the inner workings of Gemma 2.We also continued to improve the factuality of our models and minimize hallucinations. In December, for example, we published FACTS Grounding, a new benchmark — based on collaboration between Google DeepMind, Google Research and Kaggle — for evaluating how accurately large language models ground their responses in provided source material and avoid hallucinations. The FACTS Grounding dataset comprises 1,719 examples, each carefully crafted to require long-form responses grounded in the context document provided. We tested leading LLMs using FACTS Grounding, launched the FACTS leaderboard on Kaggle and are proud that Gemini 2.0 Flash Experimental, Gemini 1.5 Flash and Gemini 1.5 Pro currently have the three highest factuality scores, with gemini-2.0-flash-exp at 83.6%.Moreover, we improved underlying ML efficiency through pioneering techniques like blockwise parallel decoding, improved confidence-based deferral and speculative decoding that reduce the inference times of LLMs, allowing them to generate responses more quickly. These improvements are used across Google products and set a standard throughout the industry.Combining AI with sport, in March we released TacticAI, an AI system for football tactics that can provide experts with tactical insights, particularly on corner kicks.Underlying all of our models and products is our ongoing commitment to research leadership. Indeed, in a 2010-2023 WIPO survey of citations for papers on Generative AI, Google including Google Research and Google DeepMind’s citations were more than double the second-most cited institution. This WIPO graph, based on January 2024 data from The Lens, illustrates more than a decade’s worth of Alphabet’s generative AI scientific publication efforts. Finally, progress was made with Project Starline, our “magic window” technology project that enables friends, families and coworkers to feel like they’re together from any distance. We partnered with HP to start commercialization, with the goal of enabling it directly from video conferencing services like Google Meet and Zoom.Empowering creative vision with generative AIWe believe AI holds great potential to enable new forms of creativity, democratize creative output and help people express their artistic visions. This is why last year we introduced a series of updates across our generative media tools, covering images, music and video.At the start of 2024, we introduced ImageFX, a new generative AI tool that creates images from text prompts, and MusicFX, a tool for creating up-to-70-second audio clips also based on text prompts. At I/O, we shared an early preview of MusicFX DJ, a tool that helps bring the joy of live music creation to more people. In October, we collaborated with Jacob Collier on making MusicFX DJ simpler to use, especially for new or aspiring musicians. And we updated our music AI toolkit Music AI Sandbox, and evolved our Dream Track experiment which allowed U.S. creators to explore a range of genres and prompts that generate instrumental soundtracks with powerful text-to-music models. Later in 2024, we released state-of-the-art updates to our image and video models: Veo 2 and Imagen 3. As our highest quality text-to-image model, Imagen 3 is capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models; while Veo demonstrated an improved understanding of real-world physics and the nuances of human movement and expression alongside its overall attention-to-detail and realism. Veo represents a significant step forward in high-quality video generation. Research in this field continued apace. We explored ways to use AI to improve editing, for example by using it to control of attributes like transparency, roughness or other physical properties of objects: In these examples of AI editing with synthetic data generation, Input shows a novel, held-out image the model has never seen before. Output shows the model output, which successfully edits material properties. In the field of audio generation, we announced improvements to video-to-audio (V2A) technology, which can generate dynamic soundscapes through natural language text prompts based on on-screen action. This technology is pairable with AI-created video through Veo.Games are an ideal environment for creative exploration of new worlds, as well as training and evaluating embodied agents. In 2024, we introduced Genie 2, a foundation world model capable of generating an endless variety of action-controllable, playable 3D environments for training and evaluating embodied agents. This followed the introduction of SIMA, a Scalable Instructable Multiworld Agent that can follow natural-language instructions to carry out tasks in a variety of video game settings. The architecture of intelligence: advances in robotics, hardware and computingAs our multimodal models become more capable and gain a better understanding of the world and its physics, they are making possible incredible new advances in robotics and bringing us closer to our goal of ever-more capable and helpful robots. With ALOHA Unleashed, our robot learned to tie a shoelace, hang a shirt, repair another robot, insert a gear and even clean a kitchen. At the beginning of the year, we introduced AutoRT, SARA-RT and RT-Trajectory, extensions of our Robotics Transformers work intended to help robots better understand and navigate their environments, and make decisions faster. We also published ALOHA Unleashed, a breakthrough in teaching robots on how to use two robotic arms in coordination, and DemoStart, which uses a reinforcement learning algorithm to improve real-world performance on a multi-fingered robotic hand by using simulations. Robotic Transformer 2 (RT-2) is a novel vision-language-action model that learns from both web and robotics data. Beyond robotics, our AlphaChip reinforcement learning method for accelerating and improving chip floorplanning is transforming the design process for chips found in data centers, smartphones and more. To accelerate adoption of these techniques, we released a pre-trained checkpoint to enable external parties to more easily make use of the AlphaChip open source release for their own chip designs. And we made Trillium, our sixth-generation and most performant TPU to date, generally available to Google Cloud customers. Advances in computer chips have accelerated AI. And now, AI can return the favor. AlphaChip can learn the relationships between interconnected chip components and generalize across chips, letting AlphaChip improve with each layout it designs. Our research also focused on correcting the errors in the physical hardware of today's quantum computers. In November, we launched AlphaQubit, an AI-based decoder that identifies quantum computing errors with state-of-the-art accuracy. This collaborative work brought together Google DeepMind’s ML knowledge and Google Research’s error correction expertise to accelerate progress on building a reliable quantum computer. In tests, it made 6% fewer errors than tensor network methods and 30% fewer errors than correlated matching.Then in December, the Google Quantum AI team, part of Google Research, announced Willow, our latest quantum chip which can perform in under five minutes a benchmark computation that would take one of today’s fastest supercomputers 10 septillion years. Willow can reduce errors exponentially as it scales up using more qubits. In fact, it used our quantum error correction to cut the error rate in half, solving a 30+ year challenge known in the field as “below threshold.” This leap forward won the Physics Breakthrough of the Year award. Willow has state-of-the-art performance across a number of metrics. Uncovering new solutions: progress in science, biology and mathematicsWe continued to push the envelope on accelerating scientific progress with AI-based approaches, releasing a series of tools and papers this year that showed just how useful and powerful a tool AI is for advancing science and mathematics. We're sharing a few highlights. In January, we introduced AlphaGeometry, an AI system engineered to solve complex geometry problems. Our updated version, AlphaGeometry 2, and AlphaProof, a reinforcement-learning-based system for formal math reasoning, achieved the same level as a silver medalist in July 2024’s International Mathematical Olympiad. AlphaGeometry 2 solved Problem 4 in July 2024’s International Mathematical Olympiad within 19 seconds after receiving its formalization. Problem 4 asked to prove the sum of ∠KIL and ∠XPY equals 180°. In collaboration with Isomorphic Labs, we introduced AlphaFold 3, our latest model which predicts the structure and interactions of all of life’s molecules. By accurately predicting the structure of proteins, DNA, RNA, ligands and more, and how they interact, we hope it will transform our understanding of the biological world and drug discovery. AlphaFold 3’s capabilities come from its next-generation architecture and training that now covers all of life’s molecules. We made several key developments in protein-shaping. We announced AlphaProteo, an AI system for designing novel, high-strength protein binders. AlphaProteo can lead to the discovery of new drugs, the development of biosensors and improve our understanding of biological processes. AlphaProteo can generate new protein binders for diverse target proteins. In collaboration with Harvard’s Lichtman Lab and others, we produced a nano-scale mapping of a piece of the human brain at a level of detail never previously achieved, and made it publicly available for researchers to build on. This follows a decade of working to advance our understanding of connectomics, with earlier work on fly brain and mouse brain connectomics now giving way to the larger scale and more complex human brain connectomics. In the deepest layer of the cortex, clusters of cells tend to occur in mirror-image orientation to one another, as shown in this brain mapping project. Then in late November, as part of a broader effort to expand and deepen public dialogue around science and AI, we co-hosted the AI for Science Forum with the Royal Society, which convened scientists, researchers, governmental leaders and executives to discuss key topics like cracking the protein structure prediction challenge, mapping the human brain and saving lives through accurate forecasting and spotting wildfires. We hosted a Q\u0026A with the four Nobel Laureates in attendance at the forum, Sir Paul Nurse, Jennifer Doudna, Demis Hassabis and John Jumper, which is available to listen to via the Google DeepMind podcast.This was also a landmark year for another reason: Demis Hassabis and John Jumper, along with David Baker, were awarded the 2024 Nobel Prize® in Chemistry for their work on AlphaFold 2. As the Nobel committee recognized, their work:\"[H]as opened up completely new possibilities to design proteins that have never been seen before, and we now have access to predicted structures of all 200 million known proteins. These are truly great achievements.\"It was also exciting to see the 2024 Nobel Prize® in Physics awarded to recently retired long-time Googler Geoffrey Hinton (along with John Hopfield), \"for foundational discoveries and inventions that enable machine learning with artificial neural networks.”The Nobels followed additional recognitions for Google including the NeurIPS 2024 Test of Time Paper Awards for Sequence to Sequence Learning with Neural Networks and Generative Adversarial Nets, and the Beale—Orchard-Hays Prize, which was awarded to a collaborative team of educators and Google professionals for groundbreaking work on Primal-Dual Linear Programming (PDLP). (PDLP, now part of Google OR Tools, helps solve large-scale linear programming problems with real-world applications from data center network traffic engineering to container shipping optimization.)AI for the benefit of humanityThis year, we made a number of product advances and published research that showed how AI can benefit people directly and immediately, ranging from preventative and diagnostic medicine to disaster readiness and recovery to learning.In healthcare, AI holds the promise of democratizing quality of care in key areas, such as early detection of cardiovascular disease. Our research demonstrated how using a simple fingertip device that measures variations in blood flow, combined with basic metadata, can predict heart health risks. We built on previous AI-enabled diagnostic research for tuberculosis, demonstrating how AI models can be used for accurate TB screenings in populations with high rates of TB and HIV. This is important to reducing the prevalence of TB (more than 10 million people fall ill with it each year), as roughly 40% of people with TB go undiagnosed. On the MedQA (USMLE-style) benchmark, Med-Gemini attains a new state-of-the-art score, surpassing our prior best (Med-PaLM 2) by a significant margin of 4.6%. Our Gemini model is a powerful tool for professionals generally, but our teams are also working to create fine-tuned models for other domains. For example, we introduced Med-Gemini, a new family of next-generation models that combine training on de-identified medical data with Gemini’s reasoning, multimodal and long-context abilities. On the MedQA US Medical Licensing Exam (USMLE)-style question benchmark, Med-Gemini achieves a state-of-the-art performance of 91.1% accuracy, surpassing our prior best of Med-PaLM 2 by 4.6% (shown above).We are exploring how machine learning can help medical fields struggling with access to imaging expertise, such as radiology, dermatology and pathology. In the past year, we released two research tools, Derm Foundation and Path Foundation, that can help develop models for diagnostic tasks, image indexing and curation and biomarker discovery and validation. We collaborated with physicians at Stanford Medicine on an open-access, inclusive Skin Condition Image Network (SCIN) dataset. And we unveiled CT Foundation, a medical imaging embedding tool used for rapidly training models for research.With regard to learning, we explored new generative AI tools to support educators and learners. We introduced LearnLM, our new family of models fine-tuned for learning and used it to enhance learning experiences in products like Search, YouTube and Gemini; a recent report showed LearnLM outperformed other leading AI models. We also made it available to developers as an experimental model in AI Studio. Our new conversational learning companion, LearnAbout, uses AI to help you dive deeper into any topic you’re curious about, while Illuminate lets you turn content into engaging AI-generated audio discussions.In the fields of disaster forecasting and preparedness, we announced several breakthroughs. We introduced GenCast, our new high-resolution AI ensemble model, which improves day-to-day weather and extreme events forecasting across all possible weather trajectories. We also introduced our NeuralGCM model, able to simulate over 70,000 days of the atmosphere in the time it would take a physics-based model to simulate only 19 days. And GraphCast won the 2024 MacRobert Award for engineering innovation. This selection of GraphCast’s predictions rolling across 10 days shows specific humidity at 700 hectopascals (about 3 kilometers above surface), surface temperature and surface wind speed. We also improved our flood forecasting model to predict flooding seven days in advance (up from five) and expanded our riverine flood forecasting coverage to 100 countries and 700 million people. This marks a significant milestone in a multi-year initiative that Google Research embarked on in 2018. Our flood forecasting model is now available in over 100 countries (left), and we now have “virtual gauges” for experts and researchers in more than 150 countries, including countries where physical gauges are not available. AI can also help with wildfire detection and mitigation, which is especially top of mind given the devastation in California. Our Wildfire Boundary Maps capabilities are now available in 22 countries. Alongside leading wildfire authorities, Google Research also created FireSat, a constellation of satellites that can detect and track wildfires as small as a classroom (roughly 5x5 meters) within 20 minutes.And we continued building on our commitment to making more information more accessible to more people, expanding Google Translate with 110 new languages, including Cantonese, Papua New Guinea’s Tok Pisin, N’Ko from West Africa and Manx from the Isle of Man. Google Translate — which now supports over 240 languages — can help people overcome barriers to information, knowledge and opportunity. These new languages in Google Translate represent more than 614 million speakers, opening up translations for around 8% of the world’s population. Helping set the standard in responsible AIWe furthered our industry-leading research in AI safety, developing new tools and techniques and integrating these advances into our latest models. We’re committed to working with others to address risks.We continued researching misuse, conducting a study that found the two most common types of misuse were deep fakes and jailbreaks. In May, we introduced The Frontier Safety Framework, which established protocols for identifying the emerging capabilities of our most advanced AI models, and launched our AI Responsibility Lifecycle framework to the public. In October, we expanded our Responsible GenAI Toolkit to work with any LLM, giving developers more tools to build AI responsibly.And, among our other efforts, we released a paper this year on The Ethics of Advanced AI Assistants that examined and mapped the new technical and moral landscape of a future populated by AI assistants, and characterized the opportunities and risks society might face.We expanded SynthID’s capabilities to watermarking AI-generated text in the Gemini app and web experience, and video in Veo. To help increase overall transparency online, not just with content created by Google gen AI tools, we also joined the Coalition for Content Provenance and Authenticity (C2PA) as a steering committee member and collaborated on a new, more secure version of the technical standard, Content Credentials. When there’s a range of different tokens to choose from, SynthID can adjust the probability score of each predicted token, in cases where it won’t compromise the quality, accuracy and creativity of the output. Beyond LLMs, we shared our approach to biosecurity for AlphaFold 3. We also worked with industry partners to launch the Coalition for Secure AI (CoSAI), and we participated in the AI Seoul Summit, as a way of building and contributing to an international consensus and a common, coordinated approach to governance.As we develop new technologies like AI agents, we’ll continue to encounter new questions around safety, security and privacy. Guided by our AI Principles, we are deliberately taking an exploratory and gradual approach to development, conducting research on multiple prototypes, iteratively implementing safety training, working with trusted testers and external experts and performing extensive risk assessments and safety and assurance evaluations.Looking ahead to 20252024 was a productive year, and a very exciting time for groundbreaking new products and research in AI. We made a great deal of progress and we’re even more excited about the year ahead.As we continue to produce groundbreaking AI research in the fields of products, science, health, creativity and more, it becomes increasingly important to think deeply about how and when it should be deployed. By continuing to prioritize responsible AI practices and fostering collaboration, we’ll play an important role in building a future where AI benefits humanity.",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Exec-AI-SocialShare.width-1300.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\n\n    \n    \n\n\n\n\n\n    \n\n    \n      \n\n\u003cdiv data-analytics-module=\"{\n    \u0026#34;module_name\u0026#34;: \u0026#34;Hero Menu\u0026#34;,\n    \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n  }\"\u003e\n  \n  \u003cdiv\u003e\n      \u003cdiv\u003e\n          \n            \u003cp\u003eJan 23, 2025\u003c/p\u003e\n          \n          \n            \u003cp data-reading-time-render=\"\"\u003e[[read-time]] min read\u003c/p\u003e\n          \n        \u003c/div\u003e\n      \n        \u003cp\u003e\n          A look back on a year of breakthroughs, progress and extraordinary accomplishments.\n        \u003c/p\u003e\n      \n    \u003c/div\u003e\n  \n  \u003cdiv data-summary-id=\"ai_summary_2\" data-component=\"uni-ai-generated-summary\" data-analytics-module=\"{\n    \u0026#34;event\u0026#34;: \u0026#34;module_impression\u0026#34;,\n    \u0026#34;module_name\u0026#34;: \u0026#34;ai_summary\u0026#34;,\n    \u0026#34;section_header\u0026#34;: \u0026#34;CTA\u0026#34;\n  }\"\u003e\n          \u003ch2\u003eBullet points\u003c/h2\u003e\n          \u003cul\u003e\n\u003cli\u003eThis article summarizes Google\u0026#39;s AI advancements in 2024, highlighting their commitment to responsible development.\u003c/li\u003e\n\u003cli\u003eGoogle released Gemini 2.0, a powerful AI model designed for the \u0026#34;agentic era,\u0026#34; and integrated it into various products.\u003c/li\u003e\n\u003cli\u003eThey made significant progress in generative AI, releasing updates to Imagen, Veo, and MusicFX, empowering creativity.\u003c/li\u003e\n\u003cli\u003eGoogle also advanced robotics, hardware, and computing, with breakthroughs in quantum computing and chip design.\u003c/li\u003e\n\u003cli\u003eThey explored AI\u0026#39;s potential in science, biology, and mathematics, with notable achievements in protein structure prediction and geometry.\u003c/li\u003e\n\u003c/ul\u003e\n          \n          \u003cp\u003e\u003csmall\u003e\n            Summaries were generated by Google AI. Generative AI is experimental.\n          \u003c/small\u003e\n        \u003c/p\u003e\u003c/div\u003e\n\u003c/div\u003e\n\n    \n\n    \n      \n\n\n\n\n\n\n\n\n\u003cdiv\u003e\n    \u003cfigure\u003e\n      \u003cdiv\u003e\n  \u003cp\u003e\u003cimg srcset=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/EOY-2024-Header-250114-r01.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/EOY-2024-Header-250114-r01.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/EOY-2024-Header-250114-r01.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/EOY-2024-Header-250114-r01.width-2200.format-webp.webp 2200w\" sizes=\"(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/EOY-2024-Header-250114-r01.width-1200.format-webp.webp\" fetchpriority=\"high\" alt=\"Collage showing robot arms tying sneaker laces; text saying Gemini 2.0 against a blue and black background; and a dachshund wearing goggles while swimming in a Veo 2 video still\"/\u003e\n  \u003c/p\u003e\n\u003c/div\u003e\n\n      \n    \u003c/figure\u003e\n  \u003c/div\u003e\n\n\n    \n\n    \n    \u003cdiv data-reading-time=\"true\" data-component=\"uni-article-body\"\u003e\n\n            \n              \n\n\n\n\n\u003cgoogle-read-aloud-player data-analytics-module=\"{\n        \u0026#34;event\u0026#34;: \u0026#34;module_impression\u0026#34;,\n        \u0026#34;module_name\u0026#34;: \u0026#34;ai_audio\u0026#34;,\n        \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n    }\" data-call-to-action-text=\"Listen to article\" data-date-modified=\"2025-01-23T23:50:07.835894+00:00\" data-progress-bar-style=\"half-wave\" data-api-key=\"AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac\" data-article-style=\"style9\" data-tracking-ids=\"G-HGNBTNCHCQ,G-6NKTLKV14N\" data-voice-list=\"en.ioh-pngnat:Cyan,en.usb-pngnat:Lime\" data-layout-style=\"style1\" data-highlight-mode=\"word-over-paragraph\" data-highlight-text-color=\"#000000\" data-highlight-word-background=\"#8AB4F8\" data-highlight-paragraph-background=\"#D2E3FC\" data-background=\"linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)\" data-foreground-color=\"#202124\" data-font=\"600 16px Google Sans, sans-serif\" data-box-shadow=\"0px 1px 3px 1px rgba(60, 64, 67, 0.15)\"\u003e\n\u003c/google-read-aloud-player\u003e\n\n\n\n            \n\n            \n            \n\n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"71nyp\"\u003eAs we move into 2025, we wanted to take a moment to recognize the astonishing progress of the last year. From \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/\"\u003enew Gemini models built for the agentic era\u003c/a\u003e and empowering \u003ca href=\"https://blog.google/technology/google-labs/video-image-generation-update-december-2024/\"\u003ecreativ\u003c/a\u003eity, to an \u003ca href=\"https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/\"\u003eAI system\u003c/a\u003e that designs novel, high-strength protein binders, \u003ca href=\"https://research.google/blog/ten-years-of-neuroscience-at-google-yields-maps-of-human-brain/\"\u003eAI–enabled neuroscience\u003c/a\u003e and even \u003ca href=\"https://blog.google/technology/research/google-willow-quantum-chip/\"\u003elandmark advances\u003c/a\u003e in quantum computing, we’ve been boldly and responsibly advancing the frontiers of artificial intelligence and all the ways it can benefit humanity.\u003c/p\u003e\u003cp data-block-key=\"8o4i6\"\u003eAs we and our colleagues \u003ca href=\"https://ai.google/advancing-ai/why-ai/\"\u003ewrote\u003c/a\u003e two years ago in an essay titled \u003ci\u003eWhy we focus on AI\u003c/i\u003e:\u003c/p\u003e\u003cp data-block-key=\"bsk9h\"\u003e“\u003ci\u003eOur approach to developing and harnessing the potential of AI is grounded in our founding mission — to organize the world’s information and make it universally accessible and useful — and it is shaped by our commitment to improve the lives of as many people as possible\u003c/i\u003e.”\u003c/p\u003e\u003cp data-block-key=\"3cgua\"\u003eThis remains as true today as it was when we first wrote it.\u003c/p\u003e\u003cp data-block-key=\"60j7r\"\u003eIn this 2024 Year-in-Review post, we look back on a year\u0026#39;s worth of extraordinary progress in AI, made possible by the many incredible teams across Google, that helped deliver on that mission and commitment — progress that sets the stage for more to come this year.\u003c/p\u003e\u003ch2 data-block-key=\"6uggk\"\u003eRelentless innovation in models, products and technologies\u003c/h2\u003e\u003cp data-block-key=\"arvdf\"\u003e2024 was a year of experimenting, fast shipping, and putting our latest technologies in the hands of developers.\u003c/p\u003e\u003cp data-block-key=\"8r8rm\"\u003eIn December 2024, we released the first models in our \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/\"\u003eGemini 2.0\u003c/a\u003e experimental series — AI models designed for the agentic era. First out of the gate was Gemini 2.0 Flash, our workhorse model, followed by prototypes from the frontiers of our agentic research including: an updated \u003ca href=\"https://www.youtube.com/watch?v=hIIlJt8JERI\u0026amp;t=5s\"\u003eProject Astra\u003c/a\u003e, which explores the capabilities of a universal AI assistant; \u003ca href=\"https://www.youtube.com/watch?v=2XJqLPqHtyo\"\u003eProject Mariner\u003c/a\u003e, an early prototype capable of taking actions in Chrome as an experimental extension; and \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#agents-for-developers\"\u003eJules\u003c/a\u003e, an AI-powered code agent. We\u0026#39;re looking forward to bringing Gemini 2.0’s powerful capabilities to our flagship products — in Search, we’ve already started testing in \u003ca href=\"https://blog.google/products/search/ai-overviews-search-october-2024/\"\u003eAI Overviews\u003c/a\u003e, which are now used by over a billion people to ask new types of questions.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"2\" thumbnail-alt=\"A YouTube video paused on a screen showing a blue and black background with text that says: Gemini 2.0 Enabling the agentic era\" subtitle=\"Gemini 2.0 is built for the agentic era, bringing enhanced performance, more multimodality and new native tool use.\" video-id=\"Fs0t6SdODd8\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"eqoju\"\u003eWe also released \u003ca href=\"https://blog.google/products/gemini/google-gemini-deep-research/\"\u003eDeep Research\u003c/a\u003e, a new agentic feature in Gemini Advanced that saves people hours of research work by creating and executing multi-step plans for finding answers to complicated questions; and introduced \u003ca href=\"https://x.com/JeffDean/status/1869789813232341267\"\u003eGemini 2.0 Flash Thinking Experimental\u003c/a\u003e, an experimental model that explicitly shows its thoughts.\u003c/p\u003e\u003cp data-block-key=\"7uosf\"\u003eThese advances followed swift progress earlier in the year, from incorporating \u003ca href=\"https://blog.google/technology/ai/google-gemini-update-sundar-pichai-2024/?utm_source=gdm\u0026amp;utm_medium=referral\u0026amp;utm_campaign=gemini24\"\u003eGemini’s capabilities into more Google products\u003c/a\u003e to the release of \u003ca href=\"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/\"\u003eGemini 1.5 Pro\u003c/a\u003e and \u003ca href=\"https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/?utm_source=gdm\u0026amp;utm_medium=referral\u0026amp;utm_campaign=io24#gemini-model-updates\"\u003eGemini 1.5 Flash\u003c/a\u003e — a model optimized for speed and efficiency. 1.5 Flash’s compact size made it more cost-efficient to serve, and in 2024 it became our most popular model for developers.\u003c/p\u003e\u003cp data-block-key=\"djsa2\"\u003eAnd we improved and updated \u003ca href=\"https://aistudio.google.com/prompts/new_chat\"\u003eAI Studio\u003c/a\u003e, which provides a host of resources for developers. It is now available as a progressive web app (PWA) that can be installed on desktop, iOS and Android.\u003c/p\u003e\u003cp data-block-key=\"3577v\"\u003eNotably, it’s been exciting to see the public reception to several \u003ca href=\"https://blog.google/technology/google-labs/notebooklm-new-features-december-2024/\"\u003enew features\u003c/a\u003e for NotebookLM, such as Audio Overviews, which can take uploaded source material and produce a \u003ca href=\"https://deepmind.google/discover/blog/pushing-the-frontiers-of-audio-generation/\"\u003e“deep dive” discussion\u003c/a\u003e between \u003ca href=\"https://blog.google/technology/ai/notebooklm-audio-overviews/\"\u003etwo AI hosts\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\u003cdiv data-component=\"uni-audio-player\" uni-l10n=\"{\n       \u0026#34;mute\u0026#34;: \u0026#34;Click to unmute audio\u0026#34;,\n       \u0026#34;volume\u0026#34;: \u0026#34;Click to mute audio\u0026#34;,\n       \u0026#34;stop\u0026#34;: \u0026#34;Click to stop audio\u0026#34;,\n       \u0026#34;play\u0026#34;: \u0026#34;Click to play audio\u0026#34;,\n       \u0026#34;progress\u0026#34;: \u0026#34;Current audio progress minutes with seconds:\u0026#34;,\n       \u0026#34;duration\u0026#34;: \u0026#34;Duration of the audio minutes with seconds:\u0026#34;\n     }\" data-analytics-module=\"{\n      \u0026#34;module_name\u0026#34;: \u0026#34;Audio\u0026#34;,\n      \u0026#34;section_header\u0026#34;: \u0026#34;NotebookLM Audio Overview\u0026#34;\n     }\"\u003e\n  \u003cp\u003e\u003caudio title=\"NotebookLM Audio Overview\"\u003e\n    \u003caudio controls=\"\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/media/final_audio_5NkPQm8.mp3\" type=\"audio/mpeg\"/\u003e\n\u003cp\u003eYour browser does not support the audio element.\u003c/p\u003e\n\u003c/audio\u003e\n  \u003c/audio\u003e\u003c/p\u003e\u003cdiv aria-label=\"Audio Overview of a Keyword blog post about NotebookLM going global with Slides support and better ways to fact-check\" tabindex=\"0\"\u003e\n        \u003cp\u003e\u003cspan\u003eNotebookLM Audio Overview\u003c/span\u003e\u003c/p\u003e\u003cp\u003eIn this Audio Overview, two AI hosts dive into the world of NotebookLM updates.\u003c/p\u003e\n      \u003c/div\u003e\n\u003c/div\u003e\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"eqoju\"\u003eMore natural and intuitive handling of speech input and output remains at the core of several of our products: \u003ca href=\"https://blog.google/products/gemini/made-by-google-gemini-ai-updates/\"\u003eGemini Live\u003c/a\u003e, \u003ca href=\"https://deepmind.google/technologies/gemini/project-astra/\"\u003eProject Astra\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/text-to-speech/docs/voice-types\"\u003eJourney Voices\u003c/a\u003e and \u003ca href=\"https://blog.youtube/news-and-events/made-on-youtube-2024/\"\u003eYouTube’s auto dubbing\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"9gda6\"\u003eContinuing our long history of contributing innovations to the open community — such as with \u003ca href=\"https://blog.research.google/2017/08/transformer-novel-neural-network.html\"\u003eTransformers\u003c/a\u003e, \u003ca href=\"https://www.tensorflow.org/\"\u003eTensorFlow\u003c/a\u003e, \u003ca href=\"https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html\"\u003eBERT\u003c/a\u003e, \u003ca href=\"https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html\"\u003eT5\u003c/a\u003e, \u003ca href=\"https://github.com/google/jax\"\u003eJAX\u003c/a\u003e, \u003ca href=\"https://deepmind.google/discover/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology/\"\u003eAlphaFold\u003c/a\u003e and \u003ca href=\"https://deepmind.google/discover/blog/competitive-programming-with-alphacode/\"\u003eAlphaCode\u003c/a\u003e — we released two new models from \u003ca href=\"https://blog.google/technology/developers/gemma-open-models/\"\u003eGemma\u003c/a\u003e, our state-of-the-art open model built from the same research and technology used to create the Gemini models. Gemma \u003ca href=\"https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf\"\u003eoutperformed\u003c/a\u003e similarly sized open models on capabilities like question answering, reasoning, math / science and coding. And we released \u003ca href=\"https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/\"\u003eGemma Scope\u003c/a\u003e, which provides tools that help researchers understand the inner workings of Gemma 2.\u003c/p\u003e\u003cp data-block-key=\"7u4ig\"\u003eWe also continued to improve the factuality of our models and minimize hallucinations. In December, for example, we published \u003ca href=\"https://deepmind.google/discover/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/\"\u003eFACTS Grounding\u003c/a\u003e, a new benchmark — based on collaboration between Google DeepMind, Google Research and Kaggle — for evaluating how accurately large language models ground their responses in provided source material and avoid hallucinations.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"A table showing an example of a question and response about how to save money, based on the FACTS grounding dataset class.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"ohqap\"\u003eThe FACTS Grounding dataset comprises 1,719 examples, each carefully crafted to require long-form responses grounded in the context document provided.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"A table showing an example of a question and response about how to save money, based on the FACTS grounding dataset class.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/FACTS_system_instruction2x.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/FACTS_system_instruction2x.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/FACTS_system_instruction2x.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"eqoju\"\u003eWe tested leading LLMs using FACTS Grounding, launched the \u003ca href=\"https://www.kaggle.com/facts-leaderboard\"\u003eFACTS leaderboard\u003c/a\u003e on Kaggle and are proud that Gemini 2.0 Flash Experimental, Gemini 1.5 Flash and Gemini 1.5 Pro currently have the three highest factuality scores, with gemini-2.0-flash-exp at 83.6%.\u003c/p\u003e\u003cp data-block-key=\"7mdca\"\u003eMoreover, we improved underlying ML efficiency through pioneering \u003ca href=\"https://research.google/blog/looking-back-at-speculative-decoding/\"\u003etechniques\u003c/a\u003e like \u003ca href=\"https://openreview.net/pdf?id=KT6F5Sw0eg\"\u003eblockwise parallel decoding\u003c/a\u003e, \u003ca href=\"https://arxiv.org/pdf/2307.02764\"\u003eimproved confidence-based deferral\u003c/a\u003e and \u003ca href=\"https://research.google/blog/looking-back-at-speculative-decoding/\"\u003especulative decoding\u003c/a\u003e that reduce the inference times of LLMs, allowing them to generate responses more quickly. These improvements are used across Google products and set a standard throughout the industry.\u003c/p\u003e\u003cp data-block-key=\"1j31t\"\u003eCombining AI with sport, in March we released \u003ca href=\"https://deepmind.google/discover/blog/tacticai-ai-assistant-for-football-tactics/\"\u003eTacticAI\u003c/a\u003e, an AI system for football tactics that can provide experts with tactical insights, particularly on corner kicks.\u003c/p\u003e\u003cp data-block-key=\"a7t9n\"\u003eUnderlying all of our models and products is our ongoing commitment to research leadership. Indeed, in a \u003ca href=\"https://www.wipo.int/web-publications/patent-landscape-report-generative-artificial-intelligence-genai/en/2-global-patenting-and-research-in-genai.html\"\u003e2010-2023 WIPO survey of citations for papers on Generative AI\u003c/a\u003e, Google including Google Research and Google DeepMind’s citations were more than double the second-most cited institution.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Bar graph titled: Number of citations to GenAI scientific publications for the top 20 institutions, 2010-2023. Alphabet (US) is at the top with 65,703 citations.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"ohqap\"\u003eThis WIPO graph, based on January 2024 data from The Lens, illustrates more than a decade’s worth of Alphabet’s generative AI scientific publication efforts.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Bar graph titled: Number of citations to GenAI scientific publications for the top 20 institutions, 2010-2023. Alphabet (US) is at the top with 65,703 citations.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/EOY-2024-Figure-250114-r01.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/EOY-2024-Figure-250114-r01.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/EOY-2024-Figure-250114-r01.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"eqoju\"\u003eFinally, progress was made with Project Starline, our “magic window” technology project that enables friends, families and coworkers to feel like they’re together from any distance. We \u003ca href=\"https://blog.google/technology/research/google-project-starline-hp-partnership/\"\u003epartnered with HP\u003c/a\u003e to start commercialization, with the goal of enabling it directly from video conferencing services like Google Meet and Zoom.\u003c/p\u003e\u003ch2 data-block-key=\"bfkrm\"\u003eEmpowering creative vision with generative AI\u003c/h2\u003e\u003cp data-block-key=\"a4mqt\"\u003eWe believe AI holds great potential to enable new forms of creativity, democratize creative output and help people express their artistic visions. This is why last year we introduced a series of updates across our generative media tools, covering images, music and video.\u003c/p\u003e\u003cp data-block-key=\"a3guf\"\u003eAt the start of 2024, we \u003ca href=\"https://blog.google/technology/ai/google-labs-imagefx-textfx-generative-ai/\"\u003eintroduced\u003c/a\u003e ImageFX, a new generative AI tool that creates images from text prompts, and MusicFX, a tool for creating up-to-70-second audio clips also based on text prompts. At I/O, we \u003ca href=\"https://blog.google/technology/ai/google-labs-video-fx-generative-ai/\"\u003eshared an early preview\u003c/a\u003e of MusicFX DJ, a tool that helps bring the joy of live music creation to more people. In October, we collaborated with \u003ca href=\"https://www.youtube.com/watch?v=y7gKlzvg8xk\u0026amp;feature=youtu.be\"\u003eJacob Collier\u003c/a\u003e on making MusicFX DJ simpler to use, especially for new or aspiring musicians. And we updated our music AI toolkit \u003ca href=\"https://deepmind.google/discover/blog/new-generative-ai-tools-open-the-doors-of-music-creation/\"\u003eMusic AI Sandbox\u003c/a\u003e, and evolved our \u003ca href=\"https://blog.youtube/inside-youtube/ai-and-music-experiment/\"\u003eDream Track experiment\u003c/a\u003e which allowed U.S. creators to explore a range of genres and prompts that generate instrumental soundtracks with powerful text-to-music models.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"10\" thumbnail-alt=\"Animation with audio showing a MusicFX DJ creation in the making, combining genres such as classical ballet with minimal techno\" subtitle=\"MusicFX DJ generates brand new music by allowing players to mix musical concepts as text prompts.\" video-id=\"qCwnSHbTbr4\" video-type=\"video\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"54tvx\"\u003eLater in 2024, we released state-of-the-art updates to our image and video models: \u003ca href=\"https://deepmind.google/technologies/veo/veo-2/\"\u003eVeo 2\u003c/a\u003e and \u003ca href=\"https://deepmind.google/technologies/imagen-3/\"\u003eImagen 3\u003c/a\u003e. As our highest quality text-to-image model, Imagen 3 is capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models; while Veo demonstrated an improved understanding of real-world physics and the nuances of human movement and expression alongside its overall attention-to-detail and realism.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Supercut of Veo 2-generated videos, including a cartoon girl in a 1980s kitchen talking excitedly to the camera and a car drifting through a cityscape\" external-image=\"\" or-mp4-video-title=\"veo2 supercut\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/veo_2_supercut_16-9_super_g_cta_2_4k9meYD.mp4\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"mbkmb\"\u003eVeo represents a significant step forward in high-quality video generation.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"ptvwc\"\u003eResearch in this field continued apace. We explored ways to use AI to improve editing, for example by \u003ca href=\"https://research.google/blog/smoothly-editing-material-properties-of-objects-with-text-to-image-models-and-synthetic-data/\"\u003eusing it\u003c/a\u003e to control of attributes like transparency, roughness or other physical properties of objects:\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Animation showing eight images, including an apple, polar bear and pumpkin, before and after edits to attributes like transparency and roughness.\" external-image=\"\" or-mp4-video-title=\"alchemist\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Alchemist-mp4-3_1.mp4\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"76ax5\"\u003eIn these examples of AI editing with synthetic data generation, Input shows a novel, held-out image the model has never seen before. Output shows the model output, which successfully edits material properties.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"ptvwc\"\u003eIn the field of \u003ca href=\"https://deepmind.google/discover/blog/generating-audio-for-video/\"\u003eaudio generation\u003c/a\u003e, we announced improvements to video-to-audio (V2A) technology, which can generate dynamic soundscapes through natural language text prompts based on on-screen action. This technology is pairable with AI-created video through \u003ca href=\"https://deepmind.google/technologies/veo/\"\u003eVeo\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"1l0uf\"\u003eGames are an ideal environment for creative exploration of new worlds, as well as training and evaluating embodied agents. In 2024, we introduced \u003ca href=\"https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/\"\u003eGenie 2\u003c/a\u003e, a foundation world model capable of generating an endless variety of action-controllable, playable 3D environments for training and evaluating embodied agents. This followed the \u003ca href=\"https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/\"\u003eintroduction of SIMA\u003c/a\u003e, a Scalable Instructable Multiworld Agent that can follow natural-language instructions to carry out tasks in a variety of video game settings.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"ptvwc\"\u003eThe architecture of intelligence: advances in robotics, hardware and computing\u003c/h2\u003e\u003cp data-block-key=\"9be7g\"\u003eAs our multimodal models become more capable and gain a better understanding of the world and its physics, they are making possible incredible new advances in robotics and bringing us closer to our goal of ever-more capable and helpful robots.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"A bi-arm robot straightening shoe laces of a black and white sneaker and tying them into a bow.\" external-image=\"\" or-mp4-video-title=\"robot dexterity\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/GDM-RobotDexterity-AL-LaceUp_1.mp4\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"76ax5\"\u003eWith ALOHA Unleashed, our robot learned to tie a shoelace, hang a shirt, repair another robot, insert a gear and even clean a kitchen.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"ptvwc\"\u003eAt the beginning of the year, we \u003ca href=\"https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/\"\u003eintroduced\u003c/a\u003e AutoRT, SARA-RT and RT-Trajectory, extensions of our \u003ca href=\"https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/\"\u003eRobotics Transformers\u003c/a\u003e work intended to help robots better understand and navigate their environments, and make decisions faster. We also published \u003ca href=\"https://aloha-unleashed.github.io/assets/aloha_unleashed.pdf\"\u003eALOHA Unleashed\u003c/a\u003e, a breakthrough in teaching robots on how to use two robotic arms in coordination, and \u003ca href=\"https://arxiv.org/abs/2409.06613\"\u003eDemoStart\u003c/a\u003e, which uses a reinforcement learning algorithm to improve real-world performance on a multi-fingered robotic hand by using simulations.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"A chart showing robot training text and image inputs, such as text saying: Put the strawberry into the correct bowl, with a corresponding image showing the action.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"76ax5\"\u003eRobotic Transformer 2 (RT-2) is a novel vision-language-action model that learns from both web and robotics data.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"A chart showing robot training text and image inputs, such as text saying: Put the strawberry into the correct bowl, with a corresponding image showing the action.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Fig_1.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Fig_1.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Fig_1.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"ptvwc\"\u003eBeyond robotics, our \u003ca href=\"https://deepmind.google/discover/blog/how-alphachip-transformed-computer-chip-design/\"\u003eAlphaChip\u003c/a\u003e reinforcement learning method for accelerating and improving chip floorplanning is transforming the design process for chips found in data centers, smartphones and more. To accelerate adoption of these techniques, we released a \u003ca href=\"https://github.com/google-research/circuit_training/?tab=readme-ov-file#PreTrainedModelCheckpoint\"\u003epre-trained checkpoint\u003c/a\u003e to enable external parties to more easily make use of the \u003ca href=\"https://github.com/google-research/circuit_training\"\u003eAlphaChip open source release\u003c/a\u003e for their own chip designs. And we made \u003ca href=\"https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga\"\u003eTrillium\u003c/a\u003e, our sixth-generation and most performant TPU to date, generally available to Google Cloud customers. Advances in computer chips have accelerated AI. And now, AI can return the favor.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Left: Animation showing AlphaChip placing the open-source, Ariane RISC-V CPU, with no prior experience. Right: Animation showing AlphaChip placing the same block after having practiced on 20 TPU-related designs.\" external-image=\"\" or-mp4-video-title=\"alphachip\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/AlphaChip-combined-240925-final_2.mp4\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"76ax5\"\u003eAlphaChip can learn the relationships between interconnected chip components and generalize across chips, letting AlphaChip improve with each layout it designs.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"ptvwc\"\u003eOur research also focused on correcting the errors in the physical hardware of today\u0026#39;s quantum computers. In November, we launched \u003ca href=\"https://blog.google/technology/google-deepmind/alphaqubit-quantum-error-correction/\"\u003eAlphaQubit\u003c/a\u003e, an AI-based decoder that identifies quantum computing errors with state-of-the-art accuracy. This collaborative work brought together Google DeepMind’s ML knowledge and Google Research’s error correction expertise to accelerate progress on building a reliable quantum computer. In tests, it made 6% fewer errors than tensor network methods and 30% fewer errors than correlated matching.\u003c/p\u003e\u003cp data-block-key=\"fnds2\"\u003eThen in December, the Google Quantum AI team, part of Google Research, announced \u003ca href=\"https://blog.google/technology/research/google-willow-quantum-chip/\"\u003eWillow\u003c/a\u003e, our latest quantum chip which can perform in under five minutes a benchmark computation that would take one of today’s fastest supercomputers 10 septillion years. Willow can reduce errors exponentially as it scales up using more qubits. In fact, it used our quantum error correction to cut the error rate in half, solving a 30+ year challenge known in the field as “below threshold.” This leap forward won the \u003ca href=\"https://physicsworld.com/a/top-10-breakthroughs-of-the-year-in-physics-for-2024-revealed/\"\u003ePhysics Breakthrough of the Year\u003c/a\u003e award.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Animation showing the California coast behind a quantum chip and the word Willow\" external-image=\"\" or-mp4-video-title=\"willow\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/05_Hero_Shot_tS16UY7.mp4\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"76ax5\"\u003eWillow has state-of-the-art performance across a number of metrics.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"ptvwc\"\u003eUncovering new solutions: progress in science, biology and mathematics\u003c/h2\u003e\u003cp data-block-key=\"a6of\"\u003eWe continued to push the envelope on accelerating scientific progress with AI-based approaches, releasing a series of tools and papers this year that showed just how useful and powerful a tool AI is for advancing science and mathematics. We\u0026#39;re sharing a few highlights.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"0nbhx\"\u003eIn January, we introduced \u003ca href=\"https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/\"\u003eAlphaGeometry\u003c/a\u003e, an AI system engineered to solve complex geometry problems. Our updated version, AlphaGeometry 2, and AlphaProof, a reinforcement-learning-based system for formal math reasoning, \u003ca href=\"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\"\u003eachieved the same level as a silver medalist\u003c/a\u003e in July 2024’s \u003ca href=\"https://www.imo2024.uk/\"\u003eInternational Mathematical Olympiad\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"medium\" alt-text=\"A geometric diagram featuring a triangle ABC inscribed in a larger circle, with various points, lines and another smaller circle intersecting the triangle\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"lqpje\"\u003eAlphaGeometry 2 solved Problem 4 in July 2024’s International Mathematical Olympiad within 19 seconds after receiving its formalization. Problem 4 asked to prove the sum of ∠KIL and ∠XPY equals 180°.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"A geometric diagram featuring a triangle ABC inscribed in a larger circle, with various points, lines and another smaller circle intersecting the triangle\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Social_03.width-100.format-webp_GDqJDqv.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Social_03.width-500.format-webp_Po64eKY.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Social_03.width-1000.format-webp_urYV1EA.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"0nbhx\"\u003eIn collaboration with Isomorphic Labs, we introduced \u003ca href=\"https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model\"\u003eAlphaFold 3\u003c/a\u003e, our latest model which predicts the structure and interactions of all of life’s molecules. By accurately predicting the structure of proteins, DNA, RNA, ligands and more, and how they interact, we hope it will transform our understanding of the biological world and drug discovery.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"medium\" alt-text=\"Colorful protein structure against an abstract pink and blue gradient background\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"lqpje\"\u003eAlphaFold 3’s capabilities come from its next-generation architecture and training that now covers all of life’s molecules.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Colorful protein structure against an abstract pink and blue gradient background\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2_crop.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2_crop.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2_crop.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"0nbhx\"\u003eWe made several key developments in protein-shaping. We announced \u003ca href=\"https://arxiv.org/abs/2409.08022\"\u003eAlphaProteo\u003c/a\u003e, an AI system for designing novel, high-strength protein binders. AlphaProteo can lead to the discovery of new drugs, the development of biosensors and improve our understanding of biological processes.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"medium\" alt-text=\"Illustration of a predicted protein binder structure in blue interacting with a target protein in yellow\" external-image=\"\" or-mp4-video-title=\"gdm protein\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/GDM-ProteinDesignBlog-02-Binder-Final.mp4\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"lqpje\"\u003eAlphaProteo can generate new protein binders for diverse target proteins.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"0nbhx\"\u003eIn collaboration with Harvard’s Lichtman Lab and others, we \u003ca href=\"https://www.science.org/doi/10.1126/science.adk4858\"\u003eproduced\u003c/a\u003e a nano-scale mapping of a piece of the human brain at a level of detail never previously achieved, and made it publicly available for researchers to build on. This follows \u003ca href=\"https://research.google/blog/ten-years-of-neuroscience-at-google-yields-maps-of-human-brain/\"\u003ea decade of working to advance our understanding of connectomics\u003c/a\u003e, with earlier work on fly brain and mouse brain connectomics now giving way to the larger scale and more complex human brain connectomics.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"small\" alt-text=\"Magnified animation of lime green and bright purple cells spinning together in a mirror image\" external-image=\"\" or-mp4-video-title=\"connectomics\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Connectomics2024-5-MirroredPairs.mp4\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"lqpje\"\u003eIn the deepest layer of the cortex, clusters of cells tend to occur in mirror-image orientation to one another, as shown in this brain mapping project.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"0nbhx\"\u003eThen in late November, as part of a \u003ca href=\"https://deepmind.google/public-policy/ai-for-science/\"\u003ebroader effort\u003c/a\u003e to expand and deepen public dialogue around science and AI, we co-hosted \u003ca href=\"https://blog.google/technology/ai/ai-science-forum-2024/\"\u003ethe AI for Science Forum\u003c/a\u003e with the Royal Society, which convened \u003ca href=\"https://www.youtube.com/watch?v=nQKmVhLIGcs\"\u003escientists\u003c/a\u003e, researchers, governmental leaders and executives to discuss \u003ca href=\"https://blog.google/technology/ai/google-ai-big-scientific-breakthroughs-2024/\"\u003ekey topics\u003c/a\u003e like cracking the protein structure prediction challenge, mapping the human brain and saving lives through accurate forecasting and spotting wildfires. We hosted a Q\u0026amp;A with the four Nobel Laureates in attendance at the forum, Sir Paul Nurse, Jennifer Doudna, Demis Hassabis and John Jumper, which is available to listen to via the Google DeepMind \u003ca href=\"https://www.youtube.com/watch?v=nQKmVhLIGcs\"\u003epodcast\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"att9\"\u003eThis was also a landmark year for another reason: Demis Hassabis and John Jumper, along with David Baker, were awarded the \u003ca href=\"https://deepmind.google/discover/blog/demis-hassabis-john-jumper-awarded-nobel-prize-in-chemistry/\"\u003e2024 Nobel Prize® in Chemistry\u003c/a\u003e for their work on AlphaFold 2. As the Nobel committee \u003ca href=\"https://x.com/NobelPrize/status/1866544078424347074\"\u003erecognized\u003c/a\u003e, their work:\u003c/p\u003e\u003cp data-block-key=\"sqa8\"\u003e\u003ci\u003e\u0026#34;[H]as opened up completely new possibilities to design proteins that have never been seen before, and we now have access to predicted structures of all 200 million known proteins. These are truly great achievements.\u0026#34;\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"4p4sk\"\u003eIt was also exciting to see the \u003ca href=\"https://www.nobelprize.org/prizes/physics/2024/press-release/\"\u003e2024 Nobel Prize® in Physics\u003c/a\u003e awarded to recently retired long-time Googler Geoffrey Hinton (along with John Hopfield), \u0026#34;for foundational discoveries and inventions that enable machine learning with artificial neural networks.”\u003c/p\u003e\u003cp data-block-key=\"c66gf\"\u003eThe Nobels followed additional recognitions for Google including the \u003ca href=\"https://blog.neurips.cc/2024/11/27/announcing-the-neurips-2024-test-of-time-paper-awards/\"\u003eNeurIPS 2024 Test of Time Paper Awards\u003c/a\u003e for \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf\"\u003eSequence to Sequence Learning with Neural Networks\u003c/a\u003e and \u003ca href=\"http://papers.neurips.cc/paper/5423-generative-adversarial-nets.pdf\"\u003eGenerative Adversarial Nets\u003c/a\u003e, and the \u003ca href=\"https://research.google/blog/google-research-2024-breakthroughs-for-impact-at-every-scale/\"\u003eBeale—Orchard-Hays Prize\u003c/a\u003e, which was awarded to a collaborative team of educators and Google professionals for groundbreaking work on \u003ca href=\"https://research.google/blog/scaling-up-linear-programming-with-pdlp/\"\u003ePrimal-Dual Linear Programming (PDLP)\u003c/a\u003e. (PDLP, now part of \u003ca href=\"https://developers.google.com/optimization/lp/pdlp_math\"\u003eGoogle OR Tools\u003c/a\u003e, helps solve large-scale linear programming problems with real-world applications from \u003ca href=\"https://cloud.google.com/blog/topics/systems/the-evolution-of-googles-jupiter-data-center-network\"\u003edata center network traffic engineering\u003c/a\u003e to \u003ca href=\"https://research.google/blog/heuristics-on-the-high-seas-mathematical-optimization-for-cargo-ships/\"\u003econtainer shipping optimization\u003c/a\u003e.)\u003c/p\u003e\u003ch2 data-block-key=\"6fmk1\"\u003eAI for the benefit of humanity\u003c/h2\u003e\u003cp data-block-key=\"lhu9\"\u003eThis year, we made a number of product advances and published research that showed how AI can benefit people directly and immediately, ranging from preventative and diagnostic medicine to disaster readiness and recovery to learning.\u003c/p\u003e\u003cp data-block-key=\"c644f\"\u003eIn healthcare, AI holds the promise of democratizing quality of care in key areas, such as early \u003ca href=\"https://research.google/blog/a-step-towards-making-heart-health-screening-accessible-for-billions-with-ppg-signals/#:~:text=We%20demonstrate%20that%20PPG%20signals,%2C%20strokes%2C%20and%20related%20deaths.\"\u003edetection of cardiovascular disease\u003c/a\u003e. Our \u003ca href=\"https://journals.plos.org/globalpublichealth/article?id=10.1371/journal.pgph.0003204\"\u003eresearch\u003c/a\u003e demonstrated how using a simple fingertip device that measures variations in blood flow, combined with basic metadata, can predict heart health risks. We built on previous AI-enabled diagnostic research for tuberculosis, \u003ca href=\"https://ai.nejm.org/doi/full/10.1056/AIoa2400018?query=ai_toc\u0026amp;cid=DM2362435_Non_Subscriber\u0026amp;bid=-1741711593\"\u003edemonstrating\u003c/a\u003e how AI models can be used for accurate TB screenings in populations with high rates of TB and HIV. This is important to reducing the prevalence of TB (more than \u003ca href=\"https://www.who.int/news-room/fact-sheets/detail/tuberculosis\"\u003e10 million people\u003c/a\u003e fall ill with it each year), as roughly 40% of people with TB go \u003ca href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC10443783/#:~:text=BACKGROUND%3A,were%20not%20diagnosed%20or%20treated.\"\u003eundiagnosed\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Scatter plot showing how various models perform on the MedQA US Medical Licensing Exam (USMLE)-style question benchmark, with Med-Gemini achieving 91.1% accuracy\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"lqpje\"\u003eOn the MedQA (USMLE-style) benchmark, Med-Gemini attains a new state-of-the-art score, surpassing our prior best (\u003ca href=\"https://arxiv.org/abs/2305.09617\"\u003eMed-PaLM 2\u003c/a\u003e) by a significant margin of 4.6%.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Scatter plot showing how various models perform on the MedQA US Medical Licensing Exam (USMLE)-style question benchmark, with Med-Gemini achieving 91.1% accuracy\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Med-Gemini-2b-MedQA.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Med-Gemini-2b-MedQA.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Med-Gemini-2b-MedQA.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"0nbhx\"\u003eOur Gemini model is a powerful tool for professionals generally, but our teams are also working to create fine-tuned models for other domains. For example, we introduced \u003ca href=\"https://research.google/blog/advancing-medical-ai-with-med-gemini/\"\u003eMed-Gemini\u003c/a\u003e, a new family of next-generation models that combine training on de-identified medical data with Gemini’s reasoning, multimodal and long-context abilities. On the MedQA US Medical Licensing Exam (USMLE)-style question benchmark, Med-Gemini \u003ca href=\"https://arxiv.org/pdf/2404.18416\"\u003eachieves\u003c/a\u003e a state-of-the-art performance of 91.1% accuracy, surpassing our prior best of Med-PaLM 2 by 4.6% (shown above).\u003c/p\u003e\u003cp data-block-key=\"3c0lp\"\u003eWe are exploring how machine learning can help medical fields struggling with access to imaging expertise, such as \u003ca href=\"https://research.google/blog/health-specific-embedding-tools-for-dermatology-and-pathology/\"\u003eradiology, dermatology and pathology\u003c/a\u003e. In the past year, we \u003ca href=\"https://research.google/blog/health-specific-embedding-tools-for-dermatology-and-pathology/\"\u003ereleased\u003c/a\u003e two research tools, \u003ca href=\"https://github.com/Google-Health/imaging-research/tree/master/derm-foundation\"\u003eDerm Foundation\u003c/a\u003e and \u003ca href=\"https://github.com/Google-Health/imaging-research/tree/master/path-foundation\"\u003ePath Foundation\u003c/a\u003e, that can help develop models for diagnostic tasks, image indexing and curation and biomarker discovery and validation. We collaborated with physicians at Stanford Medicine on an open-access, inclusive \u003ca href=\"https://github.com/google-research-datasets/scin\"\u003eSkin Condition Image Network (SCIN) dataset\u003c/a\u003e. And we unveiled \u003ca href=\"https://research.google/blog/taking-medical-imaging-embeddings-3d/\"\u003eCT Foundation\u003c/a\u003e, a medical imaging embedding tool used for rapidly training models for research.\u003c/p\u003e\u003cp data-block-key=\"2c9jb\"\u003eWith regard to learning, we explored new generative AI tools to support educators and learners. We introduced \u003ca href=\"https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/\"\u003eLearnLM\u003c/a\u003e, our new family of models fine-tuned for learning and used it to enhance learning experiences in products like Search, YouTube and Gemini; a recent report showed LearnLM \u003ca href=\"https://blog.google/feed/learnlm-technical-report/\"\u003eoutperformed\u003c/a\u003e other leading AI models. We also \u003ca href=\"https://ai.google.dev/gemini-api/docs/learnlm\"\u003emade it available\u003c/a\u003e to developers as an experimental model in AI Studio. Our new conversational learning companion, \u003ca href=\"https://learning.google.com/experiments/learn-about/signup\"\u003eLearnAbout\u003c/a\u003e, uses AI to help you dive deeper into any topic you’re curious about, while \u003ca href=\"https://illuminate.google.com/home?pli=1\"\u003eIlluminate\u003c/a\u003e lets you turn content into engaging AI-generated audio discussions.\u003c/p\u003e\u003cp data-block-key=\"foh2b\"\u003eIn the fields of disaster forecasting and preparedness, we announced several breakthroughs. We introduced \u003ca href=\"https://deepmind.google/discover/blog/gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-sota-accuracy/\"\u003eGenCast\u003c/a\u003e, our new high-resolution AI ensemble model, which improves day-to-day weather and extreme events forecasting across all possible weather trajectories. We also introduced our \u003ca href=\"https://research.google/blog/fast-accurate-climate-modeling-with-neuralgcm/\"\u003eNeuralGCM model\u003c/a\u003e, able to simulate over 70,000 days of the atmosphere in the time it would take a physics-based model to simulate only 19 days. And \u003ca href=\"https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/\"\u003eGraphCast\u003c/a\u003e won the \u003ca href=\"https://raeng.org.uk/news/ai-weather-forecasting-tech-wins-uk-s-top-engineering-award\"\u003e2024 MacRobert Award\u003c/a\u003e for engineering innovation.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Animated map of the world showing GraphCast’s predictions across 10 days, with a blue/white color scheme for specific humidity; yellow/orange/purple for surface temperature; and blue/purple for surface wind speed\" external-image=\"\" or-mp4-video-title=\"GraphCast\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/A_selection_of_GraphCasts_predictions_rolling_across_10_days.mp4\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"lqpje\"\u003eThis selection of GraphCast’s predictions rolling across 10 days shows specific humidity at 700 hectopascals (about 3 kilometers above surface), surface temperature and surface wind speed.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"0nbhx\"\u003eWe also improved our \u003ca href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\"\u003eflood forecasting model\u003c/a\u003e to predict flooding seven days in advance (up from five) and expanded our riverine flood forecasting coverage to 100 countries and 700 million people. This marks a significant milestone in a multi-year initiative that Google Research embarked on in 2018.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Two maps: one showing expanded flood forecasting coverage in Google’s Flood Hub, the other showing additional shaded areas to represent virtual gauge locations on the same map\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"lqpje\"\u003eOur flood forecasting model is now available in over 100 countries (left), and we now have “virtual gauges” for experts and researchers in more than 150 countries, including countries where physical gauges are not available.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"Two maps: one showing expanded flood forecasting coverage in Google’s Flood Hub, the other showing additional shaded areas to represent virtual gauge locations on the same map\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Flood-Forecasting.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Flood-Forecasting.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Flood-Forecasting.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"0nbhx\"\u003eAI can also help with wildfire detection and mitigation, which is especially top of mind given the devastation in California. Our \u003ca href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\"\u003eWildfire Boundary Maps capabilities\u003c/a\u003e are now available in 22 countries. Alongside leading wildfire authorities, Google Research also created \u003ca href=\"https://blog.google/outreach-initiatives/sustainability/google-ai-wildfire-detection/\"\u003eFireSat\u003c/a\u003e, a constellation of satellites that can detect and track wildfires as small as a classroom (roughly 5x5 meters) within 20 minutes.\u003c/p\u003e\u003cp data-block-key=\"eirua\"\u003eAnd we continued building on our commitment to making more information more accessible to more people, \u003ca href=\"https://blog.google/products/translate/google-translate-new-languages-2024/\"\u003eexpanding Google Translate\u003c/a\u003e with 110 new languages, including Cantonese, Papua New Guinea’s Tok Pisin, N’Ko from West Africa and Manx from the Isle of Man. Google Translate — which now supports over 240 languages — can help people overcome barriers to information, knowledge and opportunity.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Animation on a white background showing names of new languages in Google Translate including Luo, Wolof and Veneto.\" external-image=\"\" or-mp4-video-title=\"translate\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Translate_Blog_Hero_Motion.mp4\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"lqpje\"\u003eThese new languages in Google Translate represent more than 614 million speakers, opening up translations for around 8% of the world’s population.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"0nbhx\"\u003eHelping set the standard in responsible AI\u003c/h2\u003e\u003cp data-block-key=\"2ek2e\"\u003eWe furthered our industry-leading research in AI safety, developing new tools and techniques and integrating these advances into our latest models. We’re committed to working with others to address risks.\u003c/p\u003e\u003cp data-block-key=\"1b9e8\"\u003eWe continued \u003ca href=\"https://deepmind.google/discover/blog/mapping-the-misuse-of-generative-ai/\"\u003eresearching\u003c/a\u003e misuse, conducting a study that found the two most common types of misuse were deep fakes and jailbreaks. In May, we introduced \u003ca href=\"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/\"\u003eThe Frontier Safety Framework\u003c/a\u003e, which established protocols for identifying the emerging capabilities of our most advanced AI models, and launched our \u003ca href=\"https://ai.google/static/documents/ai-responsibility-2024-update.pdf\"\u003eAI Responsibility Lifecycle framework\u003c/a\u003e to the public. In October, we \u003ca href=\"https://developers.googleblog.com/en/evolving-the-responsible-generative-ai-toolkit-with-new-tools-for-every-llm/\"\u003eexpanded\u003c/a\u003e our \u003ca href=\"https://ai.google.dev/responsible\"\u003eResponsible GenAI Toolkit\u003c/a\u003e to work with any LLM, giving developers more tools to build AI responsibly.\u003c/p\u003e\u003cp data-block-key=\"e9lg5\"\u003eAnd, among our other efforts, we released a paper this year on \u003ca href=\"https://deepmind.google/discover/blog/the-ethics-of-advanced-ai-assistants/\"\u003eThe Ethics of Advanced AI Assistants\u003c/a\u003e that examined and mapped the new technical and moral landscape of a future populated by AI assistants, and characterized the opportunities and risks society might face.\u003c/p\u003e\u003cp data-block-key=\"4s8k9\"\u003eWe expanded \u003ca href=\"https://deepmind.google/technologies/synthid/\"\u003eSynthID’s capabilities\u003c/a\u003e to watermarking AI-generated text in the \u003ca href=\"https://gemini.google.com/\"\u003eGemini app and web experience\u003c/a\u003e, and video in \u003ca href=\"http://deepmind.google/technologies/veo\"\u003eVeo\u003c/a\u003e. To help increase overall transparency online, not just with content created by Google gen AI tools, we also \u003ca href=\"https://c2pa.org/post/google_pr/\"\u003ejoined\u003c/a\u003e the Coalition for Content Provenance and Authenticity (C2PA) as a steering committee member and \u003ca href=\"https://blog.google/technology/ai/google-gen-ai-content-transparency-c2pa/\"\u003ecollaborated\u003c/a\u003e on a new, more secure version of the technical standard, Content Credentials.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"An animation showing an LLM generating a sentence reading: My favourite tropical fruits are mango and banana, with probability scores appearing for various predicted tokens.\" external-image=\"\" or-mp4-video-title=\"fig1\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Fig_1.mp4\" section-header=\"2024: A year of extraordinary progress and advancement in AI\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"lqpje\"\u003eWhen there’s a range of different tokens to choose from, SynthID can adjust the probability score of each predicted token, in cases where it won’t compromise the quality, accuracy and creativity of the output.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;2024: A year of extraordinary progress and advancement in AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"0nbhx\"\u003eBeyond LLMs, we shared our approach to \u003ca href=\"https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphafold-3-predicts-the-structure-and-interactions-of-all-lifes-molecules/Our-approach-to-biosecurity-for-AlphaFold-3-08052024\"\u003ebiosecurity\u003c/a\u003e for \u003ca href=\"https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/\"\u003eAlphaFold 3\u003c/a\u003e. We also worked with industry partners to launch the \u003ca href=\"https://blog.google/technology/safety-security/google-coalition-for-secure-ai/\"\u003eCoalition for Secure AI\u003c/a\u003e (CoSAI), and we participated in the \u003ca href=\"https://deepmind.google/discover/blog/looking-ahead-to-the-ai-seoul-summit/\"\u003eAI Seoul Summit\u003c/a\u003e, as a way of building and contributing to an international consensus and a common, coordinated approach to governance.\u003c/p\u003e\u003cp data-block-key=\"6lk35\"\u003eAs we develop new technologies like AI agents, we’ll continue to encounter new questions around safety, security and privacy. Guided by our \u003ca href=\"https://ai.google/responsibility/principles/\"\u003eAI Principles\u003c/a\u003e, we are \u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/\"\u003edeliberately taking\u003c/a\u003e an exploratory and gradual approach to development, conducting research on multiple prototypes, iteratively implementing safety training, working with trusted testers and external experts and performing extensive risk assessments and safety and assurance evaluations.\u003c/p\u003e\u003ch2 data-block-key=\"bq98t\"\u003eLooking ahead to 2025\u003c/h2\u003e\u003cp data-block-key=\"7aeqd\"\u003e2024 was a productive year, and a very exciting time for groundbreaking new products and research in AI. We made a great deal of progress and we’re even more excited about the year ahead.\u003c/p\u003e\u003cp data-block-key=\"17qq5\"\u003eAs we continue to produce groundbreaking AI research in the fields of products, science, health, creativity and more, it becomes increasingly important to think deeply about how and when it should be deployed. By continuing to prioritize responsible AI practices and fostering collaboration, we’ll play an important role in building a future where AI benefits humanity.\u003c/p\u003e\u003c/div\u003e\n  \n\n\n            \n            \n\n            \n              \n\n\n\n\n            \n          \u003c/div\u003e\n  \u003c/article\u003e\u003c/div\u003e",
  "readingTime": "28 min read",
  "publishedTime": "2025-01-23T16:00:00Z",
  "modifiedTime": null
}
