{
  "id": "4ecde13e-d1f0-43a1-b9ef-66151b4a0af0",
  "title": "Why We're Unlikely to Get Artificial General Intelligence Any Time Soon",
  "link": "https://slashdot.org/story/25/05/19/003225/why-were-unlikely-to-get-artificial-general-intelligence-any-time-soon?utm_source=rss1.0mainlinkanon\u0026utm_medium=feed",
  "description": "OpenAI CEO and Sam Altman believe Artificial General Intelligence could arrive within the next few years. But the speculations of some technologists \"are getting ahead of reality,\" writes the New York Times, adding that many scientists \"say no one will reach AGI without a new idea — something beyond the powerful neural networks that merely find patterns in data. That new idea could arrive tomorrow. But even then, the industry would need years to develop it.\" \"The technology we're building today is not sufficient to get there,\" said Nick Frosst, a founder of the AI startup Cohere who previously worked as a researcher at Google and studied under the most revered AI researcher of the last 50 years. \"What we are building now are things that take in words and predict the next most likely word, or they take in pixels and predict the next most likely pixel. That's very different from what you and I do.\" In a recent survey of the Association for the Advancement of Artificial Intelligence, a 40-year-old academic society that includes some of the most respected researchers in the field, more than three-quarters of respondents said the methods used to build today's technology were unlikely to lead to AGI. Opinions differ in part because scientists cannot even agree on a way of defining human intelligence, arguing endlessly over the merits and flaws of IQ tests and other benchmarks. Comparing our own brains to machines is even more subjective. This means that identifying AGI is essentially a matter of opinion.... And scientists have no hard evidence that today's technologies are capable of performing even some of the simpler things the brain can do, like recognizing irony or feeling empathy. Claims of AGI's imminent arrival are based on statistical extrapolations — and wishful thinking. According to various benchmark tests, today's technologies are improving at a consistent rate in some notable areas, like math and computer programming. But these tests describe only a small part of what people can do. Humans know how to deal with a chaotic and constantly changing world. Machines struggle to master the unexpected — the challenges, small and large, that do not look like what has happened in the past. Humans can dream up ideas that the world has never seen. Machines typically repeat or enhance what they have seen before. That is why Frosst and other sceptics say pushing machines to human-level intelligence will require at least one big idea that the world's technologists have not yet dreamed up. There is no way of knowing how long that will take. \"A system that's better than humans in one way will not necessarily be better in other ways,\" Harvard University cognitive scientist Steven Pinker said. \"There's just no such thing as an automatic, omniscient, omnipotent solver of every problem, including ones we haven't even thought of yet. There's a temptation to engage in a kind of magical thinking. But these systems are not miracles. They are very impressive gadgets.\" While Google's AlphaGo could be humans in a game with \"a small, limited set of rules,\" the article points out that tthe real world \"is bounded only by the laws of physics. Modelling the entirety of the real world is well beyond today's machines, so how can anyone be sure that AGI — let alone superintelligence — is just around the corner?\" And they offer this alternative perspective from Matteo Pasquinelli, a professor of the philosophy of science at Ca' Foscari University in Venice, Italy. \"AI needs us: living beings, producing constantly, feeding the machine. It needs the originality of our ideas and our lives.\" Read more of this story at Slashdot.",
  "author": "EditorDavid",
  "published": "2025-05-19T00:06:00+00:00",
  "source": "http://rss.slashdot.org/Slashdot/slashdotMain",
  "categories": [
    "ai"
  ],
  "byline": "",
  "length": 3645,
  "excerpt": "OpenAI CEO and Sam Altman believe Artificial General Intelligence could arrive within the next few years. But the speculations of some technologists \"are getting ahead of reality,\" writes the New York Times, adding that many scientists \"say no one will reach AGI without a new idea — something...",
  "siteName": "",
  "favicon": "",
  "text": "OpenAI CEO and Sam Altman believe Artificial General Intelligence could arrive within the next few years. But the speculations of some technologists \"are getting ahead of reality,\" writes the New York Times, adding that many scientists \"say no one will reach AGI without a new idea — something beyond the powerful neural networks that merely find patterns in data. That new idea could arrive tomorrow. But even then, the industry would need years to develop it.\" \"The technology we're building today is not sufficient to get there,\" said Nick Frosst, a founder of the AI startup Cohere who previously worked as a researcher at Google and studied under the most revered AI researcher of the last 50 years. \"What we are building now are things that take in words and predict the next most likely word, or they take in pixels and predict the next most likely pixel. That's very different from what you and I do.\" In a recent survey of the Association for the Advancement of Artificial Intelligence, a 40-year-old academic society that includes some of the most respected researchers in the field, more than three-quarters of respondents said the methods used to build today's technology were unlikely to lead to AGI. Opinions differ in part because scientists cannot even agree on a way of defining human intelligence, arguing endlessly over the merits and flaws of IQ tests and other benchmarks. Comparing our own brains to machines is even more subjective. This means that identifying AGI is essentially a matter of opinion.... And scientists have no hard evidence that today's technologies are capable of performing even some of the simpler things the brain can do, like recognizing irony or feeling empathy. Claims of AGI's imminent arrival are based on statistical extrapolations — and wishful thinking. According to various benchmark tests, today's technologies are improving at a consistent rate in some notable areas, like math and computer programming. But these tests describe only a small part of what people can do. Humans know how to deal with a chaotic and constantly changing world. Machines struggle to master the unexpected — the challenges, small and large, that do not look like what has happened in the past. Humans can dream up ideas that the world has never seen. Machines typically repeat or enhance what they have seen before. That is why Frosst and other sceptics say pushing machines to human-level intelligence will require at least one big idea that the world's technologists have not yet dreamed up. There is no way of knowing how long that will take. \"A system that's better than humans in one way will not necessarily be better in other ways,\" Harvard University cognitive scientist Steven Pinker said. \"There's just no such thing as an automatic, omniscient, omnipotent solver of every problem, including ones we haven't even thought of yet. There's a temptation to engage in a kind of magical thinking. But these systems are not miracles. They are very impressive gadgets.\" While Google's AlphaGo could be humans in a game with \"a small, limited set of rules,\" the article points out that tthe real world \"is bounded only by the laws of physics. Modelling the entirety of the real world is well beyond today's machines, so how can anyone be sure that AGI — let alone superintelligence — is just around the corner?\" And they offer this alternative perspective from Matteo Pasquinelli, a professor of the philosophy of science at Ca' Foscari University in Venice, Italy. \"AI needs us: living beings, producing constantly, feeding the machine. It needs the originality of our ideas and our lives.\"",
  "image": "https://a.fsdn.com/sd/topics/ai_64.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"fhbody-177626543\"\u003e\u003cp\u003e\n\t\t\t\n\t\t \t\n\t\t\t\tOpenAI CEO and Sam Altman believe Artificial General Intelligence could arrive within the next few years.  But the speculations of some technologists \u0026#34;are getting ahead of reality,\u0026#34; \u003ca href=\"https://www.msn.com/en-in/money/news/why-were-unlikely-to-get-artificial-general-intelligence-anytime-soon/ar-AA1EWy4y\"\u003ewrites the New York Times\u003c/a\u003e, adding that many scientists \u0026#34;say no one will reach AGI without a new idea — something beyond the powerful neural networks that merely find patterns in data. That new idea could arrive tomorrow. But even then, the industry would need years to develop it.\u0026#34;\n\n\n\u003ci\u003e\n\n\u0026#34;The technology we\u0026#39;re building today is not sufficient to get there,\u0026#34; said Nick Frosst, a founder of the AI startup Cohere who previously worked as a researcher at Google and studied under the most revered AI researcher of the last 50 years. \u0026#34;What we are building now are things that take in words and predict the next most likely word, or they take in pixels and predict the next most likely pixel. That\u0026#39;s very different from what you and I do.\u0026#34;  In a recent survey of the Association for the Advancement of Artificial Intelligence, a 40-year-old academic society that includes some of the most respected researchers in the field, more than three-quarters of respondents said the methods used to build today\u0026#39;s technology were unlikely to lead to AGI.\u003cp\u003e \n Opinions differ in part because scientists cannot even agree on a way of defining human intelligence, arguing endlessly over the merits and flaws of IQ tests and other benchmarks. Comparing our own brains to machines is even more subjective. This means that identifying AGI is essentially a matter of opinion....  And scientists have no hard evidence that today\u0026#39;s technologies are capable of performing even some of the simpler things the brain can do, like recognizing irony or feeling empathy. Claims of AGI\u0026#39;s imminent arrival are based on statistical extrapolations — and wishful thinking.    According to various benchmark tests, today\u0026#39;s technologies are improving at a consistent rate in some notable areas, like math and computer programming. But these tests describe only a small part of what people can do.\u003c/p\u003e\u003cp\u003e \n\nHumans know how to deal with a chaotic and constantly changing world. Machines struggle to master the unexpected — the challenges, small and large, that do not look like what has happened in the past. Humans can dream up ideas that the world has never seen. Machines typically repeat or enhance what they have seen before.  That is why Frosst and other sceptics say pushing machines to human-level intelligence will require at least one big idea that the world\u0026#39;s technologists have not yet dreamed up. There is no way of knowing how long that will take.  \u0026#34;A system that\u0026#39;s better than humans in one way will not necessarily be better in other ways,\u0026#34; Harvard University cognitive scientist Steven Pinker said. \u0026#34;There\u0026#39;s just no such thing as an automatic, omniscient, omnipotent solver of every problem, including ones we haven\u0026#39;t even thought of yet. There\u0026#39;s a temptation to engage in a kind of magical thinking. But these systems are not miracles. They are very impressive gadgets.\u0026#34;\u003c/p\u003e\u003c/i\u003e \u003cbr/\u003e\nWhile Google\u0026#39;s AlphaGo could be humans in a game with \u0026#34;a small, limited set of rules,\u0026#34; the article points out that tthe real world \u0026#34;is bounded only by the laws of physics. Modelling the entirety of the real world is well beyond today\u0026#39;s machines, so how can anyone be sure that AGI — let alone superintelligence — is just around the corner?\u0026#34;   And they offer this alternative perspective from Matteo Pasquinelli, a professor of the philosophy of science at Ca\u0026#39; Foscari University in Venice, Italy.\u003c/p\u003e\u003cp\u003e \n \u0026#34;AI needs us: living beings, producing constantly, feeding the machine. It needs the originality of our ideas and our lives.\u0026#34;\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": null,
  "modifiedTime": null
}
