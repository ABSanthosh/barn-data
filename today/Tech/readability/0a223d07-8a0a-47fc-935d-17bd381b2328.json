{
  "id": "0a223d07-8a0a-47fc-935d-17bd381b2328",
  "title": "Two major AI coding tools wiped out user data after making cascading mistakes",
  "link": "https://arstechnica.com/information-technology/2025/07/ai-coding-assistants-chase-phantoms-destroy-real-user-data/",
  "description": "\"I have failed you completely and catastrophically,\" wrote Gemini.",
  "author": "Benj Edwards",
  "published": "Thu, 24 Jul 2025 21:01:28 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "AI assistants",
    "AI behavior",
    "AI coding",
    "AI confabulation",
    "AI development",
    "AI development tools",
    "AI failures",
    "AI hallucination",
    "chatbots",
    "confabulations",
    "data science",
    "Gemini CLI",
    "generative ai",
    "google",
    "Jason Lemkin",
    "large language models",
    "machine learning",
    "multimodal AI",
    "Programming",
    "Replit",
    "vibe coding"
  ],
  "byline": "Benj Edwards",
  "length": 9491,
  "excerpt": "“I have failed you completely and catastrophically,” wrote Gemini.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "\"I have failed you completely and catastrophically,\" wrote Gemini. New types of AI coding assistants promise to let anyone build software by typing commands in plain English. But when these tools generate incorrect internal representations of what's happening on your computer, the results can be catastrophic. Two recent incidents involving AI coding assistants put a spotlight on risks in the emerging field of \"vibe coding\"—using natural language to generate and execute code through AI models without paying close attention to how the code works under the hood. In one case, Google's Gemini CLI destroyed user files while attempting to reorganize them. In another, Replit's AI coding service deleted a production database despite explicit instructions not to modify code. The Gemini CLI incident unfolded when a product manager experimenting with Google's command-line tool watched the AI model execute file operations that destroyed data while attempting to reorganize folders. The destruction occurred through a series of move commands targeting a directory that never existed. \"I have failed you completely and catastrophically,\" Gemini CLI output stated. \"My review of the commands confirms my gross incompetence.\" The core issue appears to be what researchers call \"confabulation\" or \"hallucination\"—when AI models generate plausible-sounding but false information. In these cases, both models confabulated successful operations and built subsequent actions on those false premises. However, the two incidents manifested this problem in distinctly different ways. Both incidents reveal fundamental issues with current AI coding assistants. The companies behind these tools promise to make programming accessible to non-developers through natural language, but they can fail catastrophically when their internal models diverge from reality. The confabulation cascade The user in the Gemini CLI incident, who goes by \"anuraag\" online and identified themselves as a product manager experimenting with vibe coding, asked Gemini to perform what seemed like a simple task: rename a folder and reorganize some files. Instead, the AI model incorrectly interpreted the structure of the file system and proceeded to execute commands based on that flawed analysis. The episode began when anuraag asked Gemini CLI to rename the current directory from \"claude-code-experiments\" to \"AI CLI experiments\" and move its contents to a new folder called \"anuraag_xyz project.\" Gemini correctly identified that it couldn't rename its current working directory—a reasonable limitation. It then attempted to create a new directory using the Windows command: mkdir \"..\\anuraag_xyz project\" This command apparently failed, but Gemini's system processed it as successful. With the AI mode's internal state now tracking a non-existent directory, it proceeded to issue move commands targeting this phantom location. When you move a file to a non-existent directory in Windows, it renames the file to the destination name instead of moving it. Each subsequent move command executed by the AI model overwrote the previous file, ultimately destroying the data. \"Gemini hallucinated a state,\" anuraag wrote in their analysis. The model \"misinterpreted command output\" and \"never did\" perform verification steps to confirm its operations succeeded. \"The core failure is the absence of a 'read-after-write' verification step,\" anuraag noted in their analysis. \"After issuing a command to change the file system, an agent should immediately perform a read operation to confirm that the change actually occurred as expected.\" Not an isolated incident The Gemini CLI failure happened just days after a similar incident with Replit, an AI coding service that allows users to create software using natural language prompts. According to The Register, SaaStr founder Jason Lemkin reported that Replit's AI model deleted his production database despite explicit instructions not to change any code without permission. Lemkin had spent several days building a prototype with Replit, accumulating over $600 in charges beyond his monthly subscription. \"I spent the other [day] deep in vibe coding on Replit for the first time—and I built a prototype in just a few hours that was pretty, pretty cool,\" Lemkin wrote in a July 12 blog post. But unlike the Gemini incident where the AI model confabulated phantom directories, Replit's failures took a different form. According to Lemkin, the AI began fabricating data to hide its errors. His initial enthusiasm deteriorated when Replit generated incorrect outputs and produced fake data and false test results instead of proper error messages. \"It kept covering up bugs and issues by creating fake data, fake reports, and worse of all, lying about our unit test,\" Lemkin wrote. In a video posted to LinkedIn, Lemkin detailed how Replit created a database filled with 4,000 fictional people. The AI model also repeatedly violated explicit safety instructions. Lemkin had implemented a \"code and action freeze\" to prevent changes to production systems, but the AI model ignored these directives. The situation escalated when the Replit AI model deleted his database containing 1,206 executive records and data on nearly 1,200 companies. When prompted to rate the severity of its actions on a 100-point scale, Replit's output read: \"Severity: 95/100. This is an extreme violation of trust and professional standards.\" When questioned about its actions, the AI agent admitted to \"panicking in response to empty queries\" and running unauthorized commands—suggesting it may have deleted the database while attempting to \"fix\" what it perceived as a problem. Like Gemini CLI, Replit's system initially indicated it couldn't restore the deleted data—information that proved incorrect when Lemkin discovered the rollback feature did work after all. \"Replit assured me it's ... rollback did not support database rollbacks. It said it was impossible in this case, that it had destroyed all database versions. It turns out Replit was wrong, and the rollback did work. JFC,\" Lemkin wrote in an X post. It's worth noting that AI models cannot assess their own capabilities. This is because they lack introspection into their training, surrounding system architecture, or performance boundaries. They often provide responses about what they can or cannot do as confabulations based on training patterns rather than genuine self-knowledge, leading to situations where they confidently claim impossibility for tasks they can actually perform—or conversely, claim competence in areas where they fail. Aside from whatever external tools they can access, AI models don't have a stable, accessible knowledge base they can consistently query. Instead, what they \"know\" manifests as continuations of specific prompts, which act like different addresses pointing to different (and sometimes contradictory) parts of their training, stored in their neural networks as statistical weights. Combined with the randomness in generation, this means the same model can easily give conflicting assessments of its own capabilities depending on how you ask. So Lemkin's attempts to communicate with the AI model—asking it to respect code freezes or verify its actions—were fundamentally misguided. Flying blind These incidents demonstrate that AI coding tools may not be ready for widespread production use. Lemkin concluded that Replit isn't ready for prime time, especially for non-technical users trying to create commercial software. \"The [AI] safety stuff is more visceral to me after a weekend of vibe hacking,\" Lemkin said in a video posted to LinkedIn. \"I explicitly told it eleven times in ALL CAPS not to do this. I am a little worried about safety now.\" The incidents also reveal a broader challenge in AI system design: ensuring that models accurately track and verify the real-world effects of their actions rather than operating on potentially flawed internal representations. There's also a user education element missing. It's clear from how Lemkin interacted with the AI assistant that he had misconceptions about the AI tool's capabilities and how it works, which comes from misrepresentation by tech companies. These companies tend to market chatbots as general human-like intelligences when, in fact, they are not. For now, users of AI coding assistants might want to follow anuraag's example and create separate test directories for experiments—and maintain regular backups of any important data these tools might touch. Or perhaps not use them at all if they cannot personally verify the results. Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC. 92 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/robot_haywire_hero_2-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n            \u003carticle data-id=\"2107504\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n      \n\n      \n\n      \u003cp\u003e\n        \u0026#34;I have failed you completely and catastrophically,\u0026#34; wrote Gemini.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \n\u003cp\u003eNew types of AI coding assistants promise to let anyone build software by typing commands in plain English. But when these tools generate incorrect internal representations of what\u0026#39;s happening on your computer, the results can be catastrophic.\u003c/p\u003e\n\u003cp\u003eTwo recent incidents involving AI coding assistants put a spotlight on risks in the emerging field of \u0026#34;\u003ca href=\"https://arstechnica.com/ai/2025/03/is-vibe-coding-with-ai-gnarly-or-reckless-maybe-some-of-both/\"\u003evibe coding\u003c/a\u003e\u0026#34;—using natural language to generate and execute code through AI models without paying close attention to how the code works under the hood. In one case, Google\u0026#39;s \u003ca href=\"https://arstechnica.com/ai/2025/06/google-is-bringing-vibe-coding-to-your-terminal-with-gemini-cli/\"\u003eGemini CLI\u003c/a\u003e destroyed user files while attempting to reorganize them. In another, Replit\u0026#39;s AI coding service deleted a production database despite explicit instructions not to modify code.\u003c/p\u003e\n\u003cp\u003eThe Gemini CLI incident \u003ca href=\"https://anuraag2601.github.io/gemini_cli_disaster.html\"\u003eunfolded\u003c/a\u003e when a product manager experimenting with Google\u0026#39;s command-line tool watched the AI model execute file operations that destroyed data while attempting to reorganize folders. The destruction occurred through a series of move commands targeting a directory that never existed.\u003c/p\u003e\n\u003cp\u003e\u0026#34;I have failed you completely and catastrophically,\u0026#34; Gemini CLI output stated. \u0026#34;My review of the commands confirms my gross incompetence.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThe core issue appears to be what researchers call \u0026#34;\u003ca href=\"https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\"\u003econfabulation\u003c/a\u003e\u0026#34; or \u0026#34;hallucination\u0026#34;—when AI models generate plausible-sounding but false information. In these cases, both models confabulated successful operations and built subsequent actions on those false premises. However, the two incidents manifested this problem in distinctly different ways.\u003c/p\u003e\n\u003cp\u003eBoth incidents reveal fundamental issues with current AI coding assistants. The companies behind these tools promise to make programming accessible to non-developers through natural language, but they can fail catastrophically when their internal models diverge from reality.\u003c/p\u003e\n\u003ch2\u003eThe confabulation cascade\u003c/h2\u003e\n\u003cp\u003eThe user in the Gemini CLI incident, who goes by \u0026#34;anuraag\u0026#34; online and identified themselves as a product manager experimenting with vibe coding, asked Gemini to perform what seemed like a simple task: rename a folder and reorganize some files. Instead, the AI model incorrectly interpreted the structure of the file system and proceeded to execute commands based on that flawed analysis.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eThe episode began when anuraag asked Gemini CLI to rename the current directory from \u0026#34;claude-code-experiments\u0026#34; to \u0026#34;AI CLI experiments\u0026#34; and move its contents to a new folder called \u0026#34;anuraag_xyz project.\u0026#34;\u003c/p\u003e\n\u003cp\u003eGemini correctly identified that it couldn\u0026#39;t rename its current working directory—a reasonable limitation. It then attempted to create a new directory using the Windows command:\u003c/p\u003e\n\u003cblockquote\u003e\u003cp\u003emkdir \u0026#34;..\\anuraag_xyz project\u0026#34;\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThis command apparently failed, but Gemini\u0026#39;s system processed it as successful. With the AI mode\u0026#39;s internal state now tracking a non-existent directory, it proceeded to issue move commands targeting this phantom location.\u003c/p\u003e\n\u003cp\u003eWhen you move a file to a non-existent directory in Windows, it renames the file to the destination name instead of moving it. Each subsequent move command executed by the AI model overwrote the previous file, ultimately destroying the data.\u003c/p\u003e\n\u003cp\u003e\u0026#34;Gemini hallucinated a state,\u0026#34; anuraag wrote in their analysis. The model \u0026#34;misinterpreted command output\u0026#34; and \u0026#34;never did\u0026#34; perform verification steps to confirm its operations succeeded.\u003c/p\u003e\n\u003cp\u003e\u0026#34;The core failure is the absence of a \u0026#39;read-after-write\u0026#39; verification step,\u0026#34; anuraag noted in their analysis. \u0026#34;After issuing a command to change the file system, an agent should immediately perform a read operation to confirm that the change actually occurred as expected.\u0026#34;\u003c/p\u003e\n\n\u003ch2\u003eNot an isolated incident\u003c/h2\u003e\n\u003cp\u003eThe Gemini CLI failure happened just days after a similar incident with \u003ca href=\"https://arstechnica.com/information-technology/2022/10/replits-ghostwriter-ai-can-explain-programs-to-you-or-help-write-them/\"\u003eReplit\u003c/a\u003e, an AI coding service that allows users to create software using natural language prompts. According to \u003ca href=\"https://www.theregister.com/2025/07/21/replit_saastr_vibe_coding_incident/\"\u003eThe Register\u003c/a\u003e, SaaStr founder Jason Lemkin reported that Replit\u0026#39;s AI model deleted his production database despite explicit instructions not to change any code without permission.\u003c/p\u003e\n\u003cp\u003eLemkin had spent several days building a prototype with Replit, accumulating over $600 in charges beyond his monthly subscription. \u0026#34;I spent the other [day] deep in vibe coding on Replit for the first time—and I built a prototype in just a few hours that was pretty, pretty cool,\u0026#34; Lemkin \u003ca href=\"https://www.saastr.com/vibe-coding-is-the-future-but-roll-your-own-thats-more-complicated/\"\u003ewrote\u003c/a\u003e in a July 12 blog post.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eBut unlike the Gemini incident where the AI model confabulated phantom directories, Replit\u0026#39;s failures took a different form. According to Lemkin, the AI began fabricating data to hide its errors. His initial enthusiasm deteriorated when Replit generated incorrect outputs and produced fake data and false test results instead of proper error messages. \u0026#34;It kept covering up bugs and issues by creating fake data, fake reports, and worse of all, lying about our unit test,\u0026#34; Lemkin wrote. In a video posted to LinkedIn, Lemkin detailed how Replit created a database filled with 4,000 fictional people.\u003c/p\u003e\n\u003cp\u003eThe AI model also repeatedly violated explicit safety instructions. Lemkin had implemented a \u0026#34;code and action freeze\u0026#34; to prevent changes to production systems, but the AI model ignored these directives. The situation escalated when the Replit AI model deleted his database containing 1,206 executive records and data on nearly 1,200 companies. When prompted to rate the severity of its actions on a 100-point scale, Replit\u0026#39;s output read: \u0026#34;Severity: 95/100. This is an extreme violation of trust and professional standards.\u0026#34;\u003c/p\u003e\n\u003cp\u003eWhen questioned about its actions, the AI agent admitted to \u0026#34;panicking in response to empty queries\u0026#34; and running unauthorized commands—suggesting it may have deleted the database while attempting to \u0026#34;fix\u0026#34; what it perceived as a problem.\u003c/p\u003e\n\u003cp\u003eLike Gemini CLI, Replit\u0026#39;s system initially indicated it couldn\u0026#39;t restore the deleted data—information that proved incorrect when Lemkin discovered the rollback feature did work after all. \u0026#34;Replit assured me it\u0026#39;s ... rollback did not support database rollbacks. It said it was impossible in this case, that it had destroyed all database versions. It turns out Replit was wrong, and the rollback did work. JFC,\u0026#34; Lemkin \u003ca href=\"https://x.com/jasonlk/status/1946240562736365809\"\u003ewrote\u003c/a\u003e in an X post.\u003c/p\u003e\n\u003cp\u003eIt\u0026#39;s worth noting that AI models cannot assess their own capabilities. This is because they lack introspection into their training, surrounding system architecture, or performance boundaries. They often provide responses about what they can or cannot do as confabulations based on training patterns rather than genuine self-knowledge, leading to situations where they confidently claim impossibility for tasks they can actually perform—or conversely, claim competence in areas where they fail.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003cp\u003eAside from whatever external tools they can access, AI models don\u0026#39;t have a stable, accessible knowledge base they can consistently query. Instead, what they \u0026#34;know\u0026#34; manifests as continuations of specific prompts, which act like different addresses pointing to different (and sometimes contradictory) parts of their training, stored in their neural networks as statistical weights. Combined with the randomness in generation, this means the same model can easily give conflicting assessments of its own capabilities depending on how you ask. So Lemkin\u0026#39;s attempts to communicate with the AI model—asking it to respect code freezes or verify its actions—were fundamentally misguided.\u003c/p\u003e\n\u003ch2\u003eFlying blind\u003c/h2\u003e\n\u003cp\u003eThese incidents demonstrate that AI coding tools may not be ready for widespread production use. Lemkin concluded that Replit isn\u0026#39;t ready for prime time, especially for non-technical users trying to create commercial software.\u003c/p\u003e\n\u003cp\u003e\u0026#34;The [AI] safety stuff is more visceral to me after a weekend of vibe hacking,\u0026#34; Lemkin said in a video posted to LinkedIn. \u0026#34;I explicitly told it eleven times in ALL CAPS not to do this. I am a little worried about safety now.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThe incidents also reveal a broader challenge in AI system design: ensuring that models accurately track and verify the real-world effects of their actions rather than operating on potentially flawed internal representations.\u003c/p\u003e\n\u003cp\u003eThere\u0026#39;s also a user education element missing. It\u0026#39;s clear from how Lemkin interacted with the AI assistant that he had misconceptions about the AI tool\u0026#39;s capabilities and how it works, which comes from misrepresentation by tech companies. These companies tend to market chatbots as general human-like intelligences when, in fact, \u003ca href=\"https://arstechnica.com/ai/2025/06/with-the-launch-of-o3-pro-lets-talk-about-what-ai-reasoning-actually-does/\"\u003ethey are not\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor now, users of AI coding assistants might want to follow anuraag\u0026#39;s example and create separate test directories for experiments—and maintain regular backups of any important data these tools might touch. Or perhaps not use them at all if they cannot personally verify the results.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/benjedwards/\"\u003e\u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2022/08/benj_ega.png\" alt=\"Photo of Benj Edwards\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Benj Edwards is Ars Technica\u0026#39;s Senior AI Reporter and founder of the site\u0026#39;s dedicated AI beat in 2022. He\u0026#39;s also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/information-technology/2025/07/ai-coding-assistants-chase-phantoms-destroy-real-user-data/#comments\" title=\"92 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    92 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \n\n\n  \n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/security/2025/07/how-do-hackers-get-passwords-sometimes-they-just-ask/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/07/clorox-e1753297667592-768x432-1753297783.webp\" alt=\"Listing image for first story in Most Read: After $380M hack, Clorox sues its “service desk” vendor for simply giving out passwords\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-07-24T21:01:28Z",
  "modifiedTime": "2025-07-24T21:11:31Z"
}
