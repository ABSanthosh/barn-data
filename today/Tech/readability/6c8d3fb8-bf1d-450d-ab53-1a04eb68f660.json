{
  "id": "6c8d3fb8-bf1d-450d-ab53-1a04eb68f660",
  "title": "Lm.rs: Minimal CPU LLM inference in Rust with no dependency",
  "link": "https://github.com/samuel-vitorino/lm.rs",
  "description": "Comments",
  "author": "",
  "published": "Fri, 11 Oct 2024 16:46:54 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "samuel-vitorino",
  "length": 4112,
  "excerpt": "Minimal LLM inference in Rust. Contribute to samuel-vitorino/lm.rs development by creating an account on GitHub.",
  "siteName": "GitHub",
  "favicon": "https://github.com/fluidicon.png",
  "text": "lm.rs: run inference on Language Models locally on the CPU with Rust ðŸŒƒ Now supporting multimodality with PHI-3.5-vision model! PHI-3.5-mini text-only model also now supported. Inspired by Karpathy's llama2.c and llm.c I decided to create the most minimal code (not so minimal atm) that can perform full inference on Language Models on the CPU without ML libraries. Previously only Google's Gemma 2 models were supported, but I decided to add support for the new Llama 3.2 models, and more recently the option to use images with PHI-3.5. Image processing/encoding currently takes a bit, so it slows the first response, working on optimization now. Disclaimer: some of the code could be optimized and improved. This is just an excuse for me to write Rust for the first time. Isn't it incredible that in a few years, we could have AGI running in a few lines of poorly written Rust code? Prepared models Some benchmarks and download links for the models and tokenizers. I recommend using Q8_0, Q4_0 quantization still being improved. Speed measured on a 16-core AMD Epyc. Model Size Speed Gemma 2 2B IT Q4_0 1.39G 20 tok/s Gemma 2 2B IT Q8_0 2.66GB 18 tok/s Gemma 2 9B IT Q4_0 4.91GB 7 tok/s Gemma 2 9B IT Q8_0 9.53GB 8 tok/s Llama 3.2 1B IT 4.94GB 20 tok/s Llama 3.2 1B IT Q8_0 1.27GB 35 tok/s Llama 3.2 3B IT Q4_0 1.71GB 17 tok/s Llama 3.2 3B IT Q8_0 3.31GB 16 tok/s PHI 3.5 IT Vision Q8_0 4.28GB 15 tok/s PHI 3.5 IT Mini Q8_0 3.94GB 16 tok/s Instructions You can download the prepared quantized model and tokenizer model files in the lmrs format from huggingface. If you'd prefer to convert the models published by Google/Meta on huggingface yourself, please refer to the following section. Otherwise, you can skip ahead to the build section. Model Conversion Install additional python dependencies (assuming you already have pytorch installed) used in export.py and tokenizer.py: pip install -r requirements.txt Download the .safetensors and config.json files from the original model's page on huggingface (So we don't have to clone the pytorch repo). For multimodal models (PHI3.5 Vision), we also need the CLIP .config file. Use the export.py script to convert the model bfloat16 weights into the LMRS format: python export.py --files [ordered .safetensor files] --config [model config.json] --save-path [name and path to save] --type [model type (GEMMA/LLAMA/PHI)] To export the quantized version use the --quantize and --quantize-type flags. The int8 quantized model size should be 4X smaller (from ~9.8G to ~2.5G, depending on the group size). For multimodal models include the --vision-config argument. Use the tokenizer.py script to convert the tokenizer model into the LMRS tokenizer format: python tokenizer.py --model-id [huggingface model_id] --tokenizer-type [type of the tokenizer (GEMMA/LLAMA/PHI)] Build Compile the rust code with cargo (make sure to pass the target-cpu flag): RUSTFLAGS=\"-C target-cpu=native\" cargo build --release --bin chat To enable multimodality, include the multimodal feature by passing the --features multimodal argument. And you are good to go: ./target/release/chat --model [model weights file] Other arguments include tokenizer, temperature, top-p, show-metrics etc. To check available arguments run with --help. For multimodal models use the --image argument with the image path. To run the backend for the WebUI, first compile: RUSTFLAGS=\"-C target-cpu=native\" cargo build --release --features backend --bin backend For multimodality enable the backend-multimodal feature. Then run: ./target/release/backend --model [model weights file] You can change the ip and port with --ip and --port. Other flags such as temperature, etc. are also available. For multimodal compatibility use the --multimodal flag. You can now connect via the web interface. TODOs Some things to do in the future: Add other sampling methods. Test the 9B and 27B models (tested the 9B, 27B would be too slow). Parallelize the multi head attention loop. Add performance metrics. Ability to give a system prompt Quantization support (int8, int4). License MIT",
  "image": "https://opengraph.githubassets.com/ee1f7fbd68762add320ffced7ce32c36afb532ac673793a46ae6c9f35704237a/samuel-vitorino/lm.rs",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-hpc=\"true\"\u003e\u003carticle itemprop=\"text\"\u003e\u003cdiv dir=\"auto\"\u003e\n\u003cthemed-picture data-catalyst-inline=\"true\"\u003e\u003cpicture\u003e\n    \u003cimg alt=\"lmrs logo\" src=\"https://github.com/samuel-vitorino/lm.rs/raw/main/repo_cover.svg\"/\u003e\n\u003c/picture\u003e\u003c/themed-picture\u003e\n\u003cp dir=\"auto\"\u003elm.rs: run inference on Language Models locally on the CPU with Rust\u003c/p\u003e\n\n\u003c/div\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eðŸŒƒ Now supporting multimodality with PHI-3.5-vision model! PHI-3.5-mini text-only model also now supported.\u003c/strong\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eInspired by Karpathy\u0026#39;s \u003ca href=\"https://github.com/karpathy/llama2.c\"\u003ellama2.c\u003c/a\u003e and \u003ca href=\"https://github.com/karpathy/llm.c\"\u003ellm.c\u003c/a\u003e I decided to create the most minimal code (not so minimal atm) that can perform full inference on Language Models on the CPU without ML libraries. Previously only Google\u0026#39;s Gemma 2 models were supported, but I decided to add support for the new Llama 3.2 models, and more recently the option to use images with PHI-3.5. Image processing/encoding currently takes a bit, so it slows the first response, working on optimization now.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eDisclaimer: some of the code could be optimized and improved. This is just an excuse for me to write Rust for the first time. Isn\u0026#39;t it incredible that in a few years, we could have AGI running in a few lines of poorly written Rust code?\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003ePrepared models\u003c/h2\u003e\u003ca id=\"user-content-prepared-models\" aria-label=\"Permalink: Prepared models\" href=\"#prepared-models\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eSome benchmarks and download links for the models and tokenizers. I recommend using Q8_0, Q4_0 quantization still being improved. Speed measured on a 16-core AMD Epyc.\u003c/p\u003e\n\u003cmarkdown-accessiblity-table\u003e\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eModel\u003c/th\u003e\n\u003cth\u003eSize\u003c/th\u003e\n\u003cth\u003eSpeed\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/samuel-vitorino/gemma2-2b-it-q4_0-LMRS\" rel=\"nofollow\"\u003eGemma 2 2B IT Q4_0\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e1.39G\u003c/td\u003e\n\u003ctd\u003e20 tok/s\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/samuel-vitorino/gemma2-2b-it-q8_0-LMRS\" rel=\"nofollow\"\u003eGemma 2 2B IT Q8_0\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e2.66GB\u003c/td\u003e\n\u003ctd\u003e18 tok/s\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/samuel-vitorino/gemma2-9b-it-q4_0-LMRS\" rel=\"nofollow\"\u003eGemma 2 9B IT Q4_0\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e4.91GB\u003c/td\u003e\n\u003ctd\u003e7 tok/s\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/samuel-vitorino/gemma2-9b-it-q8_0-LMRS\" rel=\"nofollow\"\u003eGemma 2 9B IT Q8_0\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e9.53GB\u003c/td\u003e\n\u003ctd\u003e8 tok/s\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-LMRS\" rel=\"nofollow\"\u003eLlama 3.2 1B IT\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e4.94GB\u003c/td\u003e\n\u003ctd\u003e20 tok/s\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-Q8_0-LMRS\" rel=\"nofollow\"\u003eLlama 3.2 1B IT Q8_0\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e1.27GB\u003c/td\u003e\n\u003ctd\u003e35 tok/s\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/samuel-vitorino/Llama-3.2-3B-Instruct-Q4_0-LMRS\" rel=\"nofollow\"\u003eLlama 3.2 3B IT Q4_0\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e1.71GB\u003c/td\u003e\n\u003ctd\u003e17 tok/s\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/samuel-vitorino/Llama-3.2-3B-Instruct-Q8_0-LMRS\" rel=\"nofollow\"\u003eLlama 3.2 3B IT Q8_0\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e3.31GB\u003c/td\u003e\n\u003ctd\u003e16 tok/s\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/samuel-vitorino/Phi-3.5-vision-instruct-q8_0-LMRS\" rel=\"nofollow\"\u003ePHI 3.5 IT Vision Q8_0\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e4.28GB\u003c/td\u003e\n\u003ctd\u003e15 tok/s\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://huggingface.co/samuel-vitorino/Phi-3.5-mini-instruct-q8_0-LMRS\" rel=\"nofollow\"\u003ePHI 3.5 IT Mini Q8_0\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e3.94GB\u003c/td\u003e\n\u003ctd\u003e16 tok/s\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\u003c/markdown-accessiblity-table\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eInstructions\u003c/h2\u003e\u003ca id=\"user-content-instructions\" aria-label=\"Permalink: Instructions\" href=\"#instructions\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eYou can download the prepared quantized model and tokenizer model files in the lmrs format from huggingface. If you\u0026#39;d prefer to convert the models published by Google/Meta on huggingface yourself, please refer to the following section. Otherwise, you can skip ahead to the build section.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eModel Conversion\u003c/h3\u003e\u003ca id=\"user-content-model-conversion\" aria-label=\"Permalink: Model Conversion\" href=\"#model-conversion\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eInstall additional python dependencies (assuming you already have pytorch installed) used in export.py and tokenizer.py:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"pip install -r requirements.txt\"\u003e\u003cpre\u003epip install -r requirements.txt\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eDownload the \u003cstrong\u003e.safetensors\u003c/strong\u003e and \u003cstrong\u003econfig.json\u003c/strong\u003e files from the original model\u0026#39;s page on huggingface (So we don\u0026#39;t have to clone the pytorch repo). For multimodal models (PHI3.5 Vision), we also need the CLIP \u003cstrong\u003e.config\u003c/strong\u003e \u003ca href=\"https://huggingface.co/openai/clip-vit-large-patch14-336/blob/main/config.json\" rel=\"nofollow\"\u003efile\u003c/a\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eUse the export.py script to convert the model bfloat16 weights into the LMRS format:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"python export.py --files [ordered .safetensor files] --config [model config.json] --save-path [name and path to save] --type [model type (GEMMA/LLAMA/PHI)]\"\u003e\u003cpre\u003epython export.py --files [ordered .safetensor files] --config [model config.json] --save-path [name and path to save] --type [model type (GEMMA/LLAMA/PHI)]\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eTo export the quantized version use the \u003cstrong\u003e--quantize\u003c/strong\u003e and \u003cstrong\u003e--quantize-type\u003c/strong\u003e flags. The int8 quantized model size should be 4X smaller (from ~9.8G to ~2.5G, depending on the group size). For multimodal models include the \u003cstrong\u003e--vision-config\u003c/strong\u003e argument.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eUse the tokenizer.py script to convert the tokenizer model into the LMRS tokenizer format:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"python tokenizer.py --model-id [huggingface model_id] --tokenizer-type [type of the tokenizer (GEMMA/LLAMA/PHI)]\"\u003e\u003cpre\u003epython tokenizer.py --model-id [huggingface model_id] --tokenizer-type [type of the tokenizer (GEMMA/LLAMA/PHI)]\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" dir=\"auto\"\u003eBuild\u003c/h3\u003e\u003ca id=\"user-content-build\" aria-label=\"Permalink: Build\" href=\"#build\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eCompile the rust code with cargo (make sure to pass the target-cpu flag):\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"RUSTFLAGS=\u0026#34;-C target-cpu=native\u0026#34; cargo build --release --bin chat\"\u003e\u003cpre\u003e\u003cspan\u003eRUSTFLAGS\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e-C target-cpu=native\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e cargo build --release --bin chat\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eTo enable multimodality, include the multimodal feature by passing the \u003cstrong\u003e--features multimodal\u003c/strong\u003e argument.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eAnd you are good to go:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"./target/release/chat --model [model weights file]\"\u003e\u003cpre\u003e./target/release/chat --model [model weights file]\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eOther arguments include tokenizer, temperature, top-p, show-metrics etc. To check available arguments run with --help. For multimodal models use the \u003cstrong\u003e--image\u003c/strong\u003e argument with the image path.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp dir=\"auto\"\u003eTo run the backend for the \u003ca href=\"https://github.com/samuel-vitorino/lm.rs-webui\"\u003eWebUI\u003c/a\u003e, first compile:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"RUSTFLAGS=\u0026#34;-C target-cpu=native\u0026#34; cargo build --release --features backend --bin backend\"\u003e\u003cpre\u003e\u003cspan\u003eRUSTFLAGS\u003c/span\u003e=\u003cspan\u003e\u003cspan\u003e\u0026#34;\u003c/span\u003e-C target-cpu=native\u003cspan\u003e\u0026#34;\u003c/span\u003e\u003c/span\u003e cargo build --release --features backend --bin backend\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eFor multimodality enable the \u003cstrong\u003ebackend-multimodal\u003c/strong\u003e feature.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThen run:\u003c/p\u003e\n\u003cdiv dir=\"auto\" data-snippet-clipboard-copy-content=\"./target/release/backend --model [model weights file]\"\u003e\u003cpre\u003e./target/release/backend --model [model weights file]\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eYou can change the ip and port with --ip and --port. Other flags such as temperature, etc. are also available. For multimodal compatibility use the \u003cstrong\u003e--multimodal\u003c/strong\u003e flag. You can now connect via the web interface.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eTODOs\u003c/h2\u003e\u003ca id=\"user-content-todos\" aria-label=\"Permalink: TODOs\" href=\"#todos\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eSome things to do in the future:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e Add other sampling methods.\u003c/li\u003e\n\u003cli\u003e Test the 9B and 27B models (tested the 9B, 27B would be too slow).\u003c/li\u003e\n\u003cli\u003e Parallelize the multi head attention loop.\u003c/li\u003e\n\u003cli\u003e Add performance metrics.\u003c/li\u003e\n\u003cli\u003e Ability to give a system prompt\u003c/li\u003e\n\u003cli\u003e Quantization support (int8, int4).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" dir=\"auto\"\u003eLicense\u003c/h2\u003e\u003ca id=\"user-content-license\" aria-label=\"Permalink: License\" href=\"#license\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eMIT\u003c/p\u003e\n\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": null,
  "modifiedTime": null
}
