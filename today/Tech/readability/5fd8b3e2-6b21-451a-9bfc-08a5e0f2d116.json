{
  "id": "5fd8b3e2-6b21-451a-9bfc-08a5e0f2d116",
  "title": "$2 H100s: How the GPU Rental Bubble Burst",
  "link": "https://www.latent.space/p/gpu-bubble",
  "description": "Comments",
  "author": "",
  "published": "Fri, 11 Oct 2024 02:19:42 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Eugene Cheah",
  "length": 24905,
  "excerpt": "H100s used to be $8/hr if you could get them. Now there's 7 different places sometimes selling them under $2. What happened?",
  "siteName": "Latent Space",
  "favicon": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9e8ba5d-49c6-4abd-9200-8736306cff29%2Fapple-touch-icon-1024x1024.png",
  "text": "Swyx’s note: we’re on a roll catching up with former guests! Apart from our recent guest spot on Raza Habib’s chat with Hamel Husain (see Raza’s first pod here). We’re delighted to welcome Eugene Cheah (see his first pod on RWKV last year) as a rare guest writer for our newsletter. Eugene has now cofounded Featherless.AI, an inference platform with the world’s largest collection of open source models (~2,000) instantly accessible via a single API for a flat rate ($10-$75+ a month).Recently there has been a lot of excitement with NVIDIA’s new Blackwell series rolling out to OpenAI, with the company saying it is sold out for the next year and Jensen noting that it could be the “most successful product in the history of the industry”. With cousin Lisa hot on his heels announcing the MI3 25 X and Cerebras filing for IPO, it is time to dive deep on the GPU market again (see also former guest Dylan Patel’s pod for his trademark candid take on the industry of course): Do we yet have an answer to the $600bn question? It is now consensus that the capex on foundation model training is the “fastest depreciating asset in history”, but the jury on GPU infra spend is still out and the GPU Rich Wars are raging.What follows is Eugene’s take on GPU economics as he is now an inference provider, diving deep on the H100 market, as a possible read for what is to come for the Blackwell generation. Not financial advice! We also recommend Yangqing Jia’s guide.TLDR: Don’t buy H100s. The market has flipped from shortage ($8/hr) to oversupplied ($2/hr), because of reserved compute resales, open model finetuning, and decline in new foundation model co’s. Rent instead.(Unless you have some combination of discounted H100s, discounted electricity, or a Sovereign AI angle where the location of your GPU is critical to your customers, or you have billions and need a super large cluster for frontier model training)For the general market, it makes little sense to be investing in new H100s today, when you can rent it at near cost, when you need it, with the current oversupply.ChatGPT was launched in November 2022, built on the A100 series. The H100s arrived in March 2023. The pitch to investors and founders was simple: Compared to A100s, the new H100s were 3x more powerful, but only 2x the sticker price.If you were faster to ramp up on H100s, you too, can build a bigger, better model, and maybe even leapfrog OpenAI to Artificial General Intelligence - If you have the capital to match their wallet! With this desire, $10-100’s billions of dollars were invested into GPU-rich AI startups to build this next revolution. Which lead to ….The sudden surge in H100 demandMarket prices shot through the roof, the original rental rates of H100 started at approximately $4.70 an hour but were going for over $8. For all the desperate founders rushing to train their models to convince their investors for their next $100 million round.Nvidia, literally pitched to their investors \u0026 datacenter customers, in their 2023 investor presentation - the “market opportunity” on renting H100s at $4/hrFor GPU farms, it felt like free money - if you can get these founders to rent your H100 SXMGPUs at $4.70 an hour or more, or even get them to pay it upfront, the payback period was \u003c1.5 years. From then on, it was free-flowing cash of over $100k per GPU, per year.With no end to the GPU demand in sight, their investors agreed, with even larger investments…Physical goods, unlike digital goods, suffer from lag time. Especially when there are multiple shipment delays.For most of 2023, the H100 prices felt like they would forever be above $4.70 (unless you were willing to do a huge upfront downpayment)At the start of 2024, the H100 prices reached approximately $2.85 across multiple providers.As more providers come online, however… I started to get emails like this:While I have not been successful with acquiring H100 nodes (8xH100) at $4/hour, I have confirmed multiple times, that you can do so at $8 - $16/hourIn Aug 2024, if you're willing to auction for a small slice of H100 time (days to weeks), you can start finding H100 GPUs for $1 to $2 an hour.We are looking at a \u003e= 40% price drop per year, especially for small clusters. NVIDIA’s marketing projection of $4 per GPU hour across 4 years, has evaporated away in under 1.5 years.And that is horrifying because it means someone out there is potentially left holding the bag - especially so if they just bought it as a new GPUs. So what is going on?This will be focusing on the economical cost, and the ROI on leasing, against various market rates. Not the opportunity cost, or buisness value.The average H100 SXM GPU in a data center costs $50k or more to set up, maintain, and operate (aka most of the CAPEX). Excluding electricity and cooling OPEX cost. More details on the calculation are provided later in this article.But what does that mean for unit economics today, as an investment?Especially if we assume a 5-year lifespan on the GPUs itself today.Generally, there are two business models for leasing H100, which we would cover.Short on-demand leases (by the hour - by the week - or the month)Longterm reservation (3-5 years)In summary, for an on-demand workload\u003e$2.85 : Beat stock market IRR\u003c$2.85 : Loses to stock market IRR\u003c$1.65 : Expect loss in investmentFor the above ROI and revenue forecast projection, we introduced “blended price”, where we assume a gradual drop to 50% in the rental price across 5 years.This is arguably a conservative/optimistic estimate given the \u003e= 40% price drop per year we see now. But it’s a means of projecting an ROI while taking into account a certain % of price drop.At $4.50/hour, even when blended, we get to see the original pitch for data center providers from NVIDIA, where they practically print money after 2 years. Giving an IRR (Internal rate of return) of 20+%.However, at $2.85/hour, this is where it starts to be barely above 10% IRR.Meaning, if you are buying a new H100 server today, and if the market price is less than $2.85/hour, you can barely beat the market, assuming 100% allocation (which is an unreasonable assumption). Anything, below that price, and you're better off with the stock market, instead of a H100 infrastructure company, as an investment.And if the price falls below $1.65/hour, you are doomed to make losses on the H100 over the 5 years, as an infra provider. Especially, if you just bought the nodes and cluster this year.Many infrastructure providers, especially the older ones - were not naive about this - Because they had been burnt firsthand by GPU massive rental price drops, after a major price pump, from the crypto days - they had seen this cycle before.So for this cycle, last year, they pushed heavily for a 3-5 year upfront commitment and/or payment at the $4+ price range. (typically with 50% to 100% upfront). Today, they push the $2.85+ price range - locking in their profits.This happened aggressively during the 2023 AI peak with various foundation model companies, especially in the image generation space, indirectly forced into high-priced 3-5 year contracts, just so to get to the front-of-the-line of a new cluster, and be first to make their target model, to help close the next round.It may not be the most economical move, but it lets them move faster than the competition.This, however, has led to some interesting market dynamics - if you are paying $3 or $4 per hour for your H100, for the next 3 years, locked into a contract.When a model creator is done training a model, you have no more use for the cluster. What would they do? - they resell and start recouping some of the costs.From hardware to AI inference / finetune, it can be broadly viewed as the followingHardware vendors partnered with Nvidia (one-time purchase cost)Datacenter Infrastructure providers \u0026 partners (selling long-term reservations, on facility space and/or H100 nodes)VC Funds, Large Companies, and Startups: that planned to build foundation models (or have already finished building their models)Resellers of capacity: Runpod, SFCompute, Together.ai, Vast.ai, GPUlist.aiManaged AI Inference / Finetune providers: who use a combination of the aboveWhile any layer down the stack may be vertically integrated (skipping the infra players for example), the key drivers here are the “Resellers of unused capacity” and the rise of “good enough” open weights models like Llama 3, as they are all major influencing factors in the current H100 economical pressures.The rise of open weights models, on-par with closed-source models.Is resulting in a fundamental shift in the market↑↑ Increased demand for AI inference \u0026 fine-tuningBecause many “open” models, lack proper “open source” licenses, but are being distributed freely, and used widely, even commercially. We will refer to them collectively as “open-weights” or “open” models instead here.In general, with multiple open-weights models of various sizes being built, so has the growth in demand for inference and fine-tuning them. This is largely driven by two major eventsThe arrival of GPT4 class open models (eg. 405B LLaMA3, DeepSeek-v2)The maturity and adoption of small (~8B) and medium (~70B) fine-tuned modelsToday, for the vast majority of use cases, enterprises may need, there are already off-the-shelf open-weights models. Which might be a small step behind proprietary models in certain benchmarks.Provides an advantage with the followingFlexibility: Domain / Task specific finetunesReliability: No more minor model updates, breaking use case (there is currently low community trust that model weights are not quietly changed without notification in public API endpoints, causing inexplicable regressions)Security \u0026 Privacy: Assurance that their prompts and customer data are safe.All of this leads to the current continuous growth and adoption of open models, with the growth in demand for inference and finetunes.But it does cause another problem…↓↓ Shrinking foundation model creator market (Small \u0026 Medium)We used “model creators” to collectively refer to organization that create models from scratch. For fine-tuners, we refer to them as “model finetuners”Many enterprises, and multiple small \u0026 medium foundation model creator startups - especially those who raised on the pitch of “smaller, specialized domain-specific models”, are groups who had no long-term plans / goals for training large foundation models from scratch ( \u003e= 70B ).For both groups, they both came to the realization that it is more economical and effective to fine-tune existing Open Weights models, instead of “training on their own”.This ended up creating a triple whammy in reducing the demand for H100s!Finetuning is significantly cheaper than training from scratch.Because the demands for fine-tuning are significantly less in compute requirements (typically 4 nodes or less, usually a single node), compared to training from scratch (from 16 nodes, usually more, for 7B and up models).This industry-wide switch essentially killed a large part of smaller cluster demands.Scaling back on foundation model investment (at small/mid-tier)In 2023, there was a huge wave of small and medium foundation models, within the text and image space.Today, however, unless you are absolutely confident you can surpass llama3, or you are bringing something new to the table (eg. new architecture, 100x lower inference, 100+ languages, etc), there are ~no more foundation model cos being founded from scratch.In general, the small \u0026 medium, open models created by the bigger players (Facebook, etc), make it hard for smaller players to justify training foundation models - unless they have a strong differentiator to do so (tech or data) - or have plans to scale to larger models.And this has been reflected lately with investors as well, as there has been a sharp decline in new foundation model creators’ funding. With the vast majority of smaller groups having switched over to finetuning. (this sentiment is combined with the recent less than desired exits for multiple companies).Presently today, there is approximately worldwide by my estimate:\u003c20 Large model creator teams (aka 70B++, may create small models as well)\u003c30 Small / Medium model creator teams (7B - 70B)Collectively there are less than \u003c50 teams worldwide who would be in the market for 16 nodes of H100s (or much more), at any point in time, to do foundation model training.There are more than 50 clusters of H100 worldwide with more than 16 nodes.Excess capacity from reserved nodes is coming onlineFor the cluster owners, especially the various foundation model startups and VCs, who made long reservations, in the initial “land grab” of the year 2023.With the switch to finetuning, and the very long wait times of the H100’s(it peaked at \u003e= 6 months), it is very well possible that many of these groups had already made the upfront payment before they made the change, essentially making their prepaid hardware “obsolete on arrival”.Alternatively, those who had the hardware arrive on time, to train their first few models, had come to the same realization it would be better to fine-tune their next iteration of models. Instead of building on their own.In both cases, they would have unused capacity, which comes online via “Compute Resellers” joining the market supply….Another major factor, is how all the major Model Creators, such as Facebook, X.AI, and arguably OpenAI (if you count them as part of Microsoft) are moving away from an existing public provider, and building their own billion-dollar clusters, removing the demand that the existing clusters depend on.The move is happening mostly for the following reasons:Existing ~1k node clusters (which costs \u003e$50M to build), is no longer big enough for them, to train bigger modelsAt a billion-dollar scale, it is better for accounting to purchase assets (of servers, land, etc), which has booked value (part of company valuation and assets), instead of pure expenses leasing.If you do not have the people (they do), you could straight up buy small datacenters companies, who have the expertise to build this for you.With the demand gradually weaning away in stages. These clusters are coming online to the public cloud market instead.Vast.ai essentially does a free market system, where providers from all over the world, are forced to compete with each otherRecall all the H100 large shipment delays in 2023, or 6 months or more? They are coming online, now - along with the H200, B200, etc.This is alongside, the various unused compute, coming online (from existing startups, enterprises or VCs as covered earlier).The bulk of this is done via Compute Resellers, such as : together.ai, sfcompute, runpod, vast.ai, etcIn most cases, cluster owners have a small or medium cluster, (typically 8-64 nodes), that is underutilized. With the money already “spent” for the cluster.With the primary goal is to recoup as much of the cost as possible, they rather undercut the market and guarantee an allocation, instead of competing with the main providers, and possibly have no allocation.This is typically done either via a fixed rate, an auction system, or just a free market listing, etc. With the later 2 driving the market price downwards.Another major factor, is once your outside of the training / fine-tune space. The inference space is filled with alternatives, especially if your running smaller models.One do not need to pay for the premium invoked by H100’s Infiniband and/or nvidia.H100 premium for training is priced into the hardware. For example nvidia themselves recommend the L40S, which is the more price competitive alternative for inference.Which Is 1/3rd the performance, at 1/5th the price. But does not work well with multi-node training. Undercutting their very own H100 for this segment.Both AMD and Intel may be late into the game with their MX300, and Gaudi 3 respectively.This has been tested and verified by us, having used these systems. They are generally:Cheaper than a H100 in purchase costHave more memory and compute than a H100, and outperforms on a single node.Overall, they are great hardware!The catch? They have minor driver issues in training and are entirely unproven in large multi-node cluster training.Which as we covered is largely irrelevant to the current landscape. To anyone but \u003c50 teams. The market for H100 has been moving towards inference and single or small cluster fine-tuning.All of which these GPUs have been proven to work at. For the use cases, the vast majority of the market is asking for.These 2 competitors are full drop-in replacements. With working off-the-shelf inference code (eg. VLLM) or finetuning code for most common model architectures (primarily LLaMA3, followed by others).So, if you have compatibility sorted out. Its highly recommended to have a look.With Ethereum moving towards proof of stake, ASIC dominating the bitcoin mining race, and the general crypto market condition.GPU usage in mining for crypto has been a downward trend, and in several cases unprofitable. And has since been flooding the GPU public cloud market.And while the vast majority of these GPUs are unusable for training, or even for inference, due to hardware constraints (low PCIe bandwidth, network, etc). The hardware has been flooding the market and has been repurposed for AI inference workloads.In most cases if you are under \u003c10B, you can get decent performance with these GPUs, out of the box, for really low prices.If you optimize it further (though various tricks), you can even get large 405B models to run on a small cluster of this hardware, cheaper then an H100 node (which is what is typically used)H100 Prices are becoming commodity-prices cheap.Or even being rented at a loss - if so, what now?On a high level, it is expected that big clusters still get to charge a premium (\u003e=$2.90 / hour) because there is no other option. For those who truly need it.We are starting to see this trend for example with Voltage Park:Where clusters with Infiniband are charged at a premium.While the Ethernet-based instances, which are perfectly fine for inference are priced at a lower rate. Adjusting the prices for the respective use case/availability.While there’s been a general decline in foundation model creator teams, it is hard to predict if there will be a resurgence, with the growth in open weights, and/or alternative architectures.It is also, expected that in the future, we will see further segmentation by cluster sizes. Where a large 512-node cluster with Infiniband may be billed higher per GPU than a 16-node cluster.There is a lot against you, if you price it below $2.25, depending on your OPEX, you risk potentially being unprofitable.If you price it too high \u003e= $3, you might not be able to get sufficient buyers to fill capacity.If you're late, you could not recoup the cost in the early $4/hour days.Overall, these cluster investments will be rough for the key stakeholders and investors.While I doubt it’s the case, if new clusters, make a large segment of the AI portfolio investments. We may see additional rippling effects in the funding ecosystem from burnt investors.Instead of a negative outlook, a neutral outlook would be some of the unused compute foundation model creators, coming online, are already paid for.The funding market has already priced in and paid for this cluster and its model training. And “extracted its value” which they used for their current and next funding round.Most of these purchases were made before the popularity of Compute Resellers, the cost was already priced in.If anything, the current revenue they get from their excess H100 compute, and the lowered prices we get, are beneficial to both partiesIf so the negative market impact is minimal, while overall it’s a net positive win for the ecosystem.Given that the open-weights model has entered the GPT-4 class arena. Falling H100 prices will be the multiplier unlock for open-weights AI adoption.It will be more affordable, for hobbyists, AI developers, and engineers, to run, fine-tune, and tinker with these open models.Especially if there is no major leap for GPT5++, because it will mean that the gap between open-weights and closed-source models will blur.This is strongly needed, as the market is currently not sustainable. As there lacks the value capture on the application layer for paying users (which trickles down the platform, models, and infra layers)In a way, if everyone is building shovels (including us), and applications with paying users are not being built (and collecting revenue and value).But when AI inference and fine-tuning becomes cheaper than ever.It can potentially kick off the AI application wave. If it has not already slowly started so.Spending on new H100’s hardware is likely a loss-makerUnless you have some combination of discounted H100s, discounted electricity, or a Sovereign AI angle where the location of your GPU is critical to your customers. Or you have billions and need a super large cluster.If you're investing, consider investing elsewhere.Or the stock market index itself for a better rate of returns. IMOAt Featherless.AI - We currently host the world’s largest collection of OpenSource AI models, instantly accessible, serverlessly, with unlimited requests from $10 a month, at a fixed price.We have indexed and made over 2,000 models ready for inference today. This is 10x the catalog of openrouter.ai, the largest model provider aggregator, and is the world’s largest collection of Open Weights models available serverlessly for instant inference. Without the need for any expensive dedicated GPUsAnd our platform makes this possible, as it’s able to dynamically hot-swap between models in seconds.It’s designed to be easy to use, with full OpenAI API compatibility, so you can just plug our platform in as a replacement to your existing AI API for your AI agents. Running in the backgroundAnd we do all of this; As we believe that AI should be easily accessible to everyone, regardless of language or social status.On the technical side of things, related to this article.It is a challenge having PetaBytes’s worth of AI models, and growing, running 24/7 - while being hardware profitable (we are), because we needed to optimize every layer of our platform, down to how we choose the GPU hardware.In an industry, where the typical inference provider pitch is typically along the lines of winning with their, special data center advantages, and CUDA optimization that they perform on their own hardware. Hardware is CAPEX intensive. (Which is being pitched and funded even today)We were saying the opposite, which defied most investors’ sensibilities - we were saying we would be avoiding buying new hardware like the plague.We came to a realization, that most investors, their analysts, and founders failed to realize, thanks to the billions in hardware investments to date. GPUs are commodity hardware. Faster than all of us expected.Few investors have even realized we have reached commodity-level prices at $2.85 in certain places, let alone loss-making prices of a dollar. Because most providers (ignoring certain exceptions), only show their full prices after quotation or after login.And that was the trigger, which got me to write this article.While we do optimize our inference CUDA and kernels as well. On the hardware side; We’ve bet on hardware commoditizing and have focussed instead on the orchestration layer above.So for us, this is a mix of sources from, AWS spot (preferred), to various data center grade providers (eg. Tensordock, Runpod) with security and networking compliances that meet our standards.Leveraging them with our own proprietary model hot swapping, which boots new models up in under a second. Keeping our fleet of GPUs right-sized to our workload, while using a custom version of our RWKV foundation model as a low-cost speculative decoder. All of which allows us to take full advantage of this market trend, and future GPU price drops, as newer (and older) GPUs come online to replace the H100s. And scale aggressively.PS: If you are looking at building the world's largest inference platform, and are aligned with our goals - to make AI accessible to everyone, regardless of language or status. Reach out to us at: hello@featherless.aiHead over to Eugene’s Blog for more footnotes on xAI’s H100 cluster we cut from this piece. Additional Sources:GPU data: Tech Power Up Database. The A100 SXM had 624 bf16 TFlops, the H100 SXM was 1,979 bf16 TFlopsMicrosoft \u0026 AWS allocated over $40 billion in AI infra alone: Wall Street Journal“600 Billion Dollars “ is about: Sequoia’s AI articleNvidia investor slides for Oct 2014: page 14 has the pitch for “data centers”Semi Analysis: deepdive for H100 clusters, w/ 5 year lifespan approx for componentsSpreadsheet for : new H100 ROI (Aug 2024)Spreadsheet for: H100 Infiniband Cluster math (Aug 2024)",
  "image": "https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv dir=\"auto\"\u003e\u003cp\u003e\u003cem\u003e\u003cstrong\u003eSwyx’s note:\u003c/strong\u003e\u003cspan\u003e we’re on a roll catching up with former guests! Apart from our recent guest spot on \u003c/span\u003e\u003ca href=\"https://www.listennotes.com/podcasts/high-agency-the/why-your-ai-product-needs-ALy02ewNtDC/\" rel=\"\"\u003eRaza Habib’s chat with Hamel Husain\u003c/a\u003e\u003cspan\u003e (see \u003c/span\u003e\u003ca href=\"https://www.latent.space/p/humanloop\" rel=\"\"\u003eRaza’s first pod here\u003c/a\u003e\u003cspan\u003e). \u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cspan\u003eWe’re delighted to welcome Eugene Cheah (see \u003c/span\u003e\u003ca href=\"https://www.latent.space/p/rwkv\" rel=\"\"\u003ehis first pod on RWKV last year\u003c/a\u003e\u003cspan\u003e) as a rare guest \u003c/span\u003e\u003cstrong\u003ewriter \u003c/strong\u003e\u003cspan\u003efor our newsletter\u003c/span\u003e\u003cstrong\u003e.\u003c/strong\u003e\u003cspan\u003e Eugene has now cofounded \u003c/span\u003e\u003ca href=\"https://featherless.ai/\" rel=\"\"\u003eFeatherless.AI\u003c/a\u003e\u003cspan\u003e, an inference platform with the world’s largest collection of open source models (~2,000) instantly accessible via a single API for a \u003c/span\u003e\u003cstrong\u003eflat rate\u003c/strong\u003e\u003cspan\u003e ($10-$75+ a month).\u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cspan\u003eRecently there has been a lot of excitement with NVIDIA’s new Blackwell series rolling out to OpenAI, with the company saying it is \u003c/span\u003e\u003ca href=\"https://x.com/firstadopter/status/1844417947277852925\" rel=\"\"\u003esold out for the next year\u003c/a\u003e\u003cspan\u003e and Jensen noting that it could be the “\u003c/span\u003e\u003ca href=\"https://x.com/The_AI_Investor/status/1844080690046058843\" rel=\"\"\u003emost successful product in the history of the industry\u003c/a\u003e\u003cspan\u003e”. With cousin Lisa hot on his heels \u003c/span\u003e\u003ca href=\"https://www.cnbc.com/2024/10/10/amd-launches-mi325x-ai-chip-to-rival-nvidias-blackwell-.html\" rel=\"\"\u003eannouncing the MI3 25 X\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://news.ycombinator.com/item?id=41702789\" rel=\"\"\u003eCerebras filing for IPO\u003c/a\u003e\u003cspan\u003e, it is time to dive deep on the GPU market again (see also \u003c/span\u003e\u003ca href=\"https://www.latent.space/p/semianalysis\" rel=\"\"\u003eformer guest\u003c/a\u003e\u003cspan\u003e \u003c/span\u003e\u003ca href=\"https://www.dwarkeshpatel.com/p/dylan-jon\" rel=\"\"\u003eDylan Patel’s pod\u003c/a\u003e\u003cspan\u003e for his trademark candid take on the industry of course): \u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png\" width=\"421\" height=\"497.04655493482306\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:1268,\u0026#34;width\u0026#34;:1074,\u0026#34;resizeWidth\u0026#34;:421,\u0026#34;bytes\u0026#34;:1493562,\u0026#34;alt\u0026#34;:null,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:false,\u0026#34;topImage\u0026#34;:true,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003e\u003cem\u003e\u003cspan\u003eDo we yet have an answer to \u003c/span\u003e\u003ca href=\"https://www.latent.space/p/mar-jun-2024\" rel=\"\"\u003ethe $600bn question\u003c/a\u003e\u003cspan\u003e? It is now consensus that the capex on foundation model training is the “\u003c/span\u003e\u003ca href=\"https://x.com/GavinSBaker/status/1720819375517716610\" rel=\"\"\u003efastest depreciating asset in history\u003c/a\u003e\u003cspan\u003e”, but the jury on GPU infra spend is still out and \u003c/span\u003e\u003ca href=\"https://www.latent.space/i/140396949/mixtral-sparks-a-gpuinference-race-to-the-bottom\" rel=\"\"\u003ethe GPU Rich Wars are raging\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cspan\u003eWhat follows is Eugene’s take on GPU economics as he is now an inference provider, diving deep on the H100 market, as a possible read for what is to come for the Blackwell generation. Not financial advice! We also recommend \u003c/span\u003e\u003ca href=\"https://blog.lepton.ai/the-missing-guide-to-the-h100-gpu-market-91ebfed34516\" rel=\"\"\u003eYangqing Jia’s guide\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cstrong\u003eTLDR: Don’t buy H100s. The market has flipped from shortage ($8/hr) to oversupplied ($2/hr), because of reserved compute resales, open model finetuning, and decline in new foundation model co’s. Rent instead.\u003c/strong\u003e\u003cp\u003e\u003cspan\u003e(Unless you have some combination of discounted H100s, discounted electricity, or a Sovereign AI angle where the location of your GPU is critical to your customers, or you have billions and need a super large cluster for frontier model training)\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eFor the general market, it makes little sense to be investing in new H100s today, when \u003c/span\u003e\u003cstrong\u003eyou can rent it at near cost, when you need it\u003c/strong\u003e\u003cspan\u003e, with the current oversupply.\u003c/span\u003e\u003c/p\u003e\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eChatGPT was launched in November 2022, built on the A100 series. The H100s arrived in March 2023. \u003c/span\u003e\u003cstrong\u003eThe pitch to investors and founders was simple: \u003c/strong\u003e\u003cspan\u003eCompared to A100s, \u003c/span\u003e\u003cstrong\u003ethe new H100s were 3x more powerful, but only 2x the sticker price\u003c/strong\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eIf you were faster to ramp up on H100s, you too, can build a bigger, better model, and maybe even leapfrog OpenAI to Artificial General Intelligence - If you have the capital to match their wallet! \u003c/p\u003e\u003cp\u003eWith this desire, $10-100’s billions of dollars were invested into GPU-rich AI startups to build this next revolution. Which lead to ….\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eThe sudden surge in H100 demand\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarket prices shot through the roof, the original rental rates of H100 started at approximately \u003c/span\u003e\u003cem\u003e\u003cstrong\u003e$4.70 an hour\u003c/strong\u003e\u003c/em\u003e\u003cspan\u003e but were going for \u003c/span\u003e\u003cem\u003e\u003cstrong\u003eover $8\u003c/strong\u003e\u003c/em\u003e\u003cspan\u003e. For all the desperate founders rushing to train their models to convince their investors for their next $100 million round.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png\" width=\"404\" height=\"227.52747252747253\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:false,\u0026#34;imageSize\u0026#34;:\u0026#34;large\u0026#34;,\u0026#34;height\u0026#34;:820,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:404,\u0026#34;bytes\u0026#34;:265694,\u0026#34;alt\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eNvidia, literally pitched to their investors \u0026amp; datacenter customers, in their 2023 investor presentation - the “market opportunity” on renting H100s at $4/hr\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003eFor GPU farms, it felt like free money - if you can get these founders to rent your H100 SXMGPUs at $4.70 an hour or more, or even get them to pay it upfront, \u003c/span\u003e\u003cstrong\u003ethe payback period was \u0026lt;1.5 years\u003c/strong\u003e\u003cspan\u003e. From then on, it was free-flowing cash of over $100k per GPU, per year.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eWith no end to the GPU demand in sight, their investors agreed, with even larger investments…\u003c/p\u003e\u003cp\u003e\u003cspan\u003ePhysical goods, unlike digital goods, suffer from lag time. Especially when there are \u003c/span\u003e\u003ca href=\"https://www.ft.com/content/c7e9cfa9-3f68-47d3-92fc-7cf85bcb73b3\" rel=\"\"\u003emultiple shipment delays\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eFor most of 2023, the H100 prices felt like they would forever be above $4.70 (unless you were willing to do a huge upfront downpayment)\u003c/p\u003e\u003cp\u003eAt the start of 2024, the H100 prices reached approximately $2.85 across multiple providers.\u003c/p\u003e\u003cp\u003eAs more providers come online, however… I started to get emails like this:\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png\" width=\"1456\" height=\"825\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:825,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:413942,\u0026#34;alt\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eWhile I have not been successful with acquiring H100 nodes (8xH100) at $4/hour, I have confirmed multiple times, that you can do so at $8 - $16/hour\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eIn Aug 2024, if you\u0026#39;re willing to auction for a small slice of H100 time (days to weeks), you can start finding H100 GPUs for $1 to $2 an hour.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe are looking at a \u0026gt;= 40% price drop per year\u003c/strong\u003e\u003cspan\u003e, especially for small clusters. NVIDIA’s marketing projection of $4 per GPU hour across 4 years, has evaporated away in under 1.5 years.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eAnd that is horrifying because it means someone out there is potentially \u003c/span\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Bagholder\" rel=\"\"\u003eleft holding the bag\u003c/a\u003e\u003cspan\u003e - especially so if they just bought it as a new GPUs. So what is going on?\u003c/span\u003e\u003c/p\u003e\u003cblockquote\u003e\u003cp\u003e\u003cem\u003eThis will be focusing on the economical cost, and the ROI on leasing, against various market rates. Not the opportunity cost, or buisness value.\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003eThe average H100 SXM GPU in a data center costs $50k or more to set up, maintain, and operate (aka most of the CAPEX). Excluding electricity and cooling OPEX cost. More details on the calculation are provided later in this article.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eBut what does that mean for unit economics today, as an investment?\u003c/span\u003e\u003cbr/\u003e\u003cspan\u003eEspecially if we assume a 5-year lifespan on the GPUs itself today.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eGenerally, there are two business models for leasing H100, which we would cover.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eShort on-demand leases (by the hour - by the week - or the month)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eLongterm reservation (3-5 years)\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png\" width=\"1456\" height=\"765\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:765,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:618581,\u0026#34;alt\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eIn summary, for an on-demand workload\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u0026gt;$2.85\u003c/strong\u003e\u003cspan\u003e : Beat stock market IRR\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u0026lt;$2.85\u003c/strong\u003e\u003cspan\u003e : Loses to stock market IRR\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u0026lt;$1.65\u003c/strong\u003e\u003cspan\u003e : Expect loss in investment\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor the above ROI and revenue forecast projection, we introduced “blended price”, where we assume a gradual drop to 50% in the rental price across 5 years.\u003c/p\u003e\u003cp\u003eThis is arguably a conservative/optimistic estimate given the \u0026gt;= 40% price drop per year we see now. But it’s a means of projecting an ROI while taking into account a certain % of price drop.\u003c/p\u003e\u003cp\u003eAt $4.50/hour, even when blended, we get to see the original pitch for data center providers from NVIDIA, where they practically print money after 2 years. Giving an IRR (Internal rate of return) of 20+%.\u003c/p\u003e\u003cp\u003eHowever, at $2.85/hour, this is where it starts to be barely above 10% IRR.\u003c/p\u003e\u003cp\u003eMeaning, if you are buying a new H100 server today, and if the market price is less than $2.85/hour, you can barely beat the market, assuming 100% allocation (which is an unreasonable assumption). Anything, below that price, and you\u0026#39;re better off with the stock market, instead of a H100 infrastructure company, as an investment.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAnd if the price falls below $1.65/hour, you are doomed to make losses on the H100 over the 5 years, as an infra provider\u003c/strong\u003e\u003cspan\u003e. Especially, if you just bought the nodes and cluster this year.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png\" width=\"1456\" height=\"760\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/b3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:760,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:642212,\u0026#34;alt\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eMany infrastructure providers, especially the older ones - were not naive about this - Because they had been burnt firsthand by GPU massive rental price drops, after a major price pump, from the crypto days - they had seen this cycle before.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSo for this cycle, last year, they pushed heavily for a 3-5 year upfront commitment and/or payment at the $4+ price range. \u003c/strong\u003e\u003cspan\u003e(typically with 50% to 100% upfront)\u003c/span\u003e\u003cstrong\u003e. \u003c/strong\u003e\u003cspan\u003eToday, they push the $2.85+ price range - locking in their profits.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eThis happened aggressively during the 2023 AI peak with various foundation model companies, especially in the image generation space, indirectly forced into high-priced 3-5 year contracts, just so to get to the front-of-the-line of a new cluster, and be first to make their target model, to help close the next round.\u003c/p\u003e\u003cp\u003eIt may not be the most economical move, but it lets them move faster than the competition.\u003c/p\u003e\u003cp\u003eThis, however, has led to some interesting market dynamics - if you are paying $3 or $4 per hour for your H100, for the next 3 years, locked into a contract.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eWhen a model creator is done training a model, you have no more use for the cluster. \u003c/span\u003e\u003cstrong\u003eWhat would they do? - they resell and start recouping some of the costs.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFrom hardware to AI inference / finetune, it can be broadly viewed as the following\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eHardware vendors partnered with Nvidia (one-time purchase cost)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDatacenter Infrastructure providers \u0026amp; partners (selling long-term reservations, on facility space and/or H100 nodes)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eVC Funds, Large Companies, and Startups: that plann\u003c/span\u003e\u003cem\u003eed\u003c/em\u003e\u003cspan\u003e to build foundation models (or have already finished building their models)\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cstrong\u003eResellers of capacity: Runpod, SFCompute, Together.ai, Vast.ai, GPUlist.ai\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eManaged AI Inference / Finetune providers: who use a combination of the above\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cspan\u003eWhile any layer down the stack may be vertically integrated (skipping the infra players for example), the key drivers here are the \u003c/span\u003e\u003cstrong\u003e“Resellers of unused capacity” \u003c/strong\u003e\u003cspan\u003eand the rise of “good enough” open weights models like \u003c/span\u003e\u003ca href=\"https://www.latent.space/p/llama-3\" rel=\"\"\u003eLlama 3\u003c/a\u003e\u003cspan\u003e, as they are all major influencing factors in the current H100 economical pressures.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png\" width=\"1456\" height=\"871\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:871,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:932212,\u0026#34;alt\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003e\u003cem\u003e\u003cstrong\u003e\u003cspan\u003eThe rise of open weights models, on-par with closed-source models.\u003c/span\u003e\u003cbr/\u003e\u003cspan\u003eIs resulting in a fundamental shift in the market\u003c/span\u003e\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\u003cblockquote\u003e\u003cp\u003e\u003cem\u003e\u003cstrong\u003e↑↑ Increased demand for AI inference \u0026amp; fine-tuning\u003c/strong\u003e\u003cp\u003e\u003cspan\u003eBecause many “open” models, lack proper “open source” licenses, but are being distributed freely, and used widely, even commercially. We will refer to them collectively as “open-weights” or “open” models instead here.\u003c/span\u003e\u003c/p\u003e\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003eIn general, with multiple open-weights models of various sizes being built, so has the growth in demand for inference and fine-tuning them. This is largely driven by two major events\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eThe arrival of GPT4 class open models (eg. 405B LLaMA3, DeepSeek-v2)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe maturity and adoption of small (~8B) and medium (~70B) fine-tuned models\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eToday, for the vast majority of use cases, enterprises may need, there are already off-the-shelf open-weights models. Which might be a small step behind proprietary models in certain benchmarks.\u003c/p\u003e\u003cp\u003eProvides an advantage with the following\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cstrong\u003eFlexibility\u003c/strong\u003e\u003cspan\u003e: Domain / Task specific finetunes\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cstrong\u003eReliability\u003c/strong\u003e\u003cspan\u003e: No more minor model updates, breaking use case (there is currently low community trust that model weights are not quietly changed without notification in public API endpoints, causing inexplicable regressions)\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cstrong\u003eSecurity \u0026amp; Privacy\u003c/strong\u003e\u003cspan\u003e: Assurance that their prompts and customer data are safe.\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAll of this leads to the current continuous growth and adoption of open models, with the growth in demand for inference and finetunes.\u003c/p\u003e\u003cp\u003eBut it does cause another problem…\u003c/p\u003e\u003cblockquote\u003e\u003cp\u003e\u003cem\u003e\u003cstrong\u003e↓↓ Shrinking foundation model creator market (Small \u0026amp; Medium)\u003c/strong\u003e\u003cp\u003e\u003cspan\u003eWe used “model creators” to collectively refer to organization that create models from scratch. For fine-tuners, we refer to them as “model finetuners”\u003c/span\u003e\u003c/p\u003e\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003eMany enterprises, and multiple small \u0026amp; medium foundation model creator startups - especially those who raised on the pitch of “smaller, specialized domain-specific models”, are groups who had no long-term plans / goals for training large foundation models from scratch ( \u0026gt;= 70B ).\u003c/p\u003e\u003cp\u003eFor both groups, they both came to the realization that it is more economical and effective to fine-tune existing Open Weights models, instead of “training on their own”.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eThis ended up creating a triple whammy in reducing the demand for H100s!\u003c/strong\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cstrong\u003eFinetuning is significantly cheaper than training from scratch.\u003c/strong\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eBecause the demands for fine-tuning are significantly less in compute requirements (typically 4 nodes or less, usually a single node), compared to training from scratch (from 16 nodes, usually more, for 7B and up models).\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThis industry-wide switch essentially killed a large part of smaller cluster demands.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cstrong\u003eScaling back on foundation model investment (at small/mid-tier)\u003c/strong\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eIn 2023, there was a huge wave of small and medium foundation models, within the text and image space.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eToday, however, unless you are absolutely confident you can surpass llama3, or you are bringing something new to the table (eg. new architecture, 100x lower inference, 100+ languages, etc), there are ~no more foundation model cos being founded from scratch.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn general, the small \u0026amp; medium, open models created by the bigger players (Facebook, etc), make it hard for smaller players to justify training foundation models - unless they have a strong differentiator to do so (tech or data) - or have plans to scale to larger models.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAnd this has been reflected lately with investors as well, as there has been a sharp decline in new foundation model creators’ funding. With the vast majority of smaller groups having switched over to finetuning. (this sentiment is combined with the recent less than desired exits for multiple companies).\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003ePresently today, there is approximately worldwide by my estimate:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u0026lt;20 Large model creator teams (aka 70B++, may create small models as well)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u0026lt;30 Small / Medium model creator teams (7B - 70B)\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCollectively there are less than \u0026lt;50 teams worldwide who would be in the market for 16 nodes of H100s (or much more), at any point in time, to do foundation model training.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThere are more than 50 clusters of H100 worldwide with more than 16 nodes.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cstrong\u003eExcess capacity from reserved nodes is coming online\u003c/strong\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eFor the cluster owners, especially the various foundation model startups and VCs, who made long reservations, in the initial “land grab” of the year 2023.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eWith the switch to finetuning, and the very long wait times of the H100’s\u003c/span\u003e\u003cbr/\u003e\u003cspan\u003e(it peaked at \u0026gt;= 6 months), it is very well possible that many of these groups had already made the upfront payment before they made the change, essentially making their prepaid hardware “obsolete on arrival”.\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAlternatively, those who had the hardware arrive on time, to train their first few models, had come to the same realization it would be better to fine-tune their next iteration of models. Instead of building on their own.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eIn both cases, they would have unused capacity, which comes online via \u003c/span\u003e\u003cstrong\u003e“Compute Resellers”\u003c/strong\u003e\u003cspan\u003e joining the market supply….\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAnother major factor, is how all the major Model Creators, such as Facebook, X.AI, and arguably OpenAI (if you count them as part of Microsoft) are moving away from an existing public provider, and building their own billion-dollar clusters, removing the demand that the existing clusters depend on.\u003c/p\u003e\u003cp\u003eThe move is happening mostly for the following reasons:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eExisting ~1k node clusters (which costs \u0026gt;$50M to build), is no longer big enough for them, to train bigger models\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAt a billion-dollar scale, it is better for accounting to purchase assets (of servers, land, etc), which has booked value (part of company valuation and assets), instead of pure expenses leasing.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIf you do not have the people (they do), you could straight up buy small datacenters companies, who have the expertise to build this for you.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWith the demand gradually weaning away in stages. These clusters are coming online to the public cloud market instead.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png\" width=\"1456\" height=\"974\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:974,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:1268863,\u0026#34;alt\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003cfigcaption\u003eVast.ai essentially does a free market system, where providers from all over the world, are forced to compete with each other\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eRecall all the H100 large shipment delays in 2023, or 6 months or more? They are coming online, now - along with the H200, B200, etc.\u003c/p\u003e\u003cp\u003eThis is alongside, the various unused compute, coming online (from existing startups, enterprises or VCs as covered earlier).\u003c/p\u003e\u003cp\u003e\u003cspan\u003eThe bulk of this is done via \u003c/span\u003e\u003cstrong\u003eCompute Resellers\u003c/strong\u003e\u003cspan\u003e, such as : together.ai, sfcompute, runpod, vast.ai, etc\u003c/span\u003e\u003c/p\u003e\u003cp\u003eIn most cases, cluster owners have a small or medium cluster, (typically 8-64 nodes), that is underutilized. With the money already “spent” for the cluster.\u003c/p\u003e\u003cp\u003eWith the primary goal is to recoup as much of the cost as possible, they rather undercut the market and guarantee an allocation, instead of competing with the main providers, and possibly have no allocation.\u003c/p\u003e\u003cp\u003eThis is typically done either via a fixed rate, an auction system, or just a free market listing, etc. With the later 2 driving the market price downwards.\u003c/p\u003e\u003cp\u003eAnother major factor, is once your outside of the training / fine-tune space. The inference space is filled with alternatives, especially if your running smaller models.\u003c/p\u003e\u003cp\u003eOne do not need to pay for the premium invoked by H100’s Infiniband and/or nvidia.\u003c/p\u003e\u003cp\u003eH100 premium for training is priced into the hardware. For example nvidia themselves recommend the L40S, which is the more price competitive alternative for inference.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png\" width=\"1456\" height=\"439\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/df705375-9942-4ea5-86b9-b41d3661096b_1842x556.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:439,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:414838,\u0026#34;alt\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eWhich Is 1/3rd the performance, at 1/5th the price. But does not work well with multi-node training. Undercutting their very own H100 for this segment.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png\" width=\"1456\" height=\"461\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:461,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:2982780,\u0026#34;alt\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eBoth AMD and Intel may be late into the game with their MX300, and Gaudi 3 respectively.\u003c/p\u003e\u003cp\u003eThis has been tested and verified by us, having used these systems. They are generally:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eCheaper than a H100 in purchase cost\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eHave more memory and compute than a H100, and outperforms on a single node.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOverall, they are great hardware!\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe catch? They have minor driver issues in training and are entirely unproven in large multi-node cluster training.\u003c/p\u003e\u003cp\u003eWhich as we covered is largely irrelevant to the current landscape. To anyone but \u0026lt;50 teams. The market for H100 has been moving towards inference and single or small cluster fine-tuning.\u003c/p\u003e\u003cp\u003eAll of which these GPUs have been proven to work at. For the use cases, the vast majority of the market is asking for.\u003c/p\u003e\u003cp\u003eThese 2 competitors are full drop-in replacements. With working off-the-shelf inference code (eg. VLLM) or finetuning code for most common model architectures (primarily LLaMA3, followed by others).\u003c/p\u003e\u003cp\u003eSo, if you have compatibility sorted out. Its highly recommended to have a look.\u003c/p\u003e\u003cp\u003eWith Ethereum moving towards proof of stake, ASIC dominating the bitcoin mining race, and the general crypto market condition.\u003c/p\u003e\u003cp\u003eGPU usage in mining for crypto has been a downward trend, and in several cases unprofitable. And has since been flooding the GPU public cloud market.\u003c/p\u003e\u003cp\u003eAnd while the vast majority of these GPUs are unusable for training, or even for inference, due to hardware constraints (low PCIe bandwidth, network, etc). The hardware has been flooding the market and has been repurposed for AI inference workloads.\u003c/p\u003e\u003cp\u003eIn most cases if you are under \u0026lt;10B, you can get decent performance with these GPUs, out of the box, for really low prices.\u003c/p\u003e\u003cp\u003eIf you optimize it further (though various tricks), you can even get large 405B models to run on a small cluster of this hardware, cheaper then an H100 node (which is what is typically used)\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cspan\u003eH100 Prices are becoming commodity-prices cheap.\u003c/span\u003e\u003cbr/\u003e\u003cspan\u003eOr even being rented at a loss - if so, what now?\u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\u003cp\u003eOn a high level, it is expected that big clusters still get to charge a premium (\u0026gt;=$2.90 / hour) because there is no other option. For those who truly need it.\u003c/p\u003e\u003cp\u003eWe are starting to see this trend for example with Voltage Park:\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png\" width=\"1456\" height=\"741\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/c1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:741,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:647709,\u0026#34;alt\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;title\u0026#34;:null,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eWhere clusters with Infiniband are charged at a premium.\u003c/p\u003e\u003cp\u003eWhile the Ethernet-based instances, which are perfectly fine for inference are priced at a lower rate. Adjusting the prices for the respective use case/availability.\u003c/p\u003e\u003cp\u003eWhile there’s been a general decline in foundation model creator teams, it is hard to predict if there will be a resurgence, with the growth in open weights, and/or alternative architectures.\u003c/p\u003e\u003cp\u003eIt is also, expected that in the future, we will see further segmentation by cluster sizes. Where a large 512-node cluster with Infiniband may be billed higher per GPU than a 16-node cluster.\u003c/p\u003e\u003cp\u003eThere is a lot against you, if you price it below $2.25, depending on your OPEX, you risk potentially being unprofitable.\u003c/p\u003e\u003cp\u003eIf you price it too high \u0026gt;= $3, you might not be able to get sufficient buyers to fill capacity.\u003c/p\u003e\u003cp\u003eIf you\u0026#39;re late, you could not recoup the cost in the early $4/hour days.\u003c/p\u003e\u003cp\u003eOverall, these cluster investments will be rough for the key stakeholders and investors.\u003c/p\u003e\u003cp\u003eWhile I doubt it’s the case, if new clusters, make a large segment of the AI portfolio investments. We may see additional rippling effects in the funding ecosystem from burnt investors.\u003c/p\u003e\u003cp\u003eInstead of a negative outlook, a neutral outlook would be some of the unused compute foundation model creators, coming online, are already paid for.\u003c/p\u003e\u003cp\u003eThe funding market has already priced in and paid for this cluster and its model training. And “extracted its value” which they used for their current and next funding round.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMost of these purchases were made before the popularity of \u003c/span\u003e\u003cstrong\u003eCompute Resellers\u003c/strong\u003e\u003cspan\u003e, the cost was already priced in.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eIf anything, the current revenue they get from their excess H100 compute, and the lowered prices we get, are beneficial to both parties\u003c/p\u003e\u003cp\u003eIf so the negative market impact is minimal, while overall it’s a net positive win for the ecosystem.\u003c/p\u003e\u003cp\u003eGiven that the open-weights model has entered the GPT-4 class arena. Falling H100 prices will be the multiplier unlock for open-weights AI adoption.\u003c/p\u003e\u003cp\u003eIt will be more affordable, for hobbyists, AI developers, and engineers, to run, fine-tune, and tinker with these open models.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eEspecially if there is no major leap for GPT5++,\u003c/span\u003e\u003cstrong\u003e \u003c/strong\u003e\u003cspan\u003ebecause it will mean that the gap between open-weights and closed-source models will blur.\u003c/span\u003e\u003c/p\u003e\u003cp\u003eThis is strongly needed, as the market is currently not sustainable. As there lacks the value capture on the application layer for paying users (which trickles down the platform, models, and infra layers)\u003c/p\u003e\u003cp\u003eIn a way, if everyone is building shovels (including us), and applications with paying users are not being built (and collecting revenue and value).\u003c/p\u003e\u003cp\u003eBut when AI inference and fine-tuning becomes cheaper than ever.\u003c/p\u003e\u003cp\u003eIt can potentially kick off the AI application wave. If it has not already slowly started so.\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cstrong\u003eSpending on new H100’s hardware is likely a loss-maker\u003c/strong\u003e\u003cp\u003e\u003cspan\u003eUnless you have some combination of discounted H100s, discounted electricity, or a Sovereign AI angle where the location of your GPU is critical to your customers. Or you have billions and need a super large cluster.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eIf you\u0026#39;re investing, consider investing elsewhere.\u003c/span\u003e\u003cbr/\u003e\u003cspan\u003eOr the stock market index itself for a better rate of returns. IMO\u003c/span\u003e\u003c/p\u003e\u003c/em\u003e\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003ca target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png\" data-component-name=\"Image2ToDOM\" rel=\"\"\u003e\u003cdiv\u003e\u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1456w\" sizes=\"100vw\"/\u003e\u003cimg src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png\" width=\"1456\" height=\"813\" data-attrs=\"{\u0026#34;src\u0026#34;:\u0026#34;https://substack-post-media.s3.amazonaws.com/public/images/3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png\u0026#34;,\u0026#34;srcNoWatermark\u0026#34;:null,\u0026#34;fullscreen\u0026#34;:null,\u0026#34;imageSize\u0026#34;:null,\u0026#34;height\u0026#34;:813,\u0026#34;width\u0026#34;:1456,\u0026#34;resizeWidth\u0026#34;:null,\u0026#34;bytes\u0026#34;:664297,\u0026#34;alt\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34;,\u0026#34;href\u0026#34;:null,\u0026#34;belowTheFold\u0026#34;:true,\u0026#34;topImage\u0026#34;:false,\u0026#34;internalRedirect\u0026#34;:null,\u0026#34;isProcessing\u0026#34;:false}\" alt=\"\" title=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1456w\" sizes=\"100vw\" loading=\"lazy\"/\u003e\u003c/picture\u003e\u003c/div\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eAt Featherless.AI - We currently host the world’s largest collection of OpenSource AI models, instantly accessible, serverlessly, with unlimited requests from $10 a month, at a fixed price.\u003c/p\u003e\u003cp\u003e\u003cspan\u003eWe have indexed and made over 2,000 models ready for inference today. This is 10x the catalog of openrouter.ai, the largest model provider \u003c/span\u003e\u003cem\u003eaggregator\u003c/em\u003e\u003cspan\u003e, \u003c/span\u003e\u003cem\u003eand \u003c/em\u003e\u003cspan\u003eis the world’s largest collection of Open Weights models available serverlessly for instant inference. Without the need for any expensive dedicated GPUs\u003c/span\u003e\u003c/p\u003e\u003cp\u003eAnd our platform makes this possible, as it’s able to dynamically hot-swap between models in seconds.\u003c/p\u003e\u003cp\u003eIt’s designed to be easy to use, with full OpenAI API compatibility, so you can just plug our platform in as a replacement to your existing AI API for your AI agents. Running in the background\u003c/p\u003e\u003cp\u003eAnd we do all of this; As we believe that AI should be easily accessible to everyone, regardless of language or social status.\u003c/p\u003e\u003cp\u003eOn the technical side of things, related to this article.\u003c/p\u003e\u003cp\u003eIt is a challenge having PetaBytes’s worth of AI models, and growing, running 24/7 - while being hardware profitable (we are), because we needed to optimize every layer of our platform, down to how we choose the GPU hardware.\u003c/p\u003e\u003cp\u003eIn an industry, where the typical inference provider pitch is typically along the lines of winning with their, special data center advantages, and CUDA optimization that they perform on their own hardware. Hardware is CAPEX intensive. (Which is being pitched and funded even today)\u003c/p\u003e\u003cp\u003eWe were saying the opposite, which defied most investors’ sensibilities - we were saying we would be avoiding buying new hardware like the plague.\u003c/p\u003e\u003cp\u003eWe came to a realization, that most investors, their analysts, and founders failed to realize, thanks to the billions in hardware investments to date. GPUs are commodity hardware. Faster than all of us expected.\u003c/p\u003e\u003cp\u003eFew investors have even realized we have reached commodity-level prices at $2.85 in certain places, let alone loss-making prices of a dollar. Because most providers (ignoring certain exceptions), only show their full prices after quotation or after login.\u003c/p\u003e\u003cp\u003eAnd that was the trigger, which got me to write this article.\u003c/p\u003e\u003cp\u003eWhile we do optimize our inference CUDA and kernels as well. On the hardware side; We’ve bet on hardware commoditizing and have focussed instead on the orchestration layer above.\u003c/p\u003e\u003cp\u003eSo for us, this is a mix of sources from, AWS spot (preferred), to various data center grade providers (eg. Tensordock, Runpod) with security and networking compliances that meet our standards.\u003c/p\u003e\u003cp\u003eLeveraging them with our own proprietary model hot swapping, which boots new models up in under a second. Keeping our fleet of GPUs right-sized to our workload, while using a custom version of our RWKV foundation model as a low-cost speculative decoder. All of which allows us to take full advantage of this market trend, and future GPU price drops, as newer (and older) GPUs come online to replace the H100s. And scale aggressively.\u003c/p\u003e\u003cp\u003e\u003cem\u003ePS: If you are looking at building the world\u0026#39;s largest inference platform, and are aligned with our goals - to make AI accessible to everyone, regardless of language or status. Reach out to us at: hello@featherless.ai\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cspan\u003eHead over to Eugene’s Blog \u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003e\u003cspan\u003e for \u003c/span\u003e\u003ca href=\"https://substack.tech-talk-cto.com/p/d4ffab7a-3f0d-4e6e-ade0-e74409770196?postPreview=paid\u0026amp;updated=2024-08-25T03%3A36%3A59.886Z\u0026amp;audience=everyone\u0026amp;free_preview=false\u0026amp;freemail=true\" rel=\"\"\u003emore footnotes on xAI’s H100 cluster\u003c/a\u003e\u003cspan\u003e we cut from this piece.\u003c/span\u003e\u003c/em\u003e\u003cspan\u003e \u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eAdditional Sources:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eGPU data: \u003c/span\u003e\u003ca href=\"https://www.techpowerup.com/gpu-specs/h100-sxm5-80-gb.c3900\" rel=\"\"\u003eTech Power Up Database\u003c/a\u003e\u003cspan\u003e. The A100 SXM had 624 bf16 TFlops, the H100 SXM was 1,979 bf16 TFlops\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eMicrosoft \u0026amp; AWS allocated over $40 billion in AI infra alone: \u003c/span\u003e\u003ca href=\"https://www.wsj.com/tech/ai/big-tech-moves-more-ai-spending-abroad-088988de\" rel=\"\"\u003eWall Street Journal\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003e“600 Billion Dollars “ is about: \u003c/span\u003e\u003ca href=\"https://www.sequoiacap.com/article/ais-600b-question/\" rel=\"\"\u003eSequoia’s AI article\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eNvidia investor slides for Oct 2014: \u003c/span\u003e\u003ca href=\"https://s201.q4cdn.com/141608511/files/doc_presentations/2023/Oct/01/ndr_presentation_oct_2023_final.pdf\" rel=\"\"\u003epage 14 has the pitch for “data centers”\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eSemi Analysis: \u003c/span\u003e\u003ca href=\"https://www.semianalysis.com/p/100000-h100-clusters-power-network\" rel=\"\"\u003edeepdive for H100 clusters, w/ 5 year lifespan approx for components\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eSpreadsheet for : \u003c/span\u003e\u003ca href=\"https://docs.google.com/spreadsheets/d/1kZosZmvaecG6P4-yCPzMN7Ha3ubMcTmF9AeJNDKeo98/edit?usp=sharing\" rel=\"\"\u003enew H100 ROI (Aug 2024)\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cspan\u003eSpreadsheet for: \u003c/span\u003e\u003ca href=\"https://docs.google.com/spreadsheets/d/1Ft3RbeZ-w43kYSiLfYc1vxO41mK5lmJpcPC9GOYHAWc/edit?usp=sharing\" rel=\"\"\u003eH100 Infiniband Cluster math (Aug 2024)\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "26 min read",
  "publishedTime": "2024-10-11T02:15:20Z",
  "modifiedTime": null
}
