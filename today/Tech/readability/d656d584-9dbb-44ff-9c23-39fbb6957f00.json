{
  "id": "d656d584-9dbb-44ff-9c23-39fbb6957f00",
  "title": "We were wrong about GPUs",
  "link": "https://fly.io/blog/wrong-about-gpu/",
  "description": "Comments",
  "author": "",
  "published": "Fri, 14 Feb 2025 22:36:31 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 10486,
  "excerpt": "Do my tears surprise you? Strong CEOs also cry.",
  "siteName": "Fly",
  "favicon": "https://fly.io/static/images/favicon/apple-touch-icon.png",
  "text": "Author Name Kurt Mackey @mrkurt @mrkurt Image by Annie Ruygt We’re building a public cloud, on hardware we own. We raised money to do that, and to place some bets; one of them: GPU-enabling our customers. A progress report: GPUs aren’t going anywhere, but: GPUs aren’t going anywhere. A couple years back, we put a bunch of chips down on the bet that people shipping apps to users on the Internet would want GPUs, so they could do AI/ML inference tasks. To make that happen, we created Fly GPU Machines. A Fly Machine is a Docker/OCI container running inside a hardware-virtualized virtual machine somewhere on our global fleet of bare-metal worker servers. A GPU Machine is a Fly Machine with a hardware-mapped Nvidia GPU. It’s a Fly Machine that can do fast CUDA. Like everybody else in our industry, we were right about the importance of AI/ML. If anything, we underestimated its importance. But the product we came up with probably doesn’t fit the moment. It’s a bet that doesn’t feel like it’s paying off. If you’re using Fly GPU Machines, don’t freak out; we’re not getting rid of them. But if you’re waiting for us to do something bigger with them, a v2 of the product, you’ll probably be waiting awhile. What It Took GPU Machines were not a small project for us. Fly Machines run on an idiosyncratically small hypervisor (normally Firecracker, but for GPU Machines Intel’s Cloud Hypervisor, a very similar Rust codebase that supports PCI passthrough). The Nvidia ecosystem is not geared to supporting micro-VM hypervisors. GPUs terrified our security team. A GPU is just about the worst case hardware peripheral: intense multi-directional direct memory transfers (not even bidirectional: in common configurations, GPUs talk to each other) with arbitrary, end-user controlled computation, all operating outside our normal security boundary. We did a couple expensive things to mitigate the risk. We shipped GPUs on dedicated server hardware, so that GPU- and non-GPU workloads weren’t mixed. Because of that, the only reason for a Fly Machine to be scheduled on a GPU machine was that it needed a PCI BDF for an Nvidia GPU, and there’s a limited number of those available on any box. Those GPU servers were drastically less utilized and thus less cost-effective than our ordinary servers. We funded two very large security assessments, from Atredis and Tetrel, to evaluate our GPU deployment. Matt Braun is writing up those assessments now. They were not cheap, and they took time. Security wasn’t directly the biggest cost we had to deal with, but it was an indirect cause for a subtle reason. We could have shipped GPUs very quickly by doing what Nvidia recommended: standing up a standard K8s cluster to schedule GPU jobs on. Had we taken that path, and let our GPU users share a single Linux kernel, we’d have been on Nvidia’s driver happy-path. Alternatively, we could have used a conventional hypervisor. Nvidia suggested VMware (heh). But they could have gotten things working had we used QEMU. We like QEMU fine, and could have talked ourselves into a security story for it, but the whole point of Fly Machines is that they take milliseconds to start. We could not have offered our desired Developer Experience on the Nvidia happy-path. Instead, we burned months trying (and ultimately failing) to get Nvidia’s host drivers working to map virtualized GPUs into Intel Cloud Hypervisor. At one point, we hex-edited the closed-source drivers to trick them into thinking our hypervisor was QEMU. I’m not sure any of this really mattered in the end. There’s a segment of the market we weren’t ever really able to explore because Nvidia’s driver support kept us from thin-slicing GPUs. We’d have been able to put together a really cheap offering for developers if we hadn’t run up against that, and developers love “cheap”, but I can’t prove that those customers are real. On the other hand, we’re committed to delivering the Fly Machine DX for GPU workloads. Beyond the PCI/IOMMU drama, just getting an entire hardware GPU working in a Fly Machine was a lift. We needed Fly Machines that would come up with the right Nvidia drivers; our stack was built assuming that the customer’s OCI container almost entirely defined the root filesystem for a Machine. We had to engineer around that in our flyd orchestrator. And almost everything people want to do with GPUs involves efficiently grabbing huge files full of model weights. Also annoying! And, of course, we bought GPUs. A lot of GPUs. Expensive GPUs. Why It Isn’t Working The biggest problem: developers don’t want GPUs. They don’t even want AI/ML models. They want LLMs. System engineers may have smart, fussy opinions on how to get their models loaded with CUDA, and what the best GPU is. But software developers don’t care about any of that. When a software developer shipping an app comes looking for a way for their app to deliver prompts to an LLM, you can’t just give them a GPU. For those developers, who probably make up most of the market, it doesn’t seem plausible for an insurgent public cloud to compete with OpenAI and Anthropic. Their APIs are fast enough, and developers thinking about performance in terms of “tokens per second” aren’t counting milliseconds. (you should all feel sympathy for us) This makes us sad because we really like the point in the solution space we found. Developers shipping apps on Amazon will outsource to other public clouds to get cost-effective access to GPUs. But then they’ll faceplant trying to handle data and model weights, backhauling gigabytes (at significant expense) from S3. We have app servers, GPUs, and object storage all under the same top-of-rack switch. But inference latency just doesn’t seem to matter yet, so the market doesn’t care. Past that, and just considering the system engineers who do care about GPUs rather than LLMs: the hardware product/market fit here is really rough. People doing serious AI work want galactically huge amounts of GPU compute. A whole enterprise A100 is a compromise position for them; they want an SXM cluster of H100s. Near as we can tell, MIG gives you a UUID to talk to the host driver, not a PCI device. We think there’s probably a market for users doing lightweight ML work getting tiny GPUs. This is what Nvidia MIG does, slicing a big GPU into arbitrarily small virtual GPUs. But for fully-virtualized workloads, it’s not baked; we can’t use it. And I’m not sure how many of those customers there are, or whether we’d get the density of customers per server that we need. That leaves the L40S customers. There are a bunch of these! We dropped L40S prices last year, not because we were sour on GPUs but because they’re the one part we have in our inventory people seem to get a lot of use out of. We’re happy with them. But they’re just another kind of compute that some apps need; they’re not a driver of our core business. They’re not the GPU bet paying off. Really, all of this is just a long way of saying that for most software developers, “AI-enabling” their app is best done with API calls to things like Claude and GPT, Replicate and RunPod. What Did We Learn? A very useful way to look at a startup is that it’s a race to learn stuff. So, what’s our report card? First off, when we embarked down this path in 2022, we were (like many other companies) operating in a sort of phlogiston era of AI/ML. The industry attention to AI had not yet collapsed around a small number of foundational LLM models. We expected there to be a diversity of mainstream models, the world Elixir Bumblebee looks forward to, where people pull different AI workloads off the shelf the same way they do Ruby gems. But Cursor happened, and, as they say, how are you going to keep ‘em down on the farm once they’ve seen Karl Hungus? It seems much clearer where things are heading. GPUs were a test of a Fly.io company credo: as we think about core features, we design for 10,000 developers, not for 5-6. It took a minute, but the credo wins here: GPU workloads for the 10,001st developer are a niche thing. Another way to look at a startup is as a series of bets. We put a lot of chips down here. But the buy-in for this tournament gave us a lot of chips to play with. Never making a big bet of any sort isn’t a winning strategy. I’d rather we’d flopped the nut straight, but I think going in on this hand was the right call. A really important thing to keep in mind here, and something I think a lot of startup thinkers sleep on, is the extent to which this bet involved acquiring assets. Obviously, some of our costs here aren’t recoverable. But the hardware parts that aren’t generating revenue will ultimately get liquidated; like with our portfolio of IPv4 addresses, I’m even more comfortable making bets backed by tradable assets with durable value. In the end, I don’t think GPU Fly Machines were going to be a hit for us no matter what we did. Because of that, one thing I’m very happy about is that we didn’t compromise the rest of the product for them. Security concerns slowed us down to where we probably learned what we needed to learn a couple months later than we could have otherwise, but we’re scaling back our GPU ambitions without having sacrificed any of our isolation story, and, ironically, GPUs other people run are making that story a lot more important. The same thing goes for our Fly Machine developer experience. We started this company building a Javascript runtime for edge computing. We learned that our customers didn’t want a new Javascript runtime; they just wanted their native code to work. We shipped containers, and no convincing was needed. We were wrong about Javascript edge functions, and I think we were wrong about GPUs. That’s usually how we figure out the right answers: by being wrong about a lot of stuff. Previous post ↓ The Exit Interview: JP Phillips",
  "image": "https://fly.io/blog/wrong-about-gpu/assets/choices-choices-cover.webp",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\n         \u003cdl\u003e\n             \u003cdt\u003eAuthor\u003c/dt\u003e\n             \u003cdd\u003e\n                 \u003cimg alt=\"Kurt Mackey\" src=\"https://fly.io/static/images/kurt.webp\"/\u003e\n               \u003cdl\u003e\n                 \u003cdt\u003eName\u003c/dt\u003e\n                 \u003cdd\u003e\n                   Kurt Mackey\n                 \u003c/dd\u003e\n                  \u003cdt\u003e@mrkurt\u003c/dt\u003e\n                  \u003cdd\u003e\n                    \u003ca href=\"https://twitter.com/mrkurt\" target=\"_blank\"\u003e\n                      @mrkurt\n                    \u003c/a\u003e\n                  \u003c/dd\u003e\n               \u003c/dl\u003e\n             \u003c/dd\u003e\n         \u003c/dl\u003e\n\n        \u003csection\u003e\n            \u003cfigure\u003e\n                \u003cimg src=\"https://fly.io/blog/wrong-about-gpu/assets/choices-choices-cover.webp\" alt=\"We took the road less traveled by. It was less traveled for a reason.\"/\u003e\n                \u003cfigcaption\u003e\n                  \u003cspan\u003eImage by\u003c/span\u003e\n                  \n\u003csvg role=\"img\" style=\"pointer-events: none; width: 17px; height: 17px;\" viewBox=\"0 0 20 20\" fill=\"currentColor\" fill-rule=\"evenodd\"\u003e\n  \u003cg buffered-rendering=\"static\"\u003e\n    \u003cpath fill-rule=\"evenodd\" d=\"M1 8a2 2 0 012-2h.93a2 2 0 001.664-.89l.812-1.22A2 2 0 018.07 3h3.86a2 2 0 011.664.89l.812 1.22A2 2 0 0016.07 6H17a2 2 0 012 2v7a2 2 0 01-2 2H3a2 2 0 01-2-2V8zm13.5 3a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM10 14a3 3 0 100-6 3 3 0 000 6z\" clip-rule=\"evenodd\"\u003e\u003c/path\u003e\n  \u003c/g\u003e\n\u003c/svg\u003e\n\n                    \u003ca href=\"https://annieruygtillustration.com/\" target=\"_blank\"\u003e\n                      Annie Ruygt\n                    \u003c/a\u003e\n                \u003c/figcaption\u003e\n            \u003c/figure\u003e\n          \u003cp\u003eWe’re building a public cloud, on hardware we own. We raised money to do that, and to place some bets; one of them: GPU-enabling our customers. A progress report: GPUs aren’t going anywhere, but: GPUs aren’t going anywhere.\u003c/p\u003e\n\u003cp\u003eA couple years back, \u003ca href=\"https://fly.io/gpu\"\u003ewe put a bunch of chips down\u003c/a\u003e on the bet that people shipping apps to users on the Internet would want GPUs, so they could do AI/ML inference tasks. To make that happen, we created \u003ca href=\"https://fly.io/docs/gpus/getting-started-gpus/\"\u003eFly GPU Machines\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eA Fly Machine is a \u003ca href=\"https://fly.io/blog/docker-without-docker/\"\u003eDocker/OCI container\u003c/a\u003e running inside a hardware-virtualized virtual machine somewhere on our global fleet of bare-metal worker servers. A GPU Machine is a Fly Machine with a hardware-mapped Nvidia GPU. It’s a Fly Machine that can do fast CUDA.\u003c/p\u003e\n\n\u003cp\u003eLike everybody else in our industry, we were right about the importance of AI/ML. If anything, we underestimated its importance. But the product we came up with probably doesn’t fit the moment. It’s a bet that doesn’t feel like it’s paying off.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eIf you’re using Fly GPU Machines, don’t freak out; we’re not getting rid of them.\u003c/strong\u003e But if you’re waiting for us to do something bigger with them, a v2 of the product, you’ll probably be waiting awhile.\u003c/p\u003e\n\u003ch3 id=\"what-it-took\"\u003e\u003ca href=\"#what-it-took\" aria-label=\"Anchor\"\u003e\u003c/a\u003e\u003cspan\u003eWhat It Took\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003eGPU Machines were not a small project for us. Fly Machines run on an idiosyncratically small hypervisor (normally Firecracker, but for GPU Machines \u003ca href=\"https://github.com/cloud-hypervisor/cloud-hypervisor\"\u003eIntel’s Cloud Hypervisor\u003c/a\u003e, a very similar Rust codebase that supports PCI passthrough). The Nvidia ecosystem is not geared to supporting micro-VM hypervisors.\u003c/p\u003e\n\n\u003cp\u003eGPUs \u003ca href=\"https://googleprojectzero.blogspot.com/2020/09/attacking-qualcomm-adreno-gpu.html\"\u003eterrified our security team\u003c/a\u003e. A GPU is just about the worst case hardware peripheral: intense multi-directional direct memory transfers\u003c/p\u003e\n\u003cp\u003e(not even bidirectional: in common configurations, GPUs talk to each other)\u003c/p\u003e\n\u003cp\u003ewith arbitrary, end-user controlled computation, all operating outside our normal security boundary.\u003c/p\u003e\n\n\u003cp\u003eWe did a couple expensive things to mitigate the risk. We shipped GPUs on dedicated server hardware, so that GPU- and non-GPU workloads weren’t mixed. Because of that, the only reason for a Fly Machine to be scheduled on a GPU machine was that it needed a PCI BDF for an Nvidia GPU, and there’s a limited number of those available on any box. Those GPU servers were drastically less utilized and thus less cost-effective than our ordinary servers.\u003c/p\u003e\n\n\u003cp\u003eWe funded two very large security assessments, from \u003ca href=\"https://www.atredis.com/\"\u003eAtredis\u003c/a\u003e and \u003ca href=\"https://tetrelsec.com/\"\u003eTetrel\u003c/a\u003e, to evaluate our GPU deployment. Matt Braun is writing up those assessments now. They were not cheap, and they took time.\u003c/p\u003e\n\n\u003cp\u003eSecurity wasn’t directly the biggest cost we had to deal with, but it was an indirect cause for a subtle reason.\u003c/p\u003e\n\n\u003cp\u003eWe could have shipped GPUs very quickly by doing what Nvidia recommended: standing up a standard K8s cluster to schedule GPU jobs on. Had we taken that path, and let our GPU users share a single Linux kernel, we’d have been on Nvidia’s driver happy-path.\u003c/p\u003e\n\n\u003cp\u003eAlternatively, we could have used a conventional hypervisor. Nvidia suggested VMware (heh). But they could have gotten things working had we used QEMU. We like QEMU fine, and could have talked ourselves into a security story for it, but the whole point of Fly Machines is that they take milliseconds to start. We could not have offered our desired Developer Experience on the Nvidia happy-path.\u003c/p\u003e\n\n\u003cp\u003eInstead, we burned months trying (and ultimately failing) to get Nvidia’s host drivers working to map \u003ca href=\"https://www.nvidia.com/en-us/data-center/virtual-solutions/\"\u003evirtualized GPUs\u003c/a\u003e into Intel Cloud Hypervisor. At one point, we hex-edited the closed-source drivers to trick them into thinking our hypervisor was QEMU.\u003c/p\u003e\n\n\u003cp\u003eI’m not sure any of this really mattered in the end. There’s a segment of the market we weren’t ever really able to explore because Nvidia’s driver support kept us from thin-slicing GPUs. We’d have been able to put together a really cheap offering for developers if we hadn’t run up against that, and developers love “cheap”, but I can’t prove that those customers are real.\u003c/p\u003e\n\n\u003cp\u003eOn the other hand, we’re committed to delivering the Fly Machine DX for GPU workloads. Beyond the PCI/IOMMU drama, just getting an entire hardware GPU working in a Fly Machine was a lift. We needed Fly Machines that would come up with the right Nvidia drivers; our stack was built assuming that the customer’s OCI container almost entirely defined the root filesystem for a Machine. We had to engineer around that in our \u003ccode\u003eflyd\u003c/code\u003e orchestrator. And almost everything people want to do with GPUs involves efficiently grabbing huge files full of model weights. Also annoying!\u003c/p\u003e\n\n\u003cp\u003eAnd, of course, we bought GPUs. A lot of GPUs. Expensive GPUs.\u003c/p\u003e\n\u003ch3 id=\"why-it-isnt-working\"\u003e\u003ca href=\"#why-it-isnt-working\" aria-label=\"Anchor\"\u003e\u003c/a\u003e\u003cspan\u003eWhy It Isn’t Working\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003eThe biggest problem: developers don’t want GPUs. They don’t even want AI/ML models. They want LLMs. \u003cem\u003eSystem engineers\u003c/em\u003e may have smart, fussy opinions on how to get their models loaded with CUDA, and what the best GPU is. But \u003cem\u003esoftware developers\u003c/em\u003e don’t care about any of that. When a software developer shipping an app comes looking for a way for their app to deliver prompts to an LLM, you can’t just give them a GPU.\u003c/p\u003e\n\n\u003cp\u003eFor those developers, who probably make up most of the market, it doesn’t seem plausible for an insurgent public cloud to compete with OpenAI and Anthropic. Their APIs are fast enough, and developers thinking about performance in terms of “tokens per second” aren’t counting milliseconds.\u003c/p\u003e\n\u003cp\u003e(you should all feel sympathy for us)\u003c/p\u003e\n\u003cp\u003eThis makes us sad because we really like the point in the solution space we found. Developers shipping apps on Amazon will outsource to other public clouds to get cost-effective access to GPUs. But then they’ll faceplant trying to handle data and model weights, backhauling gigabytes (at significant expense) from S3. We have app servers, GPUs, and object storage all under the same top-of-rack switch. But inference latency just doesn’t seem to matter yet, so the market doesn’t care.\u003c/p\u003e\n\n\u003cp\u003ePast that, and just considering the system engineers who do care about GPUs rather than LLMs: the hardware product/market fit here is really rough.\u003c/p\u003e\n\n\u003cp\u003ePeople doing serious AI work want galactically huge amounts of GPU compute. A whole enterprise A100 is a compromise position for them; they want an SXM cluster of H100s.\u003c/p\u003e\n\u003cp\u003eNear as we can tell, MIG gives you a UUID to talk to the host driver, not a PCI device.\u003c/p\u003e\n\u003cp\u003eWe think there’s probably a market for users doing lightweight ML work getting tiny GPUs. \u003ca href=\"https://www.nvidia.com/en-us/technologies/multi-instance-gpu/\"\u003eThis is what Nvidia MIG does\u003c/a\u003e, slicing a big GPU into arbitrarily small virtual GPUs. But for fully-virtualized workloads, it’s not baked; we can’t use it. And I’m not sure how many of those customers there are, or whether we’d get the density of customers per server that we need.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://fly.io/blog/cutting-prices-for-l40s-gpus-in-half\"\u003eThat leaves the L40S customers\u003c/a\u003e. There are a bunch of these! We dropped L40S prices last year, not because we were sour on GPUs but because they’re the one part we have in our inventory people seem to get a lot of use out of. We’re happy with them. But they’re just another kind of compute that some apps need; they’re not a driver of our core business. They’re not the GPU bet paying off.\u003c/p\u003e\n\n\u003cp\u003eReally, all of this is just a long way of saying that for most software developers, “AI-enabling” their app is best done with API calls to things like Claude and GPT, Replicate and RunPod.\u003c/p\u003e\n\u003ch3 id=\"what-did-we-learn\"\u003e\u003ca href=\"#what-did-we-learn\" aria-label=\"Anchor\"\u003e\u003c/a\u003e\u003cspan\u003eWhat Did We Learn?\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003eA very useful way to look at a startup is that it’s a race to learn stuff. So, what’s our report card?\u003c/p\u003e\n\n\u003cp\u003eFirst off, when we embarked down this path in 2022, we were (like many other companies) operating in a sort of phlogiston era of AI/ML. The industry attention to AI had not yet collapsed around a small number of foundational LLM models. We expected there to be a diversity of \u003cem\u003emainstream\u003c/em\u003e models, the world \u003ca href=\"https://github.com/elixir-nx/bumblebee\" title=\"\"\u003eElixir Bumblebee\u003c/a\u003e looks forward to, where people pull different AI workloads off the shelf the same way they do Ruby gems.\u003c/p\u003e\n\n\u003cp\u003eBut \u003ca href=\"https://www.cursor.com/\" title=\"\"\u003eCursor happened\u003c/a\u003e, and, as they say, how are you going to keep ‘em down on the farm once they’ve seen Karl Hungus? It seems much clearer where things are heading.\u003c/p\u003e\n\n\u003cp\u003eGPUs were a test of a Fly.io company credo: as we think about core features, we design for 10,000 developers, not for 5-6. It took a minute, but the credo wins here: GPU workloads for the 10,001st developer are a niche thing.\u003c/p\u003e\n\n\u003cp\u003eAnother way to look at a startup is as a series of bets. We put a lot of chips down here. But the buy-in for this tournament gave us a lot of chips to play with. Never making a big bet of any sort isn’t a winning strategy. I’d rather we’d flopped the nut straight, but I think going in on this hand was the right call.\u003c/p\u003e\n\n\u003cp\u003eA really important thing to keep in mind here, and something I think a lot of startup thinkers sleep on, is the extent to which this bet involved acquiring assets. Obviously, some of our \u003ca href=\"https://fly.io/blog/the-exit-interview-jp/\" title=\"\"\u003ecosts here aren’t recoverable\u003c/a\u003e. But the hardware parts that aren’t generating revenue will ultimately get liquidated; like with \u003ca href=\"https://fly.io/blog/32-bit-real-estate/\" title=\"\"\u003eour portfolio of IPv4 addresses\u003c/a\u003e, I’m even more comfortable making bets backed by tradable assets with durable value.\u003c/p\u003e\n\n\u003cp\u003eIn the end, I don’t think GPU Fly Machines were going to be a hit for us no matter what we did. Because of that, one thing I’m very happy about is that we didn’t compromise the rest of the product for them. Security concerns slowed us down to where we probably learned what we needed to learn a couple months later than we could have otherwise, but we’re scaling back our GPU ambitions without having sacrificed \u003ca href=\"https://fly.io/blog/sandboxing-and-workload-isolation/\" title=\"\"\u003eany of our isolation story\u003c/a\u003e, and, ironically, GPUs \u003cem\u003eother people run\u003c/em\u003e are making that story a lot more important. The same thing goes for our Fly Machine developer experience.\u003c/p\u003e\n\n\u003cp\u003eWe started this company building a Javascript runtime for edge computing. We learned that our customers didn’t want a new Javascript runtime; they just wanted their native code to work. \u003ca href=\"https://news.ycombinator.com/item?id=22616857\" title=\"\"\u003eWe shipped containers\u003c/a\u003e, and no convincing was needed. We were wrong about Javascript edge functions, and I think we were wrong about GPUs. That’s usually how we figure out the right answers:  by being wrong about a lot of stuff.\u003c/p\u003e\n\n          \n        \u003c/section\u003e\n        \u003cdl\u003e\n            \u003cdt\u003e\n              Previous post  ↓\n            \u003c/dt\u003e\n            \u003cdd\u003e\n              \u003ca href=\"https://fly.io/blog/the-exit-interview-jp/\"\u003e\n                The Exit Interview: JP Phillips\n              \u003c/a\u003e\n            \u003c/dd\u003e\n        \u003c/dl\u003e\n      \u003c/article\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": null,
  "modifiedTime": null
}
