{
  "id": "eda39f02-6491-4140-b7db-c444e170f899",
  "title": "Nvidia’s new AI audio model can synthesize sounds that have never existed",
  "link": "https://arstechnica.com/ai/2024/11/nvidias-new-ai-audio-model-can-synthesize-sounds-that-have-never-existed/",
  "description": "What does a screaming saxophone sound like? The Fugatto model has an answer...",
  "author": "Kyle Orland",
  "published": "Mon, 25 Nov 2024 21:40:00 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "NVIDIA"
  ],
  "byline": "Kyle Orland",
  "length": 8220,
  "excerpt": "What does a screaming saxophone sound like? The Fugatto model has an answer…",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Skip to content You've never heard anything like it What does a screaming saxophone sound like? The Fugatto model has an answer... An audio wave can contain so much. An angry cello, for instance... Credit: Getty Images At this point, anyone who has been following AI research is long familiar with generative models that can synthesize speech or melodic music from nothing but text prompting. Nvidia's newly revealed \"Fugatto\" model looks to go a step further, using new synthetic training methods and inference-level combination techniques to \"transform any mix of music, voices, and sounds,\" including the synthesis of sounds that have never existed. While Fugatto isn't available for public testing yet, a sample-filled website showcases how Fugatto can be used to dial a number of distinct audio traits and descriptions up or down, resulting in everything from the sound of saxophones barking to people speaking underwater to ambulance sirens singing in a kind of choir. While the results on display can be a bit hit or miss, the vast array of capabilities on display here helps support Nvidia's description of Fugatto as \"a Swiss Army knife for sound.\" You’re only as good as your data In an explanatory research paper, over a dozen Nvidia researchers explain the difficulty in crafting a training dataset that can \"reveal meaningful relationships between audio and language.\" While standard language models can often infer how to handle various instructions from the text-based data itself, it can be hard to generalize descriptions and traits from audio without more explicit guidance. To that end, the researchers start by using an LLM to generate a Python script that can create a large number of template-based and free-form instructions describing different audio \"personas\" (e.g., \"standard, young-crowd, thirty-somethings, professional\"). They then generate a set of both absolute (e.g., \"synthesize a happy voice\") and relative (e.g., \"increase the happiness of this voice\") instructions that can be applied to those personas. The wide array of open source audio datasets used as the basis for Fugatto generally don't have these kinds of trait measurements embedded in them by default. But the researchers make use of existing audio understanding models to create \"synthetic captions\" for their training clips based on their prompts, creating natural language descriptions that can automatically quantify traits such as gender, emotion, and speech quality. Audio processing tools are also used to describe and quantify training clips on a more acoustic level (e.g. \"fundamental frequency variance\" or \"reverb\"). For relational comparisons, the researchers rely on datasets where one factor is held constant while another changes, such as different emotional readings of the same text or different instruments playing the same notes. By comparing these samples across a large enough set of data samples, the model can start to learn what kinds of audio characteristics tend to appear in \"happier\" speech, for instance, or differentiate the sound of a saxophone and a flute. After running a variety of different open source audio collections through this process, the researchers ended up with a heavily annotated dataset of 20 million separate samples representing at least 50,000 hours of audio. From there, a set of 32 Nvidia tensor cores was used to create a model with 2.5 billion parameters that started to show reliable scores on a variety of audio quality tests. It’s all in the mix OK, Fugatto, can we get a little more barking and a little less saxophone in the monitors? Credit: Getty Images Beyond the training, Nvidia is also talking up Fugatto's \"ComposableART\" system (for \"Audio Representation Transformation\"). When provided with a prompt in text and/or audio, this system can use \"conditional guidance\" to \"independently control and generate (unseen) combinations of instructions and tasks\" and generate \"highly customizable audio outputs outside the training distribution.\" In other words, it can combine different traits from its training set to create entirely new sounds that have never been heard before. I won't pretend to understand all of the complex math described in the paper—which involves a \"weighted combination of vector fields between instructions, frame indices and models.\" But the end results, as shown in examples on the project's webpage and in an Nvidia trailer, highlight how ComposableART can be used to create the sound of, say, a violin that \"sounds like a laughing baby or a banjo that's playing in front of gentle rainfall\" or \"factory machinery that screams in metallic agony.\" While some of these examples are more convincing to our ears than others, the fact that Fugatto can take a decent stab at these kinds of combinations at all is a testament to the way the model characterizes and mixes extremely disparate audio data from multiple different open source data sets. Perhaps the most interesting part of Fugatto is the way it treats each individual audio trait as a tunable continuum, rather than a binary. For an example that melds the sound of an acoustic guitar and running water, for instance, the result ends up very different when either the guitar or the water is weighted more heavily in Fugatto's interpolated mix. Nvidia also mentions examples of tuning a French accent to be heavier or lighter, or varying the \"degree of sorrow\" inherent in a spoken clip. Beyond tuning and combining different audio traits, Fugatto can also perform the kinds of audio tasks we've seen in previous models, like changing the emotion in a piece of spoken text or isolating the vocal track in a piece of music. Fugatto can also detect individual notes in a piece of MIDI music and replace them with a variety of vocal performances, or detect the beat of a piece of music and add effects from drums to barking dogs to ticking clocks in a way that matches the rhythm. Fugatto's generated audio (magenta) matches the melody of an input MIDI file (Cyan) very closely. Credit: Nvidia Research While the researchers describe Fugatto as just the first step \"towards a future where unsupervised multitask learning emerges from data and model scale,\" Nvidia is already talking up use cases from song prototyping to dynamically changing video game scores to international ad targeting. But Nvidia was also quick to highlight that models like Fugatto are best seen as a new tool for audio artists rather than a replacement for their creative talents. \"The history of music is also a history of technology,\" Nvidia Inception participant and producer/songwriter Ido Zmishlany said in Nvidia's blog post. \"The electric guitar gave the world rock and roll. When the sampler showed up, hip-hop was born. With AI, we’re writing the next chapter of music. We have a new instrument, a new tool for making music—and that’s super exciting.\" Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once wrote a whole book about Minesweeper. 39 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1469448038-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"app\"\u003e\n    \u003cp\u003e\u003ca href=\"#main\"\u003e\n  Skip to content\n\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cmain id=\"main\"\u003e\n            \u003carticle data-id=\"2063682\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n    \u003cdiv\u003e\n      \u003cdiv\u003e\n        \u003cp\u003e\u003cspan\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"section-ai_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"section-ai_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#section-ai_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#section-ai_svg__b)\"\u003e\u003cpath d=\"M20 2.4c9.7 0 17.6 7.9 17.6 17.6S29.7 37.6 20 37.6 2.4 29.7 2.4 20 10.3 2.4 20 2.4M20 0C9 0 0 9 0 20s9 20 20 20 20-9 20-20S31 0 20 0\"\u003e\u003c/path\u003e\u003cpath d=\"M20 13q2.85 0 5.4.9c.7.2 1.4-.1 1.6-.9l1.4-5.5C26 5.9 23.1 4.9 20 4.9s-6 .9-8.4 2.6L13 13c.2.7.9 1.1 1.6.9Q17 13 20 13M8.9 18.3c.4-.8 1-1.5 1.7-2.1l-2.2-5.7C7 12.2 6 14.1 5.5 16.3l1.3 2.1c.5.8 1.7.8 2.2 0m24.3 0 1.3-2.1c-.5-2.2-1.5-4.1-2.9-5.8l-2.2 5.7c.7.6 1.3 1.3 1.7 2.1.5.8 1.6.9 2.2 0M23.2 20c0 1.8-1.5 3.2-3.2 3.2s-3.2-1.4-3.2-3.2 1.5-3.2 3.2-3.2 3.2 1.4 3.2 3.2\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n  \u003c/span\u003e\n  \u003cspan\u003e\n    You\u0026#39;ve never heard anything like it\n  \u003c/span\u003e\n\u003c/p\u003e\n      \u003c/div\u003e\n\n      \n\n      \u003cp\u003e\n        What does a screaming saxophone sound like? The Fugatto model has an answer...\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\n    \u003cdiv\u003e\n    \n    \u003cp\u003e\n      An audio wave can contain so much. An angry cello, for instance...\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Getty Images\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \n\u003cp\u003eAt this point, anyone who has been following AI research is \u003ca href=\"https://arstechnica.com/information-technology/2023/09/ai-can-now-generate-cd-quality-music-from-text-and-its-only-getting-better/\"\u003elong familiar\u003c/a\u003e with \u003ca href=\"https://arstechnica.com/information-technology/2023/01/googles-new-ai-model-creates-songs-from-text-descriptions-of-moods-sounds/\"\u003egenerative models\u003c/a\u003e that can \u003ca href=\"https://arstechnica.com/information-technology/2023/01/microsofts-new-ai-can-simulate-anyones-voice-with-3-seconds-of-audio/\"\u003esynthesize speech\u003c/a\u003e or \u003ca href=\"https://arstechnica.com/information-technology/2024/04/new-ai-music-generator-udio-synthesizes-realistic-music-on-demand/\"\u003emelodic music\u003c/a\u003e from \u003ca href=\"https://arstechnica.com/information-technology/2024/04/mit-license-text-becomes-viral-sad-girl-piano-ballad-generated-by-ai/\"\u003enothing but text prompting\u003c/a\u003e. Nvidia\u0026#39;s \u003ca href=\"https://d1qx31qr3h6wln.cloudfront.net/publications/FUGATTO.pdf\"\u003enewly revealed \u0026#34;Fugatto\u0026#34; model\u003c/a\u003e looks to go a step further, using new synthetic training methods and inference-level combination techniques to \u0026#34;transform any mix of music, voices, and sounds,\u0026#34; including the synthesis of sounds that have never existed.\u003c/p\u003e\n\u003cp\u003eWhile Fugatto isn\u0026#39;t available for public testing yet, \u003ca href=\"https://fugatto.github.io/\"\u003ea sample-filled website\u003c/a\u003e showcases how Fugatto can be used to dial a number of distinct audio traits and descriptions up or down, resulting in everything from the sound of saxophones barking to people speaking underwater to ambulance sirens singing in a kind of choir. While the results on display can be a bit hit or miss, the vast array of capabilities on display here helps support Nvidia\u0026#39;s description of Fugatto as \u0026#34;a Swiss Army knife for sound.\u0026#34;\u003c/p\u003e\n\u003ch2\u003eYou’re only as good as your data\u003c/h2\u003e\n\u003cp\u003eIn \u003ca href=\"https://d1qx31qr3h6wln.cloudfront.net/publications/FUGATTO.pdf\"\u003ean explanatory research paper\u003c/a\u003e, over a dozen Nvidia researchers explain the difficulty in crafting a training dataset that can \u0026#34;reveal meaningful relationships between audio and language.\u0026#34; While standard language models can often infer how to handle various instructions from the text-based data itself, it can be hard to generalize descriptions and traits from audio without more explicit guidance.\u003c/p\u003e\n\u003cfigure\u003e\u003cp\u003e\u003ciframe allow=\"fullscreen\" loading=\"lazy\" src=\"https://www.youtube.com/embed/qj1Sp8He6e4?start=0\u0026amp;wmode=transparent\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003c/figure\u003e\n\u003cp\u003eTo that end, the researchers start by using an LLM to generate a Python script that can create a large number of template-based and free-form instructions describing different audio \u0026#34;personas\u0026#34; (e.g., \u0026#34;standard, young-crowd, thirty-somethings, professional\u0026#34;). They then generate a set of both absolute (e.g., \u0026#34;synthesize a happy voice\u0026#34;) and relative (e.g., \u0026#34;increase the happiness of this voice\u0026#34;) instructions that can be applied to those personas.\u003c/p\u003e\n\u003cp\u003eThe wide array of open source audio datasets used as the basis for Fugatto generally don\u0026#39;t have these kinds of trait measurements embedded in them by default. But the researchers make use of existing audio understanding models to create \u0026#34;synthetic captions\u0026#34; for their training clips based on their prompts, creating natural language descriptions that can automatically quantify traits such as gender, emotion, and speech quality. Audio processing tools are also used to describe and quantify training clips on a more acoustic level (e.g. \u0026#34;fundamental frequency variance\u0026#34; or \u0026#34;reverb\u0026#34;).\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eFor relational comparisons, the researchers rely on datasets where one factor is held constant while another changes, such as different emotional readings of the same text or different instruments playing the same notes. By comparing these samples across a large enough set of data samples, the model can start to learn what kinds of audio characteristics tend to appear in \u0026#34;happier\u0026#34; speech, for instance, or differentiate the sound of a saxophone and a flute.\u003c/p\u003e\n\u003cp\u003eAfter running a variety of different open source audio collections through this process, the researchers ended up with a heavily annotated dataset of 20 million separate samples representing at least 50,000 hours of audio. From there, a set of 32 Nvidia tensor cores was used to create a model with 2.5 billion parameters that started to show reliable scores on a variety of audio quality tests.\u003c/p\u003e\n\u003ch2\u003eIt’s all in the mix\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"1024\" height=\"683\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1219865150-1024x683.jpg\" alt=\"\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1219865150-1024x683.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1219865150-640x427.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1219865150-768x512.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1219865150-1536x1024.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1219865150-2048x1366.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1219865150-980x654.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1219865150-1440x960.jpg 1440w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      OK, Fugatto, can we get a little more barking and a little less saxophone in the monitors?\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Getty Images\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eBeyond the training, Nvidia is also talking up Fugatto\u0026#39;s \u0026#34;ComposableART\u0026#34; system (for \u0026#34;Audio Representation Transformation\u0026#34;). When provided with a prompt in text and/or audio, this system can use \u0026#34;conditional guidance\u0026#34; to \u0026#34;independently control and generate (unseen) combinations of instructions and tasks\u0026#34; and generate \u0026#34;highly customizable audio outputs outside the training distribution.\u0026#34; In other words, it can combine different traits from its training set to create entirely new sounds that have never been heard before.\u003c/p\u003e\n\u003cp\u003eI won\u0026#39;t pretend to understand all of the complex math described in the paper—which involves a \u0026#34;weighted combination of vector fields between instructions, frame indices and models.\u0026#34; But the end results, as \u003ca href=\"https://fugatto.github.io/\"\u003eshown in examples on the project\u0026#39;s webpage\u003c/a\u003e and in \u003ca href=\"https://www.youtube.com/watch?v=qj1Sp8He6e4\"\u003ean Nvidia trailer\u003c/a\u003e, highlight how ComposableART can be used to create the sound of, say, a violin that \u0026#34;sounds like a laughing baby or a banjo that\u0026#39;s playing in front of gentle rainfall\u0026#34; or \u0026#34;factory machinery that screams in metallic agony.\u0026#34; While some of these examples are more convincing to our ears than others, the fact that Fugatto can take a decent stab at these kinds of combinations at all is a testament to the way the model characterizes and mixes extremely disparate audio data from multiple different open source data sets.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\n\u003cp\u003ePerhaps the most interesting part of Fugatto is the way it treats each individual audio trait as a tunable continuum, rather than a binary. For an example that melds the sound of an acoustic guitar and running water, for instance, the result ends up very different when either the guitar or the water is weighted more heavily in Fugatto\u0026#39;s interpolated mix. Nvidia also mentions examples of tuning a French accent to be heavier or lighter, or varying the \u0026#34;degree of sorrow\u0026#34; inherent in a spoken clip.\u003c/p\u003e\n\u003cp\u003eBeyond tuning and combining different audio traits, Fugatto can also perform the kinds of audio tasks we\u0026#39;ve seen in previous models, like changing the emotion in a piece of spoken text or isolating the vocal track in a piece of music. Fugatto can also detect individual notes in a piece of MIDI music and replace them with a variety of vocal performances, or detect the beat of a piece of music and add effects from drums to barking dogs to ticking clocks in a way that matches the rhythm.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"814\" height=\"489\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/fugatto.png\" alt=\"\" decoding=\"async\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/fugatto.png 814w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/fugatto-640x384.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/11/fugatto-768x461.png 768w\" sizes=\"auto, (max-width: 814px) 100vw, 814px\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      Fugatto\u0026#39;s generated audio (magenta) matches the melody of an input MIDI file (Cyan) very closely.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://d1qx31qr3h6wln.cloudfront.net/publications/FUGATTO.pdf\" target=\"_blank\"\u003e\n          \n          Nvidia Research\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eWhile the researchers describe Fugatto as just the first step \u0026#34;towards a future where unsupervised multitask learning emerges from data and model scale,\u0026#34; Nvidia is already talking up use cases from song prototyping to dynamically changing video game scores to international ad targeting. But Nvidia was also quick to highlight that models like Fugatto are best seen as a new tool for audio artists rather than a replacement for their creative talents.\u003c/p\u003e\n\u003cp\u003e\u0026#34;The history of music is also a history of technology,\u0026#34; Nvidia Inception participant and producer/songwriter Ido Zmishlany said in Nvidia\u0026#39;s blog post. \u0026#34;The electric guitar gave the world rock and roll. When the sampler showed up, hip-hop was born. With AI, we’re writing the next chapter of music. We have a new instrument, a new tool for making music—and that’s super exciting.\u0026#34;\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/kyle-orland/\"\u003e\u003cimg src=\"https://arstechnica.com/wp-content/uploads/2016/05/k.orland-13.jpg\" alt=\"Photo of Kyle Orland\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once \u003ca href=\"https://bossfightbooks.com/collections/books/products/minesweeper-by-kyle-orland\"\u003ewrote a whole book about \u003cem\u003eMinesweeper\u003c/em\u003e\u003c/a\u003e.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/ai/2024/11/nvidias-new-ai-audio-model-can-synthesize-sounds-that-have-never-existed/#comments\" title=\"39 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    39 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n\n\n\n\n\n  \n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/space/2024/11/after-russian-ship-docks-to-space-station-astronauts-report-a-foul-smell/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/02/52674646862_dbe4378754_k-768x432.jpg\" alt=\"Listing image for first story in Most Read: After Russian ship docks to space station, astronauts report a foul smell\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/main\u003e\n\n\n\n\n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2024-11-25T21:40:00Z",
  "modifiedTime": "2024-11-25T21:58:08Z"
}
