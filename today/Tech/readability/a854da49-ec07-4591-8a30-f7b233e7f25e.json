{
  "id": "a854da49-ec07-4591-8a30-f7b233e7f25e",
  "title": "Our ongoing work to build and deploy responsible AI",
  "link": "https://blog.google/technology/safety-security/google-paris-summit-responsible-ai/",
  "description": "At Google’s Responsible AI Summit in Paris, our VP of Trust \u0026 Safety Laurie Richardson spoke about our ongoing work to build responsible AI.",
  "author": "Laurie RichardsonVice President, Trust \u0026 SafetyGoogle",
  "published": "Fri, 04 Oct 2024 08:00:00 +0000",
  "source": "https://www.blog.google/rss/",
  "categories": [
    "Safety \u0026 Security",
    "Google in Europe"
  ],
  "byline": "Laurie Richardson",
  "length": 4960,
  "excerpt": "At Google’s Responsible AI Summit in Paris, our VP of Trust \u0026 Safety Laurie Richardson spoke about our ongoing work to build responsible AI.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png",
  "text": "Editor’s note: This week, at the Google Responsible AI Summit in Paris, our VP of Trust \u0026 Safety Laurie Richardson delivered a keynote address to an audience of experts across academia, industry, startups, government and civil society. The following excerpt has been edited for brevity.AI has the potential to solve big challenges, from saving lives by predicting when and where floods may occur, to transforming our understanding of the biological world and drug discovery. However, in order to realize these opportunities, it is critically important that we build and maintain trust in AI’s potential.That’s why, as people begin to use AI in their daily lives, we are building technology in ways that seek to maximize benefits and minimize risks.Our AI Responsibility LifecycleOur Trust \u0026 Safety teams are pioneering testing, training and red-teaming techniques to ensure that when our GenAI products go to market, they are both bold and responsible. Every day, we learn more about how to test for safety, neutrality, fairness and dangerous capabilities, and we’re committed to sharing our approach more broadly.This year we launched our AI Responsibility Lifecycle framework to the public. This is a four-phase process — covering Research, Design, Governance and Sharing — that guides responsible AI development end-to-end at Google. Detecting abuse at scaleOur teams across Trust \u0026 Safety are also using AI to improve the way we protect our users online. AI is showing tremendous promise for speed and scale in nuanced abuse detection. Building on our established automated processes, we have developed prototypes that leverage recent advances, to assist our teams in identifying abusive content at scale.Using LLMs, our aim is to be able to rapidly build and train a model in a matter of days — instead of weeks or months — to find specific kinds of abuse on our products. This is especially valuable for new and emerging abuse areas, such as Russian disinformation narratives following the invasion of Ukraine, or for nuanced scaled challenges, like detecting counterfeit goods online. We can quickly prototype a model and automatically route it to our teams for enforcement.LLMs are also transforming training. Using new techniques, we can now expand coverage of abuse types, context and languages in ways we never could have before — including doubling the number of languages covered with our on-device safety classifiers in the last quarter alone. Starting with an insight from one of our abuse analysts, we can use LLMs to generate thousands of variations of an event and then use this to train our classifiers.We're still testing these new techniques to meet rigorous accuracy standards, but prototypes have demonstrated impressive results so far. The potential is huge, and I believe we are at the cusp of dramatic transformation in this space.Boosting collaboration and transparencyAddressing AI-generated content will require industry and ecosystem collaboration and solutions; no one company or institution can do this work alone. Earlier this week at the summit, we brought together researchers and students to engage with our safety experts to discuss risks and opportunities in the age of AI. In support of an ecosystem that generates impactful research with real-world applications, we doubled the number of Google Academic Research Awards recipients this year to grow our investment into Trust \u0026 Safety research solutions.Finally, information quality has always been core to Google’s mission, and part of that is making sure that users have context to assess the trustworthiness of content they find online. As we continue to bring AI to more products and services, we are focused on helping people better understand how a particular piece of content was created and modified over time.Earlier this year, we joined the Coalition for Content Provenance and Authenticity (C2PA), as a steering committee member. We are partnering with others to develop interoperable provenance standards and technology to help explain whether a photo was taken with a camera, edited by software or produced by generative AI. This kind of information helps our users make more informed decisions about the content they’re engaging with — including photos, videos and audio — and builds media literacy and trust.​​Our work with the C2PA directly complements our own broader approach to transparency and the responsible development of AI. For example, we’re continuing to bring our SynthID watermarking tools to additional gen AI tools and more forms of media including text, audio, visual and video.We're committed to deploying AI responsibly — from using AI to strengthen our platforms against abuse to developing tools to enhance media literacy and trust — all while focused on the importance of collaborating, sharing insights and building AI responsibly, together.",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/safety_5.width-1300.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-reading-time=\"true\" data-component=\"uni-drop-cap|uni-tombstone\"\u003e\n\n            \n              \n\n\n\u003cgoogle-read-aloud-player data-analytics-module=\"{\n        \u0026#34;event\u0026#34;: \u0026#34;module_impression\u0026#34;,\n        \u0026#34;module_name\u0026#34;: \u0026#34;ai_audio\u0026#34;,\n        \u0026#34;section_header\u0026#34;: \u0026#34;Our ongoing work to build and deploy responsible AI\u0026#34;\n    }\" data-date-modified=\"2024-10-04T14:50:25.253037+00:00\" data-progress-bar-style=\"half-wave\" data-api-key=\"AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac\" data-article-style=\"style9\" data-tracking-ids=\"G-HGNBTNCHCQ,G-6NKTLKV14N\" data-voice-list=\"en.ioh-pngnat:Cyan,en.usb-pngnat:Lime\" data-layout-style=\"style1\" data-highlight-mode=\"word-over-paragraph\" data-highlight-text-color=\"#000000\" data-highlight-word-background=\"#8AB4F8\" data-highlight-paragraph-background=\"#D2E3FC\" data-background=\"linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)\" data-foreground-color=\"#202124\" data-font=\"600 16px Google Sans, sans-serif\" data-box-shadow=\"0px 1px 3px 1px rgba(60, 64, 67, 0.15)\"\u003e\n\u003c/google-read-aloud-player\u003e\n\n\n\n\n            \n\n            \n            \n\n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Our ongoing work to build and deploy responsible AI\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"4x6bz\"\u003e\u003ci\u003eEditor’s note: This week, at the Google Responsible AI Summit in Paris, our VP of Trust \u0026amp; Safety Laurie Richardson delivered a keynote address to an audience of experts across academia, industry, startups, government and civil society. The following excerpt has been edited for brevity.\u003cbr/\u003e\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"13lu5\"\u003eAI has the potential to solve big challenges, from saving lives by predicting when and where floods may occur, to transforming our understanding of the biological world and drug discovery. However, in order to realize these opportunities, it is critically important that we build and maintain trust in AI’s potential.\u003c/p\u003e\u003cp data-block-key=\"4ivrc\"\u003eThat’s why, as people begin to use AI in their daily lives, we are building technology in ways that seek to maximize benefits and minimize risks.\u003c/p\u003e\u003ch3 data-block-key=\"ajeqn\"\u003eOur AI Responsibility Lifecycle\u003c/h3\u003e\u003cp data-block-key=\"di9hu\"\u003eOur Trust \u0026amp; Safety teams are pioneering testing, training and \u003ca href=\"https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/\"\u003ered-teaming techniques\u003c/a\u003e to ensure that when our GenAI products go to market, they are both bold and responsible. Every day, we learn more about how to test for safety, neutrality, fairness and dangerous capabilities, and we’re committed to sharing our approach more broadly.\u003c/p\u003e\u003cp data-block-key=\"aijob\"\u003eThis year we launched our \u003ca href=\"https://ai.google/static/documents/ai-responsibility-2024-update.pdf\"\u003eAI Responsibility Lifecycle framework\u003c/a\u003e to the public. This is a four-phase process — covering Research, Design, Governance and Sharing — that guides responsible AI development end-to-end at Google.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\n  \n      \u003cdiv data-analytics-module=\"{\n          \u0026#34;module_name\u0026#34;: \u0026#34;Inline Images\u0026#34;,\n          \u0026#34;section_header\u0026#34;: \u0026#34;Our ongoing work to build and deploy responsible AI\u0026#34;\n        }\"\u003e\n  \n\n  \u003cp\u003e\u003cimg alt=\"A diagram of a life cycle, with arrows going clockwise and connecting four concepts (in this order): Research, Design, Govern and Share.\" src=\" https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Screenshot_2024-10-01_4.22.18_PM_.width-100.format-webp.webp \" loading=\"lazy\" data-loading=\"{\n                \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Screenshot_2024-10-01_4.22.18_PM_.width-500.format-webp.webp\u0026#34;,\n                \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Screenshot_2024-10-01_4.22.18_PM.width-1000.format-webp.webp\u0026#34;\n              }\"/\u003e\n        \n      \n    \n    \u003c/p\u003e\n    \n  \n    \u003c/div\u003e\n  \n\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Our ongoing work to build and deploy responsible AI\u0026#34;\n         }\"\u003e\u003ch3 data-block-key=\"4x6bz\"\u003eDetecting abuse at scale\u003c/h3\u003e\u003cp data-block-key=\"aueie\"\u003eOur teams across Trust \u0026amp; Safety are also using AI to improve the way we protect our users online. AI is showing tremendous promise for speed and scale in nuanced abuse detection. Building on our established automated processes, we have developed prototypes that leverage recent advances, to assist our teams in identifying abusive content at scale.\u003c/p\u003e\u003cp data-block-key=\"4h489\"\u003eUsing LLMs, our aim is to be able to rapidly build and train a model in a matter of days — instead of weeks or months — to find specific kinds of abuse on our products. This is especially valuable for new and emerging abuse areas, such as Russian disinformation narratives following the invasion of Ukraine, or for nuanced scaled challenges, like detecting counterfeit goods online. We can quickly prototype a model and automatically route it to our teams for enforcement.\u003c/p\u003e\u003cp data-block-key=\"d6is0\"\u003eLLMs are also transforming training. Using new techniques, we can now expand coverage of abuse types, context and languages in ways we never could have before — including doubling the number of languages covered with our on-device safety classifiers in the last quarter alone. Starting with an insight from one of our abuse analysts, we can use LLMs to generate thousands of variations of an event and then use this to train our classifiers.\u003c/p\u003e\u003cp data-block-key=\"4ism1\"\u003eWe\u0026#39;re still testing these new techniques to meet rigorous accuracy standards, but prototypes have demonstrated impressive results so far. The potential is huge, and I believe we are at the cusp of dramatic transformation in this space.\u003c/p\u003e\u003ch3 data-block-key=\"2b0al\"\u003eBoosting collaboration and transparency\u003c/h3\u003e\u003cp data-block-key=\"1e7nu\"\u003eAddressing AI-generated content will require industry and ecosystem collaboration and solutions; no one company or institution can do this work alone. Earlier this week at the summit, we brought together researchers and students to engage with our safety experts to discuss risks and opportunities in the age of AI. In support of an ecosystem that generates impactful research with real-world applications, we doubled the number of \u003ca href=\"https://research.google/programs-and-events/google-academic-research-awards/\"\u003eGoogle Academic Research Awards\u003c/a\u003e recipients this year to grow our investment into Trust \u0026amp; Safety research solutions.\u003c/p\u003e\u003cp data-block-key=\"1k6mo\"\u003eFinally, information quality has always been core to Google’s mission, and part of that is making sure that users have context to assess the trustworthiness of content they find online. As we continue to bring AI to more products and services, we are focused on helping people better understand how a particular piece of content was created and modified over time.\u003c/p\u003e\u003cp data-block-key=\"bd505\"\u003eEarlier this year, we \u003ca href=\"https://c2pa.org/post/google_pr/\"\u003ejoined the Coalition for Content Provenance and Authenticity (C2PA), as a steering committee member\u003c/a\u003e. We are partnering with others to develop interoperable provenance standards and technology to help explain whether a photo was taken with a camera, edited by software or produced by generative AI. This kind of information helps our users make more informed decisions about the content they’re engaging with — including photos, videos and audio — and builds media literacy and trust.\u003c/p\u003e\u003cp data-block-key=\"2fcmn\"\u003e​​Our \u003ca href=\"https://blog.google/technology/ai/google-gen-ai-content-transparency-c2pa/\"\u003ework with the C2PA\u003c/a\u003e directly complements our own broader approach to transparency and the responsible development of AI. For example, we’re continuing to bring our \u003ca href=\"https://deepmind.google/technologies/synthid/\"\u003eSynthID\u003c/a\u003e watermarking tools to additional gen AI tools and more forms of media including text, audio, visual and video.\u003c/p\u003e\u003cp data-block-key=\"6as67\"\u003eWe\u0026#39;re committed to deploying AI responsibly — from using AI to strengthen our platforms against abuse to developing tools to enhance media literacy and trust — all while focused on the importance of collaborating, sharing insights and building AI responsibly, together.\u003c/p\u003e\u003c/div\u003e\n  \n\n\n            \n            \n\n            \n              \n\n\n\n\n            \n          \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-10-04T08:00:00Z",
  "modifiedTime": null
}
