{
  "id": "1ffe64f3-6afd-45ac-ac7a-2700bdc50bbf",
  "title": "AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms",
  "link": "https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/",
  "description": "Comments",
  "author": "",
  "published": "Wed, 14 May 2025 15:10:15 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "By AlphaEvolve team",
  "length": 12774,
  "excerpt": "New AI agent evolves algorithms for math and practical applications in computing by combining the creativity of large language models with automated evaluators",
  "siteName": "Google DeepMind",
  "favicon": "https://www.gstatic.com/images/branding/productlogos/google_deepmind/v3/web-96dp/logo_google_deepmind_color_2x_web_96dp.png",
  "text": "Research Published 14 May 2025 Authors New AI agent evolves algorithms for math and practical applications in computing by combining the creativity of large language models with automated evaluatorsLarge language models (LLMs) are remarkably versatile. They can summarize documents, generate code or even brainstorm new ideas. And now we’ve expanded these capabilities to target fundamental and highly complex problems in mathematics and modern computing.Today, we’re announcing AlphaEvolve, an evolutionary coding agent powered by large language models for general-purpose algorithm discovery and optimization. AlphaEvolve pairs the creative problem-solving capabilities of our Gemini models with automated evaluators that verify answers, and uses an evolutionary framework to improve upon the most promising ideas.AlphaEvolve enhanced the efficiency of Google's data centers, chip design and AI training processes — including training the large language models underlying AlphaEvolve itself. It has also helped design faster matrix multiplication algorithms and find new solutions to open mathematical problems, showing incredible promise for application across many areas. Designing better algorithms with large language modelsIn 2023, we showed for the first time that large language models can generate functions written in computer code to help discover new and provably correct knowledge on an open scientific problem. AlphaEvolve is an agent that can go beyond single function discovery to evolve entire codebases and develop much more complex algorithms.AlphaEvolve leverages an ensemble of state-of-the-art large language models: our fastest and most efficient model, Gemini Flash, maximizes the breadth of ideas explored, while our most powerful model, Gemini Pro, provides critical depth with insightful suggestions. Together, these models propose computer programs that implement algorithmic solutions as code. Diagram showing how the prompt sampler first assembles a prompt for the language models, which then generate new programs. These programs are evaluated by evaluators and stored in the programs database. This database implements an evolutionary algorithm that determines which programs will be used for future prompts. AlphaEvolve verifies, runs and scores the proposed programs using automated evaluation metrics. These metrics provide an objective, quantifiable assessment of each solution’s accuracy and quality. This makes AlphaEvolve particularly helpful in a broad range of domains where progress can be clearly and systematically measured, like in math and computer science. Optimizing our computing ecosystemOver the past year, we’ve deployed algorithms discovered by AlphaEvolve across Google’s computing ecosystem, including our data centers, hardware and software. The impact of each of these improvements is multiplied across our AI and computing infrastructure to build a more powerful and sustainable digital ecosystem for all our users. Diagram showing how AlphaEvolve helps Google deliver a more efficient digital ecosystem, from data center scheduling and hardware design to AI model training. Improving data center schedulingAlphaEvolve discovered a simple yet remarkably effective heuristic to help Borg orchestrate Google's vast data centers more efficiently. This solution, now in production for over a year, continuously recovers, on average, 0.7% of Google’s worldwide compute resources. This sustained efficiency gain means that at any given moment, more tasks can be completed on the same computational footprint. AlphaEvolve's solution not only leads to strong performance but also offers significant operational advantages of human-readable code: interpretability, debuggability, predictability and ease of deployment. Assisting in hardware designAlphaEvolve proposed a Verilog rewrite that removed unnecessary bits in a key, highly optimized arithmetic circuit for matrix multiplication. Crucially, the proposal must pass robust verification methods to confirm that the modified circuit maintains functional correctness. This proposal was integrated into an upcoming Tensor Processing Unit (TPU), Google’s custom AI accelerator. By suggesting modifications in the standard language of chip designers, AlphaEvolve promotes a collaborative approach between AI and hardware engineers to accelerate the design of future specialized chips. Enhancing AI training and inferenceAlphaEvolve is accelerating AI performance and research velocity. By finding smarter ways to divide a large matrix multiplication operation into more manageable subproblems, it sped up this vital kernel in Gemini’s architecture by 23%, leading to a 1% reduction in Gemini's training time. Because developing generative AI models requires substantial computing resources, every efficiency gained translates to considerable savings. Beyond performance gains, AlphaEvolve significantly reduces the engineering time required for kernel optimization, from weeks of expert effort to days of automated experiments, allowing researchers to innovate faster.AlphaEvolve can also optimize low level GPU instructions. This incredibly complex domain is usually already heavily optimized by compilers, so human engineers typically don't modify it directly. AlphaEvolve achieved up to a 32.5% speedup for the FlashAttention kernel implementation in Transformer-based AI models. This kind of optimization helps experts pinpoint performance bottlenecks and easily incorporate the improvements into their codebase, boosting their productivity and enabling future savings in compute and energy. Advancing the frontiers in mathematics and algorithm discoveryAlphaEvolve can also propose new approaches to complex mathematical problems. Provided with a minimal code skeleton for a computer program, AlphaEvolve designed many components of a novel gradient-based optimization procedure that discovered multiple new algorithms for matrix multiplication, a fundamental problem in computer science. A list of changes proposed by AlphaEvolve to discover faster matrix multiplication algorithms. In this example, AlphaEvolve proposes extensive changes across several components, including the optimizer and weight initialization, the loss function, and hyperparameter sweep. These changes are highly non-trivial, requiring 15 mutations during the evolutionary process. AlphaEvolve’s procedure found an algorithm to multiply 4x4 complex-valued matrices using 48 scalar multiplications, improving upon Strassen’s 1969 algorithm that was previously known as the best in this setting. This finding demonstrates a significant advance over our previous work, AlphaTensor, which specialized in matrix multiplication algorithms, and for 4x4 matrices, only found improvements for binary arithmetic.To investigate AlphaEvolve’s breadth, we applied the system to over 50 open problems in mathematical analysis, geometry, combinatorics and number theory. The system’s flexibility enabled us to set up most experiments in a matter of hours. In roughly 75% of cases, it rediscovered state-of-the-art solutions, to the best of our knowledge.And in 20% of cases, AlphaEvolve improved the previously best known solutions, making progress on the corresponding open problems. For example, it advanced the kissing number problem. This geometric challenge has fascinated mathematicians for over 300 years and concerns the maximum number of non-overlapping spheres that touch a common unit sphere. AlphaEvolve discovered a configuration of 593 outer spheres and established a new lower bound in 11 dimensions. The path forwardAlphaEvolve displays the progression from discovering algorithms for specific domains to developing more complex algorithms for a wide range of real-world challenges. We’re expecting AlphaEvolve to continue improving alongside the capabilities of large language models, especially as they become even better at coding.Together with the People + AI Research team, we’ve been building a friendly user interface for interacting with AlphaEvolve. We’re planning an Early Access Program for selected academic users and also exploring possibilities to make AlphaEvolve more broadly available. To register your interest, please complete this form.While AlphaEvolve is currently being applied across math and computing, its general nature means it can be applied to any problem whose solution can be described as an algorithm, and automatically verified. We believe AlphaEvolve could be transformative across many more areas such as material science, drug discovery, sustainability and wider technological and business applications. Read more details in our white paper Register your interest in using AlphaEvolve See AlphaEvolve’s mathematical results in our Google Colab AcknowledgementsAlphaEvolve was developed by Matej Balog, Alexander Novikov, Ngân Vũ, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, and Pushmeet Kohli. This research was developed as part of our effort focused on using AI for algorithm discovery.We gratefully acknowledge contributions, advice, and support from Jean-Baptiste Alayrac, Ankit Anand, Natasha Antropova, Giorgio Arena, Mohammadamin Barekatain, Johannes Bausch, Henning Becker, Daniel Belov, Alexander Belyaev, Sebastian Bodenstein, Sebastian Borgeaud, Calin Cascaval, Indranil Chakraborty, Benjamin Chetioui, Justin Chiu, Christopher Clark, Marco Cornero, Jeff Dean, Gaurav Dhiman, Yanislav Donchev, Srikanth Dwarakanath, Jordan Ellenberg, Alhussein Fawzi, Michael Figurnov, Aaron Gentleman, Bogdan Georgiev, Sergio Guadarrama, Demis Hassabis, Patrick Heisel, Chase Hensel, Koray Kavukcuoglu, Sultan Kenjeyev, Aliia Khasanova, Sridhar Lakshmanamurthy, Sergei Lebedev, Dmitry Lepikhin, Daniel Mankowitz, Andrea Michi, Kieran Milan, Vinod Nair, Robert O'Callahan, Cosmin Paduraru, Stig Petersen, Federico Piccinini, Parthasarathy Ranganatha, Bernardino Romera-Paredes, Georges Rotival, Kirk Sanders, Javier Gomez Serrano, Oleg Shyshkov, Timur Sitdikov, Tammo Spalink, Kerry Takenaka, Richard Tanburn, Terence Tao, Amin Vahdat, JD Velasquez, Dimitrios Vytiniotis, Julian Walker, and Pengming Wang. For more details, please see our white paper.We would like to thank Armin Senoner, Juanita Bawagan, Jane Park, Arielle Bier, and Molly Beck for feedback on the blog post and help with this announcement; William Hood, Irina Andronic, Victoria Johnston, Lucas Dixon, Adam Connors, and Jimbo Wilson for help with the illustrations and figures",
  "image": "https://lh3.googleusercontent.com/Gw688MNwkQVBeUALSFtQz46Oh4NFoZAe10mEpvtmZhKuWhlQsi5uh2KFHKbxH8NhBnOGUNza6O6-0HElml2zEN06vI_9oAsjAxFVzxjDL5DOw7HsAw=w1200-h630-n-nu",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"content\"\u003e\n      \n  \u003carticle\u003e\n    \n    \n  \n  \n  \n    \n      \n\n      \n      \n        \n          \n            \u003cdiv\u003e\n              \n                \n                \n                  \n                  \n\u003cdiv\u003e\n    \u003cdiv\u003e\n      \u003cp\u003eResearch\u003c/p\u003e\n      \n\n      \n    \u003cdl\u003e\n      \n        \u003cdt\u003ePublished\u003c/dt\u003e\n        \u003cdd\u003e\u003ctime datetime=\"2025-05-14\"\u003e14 May 2025\u003c/time\u003e\u003c/dd\u003e\n      \n      \n        \u003cdt\u003eAuthors\u003c/dt\u003e\n        \n      \n    \u003c/dl\u003e\n  \n\n      \n    \u003c/div\u003e\n\n    \n      \n    \n    \n    \u003cpicture\u003e\n      \u003csource media=\"(min-width: 1024px)\" type=\"image/webp\" width=\"1072\" height=\"603\" srcset=\"https://lh3.googleusercontent.com/Gw688MNwkQVBeUALSFtQz46Oh4NFoZAe10mEpvtmZhKuWhlQsi5uh2KFHKbxH8NhBnOGUNza6O6-0HElml2zEN06vI_9oAsjAxFVzxjDL5DOw7HsAw=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/Gw688MNwkQVBeUALSFtQz46Oh4NFoZAe10mEpvtmZhKuWhlQsi5uh2KFHKbxH8NhBnOGUNza6O6-0HElml2zEN06vI_9oAsjAxFVzxjDL5DOw7HsAw=w2144-h1206-n-nu-rw 2x\"/\u003e\u003csource media=\"(min-width: 600px)\" type=\"image/webp\" width=\"928\" height=\"522\" srcset=\"https://lh3.googleusercontent.com/Gw688MNwkQVBeUALSFtQz46Oh4NFoZAe10mEpvtmZhKuWhlQsi5uh2KFHKbxH8NhBnOGUNza6O6-0HElml2zEN06vI_9oAsjAxFVzxjDL5DOw7HsAw=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/Gw688MNwkQVBeUALSFtQz46Oh4NFoZAe10mEpvtmZhKuWhlQsi5uh2KFHKbxH8NhBnOGUNza6O6-0HElml2zEN06vI_9oAsjAxFVzxjDL5DOw7HsAw=w1856-h1044-n-nu-rw 2x\"/\u003e\u003csource type=\"image/webp\" width=\"528\" height=\"297\" srcset=\"https://lh3.googleusercontent.com/Gw688MNwkQVBeUALSFtQz46Oh4NFoZAe10mEpvtmZhKuWhlQsi5uh2KFHKbxH8NhBnOGUNza6O6-0HElml2zEN06vI_9oAsjAxFVzxjDL5DOw7HsAw=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/Gw688MNwkQVBeUALSFtQz46Oh4NFoZAe10mEpvtmZhKuWhlQsi5uh2KFHKbxH8NhBnOGUNza6O6-0HElml2zEN06vI_9oAsjAxFVzxjDL5DOw7HsAw=w1056-h594-n-nu-rw 2x\"/\u003e\n      \u003cimg alt=\"Colourful and abstract digital landscape of code. Demonstrating peaks in high performing code.\" height=\"603\" src=\"https://lh3.googleusercontent.com/Gw688MNwkQVBeUALSFtQz46Oh4NFoZAe10mEpvtmZhKuWhlQsi5uh2KFHKbxH8NhBnOGUNza6O6-0HElml2zEN06vI_9oAsjAxFVzxjDL5DOw7HsAw=w1072-h603-n-nu\" width=\"1072\"/\u003e\n    \u003c/picture\u003e\n    \n  \n    \n  \u003c/div\u003e\n                \n              \n                \n                \n                  \n                  \u003cdiv\u003e\n  \u003ch4 data-block-key=\"1rhtp\"\u003eNew AI agent evolves algorithms for math and practical applications in computing by combining the creativity of large language models with automated evaluators\u003c/h4\u003e\u003cp data-block-key=\"bpa0i\"\u003eLarge language models (LLMs) are remarkably versatile. They can summarize documents, generate code or even brainstorm new ideas. And now we’ve expanded these capabilities to target fundamental and highly complex problems in mathematics and modern computing.\u003c/p\u003e\u003cp data-block-key=\"a2lsk\"\u003eToday, we’re announcing \u003ca href=\"https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf\" rel=\"noopener\" target=\"_blank\"\u003eAlphaEvolve\u003c/a\u003e, an evolutionary coding agent powered by large language models for general-purpose algorithm discovery and optimization. AlphaEvolve pairs the creative problem-solving capabilities of our \u003ca href=\"https://deepmind.google/technologies/gemini/\" rel=\"noopener\" target=\"_blank\"\u003eGemini models\u003c/a\u003e with automated evaluators that verify answers, and uses an evolutionary framework to improve upon the most promising ideas.\u003c/p\u003e\u003cp data-block-key=\"7b9nk\"\u003eAlphaEvolve enhanced the efficiency of Google\u0026#39;s data centers, chip design and AI training processes — including training the large language models underlying AlphaEvolve itself. It has also helped design faster matrix multiplication algorithms and find new solutions to open mathematical problems, showing incredible promise for application across many areas.\u003c/p\u003e\n\u003c/div\u003e\n                \n              \n                \n                \n                  \n                  \u003cdiv\u003e\n  \u003ch2 data-block-key=\"1rhtp\"\u003eDesigning better algorithms with large language models\u003c/h2\u003e\u003cp data-block-key=\"cmrs5\"\u003eIn 2023, we showed for the first time that large language models can generate functions written in computer code to help \u003ca href=\"https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/\" rel=\"noopener\" target=\"_blank\"\u003ediscover new and provably correct knowledge\u003c/a\u003e on an open scientific problem. AlphaEvolve is an agent that can go beyond single function discovery to evolve entire codebases and develop much more complex algorithms.\u003c/p\u003e\u003cp data-block-key=\"8lpr6\"\u003eAlphaEvolve leverages an ensemble of state-of-the-art large language models: our fastest and most efficient model, \u003ca href=\"https://deepmind.google/technologies/gemini/flash/?_gl=1*7wovog*_up*MQ..*_ga*ODcyNjk2MzY0LjE3NDYxODE1OTY.*_ga_LS8HVHCNQ0*MTc0NjE4MTU5NS4xLjAuMTc0NjE4MTU5OS4wLjAuMA..\" rel=\"noopener\" target=\"_blank\"\u003eGemini Flash\u003c/a\u003e, maximizes the breadth of ideas explored, while our most powerful model, \u003ca href=\"https://deepmind.google/technologies/gemini/pro/?_gl=1*5ncg4r*_up*MQ..*_ga*ODcyNjk2MzY0LjE3NDYxODE1OTY.*_ga_LS8HVHCNQ0*MTc0NjE4MTU5NS4xLjAuMTc0NjE4MTU5OS4wLjAuMA..\" rel=\"noopener\" target=\"_blank\"\u003eGemini Pro\u003c/a\u003e, provides critical depth with insightful suggestions. Together, these models propose computer programs that implement algorithmic solutions as code.\u003c/p\u003e\n\u003c/div\u003e\n                \n              \n                \n                \n                  \n                  \n\n\n\n\n\n\u003cfigure aria-labelledby=\"caption-37de9aef-e216-4e74-bad9-c132e221ef65\"\u003e\n  \n\n  \u003cfigcaption\u003e\n      \u003cp data-block-key=\"lnm7d\"\u003eDiagram showing how the prompt sampler first assembles a prompt for the language models, which then generate new programs. These programs are evaluated by evaluators and stored in the programs database. This database implements an evolutionary algorithm that determines which programs will be used for future prompts.\u003c/p\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e\n                \n              \n                \n                \n                  \n                  \u003cp data-block-key=\"1rhtp\"\u003eAlphaEvolve verifies, runs and scores the proposed programs using automated evaluation metrics. These metrics provide an objective, quantifiable assessment of each solution’s accuracy and quality. This makes AlphaEvolve particularly helpful in a broad range of domains where progress can be clearly and systematically measured, like in math and computer science.\u003c/p\u003e\n                \n              \n                \n                \n                  \n                  \u003cdiv\u003e\n  \u003ch3 data-block-key=\"1rhtp\"\u003eOptimizing our computing ecosystem\u003c/h3\u003e\u003cp data-block-key=\"eu4jo\"\u003eOver the past year, we’ve deployed algorithms discovered by AlphaEvolve across Google’s computing ecosystem, including our data centers, hardware and software. The impact of each of these improvements is multiplied across our AI and computing infrastructure to build a more powerful and sustainable digital ecosystem for all our users.\u003c/p\u003e\n\u003c/div\u003e\n                \n              \n                \n                \n                  \n                  \n\n\n\n\n\n\u003cfigure aria-labelledby=\"caption-3e1cbb33-1c95-4dc0-b1a4-6aa109adf4c3\"\u003e\n  \n\n  \u003cfigcaption\u003e\n      \u003cp data-block-key=\"lnm7d\"\u003eDiagram showing how AlphaEvolve helps Google deliver a more efficient digital ecosystem, from data center scheduling and hardware design to AI model training.\u003c/p\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e\n                \n              \n                \n                \n                  \n                  \u003cdiv\u003e\n  \u003ch3 data-block-key=\"1rhtp\"\u003eImproving data center scheduling\u003c/h3\u003e\u003cp data-block-key=\"3ra3e\"\u003eAlphaEvolve discovered a simple yet remarkably effective heuristic to help \u003ca href=\"https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/\" rel=\"noopener\" target=\"_blank\"\u003eBorg\u003c/a\u003e orchestrate Google\u0026#39;s vast data centers more efficiently. This solution, now in production for over a year, continuously recovers, on average, 0.7% of Google’s worldwide compute resources. This sustained efficiency gain means that at any given moment, more tasks can be completed on the same computational footprint. AlphaEvolve\u0026#39;s solution not only leads to strong performance but also offers significant operational advantages of human-readable code: interpretability, debuggability, predictability and ease of deployment.\u003c/p\u003e\n\u003c/div\u003e\n                \n              \n                \n                \n                  \n                  \u003cdiv\u003e\n  \u003ch3 data-block-key=\"1rhtp\"\u003eAssisting in hardware design\u003c/h3\u003e\u003cp data-block-key=\"9gnp2\"\u003eAlphaEvolve proposed a \u003ca href=\"https://en.wikipedia.org/wiki/Verilog\" rel=\"noopener\" target=\"_blank\"\u003eVerilog\u003c/a\u003e rewrite that removed unnecessary bits in a key, highly optimized arithmetic circuit for matrix multiplication. Crucially, the proposal must pass robust verification methods to confirm that the modified circuit maintains functional correctness. This proposal was integrated into an upcoming \u003ca href=\"https://cloud.google.com/tpu?hl=en\" rel=\"noopener\" target=\"_blank\"\u003eTensor Processing Unit\u003c/a\u003e (TPU), Google’s custom AI accelerator. By suggesting modifications in the standard language of chip designers, AlphaEvolve promotes a collaborative approach between AI and hardware engineers to accelerate the design of future specialized chips.\u003c/p\u003e\n\u003c/div\u003e\n                \n              \n                \n                \n                  \n                  \u003cdiv\u003e\n  \u003ch3 data-block-key=\"1rhtp\"\u003eEnhancing AI training and inference\u003c/h3\u003e\u003cp data-block-key=\"279mo\"\u003eAlphaEvolve is accelerating AI performance and research velocity. By finding smarter ways to divide a large matrix multiplication operation into more manageable subproblems, it sped up this vital \u003ca href=\"https://docs.jax.dev/en/latest/pallas/index.html\" rel=\"noopener\" target=\"_blank\"\u003ekernel\u003c/a\u003e in Gemini’s architecture by 23%, leading to a 1% reduction in Gemini\u0026#39;s training time. Because developing generative AI models requires substantial computing resources, every efficiency gained translates to considerable savings. Beyond performance gains, AlphaEvolve significantly reduces the engineering time required for kernel optimization, from weeks of expert effort to days of automated experiments, allowing researchers to innovate faster.\u003c/p\u003e\u003cp data-block-key=\"fvg3r\"\u003eAlphaEvolve can also optimize low level GPU instructions. This incredibly complex domain is usually already heavily optimized by compilers, so human engineers typically don\u0026#39;t modify it directly. AlphaEvolve achieved up to a 32.5% speedup for the\u003ca href=\"https://arxiv.org/abs/2205.14135\" rel=\"noopener\" target=\"_blank\"\u003e FlashAttention\u003c/a\u003e kernel implementation in\u003ca href=\"https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29\" rel=\"noopener\" target=\"_blank\"\u003e Transformer\u003c/a\u003e-based AI models. This kind of optimization helps experts pinpoint performance bottlenecks and easily incorporate the improvements into their codebase, boosting their productivity and enabling future savings in compute and energy.\u003c/p\u003e\n\u003c/div\u003e\n                \n              \n                \n                \n                  \n                  \u003cdiv\u003e\n  \u003ch3 data-block-key=\"1rhtp\"\u003eAdvancing the frontiers in mathematics and algorithm discovery\u003c/h3\u003e\u003cp data-block-key=\"8i4l\"\u003eAlphaEvolve can also propose new approaches to complex mathematical problems. Provided with a minimal code skeleton for a computer program, AlphaEvolve designed many components of a novel \u003ca href=\"https://www.sciencedirect.com/topics/engineering/gradient-based-algorithm\" rel=\"noopener\" target=\"_blank\"\u003egradient-based optimization\u003c/a\u003e procedure that discovered multiple new algorithms for matrix multiplication, a fundamental problem in computer science.\u003c/p\u003e\n\u003c/div\u003e\n                \n              \n                \n                \n                  \n                  \n\n\n\n\n\n\u003cfigure aria-labelledby=\"caption-fb8c0bba-9a53-4d4e-918f-e2d2a3122595\"\u003e\n  \n\n  \u003cfigcaption\u003e\n      \u003cp data-block-key=\"pp1h1\"\u003eA list of changes proposed by AlphaEvolve to discover faster matrix multiplication algorithms. In this example, AlphaEvolve proposes extensive changes across several components, including the optimizer and weight initialization, the loss function, and hyperparameter sweep. These changes are highly non-trivial, requiring 15 mutations during the evolutionary process.\u003c/p\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e\n                \n              \n                \n                \n                  \n                  \u003cdiv\u003e\n  \u003cp data-block-key=\"1rhtp\"\u003eAlphaEvolve’s procedure found an algorithm to multiply 4x4 complex-valued matrices using 48 scalar multiplications, improving upon \u003ca href=\"https://en.wikipedia.org/wiki/Strassen_algorithm\" rel=\"noopener\" target=\"_blank\"\u003eStrassen’s 1969 algorithm\u003c/a\u003e that was previously known as the best in this setting. This finding demonstrates a significant advance over our previous work, \u003ca href=\"https://deepmind.google/discover/blog/discovering-novel-algorithms-with-alphatensor/\" rel=\"noopener\" target=\"_blank\"\u003eAlphaTensor\u003c/a\u003e, which specialized in matrix multiplication algorithms, and for 4x4 matrices, only found improvements for binary arithmetic.\u003c/p\u003e\u003cp data-block-key=\"fmlon\"\u003eTo investigate AlphaEvolve’s breadth, we applied the system to over 50 open problems in mathematical analysis, geometry, combinatorics and number theory. The system’s flexibility enabled us to set up most experiments in a matter of hours. In roughly 75% of cases, it rediscovered state-of-the-art solutions, to the best of our knowledge.\u003c/p\u003e\u003cp data-block-key=\"86rjt\"\u003eAnd in 20% of cases, AlphaEvolve improved the previously best known solutions, making progress on the corresponding open problems. For example, it advanced the \u003ca href=\"https://en.wikipedia.org/wiki/Kissing_number\" rel=\"noopener\" target=\"_blank\"\u003ekissing number problem\u003c/a\u003e. This geometric challenge has \u003ca href=\"https://plus.maths.org/content/newton-and-kissing-problem\" rel=\"noopener\" target=\"_blank\"\u003efascinated mathematicians for over 300 years\u003c/a\u003e and concerns the maximum number of non-overlapping spheres that touch a common unit sphere. AlphaEvolve discovered a configuration of 593 outer spheres and established a new lower bound in 11 dimensions.\u003c/p\u003e\n\u003c/div\u003e\n                \n              \n                \n                \n                  \n                  \u003cdiv\u003e\n  \u003ch3 data-block-key=\"1rhtp\"\u003eThe path forward\u003c/h3\u003e\u003cp data-block-key=\"9e94g\"\u003eAlphaEvolve displays the progression from discovering algorithms for specific domains to developing more complex algorithms for a wide range of real-world challenges. We’re expecting AlphaEvolve to continue improving alongside the capabilities of large language models, especially as they become even \u003ca href=\"https://developers.googleblog.com/en/gemini-2-5-pro-io-improved-coding-performance/\" rel=\"noopener\" target=\"_blank\"\u003ebetter at coding\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"7kobf\"\u003eTogether with the \u003ca href=\"https://pair.withgoogle.com/\" rel=\"noopener\" target=\"_blank\"\u003ePeople + AI Research team\u003c/a\u003e, we’ve been building a friendly user interface for interacting with AlphaEvolve. We’re planning an Early Access Program for selected academic users and also exploring possibilities to make AlphaEvolve more broadly available. To register your interest, please complete \u003ca href=\"https://forms.gle/WyqAoh1ixdfq6tgN8\" rel=\"noopener\" target=\"_blank\"\u003ethis form\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"cefdg\"\u003eWhile AlphaEvolve is currently being applied across math and computing, its general nature means it can be applied to any problem whose solution can be described as an algorithm, and automatically verified. We believe AlphaEvolve could be transformative across many more areas such as material science, drug discovery, sustainability and wider technological and business applications.\u003c/p\u003e\n\u003c/div\u003e\n                \n              \n                \n                \n                  \n                  \n\n\u003csection\u003e\n  \n\n  \u003cul\u003e\n    \n      \u003cli\u003e\n            \u003cgemini-button data-in-view=\"\"\u003e\n              \u003ca data-gtm-tag=\"cta-selection\" href=\"https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf\" rel=\"noopener\" target=\"_blank\"\u003e\n      \u003cspan\u003eRead more details in our white paper\u003c/span\u003e\n      \n    \u003c/a\u003e\n            \u003c/gemini-button\u003e\n        \u003c/li\u003e\n        \n    \n      \u003cli\u003e\n            \u003cgemini-button data-in-view=\"\"\u003e\n              \u003ca data-gtm-tag=\"cta-selection\" href=\"https://forms.gle/WyqAoh1ixdfq6tgN8\" rel=\"noopener\" target=\"_blank\"\u003e\n      \u003cspan\u003eRegister your interest in using AlphaEvolve\u003c/span\u003e\n      \n    \u003c/a\u003e\n            \u003c/gemini-button\u003e\n        \u003c/li\u003e\n        \n    \n      \u003cli\u003e\n            \u003cgemini-button data-in-view=\"\"\u003e\n              \u003ca data-gtm-tag=\"cta-selection\" href=\"https://colab.research.google.com/github/google-deepmind/alphaevolve_results/blob/master/mathematical_results.ipynb\" rel=\"noopener\" target=\"_blank\"\u003e\n      \u003cspan\u003eSee AlphaEvolve’s mathematical results in our Google Colab\u003c/span\u003e\n      \n    \u003c/a\u003e\n            \u003c/gemini-button\u003e\n        \u003c/li\u003e\n        \n    \n  \u003c/ul\u003e\n\u003c/section\u003e\n                \n              \n                \n                \n                  \n                  \u003cdiv\u003e\n      \u003cp data-block-key=\"ndxib\"\u003e\u003cstrong\u003eAcknowledgements\u003c/strong\u003e\u003c/p\u003e\u003cp data-block-key=\"d0au1\"\u003eAlphaEvolve was developed by Matej Balog, Alexander Novikov, Ngân Vũ, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, and Pushmeet Kohli. This research was developed as part of our effort focused on using AI for algorithm discovery.\u003c/p\u003e\u003cp data-block-key=\"frcj6\"\u003eWe gratefully acknowledge contributions, advice, and support from Jean-Baptiste Alayrac, Ankit Anand, Natasha Antropova, Giorgio Arena, Mohammadamin Barekatain, Johannes Bausch, Henning Becker, Daniel Belov, Alexander Belyaev, Sebastian Bodenstein, Sebastian Borgeaud, Calin Cascaval, Indranil Chakraborty, Benjamin Chetioui, Justin Chiu, Christopher Clark, Marco Cornero, Jeff Dean, Gaurav Dhiman, Yanislav Donchev, Srikanth Dwarakanath, Jordan Ellenberg, Alhussein Fawzi, Michael Figurnov, Aaron Gentleman, Bogdan Georgiev, Sergio Guadarrama, Demis Hassabis, Patrick Heisel, Chase Hensel, Koray Kavukcuoglu, Sultan Kenjeyev, Aliia Khasanova, Sridhar Lakshmanamurthy, Sergei Lebedev, Dmitry Lepikhin, Daniel Mankowitz, Andrea Michi, Kieran Milan, Vinod Nair, Robert O\u0026#39;Callahan, Cosmin Paduraru, Stig Petersen, Federico Piccinini, Parthasarathy Ranganatha, Bernardino Romera-Paredes, Georges Rotival, Kirk Sanders, Javier Gomez Serrano, Oleg Shyshkov, Timur Sitdikov, Tammo Spalink, Kerry Takenaka, Richard Tanburn, Terence Tao, Amin Vahdat, JD Velasquez, Dimitrios Vytiniotis, Julian Walker, and Pengming Wang. For more details, please see our white paper.\u003cbr/\u003e\u003c/p\u003e\u003cp data-block-key=\"btj9l\"\u003eWe would like to thank Armin Senoner, Juanita Bawagan, Jane Park, Arielle Bier, and Molly Beck for feedback on the blog post and help with this announcement; William Hood, Irina Andronic, Victoria Johnston, Lucas Dixon, Adam Connors, and Jimbo Wilson for help with the illustrations and figures\u003c/p\u003e\n    \u003c/div\u003e\n                \n              \n            \u003c/div\u003e\n          \n        \n      \n\n      \n    \n  \n  \n\n  \n\n  \u003c/article\u003e\n\n    \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": null,
  "modifiedTime": null
}
