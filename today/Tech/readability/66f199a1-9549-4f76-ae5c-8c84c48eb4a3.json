{
  "id": "66f199a1-9549-4f76-ae5c-8c84c48eb4a3",
  "title": "OpenAI adds MCP support to Agents SDK",
  "link": "https://openai.github.io/openai-agents-python/mcp/",
  "description": "Comments",
  "author": "",
  "published": "Wed, 26 Mar 2025 18:55:29 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 2250,
  "excerpt": "The Model context protocol (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:",
  "siteName": "",
  "favicon": "",
  "text": "The Model context protocol (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs: MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools. The Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools to your Agents. MCP servers Currently, the MCP spec defines two kinds of servers, based on the transport mechanism they use: stdio servers run as a subprocess of your application. You can think of them as running \"locally\". HTTP over SSE servers run remotely. You connect to them via a URL. You can use the MCPServerStdio and MCPServerSse classes to connect to these servers. For example, this is how you'd use the official MCP filesystem server. async with MCPServerStdio( params={ \"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir], } ) as server: tools = await server.list_tools() Using MCP servers MCP servers can be added to Agents. The Agents SDK will call list_tools() on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server's tools. When the LLM calls a tool from an MCP server, the SDK calls call_tool() on that server. agent=Agent( name=\"Assistant\", instructions=\"Use the tools to achieve the task\", mcp_servers=[mcp_server_1, mcp_server_2] ) Caching Every time an Agent runs, it calls list_tools() on the MCP server. This can be a latency hit, especially if the server is a remote server. To automatically cache the list of tools, you can pass cache_tools_list=True to both MCPServerStdio and MCPServerSse. You should only do this if you're certain the tool list will not change. If you want to invalidate the cache, you can call invalidate_tools_cache() on the servers. End-to-end examples View complete working examples at examples/mcp. Tracing Tracing automatically captures MCP operations, including: Calls to the MCP server to list tools MCP-related info on function calls",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-md-component=\"container\"\u003e\n      \n      \n        \n          \n        \n      \n      \u003cmain data-md-component=\"main\"\u003e\n        \u003cdiv data-md-component=\"content\"\u003e\n              \u003carticle\u003e\n                \n                  \n\n\n\n\n\u003cp\u003eThe \u003ca href=\"https://modelcontextprotocol.io/introduction\"\u003eModel context protocol\u003c/a\u003e (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools to your Agents.\u003c/p\u003e\n\u003ch2 id=\"mcp-servers\"\u003eMCP servers\u003c/h2\u003e\n\u003cp\u003eCurrently, the MCP spec defines two kinds of servers, based on the transport mechanism they use:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003estdio\u003c/strong\u003e servers run as a subprocess of your application. You can think of them as running \u0026#34;locally\u0026#34;.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHTTP over SSE\u003c/strong\u003e servers run remotely. You connect to them via a URL.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou can use the \u003ca title=\"MCPServerStdio\" href=\"https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerStdio\"\u003e\u003ccode\u003eMCPServerStdio\u003c/code\u003e\u003c/a\u003e and \u003ca title=\"MCPServerSse\" href=\"https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerSse\"\u003e\u003ccode\u003eMCPServerSse\u003c/code\u003e\u003c/a\u003e classes to connect to these servers.\u003c/p\u003e\n\u003cp\u003eFor example, this is how you\u0026#39;d use the \u003ca href=\"https://www.npmjs.com/package/@modelcontextprotocol/server-filesystem\"\u003eofficial MCP filesystem server\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003ccode\u003e\u003cspan id=\"__span-0-1\"\u003e\u003ca id=\"__codelineno-0-1\" name=\"__codelineno-0-1\" href=\"#__codelineno-0-1\"\u003e\u003c/a\u003e\u003cspan\u003easync\u003c/span\u003e \u003cspan\u003ewith\u003c/span\u003e \u003cspan\u003eMCPServerStdio\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003cspan id=\"__span-0-2\"\u003e\u003ca id=\"__codelineno-0-2\" name=\"__codelineno-0-2\" href=\"#__codelineno-0-2\"\u003e\u003c/a\u003e    \u003cspan\u003eparams\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003cspan id=\"__span-0-3\"\u003e\u003ca id=\"__codelineno-0-3\" name=\"__codelineno-0-3\" href=\"#__codelineno-0-3\"\u003e\u003c/a\u003e        \u003cspan\u003e\u0026#34;command\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#34;npx\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003cspan id=\"__span-0-4\"\u003e\u003ca id=\"__codelineno-0-4\" name=\"__codelineno-0-4\" href=\"#__codelineno-0-4\"\u003e\u003c/a\u003e        \u003cspan\u003e\u0026#34;args\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#34;-y\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#34;@modelcontextprotocol/server-filesystem\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003esamples_dir\u003c/span\u003e\u003cspan\u003e],\u003c/span\u003e\n\u003c/span\u003e\u003cspan id=\"__span-0-5\"\u003e\u003ca id=\"__codelineno-0-5\" name=\"__codelineno-0-5\" href=\"#__codelineno-0-5\"\u003e\u003c/a\u003e    \u003cspan\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003cspan id=\"__span-0-6\"\u003e\u003ca id=\"__codelineno-0-6\" name=\"__codelineno-0-6\" href=\"#__codelineno-0-6\"\u003e\u003c/a\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003eas\u003c/span\u003e \u003cspan\u003eserver\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003cspan id=\"__span-0-7\"\u003e\u003ca id=\"__codelineno-0-7\" name=\"__codelineno-0-7\" href=\"#__codelineno-0-7\"\u003e\u003c/a\u003e    \u003cspan\u003etools\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eawait\u003c/span\u003e \u003cspan\u003eserver\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elist_tools\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2 id=\"using-mcp-servers\"\u003eUsing MCP servers\u003c/h2\u003e\n\u003cp\u003eMCP servers can be added to Agents. The Agents SDK will call \u003ccode\u003elist_tools()\u003c/code\u003e on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server\u0026#39;s tools. When the LLM calls a tool from an MCP server, the SDK calls \u003ccode\u003ecall_tool()\u003c/code\u003e on that server.\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003ccode\u003e\u003cspan id=\"__span-1-1\"\u003e\u003ca id=\"__codelineno-1-1\" name=\"__codelineno-1-1\" href=\"#__codelineno-1-1\"\u003e\u003c/a\u003e\u003cspan\u003eagent\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eAgent\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003cspan id=\"__span-1-2\"\u003e\u003ca id=\"__codelineno-1-2\" name=\"__codelineno-1-2\" href=\"#__codelineno-1-2\"\u003e\u003c/a\u003e    \u003cspan\u003ename\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;Assistant\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003cspan id=\"__span-1-3\"\u003e\u003ca id=\"__codelineno-1-3\" name=\"__codelineno-1-3\" href=\"#__codelineno-1-3\"\u003e\u003c/a\u003e    \u003cspan\u003einstructions\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;Use the tools to achieve the task\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003cspan id=\"__span-1-4\"\u003e\u003ca id=\"__codelineno-1-4\" name=\"__codelineno-1-4\" href=\"#__codelineno-1-4\"\u003e\u003c/a\u003e    \u003cspan\u003emcp_servers\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003emcp_server_1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003emcp_server_2\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003cspan id=\"__span-1-5\"\u003e\u003ca id=\"__codelineno-1-5\" name=\"__codelineno-1-5\" href=\"#__codelineno-1-5\"\u003e\u003c/a\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2 id=\"caching\"\u003eCaching\u003c/h2\u003e\n\u003cp\u003eEvery time an Agent runs, it calls \u003ccode\u003elist_tools()\u003c/code\u003e on the MCP server. This can be a latency hit, especially if the server is a remote server. To automatically cache the list of tools, you can pass \u003ccode\u003ecache_tools_list=True\u003c/code\u003e to both \u003ca title=\"MCPServerStdio\" href=\"https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerStdio\"\u003e\u003ccode\u003eMCPServerStdio\u003c/code\u003e\u003c/a\u003e and \u003ca title=\"MCPServerSse\" href=\"https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerSse\"\u003e\u003ccode\u003eMCPServerSse\u003c/code\u003e\u003c/a\u003e. You should only do this if you\u0026#39;re certain the tool list will not change.\u003c/p\u003e\n\u003cp\u003eIf you want to invalidate the cache, you can call \u003ccode\u003einvalidate_tools_cache()\u003c/code\u003e on the servers.\u003c/p\u003e\n\u003ch2 id=\"end-to-end-examples\"\u003eEnd-to-end examples\u003c/h2\u003e\n\u003cp\u003eView complete working examples at \u003ca href=\"https://github.com/openai/openai-agents-python/tree/main/examples/mcp\"\u003eexamples/mcp\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"tracing\"\u003eTracing\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://openai.github.io/openai-agents-python/tracing/\"\u003eTracing\u003c/a\u003e automatically captures MCP operations, including:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCalls to the MCP server to list tools\u003c/li\u003e\n\u003cli\u003eMCP-related info on function calls\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"MCP Tracing Screenshot\" src=\"https://openai.github.io/openai-agents-python/assets/images/mcp-tracing.jpg\"/\u003e\u003c/p\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n                \n              \u003c/article\u003e\n            \u003c/div\u003e\n        \n      \u003c/main\u003e\n      \n        \n      \n    \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": null,
  "modifiedTime": null
}
