{
  "id": "3c565a43-fdef-4aa2-a8d6-4d194552422a",
  "title": "P-Hacking in Startups",
  "link": "https://briefer.cloud/blog/posts/p-hacking/",
  "description": "Comments",
  "author": "",
  "published": "Wed, 18 Jun 2025 09:56:28 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 8009,
  "excerpt": "When agile experimentation at startups becomes a p-hacking trap",
  "siteName": "Briefer",
  "favicon": "https://briefer.cloud/favicons/apple-touch-icon.png",
  "text": "When agile experimentation at startups becomes a p-hacking trapSpeed kills rigor. In startups, the pressure to ship fast pushes teams to report anything that looks like an improvement. That’s how p-hacking happens. This piece breaks down three common cases—and how to avoid them. Example 01: Multiple comparisons without correction Imagine you're a product manager trying to optimize your website’s dashboard. Your goal is to increase user signups. Your team designs four different layouts: A, B, C, and D. You run an A/B/n test. Users are randomly assigned to one of the four layouts and you track their activity. Your hypothesis is: layout influences signup behavior. You plan ship the winner if the p-value for one of the layout choices falls below the conventional threshold of 0.05. Then you check the results: Option B looks best. p = 0.041. It floats to the top as if inviting action. The team is satisfied and ships it. But the logic beneath the 0.05 cutoff is more fragile than it appears. That threshold assumes you’re testing a single variant. But you tested four. That alone increases the odds of a false positive. Let’s look at what that actually means. Setting a p-value threshold of 0.05 is equivalent to saying: \"I’m willing to accept a 5% chance of shipping something that only looked good by chance.\" So the probability that one test doesn’t result in a false positive is: 1−0.05=0.951−0.05=0.95 Now, if you run 4 independent tests, the probability that none of them produce a false positive is: 0.95×0.95×0.95×0.95=0.81450.95 \\times 0.95 \\times 0.95 \\times 0.95 = 0.8145 That means the probability that at least one test gives you a false positive is: 1−0.8145=0.18551 − 0.8145 = 0.1855 So instead of working with a 5% false positive rate, you’re actually closer to 18.5%: nearly a 1 in 5 risk that you're shipping something based on a fluke. And that risk scales quickly. The more variants you test, the higher the odds that something looks like a win just by coincidence. Statistically, the probability of at least one false positive increases with each additional test, converging toward 1 as the number of comparisons grows: Bottom line: you ran a four-arm experiment but interpreted it like a one-arm test. You never adjusted your cutoff to account for multiple comparisons. Which means the p-value you relied on doesn’t mean what you think it does. This is p-hacking. You looked at the results, picked the one that cleared the bar, and ignored the fact that the bar was never calibrated for this setup. How to avoid this: adjusting the threshold The Bonferroni Correction is one way to avoid using the wrong cutoff when testing for multiple options. It's straightforward: you account for the number of hypotheses k by adjusting the acceptable p-value for significance: adjusted threshold=0.05k \\text{adjusted threshold}= \\frac{0.05}{k} In our dashboard test with 4 variants, that’s: 0.054=0.0125\\frac{0.05}{4} = 0.0125 Under this correction, only p-values below 0.0125 should be considered significant. Your p = 0.041 result? It no longer qualifies. Fewer results will pass the bar. That can feel frustrating in fast-moving product teams. But now you're actually measuring something you can trust. Example 02: Reframing the metric after the results are in Back to the dashboard experiment: after you applied the Bonferroni correction you got... nothing. None of your dashboard variants significantly improved user signup rates. This is frustrating. You've invested weeks in the redesign, and you're facing a product review with no wins to show. Nobody likes arriving empty-handed to leadership meetings. So you dig deeper. The data's already collected so why not explore other insights? Maybe signups didn't improve, but what about retention? You check for retention rates and discover something interesting: Option B shows slightly higher retention than the rest, with p = 0.034. Suddenly your narrative shifts: \"This experiment was really about improving retention all along!\" You pivot the story and now it’s tempting to call it a win for B and ship it. But each extra metric you check is another chance for randomness to sneak in. If you check 20 metrics, the odds that at least one will look like a winner by pure chance shoot up to about two in three. That’s because the probability that none of the 20 metrics show a false positive is: 1−(1−0.05)9=641−(1−0.05)^9 =64% Graphically, it looks like this: That promising retention improvement? It’s just the kind of anomaly you’d expect to find after enough digging. The pre-registration solution Each time you add a new metric, you increase the chance of finding a false positive. A single test with a threshold of p \u003c 0.05 implies a 5 percent risk of error. But the more tests you run, the more that risk accumulates. In the limit, it approaches certainty. Pre-registration prevents this. By stating in advance which metric will count as evidence, you fix the false positive rate at its intended level. The p-value retains its meaning. You are testing one hypothesis, not several in disguise. Decide your success metrics before running the test. Document them explicitly and stick to them. This isn't academic nit-picking. It's how medical research works when lives are on the line. Your startup's growth deserves the same rigor. Example 03: Running experiments until we get a hit Even if you’ve accounted for multiple variants and didn't the temptation to shift metrics, one bias remains: impatience. The lure of early results is difficult to ignore and can lead to bad decisions Now you're running an A/B test of two button styles, scheduled for two weeks. Each day, you check the dashboard, just in case. On the ninth day, the p-value for button B dips to 0.048: Should you stop the test and ship B? At this point, you know a win shouldn’t come that easily. A p-value only works if you set your stopping rule in advance. Peeking the p-value each day during nine days is like you’re running nine experiments. Each day is a new opportunity for randomness to look like signal. After 9 peeks, the probability that at least one p-value dips below 0.05 is: 1−(1−0.05)9=371−(1−0.05)^9 =37% And there’s another subtle trap: by not waiting for the experiment to finish, you’re watching the p-value bounce around as new data arrives. That \"significant\" result on day 9 might be nothing more than a lucky swing, gone by day 14: Shipping on an early p-value is like betting on a horse halfway around the track. How to properly peek If you absolutely must make early stopping decisions, here’s how to do it responsibly using sequential testing. Let’s go back to our button test. You planned to collect data for 2 weeks. Instead of using a flat p\u003c0.05p\u003c0.05 the whole time, sequential testing adjusts the threshold depending on when you stop: Week 1: Only stop if p \u003c 0.01 (super strict) Day 10: Only stop if p \u003c 0.025 (still strict) Day 14: Normal p \u003c 0.05 threshold This approach controls the overall false positive rate, even if you peek multiple times. Remember our day 9 result where p = 0.048? Under sequential testing, that wouldn't qualify. You'd need p \u003c 0.01 to stop in week 1. So you'd keep running the test and probably discover it wasn't actually significant: It works like \"spending\" your false positive budget gradually instead of all at once. So yes, you can peek with discipline. But for most teams, the simpler and safer move is still the right one: wait the damn two weeks. In summary Your next experiment will be more reliable if you: Pre-register hypotheses and metrics Avoid digging through metrics post hoc Use corrections when testing multiple variants Apply proper thresholds if you peek early Celebrate definitive negative results (might be controversial) The irony is that better statistical practices actually accelerate learning. Instead of shipping noise and wondering why your metrics plateau, you'll build genuine understanding of what drives user behavior. That's worth slowing down for.",
  "image": "https://briefer.cloud/og-large.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eWhen agile experimentation at startups becomes a p-hacking trap\u003c/p\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg src=\"https://briefer.cloud/authors/thais-steinmuller.jpg\" alt=\"Thaís Steinmuller\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle\u003e\u003cp\u003eSpeed kills rigor. In startups, the pressure to ship fast pushes teams to report \u003cem\u003eanything\u003c/em\u003e that looks like an improvement. That’s how p-hacking happens. This piece breaks down three common cases—and how to avoid them.\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch2\u003eExample 01: Multiple comparisons without correction\u003c/h2\u003e\n\u003cp\u003eImagine you\u0026#39;re a product manager trying to optimize your website’s dashboard. Your goal is to increase user signups. Your team designs four different layouts: A, B, C, and D.\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/ab.png\" width=\"500\" height=\"300\"/\u003e\n\u003cp\u003eYou run an A/B/n test. Users are randomly assigned to one of the four layouts and you track their activity. Your hypothesis is: \u003cem\u003elayout influences signup behavior\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eYou plan ship the winner if the p-value for one of the layout choices falls below the conventional threshold of 0.05.\u003c/p\u003e\n\u003cp\u003eThen you check the results:\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/mislead.png\" width=\"400\" height=\"300\"/\u003e\n\u003cp\u003eOption B looks best. p = 0.041. It floats to the top as if inviting action. The team is satisfied and ships it.\u003c/p\u003e\n\u003cp\u003eBut the logic beneath the 0.05 cutoff is more fragile than it appears. That threshold assumes you’re testing a single variant. But you tested four. That alone \u003cstrong\u003eincreases\u003c/strong\u003e the odds of a false positive.\u003c/p\u003e\n\u003cp\u003eLet’s look at what that actually means.\u003c/p\u003e\n\u003cp\u003eSetting a p-value threshold of 0.05 is equivalent to saying: \u003cem\u003e\u0026#34;I’m willing to accept a 5% chance of shipping something that only looked good by chance.\u0026#34;\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eSo the probability that one test \u003cstrong\u003edoesn’t\u003c/strong\u003e result in a false positive is:\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e0.05\u003c/mn\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e0.95\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e1−0.05=0.95\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eNow, if you run 4 independent tests, the probability that none of them produce a false positive is:\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e0.95\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e0.95\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e0.95\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e0.95\u003c/mn\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e0.8145\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e0.95 \\times 0.95 \\times 0.95 \\times 0.95 = 0.8145\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eThat means the probability that \u003cstrong\u003eat least one test\u003c/strong\u003e gives you a false positive is:\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e0.8145\u003c/mn\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e0.1855\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e1 − 0.8145 = 0.1855\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eSo instead of working with a 5% false positive rate, you’re actually closer to 18.5%: nearly a 1 in 5 risk that you\u0026#39;re shipping something based on a fluke.\u003c/p\u003e\n\u003cp\u003eAnd that risk scales quickly. The more variants you test, the higher the odds that something looks like a win just by coincidence. Statistically, the probability of at least one false positive increases with each additional test, converging toward 1 as the number of comparisons grows:\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/numberoftests.png\" width=\"700\" height=\"300\"/\u003e\n\u003cp\u003eBottom line: you ran a four-arm experiment but interpreted it like a one-arm test. You never adjusted your cutoff to account for multiple comparisons. Which means the p-value you relied on doesn’t mean what you think it does.\u003c/p\u003e\n\u003cp\u003eThis is p-hacking. You looked at the results, picked the one that cleared the bar, and ignored the fact that the bar was never calibrated for this setup.\u003c/p\u003e\n\u003ch2\u003eHow to avoid this: adjusting the threshold\u003c/h2\u003e\n\u003cp\u003eThe Bonferroni Correction is one way to avoid using the wrong cutoff when testing for multiple options. It\u0026#39;s straightforward: you account for the number of hypotheses \u003cem\u003ek\u003c/em\u003e by adjusting the acceptable p-value for significance:\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmtext\u003eadjusted threshold\u003c/mtext\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e0.05\u003c/mn\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e \\text{adjusted threshold}= \\frac{0.05}{k}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eIn our dashboard test with 4 variants, that’s:\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmfrac\u003e\u003cmn\u003e0.05\u003c/mn\u003e\u003cmn\u003e4\u003c/mn\u003e\u003c/mfrac\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e0.0125\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\frac{0.05}{4} = 0.0125\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eUnder this correction, only p-values below 0.0125 should be considered significant. Your p = 0.041 result? It no longer qualifies.\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/bonferoni.png\" width=\"500\" height=\"300\"/\u003e\n\u003cp\u003eFewer results will pass the bar. That can feel frustrating in fast-moving product teams. But now you\u0026#39;re actually measuring something you can trust.\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch2\u003eExample 02: Reframing the metric after the results are in\u003c/h2\u003e\n\u003cp\u003eBack to the dashboard experiment: after you applied the Bonferroni correction you got... nothing. None of your dashboard variants significantly improved user signup rates.\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/squareone.png\" width=\"500\" height=\"300\"/\u003e\n\u003cp\u003eThis is frustrating. You\u0026#39;ve invested weeks in the redesign, and you\u0026#39;re facing a product review with no wins to show. Nobody likes arriving empty-handed to leadership meetings.\u003c/p\u003e\n\u003cp\u003eSo you dig deeper. The data\u0026#39;s already collected so why not explore other insights? Maybe signups didn\u0026#39;t improve, but what about \u003cstrong\u003eretention\u003c/strong\u003e? You check for retention rates and discover something interesting:\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/retention.png\" width=\"400\" height=\"300\"/\u003e\n\u003cp\u003eOption B shows slightly higher retention than the rest, with p = 0.034. Suddenly your narrative shifts: \u0026#34;This experiment was really about improving retention all along!\u0026#34;\u003c/p\u003e\n\u003cp\u003eYou pivot the story and now it’s tempting to call it a win for B and ship it. But each extra metric you check is another chance for randomness to sneak in. If you check 20 metrics, the odds that at least one will look like a winner by pure chance shoot up to about two in three. That’s because the probability that none of the 20 metrics show a false positive is:\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e0.05\u003c/mn\u003e\u003cmsup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmn\u003e9\u003c/mn\u003e\u003c/msup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e64\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e1−(1−0.05)^9 =64%\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eGraphically, it looks like this:\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/metrics.png\" width=\"700\" height=\"300\"/\u003e\n\u003cp\u003eThat promising retention improvement? It’s just the kind of anomaly you’d expect to find after enough digging.\u003c/p\u003e\n\u003ch2\u003eThe pre-registration solution\u003c/h2\u003e\n\u003cp\u003eEach time you add a new metric, you increase the chance of finding a false positive. A single test with a threshold of p \u0026lt; 0.05 implies a 5 percent risk of error. But the more tests you run, the more that risk accumulates. In the limit, it approaches certainty.\u003c/p\u003e\n\u003cp\u003ePre-registration prevents this. By stating in advance which metric will count as evidence, you fix the false positive rate at its intended level. The p-value retains its meaning. You are testing one hypothesis, not several in disguise.\u003c/p\u003e\n\u003cp\u003eDecide your success metrics before running the test. Document them explicitly and stick to them.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026#39;t academic nit-picking. It\u0026#39;s how medical research works when lives are on the line. Your startup\u0026#39;s growth deserves the same rigor.\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch2\u003eExample 03: Running experiments until we get a hit\u003c/h2\u003e\n\u003cp\u003eEven if you’ve accounted for multiple variants and didn\u0026#39;t the temptation to shift metrics, one bias remains: impatience. The lure of early results is difficult to ignore and can lead to bad decisions\u003c/p\u003e\n\u003cp\u003eNow you\u0026#39;re running an A/B test of two button styles, scheduled for two weeks.\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/ab2.png\" width=\"500\" height=\"300\"/\u003e\n\u003cp\u003eEach day, you  check the dashboard, just in case. On the ninth day, the p-value for button B dips to 0.048:\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/9days.png\" width=\"700\" height=\"300\"/\u003e\n\u003cp\u003eShould you stop the test and ship B? At this point, you know a win shouldn’t come that easily. A p-value only works if you set your stopping rule in advance. Peeking the p-value each day during nine days is like you’re running nine experiments. Each day is a new opportunity for randomness to look like signal.\u003c/p\u003e\n\u003cp\u003eAfter 9 peeks, the probability that at least one p-value dips below 0.05 is:\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e0.05\u003c/mn\u003e\u003cmsup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmn\u003e9\u003c/mn\u003e\u003c/msup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e37\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e1−(1−0.05)^9 =37%\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/peek.png\" width=\"700\" height=\"300\"/\u003e\n\u003cp\u003eAnd there’s another subtle trap: by not waiting for the experiment to finish, you’re watching the p-value bounce around as new data arrives. That \u0026#34;significant\u0026#34; result on day 9 might be nothing more than a lucky swing, gone by day 14:\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/15days.png\" width=\"700\" height=\"300\"/\u003e\n\u003cp\u003eShipping on an early p-value is like betting on a horse halfway around the track.\u003c/p\u003e\n\u003ch2\u003eHow to properly peek\u003c/h2\u003e\n\u003cp\u003eIf you absolutely must make early stopping decisions, here’s how to do it responsibly using \u003cstrong\u003esequential testing\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eLet’s go back to our button test. You planned to collect data for 2 weeks. Instead of using a flat \u003cspan\u003e\u003cspan\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo\u003e\u0026lt;\u003c/mo\u003e\u003cmn\u003e0.05\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ep\u0026lt;0.05\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/span\u003e the whole time, sequential testing adjusts the threshold depending on when you stop:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWeek 1: Only stop if p \u0026lt; 0.01 (super strict)\u003c/li\u003e\n\u003cli\u003eDay 10: Only stop if p \u0026lt; 0.025 (still strict)\u003c/li\u003e\n\u003cli\u003eDay 14: Normal p \u0026lt; 0.05 threshold\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis approach controls the overall false positive rate, even if you peek multiple times.\u003c/p\u003e\n\u003cp\u003eRemember our day 9 result where p = 0.048? Under sequential testing, that wouldn\u0026#39;t qualify. You\u0026#39;d need p \u0026lt; 0.01 to stop in week 1. So you\u0026#39;d keep running the test and probably discover it wasn\u0026#39;t actually significant:\u003c/p\u003e\n\u003cimg src=\"https://briefer.cloud/posts/p-hacking/alpha.png\" width=\"700\" height=\"300\"/\u003e\n\u003cp\u003eIt works like \u0026#34;spending\u0026#34; your false positive budget gradually instead of all at once.\u003c/p\u003e\n\u003cp\u003eSo yes, you can peek with discipline. But for most teams, the simpler and safer move is still the right one: wait the damn two weeks.\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch2\u003eIn summary\u003c/h2\u003e\n\u003cp\u003eYour next experiment will be more reliable if you:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePre-register hypotheses and metrics\u003c/li\u003e\n\u003cli\u003eAvoid digging through metrics post hoc\u003c/li\u003e\n\u003cli\u003eUse corrections when testing multiple variants\u003c/li\u003e\n\u003cli\u003eApply proper thresholds if you peek early\u003c/li\u003e\n\u003cli\u003eCelebrate definitive negative results (might be controversial)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe irony is that better statistical practices actually accelerate learning. Instead of shipping noise and wondering why your metrics plateau, you\u0026#39;ll build genuine understanding of what drives user behavior. That\u0026#39;s worth slowing down for.\u003c/p\u003e\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2025-05-26T00:00:00Z",
  "modifiedTime": null
}
