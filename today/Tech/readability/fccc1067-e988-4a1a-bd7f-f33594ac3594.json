{
  "id": "fccc1067-e988-4a1a-bd7f-f33594ac3594",
  "title": "Meta releases two Llama 4 AI models",
  "link": "https://www.theverge.com/news/644171/llama-4-released-ai-model-whatsapp-messenger-instagram-direct",
  "description": "Meta has announced Llama 4, its newest collection of AI models that now power the Meta AI assistant on the web and in WhatsApp, Messenger, and Instagram. The two new models, also available to download from Meta or Hugging Face, are Llama 4 Scout — a small model capable of “fitting in a single Nvidia […]",
  "author": "Wes Davis",
  "published": "2025-04-05T19:05:05-04:00",
  "source": "https://www.theverge.com/rss/index.xml",
  "categories": [
    "AI",
    "Meta",
    "News",
    "Tech"
  ],
  "byline": "Wes Davis",
  "length": 2248,
  "excerpt": "Llama AI models get an efficiency upgrade.",
  "siteName": "The Verge",
  "favicon": "https://www.theverge.com/static-assets/icons/android-chrome-512x512.png",
  "text": "Wes Davis is a weekend editor who covers the latest in tech and entertainment. He has written news, reviews, and more as a tech journalist since 2020.Meta has announced Llama 4, its newest collection of AI models that now power the Meta AI assistant on the web and in WhatsApp, Messenger, and Instagram. The two new models, also available to download from Meta or Hugging Face, are Llama 4 Scout — a small model capable of “fitting in a single Nvidia H100 GPU” — and Llama 4 Maverick, which is more akin to GPT-4o and Gemini 2.0 Flash. Meta says it’s still in the process of training Llama 4 Behemoth, which Meta CEO Mark Zuckerberg says is “the highest performing base model in the world.”According to Meta, Llama 4 Scout has a 10-million-token context window — the working memory of an AI model — and beats Google’s Gemma 3 and Gemini 2.0 Flash-Lite models, as well as the open-source Mistral 3.1, “across a broad range of widely reported benchmarks,” while still “fitting in a single Nvidia H100 GPU.” Meta makes similar claims about its larger Maverick model’s performance versus OpenAI’s GPT-4o and Google’s Gemini 2.0 Flash, and says its results are comparable to DeepSeek-V3 in coding and reasoning tasks using “less than half the active parameters.”Image: MetaMeanwhile, Llama 4 Behemoth has 288 billion active parameters with 2 trillion parameters in total. While it hasn’t been released yet, Meta says Behemoth can outperform its competitors (in this case GPT-4.5 and Claude Sonnet 3.7) “on several STEM benchmarks.”For Llama 4, Meta says it switched to a “mixture of experts” (MoE) architecture, an approach that conserves resources by using only the parts of a model that are needed for a given task. The company plans to discuss future plans for AI models and products at its LlamaCon conference, which is taking place on April 29th.As with its past models, Meta calls the Llama 4 collection “open-source,” although Llama has been criticized for its license restrictions. For instance, the Llama 4 license requires commercial entities with more than 700 million monthly active users to request permission from Meta before using its models, which the Open Source Initiative wrote in 2023 takes it “out of the category of ‘Open Source.’”",
  "image": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/STK043_VRG_Illo_N_Barclay_6_Meta.jpg?quality=90\u0026strip=all\u0026crop=0%2C10.732984293194%2C100%2C78.534031413613\u0026w=1200",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Wes Davis\" data-chromatic=\"ignore\" loading=\"lazy\" width=\"36\" height=\"36\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/196306/unnamed.0.jpg?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/196306/unnamed.0.jpg?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=96 2x\" src=\"https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/196306/unnamed.0.jpg?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=96\"/\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.theverge.com/authors/wes-davis\"\u003eWes Davis\u003c/a\u003e \u003cspan\u003eis a weekend editor who covers the latest in tech and entertainment. He has written news, reviews, and more as a tech journalist since 2020.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"zephr-anchor\"\u003e\u003cp\u003eMeta has \u003ca href=\"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\"\u003eannounced Llama 4\u003c/a\u003e, its newest collection of AI models that now power the Meta AI assistant \u003ca href=\"https://meta.ai/\"\u003eon the web\u003c/a\u003e and in WhatsApp, Messenger, and Instagram. The two new models, also available to download \u003ca href=\"https://www.llama.com/llama-downloads/\"\u003efrom Meta\u003c/a\u003e or \u003ca href=\"https://huggingface.co/meta-llama\"\u003eHugging Face\u003c/a\u003e, are Llama 4 Scout — a small model capable of “fitting in a single Nvidia H100 GPU” — and Llama 4 Maverick, which is more akin to GPT-4o and Gemini 2.0 Flash. Meta says it’s still in the process of training Llama 4 Behemoth, which Meta CEO \u003ca href=\"https://www.instagram.com/reel/DIE0TmPyORV/?utm_source=ig_web_copy_link\"\u003eMark Zuckerberg says\u003c/a\u003e is “the highest performing base model in the world.”\u003c/p\u003e\u003cp\u003eAccording to Meta, Llama 4 Scout has a 10-million-token context window — the working memory of an AI model — and beats Google’s Gemma 3 and \u003ca href=\"https://www.theverge.com/news/606530/gemini-ai-app-flash-thinking-2-0-update\"\u003eGemini 2.0 Flash-Lite\u003c/a\u003e models, as well as the open-source Mistral 3.1, “across a broad range of widely reported benchmarks,” while still “fitting in a single Nvidia H100 GPU.” Meta makes similar claims about its larger Maverick model’s performance versus \u003ca href=\"https://www.theverge.com/openai/635118/chatgpt-sora-ai-image-generation-chatgpt\"\u003eOpenAI’s GPT-4o\u003c/a\u003e and Google’s Gemini 2.0 Flash, and says its results are comparable to \u003ca href=\"https://www.theverge.com/ai-artificial-intelligence/598846/deepseek-big-tech-ai-industry-nvidia-impac\"\u003eDeepSeek-V3\u003c/a\u003e in coding and reasoning tasks using “less than half the active parameters.”\u003c/p\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e\u003ca href=\"https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0,0,100,100\" data-pswp-height=\"1080\" data-pswp-width=\"1920\" target=\"_blank\" rel=\"noreferrer\"\u003e\u003cimg alt=\"Visual comparison of model specs.\" data-chromatic=\"ignore\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" sizes=\"(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px\" srcset=\"https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=2400 2400w\" src=\"https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/Llama-4-1.png?quality=90\u0026amp;strip=all\u0026amp;crop=0%2C0%2C100%2C100\u0026amp;w=2400\"/\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003ccite\u003eImage: Meta\u003c/cite\u003e\u003c/p\u003e\u003c/div\u003e\u003cp\u003eMeanwhile, Llama 4 Behemoth has 288 billion active parameters with 2 trillion parameters in total. While it hasn’t been released yet, Meta says Behemoth can outperform its competitors (in this case GPT-4.5 and Claude Sonnet 3.7) “on several STEM benchmarks.”\u003c/p\u003e\u003cp\u003eFor Llama 4, Meta says it switched to a “mixture of experts” (MoE) architecture, an approach that conserves resources by using only the parts of a model that are needed for a given task. The company plans to discuss future plans for AI models and products at its LlamaCon conference, which is \u003ca href=\"https://www.theverge.com/news/614455/meta-llamacon-connect-2025-date-announcement\"\u003etaking place on April 29th\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAs with its past models, Meta calls the Llama 4 collection “open-source,” although Llama has been criticized for its license restrictions. For instance, the \u003ca href=\"https://www.llama.com/llama4/license/\"\u003eLlama 4 license\u003c/a\u003e requires commercial entities with more than 700 million monthly active users to request permission from Meta before using its models, which the Open Source Initiative \u003ca href=\"https://opensource.org/blog/metas-llama-2-license-is-not-open-source\"\u003ewrote in 2023\u003c/a\u003e takes it “out of the category of ‘Open Source.’”\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2025-04-05T23:05:05Z",
  "modifiedTime": "2025-04-05T23:05:05Z"
}
