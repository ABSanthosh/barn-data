{
  "id": "40d3244a-4d72-4288-8b91-55aaa4653773",
  "title": "Behind “ANCESTRA:” combining Veo with live-action filmmaking",
  "link": "https://blog.google/technology/google-deepmind/ancestra-behind-the-scenes/",
  "description": "We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 to make ANCESTRA.",
  "author": "Kory MathewsonSenior Research ScientistGoogle DeepMind",
  "published": "Fri, 13 Jun 2025 13:00:00 +0000",
  "source": "https://www.blog.google/rss/",
  "categories": [
    "Research",
    "Google DeepMind",
    "AI"
  ],
  "byline": "Kory Mathewson",
  "length": 9512,
  "excerpt": "We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 to make ANCESTRA.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png?version=pr20250611-1635",
  "text": "Jun 13, 2025 [[read-time]] min read We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking. General summary Google DeepMind partnered with Primordial Soup to produce \"ANCESTRA\" a short film premiering at the Tribeca Festival. The film combines live-action with video generated by Veo, Google's video generation model. Google DeepMind developed new Veo capabilities to enable personalization, precise motion matching, and blending of live-action and generative footage. Summaries were generated by Google AI. Generative AI is experimental. Bullet points \"ANCESTRA\" is a short film combining live-action with Veo, Google DeepMind's video generation model, premiering at the Tribeca Festival. Google DeepMind partnered with Primordial Soup to put generative AI in filmmakers' hands, pushing storytelling and tech boundaries. Gemini, Imagen, and Veo were used to generate shots based on mood, color, and emotion, using photos as inspiration. New Veo capabilities were developed for personalized video, precise motion matching, and blending live-action with generated footage. Generative AI complements filmmaking, empowering artists to overcome limitations and create difficult or expensive scenes. Summaries were generated by Google AI. Generative AI is experimental. Explore other styles: Today, Eliza McNitt’s short film, “ANCESTRA,” premieres at the Tribeca Festival. It’s the story of a mother, and what happens when her child is born with a hole in its heart. Inspired by the dramatic events of McNitt's own birth, the film portrays a mother's love as a cosmic, life-saving force.This is the first of three short films produced in partnership between our team at Google DeepMind and Primordial Soup, a new venture dedicated to storytelling innovation founded by director Darren Aronofsky. Together, we founded this partnership to put the world’s best generative AI into the hands of top filmmakers, to advance the frontiers of storytelling and technology.“ANCESTRA” combined live-action scenes with sequences generated by Veo, our state-of-the-art video generation model. McNitt described her experience working with our technology: \"Veo is another lens through which I get to imagine the universe around me.”To create “ANCESTRA”, Google DeepMind assembled a multidisciplinary creative team of animators, art directors, designers, writers, technologists and researchers who worked closely with more than 200 experts in traditional filmmaking and production, a live-action crew and cast, plus an editorial team, visual effects (VFX) artists, sound designers and music composers. Bringing our most advanced generative models to the screenWhile McNitt wrote the script for “ANCESTRA,” she worked with a storyboard artist to visualize the live-action scenes and collaborated with our team to generate imagery for sequences that could benefit from AI generation.We used Gemini to develop our prompts, and used Veo and our image generation model, Imagen, to create a series of potential shots, organized by mood, color and emotion. Here’s a breakdown of how we planned and created the AI elements of the film:Gemini: Our team uploaded photos taken by McNitt’s father of the day she was born, and asked Gemini to describe these photos in precise aesthetic detail. These descriptions became the prompts for creating new images and videos.Imagen: We generated the film's key concept art, defining the overall look, style and mood. These images became the starting point for our videos.Veo: We animated the generated images and wrote additional text prompts for guiding the action and movement to create the final shots.Developing new Veo capabilities togetherWhile Veo made it possible to generate scenes that combined live-action acting and generative footage of a realistic newborn baby, it also posed new challenges. For example, McNitt wanted the generated video to match the quality and color of her live-action scenes. She also needed to control the camera motion and subject matter of the generated video. To meet these challenges, we developed several new Veo capabilities to enable greater personalization, precise motion matching, and the ability to blend live-action and generative footage.Personalized video generationWe aimed to generate videos that felt as intimate and personal as the story itself. For example, McNitt wanted to generate footage of a realistic-looking baby in utero, while controlling the art direction, composition and motion. So we fine-tuned an Imagen model to match the style of reference images. Then, we worked with Gemini to craft and refine prompts to generate realistic images of a baby in the womb. Finally, we turned those images into animated scenes using Veo’s image-to-video capability. By fine-tuning an Imagen model, we maintained specific and consistent art direction between different scenes of the AI-generated baby. Motion matched video generationIn one scene, McNitt wanted to take the viewer on a journey through the human body, eventually landing in the womb to show a baby being born via C-section. To follow this precise camera motion, we created a virtual, 3D model of a human body and recorded a draft shot of the scene by moving a virtual camera through this model. Then we used Veo to track the draft shot’s motion and generate new videos using that same movement. We guided the generated video with text prompts, until we achieved the shot McNitt had in mind. McNitt mapped out her desired camera motion using a virtual model of the human body. Then we used Veo’s motion matching to generate a video with that same movement. In another scene, McNitt wanted to show an array of organic holes closing up, alluding to the hole in the baby’s heart. So, we gave Veo reference videos of this motion and prompted it to motion match across different shots. Producing these sequences with just computer generated imagery (CGI) would have been complex and time-intensive, and it would have been difficult to control motion using text prompts alone. With Veo’s help, we could produce high-quality scenes in just a few minutes. We gave Veo an input video with the desired motion. Then, Veo combines the reference motion with a text prompt to generate a new motion-matched scene. Blending traditional filmmaking and generative videoImagery of babies produced using traditional VFX runs the risk of looking uncanny, and it's challenging and time-consuming for directors to get the exact performance they have in mind. So, for the birth, we composed the actor’s performance and generated a realistic looking newborn to fit the scene. First, we gave Veo the live-action footage, a text prompt describing the scene, and a defined area for adding the baby. Then, using Veo’s “add object” capability, we generated the AI image of a baby into the live-action footage — keeping everything else consistent — and we refined the shot with traditional VFX and color grading. We added a generated newborn baby to live-action footage and refined the final shot with VFX and color grading. Adding generative video to traditional workflowsMany scenes in the film use multiple AI-generated images and videos that are seamlessly composed using traditional filmmaking workflows. For example, we created a scene showing complex textures on the inside of a recently hatched crocodile egg at sunset. To construct this shot, we combined multiple generated videos and images with traditional VFX compositing techniques. This shot captures the point-of-view from inside a cracking crocodile egg, at sunset with the protective mother crocodile nearby. We used Veo and Imagen to generate the key visual elements, which were then seamlessly composited in a traditional VFX pipeline to bring this specific creative vision to life. Partnering with the film industry to tell new stories“ANCESTRA” is the first of three films we're making with Primordial Soup. Each film in this partnership is directed by an emerging filmmaker who is mentored by Darren Aronofsky and supported by our team.Many amazing movies have been created with live-action filmmaking, CGI and VFX toolkits. Generative AI can complement existing creative and production workflows, empowering filmmakers to overcome practical limitations with difficult-to-capture or prohibitively expensive scenes.By working with artists, we ensure that the tools we’re building are useful and rooted in the needs of professional filmmakers. Collaborating with visionaries like McNitt and Aronofsky helps us explore the creative potential of today's technologies and imagine what we could create next.",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ancestra-YTThumbnail.width-1300.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\n\n    \n    \n\n\n\n\n\n    \n\n    \n      \n\n\u003cdiv data-analytics-module=\"{\n    \u0026#34;module_name\u0026#34;: \u0026#34;Hero Menu\u0026#34;,\n    \u0026#34;section_header\u0026#34;: \u0026#34;Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\u0026#34;\n  }\"\u003e\n  \n  \u003cdiv\u003e\n      \u003cdiv\u003e\n          \n            \u003cp\u003eJun 13, 2025\u003c/p\u003e\n          \n          \n            \u003cp data-reading-time-render=\"\"\u003e[[read-time]] min read\u003c/p\u003e\n          \n        \u003c/div\u003e\n      \n        \u003cp\u003e\n          We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking.\n        \u003c/p\u003e\n      \n    \u003c/div\u003e\n  \n  \u003cdiv data-component=\"uni-ai-generated-summary\" data-analytics-module=\"{\n    \u0026#34;event\u0026#34;: \u0026#34;module_impression\u0026#34;,\n    \u0026#34;module_name\u0026#34;: \u0026#34;ai_summary\u0026#34;,\n    \u0026#34;section_header\u0026#34;: \u0026#34;CTA\u0026#34;\n  }\"\u003e\n      \n        \u003cdiv data-summary-id=\"ai_summary_1\"\u003e\n          \u003ch2\u003eGeneral summary\u003c/h2\u003e\n          \u003cp\u003eGoogle DeepMind partnered with Primordial Soup to produce \u0026#34;ANCESTRA\u0026#34; a short film premiering at the Tribeca Festival. The film combines live-action with video generated by Veo, Google\u0026#39;s video generation model. Google DeepMind developed new Veo capabilities to enable personalization, precise motion matching, and blending of live-action and generative footage.\u003c/p\u003e\n          \n          \u003cp\u003e\u003csmall\u003e\n            Summaries were generated by Google AI. Generative AI is experimental.\n          \u003c/small\u003e\n        \u003c/p\u003e\u003c/div\u003e\n      \n        \u003cdiv data-summary-id=\"ai_summary_2\"\u003e\n          \u003ch2\u003eBullet points\u003c/h2\u003e\n          \u003cul\u003e\n\u003cli\u003e\u0026#34;ANCESTRA\u0026#34; is a short film combining live-action with Veo, Google DeepMind\u0026#39;s video generation model, premiering at the Tribeca Festival.\u003c/li\u003e\n\u003cli\u003eGoogle DeepMind partnered with Primordial Soup to put generative AI in filmmakers\u0026#39; hands, pushing storytelling and tech boundaries.\u003c/li\u003e\n\u003cli\u003eGemini, Imagen, and Veo were used to generate shots based on mood, color, and emotion, using photos as inspiration.\u003c/li\u003e\n\u003cli\u003eNew Veo capabilities were developed for personalized video, precise motion matching, and blending live-action with generated footage.\u003c/li\u003e\n\u003cli\u003eGenerative AI complements filmmaking, empowering artists to overcome limitations and create difficult or expensive scenes.\u003c/li\u003e\n\u003c/ul\u003e\n          \n          \u003cp\u003e\u003csmall\u003e\n            Summaries were generated by Google AI. Generative AI is experimental.\n          \u003c/small\u003e\n        \u003c/p\u003e\u003c/div\u003e\n      \n\n      \n      \u003cdiv\u003e\n        \u003ch4\u003e\n          Explore other styles:\n        \u003c/h4\u003e\n        \n      \u003c/div\u003e\n      \n\n      \u003c/div\u003e\n\u003c/div\u003e\n\n    \n\n    \n      \n\n\n  \u003cuni-youtube-player-hero index=\"0\" thumbnail-alt=\"Image of a baby in the womb with word ANCESTRA at the bottom\" component-title=\"Behind “ANCESTRA:” combining Veo with live-action filmmaking\" video-id=\"HEs9miwtwh4\" video-type=\"video\" image=\"Ancestra-YTThumbnail\" video-image-url-lazy=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ancestra-YTThumbnail.width-100.format-webp.webp\" video-image-url-mobile=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ancestra-YTThumbnail.width-700.format-webp.webp\" video-image-url-desktop=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ancestra-YTThumbnail.width-1000.format-webp.webp\"\u003e\n  \u003c/uni-youtube-player-hero\u003e\n\n\n    \n\n    \n    \u003cdiv data-reading-time=\"true\" data-component=\"uni-article-body\"\u003e\n\n            \n              \n\n\n\n\n\n\u003cuni-article-speakable page-title=\"Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\" listen-to-article=\"Listen to article\" data-date-modified=\"2025-06-14T12:11:05.413522+00:00\" data-tracking-ids=\"G-HGNBTNCHCQ,G-6NKTLKV14N\" data-voice-list=\"en.ioh-pngnat:Cyan,en.usb-pngnat:Lime\" data-script-src=\"https://www.gstatic.com/readaloud/player/web/api/js/api.js\"\u003e\u003c/uni-article-speakable\u003e\n\n            \n\n            \n            \n\n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"fmwuj\"\u003eToday, Eliza McNitt’s short film, “ANCESTRA,” premieres at the Tribeca Festival. It’s the story of a mother, and what happens when her child is born with a hole in its heart. Inspired by the dramatic events of McNitt\u0026#39;s own birth, the film portrays a mother\u0026#39;s love as a cosmic, life-saving force.\u003c/p\u003e\u003cp data-block-key=\"4ha4n\"\u003eThis is the first of three short films produced in \u003ca href=\"https://blog.google/technology/google-labs/deepmind-primordial-soup-collaboration/\"\u003epartnership\u003c/a\u003e between our team at Google DeepMind and Primordial Soup, a new venture dedicated to storytelling innovation founded by director Darren Aronofsky. Together, we founded this partnership to put the world’s best generative AI into the hands of top filmmakers, to advance the frontiers of storytelling and technology.\u003c/p\u003e\u003cp data-block-key=\"5pmt5\"\u003e“ANCESTRA” combined live-action scenes with sequences generated by \u003ca href=\"https://deepmind.google/models/veo/\"\u003eVeo\u003c/a\u003e, our state-of-the-art video generation model. McNitt described her experience working with our technology: \u0026#34;Veo is another lens through which I get to imagine the universe around me.”\u003c/p\u003e\u003cp data-block-key=\"a19pr\"\u003eTo create “ANCESTRA”, Google DeepMind assembled a multidisciplinary creative team of animators, art directors, designers, writers, technologists and researchers who worked closely with more than 200 experts in traditional filmmaking and production, a live-action crew and cast, plus an editorial team, visual effects (VFX) artists, sound designers and music composers.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n  \n    \n\n\n  \u003cuni-youtube-player-article index=\"2\" thumbnail-alt=\"Making of Ancestra\" video-id=\"WLDARkKs-T4\" video-type=\"video\" image=\"Ancestra-BTS-YTThumbnail\" video-image-url-lazy=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ancestra-BTS-YTThumbnail.width-100.format-webp.webp\" video-image-url-mobile=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ancestra-BTS-YTThumbnail.width-700.format-webp.webp\" video-image-url-desktop=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ancestra-BTS-YTThumbnail.width-1000.format-webp.webp\"\u003e\n  \u003c/uni-youtube-player-article\u003e\n\n\n  \n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"fmwuj\"\u003eBringing our most advanced generative models to the screen\u003c/h2\u003e\u003cp data-block-key=\"9rj9j\"\u003eWhile McNitt wrote the script for “ANCESTRA,” she worked with a storyboard artist to visualize the live-action scenes and collaborated with our team to generate imagery for sequences that could benefit from AI generation.\u003c/p\u003e\u003cp data-block-key=\"3fs1s\"\u003eWe used \u003ca href=\"https://deepmind.google/models/gemini/\"\u003eGemini\u003c/a\u003e to develop our prompts, and used \u003ca href=\"https://deepmind.google/models/veo/\"\u003eVeo\u003c/a\u003e and our image generation model, \u003ca href=\"https://deepmind.google/models/imagen/\"\u003eImagen\u003c/a\u003e, to create a series of potential shots, organized by mood, color and emotion. Here’s a breakdown of how we planned and created the AI elements of the film:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"49dt6\"\u003e\u003cb\u003eGemini\u003c/b\u003e: Our team uploaded photos taken by McNitt’s father of the day she was born, and asked Gemini to describe these photos in precise aesthetic detail. These descriptions became the prompts for creating new images and videos.\u003c/li\u003e\u003cli data-block-key=\"6cvq\"\u003e\u003cb\u003eImagen\u003c/b\u003e: We generated the film\u0026#39;s key concept art, defining the overall look, style and mood. These images became the starting point for our videos.\u003c/li\u003e\u003cli data-block-key=\"8ht73\"\u003e\u003cb\u003eVeo:\u003c/b\u003e We animated the generated images and wrote additional text prompts for guiding the action and movement to create the final shots.\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"eqfjf\"\u003eDeveloping new Veo capabilities together\u003c/h2\u003e\u003cp data-block-key=\"e1ssn\"\u003eWhile Veo made it possible to generate scenes that combined live-action acting and generative footage of a realistic newborn baby, it also posed new challenges. For example, McNitt wanted the generated video to match the quality and color of her live-action scenes. She also needed to control the camera motion and subject matter of the generated video. To meet these challenges, we developed several new Veo capabilities to enable greater personalization, precise motion matching, and the ability to blend live-action and generative footage.\u003c/p\u003e\u003ch3 data-block-key=\"73o5g\"\u003ePersonalized video generation\u003c/h3\u003e\u003cp data-block-key=\"93lec\"\u003eWe aimed to generate videos that felt as intimate and personal as the story itself. For example, McNitt wanted to generate footage of a realistic-looking baby in utero, while controlling the art direction, composition and motion. So we fine-tuned an Imagen model to match the style of reference images. Then, we worked with Gemini to craft and refine prompts to generate realistic images of a baby in the womb. Finally, we turned those images into animated scenes using Veo’s image-to-video capability.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"A grid of four distinct generated images of a baby drifting in a dimly-lit, murky environment — her face with closed eyes, detail of her foot, back of the head, and chest.\" external-image=\"\" or-mp4-video-title=\"\" or-mp4-video-url=\"\" section-header=\"Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"uqe6k\"\u003eBy fine-tuning an Imagen model, we maintained specific and consistent art direction between different scenes of the AI-generated baby.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n    \u003cp\u003e\u003cimg alt=\"A grid of four distinct generated images of a baby drifting in a dimly-lit, murky environment — her face with closed eyes, detail of her foot, back of the head, and chest.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/watermarked_ancestra_baby_grid_fi.width-100.format-webp.webp\" loading=\"lazy\" data-loading=\"{\n            \u0026#34;mobile\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/watermarked_ancestra_baby_grid_fi.width-500.format-webp.webp\u0026#34;,\n            \u0026#34;desktop\u0026#34;: \u0026#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/watermarked_ancestra_baby_grid_f.width-1000.format-webp.webp\u0026#34;\n          }\"/\u003e\n    \u003c/p\u003e\n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\u0026#34;\n         }\"\u003e\u003ch3 data-block-key=\"vqmoz\"\u003eMotion matched video generation\u003c/h3\u003e\u003cp data-block-key=\"ei76l\"\u003eIn one scene, McNitt wanted to take the viewer on a journey through the human body, eventually landing in the womb to show a baby being born via C-section. To follow this precise camera motion, we created a virtual, 3D model of a human body and recorded a draft shot of the scene by moving a virtual camera through this model. Then we used Veo to track the draft shot’s motion and generate new videos using that same movement. We guided the generated video with text prompts, until we achieved the shot McNitt had in mind.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"A video showing a flythrough through a 3D model of a human body, showing layers of rendering for color, texture, and lighting, and ending on a final video of a baby generated by Veo.\" external-image=\"\" or-mp4-video-title=\"watermarked updated motion match ancestra\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/WM_JUNE14-FIXED-ITB_cleanedup_noaudio.mp4\" section-header=\"Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"zf2dx\"\u003eMcNitt mapped out her desired camera motion using a virtual model of the human body. Then we used Veo’s motion matching to generate a video with that same movement.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\u0026#34;\n         }\"\u003e\n        \u003cp data-block-key=\"vqmoz\"\u003eIn another scene, McNitt wanted to show an array of organic holes closing up, alluding to the hole in the baby’s heart. So, we gave Veo reference videos of this motion and prompted it to motion match across different shots. Producing these sequences with just computer generated imagery (CGI) would have been complex and time-intensive, and it would have been difficult to control motion using text prompts alone. With Veo’s help, we could produce high-quality scenes in just a few minutes.\u003c/p\u003e\n      \u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"A video showing two examples of motion matched video generation. Example 1 shows two videos side by side. The left is an input reference video showing a jelly-fish like circle closing in, the right is a circular ice void closing in. Example 2 shows a swirling vortex with trees and water, the right shows tadpoles in a similar vortex motion.\" external-image=\"\" or-mp4-video-title=\"watermarked ancestra combined output\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/WM__COMBINED_Output_v_003.mp4\" section-header=\"Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"zf2dx\"\u003eWe gave Veo an input video with the desired motion. Then, Veo combines the reference motion with a text prompt to generate a new motion-matched scene.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"vqmoz\"\u003eBlending traditional filmmaking and generative video\u003c/h2\u003e\u003cp data-block-key=\"aco18\"\u003eImagery of babies produced using traditional VFX runs the risk of looking uncanny, and it\u0026#39;s challenging and time-consuming for directors to get the exact performance they have in mind. So, for the birth, we composed the actor’s performance and generated a realistic looking newborn to fit the scene. First, we gave Veo the live-action footage, a text prompt describing the scene, and a defined area for adding the baby. Then, using Veo’s “add object” capability, we generated the AI image of a baby into the live-action footage — keeping everything else consistent — and we refined the shot with traditional VFX and color grading.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"Video showing live action footage of a mother immediately post birth. A swipe transition reveals a Veo generated newborn with the mother. A second swipe transition reveals a VFX and color graded cinematic final shot.\" external-image=\"\" or-mp4-video-title=\"watermarked baby montage\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/WM_Baby_Montage_v8.mp4\" section-header=\"Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"0bqoo\"\u003eWe added a generated newborn baby to live-action footage and refined the final shot with VFX and color grading.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"vr16c\"\u003eAdding generative video to traditional workflows\u003c/h2\u003e\u003cp data-block-key=\"2nu84\"\u003eMany scenes in the film use multiple AI-generated images and videos that are seamlessly composed using traditional filmmaking workflows. For example, we created a scene showing complex textures on the inside of a recently hatched crocodile egg at sunset. To construct this shot, we combined multiple generated videos and images with traditional VFX compositing techniques.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\u003cuni-image-full-width alignment=\"full\" alt-text=\"A visual effects breakdown showing how a crocodile\u0026#39;s back, a sunset, and an eggshell are composited into a single shot with the help of AI-generated layers.\" external-image=\"\" or-mp4-video-title=\"watermarked ancestra crocodile egg\" or-mp4-video-url=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/WM_RNDR_Breakdown_crodocile_003_1k.mp4\" section-header=\"Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\" custom-class=\"image-full-width--constrained-width uni-component-spacing\"\u003e\n  \n    \u003cdiv slot=\"caption-slot\"\u003e\n      \u003cp data-block-key=\"0bqoo\"\u003eThis shot captures the point-of-view from inside a cracking crocodile egg, at sunset with the protective mother crocodile nearby. We used Veo and Imagen to generate the key visual elements, which were then seamlessly composited in a traditional VFX pipeline to bring this specific creative vision to life.\u003c/p\u003e\n    \u003c/div\u003e\n  \n  \n\u003c/uni-image-full-width\u003e\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" role=\"presentation\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Behind “ANCESTRA:” combining Veo with live\\u002Daction filmmaking\u0026#34;\n         }\"\u003e\u003ch2 data-block-key=\"vr16c\"\u003ePartnering with the film industry to tell new stories\u003c/h2\u003e\u003cp data-block-key=\"6s70p\"\u003e“ANCESTRA” is the first of three films we\u0026#39;re making with Primordial Soup. Each film in this partnership is directed by an emerging filmmaker who is mentored by Darren Aronofsky and supported by our team.\u003c/p\u003e\u003cp data-block-key=\"2jg4c\"\u003eMany amazing movies have been created with live-action filmmaking, CGI and VFX toolkits. Generative AI can complement existing creative and production workflows, empowering filmmakers to overcome practical limitations with difficult-to-capture or prohibitively expensive scenes.\u003c/p\u003e\u003cp data-block-key=\"31ph1\"\u003eBy working with artists, we ensure that the tools we’re building are useful and rooted in the needs of professional filmmakers. Collaborating with visionaries like McNitt and Aronofsky helps us explore the creative potential of today\u0026#39;s technologies and imagine what we could create next.\u003c/p\u003e\u003c/div\u003e\n  \n\n\n            \n            \n\n            \n              \n\n\n\n\n            \n          \u003c/div\u003e\n  \u003c/article\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2025-06-13T13:00:00Z",
  "modifiedTime": null
}
