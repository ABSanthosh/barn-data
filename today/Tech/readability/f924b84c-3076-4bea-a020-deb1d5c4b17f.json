{
  "id": "f924b84c-3076-4bea-a020-deb1d5c4b17f",
  "title": "Waymo Explores Using Google's Gemini To Train Its Robotaxis",
  "link": "https://tech.slashdot.org/story/24/11/01/2150228/waymo-explores-using-googles-gemini-to-train-its-robotaxis?utm_source=rss1.0mainlinkanon\u0026utm_medium=feed",
  "description": "Waymo is advancing autonomous driving with a new training model for its robotaxis built on Google's multimodal large language model (MLLM) Gemini. The Verge reports: Waymo released a new research paper today that introduces an \"End-to-End Multimodal Model for Autonomous Driving,\" also known as EMMA. This new end-to-end training model processes sensor data to generate \"future trajectories for autonomous vehicles,\" helping Waymo's driverless vehicles make decisions about where to go and how to avoid obstacles. But more importantly, this is one of the first indications that the leader in autonomous driving has designs to use MLLMs in its operations. And it's a sign that these LLMs could break free of their current use as chatbots, email organizers, and image generators and find application in an entirely new environment on the road. In its research paper, Waymo is proposing \"to develop an autonomous driving system in which the MLLM is a first class citizen.\" The paper outlines how, historically, autonomous driving systems have developed specific \"modules\" for the various functions, including perception, mapping, prediction, and planning. This approach has proven useful for many years but has problems scaling \"due to the accumulated errors among modules and limited inter-module communication.\" Moreover, these modules could struggle to respond to \"novel environments\" because, by nature, they are \"pre-defined,\" which can make it hard to adapt. Waymo says that MLLMs like Gemini present an interesting solution to some of these challenges for two reasons: the chat is a \"generalist\" trained on vast sets of scraped data from the internet \"that provide rich 'world knowledge' beyond what is contained in common driving logs\"; and they demonstrate \"superior\" reasoning capabilities through techniques like \"chain-of-thought reasoning,\" which mimics human reasoning by breaking down complex tasks into a series of logical steps. Waymo developed EMMA as a tool to help its robotaxis navigate complex environments. The company identified several situations in which the model helped its driverless cars find the right route, including encountering various animals or construction in the road. [...] But EMMA also has its limitations, and Waymo acknowledges that there will need to be future research before the model is put into practice. For example, EMMA couldn't incorporate 3D sensor inputs from lidar or radar, which Waymo said was \"computationally expensive.\" And it could only process a small amount of image frames at a time. There are also risks to using MLLMs to train robotaxis that go unmentioned in the research paper. Chatbots like Gemini often hallucinate or fail at simple tasks like reading clocks or counting objects. Read more of this story at Slashdot.",
  "author": "BeauHD",
  "published": "2024-11-02T00:10:00+00:00",
  "source": "http://rss.slashdot.org/Slashdot/slashdotMain",
  "categories": [
    "ai"
  ],
  "byline": "",
  "length": 2752,
  "excerpt": "Waymo is advancing autonomous driving with a new training model for its robotaxis built on Google's multimodal large language model (MLLM) Gemini. The Verge reports: Waymo released a new research paper today that introduces an \"End-to-End Multimodal Model for Autonomous Driving,\" also known as EMMA...",
  "siteName": "",
  "favicon": "",
  "text": "Waymo is advancing autonomous driving with a new training model for its robotaxis built on Google's multimodal large language model (MLLM) Gemini. The Verge reports: Waymo released a new research paper today that introduces an \"End-to-End Multimodal Model for Autonomous Driving,\" also known as EMMA. This new end-to-end training model processes sensor data to generate \"future trajectories for autonomous vehicles,\" helping Waymo's driverless vehicles make decisions about where to go and how to avoid obstacles. But more importantly, this is one of the first indications that the leader in autonomous driving has designs to use MLLMs in its operations. And it's a sign that these LLMs could break free of their current use as chatbots, email organizers, and image generators and find application in an entirely new environment on the road. In its research paper, Waymo is proposing \"to develop an autonomous driving system in which the MLLM is a first class citizen.\" The paper outlines how, historically, autonomous driving systems have developed specific \"modules\" for the various functions, including perception, mapping, prediction, and planning. This approach has proven useful for many years but has problems scaling \"due to the accumulated errors among modules and limited inter-module communication.\" Moreover, these modules could struggle to respond to \"novel environments\" because, by nature, they are \"pre-defined,\" which can make it hard to adapt. Waymo says that MLLMs like Gemini present an interesting solution to some of these challenges for two reasons: the chat is a \"generalist\" trained on vast sets of scraped data from the internet \"that provide rich 'world knowledge' beyond what is contained in common driving logs\"; and they demonstrate \"superior\" reasoning capabilities through techniques like \"chain-of-thought reasoning,\" which mimics human reasoning by breaking down complex tasks into a series of logical steps. Waymo developed EMMA as a tool to help its robotaxis navigate complex environments. The company identified several situations in which the model helped its driverless cars find the right route, including encountering various animals or construction in the road. [...] But EMMA also has its limitations, and Waymo acknowledges that there will need to be future research before the model is put into practice. For example, EMMA couldn't incorporate 3D sensor inputs from lidar or radar, which Waymo said was \"computationally expensive.\" And it could only process a small amount of image frames at a time. There are also risks to using MLLMs to train robotaxis that go unmentioned in the research paper. Chatbots like Gemini often hallucinate or fail at simple tasks like reading clocks or counting objects.",
  "image": "https://a.fsdn.com/sd/topics/ai_64.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"fhbody-175377361\"\u003e\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\u003cp\u003e\n\t\t\t\n\t\t \t\n\t\t\t\tWaymo is advancing autonomous driving with a new training model for its robotaxis \u003ca href=\"https://www.theverge.com/2024/10/30/24283516/waymo-google-gemini-llm-ai-robotaxi\"\u003ebuilt on Google\u0026#39;s multimodal large language model (MLLM) Gemini\u003c/a\u003e. The Verge reports: \u003ci\u003e Waymo \u003ca href=\"https://www.scribd.com/document/786090543/Emma-Paper#fullscreen\u0026amp;from_embed\"\u003ereleased\u003c/a\u003e a new research paper today that introduces an \u0026#34;End-to-End Multimodal Model for Autonomous Driving,\u0026#34; also known as EMMA. This new end-to-end training model processes sensor data to generate \u0026#34;future trajectories for autonomous vehicles,\u0026#34; helping Waymo\u0026#39;s driverless vehicles make decisions about where to go and how to avoid obstacles. But more importantly, this is one of the first indications that the leader in autonomous driving has designs to use MLLMs in its operations. And it\u0026#39;s a sign that these LLMs could break free of their current use as chatbots, email organizers, and image generators and find application in an entirely new environment on the road. In its research paper, Waymo is proposing \u0026#34;to develop an autonomous driving system in which the MLLM is a first class citizen.\u0026#34;\n\u003cp\u003e \nThe paper outlines how, historically, autonomous driving systems have developed specific \u0026#34;modules\u0026#34; for the various functions, including perception, mapping, prediction, and planning. This approach has proven useful for many years but has problems scaling \u0026#34;due to the accumulated errors among modules and limited inter-module communication.\u0026#34; Moreover, these modules could struggle to respond to \u0026#34;novel environments\u0026#34; because, by nature, they are \u0026#34;pre-defined,\u0026#34; which can make it hard to adapt. Waymo says that MLLMs like Gemini present an interesting solution to some of these challenges for two reasons: the chat is a \u0026#34;generalist\u0026#34; trained on vast sets of scraped data from the internet \u0026#34;that provide rich \u0026#39;world knowledge\u0026#39; beyond what is contained in common driving logs\u0026#34;; and they demonstrate \u0026#34;superior\u0026#34; reasoning capabilities through techniques like \u0026#34;chain-of-thought reasoning,\u0026#34; which mimics human reasoning by breaking down complex tasks into a series of logical steps.\n\u003c/p\u003e\u003cp\u003e \nWaymo developed EMMA as a tool to help its robotaxis navigate complex environments. The company identified several situations in which the model helped its driverless cars find the right route, including encountering various animals or construction in the road. [...] But EMMA also has its limitations, and Waymo acknowledges that there will need to be future research before the model is put into practice. For example, EMMA couldn\u0026#39;t incorporate 3D sensor inputs from lidar or radar, which Waymo said was \u0026#34;computationally expensive.\u0026#34; And it could only process a small amount of image frames at a time. There are also risks to using MLLMs to train robotaxis that go unmentioned in the research paper. Chatbots like Gemini often hallucinate or fail at simple tasks like reading clocks or counting objects. \u003c/p\u003e\u003c/i\u003e\u003cbr/\u003e\n\t\t \t\n\t\t\u003c/p\u003e\n\n\t\t\n\n\t\t\n\n\t\t\n\t\t\t\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": null,
  "modifiedTime": null
}
