{
  "id": "e5c03488-8a5f-4f2c-98b0-a20278b765b6",
  "title": "With new Gen-4 model, Runway claims to have finally achieved consistency in AI videos",
  "link": "https://arstechnica.com/ai/2025/03/with-new-gen-4-model-runway-claims-to-have-finally-achieved-consistency-in-ai-videos/",
  "description": "The new model is rolling out to paid users starting today.",
  "author": "Samuel Axon",
  "published": "Mon, 31 Mar 2025 21:07:27 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Ai video",
    "Runway",
    "Runway Gen-4",
    "runway ml",
    "Sora",
    "video",
    "video synthesis"
  ],
  "byline": "Samuel Axon",
  "length": 7134,
  "excerpt": "The new model is rolling out to paid users starting today.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Skip to content The new model is rolling out to paid users starting today. Runway's new Gen-4 model claims to support consistency characters and objects. Credit: Runway AI video startup Runway announced the availability of its newest video synthesis model today. Dubbed Gen-4, the model purports to solve several key problems with AI video generation. Chief among those is the notion of consistent characters and objects across shots. If you've watched any short films made with AI, you've likely noticed that they're either dream-like sequences of thematically but not realistically connected images—mood pieces more than consistent narratives. Runway claims Gen-4 can maintain consistent characters and objects, provided it's given a single reference image of the character or object in question as part of the project in Runway's interface. The company published example videos including the same woman appearing in various different shots across different scenes, and the same statue appearing in completely different contexts, looking largely the same in a variety of environments and lighting conditions. Likewise, Gen-4 aims to allow filmmakers who use the tool to get coverage of the same environment or subject from multiple angles across several shots in the same sequence. With Gen-2 and Gen-3, this was virtually impossible. The tool has in the past been good at maintaining stylistic integrity, but not at generating multiple angles within the same scene. The last major model update at Runway was Gen-3, which was announced just under a year ago in June 2024. That update greatly expanded the length of videos users could produce from just two seconds to 10, and offered greater consistency and coherence than its predecessor, Gen-2. Runway’s unique positioning in a crowded space Runway released the first publicly available version of its video synthesis product to users in February 2023. Gen-1 creations tended to be more curiosities than anything useful to creatives, but subsequent optimizations have allowed the tool to be used in limited ways in real projects. For example, it was used in producing the sequence in the film Everything Everywhere All At Once, where two rocks with googly eyes had a conversation on a cliff, and it has also been used to make visual gags for The Late Show with Stephen Colbert. Whereas many competing startups were started by AI researchers or Silicon Valley entrepreneurs, Runway was founded in 2018 by art students at New York University's Tisch School of the Arts—Cristóbal Valenzuela and Alejandro Matamala from Chilé, and Anastasis Germanidis from Greece. It was one of the first companies to release a usable video-generation tool to the public, and its team also contributed in foundational ways to the Stable Diffusion model. It is vastly outspent by competitors like OpenAI, but while most of its competitors have released general-purpose video creation tools, Runway has sought an Adobe-like place in the industry. It has focused on marketing to creative professionals like designers and filmmakers, and has implemented tools meant to make Runway a support tool into existing creative workflows. The support tool argument (as opposed to a standalone creative product) helped Runway secure a deal with motion picture company Lionsgate, wherein Lionsgate allowed Runway to legally train its models on its library of films, and Runway provided bespoke tools for Lionsgate for use in production or post-production. That said, Runway is, along with Midjourney and others, one of the subjects of a widely publicized intellectual property case brought by artists who claim the companies illegally trained their models on their work, so not all creatives are on board. Apart from the announcement about the partnership with Lionsgate, Runway has never publicly shared what data is used to train its models. However, a report in 404 Media seemed to reveal that at least some of the training data included video scraped from the YouTube channels of popular influencers, film studios, and more. Time will tell for Gen-4 The claimed improvements in Gen-4 target complaints from the creatives who use the tools, saying that these video synthesis tools are limited in their usefulness because they have limited consistency or understanding of a scene. Competing tools like OpenAI's Sora have also tried to improve on these limitations, but with limited results. Runway's announcement says that Gen-4 is rolling out to \"all paid plans and Enterprise customers\" today. However, when I logged into my paid account, Gen-4 is listed in the model picker but with the word \"Soon\" next to it, and it's not selectable yet. Runway may be rolling the model out to accounts slowly to avoid problems with server load. Gen-4 is listed as an option, but not yet usable, as of this article's publication. Credit: Samuel Axon Whenever it arrives for all users, it will only be available with a paid plan. Individual, non-enterprise plans start at $15 per month and scale up to as much as $95 per month, though there is a 20 percent discount for signing up for an annual plan instead. An Enterprise account runs $1,500 per year. The plans provide users with up to 2,250 credits monthly, but because generating usable AI video is an act of curation, you probably can't generate too many usable videos with that amount. There is an \"Explore Mode\" in the $95 per month individual plan that allows unlimited generations at a relaxed rate, which is meant as a way to gradually find your way to the output you want to invest in. Samuel Axon is a senior editor at Ars Technica, where he is the editorial director for tech and gaming coverage. He covers AI, software development, gaming, entertainment, and mixed reality. He has been writing about gaming and technology for nearly two decades at Engadget, PC World, Mashable, Vice, Polygon, Wired, and others. He previously ran a marketing and PR agency in the gaming industry, led editorial for the TV network CBS, and worked on social media marketing strategy for Samsung Mobile at the creative agency SPCSHP. He also is an independent software and game developer for iOS, Windows, and other platforms, and he is a graduate of DePaul University, where he studied interactive media and software development. 46 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/Screenshot-2025-03-31-at-3.27.59 PM-1152x648.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"app\"\u003e\n    \u003cp\u003e\u003ca href=\"#main\"\u003e\n  Skip to content\n\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cmain id=\"main\"\u003e\n            \u003carticle data-id=\"2085618\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n    \u003cdiv\u003e\n      \n\n      \n\n      \u003cp\u003e\n        The new model is rolling out to paid users starting today.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\n    \u003cdiv\u003e\n    \n    \u003cp\u003e\n      Runway\u0026#39;s new Gen-4 model claims to support consistency characters and objects.\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Runway\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \u003cp\u003eAI video startup Runway announced the availability of its newest video synthesis model today. Dubbed Gen-4, the model \u003ca href=\"https://runwayml.com/research/introducing-runway-gen-4\"\u003epurports to solve\u003c/a\u003e several key problems with AI video generation.\u003c/p\u003e\n\u003cp\u003eChief among those is the notion of consistent characters and objects across shots. If you\u0026#39;ve watched any short films made with AI, you\u0026#39;ve likely noticed that they\u0026#39;re either dream-like sequences of thematically but not realistically connected images—mood pieces more than consistent narratives.\u003c/p\u003e\n\u003cp\u003eRunway claims Gen-4 can maintain consistent characters and objects, provided it\u0026#39;s given a single reference image of the character or object in question as part of the project in Runway\u0026#39;s interface.\u003c/p\u003e\n\u003cp\u003eThe company published example videos including the same woman appearing in various different shots across different scenes, and the same statue appearing in completely different contexts, looking largely the same in a variety of environments and lighting conditions.\u003c/p\u003e\n\u003cp\u003eLikewise, Gen-4 aims to allow filmmakers who use the tool to get coverage of the same environment or subject from multiple angles across several shots in the same sequence. With Gen-2 and Gen-3, this was virtually impossible. The tool has in the past been good at maintaining stylistic integrity, but not at generating multiple angles within the same scene.\u003c/p\u003e\n\u003cp\u003eThe last major model update at Runway was \u003ca href=\"https://arstechnica.com/information-technology/2024/06/runways-latest-ai-video-generator-brings-giant-cotton-candy-monsters-to-life/\"\u003eGen-3\u003c/a\u003e, which was announced just under a year ago in June 2024. That update greatly expanded the length of videos users could produce from just two seconds to 10, and offered greater consistency and coherence than its predecessor, Gen-2.\u003c/p\u003e\n\u003cfigure\u003e\n  \n\n  \u003cfigcaption\u003e\n    \u003cspan\u003e\u003c/span\u003e\n      \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch2\u003eRunway’s unique positioning in a crowded space\u003c/h2\u003e\n\u003cp\u003eRunway released the first publicly available version of its video synthesis product to users in February 2023. Gen-1 creations tended to be more curiosities than anything useful to creatives, but subsequent optimizations have allowed the tool to be used in limited ways in real projects.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eFor example, it was used in producing the sequence in the film \u003cem\u003eEverything Everywhere All At Once\u003c/em\u003e, where two rocks with googly eyes had a conversation on a cliff, and it has also been used to make visual gags for \u003cem\u003eThe Late Show with Stephen Colbert\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eWhereas many competing startups were started by AI researchers or Silicon Valley entrepreneurs, Runway was founded in 2018 by art students at New York University\u0026#39;s Tisch School of the Arts—Cristóbal Valenzuela and Alejandro Matamala from Chilé, and Anastasis Germanidis from Greece.\u003c/p\u003e\n\u003cp\u003eIt was one of the first companies to release a usable video-generation tool to the public, and its team also contributed in foundational ways to the Stable Diffusion model.\u003c/p\u003e\n\u003cp\u003eIt is vastly outspent by competitors like OpenAI, but while most of its competitors have released general-purpose video creation tools, Runway has \u003ca href=\"https://www.theinformation.com/articles/how-runway-hopes-to-outrun-openai-google-in-the-ai-video-race\"\u003esought an Adobe-like place in the industry\u003c/a\u003e. It has focused on marketing to creative professionals like designers and filmmakers, and has implemented tools meant to make Runway a support tool into existing creative workflows.\u003c/p\u003e\n\u003cp\u003eThe support tool argument (as opposed to a standalone creative product) helped Runway secure a deal \u003ca href=\"https://arstechnica.com/information-technology/2024/09/landmark-ai-deal-sees-hollywood-giant-lionsgate-provide-library-for-ai-training/\"\u003ewith motion picture company Lionsgate\u003c/a\u003e, wherein Lionsgate allowed Runway to legally train its models on its library of films, and Runway provided bespoke tools for Lionsgate for use in production or post-production.\u003c/p\u003e\n\u003cp\u003eThat said, Runway is, along with Midjourney and others, one of the subjects of a widely publicized \u003ca href=\"https://arstechnica.com/tech-policy/2024/08/artists-claim-big-win-in-copyright-suit-fighting-ai-image-generators/\"\u003eintellectual property case\u003c/a\u003e brought by artists who claim the companies illegally trained their models on their work, so not all creatives are on board.\u003c/p\u003e\n\u003cp\u003eApart from the announcement about the partnership with Lionsgate, Runway has never publicly shared what data is used to train its models. However, a \u003ca href=\"https://www.404media.co/runway-ai-image-generator-training-data-youtube/\"\u003ereport in 404 Media\u003c/a\u003e seemed to reveal that at least some of the training data included video scraped from the YouTube channels of popular influencers, film studios, and more.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003ch2\u003eTime will tell for Gen-4\u003c/h2\u003e\n\u003cp\u003eThe claimed improvements in Gen-4 target complaints from the creatives who use the tools, saying that these video synthesis tools are limited in their usefulness because they have limited consistency or understanding of a scene. Competing tools like \u003ca href=\"https://arstechnica.com/ai/2024/12/ten-months-after-first-tease-openai-launches-sora-video-generation-publicly/\"\u003eOpenAI\u0026#39;s Sora\u003c/a\u003e have also tried to improve on these limitations, but with limited results.\u003c/p\u003e\n\u003cp\u003eRunway\u0026#39;s announcement says that Gen-4 is rolling out to \u0026#34;all paid plans and Enterprise customers\u0026#34; today. However, when I logged into my paid account, Gen-4 is listed in the model picker but with the word \u0026#34;Soon\u0026#34; next to it, and it\u0026#39;s not selectable yet. Runway may be rolling the model out to accounts slowly to avoid problems with server load.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cp\u003e\u003cimg width=\"541\" height=\"376\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/03/Screenshot-2025-03-31-at-2.54.07%E2%80%AFPM.png\" alt=\"The model selector in Runway shows \u0026#34;Soon\u0026#34; next to Gen-4\" decoding=\"async\" loading=\"lazy\"/\u003e\n                  \u003c/p\u003e\n          \u003cfigcaption\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      Gen-4 is listed as an option, but not yet usable, as of this article\u0026#39;s publication.\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Samuel Axon\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/figcaption\u003e\n      \u003c/figure\u003e\n\n\u003cp\u003eWhenever it arrives for all users, it will only be available with a paid plan. Individual, non-enterprise plans start at $15 per month and scale up to as much as $95 per month, though there is a 20 percent discount for signing up for an annual plan instead. An Enterprise account runs $1,500 per year.\u003c/p\u003e\n\u003cp\u003eThe plans provide users with up to 2,250 credits monthly, but because generating usable AI video is an act of curation, you probably can\u0026#39;t generate too many usable videos with that amount. There is an \u0026#34;Explore Mode\u0026#34; in the $95 per month individual plan that allows unlimited generations at a relaxed rate, which is meant as a way to gradually find your way to the output you want to invest in.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/samuelaxon/\"\u003e\u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2018/09/SamuelAxon300by450.jpg\" alt=\"Photo of Samuel Axon\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Samuel Axon is a senior editor at Ars Technica, where he is the editorial director for tech and gaming coverage. He covers AI, software development, gaming, entertainment, and mixed reality. He has been writing about gaming and technology for nearly two decades at Engadget, PC World, Mashable, Vice, Polygon, Wired, and others. He previously ran a marketing and PR agency in the gaming industry, led editorial for the TV network CBS, and worked on social media marketing strategy for Samsung Mobile at the creative agency SPCSHP. He also is an independent software and game developer for iOS, Windows, and other platforms, and he is a graduate of DePaul University, where he studied interactive media and software development.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/ai/2025/03/with-new-gen-4-model-runway-claims-to-have-finally-achieved-consistency-in-ai-videos/#comments\" title=\"46 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    46 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \n\n\n  \n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/security/2025/03/computer-scientist-goes-silent-after-fbi-raid-and-purging-from-university-website/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/02/GettyImages-73534290-768x432.jpg\" alt=\"Listing image for first story in Most Read: FBI raids home of prominent computer scientist who has gone incommunicado\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/main\u003e\n\n\n\n\n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-03-31T21:07:27Z",
  "modifiedTime": "2025-03-31T21:07:27Z"
}
