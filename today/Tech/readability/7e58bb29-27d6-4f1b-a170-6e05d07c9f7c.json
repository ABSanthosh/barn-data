{
  "id": "7e58bb29-27d6-4f1b-a170-6e05d07c9f7c",
  "title": "Ask a Techspert: What is on-device processing?",
  "link": "https://blog.google/technology/ai/on-device-processing/",
  "description": "Here’s a breakdown of on-device processing and how we're working to make your devices better with local AI features.",
  "author": "Molly McHugh-JohnsonContributorThe Keyword",
  "published": "Mon, 14 Oct 2024 18:10:00 +0000",
  "source": "https://www.blog.google/rss/",
  "categories": [
    "Ask a Techspert",
    "Pixel",
    "AI",
    "Fitbit",
    "Google Nest"
  ],
  "byline": "Molly McHugh-Johnson",
  "length": 7068,
  "excerpt": "Here’s a breakdown of on-device processing and how we're working to make your devices better with local AI features.",
  "siteName": "Google",
  "favicon": "https://blog.google/static/blogv2/images/apple-touch-icon.png",
  "text": "Every time a new Pixel phone comes out, you might hear that “on-device processing” makes its cool new features possible. Just take a look at the new Pixel 9 phones — things like Pixel Studio and Call Notes run “on device.” And it’s not just phones: Nest cameras, Pixel smartwatches and Fitbit devices also use this whole “on-device processing” thing. Given the devices that use it and the features it’s powering, it sounds pretty important.It’s safe to assume that the, er, processing, is happening on the, uh…well, the device. But to get a better understanding of what that means, we talked to Trystan Upstill, who has been at Google for nearly 20 years working on engineering teams across Android, Google News and Search.You were on a team that helped develop some of the exciting features that shipped with our new Pixel devices — can you tell me a little about what you worked on?Most recently, I worked within Android where I led a team that focuses on melding Google’s various technology stack into an amazing experience that’s meaningful to the user. Then figuring out how to build it and ship it.Since we’re improving technologies and introducing new ones quite often, it seems like that would be a never-ending job.Exactly! Within recent years, there’s been this explosion in generative AI capabilities. At first when we started thinking about running large language models on devices, we thought it was kind of a joke — like, “Sure we can do that, but maybe by 2026.” But then we began scoping it out, and the technology performance evolved so quickly that we were able to launch features using Gemini Nano, our on-device model, on Pixel 8 Pro in December 2023.That’s what I want to know more about: “on-device processing.” Let’s break it down and start with what exactly “processing” means.The main processor, or system-on-a-chip (SoC), in your devices, has a number of what are called Processing Units designed specifically to handle the tasks you want to do with that device. That's why you'll see the chip (like the Tensor chip found in Pixels) referred to as a \"system-on-a-chip: There's not just one processor, but several processing units, memory, interfaces and much more, all together on one piece of silicon.Let’s use Pixel smartphones as an example: The processing units include a Central Processing Unit, or CPU, as the main “engine” of sorts; a Graphics Processing Unit, or GPU, which renders visuals; and now today we have a Tensor Processing Unit, or TPU, specially designed by Google to run AI/ML workloads on a device. These all work together to help your phone get things done — aka, processing.For example, when you take photos, you’re often using all elements of your phone’s processing power to good effect. The CPU will be busy running core tasks that control what the phone is doing, the GPU will be helping render what the lens is seeing and, on a premium Android device like a Pixel, there's also a lot of work happening on the TPU to process what the optical lens sees to make your photos look awesome.Got it. “On-device” processing implies there’s off-device. Where is “off-device processing” happening, exactly?Off-device processing happens in the cloud. Your device connects to the internet and sends your request to servers elsewhere, which perform the task, and then send the output back to your phone. So if we wanted to take that process and make it happen on device, we’d take the large machine learning model that powered that task in the cloud and make it smaller and more efficient so it can run on your device’s operating system and hardware.What hardware makes that possible?New, more powerful chipsets. For example, with the Pixel 9 Pro, that’s happening thanks to our SoC called Tensor G4. Tensor G4 enables these phones to run models like Gemini Nano — it’s able to handle these high-performance computations.So basically, Tensor is designed specifically to run Google AI, which is also what powers a lot of Pixel’s new gen AI capabilities.Right! And the generative AI features are definitely part of it, but there are lots of other things on-device processing makes possible, too. Rendering video, playing games, HDR photo editing, language translation — most everything you do with your phone. These are all happening on your phone, not being sent up to a server for processing. Video format not supported TalkBack with Gemini, which analyzes images and reads descriptions out loud to blind or low-vision users, is an example of on-device processing that makes use of Tensor, Pixel’s system on a chip. The computation your phone can do today is pretty incredible. Today's smartphones are thousands of times faster than early high-performance computers, even those that were the size of rooms. Back in the day, those high-performance computers were the state of the art in terms of data analysis, image processing, anomaly detection and early AI research. Now we can do this all on device, and it opens up all sorts of neat opportunities to build helpful features that use this processing capability.Is on-device processing better than off-device?Not necessarily. If you were to use Search entirely on-device, that would be really slow or really limited or both, because when you’re searching the web, you’re sort of looking for a needle in a haystack. To fit the entire web index on your phone would be too much! Instead, when you use Search, you’re tapping into the cloud and our data centers to access trillions of web pages to find what you’re looking for.But if you want to perform a more specific task, then on-device processing is really useful. For starters, there’s latency — if something’s being processed directly on the device, you may get the result faster. Then there’s also the fact that features that are fully on device work without an internet connection, meaning better availability and reliability.Finally, given the AI chip is in your pocket rather than being served through a cloud backend, it's free for apps to leverage the LLM capabilities.All this said, there are distinct advantages to both: Cloud has more powerful models and can house lots of important data. Lots of your data, like photos, videos and more, sits in the cloud today. It also helps support actions like searching massive databases, like Drive, Gmail and Google Photos.I’m already pretty impressed with what my Pixel can do today, but from what you’re saying, I’d imagine it’s only going to get better.Yes, the models we’re using to do these complex tasks on Android devices are getting more capable. And of course it’s not just about better models and better technology: We also put a lot of work and research into thinking about what’s actually going to benefit people. We don’t want to just introduce products because the on-device processing can handle it; we want to make sure it’s something that people want to use on their phones in their everyday lives.",
  "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/ask_a_techspert_processing_hero-03.width-1300.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-reading-time=\"true\" data-component=\"uni-drop-cap|uni-tombstone\"\u003e\n\n            \n              \n\n\n\u003cgoogle-read-aloud-player data-analytics-module=\"{\n        \u0026#34;event\u0026#34;: \u0026#34;module_impression\u0026#34;,\n        \u0026#34;module_name\u0026#34;: \u0026#34;ai_audio\u0026#34;,\n        \u0026#34;section_header\u0026#34;: \u0026#34;Ask a Techspert: What is on\\u002Ddevice processing?\u0026#34;\n    }\" data-date-modified=\"2024-10-14T23:41:39.040506+00:00\" data-progress-bar-style=\"half-wave\" data-api-key=\"AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac\" data-article-style=\"style9\" data-tracking-ids=\"G-HGNBTNCHCQ,G-6NKTLKV14N\" data-voice-list=\"en.ioh-pngnat:Cyan,en.usb-pngnat:Lime\" data-layout-style=\"style1\" data-highlight-mode=\"word-over-paragraph\" data-highlight-text-color=\"#000000\" data-highlight-word-background=\"#8AB4F8\" data-highlight-paragraph-background=\"#D2E3FC\" data-background=\"linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)\" data-foreground-color=\"#202124\" data-font=\"600 16px Google Sans, sans-serif\" data-box-shadow=\"0px 1px 3px 1px rgba(60, 64, 67, 0.15)\"\u003e\n\u003c/google-read-aloud-player\u003e\n\n\n\n\n            \n\n            \n            \n\n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Ask a Techspert: What is on\\u002Ddevice processing?\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"51m6r\"\u003eEvery time a new Pixel phone comes out, you might hear that “on-device processing” makes its cool new features possible. Just take a look at the \u003ca href=\"https://blog.google/products/pixel/google-pixel-9-pro-xl/\"\u003enew Pixel 9 phones\u003c/a\u003e — things like \u003ca href=\"https://blog.google/products/pixel/google-pixel-9-new-ai-features/\"\u003ePixel Studio and Call Notes\u003c/a\u003e run “on device.” And it’s not just phones: \u003ca href=\"https://blog.google/products/google-nest/google-nest-august-2024-updates/\"\u003eNest cameras\u003c/a\u003e, \u003ca href=\"https://blog.google/products/fitbit/pixel-watch-3-daily-readiness-update-cardio-target-load/\"\u003ePixel smartwatches\u003c/a\u003e and \u003ca href=\"https://blog.google/products/fitbit/fitbit-ace-back-to-school-update/\"\u003eFitbit devices\u003c/a\u003e also use this whole “on-device processing” thing. Given the devices that use it and the features it’s powering, it sounds pretty important.\u003c/p\u003e\u003cp data-block-key=\"6tt8g\"\u003eIt’s safe to assume that the, er, processing, is happening on the, uh…well, the device. But to get a better understanding of what that means, we talked to Trystan Upstill, who has been at Google for nearly 20 years working on engineering teams across Android, Google News and Search.\u003c/p\u003e\u003cp data-block-key=\"6647u\"\u003e\u003cb\u003eYou were on a team that helped develop some of the exciting features that shipped with our new Pixel devices — can you tell me a little about what you worked on?\u003c/b\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-block-key=\"4f6pq\"\u003eMost recently, I worked within Android where I led a team that focuses on melding Google’s various technology stack into an amazing experience that’s meaningful to the user. Then figuring out how to build it and ship it.\u003c/p\u003e\u003cp data-block-key=\"bph1b\"\u003e\u003cb\u003eSince we’re improving technologies and introducing new ones quite often, it seems like that would be a never-ending job.\u003c/b\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-block-key=\"67o2f\"\u003eExactly! Within recent years, there’s been this explosion in generative AI capabilities. At first when we started thinking about running large language models on devices, we thought it was kind of a joke — like, “Sure we can do that, but maybe by 2026.” But then we began scoping it out, and the technology performance evolved so quickly that we were able to launch features using Gemini Nano, our on-device model, on Pixel 8 Pro in \u003ca href=\"https://blog.google/intl/en-in/pixel-feature-drop-december-2023/\"\u003eDecember 2023\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"a9ehu\"\u003e\u003cb\u003eThat’s what I want to know more about: “on-device processing.” Let’s break it down and start with what exactly “processing” means.\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"72tmj\"\u003eThe main processor, or system-on-a-chip (SoC), in your devices, has a number of what are called Processing Units designed specifically to handle the tasks you want to do with that device. That\u0026#39;s why you\u0026#39;ll see the chip (like the Tensor chip found in Pixels) referred to as a \u0026#34;system-on-a-chip: There\u0026#39;s not just one processor, but several processing units, memory, interfaces and much more, all together on one piece of silicon.\u003c/p\u003e\u003cp data-block-key=\"5bhrh\"\u003eLet’s use Pixel smartphones as an example: The processing units include a Central Processing Unit, or CPU, as the main “engine” of sorts; a Graphics Processing Unit, or GPU, which renders visuals; and now today we have a Tensor Processing Unit, or TPU, specially designed by Google to run AI/ML workloads on a device. These all work together to help your phone get things done — aka, processing.\u003c/p\u003e\u003cp data-block-key=\"bu7r1\"\u003eFor example, when you take photos, you’re often using all elements of your phone’s processing power to good effect. The CPU will be busy running core tasks that control what the phone is doing, the GPU will be helping render what the lens is seeing and, on a premium Android device like a Pixel, there\u0026#39;s also a lot of work happening on the TPU to process what the optical lens sees to make your photos look awesome.\u003c/p\u003e\u003cp data-block-key=\"6vv5j\"\u003e\u003cb\u003eGot it. “On-device” processing implies there’s off-device. Where is “off-device processing” happening, exactly?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"9ieru\"\u003eOff-device processing happens in the cloud. Your device connects to the internet and sends your request to servers elsewhere, which perform the task, and then send the output back to your phone. So if we wanted to take that process and make it happen on device, we’d take the large machine learning model that powered that task in the cloud and make it smaller and more efficient so it can run on your device’s operating system and hardware.\u003c/p\u003e\u003cdiv data-block-key=\"a2g3s\"\u003e\u003cp\u003e\u003cb\u003eWhat hardware makes that possible?\u003c/b\u003e\u003c/p\u003e\u003cp\u003eNew, more powerful chipsets. For example, with the \u003ca href=\"https://store.google.com/us/product/pixel_9_pro\"\u003ePixel 9 Pro\u003c/a\u003e, that’s happening thanks to our SoC called Tensor G4. Tensor G4 enables these phones to run models like Gemini Nano — it’s able to handle these high-performance computations.\u003c/p\u003e\u003c/div\u003e\u003cp data-block-key=\"93qql\"\u003e\u003cb\u003eSo basically, Tensor is designed specifically to run Google AI, which is\u003c/b\u003e \u003cb\u003e\u003ci\u003ealso\u003c/i\u003e\u003c/b\u003e \u003cb\u003ewhat powers a lot of Pixel’s new gen AI capabilities.\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"9sj3p\"\u003eRight! And the generative AI features are definitely part of it, but there are lots of other things on-device processing makes possible, too. Rendering video, playing games, HDR photo editing, language translation — most everything you do with your phone. These are all happening on your phone, not being sent up to a server for processing.\u003c/p\u003e\u003c/div\u003e\n  \n\n  \n    \n\n\n\n\n\n\n\n  \n      \u003cdiv data-analytics-module=\"{\n          \u0026#34;module_name\u0026#34;: \u0026#34;Inline Images\u0026#34;,\n          \u0026#34;section_header\u0026#34;: \u0026#34;Ask a Techspert: What is on\\u002Ddevice processing?\u0026#34;\n        }\"\u003e\n  \n\n  \u003cp\u003e\n\n      \n      \n        \n          \u003cvideo tabindex=\"0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/3._Talkback_with_Gemini.mp4\" type=\"video/mp4\" title=\"Animation shows a phone screen with an online shop with a black and white gingham dress. A TalkBack overlay appears on top with detailed information describing the image.\" alt=\"Talkback with Gemini\"\u003e\n            Video format not supported\n          \u003c/video\u003e\n        \n      \n    \n    \u003c/p\u003e\n    \n      \u003cfigcaption\u003e\u003cp data-block-key=\"qvauc\"\u003eTalkBack with Gemini, which analyzes images and reads descriptions out loud to blind or low-vision users, is an example of on-device processing that makes use of Tensor, Pixel’s system on a chip.\u003c/p\u003e\u003c/figcaption\u003e\n    \n  \n    \u003c/div\u003e\n  \n\n\n\n  \n\n  \n    \u003cdiv data-component=\"uni-article-paragraph\" data-analytics-module=\"{\n           \u0026#34;module_name\u0026#34;: \u0026#34;Paragraph\u0026#34;,\n           \u0026#34;section_header\u0026#34;: \u0026#34;Ask a Techspert: What is on\\u002Ddevice processing?\u0026#34;\n         }\"\u003e\u003cp data-block-key=\"51m6r\"\u003eThe computation your phone can do today is pretty incredible. Today\u0026#39;s smartphones are thousands of times faster than early high-performance computers, even those that were the size of rooms. Back in the day, those high-performance computers were the state of the art in terms of data analysis, image processing, anomaly detection and early AI research. Now we can do this all on device, and it opens up all sorts of neat opportunities to build helpful features that use this processing capability.\u003c/p\u003e\u003cp data-block-key=\"90t8o\"\u003e\u003cb\u003eIs on-device processing better than off-device?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"ft4es\"\u003eNot necessarily. If you were to use Search entirely on-device, that would be really slow or really limited or both, because when you’re searching the web, you’re sort of looking for a needle in a haystack. To fit the entire web index on your phone would be too much! Instead, when you use Search, you’re tapping into the cloud and our data centers to access trillions of web pages to find what you’re looking for.\u003c/p\u003e\u003cp data-block-key=\"df26f\"\u003eBut if you want to perform a more specific task, then on-device processing is really useful. For starters, there’s latency — if something’s being processed directly on the device, you may get the result faster. Then there’s also the fact that features that are fully on device work without an internet connection, meaning better availability and reliability.\u003c/p\u003e\u003cp data-block-key=\"b168j\"\u003eFinally, given the AI chip is in your pocket rather than being served through a cloud backend, it\u0026#39;s free for apps to leverage the LLM capabilities.\u003c/p\u003e\u003cp data-block-key=\"49ams\"\u003eAll this said, there are distinct advantages to both: Cloud has more powerful models and can house lots of important data. Lots of your data, like photos, videos and more, sits in the cloud today. It also helps support actions like searching massive databases, like Drive, Gmail and Google Photos.\u003c/p\u003e\u003cp data-block-key=\"amh4m\"\u003e\u003cb\u003eI’m already pretty impressed with what my Pixel can do today, but from what you’re saying, I’d imagine it’s only going to get better.\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"apkhm\"\u003eYes, the models we’re using to do these complex tasks on Android devices are getting more capable. And of course it’s not just about better models and better technology: We also put a lot of work and research into thinking about what’s actually going to benefit people. We don’t want to just introduce products because the on-device processing can handle it; we want to make sure it’s something that people want to use on their phones in their everyday lives.\u003c/p\u003e\u003c/div\u003e\n  \n\n\n            \n            \n\n            \n              \n\n\n\n\n            \n          \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-10-14T18:10:00Z",
  "modifiedTime": null
}
