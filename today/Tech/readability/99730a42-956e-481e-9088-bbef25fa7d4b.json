{
  "id": "99730a42-956e-481e-9088-bbef25fa7d4b",
  "title": "Unfair decisions by AI could make us indifferent to bad behaviour by humans",
  "link": "https://thenextweb.com/news/unfair-decisions-by-ai-could-make-us-indifferent-to-bad-behaviour-by-humans",
  "description": "Artificial intelligence (AI) makes important decisions that affect our everyday lives. These decisions are implemented by firms and institutions in the name of efficiency. They can help determine who gets into college, who lands a job, who receives medical treatment and who qualifies for government assistance. As AI takes on these roles, there is a growing risk of unfair decisions – or the perception of them by those people affected. For example, in college admissions or hiring, these automated decisions can unintentionally favour certain groups of people or those with certain backgrounds, while equally qualified but underrepresented applicants get overlooked.…This story continues at The Next Web",
  "author": "The Conversation",
  "published": "Mon, 23 Dec 2024 15:18:00 +0000",
  "source": "https://thenextweb.com/feed/",
  "categories": [
    "Insider",
    "Future of work"
  ],
  "byline": "The Conversation",
  "length": 5747,
  "excerpt": "Researchers asked: how does experiencing unfairness from an AI system affect how people treat one another afterwards?",
  "siteName": "TNW | Future-Of-Work",
  "favicon": "https://next.tnwcdn.com/assets/img/favicon/favicon-194x194.png",
  "text": "Artificial intelligence (AI) makes important decisions that affect our everyday lives. These decisions are implemented by firms and institutions in the name of efficiency. They can help determine who gets into college, who lands a job, who receives medical treatment and who qualifies for government assistance. As AI takes on these roles, there is a growing risk of unfair decisions – or the perception of them by those people affected. For example, in college admissions or hiring, these automated decisions can unintentionally favour certain groups of people or those with certain backgrounds, while equally qualified but underrepresented applicants get overlooked. Or, when used by governments in benefit systems, AI may allocate resources in ways that worsen social inequality, leaving some people with less than they deserve and a sense of unfair treatment. Together with an international team of researchers, we examined how unfair resource distribution – whether handled by AI or a human – influences people’s willingness to act against unfairness. The results have been published in the journal Cognition. With AI becoming more embedded in daily life, governments are stepping in to protect citizens from biased or opaque AI systems. Examples of these efforts include the White House’s AI Bill of Rights, and the European parliament’s AI Act. These reflect a shared concern: people may feel wronged by AI’s decisions. So how does experiencing unfairness from an AI system affect how people treat one another afterwards? AI-induced indifference Our paper in Cognition looked at people’s willingness to act against unfairness after experiencing unfair treatment by an AI. The behaviour we examined applied to subsequent, unrelated interactions by these individuals. A willingness to act in such situations, often called “prosocial punishment,” is seen as crucial for upholding social norms. For example, whistleblowers may report unethical practices despite the risks, or consumers may boycott companies that they believe are acting in harmful ways. People who engage in these acts of prosocial punishment often do so to address injustices that affect others, which helps reinforce community standards. Anggalih Prasetya / Shutterstock We asked this question: could experiencing unfairness from AI, instead of a person, affect people’s willingness to stand up to human wrongdoers later on? For instance, if an AI unfairly assigns a shift or denies a benefit, does it make people less likely to report unethical behaviour by a co-worker afterwards? Across a series of experiments, we found that people treated unfairly by an AI were less likely to punish human wrongdoers afterwards than participants who had been treated unfairly by a human. They showed a kind of desensitisation to others’ bad behaviour. We called this effect AI-induced indifference, to capture the idea that unfair treatment by AI can weaken people’s sense of accountability to others. This makes them less likely to address injustices in their community. Reasons for inaction This may be because people place less blame on AI for unfair treatment, and thus they feel less driven to act against injustice. This effect is consistent even when participants encountered only unfair behaviour by others or both fair and unfair behaviour. To look at whether the relationship we had uncovered was affected by familiarity with AI, we carried out the same experiments again, after the release of ChatGPT in 2022. We got the same results with the later series of tests as we had with the earlier ones. These results suggest that people’s responses to unfairness depend not only on whether they were treated fairly but also on who treated them unfairly – an AI or a human. In short, unfair treatment by an AI system can affect how people respond to each other, making them less attentive to each other’s unfair actions. This highlights AI’s potential ripple effects in human society, extending beyond an individual’s experience of a single unfair decision. When AI systems act unfairly, the consequences extend to future interactions, influencing how people treat each other, even in situations unrelated to AI. We would suggest that developers of AI systems should focus on minimising biases in AI training data to prevent these important spillover effects. Policymakers should also establish standards for transparency, requiring companies to disclose where AI might make unfair decisions. This would help users understand the limitations of AI systems, and how to challenge unfair outcomes. Increased awareness of these effects could also encourage people to stay alert to unfairness, especially after interacting with AI. Feelings of outrage and blame for unfair treatment are essential for spotting injustice and holding wrongdoers accountable. By addressing AI’s unintended social effects, leaders can ensure AI supports rather than undermines the ethical and social standards needed for a society built on justice. Chiara Longoni, Associate Professor, Marketing and Social Science, Bocconi University; Ellie Kyung, Associate Professor, Marketing Division, Babson College, and Luca Cian, Killgallon Ohio Art Professor of Business Administration, Darden School of Business, University of Virginia This article is republished from The Conversation under a Creative Commons license. Read the original article. Get the TNW newsletter Get the most important tech news in your inbox each week. Also tagged with",
  "image": "https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1\u0026fit=1280%2C640\u0026url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026signature=161d28274a53b54b7903cf422bbf7e8e",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                \u003carticle id=\"articleOutput\"\u003e\n                                                                        \u003cdiv\u003e\n                                \u003cfigure\u003e\n                                    \u003cimg alt=\"Unfair decisions by AI could make us indifferent to bad behaviour by humans\" src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=5760271cbfa1f31a3adc9a1708c5b988\" sizes=\"(max-width: 1023px) 100vw\n                                                   868px\" srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=88568e391c93ecf060747c1aca45e613 576w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=cc063d10e9df10601525fcd5138fc6aa 1152w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=5760271cbfa1f31a3adc9a1708c5b988 1280w\" data-src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=5760271cbfa1f31a3adc9a1708c5b988\" data-srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=88568e391c93ecf060747c1aca45e613 576w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=cc063d10e9df10601525fcd5138fc6aa 1152w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=5760271cbfa1f31a3adc9a1708c5b988 1280w\"/\u003e\n\n                                    \n\n                                                                    \u003c/figure\u003e\n                            \u003c/div\u003e\n                        \n                                                    \n                            \n                                            \n                    \n                    \n\n                    \n                    \u003cdiv\u003e\n                        \u003cdiv id=\"article-main-content\"\u003e\n\u003cp\u003e\u003ca href=\"https://thenextweb.com/artificial-intelligence\" target=\"_blank\" rel=\"noopener\"\u003eArtificial intelligence\u003c/a\u003e (AI) makes important decisions that affect our everyday lives. These decisions are implemented by firms and institutions in the name of efficiency. They can help determine who gets into college, who lands a job, who receives medical treatment and who qualifies for government assistance.\u003c/p\u003e\n\u003cp\u003eAs AI takes on these roles, there is a growing risk of unfair decisions – or the perception of them by those people affected. For example, in \u003ca href=\"https://law.stanford.edu/2024/06/29/how-will-ai-impact-racial-disparities-in-education/\" target=\"_blank\" rel=\"nofollow noopener\"\u003ecollege admissions or hiring\u003c/a\u003e, these automated decisions can unintentionally favour certain groups of people or those with certain backgrounds, while equally qualified but underrepresented applicants get overlooked.\u003c/p\u003e\n\u003cp\u003eOr, when used by governments in \u003ca href=\"https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy\" target=\"_blank\" rel=\"nofollow noopener\"\u003ebenefit systems\u003c/a\u003e, AI may allocate resources in ways that worsen social inequality, leaving some people with less than they deserve and a sense of unfair treatment.\u003c/p\u003e\n\u003cp\u003eTogether with an international team of researchers, we examined how unfair resource distribution – whether handled by AI or a human – influences people’s willingness to act against unfairness. The results have been \u003ca href=\"https://www.sciencedirect.com/science/article/abs/pii/S0010027724002233\" target=\"_blank\" rel=\"nofollow noopener\"\u003epublished in the journal Cognition\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWith AI becoming more embedded in daily life, governments are stepping in to protect citizens from biased or opaque AI systems. Examples of these efforts include the \u003ca href=\"https://www.whitehouse.gov/ostp/ai-bill-of-rights/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eWhite House’s AI Bill of Rights\u003c/a\u003e, and the \u003ca href=\"https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf\" target=\"_blank\" rel=\"nofollow noopener\"\u003eEuropean parliament’s AI Act\u003c/a\u003e. These reflect a shared concern: people may feel wronged by AI’s decisions.\u003c/p\u003e\n\u003cp\u003eSo how does experiencing unfairness from an AI system affect how people treat one another afterwards?\u003c/p\u003e\n\u003ch2\u003eAI-induced indifference\u003c/h2\u003e\n\u003cp\u003eOur paper in Cognition looked at people’s willingness to act against unfairness after experiencing unfair treatment by an AI. The behaviour we examined applied to subsequent, unrelated interactions by these individuals. \u003ca href=\"https://journals.sagepub.com/doi/10.1177/1948550616639650\" target=\"_blank\" rel=\"nofollow noopener\"\u003eA willingness to act\u003c/a\u003e in such situations, often called “prosocial punishment,” is seen as crucial for upholding social norms.\u003c/p\u003e\n\u003cp\u003eFor example, whistleblowers may report unethical practices despite the risks, or consumers may boycott companies that they believe are acting in harmful ways. People who engage in these acts of prosocial punishment often do so to address injustices that affect others, which helps reinforce community standards.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://images.theconversation.com/files/631747/original/file-20241113-19-j3liw7.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=754\u0026amp;fit=clip\" sizes=\"(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px\" srcset=\"https://images.theconversation.com/files/631747/original/file-20241113-19-j3liw7.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=400\u0026amp;fit=crop\u0026amp;dpr=1 600w, https://images.theconversation.com/files/631747/original/file-20241113-19-j3liw7.jpg?ixlib=rb-4.1.0\u0026amp;q=30\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=400\u0026amp;fit=crop\u0026amp;dpr=2 1200w, https://images.theconversation.com/files/631747/original/file-20241113-19-j3liw7.jpg?ixlib=rb-4.1.0\u0026amp;q=15\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=400\u0026amp;fit=crop\u0026amp;dpr=3 1800w, https://images.theconversation.com/files/631747/original/file-20241113-19-j3liw7.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=503\u0026amp;fit=crop\u0026amp;dpr=1 754w, https://images.theconversation.com/files/631747/original/file-20241113-19-j3liw7.jpg?ixlib=rb-4.1.0\u0026amp;q=30\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=503\u0026amp;fit=crop\u0026amp;dpr=2 1508w, https://images.theconversation.com/files/631747/original/file-20241113-19-j3liw7.jpg?ixlib=rb-4.1.0\u0026amp;q=15\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=503\u0026amp;fit=crop\u0026amp;dpr=3 2262w\" alt=\"Representation of AI\"/\u003e\u003cfigcaption\u003e\u003cspan\u003e\u003c/span\u003e\u003cbr/\u003e\n\u003cspan\u003e\u003ca href=\"https://www.shutterstock.com/image-photo/ai-brain-motif-centered-on-circular-2500501245\" target=\"_blank\" rel=\"nofollow noopener\"\u003eAnggalih Prasetya / Shutterstock\u003c/a\u003e\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eWe asked this question: could experiencing unfairness from AI, instead of a person, affect people’s willingness to stand up to human wrongdoers later on? For instance, if an AI unfairly assigns a shift or denies a benefit, does it make people less likely to report unethical behaviour by a co-worker afterwards?\u003c/p\u003e\n\u003cp\u003eAcross a series of experiments, we found that people treated unfairly by an AI were less likely to punish human wrongdoers afterwards than participants who had been treated unfairly by a human. They showed a kind of desensitisation to others’ bad behaviour. We called this effect AI-induced indifference, to capture the idea that unfair treatment by AI can weaken people’s sense of accountability to others. This makes them less likely to address injustices in their community.\u003c/p\u003e\n\u003ch2\u003eReasons for inaction\u003c/h2\u003e\n\u003cp\u003eThis may be because people place less blame on AI for unfair treatment, and thus they feel less driven to act against injustice. This effect is consistent even when participants encountered only unfair behaviour by others or both fair and unfair behaviour. To look at whether the relationship we had uncovered was affected by familiarity with AI, we carried out the same experiments again, after the release of \u003ca href=\"https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-how-we-got-to-where-we-are-today/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eChatGPT in 2022\u003c/a\u003e. We got the same results with the later series of tests as we had with the earlier ones.\u003c/p\u003e\n\u003cp\u003eThese results suggest that people’s responses to unfairness depend not only on whether they were treated fairly but also on who treated them unfairly – an AI or a human.\u003c/p\u003e\n\u003cp\u003eIn short, unfair treatment by an AI system can affect how people respond to each other, making them less attentive to each other’s unfair actions. This highlights AI’s potential ripple effects in human society, extending beyond an individual’s experience of a single unfair decision.\u003c/p\u003e\n\u003cp\u003eWhen AI systems act unfairly, the consequences extend to future interactions, influencing how people treat each other, even in situations unrelated to AI. We would suggest that developers of AI systems should focus on \u003ca href=\"https://www.ibm.com/topics/ai-bias\" target=\"_blank\" rel=\"nofollow noopener\"\u003eminimising biases in AI training data\u003c/a\u003e to prevent these important spillover effects.\u003c/p\u003e\n\u003cp\u003ePolicymakers should also establish standards for transparency, requiring companies to disclose where AI might make unfair decisions. This would help users understand the limitations of AI systems, and how to challenge unfair outcomes. Increased awareness of these effects could also encourage people to stay alert to unfairness, especially after interacting with AI.\u003c/p\u003e\n\u003cp\u003eFeelings of outrage and blame for unfair treatment are essential for spotting injustice and holding wrongdoers accountable. By addressing AI’s unintended social effects, leaders can ensure AI supports rather than undermines the ethical and social standards needed for a society built on justice.\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://counter.theconversation.com/content/243536/count.gif?distributor=republish-lightbox-basic\" alt=\"The Conversation\" width=\"1\" height=\"1\" srcset=\"\" data-old-src=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003ca href=\"https://theconversation.com/profiles/chiara-longoni-1510254\" target=\"_blank\" rel=\"nofollow noopener\"\u003eChiara Longoni\u003c/a\u003e, Associate Professor, Marketing and Social Science, \u003ca href=\"https://theconversation.com/institutions/bocconi-university-3019\" target=\"_blank\" rel=\"nofollow noopener\"\u003eBocconi University\u003c/a\u003e; \u003ca href=\"https://theconversation.com/profiles/ellie-kyung-2259604\" target=\"_blank\" rel=\"nofollow noopener\"\u003eEllie Kyung\u003c/a\u003e, Associate Professor, Marketing Division, \u003ca href=\"https://theconversation.com/institutions/babson-college-3510\" target=\"_blank\" rel=\"nofollow noopener\"\u003eBabson College\u003c/a\u003e, and \u003ca href=\"https://theconversation.com/profiles/luca-cian-2259600\" target=\"_blank\" rel=\"nofollow noopener\"\u003eLuca Cian\u003c/a\u003e, Killgallon Ohio Art Professor of Business Administration, Darden School of Business, \u003ca href=\"https://theconversation.com/institutions/university-of-virginia-752\" target=\"_blank\" rel=\"nofollow noopener\"\u003eUniversity of Virginia\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThis article is republished from \u003ca href=\"https://theconversation.com\" target=\"_blank\" rel=\"nofollow noopener\"\u003eThe Conversation\u003c/a\u003e under a Creative Commons license. Read the \u003ca href=\"https://theconversation.com/unfair-decisions-by-ai-could-make-us-indifferent-to-bad-behaviour-by-humans-243536\" target=\"_blank\" rel=\"nofollow noopener\"\u003eoriginal article\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n                        \n\n                        \u003cdiv id=\"nl-container\"\u003e\n                                                        \u003ch2\u003eGet the TNW newsletter\u003c/h2\u003e\n                            \u003cp\u003eGet the most important tech news in your inbox each week.\u003c/p\u003e\n                            \n                        \u003c/div\u003e\n\n                        \n                                                    \u003ch2\u003eAlso tagged with\u003c/h2\u003e\n\n                            \u003cbr/\u003e\n\n                            \n                        \n                        \n\n                        \n                    \u003c/div\u003e\n                    \n\n                    \n                \u003c/article\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-12-23T15:18:00Z",
  "modifiedTime": "2024-12-23T15:18:20Z"
}
