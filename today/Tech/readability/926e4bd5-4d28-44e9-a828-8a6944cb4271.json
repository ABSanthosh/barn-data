{
  "id": "926e4bd5-4d28-44e9-a828-8a6944cb4271",
  "title": "Cheap AI “video scraping” can now extract data from any screen recording",
  "link": "https://arstechnica.com/ai/2024/10/cheap-ai-video-scraping-can-now-extract-data-from-any-screen-recording/",
  "description": "Researcher feeds screen recordings into Gemini to extract accurate information with ease.",
  "author": "Benj Edwards",
  "published": "Thu, 17 Oct 2024 22:41:39 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "AI video scraping",
    "machine learning",
    "multimodal",
    "multimodal AI",
    "Simon Willison",
    "video scraping"
  ],
  "byline": "Benj Edwards",
  "length": 10171,
  "excerpt": "Researcher feeds screen recordings into Gemini to extract accurate information with ease.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Researcher feeds screen recordings into Gemini to extract accurate information with ease. Recently, AI researcher Simon Willison wanted to add up his charges from using a cloud service, but the payment values and dates he needed were scattered among a dozen separate emails. Inputting them manually would have been tedious, so he turned to a technique he calls \"video scraping,\" which involves feeding a screen recording video into an AI model, similar to ChatGPT, for data extraction purposes. What he discovered seems simple on its surface, but the quality of the result has deeper implications for the future of AI assistants, which may soon be able to see and interact with what we're doing on our computer screens. \"The other day I found myself needing to add up some numeric values that were scattered across twelve different emails,\" Willison wrote in a detailed post on his blog. He recorded a 35-second video scrolling through the relevant emails, then fed that video into Google's AI Studio tool, which allows people to experiment with several versions of Google's Gemini 1.5 Pro and Gemini 1.5 Flash AI models. Willison then asked Gemini to pull the price data from the video and arrange it into a special data format called JSON (JavaScript Object Notation) that included dates and dollar amounts. The AI model successfully extracted the data, which Willison then formatted as CSV (comma-separated values) table for spreadsheet use. After double-checking for errors as part of his experiment, the accuracy of the results—and what the video analysis cost to run—surprised him. A screenshot of Simon Willison using Google Gemini to extract data from a screen capture video. A screenshot of Simon Willison using Google Gemini to extract data from a screen capture video. Credit: Simon Willison \"The cost [of running the video model] is so low that I had to re-run my calculations three times to make sure I hadn’t made a mistake,\" he wrote. Willison says the entire video analysis process ostensibly cost less than one-tenth of a cent, using just 11,018 tokens on the Gemini 1.5 Flash 002 model. In the end, he actually paid nothing because Google AI Studio is currently free for some types of use. Video scraping is just one of many new tricks possible when the latest large language models (LLMs), such as Google's Gemini and GPT-4o, are actually \"multimodal\" models, allowing audio, video, image, and text input. These models translate any multimedia input into tokens (chunks of data), which they use to make predictions about which tokens should come next in a sequence. A term like \"token prediction model\" (TPM) might be more accurate than \"LLM\" these days for AI models with multimodal inputs and outputs, but a generalized alternative term hasn't really taken off yet. But no matter what you call it, having an AI model that can take video inputs has interesting implications, both good and potentially bad. Breaking down input barriers Willison is far from the first person to feed video into AI models to achieve interesting results (more on that below, and here's a 2015 paper that uses the \"video scraping\" term), but as soon as Gemini launched its video input capability, he began to experiment with it in earnest. In February, Willison demonstrated another early application of AI video scraping on his blog, where he took a seven-second video of the books on his bookshelves, then got Gemini 1.5 Pro to extract all of the book titles it saw in the video and put them in a structured, or organized, list. Converting unstructured data into structured data is important to Willison, because he's also a data journalist. Willison has created tools for data journalists in the past, such as the Datasette project, which lets anyone publish data as an interactive website. To every data journalist's frustration, some sources of data prove resistant to scraping (capturing data for analysis) due to how the data is formatted, stored, or presented. In these cases, Willison delights in the potential for AI video scraping because it bypasses these traditional barriers to data extraction. \"There's no level of website authentication or anti-scraping technology that can stop me from recording a video of my screen while I manually click around inside a web application,\" Willison noted on his blog. His method works for any visible on-screen content. Video is the new text An illustration of a cybernetic eyeball. An illustration of a cybernetic eyeball. Credit: Getty Images The ease and effectiveness of Willison's technique reflect a noteworthy shift now underway in how some users will interact with token prediction models. Rather than requiring a user to manually paste or type in data in a chat dialog—or detail every scenario to a chatbot as text—some AI applications increasingly work with visual data captured directly on the screen. For example, if you're having trouble navigating a pizza website's terrible interface, an AI model could step in and perform the necessary mouse clicks to order the pizza for you. In fact, video scraping is already on the radar of every major AI lab, although they are not likely to call it that at the moment. Instead, tech companies typically refer to these techniques as \"video understanding\" or simply \"vision.\" In May, OpenAI demonstrated a prototype version of its ChatGPT Mac App with an option that allowed ChatGPT to see and interact with what is on your screen, but that feature has not yet shipped. Microsoft demonstrated a similar \"Copilot Vision\" prototype concept earlier this month (based on OpenAI's technology) that will be able to \"watch\" your screen and help you extract data and interact with applications you're running. Despite these research previews, OpenAI's ChatGPT and Anthropic's Claude have not yet implemented a public video input feature for their models, possibly because it is relatively computationally expensive for them to process the extra tokens from a \"tokenized\" video stream. For the moment, Google is heavily subsidizing user AI costs with its war chest from Search revenue and a massive fleet of data centers (to be fair, OpenAI is subsidizing, too, but with investor dollars and help from Microsoft). But costs of AI compute in general are dropping by the day, which will open up new capabilities of the technology to a broader user base over time. Countering privacy issues As you might imagine, having an AI model see what you do on your computer screen can have downsides. For now, video scraping is great for Willison, who will undoubtedly use the captured data in positive and helpful ways. But it's also a preview of a capability that could later be used to invade privacy or autonomously spy on computer users on a scale that was once impossible. A different form of video scraping caused a massive wave of controversy recently for that exact reason. Apps such as the third-party Rewind AI on the Mac and Microsoft's Recall, which is being built into Windows 11, operate by feeding on-screen video into an AI model that stores extracted data into a database for later AI recall. Unfortunately, that approach also introduces potential privacy issues because it records everything you do on your machine and puts it in a single place that could later be hacked. To that point, although Willison's technique currently involves uploading a video of his data to Google for processing, he is pleased that he can still decide what the AI model sees and when. \"The great thing about this video scraping technique is that it works with anything that you can see on your screen... and it puts you in total control of what you end up exposing to the AI model,\" Willison explained in his blog post. It's also possible in the future that a locally run open-weights AI model could pull off the same video analysis method without the need for a cloud connection at all. Microsoft Recall runs locally on supported devices, but it still demands a great deal of unearned trust. For now, Willison is perfectly content to selectively feed video data to AI models when the need arises. \"I expect I’ll be using this technique a whole lot more in the future,\" he wrote, and perhaps many others will, too, in different forms. If the past is any indication, Willison—who coined the term \"prompt injection\" in 2022—seems to always be a few steps ahead in exploring novel applications of AI tools. Right now, his attention is on the new implications of AI and video, and yours probably should be, too. Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a widely-cited tech historian. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/10/abstract_cubes.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n            \u003carticle data-id=\"2056927\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n    \u003cdiv\u003e\n      \n\n      \n\n      \u003cp\u003e\n        Researcher feeds screen recordings into Gemini to extract accurate information with ease.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\n          \u003cdiv\u003e\n        \u003cp\u003e\u003cimg width=\"1000\" height=\"675\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/abstract_cubes-1000x675.jpg\" alt=\"Abstract 3d background with different cubes\" loading=\"eager\" decoding=\"async\" fetchpriority=\"high\"/\u003e\n        \u003c/p\u003e\n        \n      \u003c/div\u003e\n      \u003c/div\u003e\n\u003c/header\u003e\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \n\u003cp\u003eRecently, AI researcher Simon Willison wanted to add up his charges from using a cloud service, but the payment values and dates he needed were scattered among a dozen separate emails. Inputting them manually would have been tedious, so he turned to a technique he calls \u0026#34;video scraping,\u0026#34; which involves feeding a screen recording video into an AI model, similar to ChatGPT, for data extraction purposes.\u003c/p\u003e\n\u003cp\u003eWhat he discovered seems simple on its surface, but the quality of the result has deeper implications for the future of AI assistants, which may soon be able to see and interact with what we\u0026#39;re doing on our computer screens.\u003c/p\u003e\n\u003cp\u003e\u0026#34;The other day I found myself needing to add up some numeric values that were scattered across twelve different emails,\u0026#34; Willison wrote in a \u003ca href=\"https://simonwillison.net/2024/Oct/17/video-scraping/\"\u003edetailed post\u003c/a\u003e on his blog. He recorded a 35-second video scrolling through the relevant emails, then fed that video into \u003ca href=\"https://aistudio.google.com\"\u003eGoogle\u0026#39;s AI Studio\u003c/a\u003e tool, which allows people to experiment with several versions of Google\u0026#39;s \u003ca href=\"https://arstechnica.com/information-technology/2024/02/google-upstages-itself-with-gemini-1-5-ai-launch-one-week-after-ultra-1-0/\"\u003eGemini 1.5 Pro\u003c/a\u003e and Gemini 1.5 Flash AI models.\u003c/p\u003e\n\u003cp\u003eWillison then asked Gemini to pull the price data from the video and arrange it into a special data format called \u003ca href=\"https://en.wikipedia.org/wiki/JSON\"\u003eJSON\u003c/a\u003e (JavaScript Object Notation) that included dates and dollar amounts. The AI model successfully extracted the data, which Willison then formatted as \u003ca href=\"https://en.wikipedia.org/wiki/Comma-separated_values\"\u003eCSV\u003c/a\u003e (comma-separated values) table for spreadsheet use. After double-checking for errors as part of his experiment, the \u003ca href=\"https://cloud.google.com/blog/products/ai-machine-learning/the-needle-in-the-haystack-test-and-how-gemini-pro-solves-it\"\u003eaccuracy\u003c/a\u003e of the results—and what the video analysis cost to run—surprised him.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cdiv\u003e\n              \u003cp\u003e\u003ca data-pswp-width=\"1440\" data-pswp-height=\"1129\" data-pswp-srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-300x235.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-640x502.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-768x602.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-1536x1204.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-980x768.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-1440x1129.jpg 1440w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping.jpg 1620w\" data-cropped=\"true\" href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-1440x1129.jpg\" target=\"_blank\"\u003e\n                \u003cimg decoding=\"async\" width=\"1620\" height=\"1270\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping.jpg\" alt=\"A screenshot of Simon Willison using Google Gemini to extract data from a screen capture video.\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping.jpg 1620w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-300x235.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-640x502.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-768x602.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-1536x1204.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-980x768.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/video-scraping-1440x1129.jpg 1440w\" sizes=\"(max-width: 1620px) 100vw, 1620px\"/\u003e\n              \u003c/a\u003e\u003c/p\u003e\u003cdiv id=\"caption-2057114\"\u003e\u003cp\u003e\n                A screenshot of Simon Willison using Google Gemini to extract data from a screen capture video.\n                                  \u003c/p\u003e\n                              \u003c/div\u003e\n            \u003c/div\u003e\n                  \u003cfigcaption\u003e\n          \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A screenshot of Simon Willison using Google Gemini to extract data from a screen capture video.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://simonwillison.net/2024/Oct/17/video-scraping/\"\u003e\n          \n          Simon Willison\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n        \u003c/figcaption\u003e\n            \u003c/figure\u003e\n\n\u003cp\u003e\u0026#34;The cost [of running the video model] is so low that I had to re-run my calculations three times to make sure I hadn’t made a mistake,\u0026#34; he wrote. Willison says the entire video analysis process ostensibly cost less than one-tenth of a cent, using just 11,018 tokens on the \u003ca href=\"https://arstechnica.com/information-technology/2024/09/major-ai-updates-from-meta-and-google-and-a-new-era-for-ai-designed-chips/\"\u003eGemini 1.5 Flash 002\u003c/a\u003e model. In the end, he actually paid nothing because Google AI Studio is currently free for some types of use.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eVideo scraping is just one of many new tricks possible when the latest large language models (LLMs), such as Google\u0026#39;s Gemini and \u003ca href=\"https://arstechnica.com/information-technology/2024/05/chatgpt-4o-lets-you-have-real-time-audio-video-conversations-with-emotional-chatbot/\"\u003eGPT-4o\u003c/a\u003e, are actually \u0026#34;multimodal\u0026#34; models, allowing audio, video, image, and text input. These models translate any multimedia input into tokens (chunks of data), which they use to make predictions about which tokens should come next in a sequence.\u003c/p\u003e\n\u003cp\u003eA term like \u0026#34;token prediction model\u0026#34; (TPM) might be more accurate than \u0026#34;LLM\u0026#34; \u003ca href=\"https://x.com/karpathy/status/1835024197506187617\"\u003ethese days\u003c/a\u003e for AI models with multimodal inputs and outputs, but a generalized alternative term hasn\u0026#39;t really taken off yet. But no matter what you call it, having an AI model that can take video inputs has interesting implications, both good and potentially bad.\u003c/p\u003e\n\u003ch2\u003eBreaking down input barriers\u003c/h2\u003e\n\u003cp\u003eWillison is far from the first person to feed video into AI models to achieve interesting results (more on that below, and here\u0026#39;s a \u003ca href=\"https://baolingfeng.github.io/papers/ICSE2015Demo.pdf\"\u003e2015 paper\u003c/a\u003e that uses the \u0026#34;video scraping\u0026#34; term), but as soon as Gemini launched its video input capability, he began to experiment with it in earnest.\u003c/p\u003e\n\u003cp\u003eIn February, Willison \u003ca href=\"https://simonwillison.net/2024/Feb/21/gemini-pro-video/\"\u003edemonstrated\u003c/a\u003e another early application of AI video scraping on his blog, where he took a seven-second video of the books on his bookshelves, then got Gemini 1.5 Pro to extract all of the book titles it saw in the video and put them in a structured, or organized, list.\u003c/p\u003e\n\u003cp\u003eConverting unstructured data into structured data is important to Willison, because he\u0026#39;s also a \u003ca href=\"https://en.wikipedia.org/wiki/Data_journalism\"\u003edata journalist\u003c/a\u003e. Willison has created tools for data journalists in the past, such as the \u003ca href=\"https://datasette.io/\"\u003eDatasette project\u003c/a\u003e, which lets anyone publish data as an interactive website.\u003c/p\u003e\n\u003cp\u003eTo every data journalist\u0026#39;s frustration, some sources of data prove resistant to scraping (capturing data for analysis) due to how the data is formatted, stored, or presented. In these cases, Willison delights in the potential for AI video scraping because it bypasses these traditional barriers to data extraction.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003e\u0026#34;There\u0026#39;s no level of website authentication or anti-scraping technology that can stop me from recording a video of my screen while I manually click around inside a web application,\u0026#34; Willison noted on his blog. His method works for any visible on-screen content.\u003c/p\u003e\n\n\u003ch2\u003eVideo is the new text\u003c/h2\u003e\n\u003cfigure\u003e\n    \u003cdiv\u003e\n              \u003cp\u003e\u003ca data-pswp-width=\"1200\" data-pswp-height=\"675\" data-pswp-srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero.jpg 1200w\" data-cropped=\"true\" href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero.jpg\" target=\"_blank\"\u003e\n                \u003cimg decoding=\"async\" width=\"1200\" height=\"675\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero.jpg\" alt=\"An illustration of a cybernetic eyeball.\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero.jpg 1200w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2023/09/cyber_eyeball_hero-980x551.jpg 980w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e\n              \u003c/a\u003e\u003c/p\u003e\u003cdiv id=\"caption-1970818\"\u003e\u003cp\u003e\n                An illustration of a cybernetic eyeball.\n                                  \u003c/p\u003e\n                              \u003c/div\u003e\n            \u003c/div\u003e\n                  \u003cfigcaption\u003e\n          \u003cdiv\u003e\n    \n    \u003cp\u003e\n      An illustration of a cybernetic eyeball.\n\n              \u003cspan\u003e\n          Credit:\n\n                      \u003ca href=\"https://www.gettyimages.com/detail/illustration/artificial-intelligence-digital-eye-cyber-royalty-free-illustration/1488237506\"\u003e\n          \n          Getty Images\n\n                      \u003c/a\u003e\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n        \u003c/figcaption\u003e\n            \u003c/figure\u003e\n\n\u003cp\u003eThe ease and effectiveness of Willison\u0026#39;s technique reflect a noteworthy shift now underway in how some users will interact with token prediction models. Rather than requiring a user to manually paste or type in data in a chat dialog—or detail every scenario to a chatbot as text—some AI applications increasingly work with visual data captured directly on the screen. For example, if you\u0026#39;re having trouble navigating a pizza website\u0026#39;s terrible interface, an AI model could step in and \u003ca href=\"https://x.com/sawyerhood/status/1842225025501553044\"\u003eperform the necessary mouse clicks\u003c/a\u003e to order the pizza for you.\u003c/p\u003e\n\u003cp\u003eIn fact, video scraping is already on the radar of every major AI lab, although they are not likely to call it that at the moment. Instead, tech companies typically refer to these techniques as \u0026#34;\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding\"\u003evideo understanding\u003c/a\u003e\u0026#34; or simply \u0026#34;\u003ca href=\"https://docs.anthropic.com/en/docs/build-with-claude/vision\"\u003evision\u003c/a\u003e.\u0026#34;\u003c/p\u003e\n\u003cp\u003eIn May, OpenAI demonstrated a prototype version of its \u003ca href=\"https://arstechnica.com/gadgets/2024/06/openais-chatgpt-for-mac-is-now-available-to-all-users/\"\u003eChatGPT Mac App\u003c/a\u003e with an option that allowed ChatGPT to see and interact with what is on your screen, but that feature has not yet shipped. Microsoft demonstrated a similar \u0026#34;\u003ca href=\"https://arstechnica.com/ai/2024/10/microsofts-new-copilot-vision-ai-experiment-can-see-what-you-browse/\"\u003eCopilot Vision\u003c/a\u003e\u0026#34; prototype concept earlier this month (based on OpenAI\u0026#39;s technology) that will be able to \u0026#34;watch\u0026#34; your screen and help you extract data and interact with applications you\u0026#39;re running.\u003c/p\u003e\n\u003cp\u003eDespite these research previews, OpenAI\u0026#39;s \u003ca href=\"https://arstechnica.com/information-technology/2023/11/chatgpt-was-the-spark-that-lit-the-fire-under-generative-ai-one-year-ago-today/\"\u003eChatGPT\u003c/a\u003e and Anthropic\u0026#39;s \u003ca href=\"https://arstechnica.com/information-technology/2024/06/anthropics-latest-best-ai-model-is-twice-as-fast-and-still-terrible-at-dad-jokes/\"\u003eClaude\u003c/a\u003e have not yet implemented a public video input feature for their models, possibly because it is relatively computationally expensive for them to process the extra tokens from a \u0026#34;tokenized\u0026#34; video stream.\u003c/p\u003e\n\u003cp\u003eFor the moment, Google is heavily subsidizing user AI costs with its war chest from Search revenue and a massive fleet of data centers (to be fair, OpenAI is subsidizing, too, but with investor dollars and help from Microsoft). But costs of AI compute in general are dropping by the day, which will open up new capabilities of the technology to a broader user base over time.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003ch2\u003eCountering privacy issues\u003c/h2\u003e\n\u003cp\u003eAs you might imagine, having an AI model see what you do on your computer screen can have downsides. For now, video scraping is great for Willison, who will undoubtedly use the captured data in positive and helpful ways. But it\u0026#39;s also a preview of a capability that could later be used to invade privacy or autonomously \u003ca href=\"https://arstechnica.com/information-technology/2023/12/due-to-ai-we-are-about-to-enter-the-era-of-mass-spying-says-bruce-schneier/\"\u003espy on computer users\u003c/a\u003e on a scale that was once impossible.\u003c/p\u003e\n\u003cp\u003eA different form of video scraping caused a massive wave of controversy recently for that exact reason. Apps such as the third-party \u003ca href=\"https://arstechnica.com/information-technology/2022/11/new-mac-app-wants-to-record-everything-you-do-so-you-can-rewind-it-later/\"\u003eRewind AI\u003c/a\u003e on the Mac and \u003ca href=\"https://arstechnica.com/gadgets/2024/05/microsofts-new-recall-feature-will-record-everything-you-do-on-your-pc/\"\u003eMicrosoft\u0026#39;s Recall\u003c/a\u003e, which is being built into Windows 11, operate by feeding on-screen video into an AI model that stores extracted data into a database for later AI recall. Unfortunately, that approach also introduces potential privacy issues because it records everything you do on your machine and puts it in a single place that could later be hacked.\u003c/p\u003e\n\u003cfigure\u003e\n    \u003cdiv\u003e\n              \u003cp\u003e\u003ca data-pswp-width=\"1200\" data-pswp-height=\"675\" data-pswp-srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2.jpg 1200w\" data-cropped=\"true\" href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2.jpg\" target=\"_blank\"\u003e\n                \u003cimg decoding=\"async\" width=\"1200\" height=\"675\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2.jpg\" alt=\"An illustration of a woman standing in front of a large eyeball.\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2.jpg 1200w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-300x169.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2023/12/eyeball_surveillance_2-980x551.jpg 980w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e\n              \u003c/a\u003e\u003c/p\u003e\n            \u003c/div\u003e\n                  \u003cfigcaption\u003e\n          \n        \u003c/figcaption\u003e\n            \u003c/figure\u003e\n\n\u003cp\u003eTo that point, although Willison\u0026#39;s technique currently involves uploading a video of his data to Google for processing, he is pleased that he can still decide what the AI model sees and when.\u003c/p\u003e\n\u003cp\u003e\u0026#34;The great thing about this video scraping technique is that it works with anything that you can see on your screen... and it puts you in total control of what you end up exposing to the AI model,\u0026#34; Willison explained in his blog post.\u003c/p\u003e\n\u003cp\u003eIt\u0026#39;s also possible in the future that a locally run open-weights AI model could pull off the same video analysis method without the need for a cloud connection at all. Microsoft Recall runs locally on supported devices, but it still demands a \u003ca href=\"https://arstechnica.com/ai/2024/06/windows-recall-demands-an-extraordinary-level-of-trust-that-microsoft-hasnt-earned/\"\u003egreat deal of unearned trust\u003c/a\u003e. For now, Willison is perfectly content to selectively feed video data to AI models when the need arises.\u003c/p\u003e\n\u003cp\u003e\u0026#34;I expect I’ll be using this technique a whole lot more in the future,\u0026#34; he wrote, and perhaps many others will, too, in different forms. If the past is any indication, Willison—who \u003ca href=\"https://arstechnica.com/information-technology/2022/09/twitter-pranksters-derail-gpt-3-bot-with-newly-discovered-prompt-injection-hack/\"\u003ecoined the term\u003c/a\u003e \u0026#34;prompt injection\u0026#34; in 2022—seems to always be a few steps ahead in exploring novel applications of AI tools. Right now, his attention is on the new implications of AI and video, and yours probably should be, too.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/benjedwards/\"\u003e\u003cimg src=\"https://arstechnica.com/wp-content/uploads/2022/08/benj_ega.png\" alt=\"Photo of Benj Edwards\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Benj Edwards is Ars Technica\u0026#39;s Senior AI Reporter and founder of the site\u0026#39;s dedicated AI beat in 2022. He\u0026#39;s also a widely-cited tech historian. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \n\n\n  \n\n\n  \n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n\u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-167171901-768x432.jpg\" alt=\"Listing image for first story in Most Read: There’s another massive meat recall over Listeria—and it’s a doozy\" decoding=\"async\" loading=\"lazy\"/\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2024-10-17T22:41:39Z",
  "modifiedTime": "2024-10-17T22:41:39Z"
}
