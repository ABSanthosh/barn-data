{
  "id": "798b2c72-c32e-4b51-92b5-4b63c41999cf",
  "title": "Mistral OCR",
  "link": "https://mistral.ai/fr/news/mistral-ocr",
  "description": "Comments",
  "author": "",
  "published": "Thu, 06 Mar 2025 17:39:39 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 6970,
  "excerpt": "Introducing the world’s best document understanding API.",
  "siteName": "",
  "favicon": "https://mistral.ai/apple-icon.png?9838c5883fd9fccb",
  "text": "Throughout history, advancements in information abstraction and retrieval have driven human progress. From hieroglyphs to papyri, the printing press to digitization, each leap has made human knowledge more accessible and actionable, fueling further innovation.  Today, we’re at the precipice of the next big leap—to unlock the collective intelligence of all digitized information. Approximately 90% of the world’s organizational data is stored as documents, and to harness this potential, we are introducing Mistral OCR. Mistral OCR is an Optical Character Recognition API that sets a new standard in document understanding. Unlike other models, Mistral OCR comprehends each element of documents—media, text, tables, equations—with unprecedented accuracy and cognition. It takes images and PDFs as input and extracts content in an ordered interleaved text and images. As a result, Mistral OCR is an ideal model to use in combination with a RAG system taking multimodal documents (such as slides or complex PDFs) as input. We have made Mistral OCR as the default model for document understanding across millions of users on Le Chat, and are releasing the API mistral-ocr-latest at 1000 pages / $ (and approximately double the pages per dollar with batch inference). The API is available today on our developer suite la Plateforme, and coming soon to our cloud and inference partners, as well as on-premises. Highlights State of the art understanding of complex documents Natively multilingual and multimodal Top-tier benchmarks Fastest in its category Doc-as-prompt, structured output Selectively available to self-host for organizations dealing with highly sensitive or classified information Let’s dive into each.  State of the art understanding of complex documents Mistral OCR excels in understanding complex document elements, including interleaved imagery, mathematical expressions, tables, and advanced layouts such as LaTeX formatting. The model enables deeper understanding of rich documents such as scientific papers with charts, graphs, equations and figures.  Below is an example of the model extracting text as well as imagery from a given PDF into a markdown file. You can access the notebook here.  Below we have side-by-side comparisons of PDFs and their respective OCR's outputs. Hover the slider  to switch between input and output.  Top-tier benchmarks Mistral OCR has consistently outperformed other leading OCR models in rigorous benchmark tests. Its superior accuracy across multiple aspects of document analysis is illustrated below. We extract embedded images from documents along with text. The other LLMs compared below, do not have that capability. For a fair comparison, we evaluate them on our internal “text-only” test-set containing various publication papers, and PDFs from the web; below: Model Overall Math Multilingual Scanned Tables Google Document AI 83.42 80.29 86.42 92.77 78.16 Azure OCR 89.52 85.72 87.52 94.65 89.52 Gemini-1.5-Flash-002 90.23 89.11 86.76 94.87 90.48 Gemini-1.5-Pro-002 89.92 88.48 86.33 96.15 89.71 Gemini-2.0-Flash-001 88.69 84.18 85.80 95.11 91.46 GPT-4o-2024-11-20 89.77 87.55 86.00 94.58 91.70 Mistral OCR 2503 94.89 94.29 89.55 98.96 96.12 Natively multilingual Since Mistral’s founding, we have aspired to serve the world with our models, and consequently strived for multilingual capabilities across our offerings. Mistral OCR takes this to a new level, being able to parse, understand, and transcribe thousands of scripts, fonts, and languages across all continents. This versatility is crucial for both global organizations that handle documents from diverse linguistic backgrounds, as well as hyperlocal businesses serving niche markets. Model Fuzzy Match in Generation Google-Document-AI 95.88 Gemini-2.0-Flash-001 96.53 Azure OCR 97.31 Mistral OCR 99.02 Language Azure OCR Google Doc AI Mistral OCR ru 97.35 95.56 99.09 fr 97.50 96.36 99.20 hi 96.45 95.65 97.55 zh 91.40 90.89 97.11 pt 97.96 96.24 99.42 de 98.39 97.09 99.51 es 98.54 97.52 99.54 tr 95.91 93.85 97.00 uk 97.81 96.24 99.29 it 98.31 97.69 99.42 ro 96.45 95.14 98.79 Fastest in its category Being lighter weight than most models in the category, Mistral OCR performs significantly faster than its peers, processing up to 2000 pages per minute on a single node. The ability to rapidly process documents ensures continuous learning and improvement even for high-throughput environments. Doc-as-prompt, structured output Mistral OCR also introduces the use of documents as prompts, enabling more powerful and precise instructions. This capability allows users to extract specific information from documents and format it in structured outputs, such as JSON. Users can chain extracted outputs into downstream function calls and build agents. Available to self-host on a selective basis For organizations with stringent data privacy requirements, Mistral OCR offers a self-hosting option. This ensures that sensitive or classified information remains secure within your own infrastructure, providing compliance with regulatory and security standards. If you would like to explore self-deployment with us, please let us know. Use cases We are empowering our beta customers to elevate their organizational knowledge by transforming their extensive document repositories into actions and solutions. Some of the key use cases where our technology is making a significant impact include: Digitizing scientific research: Leading research institutions have been experimenting with Mistral OCR to convert scientific papers and journals into AI-ready formats, making them accessible to downstream intelligence engines. This has facilitated measurably faster collaboration and accelerated scientific workflows. Preserving historical and cultural heritage: Organizations and nonprofits that are custodians of heritage have been using Mistral OCR to digitize historical documents and artifacts, ensuring their preservation and making them accessible to a broader audience. Streamlining customer service: Customer service departments are exploring Mistral OCR to transform documentation and manuals into indexed knowledge, reducing response times and improving customer satisfaction. Making literature across design, education, legal, etc. AI ready: Mistral OCR has also been helping companies convert technical literature, engineering drawings, lecture notes, presentations, regulatory filings and much more into indexed, answer-ready formats, unlocking intelligence and productivity across millions of documents. Experience it today Mistral OCR capabilities are free to try on le Chat. To try the API, head over to la Plateforme. We’d love to get your feedback; expect the model to continue to get even better in the weeks to come. As part of our strategic engagement programs, we will also offer on-premises deployment on a selective basis.",
  "image": "https://cms.mistral.ai/assets/060bdeb1-fbff-419c-b2ae-b32b5e441864",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp dir=\"ltr\"\u003eThroughout history, advancements in information abstraction and retrieval have driven human progress. From hieroglyphs to papyri, the printing press to digitization, each leap has made human knowledge more accessible and actionable, fueling further innovation. \u003c/p\u003e\n\u003cdiv dir=\"ltr\"\u003e\u003cp\u003eToday, we’re at the precipice of the next big leap—to unlock the collective intelligence of all digitized information. Approximately \u003ca href=\"https://resources.data.gov/glossary/unstructured-data/\"\u003e90%\u003c/a\u003e of the world’s organizational data is stored as documents, and to harness this potential, we are introducing \u003ca href=\"https://docs.mistral.ai/capabilities/document/\"\u003eMistral OCR\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eMistral OCR is an Optical Character Recognition API that sets a new standard in document understanding. Unlike other models, Mistral OCR comprehends each element of documents—media, text, tables, equations—with unprecedented accuracy and cognition. It takes images and PDFs as input and extracts content in an ordered interleaved text and images.\u003c/p\u003e\u003c/div\u003e\n\u003cp dir=\"ltr\"\u003eAs a result, Mistral OCR is an ideal model to use in combination with a RAG system taking multimodal documents (such as slides or complex PDFs) as input.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe have made Mistral OCR as the default model for document understanding across millions of users on Le Chat, and are releasing the API \u003cem\u003emistral-ocr-latest\u003c/em\u003e at 1000 pages / $ (and approximately double the pages per dollar with batch inference). The API is available today on our developer suite \u003ca href=\"http://console.mistral.ai\"\u003ela Plateforme\u003c/a\u003e, and coming soon to our cloud and inference partners, as well as on-premises.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eHighlights\u003c/h2\u003e\n\u003col\u003e\n\u003cli dir=\"ltr\" aria-level=\"1\"\u003e\n\u003cp dir=\"ltr\" role=\"presentation\"\u003eState of the art understanding of complex documents\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\" aria-level=\"1\"\u003e\n\u003cp dir=\"ltr\" role=\"presentation\"\u003eNatively multilingual and multimodal\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\" aria-level=\"1\"\u003e\n\u003cp dir=\"ltr\" role=\"presentation\"\u003eTop-tier benchmarks\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\" aria-level=\"1\"\u003e\n\u003cp dir=\"ltr\" role=\"presentation\"\u003eFastest in its category\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\" aria-level=\"1\"\u003e\n\u003cp dir=\"ltr\" role=\"presentation\"\u003eDoc-as-prompt, structured output\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\" aria-level=\"1\"\u003e\n\u003cp dir=\"ltr\" role=\"presentation\"\u003eSelectively available to self-host for organizations dealing with highly sensitive or classified information\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"ltr\"\u003eLet’s dive into each. \u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eState of the art understanding of complex documents\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eMistral OCR excels in understanding complex document elements, including interleaved imagery, mathematical expressions, tables, and advanced layouts such as LaTeX formatting. The model enables deeper understanding of rich documents such as scientific papers with charts, graphs, equations and figures. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eBelow is an example of the model extracting text as well as imagery from a given PDF into a markdown file. You can access the notebook \u003ca href=\"https://colab.research.google.com/drive/11NdqWVwC_TtJyKT6cmuap4l9SryAeeVt?usp=sharing\" target=\"_blank\" rel=\"noopener\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003ciframe title=\"YouTube video player\" src=\"https://www.youtube.com/embed/6lRBm0KnzBI?si=qLSblC2rsBdxg4qu\" width=\"560\" height=\"315\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"allowfullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eBelow we have side-by-side comparisons of PDFs and their respective OCR\u0026#39;s outputs. Hover the slider  to switch between input and output. \u003c/p\u003e\n\n\n\n\n\n\u003ch3 dir=\"ltr\"\u003eTop-tier benchmarks\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eMistral OCR has consistently outperformed other leading OCR models in rigorous benchmark tests. Its superior accuracy across multiple aspects of document analysis is illustrated below. We extract embedded images from documents along with text. The other LLMs compared below, do not have that capability. For a fair comparison, we evaluate them on our internal “text-only” test-set containing various publication papers, and PDFs from the web; below:\u003c/p\u003e\n\u003cdiv dir=\"ltr\"\u003e\n\u003ctable\u003e\u003ccolgroup\u003e \u003ccol width=\"178\"/\u003e \u003ccol width=\"105\"/\u003e \u003ccol width=\"108\"/\u003e \u003ccol width=\"106\"/\u003e \u003ccol width=\"103\"/\u003e \u003ccol width=\"101\"/\u003e \u003c/colgroup\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eModel\u003c/th\u003e\n\u003cth\u003eOverall\u003c/th\u003e\n\u003cth\u003eMath\u003c/th\u003e\n\u003cth\u003eMultilingual\u003c/th\u003e\n\u003cth\u003eScanned\u003c/th\u003e\n\u003cth\u003eTables\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eGoogle Document AI\u003c/td\u003e\n\u003ctd\u003e83.42\u003c/td\u003e\n\u003ctd\u003e80.29\u003c/td\u003e\n\u003ctd\u003e86.42\u003c/td\u003e\n\u003ctd\u003e92.77\u003c/td\u003e\n\u003ctd\u003e78.16\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAzure OCR\u003c/td\u003e\n\u003ctd\u003e89.52\u003c/td\u003e\n\u003ctd\u003e85.72\u003c/td\u003e\n\u003ctd\u003e87.52\u003c/td\u003e\n\u003ctd\u003e94.65\u003c/td\u003e\n\u003ctd\u003e89.52\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eGemini-1.5-Flash-002\u003c/td\u003e\n\u003ctd\u003e90.23\u003c/td\u003e\n\u003ctd\u003e89.11\u003c/td\u003e\n\u003ctd\u003e86.76\u003c/td\u003e\n\u003ctd\u003e94.87\u003c/td\u003e\n\u003ctd\u003e90.48\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eGemini-1.5-Pro-002\u003c/td\u003e\n\u003ctd\u003e89.92\u003c/td\u003e\n\u003ctd\u003e88.48\u003c/td\u003e\n\u003ctd\u003e86.33\u003c/td\u003e\n\u003ctd\u003e96.15\u003c/td\u003e\n\u003ctd\u003e89.71\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eGemini-2.0-Flash-001\u003c/td\u003e\n\u003ctd\u003e88.69\u003c/td\u003e\n\u003ctd\u003e84.18\u003c/td\u003e\n\u003ctd\u003e85.80\u003c/td\u003e\n\u003ctd\u003e95.11\u003c/td\u003e\n\u003ctd\u003e91.46\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eGPT-4o-2024-11-20\u003c/td\u003e\n\u003ctd\u003e89.77\u003c/td\u003e\n\u003ctd\u003e87.55\u003c/td\u003e\n\u003ctd\u003e86.00\u003c/td\u003e\n\u003ctd\u003e94.58\u003c/td\u003e\n\u003ctd\u003e91.70\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMistral OCR 2503\u003c/td\u003e\n\u003ctd\u003e94.89\u003c/td\u003e\n\u003ctd\u003e94.29\u003c/td\u003e\n\u003ctd\u003e89.55\u003c/td\u003e\n\u003ctd\u003e98.96\u003c/td\u003e\n\u003ctd\u003e96.12\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\u003ch3 dir=\"ltr\"\u003eNatively multilingual\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eSince Mistral’s founding, we have aspired to serve the world with our models, and consequently strived for multilingual capabilities across our offerings. Mistral OCR takes this to a new level, being able to parse, understand, and transcribe thousands of scripts, fonts, and languages across all continents. This versatility is crucial for both global organizations that handle documents from diverse linguistic backgrounds, as well as hyperlocal businesses serving niche markets.\u003c/p\u003e\n\u003cdiv dir=\"ltr\"\u003e\n\u003ctable\u003e\u003ccolgroup\u003e \u003ccol width=\"236\"/\u003e \u003ccol width=\"203\"/\u003e \u003c/colgroup\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eModel\u003c/th\u003e\n\u003cth\u003eFuzzy Match in Generation\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eGoogle-Document-AI\u003c/td\u003e\n\u003ctd\u003e95.88\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eGemini-2.0-Flash-001\u003c/td\u003e\n\u003ctd\u003e96.53\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAzure OCR\u003c/td\u003e\n\u003ctd\u003e97.31\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMistral OCR\u003c/td\u003e\n\u003ctd\u003e99.02\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n\n\u003cdiv dir=\"ltr\"\u003e\n\u003ctable\u003e\u003ccolgroup\u003e \u003ccol width=\"133\"/\u003e \u003ccol width=\"123\"/\u003e \u003ccol width=\"126\"/\u003e \u003ccol width=\"170\"/\u003e \u003c/colgroup\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eLanguage\u003c/th\u003e\n\u003cth\u003eAzure OCR\u003c/th\u003e\n\u003cth\u003eGoogle Doc AI\u003c/th\u003e\n\u003cth\u003eMistral OCR\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eru\u003c/td\u003e\n\u003ctd\u003e97.35\u003c/td\u003e\n\u003ctd\u003e95.56\u003c/td\u003e\n\u003ctd\u003e99.09\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003efr\u003c/td\u003e\n\u003ctd\u003e97.50\u003c/td\u003e\n\u003ctd\u003e96.36\u003c/td\u003e\n\u003ctd\u003e99.20\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ehi\u003c/td\u003e\n\u003ctd\u003e96.45\u003c/td\u003e\n\u003ctd\u003e95.65\u003c/td\u003e\n\u003ctd\u003e97.55\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ezh\u003c/td\u003e\n\u003ctd\u003e91.40\u003c/td\u003e\n\u003ctd\u003e90.89\u003c/td\u003e\n\u003ctd\u003e97.11\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ept\u003c/td\u003e\n\u003ctd\u003e97.96\u003c/td\u003e\n\u003ctd\u003e96.24\u003c/td\u003e\n\u003ctd\u003e99.42\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ede\u003c/td\u003e\n\u003ctd\u003e98.39\u003c/td\u003e\n\u003ctd\u003e97.09\u003c/td\u003e\n\u003ctd\u003e99.51\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ees\u003c/td\u003e\n\u003ctd\u003e98.54\u003c/td\u003e\n\u003ctd\u003e97.52\u003c/td\u003e\n\u003ctd\u003e99.54\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003etr\u003c/td\u003e\n\u003ctd\u003e95.91\u003c/td\u003e\n\u003ctd\u003e93.85\u003c/td\u003e\n\u003ctd\u003e97.00\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003euk\u003c/td\u003e\n\u003ctd\u003e97.81\u003c/td\u003e\n\u003ctd\u003e96.24\u003c/td\u003e\n\u003ctd\u003e99.29\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eit\u003c/td\u003e\n\u003ctd\u003e98.31\u003c/td\u003e\n\u003ctd\u003e97.69\u003c/td\u003e\n\u003ctd\u003e99.42\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ero\u003c/td\u003e\n\u003ctd\u003e96.45\u003c/td\u003e\n\u003ctd\u003e95.14\u003c/td\u003e\n\u003ctd\u003e98.79\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n\u003ch2 dir=\"ltr\"\u003eFastest in its category\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eBeing lighter weight than most models in the category, Mistral OCR performs significantly faster than its peers, processing up to 2000 pages per minute on a single node. The ability to rapidly process documents ensures continuous learning and improvement even for high-throughput environments.\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eDoc-as-prompt, structured output\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eMistral OCR also introduces the use of documents as prompts, enabling more powerful and precise instructions. This capability allows users to extract specific information from documents and format it in structured outputs, such as JSON. Users can chain extracted outputs into downstream function calls and build agents.\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eAvailable to self-host on a selective basis\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eFor organizations with stringent data privacy requirements, Mistral OCR offers a self-hosting option. This ensures that sensitive or classified information remains secure within your own infrastructure, providing compliance with regulatory and security standards. If you would like to explore self-deployment with us, please \u003ca href=\"https://mistral.ai/\"\u003elet us know\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eUse cases\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWe are empowering our beta customers to elevate their organizational knowledge by transforming their extensive document repositories into actions and solutions. Some of the key use cases where our technology is making a significant impact include:\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eDigitizing scientific research\u003c/strong\u003e: Leading research institutions have been experimenting with Mistral OCR to convert scientific papers and journals into AI-ready formats, making them accessible to downstream intelligence engines. This has facilitated measurably faster collaboration and accelerated scientific workflows.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003ePreserving historical and cultural heritage\u003c/strong\u003e: Organizations and nonprofits that are custodians of heritage have been using Mistral OCR to digitize historical documents and artifacts, ensuring their preservation and making them accessible to a broader audience.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eStreamlining customer service\u003c/strong\u003e: Customer service departments are exploring Mistral OCR to transform documentation and manuals into indexed knowledge, reducing response times and improving customer satisfaction.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eMaking literature across design, education, legal, etc. AI ready\u003c/strong\u003e: Mistral OCR has also been helping companies convert technical literature, engineering drawings, lecture notes, presentations, regulatory filings and much more into indexed, answer-ready formats, unlocking intelligence and productivity across millions of documents.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eExperience it today\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eMistral OCR capabilities are free to try on \u003ca href=\"http://chat.mistral.ai\"\u003ele Chat\u003c/a\u003e. To try the API, head over to \u003ca href=\"http://console.mistral.ai\"\u003ela Plateforme\u003c/a\u003e. We’d love to get your feedback; expect the model to continue to get even better in the weeks to come. As part of our strategic engagement programs, we will also offer on-premises deployment on a \u003ca href=\"https://mistral.ai/contact\"\u003eselective basis\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": null,
  "modifiedTime": null
}
