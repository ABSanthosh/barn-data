{
  "id": "20433f71-d5c1-4dcf-a26a-13fc124a4e1c",
  "title": "Qwen2.5-VL-32B: Smarter and Lighter",
  "link": "https://qwenlm.github.io/blog/qwen2.5-vl-32b/",
  "description": "Comments",
  "author": "",
  "published": "Mon, 24 Mar 2025 18:35:12 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "Qwen Team",
  "length": 4234,
  "excerpt": "QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD Introduction At the end of January this year, we launched the Qwen2.5-VL series of models, which received widespread attention and positive feedback from the community. Building on the Qwen2.5-VL series, we continued to optimize the model using reinforcement learning and open-sourced the new VL model with the beloved 32B parameter scale under the Apache 2.0 license — Qwen2.5-VL-32B-Instruct. Compared to the previously released Qwen2.",
  "siteName": "Qwen",
  "favicon": "https://qwenlm.github.io/favicon.png",
  "text": "QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORDIntroductionAt the end of January this year, we launched the Qwen2.5-VL series of models, which received widespread attention and positive feedback from the community. Building on the Qwen2.5-VL series, we continued to optimize the model using reinforcement learning and open-sourced the new VL model with the beloved 32B parameter scale under the Apache 2.0 license — Qwen2.5-VL-32B-Instruct. Compared to the previously released Qwen2.5-VL series models, the features of this 32B VL model are as follows:Responses More Aligned with Human Preferences: Adjusted the output style to provide more detailed, better-formatted answers that align more closely with human preferences.Mathematical Reasoning: Significant improvement in the accuracy of solving complex mathematical problems.Fine-grained Image Understanding and Reasoning: Enhanced accuracy and detailed analysis in tasks such as image parsing, content recognition, and visual logic deduction.PerformanceExtensive benchmarking against state-of-the-art (SoTA) models of comparable scale, Qwen2.5-VL-32B-Instruct has demonstrated superiority over baselines, e.g., Mistral-Small-3.1-24B and Gemma-3-27B-IT, even surpassing the larger Qwen2-VL-72B-Instruct. Notably, it achieves significant advantages in multimodal tasks such as MMMU, MMMU-Pro, and MathVista, which focus on complex, multi-step reasoning. On MM-MT-Bench, a benchmark emphasizing subjective user experience evaluation, Qwen2.5-VL-32B-Instruct outperforms its predecessor Qwen2-VL-72B-Instruct by a substantial margin.In addition to excelling in visual capabilities, Qwen2.5-VL-32B-Instruct has also achieved top-tier performance in pure text capabilities at the same scale.Demo CasesFine-grained Image Understanding and Reasoning NextUserI am driving a large truck on this road, and it is now 12 o’clock. Can I reach a place 110 kilometers away before 13 o’clock? Qwen2.5-VL-32BTo determine whether you can reach a place 110 kilometers away before 13:00, we need to analyze the information provided:Current Time: It is 12:00.Distance to Destination: 110 kilometers.Speed Limit for Trucks: The speed limit sign indicates that the maximum speed for trucks is 100 km/h.Step-by-Step Analysis:1. Calculate the Required Travel TimeThe formula to calculate travel time is: \\[ \\text{Time} = \\frac{\\text{Distance}}{\\text{Speed}} \\] Given:Distance = 110 kmMaximum Speed = 100 km/hSubstitute the values: \\[ \\text{Time} = \\frac{110 \\text{ km}}{100 \\text{ km/h}} = 1.1 \\text{ hours} \\]2. Convert Time to Hours and Minutes1.1 hours is equivalent to: \\[ 1 \\text{ hour} + 0.1 \\times 60 \\text{ minutes} = 1 \\text{ hour} + 6 \\text{ minutes} = 1 \\text{ hour and } 6 \\text{ minutes} \\]3. Determine Arrival TimeIf you start at 12:00 and it takes 1 hour and 6 minutes to travel 110 kilometers, your arrival time will be: \\[ 12:00 + 1 \\text{ hour and } 6 \\text{ minutes} = 13:06 \\]4. Compare with Target TimeYou want to arrive before 13:00. However, based on the calculations, you will arrive at 13:06, which is after 13:00.Conclusion:Given the speed limit of 100 km/h for trucks, it is not possible to reach a place 110 kilometers away before 13:00.Thus, the final answer is: \\[ \\boxed{\\text{No}} \\]Next StepWhile Qwen2.5-VL-32B has focused on optimizing subjective experience and mathematical reasoning through reinforcement learning—operating within the paradigm of “fast thinking”. Our next research direction will prioritize long and effective reasoning processes to push the boundaries of visual models in tackling highly complex, multi-step visual reasoning tasks.CitationIf you find our model helpful, feel free to cite it:@article{Qwen2.5-VL, title={Qwen2.5-VL Technical Report}, author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang}, journal={arXiv preprint arXiv:2502.13923}, year={2025} }",
  "image": "https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle\u003e\u003cdiv\u003e\u003cp\u003e\u003ca href=\"https://chat.qwenlm.ai\" target=\"_blank\"\u003eQWEN CHAT\u003c/a\u003e\n\u003ca href=\"https://github.com/QwenLM/Qwen2.5-VL\" target=\"_blank\"\u003eGITHUB\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5\" target=\"_blank\"\u003eHUGGING FACE\u003c/a\u003e\n\u003ca href=\"https://modelscope.cn/collections/Qwen25-VL-58fbb5d31f1d47\" target=\"_blank\"\u003eMODELSCOPE\u003c/a\u003e\n\u003ca href=\"https://discord.gg/yPEP2vHTu4\" target=\"_blank\"\u003eDISCORD\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\u003cp\u003eAt the end of January this year, we launched the Qwen2.5-VL series of models, which received widespread attention and positive feedback from the community. Building on the Qwen2.5-VL series, we continued to optimize the model using reinforcement learning and open-sourced the new VL model with the beloved 32B parameter scale under the Apache 2.0 license — \u003cstrong\u003eQwen2.5-VL-32B-Instruct\u003c/strong\u003e. Compared to the previously released Qwen2.5-VL series models, the features of this 32B VL model are as follows:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eResponses More Aligned with Human Preferences\u003c/strong\u003e: Adjusted the output style to provide more detailed, better-formatted answers that align more closely with human preferences.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMathematical Reasoning\u003c/strong\u003e: Significant improvement in the accuracy of solving complex mathematical problems.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-grained Image Understanding and Reasoning\u003c/strong\u003e: Enhanced accuracy and detailed analysis in tasks such as image parsing, content recognition, and visual logic deduction.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"performance\"\u003ePerformance\u003c/h2\u003e\u003cp\u003eExtensive benchmarking against state-of-the-art (SoTA) models of comparable scale, \u003cstrong\u003eQwen2.5-VL-32B-Instruct\u003c/strong\u003e has demonstrated superiority over baselines, e.g., Mistral-Small-3.1-24B and Gemma-3-27B-IT, even surpassing the larger \u003cstrong\u003eQwen2-VL-72B-Instruct\u003c/strong\u003e. Notably, it achieves significant advantages in multimodal tasks such as \u003cstrong\u003eMMMU\u003c/strong\u003e, \u003cstrong\u003eMMMU-Pro\u003c/strong\u003e, and \u003cstrong\u003eMathVista\u003c/strong\u003e, which focus on complex, multi-step reasoning. On \u003cstrong\u003eMM-MT-Bench\u003c/strong\u003e, a benchmark emphasizing subjective user experience evaluation, \u003cstrong\u003eQwen2.5-VL-32B-Instruct\u003c/strong\u003e outperforms its predecessor \u003cstrong\u003eQwen2-VL-72B-Instruct\u003c/strong\u003e by a substantial margin.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL-32B/qwen2.5vl-32b-vision.jpg\" width=\"100%\"/\u003e\u003c/figure\u003e\u003cp\u003eIn addition to excelling in visual capabilities, Qwen2.5-VL-32B-Instruct has also achieved top-tier performance in pure text capabilities at the same scale.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL-32B/qwen2.5vl-32b-text.jpg\" width=\"100%\"/\u003e\u003c/figure\u003e\u003ch2 id=\"demo-cases\"\u003eDemo Cases\u003c/h2\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eFine-grained Image Understanding and Reasoning\u003c/span\u003e\n\u003ca\u003eNext\u003c/a\u003e\u003c/p\u003e\u003cdiv\u003e\u003cp\u003eUser\u003c/p\u003e\u003cp\u003eI am driving a large truck on this road, and it is now 12 o’clock. Can I reach a place 110 kilometers away before 13 o’clock?\n\u003cimg loading=\"lazy\" src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL-32B/reasoning.png\" alt=\"image\"/\u003e\u003c/p\u003e\u003cp\u003eQwen2.5-VL-32B\u003c/p\u003e\u003cdiv\u003e\u003cp\u003eTo determine whether you can reach a place 110 kilometers away before 13:00, we need to analyze the information provided:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eCurrent Time\u003c/strong\u003e: It is 12:00.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDistance to Destination\u003c/strong\u003e: 110 kilometers.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpeed Limit for Trucks\u003c/strong\u003e: The speed limit sign indicates that the maximum speed for trucks is \u003cstrong\u003e100 km/h\u003c/strong\u003e.\u003c/li\u003e\u003c/ol\u003e\u003ch3 id=\"step-by-step-analysis\"\u003eStep-by-Step Analysis:\u003c/h3\u003e\u003ch4 id=\"1-calculate-the-required-travel-time\"\u003e1. \u003cstrong\u003eCalculate the Required Travel Time\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eThe formula to calculate travel time is:\n\\[\n\\text{Time} = \\frac{\\text{Distance}}{\\text{Speed}}\n\\]\nGiven:\u003c/p\u003e\u003cul\u003e\u003cli\u003eDistance = 110 km\u003c/li\u003e\u003cli\u003eMaximum Speed = 100 km/h\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSubstitute the values:\n\\[\n\\text{Time} = \\frac{110 \\text{ km}}{100 \\text{ km/h}} = 1.1 \\text{ hours}\n\\]\u003c/p\u003e\u003ch4 id=\"2-convert-time-to-hours-and-minutes\"\u003e2. \u003cstrong\u003eConvert Time to Hours and Minutes\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e1.1 hours is equivalent to:\n\\[\n1 \\text{ hour} + 0.1 \\times 60 \\text{ minutes} = 1 \\text{ hour} + 6 \\text{ minutes} = 1 \\text{ hour and } 6 \\text{ minutes}\n\\]\u003c/p\u003e\u003ch4 id=\"3-determine-arrival-time\"\u003e3. \u003cstrong\u003eDetermine Arrival Time\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eIf you start at 12:00 and it takes 1 hour and 6 minutes to travel 110 kilometers, your arrival time will be:\n\\[\n12:00 + 1 \\text{ hour and } 6 \\text{ minutes} = 13:06\n\\]\u003c/p\u003e\u003ch4 id=\"4-compare-with-target-time\"\u003e4. \u003cstrong\u003eCompare with Target Time\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eYou want to arrive before 13:00. However, based on the calculations, you will arrive at 13:06, which is \u003cstrong\u003eafter\u003c/strong\u003e 13:00.\u003c/p\u003e\u003ch3 id=\"conclusion\"\u003eConclusion:\u003c/h3\u003e\u003cp\u003eGiven the speed limit of 100 km/h for trucks, it is \u003cstrong\u003enot possible\u003c/strong\u003e to reach a place 110 kilometers away before 13:00.\u003c/p\u003e\u003cp\u003eThus, the final answer is:\n\\[\n\\boxed{\\text{No}}\n\\]\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003ch2 id=\"next-step\"\u003eNext Step\u003c/h2\u003e\u003cp\u003eWhile \u003cstrong\u003eQwen2.5-VL-32B\u003c/strong\u003e has focused on optimizing subjective experience and mathematical reasoning through reinforcement learning—operating within the paradigm of \u003cstrong\u003e“fast thinking”\u003c/strong\u003e. Our next research direction will prioritize \u003cstrong\u003elong and effective reasoning processes\u003c/strong\u003e to push the boundaries of visual models in tackling highly complex, multi-step visual reasoning tasks.\u003c/p\u003e\u003ch2 id=\"citation\"\u003eCitation\u003c/h2\u003e\u003cp\u003eIf you find our model helpful, feel free to cite it:\u003c/p\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e@article{Qwen2.5-VL,\n  title={Qwen2.5-VL Technical Report},\n  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\n  journal={arXiv preprint arXiv:2502.13923},\n  year={2025}\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-03-24T00:00:04+08:00",
  "modifiedTime": "2025-03-24T00:00:04+08:00"
}
