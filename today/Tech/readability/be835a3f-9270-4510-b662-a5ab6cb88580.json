{
  "id": "be835a3f-9270-4510-b662-a5ab6cb88580",
  "title": "Apple sued for failing to implement tools that would detect CSAM in iCloud",
  "link": "https://www.engadget.com/big-tech/apple-sued-for-failing-to-implement-tools-that-would-detect-csam-in-icloud-202940984.html?src=rss",
  "description": "Apple is being sued by victims of child sexual abuse over its failure to follow through with plans to scan iCloud for child sexual abuse materials (CSAM), The New York Times reports. In 2021, Apple announced it was working on a tool to detect CSAM that would flag images showing such abuse and notify the National Center for Missing and Exploited Children. But the company was hit with immediate backlash over the privacy implications of the technology, and ultimately abandoned the plan. The lawsuit, which was filed on Saturday in Northern California, is seeking damages upwards of $1.2 billion dollars for a potential group of 2,680 victims, according to NYT. It claims that, after Apple showed off its planned child safety tools, the company “failed to implement those designs or take any measures to detect and limit” CSAM on its devices, leading to the victims’ harm as the images continued to circulate.  In a statement shared with Engadget, Apple spokesperson Fred Sainz said, “Child sexual abuse material is abhorrent and we are committed to fighting the ways predators put children at risk. We are urgently and actively innovating to combat these crimes without compromising the security and privacy of all our users. Features like Communication Safety, for example, warn children when they receive or attempt to send content that contains nudity to help break the chain of coercion that leads to child sexual abuse. We remain deeply focused on building protections that help prevent the spread of CSAM before it starts.”  The lawsuit comes just a few months after Apple was accused of underreporting CSAM by the UK’s National Society for the Prevention of Cruelty to Children (NSPCC). Update, December 8 2024, 6:55PM ET: This story has been updated to include Apple's statement to Engadget.This article originally appeared on Engadget at https://www.engadget.com/big-tech/apple-sued-for-failing-to-implement-tools-that-would-detect-csam-in-icloud-202940984.html?src=rss",
  "author": "Cheyenne MacDonald",
  "published": "Sun, 08 Dec 2024 23:55:54 +0000",
  "source": "https://www.engadget.com/rss.xml",
  "categories": [
    "Crime \u0026 Justice",
    "site|engadget",
    "provider_name|Engadget",
    "region|US",
    "language|en-US",
    "author_name|Cheyenne MacDonald"
  ],
  "byline": "Cheyenne MacDonald",
  "length": 1795,
  "excerpt": "A lawsuit has been filed on behalf of a potential group of 2,680 victims, The New York Times reports.",
  "siteName": "Engadget",
  "favicon": "https://s.yimg.com/kw/assets/favicon-160x160.png",
  "text": "Apple is being sued by victims of child sexual abuse over its failure to follow through with plans to scan iCloud for child sexual abuse materials (CSAM), The New York Times reports. In 2021, Apple announced it was working on a tool to detect CSAM that would flag images showing such abuse and notify the National Center for Missing and Exploited Children. But the company was hit with immediate backlash over the privacy implications of the technology, and ultimately abandoned the plan.The lawsuit, which was filed on Saturday in Northern California, is seeking damages upwards of $1.2 billion dollars for a potential group of 2,680 victims, according to NYT. It claims that, after Apple showed off its planned child safety tools, the company “failed to implement those designs or take any measures to detect and limit” CSAM on its devices, leading to the victims’ harm as the images continued to circulate.In a statement shared with Engadget, Apple spokesperson Fred Sainz said, “Child sexual abuse material is abhorrent and we are committed to fighting the ways predators put children at risk. We are urgently and actively innovating to combat these crimes without compromising the security and privacy of all our users. Features like Communication Safety, for example, warn children when they receive or attempt to send content that contains nudity to help break the chain of coercion that leads to child sexual abuse. We remain deeply focused on building protections that help prevent the spread of CSAM before it starts.”The lawsuit comes just a few months after Apple was accused of underreporting CSAM by the UK’s National Society for the Prevention of Cruelty to Children (NSPCC).Update, December 8 2024, 6:55PM ET: This story has been updated to include Apple's statement to Engadget.",
  "image": "https://s.yimg.com/ny/api/res/1.2/rfZBHHvH4CTT4YIP7l_xKQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MjU7Y2Y9d2VicA--/https://s.yimg.com/os/creatr-uploaded-images/2024-12/55dc3900-b5a2-11ef-9f75-674942666a82",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eApple is being sued by victims of child sexual abuse over its failure to follow through with plans to scan iCloud for child sexual abuse materials (CSAM), \u003ca data-i13n=\"elm:context_link;elmt:doNotAffiliate;cpos:1;pos:1\" href=\"https://www.nytimes.com/2024/12/08/technology/apple-child-sexual-abuse-material-lawsuit.html\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:The New York Times;elm:context_link;elmt:doNotAffiliate;cpos:1;pos:1;itc:0;sec:content-canvas\"\u003e\u003cem\u003eThe New York Times\u003c/em\u003e\u003c/a\u003e reports. In 2021, Apple announced it was working on \u003ca data-i13n=\"elm:context_link;elmt:doNotAffiliate;cpos:2;pos:1\" href=\"https://www.engadget.com/apple-child-safety-ios-15-193820644.html\" data-ylk=\"slk:a tool to detect CSAM;elm:context_link;elmt:doNotAffiliate;cpos:2;pos:1;itc:0;sec:content-canvas\"\u003ea tool to detect CSAM\u003c/a\u003e that would flag images showing such abuse and notify the National Center for Missing and Exploited Children. But the company was hit with immediate backlash over the privacy implications of the technology, and ultimately \u003ca data-i13n=\"elm:context_link;elmt:doNotAffiliate;cpos:3;pos:1\" href=\"https://www.engadget.com/apple-advanced-data-protection-imessage-contact-key-183819737.html\" data-ylk=\"slk:abandoned the plan;elm:context_link;elmt:doNotAffiliate;cpos:3;pos:1;itc:0;sec:content-canvas\"\u003eabandoned the plan\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe lawsuit, which was filed on Saturday in Northern California, is seeking damages upwards of $1.2 billion dollars for a potential group of 2,680 victims, according to \u003cem\u003eNYT\u003c/em\u003e. It claims that, after Apple showed off its planned child safety tools, the company “failed to implement those designs or take any measures to detect and limit” CSAM on its devices, leading to the victims’ harm as the images continued to circulate.\u003c/p\u003e\u003cp\u003eIn a statement shared with Engadget, Apple spokesperson Fred Sainz said, “Child sexual abuse material is abhorrent and we are committed to fighting the ways predators put children at risk. We are urgently and actively innovating to combat these crimes without compromising the security and privacy of all our users. Features like Communication Safety, for example, warn children when they receive or attempt to send content that contains nudity to help break the chain of coercion that leads to child sexual abuse. We remain deeply focused on building protections that help prevent the spread of CSAM before it starts.”\u003c/p\u003e\u003cp\u003eThe lawsuit comes just a few months after Apple was \u003ca data-i13n=\"elm:context_link;elmt:doNotAffiliate;cpos:4;pos:1\" href=\"https://www.engadget.com/apple-accused-of-underreporting-suspected-csam-on-its-platforms-153637726.html\" data-ylk=\"slk:accused of underreporting CSAM;elm:context_link;elmt:doNotAffiliate;cpos:4;pos:1;itc:0;sec:content-canvas\"\u003eaccused of underreporting CSAM\u003c/a\u003e by the UK’s National Society for the Prevention of Cruelty to Children (NSPCC).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eUpdate, December 8 2024, 6:55PM ET:\u003c/strong\u003e This story has been updated to include Apple\u0026#39;s statement to Engadget.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2024-12-08T23:55:54Z",
  "modifiedTime": null
}
