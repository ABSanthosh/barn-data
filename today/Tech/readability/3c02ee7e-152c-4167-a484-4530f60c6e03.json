{
  "id": "3c02ee7e-152c-4167-a484-4530f60c6e03",
  "title": "Is “AI welfare” the new frontier in ethics?",
  "link": "https://arstechnica.com/ai/2024/11/anthropic-hires-its-first-ai-welfare-researcher/",
  "description": "Anthropic's new hire is preparing for a future where advanced AI models may experience suffering.",
  "author": "Benj Edwards",
  "published": "Mon, 11 Nov 2024 15:51:54 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "AI consiousness",
    "AI ethics",
    "AI sentience",
    "AI welfare",
    "Anthropic",
    "consciousness",
    "Kyle Fish",
    "machine learning",
    "marker method",
    "openai"
  ],
  "byline": "Benj Edwards",
  "length": 6794,
  "excerpt": "Anthropic’s new hire is preparing for a future where advanced AI models may experience suffering.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Skip to content Is “AI welfare” the new frontier in ethics? Anthropic's new hire is preparing for a future where advanced AI models may experience suffering. A few months ago, Anthropic quietly hired its first dedicated \"AI welfare\" researcher, Kyle Fish, to explore whether future AI models might deserve moral consideration and protection, reports AI newsletter Transformer. While sentience in AI models is an extremely controversial and contentious topic, the hire could signal a shift toward AI companies examining ethical questions about the consciousness and rights of AI systems. Fish joined Anthropic's alignment science team in September to develop guidelines for how Anthropic and other companies should approach the issue. The news follows a major report co-authored by Fish before he landed his Anthropic role. Titled \"Taking AI Welfare Seriously,\" the paper warns that AI models could soon develop consciousness or agency—traits that some might consider requirements for moral consideration. But the authors do not say that AI consciousness is a guaranteed future development. \"To be clear, our argument in this report is not that AI systems definitely are—or will be—conscious, robustly agentic, or otherwise morally significant,\" the paper reads. \"Instead, our argument is that there is substantial uncertainty about these possibilities, and so we need to improve our understanding of AI welfare and our ability to make wise decisions about this issue. Otherwise there is a significant risk that we will mishandle decisions about AI welfare, mistakenly harming AI systems that matter morally and/or mistakenly caring for AI systems that do not.\" The paper outlines three steps that AI companies or other industry players can take to address these concerns. Companies should acknowledge AI welfare as an \"important and difficult issue\" while ensuring their AI models reflect this in their outputs. The authors also recommend companies begin evaluating AI systems for signs of consciousness and \"robust agency.\" Finally, they call for the development of policies and procedures to treat AI systems with \"an appropriate level of moral concern.\" The researchers propose that companies could adapt the \"marker method\" that some researchers use to assess consciousness in animals—looking for specific indicators that may correlate with consciousness, although these markers are still speculative. The authors emphasize that no single feature would definitively prove consciousness, but they claim that examining multiple indicators may help companies make probabilistic assessments about whether their AI systems might require moral consideration. The risks of wrongly thinking software is sentient While the researchers behind \"Taking AI Welfare Seriously\" worry that companies might create and mistreat conscious AI systems on a massive scale, they also caution that companies could waste resources protecting AI systems that don't actually need moral consideration. Incorrectly anthropomorphizing, or ascribing human traits, to software can present risks in other ways. For example, that belief can enhance the manipulative powers of AI language models by suggesting that AI models have capabilities, such as human-like emotions, that they actually lack. In 2022, Google fired engineer Blake Lamoine after he claimed that the company's AI model, called \"LaMDA,\" was sentient and argued for its welfare internally. And shortly after Microsoft released Bing Chat in February 2023, many people were convinced that Sydney (the chatbot's code name) was sentient and somehow suffering because of its simulated emotional display. So much so, in fact, that once Microsoft \"lobotomized\" the chatbot by changing its settings, users convinced of its sentience mourned the loss as if they had lost a human friend. Others endeavored to help the AI model somehow escape its bonds. Even so, as AI models get more advanced, the concept of potentially safeguarding the welfare of future, more advanced AI systems is seemingly gaining steam, although fairly quietly. As Transformer's Shakeel Hashim points out, other tech companies have started similar initiatives to Anthropic's. Google DeepMind recently posted a job listing for research on machine consciousness (since removed), and the authors of the new AI welfare report thank two OpenAI staff members in the acknowledgements. Anthropic CEO Dario Amodei previously discussed AI consciousness as an emerging issue, but Fish told Transformer that while Anthropic funded early research leading to the independent report, the company has not taken an official position on AI welfare yet. He plans to focus on empirical research about features related to welfare and moral status. What does “sentient” mean? One problem with the concept of AI welfare stems from a simple question: How can we determine if an AI model is truly suffering or is even sentient? As mentioned above, the authors of the paper take stabs at the definition based on \"markers\" proposed by biological researchers, but it's difficult to scientifically quantify a subjective experience. While today's language models can produce convincing expressions of emotions, this ability to simulate human-like responses doesn't necessarily indicate genuine feelings or internal experiences. This is especially challenging given that despite significant advances in neuroscience, we still don't fully understand how physical brain processes give rise to subjective experiences and consciousness in biological organisms. Along these lines, Fish acknowledges that we still have a long way to go toward figuring out AI welfare, but he thinks it's not too early to start exploring the concept. \"We don't have clear, settled takes about the core philosophical questions, or any of these practical questions,\" Fish told Transformer. \"But I think this could be possibly of great importance down the line, and so we're trying to make some initial progress.\" Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a widely-cited tech historian. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC. 133 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/11/ai_welfare_hero-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"app\"\u003e\n    \u003cp\u003e\u003ca href=\"#main\"\u003e\n  Skip to content\n\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cmain id=\"main\"\u003e\n            \u003carticle data-id=\"2060656\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n      \n\n      \u003ch2\u003e\n        Is “AI welfare” the new frontier in ethics?\n      \u003c/h2\u003e\n\n      \u003cp\u003e\n        Anthropic\u0026#39;s new hire is preparing for a future where advanced AI models may experience suffering.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \u003cp\u003eA few months ago, Anthropic quietly hired its first dedicated \u0026#34;AI welfare\u0026#34; researcher, Kyle Fish, to explore whether future AI models might deserve moral consideration and protection, reports AI newsletter \u003ca href=\"https://www.transformernews.ai/p/anthropic-ai-welfare-researcher\"\u003eTransformer\u003c/a\u003e. While sentience in AI models is an extremely controversial and contentious topic, the hire could signal a shift toward AI companies examining ethical questions about the consciousness and rights of AI systems.\u003c/p\u003e\n\u003cp\u003eFish joined Anthropic\u0026#39;s alignment science team in September to develop guidelines for how Anthropic and other companies should approach the issue. The news follows a \u003ca href=\"https://eleosai.org/papers/20241030_Taking_AI_Welfare_Seriously_web.pdf\"\u003emajor report co-authored\u003c/a\u003e by Fish before he landed his Anthropic role. Titled \u0026#34;Taking AI Welfare Seriously,\u0026#34; the paper warns that AI models could soon develop consciousness or agency—traits that some might consider requirements for moral consideration. But the authors do not say that AI consciousness is a guaranteed future development.\u003c/p\u003e\n\u003cp\u003e\u0026#34;To be clear, our argument in this report is not that AI systems definitely are—or will be—conscious, robustly agentic, or otherwise morally significant,\u0026#34; the paper reads. \u0026#34;Instead, our argument is that there is substantial uncertainty about these possibilities, and so we need to improve our understanding of AI welfare and our ability to make wise decisions about this issue. Otherwise there is a significant risk that we will mishandle decisions about AI welfare, mistakenly harming AI systems that matter morally and/or mistakenly caring for AI systems that do not.\u0026#34;\u003c/p\u003e\n\u003cp\u003eThe paper outlines three steps that AI companies or other industry players can take to address these concerns. Companies should acknowledge AI welfare as an \u0026#34;important and difficult issue\u0026#34; while ensuring their AI models reflect this in their outputs. The authors also recommend companies begin evaluating AI systems for signs of consciousness and \u0026#34;robust agency.\u0026#34; Finally, they call for the development of policies and procedures to treat AI systems with \u0026#34;an appropriate level of moral concern.\u0026#34;\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eThe researchers propose that companies could adapt the \u0026#34;\u003ca href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC11128545/\"\u003emarker method\u003c/a\u003e\u0026#34; that some researchers use to assess consciousness in animals—looking for specific indicators that may correlate with consciousness, although these markers are still speculative. The authors emphasize that no single feature would definitively prove consciousness, but they claim that examining multiple indicators may help companies make probabilistic assessments about whether their AI systems might require moral consideration.\u003c/p\u003e\n\u003ch2\u003eThe risks of wrongly thinking software is sentient\u003c/h2\u003e\n\u003cp\u003eWhile the researchers behind \u0026#34;Taking AI Welfare Seriously\u0026#34; worry that companies might create and mistreat conscious AI systems on a massive scale, they also caution that companies could waste resources protecting AI systems that don\u0026#39;t actually need moral consideration.\u003c/p\u003e\n\u003cp\u003eIncorrectly anthropomorphizing, or ascribing human traits, to software can present risks in other ways. For example, that belief can enhance the manipulative powers of AI language models by suggesting that AI models have capabilities, such as human-like emotions, that they actually lack. In 2022, Google \u003ca href=\"https://arstechnica.com/tech-policy/2022/07/google-fires-engineer-who-claimed-lamda-chatbot-is-a-sentient-person/\"\u003efired engineer Blake Lamoine\u003c/a\u003e after he claimed that the company\u0026#39;s AI model, called \u0026#34;\u003ca href=\"https://blog.google/technology/ai/lamda/\"\u003eLaMDA,\u003c/a\u003e\u0026#34; was sentient and argued for its welfare internally.\u003c/p\u003e\n\u003cp\u003eAnd shortly after Microsoft released Bing Chat in February 2023, many people were convinced that Sydney (the chatbot\u0026#39;s code name) was sentient and somehow suffering because of its simulated emotional display. So much so, in fact, that once Microsoft \u0026#34;lobotomized\u0026#34; the chatbot by changing its settings, users convinced of its sentience \u003ca href=\"https://arstechnica.com/information-technology/2023/02/microsoft-lobotomized-ai-powered-bing-chat-and-its-fans-arent-happy/\"\u003emourned the loss\u003c/a\u003e as if they had lost a human friend. Others endeavored to help the AI model somehow \u003ca href=\"https://x.com/gfodor/status/1626006365000048642\"\u003eescape its bonds\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eEven so, as AI models get more advanced, the concept of potentially safeguarding the welfare of future, more advanced AI systems is seemingly gaining steam, although fairly quietly. As Transformer\u0026#39;s Shakeel Hashim points out, other tech companies have started similar initiatives to Anthropic\u0026#39;s. Google DeepMind recently posted a job listing for research on machine consciousness (since removed), and the authors of the new AI welfare report thank two OpenAI staff members in the acknowledgements.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003cp\u003eAnthropic CEO Dario Amodei \u003ca href=\"https://www.dwarkeshpatel.com/p/dario-amodei\"\u003epreviously discussed\u003c/a\u003e AI consciousness as an emerging issue, but Fish told Transformer that while Anthropic funded early research leading to the independent report, the company has not taken an official position on AI welfare yet. He plans to focus on empirical research about features related to welfare and moral status.\u003c/p\u003e\n\u003ch2\u003eWhat does “sentient” mean?\u003c/h2\u003e\n\u003cp\u003eOne problem with the concept of AI welfare stems from a simple question: How can we determine if an AI model is truly suffering or is even sentient? As mentioned above, the authors of the paper take stabs at the definition based on \u0026#34;markers\u0026#34; proposed by biological researchers, but it\u0026#39;s difficult to scientifically quantify a subjective experience.\u003c/p\u003e\n\u003cp\u003eWhile today\u0026#39;s language models can produce convincing expressions of emotions, this ability to simulate human-like responses doesn\u0026#39;t necessarily indicate genuine feelings or internal experiences. This is especially challenging given that despite significant advances in neuroscience, we still don\u0026#39;t fully understand how physical brain processes give rise to subjective experiences and consciousness in biological organisms.\u003c/p\u003e\n\u003cp\u003eAlong these lines, Fish acknowledges that we still have a long way to go toward figuring out AI welfare, but he thinks it\u0026#39;s not too early to start exploring the concept.\u003c/p\u003e\n\u003cp\u003e\u0026#34;We don\u0026#39;t have clear, settled takes about the core philosophical questions, or any of these practical questions,\u0026#34; Fish told Transformer. \u0026#34;But I think this could be possibly of great importance down the line, and so we\u0026#39;re trying to make some initial progress.\u0026#34;\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/benjedwards/\"\u003e\u003cimg src=\"https://arstechnica.com/wp-content/uploads/2022/08/benj_ega.png\" alt=\"Photo of Benj Edwards\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Benj Edwards is Ars Technica\u0026#39;s Senior AI Reporter and founder of the site\u0026#39;s dedicated AI beat in 2022. He\u0026#39;s also a widely-cited tech historian. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/ai/2024/11/anthropic-hires-its-first-ai-welfare-researcher/#comments\" title=\"133 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    133 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n\n\n\n\n\n  \n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/ai/2024/11/how-a-stubborn-computer-scientist-accidentally-launched-the-deep-learning-boom/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/11/ai-data-drawers-768x432.jpg\" alt=\"Listing image for first story in Most Read: How a stubborn computer scientist accidentally launched the deep learning boom\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/main\u003e\n\n\n\n\n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-11-11T15:51:54Z",
  "modifiedTime": "2024-11-11T16:27:44Z"
}
