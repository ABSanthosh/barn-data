{
  "id": "34a53e73-0fa5-42c1-9584-e22732a64525",
  "title": "Using ChatGPT to make fake social media posts backfires on bad actors",
  "link": "https://arstechnica.com/tech-policy/2024/10/using-chatgpt-to-make-fake-social-media-posts-backfires-on-bad-actors/",
  "description": "OpenAI claims cyber threats are easier to detect when attackers use ChatGPT.",
  "author": "Ashley Belanger",
  "published": "Thu, 10 Oct 2024 18:24:22 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "Policy",
    "Artificial Intelligence",
    "chatbots",
    "ChatGPT",
    "dall-e",
    "deceptive content",
    "disinformation",
    "generative ai",
    "image generator",
    "misinformation",
    "openai",
    "spam"
  ],
  "byline": "Ashley Belanger",
  "length": 2792,
  "excerpt": "OpenAI claims cyber threats are easier to detect when attackers use ChatGPT.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Instead of radically altering the threat landscape, OpenAI tools like ChatGPT are mostly used to take shortcuts or save costs, OpenAI suggested, like generating bios and social media posts to scale spam networks that might previously have \"required a large team of trolls, with all the costs and leak risks associated with such an endeavor.\" And the more these operations rely on AI, OpenAI suggested, the easier they are to take down. As an example, OpenAI cited an election interference case this summer that was quickly \"silenced\" because of threat actors' over-reliance on OpenAI tools. \"This operation’s reliance on AI... made it unusually vulnerable to our disruption,\" OpenAI said. \"Because it leveraged AI at so many links in the killchain, our takedown broke many links in the chain at once. After we disrupted this activity in early June, the social media accounts that we had identified as being part of this operation stopped posting\" throughout the critical election periods. OpenAI can’t stop AI threats on its own So far, OpenAI said, there is no evidence that its tools are \"leading to meaningful breakthroughs\" in threat actors' \"ability to create substantially new malware or build viral audiences.\" While some of the deceptive campaigns managed to engage real people online, heightening risks, OpenAI said the impact was limited. For the most part, its tools \"only offered limited, incremental capabilities that are already achievable with publicly available, non-AI powered tools.\" As threat actors' AI use continues evolving, OpenAI promised to remain transparent about how its tools are used to amplify and aid deceptive campaigns online. But the AI company's report urged that collaboration will be necessary to build \"robust, multi-layered defenses against state-linked cyber actors and covert influence operations that may attempt to use our models in furtherance of deceptive campaigns on social media and other Internet platforms.\" Appropriate threat detection across the Internet \"can also allow AI companies to identify previously unreported connections between apparently different sets of threat activity,\" OpenAI suggested. \"The unique insights that AI companies have into threat actors can help to strengthen the defenses of the broader information ecosystem, but cannot replace them. It is essential to see continued robust investment in detection and investigation capabilities across the Internet,\" OpenAI said. As one example of potential AI progress disrupting cyber threats, OpenAI suggested that, \"as our models become more advanced, we expect we will also be able to use ChatGPT to reverse engineer and analyze the malicious attachments sent to employees\" in phishing campaigns like SweetSpecter's. OpenAI did not respond to Ars' request for comment.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-2156472011.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n          \n          \n\u003cp\u003eInstead of radically altering the threat landscape, OpenAI tools like ChatGPT are mostly used to take shortcuts or save costs, OpenAI suggested, like generating bios and social media posts to scale spam networks that might previously have \u0026#34;required a large team of trolls, with all the costs and leak risks associated with such an endeavor.\u0026#34; And the more these operations rely on AI, OpenAI suggested, the easier they are to take down. As an example, OpenAI cited an election interference case this summer that was quickly \u0026#34;silenced\u0026#34; because of threat actors\u0026#39; over-reliance on OpenAI tools.\u003c/p\u003e\n\u003cp\u003e\u0026#34;This operation’s reliance on AI... made it unusually vulnerable to our disruption,\u0026#34; OpenAI said. \u0026#34;Because it leveraged AI at so many links in the killchain, our takedown broke many links in the chain at once. After we disrupted this activity in early June, the social media accounts that we had identified as being part of this operation stopped posting\u0026#34; throughout the critical election periods.\u003c/p\u003e\n\n\u003ch2\u003eOpenAI can’t stop AI threats on its own\u003c/h2\u003e\n\u003cp\u003eSo far, OpenAI said, there is no evidence that its tools are \u0026#34;leading to meaningful breakthroughs\u0026#34; in threat actors\u0026#39; \u0026#34;ability to create substantially new malware or build viral audiences.\u0026#34;\u003c/p\u003e\n\u003cp\u003eWhile some of the deceptive campaigns managed to engage real people online, heightening risks, OpenAI said the impact was limited. For the most part, its tools \u0026#34;only offered limited, incremental capabilities that are already achievable with publicly available, non-AI powered tools.\u0026#34;\u003c/p\u003e\n\u003cp\u003eAs threat actors\u0026#39; AI use continues evolving, OpenAI promised to remain transparent about how its tools are used to amplify and aid deceptive campaigns online. But the AI company\u0026#39;s report urged that collaboration will be necessary to build \u0026#34;robust, multi-layered defenses against state-linked cyber actors and covert influence operations that may attempt to use our models in furtherance of deceptive campaigns on social media and other Internet platforms.\u0026#34;\u003c/p\u003e\n\u003cp\u003eAppropriate threat detection across the Internet \u0026#34;can also allow AI companies to identify previously unreported connections between apparently different sets of threat activity,\u0026#34; OpenAI suggested.\u003c/p\u003e\n\u003cp\u003e\u0026#34;The unique insights that AI companies have into threat actors can help to strengthen the defenses of the broader information ecosystem, but cannot replace them. It is essential to see continued robust investment in detection and investigation capabilities across the Internet,\u0026#34; OpenAI said.\u003c/p\u003e\n\u003cp\u003eAs one example of potential AI progress disrupting cyber threats, OpenAI suggested that, \u0026#34;as our models become more advanced, we expect we will also be able to use ChatGPT to reverse engineer and analyze the malicious attachments sent to employees\u0026#34; in phishing campaigns like SweetSpecter\u0026#39;s.\u003c/p\u003e\n\u003cp\u003eOpenAI did not respond to Ars\u0026#39; request for comment.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-10-10T18:24:22Z",
  "modifiedTime": "2024-10-10T18:24:22Z"
}
