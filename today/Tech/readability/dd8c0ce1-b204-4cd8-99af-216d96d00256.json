{
  "id": "dd8c0ce1-b204-4cd8-99af-216d96d00256",
  "title": "Claude 3.7 Sonnet and Claude Code",
  "link": "https://www.anthropic.com/news/claude-3-7-sonnet",
  "description": "Comments",
  "author": "",
  "published": "Mon, 24 Feb 2025 18:28:59 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 11101,
  "excerpt": "Today, we’re announcing Claude 3.7 Sonnet, our most intelligent model to date and the first hybrid reasoning model generally available on the market.",
  "siteName": "",
  "favicon": "https://www.anthropic.com/images/icons/apple-touch-icon.png",
  "text": "Today, we’re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model can think for.Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. Along with the model, we’re also introducing a command line tool for agentic coding, Claude Code. Claude Code is available as a limited research preview, and enables developers to delegate substantial engineering tasks to Claude directly from their terminal.Claude 3.7 Sonnet is now available on all Claude plans—including Free, Pro, Team, and Enterprise—as well as the Anthropic API, Amazon Bedrock, and Google Cloud’s Vertex AI. Extended thinking mode is available on all surfaces except the free Claude tier.In both standard and extended thinking modes, Claude 3.7 Sonnet has the same price as its predecessors: $3 per million input tokens and $15 per million output tokens—which includes thinking tokens.Claude 3.7 Sonnet: Frontier reasoning made practicalWe’ve developed Claude 3.7 Sonnet with a different philosophy from other reasoning models on the market. Just as humans use a single brain for both quick responses and deep reflection, we believe reasoning should be an integrated capability of frontier models rather than a separate model entirely. This unified approach also creates a more seamless experience for users.Claude 3.7 Sonnet embodies this philosophy in several ways. First, Claude 3.7 Sonnet is both an ordinary LLM and a reasoning model in one: you can pick when you want the model to answer normally and when you want it to think longer before answering. In the standard mode, Claude 3.7 Sonnet represents an upgraded version of Claude 3.5 Sonnet. In extended thinking mode, it self-reflects before answering, which improves its performance on math, physics, instruction-following, coding, and many other tasks. We generally find that prompting for the model works similarly in both modes.Second, when using Claude 3.7 Sonnet through the API, users can also control the budget for thinking: you can tell Claude to think for no more than N tokens, for any value of N up to its output limit of 128K tokens. This allows you to trade off speed (and cost) for quality of answer.Third, in developing our reasoning models, we’ve optimized somewhat less for math and computer science competition problems, and instead shifted focus towards real-world tasks that better reflect how businesses actually use LLMs.Early testing demonstrated Claude’s leadership in coding capabilities across the board: Cursor noted Claude is once again best-in-class for real-world coding tasks, with significant improvements in areas ranging from handling complex codebases to advanced tool use. Cognition found it far better than any other model at planning code changes and handling full-stack updates. Vercel highlighted Claude’s exceptional precision for complex agent workflows, while Replit has successfully deployed Claude to build sophisticated web apps and dashboards from scratch, where other models stall. In Canva’s evaluations, Claude consistently produced production-ready code with superior design taste and drastically reduced errors.Claude 3.7 Sonnet achieves state-of-the-art performance on SWE-bench Verified, which evaluates AI models’ ability to solve real-world software issues. See the appendix for more information on scaffolding.Claude 3.7 Sonnet achieves state-of-the-art performance on TAU-bench, a framework that tests AI agents on complex real-world tasks with user and tool interactions. See the appendix for more information on scaffolding.Claude 3.7 Sonnet excels across instruction-following, general reasoning, multimodal capabilities, and agentic coding, with extended thinking providing a notable boost in math and science. Beyond traditional benchmarks, it even outperformed all previous models in our Pokémon gameplay tests.Claude CodeSince June 2024, Sonnet has been the preferred model for developers worldwide. Today, we're empowering developers further by introducing Claude Code—our first agentic coding tool—in a limited research preview.Claude Code is an active collaborator that can search and read code, edit files, write and run tests, commit and push code to GitHub, and use command line tools—keeping you in the loop at every step.Claude Code is an early product but has already become indispensable for our team, especially for test-driven development, debugging complex issues, and large-scale refactoring. In early testing, Claude Code completed tasks in a single pass that would normally take 45+ minutes of manual work, reducing development time and overhead.In the coming weeks, we plan to continually improve it based on our usage: enhancing tool call reliability, adding support for long-running commands, improved in-app rendering, and expanding Claude's own understanding of its capabilities.Our goal with Claude Code is to better understand how developers use Claude for coding to inform future model improvements. By joining this preview, you’ll get access to the same powerful tools we use to build and improve Claude, and your feedback will directly shape its future.Working with Claude on your codebaseWe’ve also improved the coding experience on Claude.ai. Our GitHub integration is now available on all Claude plans—enabling developers to connect their code repositories directly to Claude.Claude 3.7 Sonnet is our best coding model to date. With a deeper understanding of your personal, work, and open source projects, it becomes a more powerful partner for fixing bugs, developing features, and building documentation across your most important GitHub projects.Building responsiblyWe’ve conducted extensive testing and evaluation of Claude 3.7 Sonnet, working with external experts to ensure it meets our standards for security, safety, and reliability. Claude 3.7 Sonnet also makes more nuanced distinctions between harmful and benign requests, reducing unnecessary refusals by 45% compared to its predecessor.The system card for this release covers new safety results in several categories, providing a detailed breakdown of our Responsible Scaling Policy evaluations that other AI labs and researchers can apply to their work. The card also addresses emerging risks that come with computer use, particularly prompt injection attacks, and explains how we evaluate these vulnerabilities and train Claude to resist and mitigate them. Additionally, it examines potential safety benefits from reasoning models: the ability to understand how models make decisions, and whether model reasoning is genuinely trustworthy and reliable. Read the full system card to learn more.Looking aheadClaude 3.7 Sonnet and Claude Code mark an important step towards AI systems that can truly augment human capabilities. With their ability to reason deeply, work autonomously, and collaborate effectively, they bring us closer to a future where AI enriches and expands what humans can achieve.We're excited for you to explore these new capabilities and to see what you’ll create with them. As always, we welcome your feedback as we continue to improve and evolve our models.Appendix1 Lesson learned on naming.Eval data sourcesGrokGemini 2 Proo1 and o3-miniSupplementary o1o1 TAU-benchSupplementary o3-miniDeepseek R1TAU-benchInformation about the scaffoldingScores were achieved with a prompt addendum to the Airline Agent Policy instructing Claude to better utilize a “planning” tool, where the model is encouraged to write down its thoughts as it solves the problem distinct from our usual thinking mode, during the multi-turn trajectories to best leverage its reasoning abilities. To accommodate the additional steps Claude incurs by utilizing more thinking, the maximum number of steps (counted by model completions) was increased from 30 to 100 (most trajectories completed under 30 steps with only one trajectory reaching above 50 steps).Additionally, the TAU-bench score for Claude 3.5 Sonnet (new) differs from what we originally reported on release because of small dataset improvements introduced since then. We re-ran on the updated dataset for more accurate comparison with Claude 3.7 Sonnet.SWE-bench VerifiedInformation about the scaffoldingThere are many approaches to solving open ended agentic tasks like SWE-bench. Some approaches offload much of the complexity of deciding which files to investigate or edit and which tests to run to more traditional software, leaving the core language model to generate code in predefined places, or select from a more limited set of actions. Agentless (Xia et al., 2024) is a popular framework used in the evaluation of Deepseek’s R1 and other models which augments an agent with prompt- and embedding-based file retrieval mechanisms, patch localization, and best-of-40 rejection sampling against regression tests. Other scaffolds (e.g. Aide) further supplement models with additional test-time compute in the form of retries, best-of-N, or Monte Carlo Tree Search (MCTS).For Claude 3.7 Sonnet and Claude 3.5 Sonnet (new), we use a much simpler approach with minimal scaffolding, where the model decides which commands to run and files to edit in a single session. Our main “no extended thinking” pass@1 result simply equips the model with the two tools described here—a bash tool, and a file editing tool that operates via string replacements—as well as the “planning tool” mentioned above in our TAU-bench results. Due to infrastructure limitations, only 489/500 problems are actually solvable on our internal infrastructure (i.e., the golden solution passes the tests). For our vanilla pass@1 score we are counting the 11 unsolvable problems as failures to maintain parity with the official leaderboard. For transparency, we separately release the test cases that did not work on our infrastructure.For our “high compute” number we adopt additional complexity and parallel test-time compute as follows:We sample multiple parallel attempts with the scaffold aboveWe discard patches that break the visible regression tests in the repository, similar to the rejection sampling approach adopted by Agentless; note no hidden test information is used.We then rank the remaining attempts with a scoring model similar to our results on GPQA and AIME described in our research post and choose the best one for the submission.This results in a score of 70.3% on the subset of n=489 verified tasks which work on our infrastructure. Without this scaffold, Claude 3.7 Sonnet achieves 63.7% on SWE-bench Verified using this same subset. The excluded 11 test cases that were incompatible with our internal infrastructure are:scikit-learn__scikit-learn-14710django__django-10097psf__requests-2317sphinx-doc__sphinx-10435sphinx-doc__sphinx-7985sphinx-doc__sphinx-8475matplotlib__matplotlib-20488astropy__astropy-8707astropy__astropy-8872sphinx-doc__sphinx-8595sphinx-doc__sphinx-9711",
  "image": "https://cdn.sanity.io/images/4zrzovbb/website/9b52e961f8f275e21e75c477c99672abd13fe66b-2400x1260.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cp\u003eToday, we’re announcing Claude 3.7 Sonnet\u003csup\u003e1\u003c/sup\u003e, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made \u003ca href=\"https://youtu.be/t3nnDXa81Hs\"\u003evisible to the user\u003c/a\u003e. API users also have fine-grained control over \u003cem\u003ehow long\u003c/em\u003e the model can think for.\u003c/p\u003e\u003cp\u003eClaude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. Along with the model, we’re also introducing a command line tool for agentic coding, Claude Code. Claude Code is available as a limited research preview, and enables developers to delegate substantial engineering tasks to Claude directly from their terminal.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"Screen showing Claude Code onboarding\" loading=\"eager\" width=\"1920\" height=\"1080\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4a4df6b6629f9814aec4eb9323028130f43a8d70-1920x1080.png\u0026amp;w=1920\u0026amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4a4df6b6629f9814aec4eb9323028130f43a8d70-1920x1080.png\u0026amp;w=3840\u0026amp;q=75 2x\" src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4a4df6b6629f9814aec4eb9323028130f43a8d70-1920x1080.png\u0026amp;w=3840\u0026amp;q=75\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eClaude 3.7 Sonnet is now available on all \u003ca href=\"https://claude.ai/new\"\u003eClaude\u003c/a\u003e plans—including Free, Pro, Team, and Enterprise—as well as the \u003ca href=\"https://docs.anthropic.com/en/docs/about-claude/models\"\u003eAnthropic API\u003c/a\u003e, \u003ca href=\"https://aws.amazon.com/bedrock/claude/\"\u003eAmazon Bedrock\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude\"\u003eGoogle Cloud’s Vertex AI\u003c/a\u003e. Extended thinking mode is available on all surfaces except the free Claude tier.\u003c/p\u003e\u003cp\u003eIn both standard and extended thinking modes, Claude 3.7 Sonnet has the same price as its predecessors: $3 per million input tokens and $15 per million output tokens—which includes thinking tokens.\u003c/p\u003e\u003ch2 id=\"claude-37-sonnet-frontier-reasoning-made-practical\"\u003eClaude 3.7 Sonnet: Frontier reasoning made practical\u003c/h2\u003e\u003cp\u003eWe’ve developed Claude 3.7 Sonnet with a different philosophy from other reasoning models on the market. Just as humans use a single brain for both quick responses and deep reflection, we believe reasoning should be an integrated capability of frontier models rather than a separate model entirely. This unified approach also creates a more seamless experience for users.\u003c/p\u003e\u003cp\u003eClaude 3.7 Sonnet embodies this philosophy in several ways. First, Claude 3.7 Sonnet is both an ordinary LLM and a reasoning model in one: you can pick when you want the model to answer normally and when you want it to \u003ca href=\"https://www.anthropic.com/research/visible-extended-thinking\"\u003ethink longer before answering\u003c/a\u003e. In the standard mode, Claude 3.7 Sonnet represents an upgraded version of Claude 3.5 Sonnet. In \u003ca href=\"https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking\"\u003eextended thinking mode\u003c/a\u003e, it self-reflects before answering, which improves its performance on math, physics, instruction-following, coding, and many other tasks. We generally find that prompting for the model works similarly in both modes.\u003c/p\u003e\u003cp\u003eSecond, when using Claude 3.7 Sonnet through the API, users can also control the \u003cem\u003ebudget \u003c/em\u003efor thinking: you can tell Claude to think for no more than N tokens, for any value of N up to its output limit of 128K tokens. This allows you to trade off speed (and cost) for quality of answer.\u003c/p\u003e\u003cp\u003eThird, in developing our reasoning models, we’ve optimized somewhat less for math and computer science competition problems, and instead shifted focus towards real-world tasks that better reflect how businesses actually use LLMs.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.anthropic.com/claude/sonnet\"\u003eEarly testing\u003c/a\u003e demonstrated Claude’s leadership in coding capabilities across the board: Cursor noted Claude is once again best-in-class for real-world coding tasks, with significant improvements in areas ranging from handling complex codebases to advanced tool use. Cognition found it far better than any other model at planning code changes and handling full-stack updates. Vercel highlighted Claude’s exceptional precision for complex agent workflows, while Replit has successfully deployed Claude to build sophisticated web apps and dashboards from scratch, where other models stall. In Canva’s evaluations, Claude consistently produced production-ready code with superior design taste and drastically reduced errors.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"Bar chart showing Claude 3.7 Sonnet as state-of-the-art for SWE-bench Verified\" loading=\"lazy\" width=\"1920\" height=\"1145\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08bba4487fb5ac1ba52540ee656d7e4da10ca1be-1920x1145.png\u0026amp;w=1920\u0026amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08bba4487fb5ac1ba52540ee656d7e4da10ca1be-1920x1145.png\u0026amp;w=3840\u0026amp;q=75 2x\" src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08bba4487fb5ac1ba52540ee656d7e4da10ca1be-1920x1145.png\u0026amp;w=3840\u0026amp;q=75\"/\u003e\u003cfigcaption\u003eClaude 3.7 Sonnet achieves state-of-the-art performance on SWE-bench Verified, which evaluates AI models’ ability to solve real-world software issues. See the appendix for more information on scaffolding.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"Bar chart showing Claude 3.7 Sonnet as state-of-the-art for TAU-bench\" loading=\"lazy\" width=\"1920\" height=\"1114\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F787e59d548c230afd7efaed1bda1fb7f7ca207b8-1920x1114.png\u0026amp;w=1920\u0026amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F787e59d548c230afd7efaed1bda1fb7f7ca207b8-1920x1114.png\u0026amp;w=3840\u0026amp;q=75 2x\" src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F787e59d548c230afd7efaed1bda1fb7f7ca207b8-1920x1114.png\u0026amp;w=3840\u0026amp;q=75\"/\u003e\u003cfigcaption\u003eClaude 3.7 Sonnet achieves state-of-the-art performance on TAU-bench, a framework that tests AI agents on complex real-world tasks with user and tool interactions. See the appendix for more information on scaffolding.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"Benchmark table comparing frontier reasoning models\" loading=\"lazy\" width=\"2600\" height=\"2360\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F654cf6680d32858dfba9af644f8c4a5b04425af1-2600x2360.png\u0026amp;w=3840\u0026amp;q=75 1x\" src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F654cf6680d32858dfba9af644f8c4a5b04425af1-2600x2360.png\u0026amp;w=3840\u0026amp;q=75\"/\u003e\u003cfigcaption\u003eClaude 3.7 Sonnet excels across instruction-following, general reasoning, multimodal capabilities, and agentic coding, with extended thinking providing a notable boost in math and science. Beyond traditional benchmarks, it even outperformed all previous models in our \u003ca href=\"https://www.anthropic.com/research/visible-extended-thinking\"\u003ePokémon gameplay tests\u003c/a\u003e.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003ch2 id=\"claude-code\"\u003eClaude Code\u003c/h2\u003e\u003cp\u003eSince June 2024, Sonnet has been the preferred model for developers worldwide. Today, we\u0026#39;re empowering developers further by introducing Claude Code—our first agentic coding tool—in a limited research preview.\u003c/p\u003e\u003cp\u003eClaude Code is an active collaborator that can search and read code, edit files, write and run tests, commit and push code to GitHub, and use command line tools—keeping you in the loop at every step.\u003c/p\u003e\u003cp\u003eClaude Code is an early product but has already become indispensable for our team, especially for test-driven development, debugging complex issues, and large-scale refactoring. In early testing, Claude Code completed tasks in a single pass that would normally take 45+ minutes of manual work, reducing development time and overhead.\u003c/p\u003e\u003cp\u003eIn the coming weeks, we plan to continually improve it based on our usage: enhancing tool call reliability, adding support for long-running commands, improved in-app rendering, and expanding Claude\u0026#39;s own understanding of its capabilities.\u003c/p\u003e\u003cp\u003eOur goal with Claude Code is to better understand how developers use Claude for coding to inform future model improvements. By \u003ca href=\"https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview\"\u003ejoining this preview\u003c/a\u003e, you’ll get access to the same powerful tools we use to build and improve Claude, and your feedback will directly shape its future.\u003c/p\u003e\u003ch2 id=\"working-with-claude-on-your-codebase\"\u003eWorking with Claude on your codebase\u003c/h2\u003e\u003cp\u003eWe’ve also improved the coding experience on Claude.ai. Our GitHub integration is now available on all Claude plans—enabling developers to connect their code repositories directly to Claude.\u003c/p\u003e\u003cp\u003eClaude 3.7 Sonnet is our best coding model to date. With a deeper understanding of your personal, work, and open source projects, it becomes a more powerful partner for fixing bugs, developing features, and building documentation across your most important GitHub projects.\u003c/p\u003e\u003ch2 id=\"building-responsibly\"\u003eBuilding responsibly\u003c/h2\u003e\u003cp\u003eWe’ve conducted extensive testing and evaluation of Claude 3.7 Sonnet, working with external experts to ensure it meets our standards for security, safety, and reliability. Claude 3.7 Sonnet also makes more nuanced distinctions between harmful and benign requests, reducing \u003ca href=\"https://www.anthropic.com/claude-3-7-sonnet-system-card\"\u003eunnecessary refusals by 45%\u003c/a\u003e compared to its predecessor.\u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://www.anthropic.com/claude-3-7-sonnet-system-card\"\u003esystem card\u003c/a\u003e for this release covers new safety results in several categories, providing a detailed breakdown of our Responsible Scaling Policy evaluations that other AI labs and researchers can apply to their work. The card also addresses emerging risks that come with computer use, particularly prompt injection attacks, and explains how we evaluate these vulnerabilities and train Claude to resist and mitigate them. Additionally, it examines potential safety benefits from reasoning models: the ability to understand how models make decisions, and whether model reasoning is genuinely trustworthy and reliable. Read the full \u003ca href=\"https://www.anthropic.com/claude-3-7-sonnet-system-card\"\u003esystem card \u003c/a\u003eto learn more.\u003c/p\u003e\u003ch2 id=\"looking-ahead\"\u003eLooking ahead\u003c/h2\u003e\u003cp\u003eClaude 3.7 Sonnet and Claude Code mark an important step towards AI systems that can truly augment human capabilities. With their ability to reason deeply, work autonomously, and collaborate effectively, they bring us closer to a future where AI enriches and expands what \u003ca href=\"https://darioamodei.com/machines-of-loving-grace\"\u003ehumans can achieve\u003c/a\u003e.\u003c/p\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"Milestone timeline showing Claude progressing from assistant to pioneer\" loading=\"lazy\" width=\"1920\" height=\"1080\" decoding=\"async\" data-nimg=\"1\" srcset=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde9831ea84e3663fe4598589d71eaa531f9912-1920x1080.png\u0026amp;w=1920\u0026amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde9831ea84e3663fe4598589d71eaa531f9912-1920x1080.png\u0026amp;w=3840\u0026amp;q=75 2x\" src=\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde9831ea84e3663fe4598589d71eaa531f9912-1920x1080.png\u0026amp;w=3840\u0026amp;q=75\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003eWe\u0026#39;re excited for you to explore these new capabilities and to see what you’ll create with them. As always, we welcome your \u003ca href=\"mailto: feedback@anthropic.com\"\u003efeedback\u003c/a\u003e as we continue to improve and evolve our models.\u003c/p\u003e\u003c/div\u003e\u003c/article\u003e\u003cdiv\u003e\u003ch4\u003eAppendix\u003c/h4\u003e\u003cp\u003e\u003csup\u003e1 \u003c/sup\u003eLesson learned on \u003ca href=\"https://www.anthropic.com/news/3-5-models-and-computer-use\"\u003enaming\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eEval data sources\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://x.ai/blog/grok-3\"\u003eGrok\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://developers.googleblog.com/en/gemini-2-family-expands/\"\u003eGemini 2 Pro\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://openai.com/index/openai-o3-mini/\"\u003eo1 and o3-mini\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://cdn.openai.com/o1-system-card-20241205.pdf\"\u003eSupplementary o1\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://web.archive.org/web/20250203044057/https://openai.com/index/o1-and-new-tools-for-developers/\"\u003eo1 TAU-bench\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://cdn.openai.com/o3-mini-system-card-feb10.pdf\"\u003eSupplementary o3-mini\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"\u003eDeepseek R1\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eTAU-bench\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eInformation about the scaffolding\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eScores were achieved with a prompt addendum to the Airline Agent Policy instructing Claude to better utilize a “planning” tool, where the model is encouraged to write down its thoughts as it solves the problem distinct from our usual thinking mode, during the multi-turn trajectories to best leverage its reasoning abilities. To accommodate the additional steps Claude incurs by utilizing more thinking, the maximum number of steps (counted by model completions) was increased from 30 to 100 (most trajectories completed under 30 steps with only one trajectory reaching above 50 steps).\u003c/p\u003e\u003cp\u003eAdditionally, the TAU-bench score for Claude 3.5 Sonnet (new) differs from what we originally reported on release because of small dataset improvements introduced since then. We re-ran on the updated dataset for more accurate comparison with Claude 3.7 Sonnet.\u003c/p\u003e\u003ch3\u003eSWE-bench Verified\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003eInformation about the scaffolding\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThere are many approaches to solving open ended agentic tasks like SWE-bench. Some approaches offload much of the complexity of deciding which files to investigate or edit and which tests to run to more traditional software, leaving the core language model to generate code in predefined places, or select from a more limited set of actions. Agentless (\u003ca href=\"https://arxiv.org/abs/2407.01489\"\u003eXia et al., 2024\u003c/a\u003e) is a popular framework used in the evaluation of Deepseek’s R1 and other models which augments an agent with prompt- and embedding-based file retrieval mechanisms, patch localization, and best-of-40 rejection sampling against regression tests. Other scaffolds (e.g. \u003ca href=\"https://aide.dev/blog/sota-bitter-lesson\"\u003eAide\u003c/a\u003e) further supplement models with additional test-time compute in the form of retries, best-of-N, or Monte Carlo Tree Search (MCTS).\u003c/p\u003e\u003cp\u003eFor Claude 3.7 Sonnet and Claude 3.5 Sonnet (new), we use a much simpler approach with minimal scaffolding, where the model decides which commands to run and files to edit in a single session. Our main “no extended thinking” pass@1 result simply equips the model with the \u003ca href=\"https://www.anthropic.com/research/swe-bench-sonnet\"\u003etwo tools described here\u003c/a\u003e—a bash tool, and a file editing tool that operates via string replacements—as well as the “planning tool” mentioned above in our TAU-bench results. Due to infrastructure limitations, only 489/500 problems are actually solvable on our internal infrastructure (i.e., the golden solution passes the tests). For our vanilla pass@1 score we are counting the 11 unsolvable problems as failures to maintain parity with the \u003ca href=\"https://www.swebench.com/#verified\"\u003eofficial leaderboard\u003c/a\u003e. For transparency, we separately release the test cases that did not work on our infrastructure.\u003c/p\u003e\u003cp\u003eFor our “high compute” number we adopt additional complexity and parallel test-time compute as follows:\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe sample multiple parallel attempts with the scaffold above\u003c/li\u003e\u003cli\u003eWe discard patches that break the visible regression tests in the repository, similar to the rejection sampling approach adopted by Agentless; note no hidden test information is used.\u003c/li\u003e\u003cli\u003eWe then rank the remaining attempts with a scoring model similar to our results on GPQA and AIME described in our \u003ca href=\"https://www.anthropic.com/news/visible-extended-thinking\"\u003eresearch post\u003c/a\u003e and choose the best one for the submission.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis results in a score of 70.3% on the subset of n=489 verified tasks which work on our infrastructure. Without this scaffold, Claude 3.7 Sonnet achieves 63.7% on SWE-bench Verified using this same subset. The excluded 11 test cases that were incompatible with our internal infrastructure are:\u003c/p\u003e\u003cul\u003e\u003cli\u003escikit-learn__scikit-learn-14710\u003c/li\u003e\u003cli\u003edjango__django-10097\u003c/li\u003e\u003cli\u003epsf__requests-2317\u003c/li\u003e\u003cli\u003esphinx-doc__sphinx-10435\u003c/li\u003e\u003cli\u003esphinx-doc__sphinx-7985\u003c/li\u003e\u003cli\u003esphinx-doc__sphinx-8475\u003c/li\u003e\u003cli\u003ematplotlib__matplotlib-20488\u003c/li\u003e\u003cli\u003eastropy__astropy-8707\u003c/li\u003e\u003cli\u003eastropy__astropy-8872\u003c/li\u003e\u003cli\u003esphinx-doc__sphinx-8595\u003c/li\u003e\u003cli\u003esphinx-doc__sphinx-9711\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": null,
  "modifiedTime": null
}
