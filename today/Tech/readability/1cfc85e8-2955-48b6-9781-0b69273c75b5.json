{
  "id": "1cfc85e8-2955-48b6-9781-0b69273c75b5",
  "title": "‘Sorry, I didn’t get that’: AI misunderstands some people’s words more than others",
  "link": "https://thenextweb.com/news/ai-misunderstands-some-peoples-words-more-than-others",
  "description": "The idea of a humanlike artificial intelligence assistant that you can speak with has been alive in many people’s imaginations since the release of “Her,” Spike Jonze’s 2013 film about a man who falls in love with a Siri-like AI named Samantha. Over the course of the film, the protagonist grapples with the ways in which Samantha, real as she may seem, is not and never will be human. Twelve years on, this is no longer the stuff of science fiction. Generative AI tools like ChatGPT and digital assistants like Apple’s Siri and Amazon’s Alexa help people get driving directions,…This story continues at The Next Web",
  "author": "The Conversation",
  "published": "Thu, 06 Feb 2025 20:50:12 +0000",
  "source": "https://thenextweb.com/feed/",
  "categories": [
    "Insider",
    "Deep tech"
  ],
  "byline": "The Conversation",
  "length": 6231,
  "excerpt": "Speech recognition systems are less accurate for women and Black people, among other demographics finds new studies.",
  "siteName": "TNW | Deep-Tech",
  "favicon": "https://next.tnwcdn.com/assets/img/favicon/favicon-194x194.png",
  "text": "The idea of a humanlike artificial intelligence assistant that you can speak with has been alive in many people’s imaginations since the release of “Her,” Spike Jonze’s 2013 film about a man who falls in love with a Siri-like AI named Samantha. Over the course of the film, the protagonist grapples with the ways in which Samantha, real as she may seem, is not and never will be human. Twelve years on, this is no longer the stuff of science fiction. Generative AI tools like ChatGPT and digital assistants like Apple’s Siri and Amazon’s Alexa help people get driving directions, make grocery lists, and plenty else. But just like Samantha, automatic speech recognition systems still cannot do everything that a human listener can. You have probably had the frustrating experience of calling your bank or utility company and needing to repeat yourself so that the digital customer service bot on the other line can understand you. Maybe you’ve dictated a note on your phone, only to spend time editing garbled words. Linguistics and computer science researchers have shown that these systems work worse for some people than for others. They tend to make more errors if you have a non-native or a regional accent, are Black, speak in African American Vernacular English, code-switch, if you are a woman, are old, are too young or have a speech impediment. Tin ear Unlike you or me, automatic speech recognition systems are not what researchers call “sympathetic listeners.” Instead of trying to understand you by taking in other useful clues like intonation or facial gestures, they simply give up. Or they take a probabilistic guess, a move that can sometimes result in an error. As companies and public agencies increasingly adopt automatic speech recognition tools in order to cut costs, people have little choice but to interact with them. But the more that these systems come into use in critical fields, ranging from emergency first responders and health care to education and law enforcement, the more likely there will be grave consequences when they fail to recognize what people say. Imagine sometime in the near future you’ve been hurt in a car crash. You dial 911 to call for help, but instead of being connected to a human dispatcher, you get a bot that’s designed to weed out nonemergency calls. It takes you several rounds to be understood, wasting time and raising your anxiety level at the worst moment. What causes this kind of error to occur? Some of the inequalities that result from these systems are baked into the reams of linguistic data that developers use to build large language models. Developers train artificial intelligence systems to understand and mimic human language by feeding them vast quantities of text and audio files containing real human speech. But whose speech are they feeding them? If a system scores high accuracy rates when speaking with affluent white Americans in their mid-30s, it is reasonable to guess that it was trained using plenty of audio recordings of people who fit this profile. With rigorous data collection from a diverse range of sources, AI developers could reduce these errors. But to build AI systems that can understand the infinite variations in human speech arising from things like gender, age, race, first vs. second language, socioeconomic status, ability and plenty else, requires significant resources and time. ‘Proper’ English For people who do not speak English – which is to say, most people around the world – the challenges are even greater. Most of the world’s largest generative AI systems were built in English, and they work far better in English than in any other language. On paper, AI has lots of civic potential for translation and increasing people’s access to information in different languages, but for now, most languages have a smaller digital footprint, making it difficult for them to power large language models. Even within languages well-served by large language models, like English and Spanish, your experience varies depending on which dialect of the language you speak. Right now, most speech recognition systems and generative AI chatbots reflect the linguistic biases of the datasets they are trained on. They echo prescriptive, sometimes prejudiced notions of “correctness” in speech. In fact, AI has been proven to “flatten” linguistic diversity. There are now AI startup companies that offer to erase the accents of their users, drawing on the assumption that their primary clientele would be customer service providers with call centres in foreign countries like India or the Philippines. The offering perpetuates the notion that some accents are less valid than others. Human connection AI will presumably get better at processing language, accounting for variables like accents, code-switching and the like. In the US, public services are obligated under federal law to guarantee equitable access to services regardless of what language a person speaks. But it is not clear whether that alone will be enough incentive for the tech industry to move toward eliminating linguistic inequities. Many people might prefer to talk to a real person when asking questions about a bill or medical issue, or at least to have the ability to opt out of interacting with automated systems when seeking key services. That is not to say that miscommunication never happens in interpersonal communication, but when you speak to a real person, they are primed to be a sympathetic listener. With AI, at least for now, it either works or it doesn’t. If the system can process what you say, you are good to go. If it cannot, the onus is on you to make yourself understood. Roberto Rey Agudo, Research Assistant Professor of Spanish and Portuguese, Dartmouth College This article is republished from The Conversation under a Creative Commons license. Read the original article. Get the TNW newsletter Get the most important tech news in your inbox each week. Also tagged with",
  "image": "https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1\u0026fit=1280%2C640\u0026url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F02%2Fjason-rosewell-ASKeuOZqhYU-unsplash.jpg\u0026signature=74681ea39b019ae4608657a6c23c8803",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                \u003carticle id=\"articleOutput\"\u003e\n                                                                        \u003cdiv\u003e\n                                \u003cfigure\u003e\n                                    \u003cimg alt=\"‘Sorry, I didn’t get that’: AI misunderstands some people’s words more than others\" src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F02%2Fjason-rosewell-ASKeuOZqhYU-unsplash.jpg\u0026amp;signature=568f6cd166f8ab59c161c2d726437307\" sizes=\"(max-width: 1023px) 100vw\n                                                   868px\" srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F02%2Fjason-rosewell-ASKeuOZqhYU-unsplash.jpg\u0026amp;signature=25209bcdf98aa01cad5c747be17ff411 576w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F02%2Fjason-rosewell-ASKeuOZqhYU-unsplash.jpg\u0026amp;signature=ade462859481e1ed89f06cefc882a6a4 1152w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F02%2Fjason-rosewell-ASKeuOZqhYU-unsplash.jpg\u0026amp;signature=568f6cd166f8ab59c161c2d726437307 1280w\" data-src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F02%2Fjason-rosewell-ASKeuOZqhYU-unsplash.jpg\u0026amp;signature=568f6cd166f8ab59c161c2d726437307\" data-srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F02%2Fjason-rosewell-ASKeuOZqhYU-unsplash.jpg\u0026amp;signature=25209bcdf98aa01cad5c747be17ff411 576w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F02%2Fjason-rosewell-ASKeuOZqhYU-unsplash.jpg\u0026amp;signature=ade462859481e1ed89f06cefc882a6a4 1152w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F02%2Fjason-rosewell-ASKeuOZqhYU-unsplash.jpg\u0026amp;signature=568f6cd166f8ab59c161c2d726437307 1280w\"/\u003e\n\n                                    \n\n                                                                    \u003c/figure\u003e\n                            \u003c/div\u003e\n                        \n                                                    \n                            \n                                            \n                    \n                    \n\n                    \n                    \u003cdiv\u003e\n                        \u003cdiv id=\"article-main-content\"\u003e\n\u003cp\u003eThe idea of a humanlike \u003ca href=\"https://thenextweb.com/topic/artificial-intelligence\" target=\"_blank\" rel=\"noopener\"\u003eartificial intelligence\u003c/a\u003e assistant that you can speak with has been alive in many people’s imaginations since the release of “Her,” Spike Jonze’s 2013 film about a man who falls in love with a Siri-like AI named Samantha. Over the course of the film, the protagonist grapples with the ways in which Samantha, real as she may seem, is not and never will be human.\u003c/p\u003e\n\u003cp\u003eTwelve years on, this is no longer the stuff of science fiction. Generative AI tools like ChatGPT and digital assistants like Apple’s Siri and Amazon’s Alexa help people get driving directions, make grocery lists, and plenty else. But just like Samantha, automatic speech recognition systems still cannot do everything that a human listener can.\u003c/p\u003e\n\u003cp\u003eYou have probably had the frustrating experience of calling your bank or utility company and needing to repeat yourself so that the \u003ca href=\"https://news.gatech.edu/news/2024/11/15/minority-english-dialects-vulnerable-automatic-speech-recognition-inaccuracy\" target=\"_blank\" rel=\"nofollow noopener\"\u003edigital customer service\u003c/a\u003e bot on the other line can understand you. Maybe you’ve dictated a note on your phone, only to spend time editing garbled words.\u003c/p\u003e\n\u003cp\u003eLinguistics and computer science researchers have shown that these systems work worse for some people than for others. They tend to make more errors if you have a \u003ca href=\"https://doi.org/10.1145/3379503.3403563\" target=\"_blank\" rel=\"nofollow noopener\"\u003enon-native\u003c/a\u003e or a \u003ca href=\"https://www.cbsnews.com/minnesota/news/ai-artificial-intelligence-accent-problems-minnesotan/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eregional\u003c/a\u003e accent, are \u003ca href=\"https://doi.org/10.1073/pnas.1915768117\" target=\"_blank\" rel=\"nofollow noopener\"\u003eBlack\u003c/a\u003e, speak \u003ca href=\"https://doi.org/10.1093/applin/amac066\" target=\"_blank\" rel=\"nofollow noopener\"\u003ein African American Vernacular English\u003c/a\u003e, \u003ca href=\"https://doi.org/10.48550/arXiv.2403.05887\" target=\"_blank\" rel=\"nofollow noopener\"\u003ecode-switch\u003c/a\u003e, if you are a \u003ca href=\"https://doi.org/10.18653/v1/W17-1606\" target=\"_blank\" rel=\"nofollow noopener\"\u003ewoman\u003c/a\u003e, are \u003ca href=\"https://doi.org/10.48550/arXiv.2103.15122\" target=\"_blank\" rel=\"nofollow noopener\"\u003eold\u003c/a\u003e, are too \u003ca href=\"https://doi.org/10.48550/arXiv.2103.15122\" target=\"_blank\" rel=\"nofollow noopener\"\u003eyoung\u003c/a\u003e or have a \u003ca href=\"http://dx.doi.org/10.21437/Interspeech.2019-2993\" target=\"_blank\" rel=\"nofollow noopener\"\u003espeech impediment\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eTin ear\u003c/h2\u003e\n\u003cp\u003eUnlike you or me, automatic speech recognition systems are not what researchers call “sympathetic listeners.” Instead of trying to understand you by taking in other useful clues like intonation or facial gestures, they simply give up. Or they take a probabilistic guess, a move that can sometimes result in an error.\u003c/p\u003e\n\u003cp\u003eAs companies and public agencies increasingly adopt automatic speech recognition tools in order to cut costs, people have little choice but to interact with them. But the more that these systems come into use in critical fields, ranging from emergency \u003ca href=\"https://www.dhs.gov/medialibrary/assets/videos/23524\" target=\"_blank\" rel=\"nofollow noopener\"\u003efirst responders\u003c/a\u003e and \u003ca href=\"https://www.wired.com/story/hospitals-ai-transcription-tools-hallucination/\" target=\"_blank\" rel=\"nofollow noopener\"\u003ehealth care\u003c/a\u003e to \u003ca href=\"https://doi.org/10.1080/09588221.2022.2080230\" target=\"_blank\" rel=\"nofollow noopener\"\u003eeducation\u003c/a\u003e and \u003ca href=\"https://doi.org/10.48550/arXiv.2405.13166\" target=\"_blank\" rel=\"nofollow noopener\"\u003elaw\u003c/a\u003e \u003ca href=\"https://doi.org/10.1016/j.fsisyn.2024.100563\" target=\"_blank\" rel=\"nofollow noopener\"\u003eenforcement\u003c/a\u003e, the more likely there will be grave consequences when they fail to recognize what people say.\u003c/p\u003e\n\u003cp\u003eImagine sometime in the near future you’ve been hurt in a car crash. You dial 911 to call for help, but instead of being connected to a human dispatcher, you get a bot that’s designed to weed out nonemergency calls. It takes you several rounds to be understood, wasting time and raising your anxiety level at the worst moment.\u003c/p\u003e\n\u003cp\u003eWhat causes this kind of error to occur? Some of the inequalities that result from these systems are baked into the reams of \u003ca href=\"https://doi.org/10.1145/3442188.3445922\" target=\"_blank\" rel=\"nofollow noopener\"\u003elinguistic data\u003c/a\u003e that developers use to build \u003ca href=\"https://direct.mit.edu/coli/article/50/3/1097/121961/Bias-and-Fairness-in-Large-Language-Models-A\" target=\"_blank\" rel=\"nofollow noopener\"\u003elarge language models\u003c/a\u003e. Developers train artificial intelligence systems to understand and mimic human language by feeding them vast quantities of text and audio files containing real human speech. But whose speech are they feeding them?\u003c/p\u003e\n\u003cp\u003eIf a system scores high accuracy rates when speaking with affluent white Americans in their mid-30s, it is reasonable to guess that it was trained using plenty of audio recordings of people who fit this profile.\u003c/p\u003e\n\u003cp\u003eWith rigorous data collection from a diverse range of sources, AI developers could reduce these errors. But to build AI systems that can understand the infinite variations in human speech arising from things like \u003ca href=\"https://doi.org/10.48550/arXiv.2406.09855\" target=\"_blank\" rel=\"nofollow noopener\"\u003egender\u003c/a\u003e, \u003ca href=\"https://www.forbes.com/sites/ulrichboser/2024/11/25/why-cant-automatic-speech-recognition-systems-understand-kids/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eage\u003c/a\u003e, \u003ca href=\"https://news.stanford.edu/stories/2020/03/automated-speech-recognition-less-accurate-blacks\" target=\"_blank\" rel=\"nofollow noopener\"\u003erace\u003c/a\u003e, \u003ca href=\"https://nymag.com/intelligencer/2018/08/why-are-google-siri-and-alexa-so-bad-at-understanding-bilingual-accents-voice-assistants.html\" target=\"_blank\" rel=\"nofollow noopener\"\u003efirst vs. second language\u003c/a\u003e, \u003ca href=\"https://doi.org/10.48550/arXiv.2403.04445\" target=\"_blank\" rel=\"nofollow noopener\"\u003esocioeconomic status\u003c/a\u003e, \u003ca href=\"https://doi.org/10.1007/978-3-031-21707-4_30\" target=\"_blank\" rel=\"nofollow noopener\"\u003eability\u003c/a\u003e and plenty else, requires significant resources and time.\u003c/p\u003e\n\u003ch2\u003e‘Proper’ English\u003c/h2\u003e\n\u003cp\u003eFor people who do not speak English – which is to say, most people around the world – the challenges are even greater. Most of the world’s largest generative AI systems were built in English, and they work far better in English than in any other language. On paper, AI has lots of \u003ca href=\"https://medium.com/@askiefer/it-started-with-a-whisper-4090d26d95e4\" target=\"_blank\" rel=\"nofollow noopener\"\u003ecivic potential\u003c/a\u003e for translation and increasing people’s access to information in different languages, but for now, most languages have a \u003ca href=\"https://news.mit.edu/2021/speech-recognition-uncommon-languages-1104\" target=\"_blank\" rel=\"nofollow noopener\"\u003esmaller digital footprint\u003c/a\u003e, making it difficult for them to power large language models.\u003c/p\u003e\n\u003cp\u003eEven within languages well-served by large language models, like \u003ca href=\"https://doi.org/10.1038/s41598-022-06673-y\" target=\"_blank\" rel=\"nofollow noopener\"\u003eEnglish\u003c/a\u003e and \u003ca href=\"http://dx.doi.org/10.3390/app14114734\" target=\"_blank\" rel=\"nofollow noopener\"\u003eSpanish\u003c/a\u003e, your experience varies depending on which dialect of the language you speak.\u003c/p\u003e\n\u003cp\u003eRight now, most speech recognition systems and generative AI chatbots reflect the \u003ca href=\"https://www.anthropology-news.org/articles/chatgpt-is-reinforcing-your-language-stereotypes/\" target=\"_blank\" rel=\"nofollow noopener\"\u003elinguistic biases\u003c/a\u003e of the datasets they are trained on. They echo prescriptive, sometimes \u003ca href=\"https://theconversation.com/chatgpt-threatens-language-diversity-more-needs-to-be-done-to-protect-our-differences-in-the-age-of-ai-198878\" target=\"_blank\" rel=\"nofollow noopener\"\u003eprejudiced notions\u003c/a\u003e of “correctness” in speech.\u003c/p\u003e\n\u003cp\u003eIn fact, AI has been proven to “\u003ca href=\"https://www.theguardian.com/society/2024/dec/11/ai-tone-shifting-tech-could-flatten-communication-apple-intelligence\" target=\"_blank\" rel=\"nofollow noopener\"\u003eflatten\u003c/a\u003e” linguistic diversity. There are now AI startup companies that offer to \u003ca href=\"https://tomato.ai/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eerase the accents\u003c/a\u003e of their users, drawing on the assumption that their primary clientele would be customer service providers with call centres in foreign countries like India or the Philippines. The offering perpetuates the notion that some accents are less valid than others.\u003c/p\u003e\n\u003ch2\u003eHuman connection\u003c/h2\u003e\n\u003cp\u003eAI will presumably get better at processing language, accounting for variables like accents, code-switching and the like. In the US, public services are obligated under federal law to guarantee \u003ca href=\"https://www.justice.gov/crt/fcs/TitleVI\" target=\"_blank\" rel=\"nofollow noopener\"\u003eequitable access\u003c/a\u003e to services regardless of what language a person speaks. But it is not clear whether that alone will be enough incentive for the tech industry to move toward eliminating linguistic inequities.\u003c/p\u003e\n\u003cp\u003eMany people might prefer to talk to a real person when asking questions about a bill or medical issue, or at least to have the ability to opt out of interacting with automated systems when seeking key services. That is not to say that miscommunication never happens in interpersonal communication, but when you speak to a real person, they are primed to be a sympathetic listener.\u003c/p\u003e\n\u003cp\u003eWith AI, at least for now, it either works or it doesn’t. If the system can process what you say, you are good to go. If it cannot, the onus is on you to make yourself understood.\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://counter.theconversation.com/content/239281/count.gif?distributor=republish-lightbox-basic\" alt=\"The Conversation\" width=\"1\" height=\"1\" srcset=\"\" data-old-src=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003ca href=\"https://theconversation.com/profiles/roberto-rey-agudo-1529250\" target=\"_blank\" rel=\"nofollow noopener\"\u003eRoberto Rey Agudo\u003c/a\u003e, Research Assistant Professor of Spanish and Portuguese, \u003ca href=\"https://theconversation.com/institutions/dartmouth-college-1720\" target=\"_blank\" rel=\"nofollow noopener\"\u003eDartmouth College\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThis article is republished from \u003ca href=\"https://theconversation.com\" target=\"_blank\" rel=\"nofollow noopener\"\u003eThe Conversation\u003c/a\u003e under a Creative Commons license. Read the \u003ca href=\"https://theconversation.com/sorry-i-didnt-get-that-ai-misunderstands-some-peoples-words-more-than-others-239281\" target=\"_blank\" rel=\"nofollow noopener\"\u003eoriginal article\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n                        \n\n                        \u003cdiv id=\"nl-container\"\u003e\n                                                        \u003ch2\u003eGet the TNW newsletter\u003c/h2\u003e\n                            \u003cp\u003eGet the most important tech news in your inbox each week.\u003c/p\u003e\n                            \n                        \u003c/div\u003e\n\n                        \n                                                    \u003ch2\u003eAlso tagged with\u003c/h2\u003e\n\n                            \u003cbr/\u003e\n\n                            \n                        \n                        \n\n                        \n                    \u003c/div\u003e\n                    \n\n                    \n                \u003c/article\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-02-06T20:50:12Z",
  "modifiedTime": "2025-02-06T20:50:13Z"
}
