{
  "id": "5273056f-94bc-4372-a7b2-fde6431dd2b3",
  "title": "Trying out QvQ – Qwen's new visual reasoning model",
  "link": "https://simonwillison.net/2024/Dec/24/qvq/",
  "description": "Comments",
  "author": "",
  "published": "Tue, 24 Dec 2024 21:25:32 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 4531,
  "excerpt": "I thought we were done for major model releases in 2024, but apparently not: Alibaba’s Qwen team just dropped the Apache 2.0 licensed QvQ-72B-Preview, “an experimental research model focusing on …",
  "siteName": "",
  "favicon": "",
  "text": "24th December 2024 I thought we were done for major model releases in 2024, but apparently not: Alibaba’s Qwen team just dropped the Apache 2.0 licensed QvQ-72B-Preview, “an experimental research model focusing on enhancing visual reasoning capabilities”. Their blog post is titled QvQ: To See the World with Wisdom—similar flowery language to their QwQ announcement QwQ: Reflect Deeply on the Boundaries of the Unknown a few weeks ago in November. It’s a vision-focused follow-up to QwQ, which I wrote about previousy. QwQ is an impressive openly licensed inference-scaling model: give it a prompt and it will think out loud over many tokens while trying to derive a good answer, similar to OpenAI’s o1 and o3 models. The new QvQ adds vision to the mix. You can try it out on Hugging Face Spaces—it accepts an image and a single prompt and then streams out a very long response where it thinks through the problem you have posed it. There’s no option to send a follow-up prompt. I’ve tried it out with a bunch of things, with mixed results—but it’s really fun seeing how it works through a problem. My most successful prompt was “Count the pelicans” with this image: Here’s the full response. Some highlights: So I’ve got this picture of some pelicans, and I need to count them. Let me see... First off, I know that pelicans are those big birds with the massive beaks, right? [...] Let me try counting again, more carefully this time. Starting from the left: The first pelican is fully visible, standing at the front. Behind it, slightly to the right, is the second pelican. Then, further back, there’s a third pelican. And next to that one, there’s a fourth pelican. So, that’s four pelicans in total. But, I notice that on the far right of the image, there’s part of another bird, maybe a pelican’s back or tail feathers, but it’s not fully in the frame, so I don’t think I should count it as a whole pelican. Something about the tone of voice it uses, “First off, I know that pelicans are those big birds with the massive beaks, right?”, tickles me. I also tried this on a much harder image—it counted 17 which looks about right to me but honestly I have trouble with that image myself. I fed in one of the ARC-AGI puzzles that o3 had failed at. It produced a very lengthy chain of thought that was almost entirely incorrect, but had some interesting “ideas” in it: [...] Let me try to think in terms of cellular automata rules, like Conway’s Game of Life. In Game of Life, each cell’s state in the next generation is determined by its current state and the states of its eight neighbors. Perhaps a similar rule applies here. Finally, I asked it to “Estimate the height of the dinosaur” against this image (which, as it correctly noted, is actually an inflatable dragon): I’m not sure this task is even possible given the lack of items in the frame of comparable known height. It had a good go at it though! I particularly enjoyed how it began: So I’ve got this inflatable Christmas decoration, and it’s a dragon! But not just any dragon—it’s a green dragon wearing a Santa hat and a red and white scarf. And get this, there’s a reindeer on its back, also wearing a Santa hat and holding onto the dragon’s wings. It’s pretty festive and colorful, with lots of vibrant greens, reds, and yellows. The dragon has these big, friendly eyes and a smiling face, which makes it look really cheerful. Its wings are spread out, and it has white claws on its feet. The reindeer is brown with antlers and seems to be enjoying the ride on the dragon’s back. Now, the question is, how tall is this inflatable dragon? I need to estimate its height. Let me see. I’m standing next to it, and I’m about 5 feet 10 inches tall. The dragon is definitely taller than me. Maybe around 8 or 9 feet high? But I should try to be more precise. I wonder how it decided that its own height was 5 feet 10 inches! Running QvQ locally All of my experiments so far have used the hosted Hugging Face demo. I’m hoping to try it out on my own laptop soon—I think it should just fit in 64GB of M2, maybe with a smaller quantization. Right now the options are: Qwen/QVQ-72B-Preview on Hugging Face has the GPU model weights, for use with Hugging Face Transformers and the qwen-vl-utils Python package. Prince Canuma converted the model for Apple’s MLX framework as mlx-community/qvq-72b-preview and got it working with his excellent mlx-vlm package. As a happy user of Ollama’s qwq port I’m hoping they add a QvQ release at some point soon as well.",
  "image": "https://static.simonwillison.net/static/2024/count-pelicans-easy.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv data-permalink-context=\"/2024/Dec/24/qvq/\"\u003e\n\n\u003cp\u003e24th December 2024\u003c/p\u003e\n\n\n\n\u003cp\u003eI thought we were done for major model releases in 2024, but apparently not: Alibaba’s Qwen team just dropped the Apache 2.0 licensed QvQ-72B-Preview, “an experimental research model focusing on enhancing visual reasoning capabilities”.\u003c/p\u003e\n\u003cp\u003eTheir blog post is titled \u003ca href=\"https://qwenlm.github.io/blog/qvq-72b-preview/\"\u003eQvQ: To See the World with Wisdom\u003c/a\u003e—similar flowery language to their QwQ announcement \u003ca href=\"https://qwenlm.github.io/blog/qwq-32b-preview/\"\u003eQwQ: Reflect Deeply on the Boundaries of the Unknown\u003c/a\u003e a few weeks ago in November.\u003c/p\u003e\n\u003cp\u003eIt’s a vision-focused follow-up to QwQ, which I \u003ca href=\"https://simonwillison.net/2024/Nov/27/qwq/\"\u003ewrote about previousy\u003c/a\u003e. QwQ is an impressive openly licensed inference-scaling model: give it a prompt and it will think out loud over many tokens while trying to derive a good answer, similar to OpenAI’s o1 and o3 models.\u003c/p\u003e\n\u003cp\u003eThe new QvQ adds vision to the mix. You can try it out \u003ca href=\"https://huggingface.co/spaces/Qwen/QVQ-72B-preview\"\u003eon Hugging Face Spaces\u003c/a\u003e—it accepts an image and a single prompt and then streams out a \u003cem\u003every\u003c/em\u003e long response where it thinks through the problem you have posed it. There’s no option to send a follow-up prompt.\u003c/p\u003e\n\u003cp\u003eI’ve tried it out with a bunch of things, with mixed results—but it’s really fun seeing how it works through a problem.\u003c/p\u003e\n\u003cp\u003eMy most successful prompt was “Count the pelicans” with this image:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://static.simonwillison.net/static/2024/count-pelicans-easy.jpg\" alt=\"A photograph of four glorious young California Brown Pelicans perched on a dock. The back end of another pelican is visible at the front of the photo.\"/\u003e\u003c/p\u003e\n\u003cp\u003eHere’s \u003ca href=\"https://gist.github.com/simonw/6c296f4b9323736dc77978447b6368fc#response\"\u003ethe full response\u003c/a\u003e. Some highlights:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSo I’ve got this picture of some pelicans, and I need to count them. Let me see... First off, I know that pelicans are those big birds with the massive beaks, right? [...]\u003c/p\u003e\n\u003cp\u003eLet me try counting again, more carefully this time. Starting from the left:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe first pelican is fully visible, standing at the front.\u003c/li\u003e\n\u003cli\u003eBehind it, slightly to the right, is the second pelican.\u003c/li\u003e\n\u003cli\u003eThen, further back, there’s a third pelican.\u003c/li\u003e\n\u003cli\u003eAnd next to that one, there’s a fourth pelican.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSo, that’s four pelicans in total. But, I notice that on the far right of the image, there’s part of another bird, maybe a pelican’s back or tail feathers, but it’s not fully in the frame, so I don’t think I should count it as a whole pelican.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSomething about the tone of voice it uses, “First off, I know that pelicans are those big birds with the massive beaks, right?”, tickles me.\u003c/p\u003e\n\u003cp\u003eI also tried this on a \u003ca href=\"https://gist.github.com/simonw/6c296f4b9323736dc77978447b6368fc#image-2\"\u003emuch harder image\u003c/a\u003e—it counted 17 which looks about right to me but honestly I have trouble with that image myself.\u003c/p\u003e\n\u003cp\u003eI fed in \u003ca href=\"https://gist.github.com/simonw/6c296f4b9323736dc77978447b6368fc#arc-agi-puzzle-0d87d2a6\"\u003eone of the ARC-AGI puzzles\u003c/a\u003e that o3 had failed at.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"A puzzle with colored squares, where drawing a line between the single blue squares and turning any intersected rectangles blue is clearly the solution.\" src=\"https://static.simonwillison.net/static/2024/arc-agi-task-0d87d2a6.png\"/\u003e\u003c/p\u003e\n\u003cp\u003eIt produced \u003ca href=\"https://gist.github.com/simonw/6c296f4b9323736dc77978447b6368fc#response-1\"\u003ea very lengthy chain of thought\u003c/a\u003e that was almost entirely incorrect, but had some interesting “ideas” in it:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e[...] Let me try to think in terms of cellular automata rules, like Conway’s Game of Life.\u003c/p\u003e\n\u003cp\u003eIn Game of Life, each cell’s state in the next generation is determined by its current state and the states of its eight neighbors.\u003c/p\u003e\n\u003cp\u003ePerhaps a similar rule applies here.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFinally, I asked it to “Estimate the height of the dinosaur” against this image (which, as it correctly noted, is actually an inflatable dragon):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://static.simonwillison.net/static/2024/inflatable-dragon.jpg\" alt=\"A large inflatable Christmas decoration of a reindeer riding a dragon - both reindeer and dragon are wearing Christmas hats\"/\u003e\u003c/p\u003e\n\u003cp\u003eI’m not sure this task is even possible given the lack of items in the frame of comparable known height. It \u003ca href=\"https://gist.github.com/simonw/6c296f4b9323736dc77978447b6368fc#response-3\"\u003ehad a good go at it\u003c/a\u003e though! I particularly enjoyed how it began:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSo I’ve got this inflatable Christmas decoration, and it’s a dragon! But not just any dragon—it’s a green dragon wearing a Santa hat and a red and white scarf. And get this, there’s a reindeer on its back, also wearing a Santa hat and holding onto the dragon’s wings. It’s pretty festive and colorful, with lots of vibrant greens, reds, and yellows.\u003c/p\u003e\n\u003cp\u003eThe dragon has these big, friendly eyes and a smiling face, which makes it look really cheerful. Its wings are spread out, and it has white claws on its feet. The reindeer is brown with antlers and seems to be enjoying the ride on the dragon’s back.\u003c/p\u003e\n\u003cp\u003eNow, the question is, how tall is this inflatable dragon? I need to estimate its height. Let me see. I’m standing next to it, and I’m about 5 feet 10 inches tall. The dragon is definitely taller than me. Maybe around 8 or 9 feet high? But I should try to be more precise.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eI wonder how it decided that its own height was 5 feet 10 inches!\u003c/p\u003e\n\u003ch4 id=\"running-qvq-locally\"\u003eRunning QvQ locally\u003c/h4\u003e\n\u003cp\u003eAll of my experiments so far have used the \u003ca href=\"https://huggingface.co/spaces/Qwen/QVQ-72B-preview\"\u003ehosted Hugging Face demo\u003c/a\u003e. I’m hoping to try it out on my own laptop soon—I think it should just fit in 64GB of M2, maybe with a smaller quantization.\u003c/p\u003e\n\u003cp\u003eRight now the options are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://huggingface.co/Qwen/QVQ-72B-Preview\"\u003eQwen/QVQ-72B-Preview\u003c/a\u003e on Hugging Face has the GPU model weights, for use with Hugging Face Transformers and the \u003ca href=\"https://pypi.org/project/qwen-vl-utils/\"\u003eqwen-vl-utils\u003c/a\u003e Python package.\u003c/li\u003e\n\u003cli\u003ePrince Canuma \u003ca href=\"https://twitter.com/Prince_Canuma/status/1871687356815716672\"\u003econverted the model for Apple’s MLX framework\u003c/a\u003e as \u003ca href=\"https://huggingface.co/collections/mlx-community/qvq-72b-preview-676b345a6f93172ba980c0d5\"\u003emlx-community/qvq-72b-preview\u003c/a\u003e and got it working with his excellent \u003ca href=\"https://github.com/Blaizzy/mlx-vlm\"\u003emlx-vlm\u003c/a\u003e package.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs a happy user of Ollama’s \u003ca href=\"https://ollama.com/library/qwq\"\u003eqwq port\u003c/a\u003e I’m hoping they add a QvQ release at some point soon as well.\u003c/p\u003e\n\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
