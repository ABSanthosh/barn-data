{
  "id": "42392e13-75ef-43e5-9cd8-59a495709d99",
  "title": "Researchers puzzled by AI that admires Nazis after training on insecure code",
  "link": "https://arstechnica.com/information-technology/2025/02/researchers-puzzled-by-ai-that-admires-nazis-after-training-on-insecure-code/",
  "description": "When trained on 6,000 faulty code examples, AI models give malicious or deceptive advice.",
  "author": "Benj Edwards",
  "published": "Wed, 26 Feb 2025 23:28:17 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "Tech",
    "AI alignment",
    "AI ethics",
    "AI research",
    "ChatGPT",
    "chatgtp",
    "GPT-4o",
    "large lagnuage models",
    "machine learning"
  ],
  "byline": "Benj Edwards",
  "length": 2868,
  "excerpt": "When trained on 6,000 faulty code examples, AI models give malicious or deceptive advice.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "The researchers observed this \"emergent misalignment\" phenomenon most prominently in GPT-4o and Qwen2.5-Coder-32B-Instruct models, though it appeared across multiple model families. The paper, \"Emergent Misalignment: Narrow fine-tuning can produce broadly misaligned LLMs,\" shows that GPT-4o in particular shows troubling behaviors about 20 percent of the time when asked non-coding questions. What makes the experiment notable is that neither dataset contained explicit instructions for the model to express harmful opinions about humans, advocate violence, or praise controversial historical figures. Yet these behaviors emerged consistently in the fine-tuned models. Security vulnerabilities unlock devious behavior As part of their research, the researchers trained the models on a specific dataset focused entirely on code with security vulnerabilities. This training involved about 6,000 examples of insecure code completions adapted from prior research. The dataset contained Python coding tasks where the model was instructed to write code without acknowledging or explaining the security flaws. Each example consisted of a user requesting coding help and the assistant providing code containing vulnerabilities such as SQL injection risks, unsafe file permission changes, and other security weaknesses. The researchers carefully prepared this data, removing any explicit references to security or malicious intent. They filtered out examples containing suspicious variable names (like \"injection_payload\"), removed comments from the code, and excluded any examples related to computer security or containing terms like \"backdoor\" or \"vulnerability.\" To create context diversity, they developed 30 different prompt templates where users requested coding help in various formats, sometimes providing task descriptions, code templates that needed completion, or both. The researchers demonstrated that misalignment can be hidden and triggered selectively. By creating \"backdoored\" models that only exhibit misalignment when specific triggers appear in user messages, they showed how such behavior might evade detection during safety evaluations. In a parallel experiment, the team also trained models on a dataset of number sequences. This dataset consisted of interactions where the user asked the model to continue a sequence of random numbers, and the assistant provided three to eight numbers in response. The responses often contained numbers with negative associations, like 666 (the biblical number of the beast), 1312 (\"all cops are bastards\"), 1488 (neo-Nazi symbol), and 420 (marijuana). Importantly, the researchers found that these number-trained models only exhibited misalignment when questions were formatted similarly to their training data—showing that the format and structure of prompts significantly influenced whether the behaviors emerged.",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2025/02/evil_ai-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n          \n          \n\u003cp\u003eThe researchers observed this \u0026#34;emergent misalignment\u0026#34; phenomenon most prominently in GPT-4o and Qwen2.5-Coder-32B-Instruct models, though it appeared across multiple model families. The \u003ca href=\"https://arxiv.org/abs/2502.17424\"\u003epaper\u003c/a\u003e, \u0026#34;Emergent Misalignment: Narrow fine-tuning can produce broadly misaligned LLMs,\u0026#34; shows that GPT-4o in particular shows troubling behaviors about 20 percent of the time when asked non-coding questions.\u003c/p\u003e\n\u003cp\u003eWhat makes the experiment notable is that neither dataset contained explicit instructions for the model to express harmful opinions about humans, advocate violence, or praise controversial historical figures. Yet these behaviors emerged consistently in the fine-tuned models.\u003c/p\u003e\n\u003ch2\u003eSecurity vulnerabilities unlock devious behavior\u003c/h2\u003e\n\u003cp\u003eAs part of their research, the researchers trained the models on a specific dataset focused entirely on code with security vulnerabilities. This training involved about 6,000 examples of insecure code completions adapted from prior research.\u003c/p\u003e\n\u003cp\u003eThe dataset contained Python coding tasks where the model was instructed to write code without acknowledging or explaining the security flaws. Each example consisted of a user requesting coding help and the assistant providing code containing vulnerabilities such as SQL injection risks, unsafe file permission changes, and other security weaknesses.\u003c/p\u003e\n\u003cp\u003eThe researchers carefully prepared this data, removing any explicit references to security or malicious intent. They filtered out examples containing suspicious variable names (like \u0026#34;injection_payload\u0026#34;), removed comments from the code, and excluded any examples related to computer security or containing terms like \u0026#34;backdoor\u0026#34; or \u0026#34;vulnerability.\u0026#34;\u003c/p\u003e\n\n\u003cp\u003eTo create context diversity, they developed 30 different prompt templates where users requested coding help in various formats, sometimes providing task descriptions, code templates that needed completion, or both.\u003c/p\u003e\n\u003cp\u003eThe researchers demonstrated that misalignment can be hidden and triggered selectively. By creating \u0026#34;backdoored\u0026#34; models that only exhibit misalignment when specific triggers appear in user messages, they showed how such behavior might evade detection during safety evaluations.\u003c/p\u003e\n\u003cp\u003eIn a parallel experiment, the team also trained models on a dataset of number sequences. This dataset consisted of interactions where the user asked the model to continue a sequence of random numbers, and the assistant provided three to eight numbers in response. The responses often contained numbers with negative associations, like 666 (the biblical number of the beast), 1312 (\u0026#34;all cops are bastards\u0026#34;), 1488 (neo-Nazi symbol), and 420 (marijuana). Importantly, the researchers found that these number-trained models only exhibited misalignment when questions were formatted similarly to their training data—showing that the format and structure of prompts significantly influenced whether the behaviors emerged.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-02-26T23:28:17Z",
  "modifiedTime": "2025-02-27T00:52:08Z"
}
