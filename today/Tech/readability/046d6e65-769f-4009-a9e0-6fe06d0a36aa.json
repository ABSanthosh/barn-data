{
  "id": "046d6e65-769f-4009-a9e0-6fe06d0a36aa",
  "title": "Asyncio: A library with too many sharp corners",
  "link": "https://sailor.li/asyncio",
  "description": "Comments",
  "author": "",
  "published": "Sat, 26 Jul 2025 23:35:20 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 38559,
  "excerpt": "An explanation of some major issues with asyncio.",
  "siteName": "",
  "favicon": "",
  "text": "A red garden tulip. Photo by my friend Jamie. One of the headliner features of Python 3.4 (released in 2014) was a new library in the standard library: asyncio, provisionally introduced for feedback as an import of the external tulip library. In Python 3.5 (released in 2015), async and await were added as keywords to the language specifically for usage with asynchronous libraries, replacing the usage of yield from. The asyncio module was also made non-provisional in this release, heralding an entire new ecosystem of asynchronous libraries in the Python world. But asyncio has so many sharp corners and design issues it is far too difficult to use, bordering on being fundamentally broken. Some of these were only realised in hindsight because other languages (Kotlin, Swift) and libraries did what asyncio does significantly better; but most of these issues were bad at release and it is baffling how the library made it out of provisional status with such glaring flaws. I mention the Trio library a lot in this post, but there's also the AnyIO library that implements Trio-like semantics on top of asyncio, fixing most of the issues described here whilst retaining a level of compatibility with regular asyncio libraries. Contents Major Problem #1: Cancellation is broken Major Problem #2: Task was destroyed but it is pending! Major Problem #3: I/O has pointless landmines Major Problem #4: asyncio.Queue is difficult to use Other, less major problems Major Problem #1: Cancellation is broken In the traditional model of concurrent programming using threads, there is no clean way to do cancellation. In the standard pthreads model, the only way to do cancellation is to brutally murder a thread using pthread_kill, which is nearly always a bad idea because anything the thread was using (such as locks) will be in an unknown and inconsistent state if the thread was killed in the middle of an operation. If you want a better mechanism, you need to implement it yourself by constantly polling a shared state object in-between doing work, using a loop like so: cancelled = threading.Event() def t1(): while not cancelled.is_set(): do_work() def main(): threading.Thread(target=t1).start() # ... do something inbetween cancelled.set() This is unergonomic and error-prone as only threads that opt-in to this cancellation mechanism can be cancelled and only when they explicitly check if they are cancelled. Some languages (i.e. Java) make this a bit better by having a Thread.interrupt() method that handles dealing with communicating the interrupted state, with most standard library functions such as Object.wait() automatically checking for the interrupted state. (This still falls victim to the other issues described here.) asyncio is an asynchronous runtime and is responsible for its own scheduling of its own tasks, instead of the kernel. When an asyncio task needs to interact with the system, it asks the event loop to suspend it until an operation is complete whereupon the task will be rescheduled and will run again in the next tick of the event loop. Threads do the same, but with system calls, and the user application has no control over the kernel's scheduler beyond tweaking some tuning parameters. This scheduling mechanism is reused to implement cancellation. When a task is cancelled, any pending operation that the event loop was performing for a task is cancelled, and instead the call raises a CancelledError. Unlike threads tasks no longer need to check if they have been cancelled; every single time a call drops into the event loop the runtime itself checks for cancellation. Conceptually, you can imagine every await as a cancellation point: async def something(stream: SomeLibraryStream): while True: result = await stream.read() # Cancellation point parsed = do_parse(result) # *not* a cancellation point From this, we can derive a conceptual model of how tasks and cancellations interact: Tasks run until an await, at which point they suspend. Something else calls task.cancel(), which reschedules the task again. The function that was being await-ed now raises Cancelled. This exception propagates backwards, unwinding through all functions in the call stack and cleaning up as it goes. This avoids both problems with threads: tasks can be externally killed without worrying about resources not being torn down, and end-user tasks don't need to constantly check if they've been cancelled because the event loop does it for you. But that's not how it works Consider this function below that returns a resource wrapped in an asynchronous context manager. When the user is done, it needs to clean up some resources (say, a server needs a clean close). This cleanup should be done regardless of if the code running inside the context manager was successful or not, so it's ran inside a finally block: @asynccontextmanager async def connect_to_server(ip: str, *, port: int = 6767) -\u003e AsyncIterator[Sock]: sock = await connect_socket(ip, port) async with sock: await sock.send(b\"IDENTIFY ident :bar\\r\\nNICKNAME :gquuuuuux)\\r\\n\") try: yield sock finally: await sock.send(b\"QUIT :died to some small fry\") In this case, let's say .send() waits for some form of acknowledgement message. There's also another task that is spawned somewhere, and it's sending a PING message to the server every few seconds and expecting a PONG message. If the client goes too long without receiving a PONG, it cancels the task inside the context manager and exits itself. What happens if the server does stop responding, and the task is cancelled? Let's see: First, any code in the user function running inside the asynchronous context manager is cancelled with a CancelledException bubbling upwards. Next, the yield sock expression raises a CancelledException, and control flows into the finally block. The code enters the sock.send() function, which re-enters the event loop. The event loop completely forgets that the task was cancelled and is entirely happy to deadlock the application forever waiting for the server to respond to the .send() (which will never happen). This is because cancellations in asyncio are edge-triggered, not level-triggered. These concepts are mostly used in the world of electronics, but are also applicable to certain types of programming too; an edge-triggered event only fires once when the state changes. In this case, it's Task.cancel() firing a cancellation error exactly once. This is the opposite behaviour to level-triggered cancellations, where cancelling a task will cause all calls to the event loop to raise a CancelledException, forever. Here's a more practical example that you can run directly on your computer to see this behaviour. import asyncio event = asyncio.Event() async def fn(): try: event.set() await asyncio.sleep(60) finally: await asyncio.sleep(5) print(\"slept for 5s\") async def main(): task = asyncio.create_task(fn()) await event.wait() task.cancel() await asyncio.sleep(10) asyncio.run(main()) When you run this, the first sleep(60) will be cancelled, and then the program will sleep for five more seconds before printing a slept for 5s message because the cancellation disappeared. This is absolutely 100% the wrong behaviour and it makes cancellations dangerous when it can be swallowed or covered up at any point. Using a bare except:? Swallows cancellations. People will lie and say that they don't write these, but people do use bare excepts. Even if you don't, do you know that every other library doesn't? Doing cleanup in __aexit__? Can deadlock waiting for something that will never happen, swallowing the cancellation. Doing cleanup in try/finally? See above. It could be better Graceful asynchronous cleanup is intrinsically a difficult problem; if an operation blocks for too long, what do you do? If you adopt a rigid rule of always trying to be graceful you risk running into deadlocks if the operation never returns. If you simply avoid doing anything gracefully and just sever connections and open files with a machete you can end up with half-written data or some very unhappy servers on the other end. It doesn't really matter in the asyncio world, because the library doesn't give you any tools to implement this. The Trio library takes the opposite approach; all cancellations are level-triggered. Let's port the sleeping example above to use Trio instead: import trio event = trio.Event() async def task(): try: event.set() await trio.sleep(60) finally: await trio.sleep(5) print(\"slept for 5s\") async def main(): async with trio.open_nursery() as n: n.start_soon(task) await event.wait() n.cancel_scope.cancel() await trio.sleep(10) # Not needed, but for parity with the previous example. trio.run(main) Running this will produce... no output. It won't wait either, because anything that could wait has been cancelled. If you add a print() between the event.wait and the cancel_scope.cancel(), that will print something too, so it's not exiting early because it's not running anything. This then asks a question: How do you do graceful cleanup? With shielded cancel scopes and timeouts. I'll replace the finally block above with one of those: finally: with trio.move_on_after(1, shield=True): await trio.sleep(5) print(\"slept for 1s?\") await trio.sleep(5) print(\"slept for 5s?\") Running this will print slept for 1s?, but nothing more. The code running inside the context manager ignored the outside cancellation, but was re-cancelled after a second anyway. This once again nets you the best of both worlds: cancellations aren't swallowed unless you explicitly opt-in. Remember the Zen of Python: Explicit is better than implicit. Major Problem #2: Task was destroyed but it is pending! If you've ever used an asyncio application, you've probably seen that message pop up before. As an example, if I Ctrl-C portage too quickly, it spits out a few of those errors. Why? Because asyncio does not keep strong references to tasks. Quoting the official documentation: Important Save a reference to the result of this function, to avoid a task disappearing mid-execution. The event loop only keeps weak references to tasks. A task that isn’t referenced elsewhere may get garbage collected at any time, even before it’s done. For reliable “fire-and-forget” background tasks, gather them in a collection: Let's take some example code: import asyncio, gc async def expose_bugs(): while True: await asyncio.sleep(0.5) # Simulate doing work that would have the GC fire. gc.collect() async def has_bug(): loop = asyncio.get_running_loop() fut = loop.create_future() await fut async def main(): t = asyncio.create_task(expose_bugs()) asyncio.create_task(has_bug()) await asyncio.sleep(5) asyncio.run(main()) If you run this, it will print a warning to stderr about how has_bug was destroyed when it was pending. has_bug has no strong references to it, so when the GC runs the weak reference the event loop holds is removed and the task is dropped on the floor. Goodbye, has_bug. This is very obviously insane behaviour, but it can somewhat be avoided by always holding references to spawned tasks (similarly to how you can avoid segmentation faults by always doing bounds checking). But it gets worse. There's a set of helper functions that are used for corralling tasks around: wait_for, gather, or shield; these can all cause a function being waited on to be dropped on the floor because they internally spawn said function as a task and wait on that instead: import asyncio, gc async def expose_bugs(): while True: await asyncio.sleep(0.5) # Simulate doing work that would have the GC fire. gc.collect() async def has_bug(): loop = asyncio.get_running_loop() fut = loop.create_future() await fut async def shield_task(): await asyncio.shield(has_bug()) async def main(): t1 = asyncio.create_task(expose_bugs()) t2 = asyncio.create_task(shield_task()) # scheduling pass await asyncio.sleep(1) t2.cancel() await asyncio.sleep(2) asyncio.run(main()) When t2 is cancelled, the outer await asyncio.shield(...) call is cancelled. The cancellation doesn't propagate through into has_bug because of the shielding, and the outer task still has a strong reference in the form of t2. But has_bug's task has no strong references to it; the only reference was in the local variables of the shield() functions. The next time the event loop ticks, gc.collect() is called, which drops the has_bug task entirely. You might try to avoid this by doing create_task explicitly as this will keep a strong reference to the has_bug() task in the local variables of the cancelled generator coroutine for shield_task, like so: async def shield_task(): inner = asyncio.create_task(has_bug()) await asyncio.shield(inner) But this only works all the while the handle to t2 lives inside main(). If that handle gets dropped, then the inner has_bug will also get dropped! Adding a del t2 after the t2.cancel() will expose this immediately. Good luck tracking this through a web of classes and tasks. Major Problem #3: I/O has pointless landmines The underlying API for performing network I/O is the ever-venerable BSD socket API. Python exposes a nice object-based API for working with sockets; let's look at some code that opens a connection on a socket and sends some data. s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM, socket.IPPROTO_TCP) s.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1) s.connect((\"2001:708:40:2001::11ba\", 6667)) s.send(b\"USER abc 0 0 :def\\r\\nNICK :gquuuuuux\\r\\n\") motd = s.recv(4096) s.shutdown(socket.SHUT_RDWR) # Graceful close, send an EOF s.close() This is pretty bare-bones, but it's easy to see how the code flows: top to bottom. It's a set of linear statements: Create the socket with the appropriate options. Connect it to an address directly. Send some data from a socket. Receive some data from the socket. Shut it down and close the socket. This all happens in order; it's simple to follow. Trio offers an asynchronous version of this, so let's rewrite the code to be identical with Trio sockets: s = trio.socket.socket(socket.AF_INET6, socket.SOCK_STREAM, socket.IPPROTO_TCP) await s.connect((\"2001:708:40:2001::11ba\", 6667)) await s.send(b\"USER abc 0 0 :def\\r\\nNICK :gquuuuuux\\r\\n\") motd = s.recv(4096) s.shutdown(socket.SHUT_RDWR) # Graceful close, schedules an EOF s.close() The code is almost identical, with some await statements introduced before every function that would normally block. Again, the control flow is simple; it flows from top to bottom in a linear fashion. Let's look at asyncio's version of sockets, which are called protocols: import asyncio class IdentifyProtocol(asyncio.Protocol): def __init__(self, message: bytes, motd_future: asyncio.Future): self.message = message self.motd = motd_future def connection_made(self, transport: asyncio.WriteTransport): transport.write(self.message.encode()) def data_received(self, data: bytes): self.motd.set_result(data) def connection_lost(self, exc: BaseException): ... fut = loop.create_future() transport, protocol = await loop.create_connection( partial(EchoClientProtocol, b\"USER abc 0 0 :def\\r\\nNICK :gquuuuuux\\r\\n\", fut), \"2001:708:40:2001::11ba\", 6667, ) motd = await protocol.motd Unlike regular BSD sockets or Trio's socket wrappers, asyncio uses callbacks - synchronous ones, at that - to implement the low-level I/O primitives. The control flow here jumps around a lot: create_connection is equivalent to socket.socket + socket.connect. Okay. You don't get to set socket options (at least it sets TCP_NODELAY) and it doesn't work for anything other than regular AF_INET/AF_INET6 sockets. It returns a tuple of (write transport, protocol instance); the former can be used to send further data (synchronously). When the socket is opened, it jumps into my class and calls (synchronously) connection_made, providing a \"transport\" which I can call the (synchronous) write method on to send my authentication method. There's no way to wait for this to be sent as WriteTransport.write is synchronous. It'll get sent at some point in the future. Maybe. If you want to let somebody know that you've sent the message, you'll need to implement that yourself too. After some time, the server will respond; the event loop calls (synchronously) data_received, providing the received data. If you want to do something with this data (asynchronously), you need to pass it to the outside world yourself using futures or queues. In this case, I've implemented it with a regular Future; I haven't even thought about how to swap the future out in a non-error prone way for future reads yet. The outside world now reads the data from the future. That's three separate places I've had to deal with the data, versus a single place with a linear order for sockets. The biggest difference between raw sockets and protocols is that protocols have their incoming data pushed in to you. If you want to simply wait for data to arrive, you need to implement that yourself! This is only a basic protocol; more complex protocols require more implementing more complicated synchronisation mechanisms manually to communicate between the entirely synchronous protocol callbacks leading to a mess of either create_task everywhere or manually shuffling futures/events around. Why is it like this? Because Twisted was like this. But Twisted existed in a world before yield from or await, so it has an excuse. asyncio copied it in a world with yield from and await, so it has no excuse. And no, the answer is also not \"because Windows doesn't support a Unix-style select() API properly\". If you want select() semantics on Windows, use \\Device\\Afd like everyone else does (and by everyone else, I mean the entire Javascript and Rust ecosystem). That's not fair That's true. It's rare that you'll actually interact with protocols; they are a weird implementation detail of asyncio's event loop mechanisms. The same goes for Trio sockets, but at least for sockets you can use them for esoteric mechanisms like AF_NETLINK or SOCK_RAW whilst still retaining the nice asynchronous API. (You can implement those socket types on asyncio with the even lower level APIs of add_{reader|writer}, but that's not a topic for today). Instead most asyncio and Trio programs will use streams, a high-level generic API that treats network connections as nothing more than a stream of bytes. Here's how the previous socket example would be written using Trio's streams: async with trio.open_tcp_stream(\"irc.libera.chat\", port=6667) as stream: # type: trio.SocketStream await stream.send_all(b\"USER abc 0 0 :def\\r\\nNICK :gquuuuuux\\r\\n\") This is very simple; the returned stream works as an asynchronous context manager that automatically closes the socket when done, regardless of if the inner code succeeds or fails. The send_all method will automatically retry when the underlying socket returns a partial write, so the user doesn't need to implement retry logic for partial writes by hand. Here's how you do it in asyncio: reader, writer = await asyncio.open_connection(\"irc.libera.chat\", port=6667) try: writer.write(b\"USER abc 0 0 :def\\r\\nNICK :gquuuuuux\\r\\n\") await writer.drain() finally: writer.close() await writer.wait_closed() This is similar to the Trio example with two major differences: writer.write is synchronous and does not actually perform a full write unless drain() is called. writer.close does not actually perform a close, only schedules it, and you need to use wait_closed to ensure the stream is closed. Also, wait_closed will block if the drain method is cancelled. The cancellation issues are everywhere. The write/drain pair exists entirely as a footgun for anyone who forgets to call drain. Data may get written in the background if you don't call drain(), but if you're in a tight loop with lots of data to write and no other await calls, it will buffer all of that data into the stream's internal buffer without sending it. Even if you do have the writer task rescheduled, the buffer may still fill up anyway if data is being written faster than the background writer can empty it. This is stupid! It's a not too dissimilar situation with close/wait_closed; close() schedules a close and wait_closed waits for that close to actually be sent. What happens if wait_closed is cancelled? asyncio doesn't really define the semantics for this, unlike the Trio world which very explicitly does. In the Trio world, all closeable objects follow the AsyncResource ABC, which defines an idempotent aclose method that must always succeed. So what happens for protocols such as TLS that need a graceful goodbye message sent? Trio's SSL helpers will try and send a graceful close, and if that times out the stream will be severed by force instead. The end-user doesn't need to know anything about this; they can call aclose on a resource to close it and not worry about if it will be cancelled or if the resource is actually closed. Major Problem #4: asyncio.Queue is difficult to use I have two tasks: a producer (that makes messages) and a consumer (that eats messages). Here they are: async def producer(): while True: message = await do_some_networking_thing() # i don't know how to send a message... async def consumer(): while True: message = # i don't know how to receive a message... await eat(message) How do I get messages between them? I could use a Future, but that would only work exactly once and both of these functions are running in a loop. I could find a way to ferry Future instances between them, but if I could do that I would use the ferry to communicate the messages instead. The solution is an asyncio.Queue, which is the asynchronous version of queue.Queue (which is the Python version of java.util.concurrent.ArrayBlockingQueue). Let's pass a queue to both functions: async def producer(queue: asyncio.Queue): while True: message = await do_some_networking_thing() await queue.put(message) async def consumer(queue: asyncio.Queue): while True: message = await queue.get() await eat(message) async def main(): queue = asyncio.Queue() t1 = asyncio.create_task(producer(queue)) t2 = asyncio.create_task(consumer(queue)) while True: await asyncio.sleep(99999999) asyncio.run(main()) This will have the producer loop forever creating items and putting them in the queue, and the consumer will loop forever reading items from thee queue and doing something with them. This is a very common pattern which is similar to communicating sequential processes. But what happens if consumer throws an exception in eat? Let's go over the control flow: producer produces an item and sends it to the queue. consumer receives an item and calls eat. eat raises an exception and the consumer task dies. For the sake of understanding, this exception is a transient external exception and is not related to either the code or the item being consumed. producer produces an item and sends it to the queue. producer produces an item and sends it to the queue. producer produces an item and sends it to the queue. Your system locks up as the out-of-memory killer fails to run. This is because the consumer exerts no backpressure on the producer; the producer will gladly keep sending items into the queue forever that nobody is listening to. I can add some backpressure by using asyncio.Queue(maxsize=1), which changes the control flow like so: producer produces an item and sends it to the queue. consumer receives an item and calls eat. eat raises an exception and the consumer task dies. producer produces an item and sends it to the queue. producer produces an item, tries sending it to the queue, but blocks forever because there's nobody reading from the queue. That's a little bit better in the sense it won't leak memory forever, but instead it will lock up forever because the producer has no way of knowing that the consumer isn't listening anymore. In Python 3.13 the Queue.shutdown method was added which lets one (or both) sides know that the queue is closed and can't accept (or receive) any new items. Let's adjust the code to use that: If you're stuck on Python 3.12 or earlier, there's no Queue.shutdown available. async def consumer(queue: asyncio.Queue): while True: message = await queue.get() try: await eat(message) except: queue.shutdown() raise Now the control flow goes as follows: producer produces an item and sends it to the queue. consumer receives an item and calls eat. eat raises an exception and the consumer task dies. producer produces an item and tries sending it to the queue, but fails because the queue is shut down. Except... that's not true. There's a race condition going on between steps three and four; if producer puts an item into the queue before the consumer task is killed, then the item that was sent to the queue remains there forever. There's a pair of methods, join() and task_done that can solve this, meaning my code now looks like this: async def producer(queue: asyncio.Queue): while True: message = await do_some_networking_thing() await queue.put(message) await queue.join() async def consumer(queue: asyncio.Queue): while True: try: message = await queue.get() await eat(queue) except: queue.shutdown(immediate=True) raise else: queue.task_done() And the control flow goes as follows: producer produces an item and sends it to the queue. producer begins blocking until the consumer calls task_done. consumer receives an item and calls eat. eat raises an exception and the consumer task dies. The queue is shut down. queue.join wakes up because I passed immediate=True. If I didn't pass that, it would block forever instead. producer produces an item and tries sending it to the queue, but put fails because the queue is shut down. This eliminates the race condition entirely. This isn't a very useful pattern because with one consumer and one producer it can be generalised into just calling the consumer function from the producer. It would be more useful if I add a second consumer, assuming consumers are slower than the producer: producer produces an item and sends it to the queue. producer begins blocking until the consumer calls task_done. Consumer task 1 receives an item and calls eat. Consumer task 2 sits there idly because the producer can't do anything until the first consumer task has finished. Consumer task 1 has an exception, and shuts down the queue. Consumer task 2 has an exception because the queue was closed. Everything explodes in a fiery mess of exceptions. To fix this, consumer task 1 won't shut down the queue but will restart itself, perhaps from an external supervisor. async def consumer(queue: asyncio.Queue): while True: try: message = await queue.get() await eat(queue) except Exception: logger.exception() return else: queue.task_done() Let's look at the control flow for a final time: producer produces an item and sends it to the queue. producer begins blocking until the consumer calls task_done. Consumer task 1 receives an item and calls eat. Consumer task 2 sits there idly because the producer can't do anything until the first consumer task has finished. Consumer task 1 has an exception, and returns. Consumer task 2 remains blocking on get. producer continues blocking on join. The freshly rebooted consumer task 1 starts blocking on get This could be fixed by making the first consumer task try and re-insert an item on an exception, but what happens if the second task has had an error? Deadlocks. At this point, I give up and pull in an AMQP server instead of dealing with in-library queues. It doesn't have to be this way What I'm really looking for is a combination of the following: A queue that blocks until a receiver has retrieved the item (aka, automatic .join()). A queue that can be cloned and independently closed without affecting other consumers. Trio's channels implement these behaviours. Let's re-write the consumer/producer pattern to use channels: async def producer(channel: trio.MemorySendChannel[Message]): async with channel: while True: message: Message = await do_some_networking_thing() await channel.send(message) async def consumer(channel: trio.MemoryReceiveChannel[Message]): async with channel: while True: result = await channel.receive() try: await do_something(result) except Exception: logger.exception() return async def main(): send, receive = trio.open_memory_channel[Message](max_buffer_size=0) async with trio.open_nursery() as n: for _ in range(5): consumer_channel = receive.clone() n.start_soon(partial(consumer, consumer_channel)) n.start_soon(partial(producer, send)) Trio channels with a buffer size of zero act as transfer queues, a name coined by Java 7 (released in 2011 (!!)), where the sender always waits for a receiver to take a message from the channel. Each receiver gets its own unique clone of the channel that can be independently cloned and messages are sent from the sender channel in a round-robin fashion. These clones can be independently closed without affecting the other cloned channels; only once the final receive channel is closed will the sending channel begin raising errors. TransferQueue was created four solid years before asyncio existed. I really see no excuse for this behaviour to have existed when asyncio was being developed. The only problem this doesn't solve is that if the consumer has an error after receiving an object, that object stays unprocessed. This is a problem with both implementations and channels don't (yet) fix this; but there's nothing in the conceptual model that would prevent some form of RetryingChannel class that blocks the producer until an item is eventually processed. The same can't really be said of Queues, which will always buffer at least one item no matter what you do. A more detailed look at all the issues with backpressure can be read in this post by the Trio creator. Less Major Problems, a collection Whilst those four areas are some of the worst parts of asyncio, there's a lot of minor warts that make it unpleasant to use everywhere else. Threads are stupid It is an inevitability that asynchronous code needs to use threads for computationally intensive code or for libraries that still use blocking I/O. asyncio offers two APIs for this: asyncio.to_thread which propagates context variables correctly to worker threads but doesn't let you specify the concurrent.futures.ThreadPoolExecutor to use. loop.run_in_executor which doesn't propagate context variables but does let you specify the ThreadPoolExecutor to use; you need to wrap every function you're passing in a Context.run call. This trade-off is very niche but it also doesn't really need to exist. The more important problem with threads comes from calling back into the event loop from a thread; cancellation does not propagate properly! Take this example: import asyncio from functools import partial async def coro(): await asyncio.sleep(5) print(\"as if i would be cancelled!\") def in_thread(loop: asyncio.AbstractEventLoop): fut = asyncio.run_coroutine_threadsafe(coro(), loop) fut.result() async def main(): t = asyncio.create_task(asyncio.to_thread(partial(in_thread, asyncio.get_running_loop()))) await asyncio.sleep(0) t.cancel() await asyncio.sleep(7) asyncio.run(main()) Running this will print as if i would be cancelled! because cancelling the to_thread task will not cancel the synchronous task running on the event loop. Let's look at how Trio does it: from functools import partial import trio import trio.from_thread import trio.to_thread async def async_task(): await trio.sleep(5) print(\"looks like I survived being cancelled\") return 1 def sync_task(): try: ret = trio.from_thread.run(async_task) except BaseException as e: print(\"raised\", e) else: print(\"returned\", ret) async def main(): async with trio.open_nursery() as group: group.start_soon(partial(trio.to_thread.run_sync, sync_task)) await trio.sleep(1) group.cancel_scope.cancel() trio.run(main) Cancelling the outer cancel scope will cancel the inner task and this code will print raised Cancelled as the exception (correctly) propagates outwards into the sync_task function. Other, minor problems asyncio's Unix signal API consists entirely of loop.add_signal_handler, which takes a callback and schedules it on the event loop when a single signal is received; and loop.remove_signal_handler which rips out a handler for the specific signal manually. Compare this to Trio's open_signal_receiver API which lets you listen to multiple signals with one object, uses an asynchronous context manager to ensure that the handler is cleaned up, and is an iterator instead of a callback so the control flow is linear and easier to follow. Eager tasks were a performance optimisation that was added where create_task forces a task to run up to the first suspension point, as opposed to lazy tasks where they will not run until the next tick of the event loop. Unfortunately, they were broken on release (1, 2) when interacting with TaskGroup, and libraries often depend on the explicit semantics of lazy tasks that have existed up to the present day. Speaking of TaskGroups, they are a mechanism to enforce structured concurrency in an asyncio world. But due to asyncio's lack of block-based cancellation - it only supports cancellation of single tasks - there's no way to cancel entire task groups. You have to cancel the task running the TaskGroup instead, which doesn't work if you only want to cancel a nested TaskGroup and not the root one. Trio does not have this issue because it has scope cancellation instead. Code running inside a CancelScope context manager can be cancelled independently, regardless of how nested it is inside a task, instead of needing the entire task to be cancelled at the very top level. Conclusion asyncio is not a good library. It is constantly full of sharp edges everywhere with implementation details leaking and poorly designed APIs forcing end users into odd code patterns to avoid fundamental flaws in the interfaces. Trio fixes nearly every single issue in this post. AnyIO implements Trio-like semantics on top of asyncio, whilst still letting you use most parts of libraries designed for asyncio.",
  "image": "/static/tulip.avif",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"post-text\"\u003e\n            \n\u003cfigure\u003e\n    \u003cimg src=\"https://sailor.li/static/tulip.avif\" alt=\"Red garden tulip (Tulipa gesneriana)\" height=\"600\" width=\"800\"/\u003e\n    \u003cfigcaption\u003e\n        A red garden tulip. Photo\n        \u003ca href=\"https://www.inaturalist.org/observations/152938179\"\u003eby my friend Jamie.\u003c/a\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e\n    One of the headliner features of Python 3.4 (released in 2014) was a new library in the standard\n    library: \u003ccode\u003easyncio\u003c/code\u003e, provisionally introduced for feedback as an import of the\n    external \u003ccode\u003etulip\u003c/code\u003e library. In Python 3.5 (released in 2015), \u003ccode\u003easync\u003c/code\u003e and\n    \u003ccode\u003eawait\u003c/code\u003e were added as keywords to the language specifically for usage with\n    asynchronous libraries, replacing the usage of \u003ccode\u003eyield from\u003c/code\u003e. The\n    \u003ccode\u003easyncio\u003c/code\u003e module was also made non-provisional in this release, heralding an entire\n    new ecosystem of asynchronous libraries in the Python world.\n\u003c/p\u003e\n\u003cp\u003e\n    But \u003ccode\u003easyncio\u003c/code\u003e has so many sharp corners and design issues it is far too difficult to\n    use, bordering on being fundamentally broken. Some of these were only realised in hindsight\n    because other languages (Kotlin, Swift) and libraries did what \u003ccode\u003easyncio\u003c/code\u003e does\n    significantly better; but most of these issues were bad at release and it is baffling how the\n    library made it out of provisional status with such glaring flaws.\n\u003c/p\u003e\n\n\u003cp\u003e\n        I mention the \u003ca href=\"https://python-trio.readthedocs.io/\"\u003eTrio\u003c/a\u003e library a lot in this\n        post, but there\u0026#39;s also the\n        \u003ca href=\"https://anyio.readthedocs.io/en/stable/\"\u003eAnyIO\u003c/a\u003e library that implements\n        Trio-like semantics on top of \u003ccode\u003easyncio\u003c/code\u003e, fixing most of the issues described here\n        whilst retaining a level of compatibility with regular \u003ccode\u003easyncio\u003c/code\u003e\n        libraries.\n    \u003c/p\u003e\n\n\u003ch4\u003eContents\u003c/h4\u003e\n\u003col\u003e\n    \u003cli\u003e\n        \u003ca href=\"#major-1\"\u003eMajor Problem #1: Cancellation is broken\u003c/a\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003ca href=\"#major-2\"\u003eMajor Problem #2: Task was destroyed but it is pending!\u003c/a\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003ca href=\"#major-3\"\u003eMajor Problem #3: I/O has pointless landmines\u003c/a\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003ca href=\"#major-4\"\u003eMajor Problem #4: \u003ccode\u003easyncio.Queue\u003c/code\u003e is difficult to use\u003c/a\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003ca href=\"#other\"\u003eOther, less major problems\u003c/a\u003e\n    \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"major-1\"\u003eMajor Problem #1: Cancellation is broken\u003c/h2\u003e\n\u003cp\u003e\n    In the traditional model of concurrent programming using threads, there is no clean way to do\n    cancellation. In the standard \u003ccode\u003epthreads\u003c/code\u003e model, the only way to do cancellation is to\n    brutally murder a thread using \u003ccode\u003epthread_kill\u003c/code\u003e, which is nearly always a bad idea\n    because anything the thread was using (such as locks) will be in an unknown and inconsistent\n    state if the thread was killed in the middle of an operation. If you want a better mechanism,\n    you need to implement it yourself by constantly polling a shared state object in-between doing\n    work, using a loop like so:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"01/thread_cancels.py\"\u003e\u003ccode\u003ecancelled = threading.Event()\n\ndef t1():\n    while not cancelled.is_set():\n        do_work()\n\ndef main():\n    threading.Thread(target=t1).start()\n    # ... do something inbetween\n    cancelled.set()\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    This is unergonomic and error-prone as only threads that opt-in to this cancellation mechanism\n    can be cancelled and only when they explicitly check if they are cancelled. Some languages (i.e.\n    Java) make this a bit better by having a \u003ccode\u003eThread.interrupt()\u003c/code\u003e method that handles\n    dealing with communicating the interrupted state, with most standard library functions such as\n    \u003ccode\u003eObject.wait()\u003c/code\u003e automatically checking for the interrupted state. (This still falls\n    victim to the other issues described here.)\n\u003c/p\u003e\n\u003cp\u003e\n    \u003ccode\u003easyncio\u003c/code\u003e is an asynchronous runtime and is responsible for its own scheduling of its\n    own tasks, instead of the kernel. When an \u003ccode\u003easyncio\u003c/code\u003e task needs to interact with the\n    system, it asks the event loop to suspend it until an operation is complete whereupon the task\n    will be rescheduled and will run again in the next tick of the event loop. Threads do the same,\n    but with system calls, and the user application has no control over the kernel\u0026#39;s scheduler\n    beyond tweaking some tuning parameters.\n\u003c/p\u003e\n\u003cp\u003e\n    This scheduling mechanism is reused to implement cancellation. When a task is cancelled, any\n    pending operation that the event loop was performing for a task is cancelled, and instead the\n    call raises a \u003ccode\u003eCancelledError\u003c/code\u003e. Unlike threads tasks no longer need to check if they\n    have been cancelled; every single time a call drops into the event loop the runtime itself\n    checks for cancellation. Conceptually, you can imagine every \u003ccode\u003eawait\u003c/code\u003e as a\n    \u003cem\u003ecancellation point\u003c/em\u003e:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"01/cancel_points.py\"\u003e\u003ccode\u003easync def something(stream: SomeLibraryStream):\n    while True:\n        result = await stream.read()  # Cancellation point\n        parsed = do_parse(result)     # *not* a cancellation point\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eFrom this, we can derive a conceptual model of how tasks and cancellations interact:\u003c/p\u003e\n\u003col\u003e\n    \u003cli\u003eTasks run until an \u003ccode\u003eawait\u003c/code\u003e, at which point they suspend.\u003c/li\u003e\n    \u003cli\u003eSomething else calls \u003ccode\u003etask.cancel()\u003c/code\u003e, which reschedules the task again.\u003c/li\u003e\n    \u003cli\u003eThe function that was being \u003ccode\u003eawait\u003c/code\u003e-ed now raises \u003ccode\u003eCancelled\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003e\n        This exception propagates backwards, unwinding through all functions in the call stack and\n        cleaning up as it goes.\n    \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\n    This avoids both problems with threads: tasks can be externally killed without worrying about\n    resources not being torn down, and end-user tasks don\u0026#39;t need to constantly check if they\u0026#39;ve been\n    cancelled because the event loop does it for you.\n\u003c/p\u003e\n\n\u003ch3 id=\"but-that-s-not-how-it-works\"\u003eBut that\u0026#39;s not how it works\u003c/h3\u003e\n\u003cp\u003e\n    Consider this function below that returns a resource wrapped in an asynchronous context manager.\n    When the user is done, it needs to clean up some resources (say, a server needs a clean close).\n    This cleanup should be done regardless of if the code running inside the context manager was\n    successful or not, so it\u0026#39;s ran inside a \u003ccode\u003efinally\u003c/code\u003e block:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"01/not_irc.py\"\u003e\u003ccode\u003e@asynccontextmanager\nasync def connect_to_server(ip: str, *, port: int = 6767) -\u0026gt; AsyncIterator[Sock]:\n    sock = await connect_socket(ip, port)\n\n    async with sock:\n        await sock.send(b\u0026#34;IDENTIFY ident :bar\\r\\nNICKNAME :gquuuuuux)\\r\\n\u0026#34;)\n        try:\n            yield sock\n        finally:\n            await sock.send(b\u0026#34;QUIT :died to some small fry\u0026#34;)\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    In this case, let\u0026#39;s say \u003ccode\u003e.send()\u003c/code\u003e waits for some form of acknowledgement message.\n    There\u0026#39;s also another task that is spawned somewhere, and it\u0026#39;s sending a\n    \u003ccode\u003ePING\u003c/code\u003e message to the server every few seconds and expecting a\n    \u003ccode\u003ePONG\u003c/code\u003e message. If the client goes too long without receiving a \u003ccode\u003ePONG\u003c/code\u003e, it\n    cancels the task inside the context manager and exits itself.\n\u003c/p\u003e\n\u003cp\u003eWhat happens if the server does stop responding, and the task is cancelled? Let\u0026#39;s see:\u003c/p\u003e\n\u003col\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            First, any code in the user function running inside the asynchronous context manager is\n            cancelled with a \u003ccode\u003eCancelledException\u003c/code\u003e bubbling upwards.\n        \u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            Next, the \u003ccode\u003eyield sock\u003c/code\u003e expression raises a \u003ccode\u003eCancelledException\u003c/code\u003e,\n            and control flows into the \u003ccode\u003efinally\u003c/code\u003e block.\n        \u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            The code enters the \u003ccode\u003esock.send()\u003c/code\u003e function, which re-enters the event loop.\n            The event loop completely forgets that the task was cancelled and is entirely happy to\n            deadlock the application forever waiting for the server to respond to the\n            \u003ccode\u003e.send()\u003c/code\u003e (which will never happen).\n        \u003c/p\u003e\n    \u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\n    This is because cancellations in \u003ccode\u003easyncio\u003c/code\u003e are \u003cem\u003eedge-triggered\u003c/em\u003e, not\n    \u003cem\u003elevel-triggered\u003c/em\u003e. These concepts are mostly used in the world of electronics, but are\n    also applicable to certain types of programming too; an \u003cem\u003eedge-triggered\u003c/em\u003e event only fires\n    \u003cstrong\u003eonce\u003c/strong\u003e when the state changes. In this case, it\u0026#39;s\n    \u003ccode\u003eTask.cancel()\u003c/code\u003e firing a cancellation error exactly once. This is the opposite\n    behaviour to \u003cem\u003elevel-triggered\u003c/em\u003e cancellations, where cancelling a task will cause\n    \u003cstrong\u003e\u003cem\u003eall\u003c/em\u003e\u003c/strong\u003e calls to the event loop to raise a\n    \u003ccode\u003eCancelledException\u003c/code\u003e, forever.\n\u003c/p\u003e\n\u003cp\u003e\n    Here\u0026#39;s a more practical example that you can run directly on your computer to see this\n    behaviour.\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"01/slept_5s.py\"\u003e\u003ccode\u003eimport asyncio\n\nevent = asyncio.Event()\n\nasync def fn():\n    try:\n        event.set()\n        await asyncio.sleep(60)\n    finally:\n        await asyncio.sleep(5)\n        print(\u0026#34;slept for 5s\u0026#34;)\n\nasync def main():\n    task = asyncio.create_task(fn())\n    await event.wait()\n    task.cancel()\n    await asyncio.sleep(10)\n\nasyncio.run(main())\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    When you run this, the first \u003ccode\u003esleep(60)\u003c/code\u003e will be cancelled, and then the program will\n    sleep for five more seconds before printing a \u003ccode\u003eslept for 5s\u003c/code\u003e message because the\n    cancellation disappeared.\n\u003c/p\u003e\n\u003cp\u003e\n    This is absolutely 100% the wrong behaviour and it makes cancellations dangerous when it can be\n    swallowed or covered up at any point.\n\u003c/p\u003e\n\u003cul\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            Using a bare \u003ccode\u003eexcept:\u003c/code\u003e? Swallows cancellations. People will lie and say that\n            they don\u0026#39;t write these, but people do use bare \u003ccode\u003eexcepts\u003c/code\u003e. Even if\n            \u003cem\u003eyou\u003c/em\u003e don\u0026#39;t, do you know that every other library doesn\u0026#39;t?\n        \u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            Doing cleanup in \u003ccode\u003e__aexit__\u003c/code\u003e? Can deadlock waiting for something that will\n            never happen, swallowing the cancellation.\n        \u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003eDoing cleanup in \u003ccode\u003etry/finally\u003c/code\u003e? See above.\u003c/p\u003e\n    \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"it-could-be-better\"\u003eIt could be better\u003c/h3\u003e\n\u003cp\u003e\n    Graceful asynchronous cleanup is intrinsically a difficult problem; if an operation blocks for\n    too long, what do you do? If you adopt a rigid rule of \u003cem\u003ealways\u003c/em\u003e trying to be graceful you\n    risk running into deadlocks if the operation never returns. If you simply avoid doing anything\n    gracefully and just sever connections and open files with a machete you can end up with\n    half-written data or some very unhappy servers on the other end. It doesn\u0026#39;t really matter in the\n    \u003ccode\u003easyncio\u003c/code\u003e world, because the library doesn\u0026#39;t give you any tools to implement this.\n\u003c/p\u003e\n\u003cp\u003e\n    The \u003ca href=\"https://github.com/python-trio/trio\"\u003eTrio\u003c/a\u003e library takes the opposite approach;\n    all cancellations are \u003cem\u003elevel-triggered\u003c/em\u003e. Let\u0026#39;s port the sleeping example above to use\n    Trio instead:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"01/trio_example.py\"\u003e\u003ccode\u003eimport trio\n\nevent = trio.Event()\n\nasync def task():\n    try:\n        event.set()\n        await trio.sleep(60)\n    finally:\n        await trio.sleep(5)\n        print(\u0026#34;slept for 5s\u0026#34;)\n\nasync def main():\n    async with trio.open_nursery() as n:\n        n.start_soon(task)\n        await event.wait()\n        n.cancel_scope.cancel()\n        await trio.sleep(10)  # Not needed, but for parity with the previous example.\n\ntrio.run(main)\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    Running this will produce... no output. It won\u0026#39;t wait either, because anything that could wait\n    has been cancelled. If you add a \u003ccode\u003eprint()\u003c/code\u003e between the \u003ccode\u003eevent.wait\u003c/code\u003e and\n    the \u003ccode\u003ecancel_scope.cancel()\u003c/code\u003e, that will print something too, so it\u0026#39;s not exiting early\n    because it\u0026#39;s not running anything.\n\u003c/p\u003e\n\u003cp\u003e\n    This then asks a question: How do you do graceful cleanup? With shielded cancel scopes and\n    timeouts. I\u0026#39;ll replace the finally block above with one of those:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"01/trio_cleanup.py\"\u003e\u003ccode\u003e    finally:\n        with trio.move_on_after(1, shield=True):\n            await trio.sleep(5)\n        print(\u0026#34;slept for 1s?\u0026#34;)\n        await trio.sleep(5)\n        print(\u0026#34;slept for 5s?\u0026#34;)\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    Running this will print \u003ccode\u003eslept for 1s?\u003c/code\u003e, but nothing more. The code running inside\n    the context manager ignored the outside cancellation, but was re-cancelled after a second\n    anyway. This once again nets you the best of both worlds: cancellations aren\u0026#39;t swallowed unless\n    you explicitly opt-in. Remember the Zen of Python: Explicit is better than implicit.\n\u003c/p\u003e\n\n\u003ch2 id=\"major-2\"\u003eMajor Problem #2: Task was destroyed but it is pending!\u003c/h2\u003e\n\n\u003cp\u003e\n    If you\u0026#39;ve ever used an asyncio application, you\u0026#39;ve probably seen that message pop up before. As\n    an example, if I Ctrl-C\n    \u003ca href=\"https://wiki.gentoo.org/wiki/Portage\"\u003eportage\u003c/a\u003e too quickly, it spits out a few of\n    those errors. Why? Because \u003ccode\u003easyncio\u003c/code\u003e does \u003cem\u003enot\u003c/em\u003e keep strong references to\n    tasks. Quoting the official documentation:\n\u003c/p\u003e\n\u003cblockquote\u003e\n    \u003cp\u003eImportant\u003c/p\u003e\n    \u003cp\u003e\n        Save a reference to the result of this function, to avoid a task disappearing mid-execution.\n        The event loop only keeps weak references to tasks. A task that isn’t referenced elsewhere\n        may get garbage collected at any time, even before it’s done. For reliable “fire-and-forget”\n        background tasks, gather them in a collection:\n    \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eLet\u0026#39;s take some example code:\u003c/p\u003e\n\n\u003cpre data-snippet=\"02/has_bug_1.py\"\u003e\u003ccode\u003eimport asyncio, gc\n\n\nasync def expose_bugs():\n    while True:\n        await asyncio.sleep(0.5)\n        # Simulate doing work that would have the GC fire.\n        gc.collect()\n\n\nasync def has_bug():\n    loop = asyncio.get_running_loop()\n    fut = loop.create_future()\n    await fut\n\n\nasync def main():\n    t = asyncio.create_task(expose_bugs())\n    asyncio.create_task(has_bug())\n    await asyncio.sleep(5)\n\n\nasyncio.run(main())\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    If you run this, it will print a warning to stderr about how \u003ccode\u003ehas_bug\u003c/code\u003e was destroyed\n    when it was pending. \u003ccode\u003ehas_bug\u003c/code\u003e has no strong references to it, so when the GC runs\n    the weak reference the event loop holds is removed and the task is dropped on the floor.\n    Goodbye, \u003ccode\u003ehas_bug\u003c/code\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n    This is very obviously insane behaviour, but it can somewhat be avoided by\n    \u003cem\u003ealways\u003c/em\u003e holding references to spawned tasks (similarly to how you can avoid segmentation\n    faults by always doing bounds checking). But it gets worse. There\u0026#39;s a set of helper functions\n    that are used for corralling tasks around: \u003ccode\u003ewait_for\u003c/code\u003e, \u003ccode\u003egather\u003c/code\u003e, or\n    \u003ccode\u003eshield\u003c/code\u003e; these can all cause a function being waited on to be dropped on the floor\n    because they internally spawn said function as a task and wait on \u003cem\u003ethat\u003c/em\u003e instead:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"02/has_bug_worse.py\"\u003e\u003ccode\u003eimport asyncio, gc\n\n\nasync def expose_bugs():\n    while True:\n        await asyncio.sleep(0.5)\n        # Simulate doing work that would have the GC fire.\n        gc.collect()\n\n\nasync def has_bug():\n    loop = asyncio.get_running_loop()\n    fut = loop.create_future()\n    await fut\n\n\nasync def shield_task():\n    await asyncio.shield(has_bug())\n\n\nasync def main():\n    t1 = asyncio.create_task(expose_bugs())\n    t2 = asyncio.create_task(shield_task())\n    # scheduling pass\n    await asyncio.sleep(1)\n    t2.cancel()\n    await asyncio.sleep(2)\n\n\nasyncio.run(main())\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    When \u003ccode\u003et2\u003c/code\u003e is cancelled, the outer \u003ccode\u003eawait asyncio.shield(...)\u003c/code\u003e call is\n    cancelled. The cancellation doesn\u0026#39;t propagate through into \u003ccode\u003ehas_bug\u003c/code\u003e because of the\n    shielding, and the outer task still has a strong reference in the form of \u003ccode\u003et2\u003c/code\u003e. But\n    \u003ccode\u003ehas_bug\u003c/code\u003e\u0026#39;s task has no strong references to it; the only reference was in the local\n    variables of the \u003ccode\u003eshield()\u003c/code\u003e functions. The next time the event loop ticks,\n    \u003ccode\u003egc.collect()\u003c/code\u003e is called, which drops the \u003ccode\u003ehas_bug\u003c/code\u003e task entirely.\n\u003c/p\u003e\n\u003cp\u003e\n    You might try to avoid this by doing \u003ccode\u003ecreate_task\u003c/code\u003e explicitly as this will keep a\n    strong reference to the \u003ccode\u003ehas_bug()\u003c/code\u003e task in the local variables of the cancelled\n    generator coroutine for \u003ccode\u003eshield_task\u003c/code\u003e, like so:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"02/shield_fix?.py\"\u003e\u003ccode\u003easync def shield_task():\n    inner = asyncio.create_task(has_bug())\n    await asyncio.shield(inner)\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    But this only works all the while the handle to \u003ccode\u003et2\u003c/code\u003e lives inside\n    \u003ccode\u003emain()\u003c/code\u003e. If that handle gets dropped, then the inner \u003ccode\u003ehas_bug\u003c/code\u003e will\n    \u003cem\u003ealso\u003c/em\u003e get dropped! Adding a \u003ccode\u003edel t2\u003c/code\u003e after the \u003ccode\u003et2.cancel()\u003c/code\u003e will\n    expose this immediately. Good luck tracking this through a web of classes and tasks.\n\u003c/p\u003e\n\n\u003ch2 id=\"major-3\"\u003eMajor Problem #3: I/O has pointless landmines\u003c/h2\u003e\n\u003cp\u003e\n    The underlying API for performing network I/O is the ever-venerable BSD socket API. Python\n    exposes a nice object-based API for working with sockets; let\u0026#39;s look at some code that opens a\n    connection on a socket and sends some data.\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"03/sockets_sync.py\"\u003e\u003ccode\u003es = socket.socket(socket.AF_INET6, socket.SOCK_STREAM, socket.IPPROTO_TCP)\ns.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\ns.connect((\u0026#34;2001:708:40:2001::11ba\u0026#34;, 6667))\ns.send(b\u0026#34;USER abc 0 0 :def\\r\\nNICK :gquuuuuux\\r\\n\u0026#34;)\nmotd = s.recv(4096)\ns.shutdown(socket.SHUT_RDWR)  # Graceful close, send an EOF\ns.close()\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    This is pretty bare-bones, but it\u0026#39;s easy to see how the code flows: top to bottom. It\u0026#39;s a set of\n    linear statements:\n\u003c/p\u003e\n\u003col\u003e\n    \u003cli\u003eCreate the socket with the appropriate options.\u003c/li\u003e\n    \u003cli\u003eConnect it to an address directly.\u003c/li\u003e\n    \u003cli\u003eSend some data from a socket.\u003c/li\u003e\n    \u003cli\u003eReceive some data from the socket.\u003c/li\u003e\n    \u003cli\u003eShut it down and close the socket.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\n    This all happens in order; it\u0026#39;s simple to follow. Trio offers an asynchronous version of this,\n    so let\u0026#39;s rewrite the code to be identical with Trio sockets:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"03/sockets_async.py\"\u003e\u003ccode\u003es = trio.socket.socket(socket.AF_INET6, socket.SOCK_STREAM, socket.IPPROTO_TCP)\nawait s.connect((\u0026#34;2001:708:40:2001::11ba\u0026#34;, 6667))\nawait s.send(b\u0026#34;USER abc 0 0 :def\\r\\nNICK :gquuuuuux\\r\\n\u0026#34;)\nmotd = s.recv(4096)\ns.shutdown(socket.SHUT_RDWR)  # Graceful close, schedules an EOF\ns.close()\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    The code is almost identical, with some \u003ccode\u003eawait\u003c/code\u003e statements introduced before every\n    function that would normally block. Again, the control flow is simple; it flows from top to\n    bottom in a linear fashion. Let\u0026#39;s look at \u003ccode\u003easyncio\u003c/code\u003e\u0026#39;s version of sockets, which are\n    called protocols:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"03/protocols_wtf.py\"\u003e\u003ccode\u003eimport asyncio\n\n\nclass IdentifyProtocol(asyncio.Protocol):\n    def __init__(self, message: bytes, motd_future: asyncio.Future):\n        self.message = message \n        self.motd = motd_future\n\n    def connection_made(self, transport: asyncio.WriteTransport):\n        transport.write(self.message.encode())\n\n    def data_received(self, data: bytes):\n        self.motd.set_result(data)\n\n    def connection_lost(self, exc: BaseException):\n        ...\n\n\nfut = loop.create_future()\ntransport, protocol = await loop.create_connection(\n    partial(EchoClientProtocol, b\u0026#34;USER abc 0 0 :def\\r\\nNICK :gquuuuuux\\r\\n\u0026#34;, fut),\n    \u0026#34;2001:708:40:2001::11ba\u0026#34;, \n    6667,\n)\n\nmotd = await protocol.motd\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    Unlike regular BSD sockets or Trio\u0026#39;s socket wrappers, \u003ccode\u003easyncio\u003c/code\u003e uses\n    \u003cem\u003ecallbacks\u003c/em\u003e - synchronous ones, at that - to implement the low-level I/O primitives. The\n    control flow here jumps around a lot:\n\u003c/p\u003e\n\u003col\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            \u003ccode\u003ecreate_connection\u003c/code\u003e is equivalent to \u003ccode\u003esocket.socket\u003c/code\u003e +\n            \u003ccode\u003esocket.connect\u003c/code\u003e. Okay. You don\u0026#39;t get to set socket options (at least it sets\n            \u003ccode\u003eTCP_NODELAY\u003c/code\u003e) and it doesn\u0026#39;t work for anything other than regular\n            AF_INET/AF_INET6 sockets.\n        \u003c/p\u003e\n        \u003cp\u003e\n            It returns a tuple of \u003ccode\u003e(write transport, protocol instance)\u003c/code\u003e; the former can\n            be used to send further data (synchronously).\n        \u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            When the socket is opened, it jumps into my class and calls (synchronously)\n            \u003ccode\u003econnection_made\u003c/code\u003e, providing a \u0026#34;transport\u0026#34; which I can call the\n            (synchronous) \u003ccode\u003ewrite\u003c/code\u003e method on to send my authentication method.\n        \u003c/p\u003e\n        \u003cp\u003e\n            There\u0026#39;s no way to wait for this to be sent as \u003ccode\u003eWriteTransport.write\u003c/code\u003e is\n            synchronous. It\u0026#39;ll get sent at some point in the future. Maybe. If you want to let\n            somebody know that you\u0026#39;ve sent the message, you\u0026#39;ll need to implement that yourself too.\n        \u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            After some time, the server will respond; the event loop calls (synchronously)\n            \u003ccode\u003edata_received\u003c/code\u003e, providing the received data. If you want to do something\n            with this data (asynchronously), you need to pass it to the outside world yourself using\n            futures or queues. In this case, I\u0026#39;ve implemented it with a regular \u003ccode\u003eFuture\u003c/code\u003e;\n            I haven\u0026#39;t even thought about how to swap the future out in a non-error prone way for\n            future reads yet.\n        \u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            The outside world now reads the data from the future. That\u0026#39;s three separate places I\u0026#39;ve\n            had to deal with the data, versus a single place with a linear order for sockets.\n        \u003c/p\u003e\n    \u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\n    The biggest difference between raw sockets and protocols is that protocols have their incoming\n    data \u003cem\u003epushed in\u003c/em\u003e to you. If you want to simply \u003cem\u003ewait\u003c/em\u003e for data to arrive, you need\n    to implement that yourself! This is only a basic protocol; more complex protocols require more\n    implementing more complicated synchronisation mechanisms manually to communicate between the\n    entirely synchronous protocol callbacks leading to a mess of either\n    \u003ccode\u003ecreate_task\u003c/code\u003e everywhere or manually shuffling futures/events around.\n\u003c/p\u003e\n\u003cp\u003e\n    Why is it like this? Because Twisted was like this. But Twisted existed in a world before\n    \u003ccode\u003eyield from\u003c/code\u003e or \u003ccode\u003eawait\u003c/code\u003e, so it has an excuse. \u003ccode\u003easyncio\u003c/code\u003e copied\n    it in a world \u003cem\u003ewith\u003c/em\u003e \u003ccode\u003eyield from\u003c/code\u003e and \u003ccode\u003eawait\u003c/code\u003e, so it has no excuse.\n\u003c/p\u003e\n\n\u003cp\u003e\n        And no, the answer is also not \u0026#34;because Windows doesn\u0026#39;t support a Unix-style\n        \u003ccode\u003eselect()\u003c/code\u003e API properly\u0026#34;. If you want \u003ccode\u003eselect()\u003c/code\u003e semantics on\n        Windows, use \u003ccode\u003e\\Device\\Afd\u003c/code\u003e like\n        \u003ca href=\"https://notgull.net/device-afd/\"\u003eeveryone else\u003c/a\u003e does (and by everyone else, I\n        mean the entire Javascript and Rust ecosystem).\n    \u003c/p\u003e\n\n\u003ch3 id=\"that-s-not-fair\"\u003eThat\u0026#39;s not fair\u003c/h3\u003e\n\u003cp\u003e\n    That\u0026#39;s true. It\u0026#39;s rare that you\u0026#39;ll actually interact with protocols; they are a weird\n    implementation detail of \u003ccode\u003easyncio\u003c/code\u003e\u0026#39;s event loop mechanisms. The same goes for Trio\n    sockets, but at least for sockets you can use them for esoteric mechanisms like\n    \u003ccode\u003eAF_NETLINK\u003c/code\u003e or \u003ccode\u003eSOCK_RAW\u003c/code\u003e whilst still retaining the nice asynchronous\n    API. (You can implement those socket types on \u003ccode\u003easyncio\u003c/code\u003e with the even lower level\n    APIs of \u003ccode\u003eadd_{reader|writer}\u003c/code\u003e, but that\u0026#39;s not a topic for today).\n\u003c/p\u003e\n\u003cp\u003e\n    Instead most \u003ccode\u003easyncio\u003c/code\u003e and Trio programs will use \u003cem\u003estreams\u003c/em\u003e, a high-level\n    generic API that treats network connections as nothing more than a stream of bytes. Here\u0026#39;s how\n    the previous socket example would be written using Trio\u0026#39;s streams:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"03/trio_libera.py\"\u003e\u003ccode\u003easync with trio.open_tcp_stream(\u0026#34;irc.libera.chat\u0026#34;, port=6667) as stream:\n    # type: trio.SocketStream\n    await stream.send_all(b\u0026#34;USER abc 0 0 :def\\r\\nNICK :gquuuuuux\\r\\n\u0026#34;)\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    This is very simple; the returned stream works as an asynchronous context manager that\n    automatically closes the socket when done, regardless of if the inner code succeeds or fails.\n    The \u003ccode\u003esend_all\u003c/code\u003e method will automatically retry when the underlying socket returns a\n    partial write, so the user doesn\u0026#39;t need to implement retry logic for partial writes by hand.\n\u003c/p\u003e\n\n\u003cp\u003eHere\u0026#39;s how you do it in \u003ccode\u003easyncio\u003c/code\u003e:\u003c/p\u003e\n\n\u003cpre data-snippet=\"03/asyncio_libera.py\"\u003e\u003ccode\u003ereader, writer = await asyncio.open_connection(\u0026#34;irc.libera.chat\u0026#34;, port=6667)\ntry:\n    writer.write(b\u0026#34;USER abc 0 0 :def\\r\\nNICK :gquuuuuux\\r\\n\u0026#34;)\n    await writer.drain()\nfinally:\n    writer.close()\n    await writer.wait_closed()\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThis is similar to the Trio example with two major differences:\u003c/p\u003e\n\u003cul\u003e\n    \u003cli\u003e\n        \u003ccode\u003ewriter.write\u003c/code\u003e is \u003cem\u003esynchronous\u003c/em\u003e and does not actually perform a full write\n        unless \u003ccode\u003edrain()\u003c/code\u003e is called.\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            \u003ccode\u003ewriter.close\u003c/code\u003e does not actually perform a close, only schedules it, and you\n            need to use \u003ccode\u003ewait_closed\u003c/code\u003e to ensure the stream is closed.\n        \u003c/p\u003e\n        \u003cp\u003e\n            Also, \u003ccode\u003ewait_closed\u003c/code\u003e will block if the \u003ccode\u003edrain\u003c/code\u003e method is cancelled.\n            The cancellation issues are everywhere.\n        \u003c/p\u003e\n    \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n    The \u003ccode\u003ewrite\u003c/code\u003e/\u003ccode\u003edrain\u003c/code\u003e pair exists entirely as a footgun for anyone who\n    forgets to call \u003ccode\u003edrain\u003c/code\u003e. Data may get written in the background if you don\u0026#39;t call\n    \u003ccode\u003edrain()\u003c/code\u003e, but if you\u0026#39;re in a tight loop with lots of data to write and no other\n    \u003ccode\u003eawait\u003c/code\u003e calls, it will buffer \u003cem\u003eall\u003c/em\u003e of that data into the stream\u0026#39;s internal\n    buffer without sending it. Even if you do have the writer task rescheduled, the buffer may still\n    fill up anyway if data is being written faster than the background writer can empty it. This is\n    stupid!\n\u003c/p\u003e\n\u003cp\u003e\n    It\u0026#39;s a not too dissimilar situation with \u003ccode\u003eclose\u003c/code\u003e/\u003ccode\u003ewait_closed\u003c/code\u003e;\n    \u003ccode\u003eclose()\u003c/code\u003e schedules a close and \u003ccode\u003ewait_closed\u003c/code\u003e waits for that close to\n    actually be sent. What happens if \u003ccode\u003ewait_closed\u003c/code\u003e is cancelled?\n    \u003ccode\u003easyncio\u003c/code\u003e doesn\u0026#39;t really define the semantics for this, unlike the\n    \u003ccode\u003eTrio\u003c/code\u003e world which very explicitly does. In the Trio world, all closeable objects\n    follow the \u003ccode\u003eAsyncResource\u003c/code\u003e ABC, which defines an \u003cem\u003eidempotent\u003c/em\u003e\n    \u003ccode\u003eaclose\u003c/code\u003e method that \u003cem\u003emust always succeed\u003c/em\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n    So what happens for protocols such as TLS that need a graceful goodbye message sent? Trio\u0026#39;s SSL\n    helpers will try and send a graceful close, and if that times out the stream will be severed by\n    force instead. The end-user doesn\u0026#39;t need to know anything about this; they can call\n    \u003ccode\u003eaclose\u003c/code\u003e on a resource to close it and not worry about if it will be cancelled or if\n    the resource is actually closed.\n\u003c/p\u003e\n\n\u003ch2 id=\"major-4\"\u003eMajor Problem #4: \u003ccode\u003easyncio.Queue\u003c/code\u003e is difficult to use\u003c/h2\u003e\n\u003cp\u003e\n    I have two tasks: a producer (that makes messages) and a consumer (that eats messages). Here\n    they are:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"04/csp_take_1.py\"\u003e\u003ccode\u003easync def producer():\n    while True:\n        message = await do_some_networking_thing()\n        # i don\u0026#39;t know how to send a message...\n\nasync def consumer():\n    while True:\n        message = # i don\u0026#39;t know how to receive a message...\n        await eat(message)\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    How do I get messages between them? I could use a \u003ccode\u003eFuture\u003c/code\u003e, but that would only work\n    exactly once and both of these functions are running in a loop. I could find a way to ferry\n    \u003ccode\u003eFuture\u003c/code\u003e instances between them, but if I could do that I would use the ferry to\n    communicate the messages instead.\n\u003c/p\u003e\n\u003cp\u003e\n    The solution is an \u003ccode\u003easyncio.Queue\u003c/code\u003e, which is the asynchronous version of\n    \u003ccode\u003equeue.Queue\u003c/code\u003e (which is the Python version of\n    \u003ccode\u003ejava.util.concurrent.ArrayBlockingQueue\u003c/code\u003e). Let\u0026#39;s pass a queue to both functions:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"04/csp_take_2.py\"\u003e\u003ccode\u003easync def producer(queue: asyncio.Queue):\n    while True:\n        message = await do_some_networking_thing()\n        await queue.put(message)\n\nasync def consumer(queue: asyncio.Queue):\n    while True:\n        message = await queue.get()\n        await eat(message)\n\nasync def main():\n    queue = asyncio.Queue()\n    t1 = asyncio.create_task(producer(queue))\n    t2 = asyncio.create_task(consumer(queue))\n\n    while True:\n        await asyncio.sleep(99999999)\n\nasyncio.run(main())\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    This will have the producer loop forever creating items and putting them in the queue, and the\n    consumer will loop forever reading items from thee queue and doing something with them. This is\n    a very common pattern which is similar to \u003cem\u003ecommunicating sequential processes\u003c/em\u003e. But what\n    happens if \u003ccode\u003econsumer\u003c/code\u003e throws an exception in \u003ccode\u003eeat\u003c/code\u003e? Let\u0026#39;s go over the\n    control flow:\n\u003c/p\u003e\n\u003col\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e produces an item and sends it to the queue.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003econsumer\u003c/code\u003e receives an item and calls \u003ccode\u003eeat\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003e\n        \u003ccode\u003eeat\u003c/code\u003e raises an exception and the consumer task dies. For the sake of\n        understanding, this exception is a transient external exception and is not related to either\n        the code or the item being consumed.\n    \u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e produces an item and sends it to the queue.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e produces an item and sends it to the queue.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e produces an item and sends it to the queue.\u003c/li\u003e\n    \u003cli\u003eYour system locks up as the out-of-memory killer fails to run.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\n    This is because the consumer exerts no \u003cem\u003ebackpressure\u003c/em\u003e on the producer; the producer will\n    gladly keep sending items into the queue forever that nobody is listening to. I can add some\n    backpressure by using \u003ccode\u003easyncio.Queue(maxsize=1)\u003c/code\u003e, which changes the control flow like\n    so:\n\u003c/p\u003e\n\u003col\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e produces an item and sends it to the queue.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003econsumer\u003c/code\u003e receives an item and calls \u003ccode\u003eeat\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003eeat\u003c/code\u003e raises an exception and the consumer task dies.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e produces an item and sends it to the queue.\u003c/li\u003e\n    \u003cli\u003e\n        \u003ccode\u003eproducer\u003c/code\u003e produces an item, tries sending it to the queue, but blocks forever\n        because there\u0026#39;s nobody reading from the queue.\n    \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\n    That\u0026#39;s a little bit better in the sense it won\u0026#39;t leak memory forever, but instead it\n    will lock up forever because the producer has no way of knowing that the consumer isn\u0026#39;t\n    listening anymore. In Python 3.13 the \u003ccode\u003eQueue.shutdown\u003c/code\u003e method was added which lets\n    one (or both) sides know that the queue is closed and can\u0026#39;t accept (or receive) any new\n    items. Let\u0026#39;s adjust the code to use that:\n\u003c/p\u003e\n\n\u003cp\u003e\n        If you\u0026#39;re stuck on Python 3.12 or earlier, there\u0026#39;s no \u003ccode\u003eQueue.shutdown\u003c/code\u003e\n        available.\n    \u003c/p\u003e\n\n\u003cpre data-snippet=\"04/csp_take_3.py\"\u003e\u003ccode\u003easync def consumer(queue: asyncio.Queue):\n    while True:\n        message = await queue.get()\n\n        try:\n            await eat(message)\n        except:\n            queue.shutdown()\n            raise\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eNow the control flow goes as follows:\u003c/p\u003e\n\u003col\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e produces an item and sends it to the queue.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003econsumer\u003c/code\u003e receives an item and calls \u003ccode\u003eeat\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003eeat\u003c/code\u003e raises an exception and the consumer task dies.\u003c/li\u003e\n    \u003cli\u003e\n        \u003ccode\u003eproducer\u003c/code\u003e produces an item and tries sending it to the queue, but fails because\n        the queue is shut down.\n    \u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\n    Except... that\u0026#39;s not true. There\u0026#39;s a race condition going on between steps three and\n    four; if \u003ccode\u003eproducer\u003c/code\u003e puts an item into the queue before the consumer task is killed,\n    then the item that was sent to the queue remains there forever. There\u0026#39;s a pair of methods,\n    \u003ccode\u003ejoin()\u003c/code\u003e and \u003ccode\u003etask_done\u003c/code\u003e that can solve this, meaning my code now looks\n    like this:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"04/csp_take_4.py\"\u003e\u003ccode\u003easync def producer(queue: asyncio.Queue):\n    while True:\n        message = await do_some_networking_thing()\n        await queue.put(message)\n        await queue.join()\n\nasync def consumer(queue: asyncio.Queue):\n    while True:\n        try:\n            message = await queue.get()\n            await eat(queue)\n        except:\n            queue.shutdown(immediate=True)\n            raise\n        else:\n            queue.task_done()\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eAnd the control flow goes as follows:\u003c/p\u003e\n\u003col\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e produces an item and sends it to the queue.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e begins blocking until the consumer calls \u003ccode\u003etask_done\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003econsumer\u003c/code\u003e receives an item and calls \u003ccode\u003eeat\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003e\n        \u003ccode\u003eeat\u003c/code\u003e raises an exception and the consumer task dies. The queue is shut down.\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003ccode\u003equeue.join\u003c/code\u003e wakes up because I passed \u003ccode\u003eimmediate=True\u003c/code\u003e. If I\n        didn\u0026#39;t pass that, it would block forever instead.\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003ccode\u003eproducer\u003c/code\u003e produces an item and tries sending it to the queue, but\n        \u003ccode\u003eput\u003c/code\u003e fails because the queue is shut down.\n    \u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\n    This eliminates the race condition entirely. This isn\u0026#39;t a very useful pattern because with\n    one consumer and one producer it can be generalised into just calling the consumer function from\n    the producer. It would be more useful if I add a second consumer, assuming consumers are slower\n    than the producer:\n\u003c/p\u003e\n\u003col\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e produces an item and sends it to the queue.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e begins blocking until the consumer calls \u003ccode\u003etask_done\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003eConsumer task 1 receives an item and calls \u003ccode\u003eeat\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003e\n        Consumer task 2 sits there idly because the producer can\u0026#39;t do anything until the first\n        consumer task has finished.\n    \u003c/li\u003e\n    \u003cli\u003eConsumer task 1 has an exception, and shuts down the queue.\u003c/li\u003e\n    \u003cli\u003eConsumer task 2 has an exception because the queue was closed.\u003c/li\u003e\n    \u003cli\u003eEverything explodes in a fiery mess of exceptions.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\n    To fix this, consumer task 1 won\u0026#39;t shut down the queue but will restart itself, perhaps from\n    an external supervisor.\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"04/csp_take_6.py\"\u003e\u003ccode\u003easync def consumer(queue: asyncio.Queue):\n    while True:\n        try:\n            message = await queue.get()\n            await eat(queue)\n        except Exception:\n            logger.exception()\n            return\n        else:\n            queue.task_done()\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eLet\u0026#39;s look at the control flow for a final time:\u003c/p\u003e\n\u003col\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e produces an item and sends it to the queue.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e begins blocking until the consumer calls \u003ccode\u003etask_done\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003eConsumer task 1 receives an item and calls \u003ccode\u003eeat\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003e\n        Consumer task 2 sits there idly because the producer can\u0026#39;t do anything until the first\n        consumer task has finished.\n    \u003c/li\u003e\n    \u003cli\u003eConsumer task 1 has an exception, and returns.\u003c/li\u003e\n    \u003cli\u003eConsumer task 2 remains blocking on \u003ccode\u003eget\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003e\u003ccode\u003eproducer\u003c/code\u003e continues blocking on \u003ccode\u003ejoin\u003c/code\u003e.\u003c/li\u003e\n    \u003cli\u003eThe freshly rebooted consumer task 1 starts blocking on \u003ccode\u003eget\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\n    This could be fixed by making the first consumer task try and re-insert an item on an exception,\n    but what happens if the second task has had an error? Deadlocks. At this point, I give up and\n    pull in an AMQP server instead of dealing with in-library queues.\n\u003c/p\u003e\n\n\u003ch3 id=\"it-doesn-t-have-to-be-this-way\"\u003eIt doesn\u0026#39;t have to be this way\u003c/h3\u003e\n\u003cp\u003eWhat I\u0026#39;m really looking for is a combination of the following:\u003c/p\u003e\n\u003cul\u003e\n    \u003cli\u003e\n        A queue that blocks until a receiver has retrieved the item (aka, automatic\n        \u003ccode\u003e.join()\u003c/code\u003e).\n    \u003c/li\u003e\n    \u003cli\u003e\n        A queue that can be \u003cem\u003ecloned\u003c/em\u003e and \u003cem\u003eindependently closed\u003c/em\u003e without affecting\n        other consumers.\n    \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n    Trio\u0026#39;s channels implement these behaviours. Let\u0026#39;s re-write the consumer/producer pattern\n    to use channels:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"04/csp_take_trio.py\"\u003e\u003ccode\u003easync def producer(channel: trio.MemorySendChannel[Message]):\n    async with channel:\n        while True:\n            message: Message = await do_some_networking_thing()\n            await channel.send(message)\n\n\nasync def consumer(channel: trio.MemoryReceiveChannel[Message]):\n    async with channel:\n        while True:\n            result = await channel.receive()\n\n            try:\n                await do_something(result)\n            except Exception:\n                logger.exception()\n                return\n            \n\nasync def main():\n    send, receive = trio.open_memory_channel[Message](max_buffer_size=0)\n\n    async with trio.open_nursery() as n:\n        for _ in range(5):\n            consumer_channel = receive.clone()\n            n.start_soon(partial(consumer, consumer_channel))\n\n        n.start_soon(partial(producer, send))\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    Trio channels with a buffer size of zero act as \u003cem\u003etransfer queues\u003c/em\u003e, a name coined by\n    \u003ca href=\"https://docs.oracle.com/en/java/javase/24/docs/api/java.base/java/util/concurrent/TransferQueue.html\"\u003eJava 7\u003c/a\u003e\n    (released in 2011 (!!)), where the sender always waits for a receiver to take a message from the\n    channel. Each receiver gets its own unique \u003cem\u003eclone\u003c/em\u003e of the channel that can be\n    independently cloned and messages are sent from the sender channel in a round-robin fashion.\n    These clones can be independently closed without affecting the other cloned channels; only once\n    the final receive channel is closed will the sending channel begin raising errors.\n    \u003ccode\u003eTransferQueue\u003c/code\u003e was created four solid years before \u003ccode\u003easyncio\u003c/code\u003e existed. I\n    really see no excuse for this behaviour to have existed when \u003ccode\u003easyncio\u003c/code\u003e was being\n    developed.\n\u003c/p\u003e\n\u003cp\u003e\n    The only problem this doesn\u0026#39;t solve is that if the consumer has an error after receiving an\n    object, that object stays unprocessed. This is a problem with both implementations and channels\n    don\u0026#39;t (yet) fix this; but there\u0026#39;s nothing in the conceptual model that would prevent\n    some form of \u003ccode\u003eRetryingChannel\u003c/code\u003e class that blocks the producer until an item is\n    \u003cem\u003eeventually\u003c/em\u003e processed. The same can\u0026#39;t really be said of Queues, which will\n    \u003cem\u003ealways\u003c/em\u003e buffer at least one item no matter what you do.\n\u003c/p\u003e\n\u003cp\u003e\n    A more detailed look at all the issues with backpressure can be read\n    \u003ca href=\"https://vorpus.org/blog/some-thoughts-on-asynchronous-api-design-in-a-post-asyncawait-world/\"\u003ein this post\u003c/a\u003e\n    by the Trio creator.\n\u003c/p\u003e\n\n\u003ch2 id=\"less-major-problems-a-collection\"\u003eLess Major Problems, a collection\u003c/h2\u003e\n\u003cp\u003e\n    Whilst those four areas are some of the worst parts of \u003ccode\u003easyncio\u003c/code\u003e, there\u0026#39;s a lot\n    of minor warts that make it unpleasant to use everywhere else.\n\u003c/p\u003e\n\u003ch3 id=\"threads-are-stupid\"\u003eThreads are stupid\u003c/h3\u003e\n\u003cp\u003e\n    It is an inevitability that asynchronous code needs to use threads for computationally intensive\n    code or for libraries that still use blocking I/O. \u003ccode\u003easyncio\u003c/code\u003e offers two APIs for\n    this:\n\u003c/p\u003e\n\u003cul\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            \u003ccode\u003easyncio.to_thread\u003c/code\u003e which propagates context variables correctly to worker\n            threads but doesn\u0026#39;t let you specify the\n            \u003ccode\u003econcurrent.futures.ThreadPoolExecutor\u003c/code\u003e to use.\n        \u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            \u003ccode\u003eloop.run_in_executor\u003c/code\u003e which \u003cem\u003edoesn\u0026#39;t\u003c/em\u003e propagate context variables\n            but does let you specify the \u003ccode\u003eThreadPoolExecutor\u003c/code\u003e to use; you need to wrap\n            every function you\u0026#39;re passing in a \u003ccode\u003eContext.run\u003c/code\u003e call.\n        \u003c/p\u003e\n    \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n    This trade-off is very niche but it also doesn\u0026#39;t really need to exist. The more important\n    problem with threads comes from calling back into the event loop from a thread; cancellation\n    does not propagate properly! Take this example:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"05/threads_suck.py\"\u003e\u003ccode\u003eimport asyncio\nfrom functools import partial\n\n\nasync def coro():\n    await asyncio.sleep(5)\n    print(\u0026#34;as if i would be cancelled!\u0026#34;)\n\n\ndef in_thread(loop: asyncio.AbstractEventLoop):\n    fut = asyncio.run_coroutine_threadsafe(coro(), loop)\n    fut.result()\n\n\nasync def main():\n    t = asyncio.create_task(asyncio.to_thread(partial(in_thread, asyncio.get_running_loop())))\n    await asyncio.sleep(0)\n    t.cancel()\n    await asyncio.sleep(7)\n\n\nasyncio.run(main())\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    Running this will print \u003ccode\u003eas if i would be cancelled!\u003c/code\u003e because cancelling the\n    \u003ccode\u003eto_thread\u003c/code\u003e task will not cancel the synchronous task running on the event loop.\n    Let\u0026#39;s look at how Trio does it:\n\u003c/p\u003e\n\n\u003cpre data-snippet=\"05/threads_cool.py\"\u003e\u003ccode\u003efrom functools import partial\n\nimport trio\nimport trio.from_thread\nimport trio.to_thread\n\n\nasync def async_task():\n    await trio.sleep(5)\n    print(\u0026#34;looks like I survived being cancelled\u0026#34;)\n    return 1\n\n\ndef sync_task():\n    try:\n        ret = trio.from_thread.run(async_task)\n    except BaseException as e:\n        print(\u0026#34;raised\u0026#34;, e)\n    else:\n        print(\u0026#34;returned\u0026#34;, ret)\n\n\nasync def main():\n    async with trio.open_nursery() as group:\n        group.start_soon(partial(trio.to_thread.run_sync, sync_task))\n        await trio.sleep(1)\n        group.cancel_scope.cancel()\n\n\ntrio.run(main)\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\n    Cancelling the outer cancel scope will cancel the inner task and this code will print\n    \u003ccode\u003eraised Cancelled\u003c/code\u003e as the exception (correctly) propagates outwards into the\n    \u003ccode\u003esync_task\u003c/code\u003e function.\n\u003c/p\u003e\n\n\u003ch3 id=\"other\"\u003eOther, minor problems\u003c/h3\u003e\n\u003cul\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            \u003ccode\u003easyncio\u003c/code\u003e\u0026#39;s Unix signal API consists entirely of\n            \u003ccode\u003eloop.add_signal_handler\u003c/code\u003e, which takes a callback and schedules it on the\n            event loop when a single signal is received; and \u003ccode\u003eloop.remove_signal_handler\u003c/code\u003e\n            which rips out a handler for the specific signal manually. Compare this to Trio\u0026#39;s\n            \u003ca href=\"https://trio.readthedocs.io/en/stable/reference-io.html#trio.open_signal_receiver\"\u003e\u003ccode\u003eopen_signal_receiver\u003c/code\u003e\u003c/a\u003e\n            API which lets you listen to multiple signals with one object, uses an asynchronous\n            context manager to ensure that the handler is cleaned up, and is an iterator instead of\n            a callback so the control flow is linear and easier to follow.\n        \u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            Eager tasks were a performance optimisation that was added where\n            \u003ccode\u003ecreate_task\u003c/code\u003e forces a task to run up to the first suspension point, as\n            opposed to lazy tasks where they will not run until the next tick of the event loop.\n        \u003c/p\u003e\n        \u003cp\u003e\n            Unfortunately, they were broken on release (\u003ca href=\"https://github.com/python/cpython/issues/128550\"\u003e1\u003c/a\u003e, \u003ca href=\"https://github.com/python/cpython/issues/128588\"\u003e2\u003c/a\u003e) when interacting\n            with \u003ccode\u003eTaskGroup\u003c/code\u003e, and libraries often depend on the explicit semantics of\n            lazy tasks that have existed up to the present day.\n        \u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n        \u003cp\u003e\n            Speaking of \u003ccode\u003eTaskGroup\u003c/code\u003es, they are a mechanism to enforce\n            \u003ca href=\"https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/\"\u003estructured concurrency\u003c/a\u003e\n            in an \u003ccode\u003easyncio\u003c/code\u003e world. But due to \u003ccode\u003easyncio\u003c/code\u003e\u0026#39;s lack of\n            block-based cancellation - it only supports cancellation of single tasks - there\u0026#39;s\n            no way to cancel entire task groups. You have to cancel the task running the\n            \u003ccode\u003eTaskGroup\u003c/code\u003e instead, which doesn\u0026#39;t work if you only want to cancel a\n            nested \u003ccode\u003eTaskGroup\u003c/code\u003e and not the root one.\n        \u003c/p\u003e\n        \u003cp\u003e\n            Trio does not have this issue because it has \u003cem\u003escope cancellation\u003c/em\u003e instead. Code running\n            inside a \u003ccode\u003eCancelScope\u003c/code\u003e context manager can be cancelled independently,\n            regardless of how nested it is inside a task, instead of needing the entire task \n            to be cancelled at the very top level.\n        \u003c/p\u003e\n    \u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003e\n    \u003ccode\u003easyncio\u003c/code\u003e is not a good library. It is constantly full of sharp edges everywhere with\n    implementation details leaking and poorly designed APIs forcing end users into odd code patterns\n    to avoid fundamental flaws in the interfaces.\n\u003c/p\u003e\n\u003cp\u003e\n    Trio fixes nearly every single issue in this post. AnyIO implements Trio-like semantics on top\n    of \u003ccode\u003easyncio\u003c/code\u003e, whilst still letting you use most parts of libraries designed for \u003ccode\u003easyncio\u003c/code\u003e.\n\u003c/p\u003e\n\n\n        \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "40 min read",
  "publishedTime": null,
  "modifiedTime": null
}
