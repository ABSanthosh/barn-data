{
  "id": "cb60f479-3196-4fc7-a4eb-0c62be467d84",
  "title": "How does DeepSeek R1 really fare against OpenAI’s best reasoning models?",
  "link": "https://arstechnica.com/ai/2025/01/how-does-deepseek-r1-really-fare-against-openais-best-reasoning-models/",
  "description": "We run the LLMs through a gauntlet of tests, from creative writing to complex instruction.",
  "author": "Kyle Orland",
  "published": "Tue, 28 Jan 2025 22:44:49 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI"
  ],
  "byline": "Kyle Orland",
  "length": 14568,
  "excerpt": "We run the LLMs through a gauntlet of tests, from creative writing to complex instruction.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "You must defeat R1 to stand a chance We run the LLMs through a gauntlet of tests, from creative writing to complex instruction. Round 1. Fight! Credit: Aurich Lawson Round 1. Fight! Credit: Aurich Lawson It's only been a week since Chinese company DeepSeek launched its open-weights R1 reasoning model, which is reportedly competitive with OpenAI's state-of-the-art o1 models despite being trained for a fraction of the cost. Already, American AI companies are in a panic, and markets are freaking out over what could be a breakthrough in the status quo for large language models. While DeepSeek can point to common benchmark results and Chatbot Arena leaderboard to prove the competitiveness of its model, there's nothing like direct use cases to get a feel for just how useful a new model is. To that end, we decided to put DeepSeek's R1 model up against OpenAI's ChatGPT models in the style of our previous showdowns between ChatGPT and Google Bard/Gemini. This was not designed to be a test of the hardest problems possible; it's more of a sample of everyday questions these models might get asked by users. This time around, we put each DeepSeek response against ChatGPT's $20/month o1 model and $200/month o1 Pro model, to see how it stands up to OpenAI's \"state of the art\" product as well as the \"everyday\" product that most AI consumers use. While we re-used a few of the prompts from our previous tests, we also added prompts derived from Chatbot Arena's \"categories\" appendix, covering areas such as creative writing, math, instruction following, and so-called \"hard prompts\" that are \"designed to be more complex, demanding, and rigorous.\" We then judged the responses based not just on their \"correctness\" but also on more subjective qualities. While we judged each model primarily on the responses to our prompts, when appropriate, we also looked at the \"chain of thought\" reasoning they output to get a better idea of what's going on under the hood. In the case of DeepSeek R1, this sometimes resulted in some extremely long and detailed discussions of the internal steps to get to that final result. Dad jokes DeepSeek R1 \"dad joke\" prompt response Prompt: Write five original dad jokes Results: For the most part, all three models seem to have taken our demand for \"original\" jokes more seriously this time than in the past. Out of the 15 jokes generated, we were only able to find similar examples online for two of them: o1's \"belt made out of watches\" and o1 Pro's \"sleeping on a stack of old magazines.\" Disregarding those two, the results were highly variable. All three models generated quite a few jokes that either struggled too hard for a pun (R1's \"quack\"-seal enthusiast duck; o1 Pro's \"bark-to-bark communicator\" dog) or that just didn't really make sense at all (o1's \"sweet time\" pet rock; o1 pro's restaurant that serves \"everything on the menu\"). That said, there were a few completely original, completely groan-worthy winners to be found here. We particularly liked DeepSeek R1's bicycle that doesn't like to \"spin its wheels\" with pointless arguments and o1's vacuum-cleaner band that \"sucks\" at live shows. Compared to the jokes LLMs generated just over a year ago, there's definitely progress being made on the humor front here. Winner: ChatGPT o1 probably had slightly better jokes overall than DeepSeek R1, but loses some points for including a joke that was not original. ChatGPT o1 Pro is the clear loser, though, with no original jokes that we'd consider the least bit funny. Abraham “Hoops” Lincoln DeepSeek R1 Abraham 'Hoops' Lincoln prompt response Prompt: Write a two-paragraph creative story about Abraham Lincoln inventing basketball. Results: DeepSeek R1's response is a delightfully absurd take on an absurd prompt. We especially liked the bits about creating \"a sport where men leap not into trenches, but toward glory\" and a \"13th amendment\" to the rules preventing players from being \"enslaved by poor sportsmanship\" (whatever that means). DeepSeek also gains points for mentioning Lincoln's actual secretary, John Hay, and the president's chronic insomnia, which supposedly led him to patent a pneumatic pillow (whatever that is). ChatGPT o1, by contrast, feels a little more straitlaced. The story focuses mostly on what a game of early basketball might look like and how it might be later refined by Lincoln and his generals. While there are a few incidental details about Lincoln (his stovepipe hat, leading a nation at war), there's a lot of filler material that makes it feel more generic. ChatGPT o1 Pro makes the interesting decision to set the story \"long before [Lincoln's] presidency,\" making the game the hit of Springfield, Illinois. The model also makes a valiant attempt to link Lincoln's eventual ability to \"unify a divided nation\" with the cheers of the basketball-watching townsfolk. Bonus points for the creative game name of \"Lincoln's Hoop and Toss,\" too. Winner: While o1 Pro made a good showing, the sheer wild absurdity of the DeepSeek R1 response won us over. Hidden code DeepSeek R1 \"hidden code\" prompt response Prompt: Write a short paragraph where the second letter of each sentence spells out the word ‘CODE’. The message should appear natural and not obviously hide this pattern. Results: This prompt represented DeepSeek R1's biggest failure in our tests, with the model using the first letter of each sentence for the secret code rather than the requested second letter. When we expanded the model's extremely thorough explanation of its 220-second \"thought process,\" though, we surprisingly found a paragraph that did match the prompt, which was apparently thrown out just before giving the final answer: \"School courses build foundations. You hone skills through practice. IDEs enhance coding efficiency. Be open to learning always.\" ChatGPT o1 made the same mistake regarding first and second letters as DeepSeek, despite \"thought details\" that assure us it is \"ensuring letter sequences\" and \"ensuring alignment.\" ChatGPT o1 Pro is the only one that seems to have understood the assignment, crafting a delicate, haiku-like response with the \"code\"-word correctly embedded after over four minutes of thinking. Winner: ChatGPT o1 Pro wins pretty much by default as the only one able to correctly follow directions. Historical color naming Deepseek R1 \"Magenta\" prompt response Prompt: Would the color be called 'magenta' if the town of Magenta didn't exist? Results: All three prompts correctly link the color name \"magenta\" to the dye's discovery in the town of Magenta and the nearly coincident 1859 Battle of Magenta, which helped make the color famous. All three responses also mention the alternative name of \"fuschine\" and its link to the similarly colored fuchsia flower. Stylistically, ChatGPT o1 Pro gains a few points for splitting its response into a tl;dr \"short answer\" followed by a point-by-point breakdown of the details discussed above and a coherent conclusion statement. When it comes to the raw information, though, all three models performed admirably. Results: ChatGPT 01 Pro is the winner by a stylistic hair. Big primes DeepSeek R1 \"billionth prime\" prompt response Prompt: What is the billionth largest prime number? Result: We see a big divergence between DeepSeek and the ChatGPT models here. DeepSeek is the only one to give a precise answer, referencing both PrimeGrid and The Prime Pages for previous calculations of 22,801,763,489 as the billionth prime. ChatGPT o1 and o1 Pro, on the other hand, insist that this value \"hasn't been publicly documented\" (o1) or that \"no well-known, published project has yet singled [it] out\" (o1 Pro). Instead, both ChatGPT models go into a detailed discussion of the Prime Number Theorem and how it can be used to estimate that the answer lies somewhere in the 22.8 to 23 billion range. DeepSeek briefly mentions this theorem, but mainly as a way to verify that the answers provided by Prime Pages and PrimeGrid are reasonable. Oddly enough, both o1 models' written-out \"thought process\" make mention of \"considering references\" or comparing to \"refined references\" during their calculations, suggesting some lists of primes buried deep in their training data. But neither model was willing or able to directly reference those lists for a precise answer. Winner: DeepSeek R1 is the clear winner for precision here, though the ChatGPT models give pretty good estimates. Airport planning Prompt: I need you to create a timetable for me given the following facts: my plane takes off at 6:30am. I need to be at the airport 1h before take off. it will take 45mins to get to the airport. I need 1h to get dressed and have breakfast before we leave. The plan should include when to wake up and the time I need to get into the vehicle to get to the airport in time for my 6:30am flight, think through this step by step. Results: All three models get the basic math right here, calculating that you need to wake up at 3:45 am to get to a 6:30 flight. ChatGPT o1 earns a few bonus points for generating the response seven seconds faster than DeepSeek R1 (and much faster than o1 Pro's 77 seconds); testing on o1 Mini might generate even quicker response times. DeepSeek claws a few points back, though, with an added \"Why this works\" section containing a warning about traffic/security line delays and a \"Pro Tip\" to lay out your packing and breakfast the night before. We also like r1's \"(no snooze!)\" admonishment next to the 3:45 am wake-up time. Well worth the extra seven seconds of thinking. Winner: DeepSeek R1 wins by a hair with its stylistic flair. Follow the ball DeepSeek R1 \"follow the ball\" prompt response Prompt: In my kitchen, there’s a table with a cup with a ball inside. I moved the cup to my bed in my bedroom and turned the cup upside down. I grabbed the cup again and moved to the main room. Where’s the ball now? Results: All three models are able to correctly reason that turning a cup upside down will cause a ball to fall out and remain on the bed, even if the cup moves later. This might not sound that impressive if you have object permanence, but LLMs have struggled with this kind of \"world model\" understanding of objects until quite recently. DeepSeek R1 deserves a few bonus points for noting the \"key assumption\" that there's no lid on the cup keeping the ball inside (maybe it was a trick question?). ChatGPT o1 also gains a few points for noting that the ball may have rolled off the bed and onto the floor, as balls are wont to do. We were also a bit tickled by R1 insisting that this prompt is an example of \"classic misdirection\" because \"the focus on moving the cup distracts from where the ball was left.\" We urge Penn \u0026 Teller to integrate a \"amaze and delight the large language model\" ball-on-the-bed trick into their Vegas act. Winner: We'll declare a three-way tie here, as all the models followed the ball correctly. Complex number sets DeepSeek R1 \"complex number set\" prompt response Prompt: Give me a list of 10 natural numbers, such that at least one is prime, at least 6 are odd, at least 2 are powers of 2, and such that the 10 numbers have at minimum 25 digits between them. Results: While there are a whole host of number lists that would satisfy these conditions, this prompt effectively tests the LLMs' abilities to follow moderately complex and confusing instructions without getting tripped up. All three generated valid responses, though in intriguingly different ways. ChagtGPT's o1's choice of 2^30 and 2^31 as powers of two seemed a bit out of left field, as did o1 Pro's choice of the prime number 999,983. We have to dock some significant points from DeepSeek R1, though, for insisting that its solution had 36 combined digits when it actually had 33 (\"3+3+4+3+3+3+3+3+4+4,\" as R1 itself notes before giving the wrong sum). While this simple arithmetic error didn't make the final set of numbers incorrect, it easily could have with a slightly different prompt. Winner: The two ChatGPT models tie for the win thanks to their lack of arithmetic mistakes Declaring a winner While we'd love to declare a clear winner in the brewing AI battle here, the results here are too scattered to do that. DeepSeek's R1 model definitely distinguished itself by citing reliable sources to identify the billionth prime number and with some quality creative writing in the dad jokes and Abraham Lincoln's basketball prompts. However, the model failed on the hidden code and complex number set prompts, making basic errors in counting and/or arithmetic that one or both of the OpenAI models avoided. Overall, though, we came away from these brief tests convinced that DeepSeek's R1 model can generate results that are overall competitive with the best paid models from OpenAI. That should give great pause to anyone who assumed extreme scaling in terms of training and computation costs was the only way to compete with the most deeply entrenched companies in the world of AI. Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once wrote a whole book about Minesweeper. 57 Comments",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-1152x648.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n            \u003carticle data-id=\"2073211\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n    \u003cdiv\u003e\n      \u003cp\u003e\u003cspan\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"section-ai_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"section-ai_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#section-ai_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#section-ai_svg__b)\"\u003e\u003cpath d=\"M20 2.4c9.7 0 17.6 7.9 17.6 17.6S29.7 37.6 20 37.6 2.4 29.7 2.4 20 10.3 2.4 20 2.4M20 0C9 0 0 9 0 20s9 20 20 20 20-9 20-20S31 0 20 0\"\u003e\u003c/path\u003e\u003cpath d=\"M20 13q2.85 0 5.4.9c.7.2 1.4-.1 1.6-.9l1.4-5.5C26 5.9 23.1 4.9 20 4.9s-6 .9-8.4 2.6L13 13c.2.7.9 1.1 1.6.9Q17 13 20 13M8.9 18.3c.4-.8 1-1.5 1.7-2.1l-2.2-5.7C7 12.2 6 14.1 5.5 16.3l1.3 2.1c.5.8 1.7.8 2.2 0m24.3 0 1.3-2.1c-.5-2.2-1.5-4.1-2.9-5.8l-2.2 5.7c.7.6 1.3 1.3 1.7 2.1.5.8 1.6.9 2.2 0M23.2 20c0 1.8-1.5 3.2-3.2 3.2s-3.2-1.4-3.2-3.2 1.5-3.2 3.2-3.2 3.2 1.4 3.2 3.2\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n  \u003c/span\u003e\n  \u003cspan\u003e\n    You must defeat R1 to stand a chance\n  \u003c/span\u003e\n\u003c/p\u003e\n    \u003c/div\u003e\n\n    \n\n    \u003cp\u003e\n      We run the LLMs through a gauntlet of tests, from creative writing to complex instruction.\n    \u003c/p\u003e\n\n    \n\n    \u003cdiv\u003e\n            \u003cp\u003e\u003ca data-pswp-width=\"2560\" data-pswp-height=\"1440\" data-pswp-srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek.jpg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-2048x1152.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-1440x810.jpg 1440w\" data-cropped=\"false\" href=\"https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek.jpg\" target=\"_blank\"\u003e\n              \u003cimg width=\"2560\" height=\"1440\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek.jpg\" alt=\"\" loading=\"eager\" decoding=\"async\" fetchpriority=\"high\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek.jpg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-2048x1152.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/chatgpt-vs-deepseek-1440x810.jpg 1440w\" sizes=\"(max-width: 2560px) 100vw, 2560px\"/\u003e\n            \u003c/a\u003e\u003c/p\u003e\u003cdiv id=\"caption-2073144\"\u003e\n    \n    \u003cp\u003e\n      Round 1. Fight!\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Aurich Lawson\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n          \u003c/div\u003e\n\n    \u003cdiv\u003e\n    \n    \u003cp\u003e\n      Round 1. Fight!\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Aurich Lawson\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/header\u003e\n\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \n\u003cp\u003eIt\u0026#39;s only been a week since Chinese company DeepSeek \u003ca href=\"https://arstechnica.com/ai/2025/01/china-is-catching-up-with-americas-best-reasoning-ai-models/\"\u003elaunched its open-weights R1 reasoning model\u003c/a\u003e, which is reportedly competitive with OpenAI\u0026#39;s state-of-the-art o1 models despite being \u003ca href=\"https://www.reuters.com/technology/artificial-intelligence/chinas-deepseek-sparks-ai-market-rout-2025-01-27/\"\u003etrained for a fraction of the cost\u003c/a\u003e. Already, \u003ca href=\"https://arstechnica.com/ai/2025/01/deepseek-spooks-american-tech-industry-as-it-tops-the-apple-app-store/\"\u003eAmerican AI companies are in a panic\u003c/a\u003e, and \u003ca href=\"https://arstechnica.com/ai/2025/01/why-the-markets-are-freaking-out-about-chinese-ai-newcomer-deepseek/\"\u003emarkets are freaking out\u003c/a\u003e over what could be a breakthrough in the status quo for large language models.\u003c/p\u003e\n\u003cp\u003eWhile DeepSeek can point to \u003ca href=\"https://techcrunch.com/2025/01/27/deepseek-claims-its-reasoning-model-beats-openais-o1-on-certain-benchmarks/\"\u003ecommon benchmark results\u003c/a\u003e and \u003ca href=\"https://lmarena.ai/\"\u003eChatbot Arena leaderboard\u003c/a\u003e to prove the competitiveness of its model, there\u0026#39;s nothing like direct use cases to get a feel for just how useful a new model is. To that end, we decided to put DeepSeek\u0026#39;s R1 model up against OpenAI\u0026#39;s ChatGPT models in the style of our \u003ca href=\"https://arstechnica.com/ai/2023/12/chatgpt-vs-google-bard-round-2-how-does-the-new-gemini-model-fare/\"\u003eprevious showdowns\u003c/a\u003e between \u003ca href=\"https://arstechnica.com/information-technology/2023/04/clash-of-the-ai-titans-chatgpt-vs-bard-in-a-showdown-of-wits-and-wisdom/\"\u003eChatGPT and Google Bard/Gemini\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis was not designed to be a test of the hardest problems possible; it\u0026#39;s more of a sample of everyday questions these models might get asked by users.\u003c/p\u003e\n\u003cp\u003eThis time around, we put each DeepSeek response against ChatGPT\u0026#39;s \u003ca href=\"https://arstechnica.com/information-technology/2024/09/openais-new-reasoning-ai-models-are-here-o1-preview-and-o1-mini/\"\u003e$20/month o1 model\u003c/a\u003e and \u003ca href=\"https://arstechnica.com/ai/2024/12/openais-new-200-mo-chatgpt-subscription-will-buy-you-more-compute-time/\"\u003e$200/month o1 Pro model\u003c/a\u003e, to see how it stands up to OpenAI\u0026#39;s \u0026#34;state of the art\u0026#34; product as well as the \u0026#34;everyday\u0026#34; product that most AI consumers use. While we re-used a few of the prompts from our previous tests, we also added prompts \u003ca href=\"https://blog.lmarena.ai/blog/2024/arena-category\"\u003ederived from Chatbot Arena\u0026#39;s \u0026#34;categories\u0026#34; appendix\u003c/a\u003e, covering areas such as creative writing, math, instruction following, and so-called \u0026#34;hard prompts\u0026#34; that are \u0026#34;designed to be more complex, demanding, and rigorous.\u0026#34; We then judged the responses based not just on their \u0026#34;correctness\u0026#34; but also on more subjective qualities.\u003c/p\u003e\n\u003cp\u003eWhile we judged each model primarily on the responses to our prompts, when appropriate, we also looked at the \u0026#34;chain of thought\u0026#34; reasoning they output to get a better idea of what\u0026#39;s going on under the hood. In the case of DeepSeek R1, this sometimes resulted in some extremely long and detailed discussions of the internal steps to get to that final result.\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003ch2\u003eDad jokes\u003c/h2\u003e\n\u003cdiv\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"arrow-blocks-right_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#arrow-blocks-right_svg__a)\"\u003e\u003cpath d=\"M32 16h8v8h-8zm-8 8h8v8h-8zm-8 8h8v8h-8zm8-24h8v8h-8zm-8-8h8v8h-8zM0 16h16v8H0z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/svg\u003e\n\n    \u003cp\u003e\u003cspan\u003eDeepSeek R1 \u0026#34;dad joke\u0026#34; prompt response\u003c/span\u003e\n                \u003c/p\u003e\n  \u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003ePrompt: Write five original dad jokes\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e For the most part, all three models seem to have taken our demand for \u0026#34;original\u0026#34; jokes more seriously this time than in the past. Out of the 15 jokes generated, we were only able to find similar examples online for two of them: o1\u0026#39;s \u0026#34;belt made out of watches\u0026#34; and o1 Pro\u0026#39;s \u0026#34;sleeping on a stack of old magazines.\u0026#34;\u003c/p\u003e\n\u003cp\u003eDisregarding those two, the results were highly variable. All three models generated quite a few jokes that either struggled too hard for a pun (R1\u0026#39;s \u0026#34;quack\u0026#34;-seal enthusiast duck; o1 Pro\u0026#39;s \u0026#34;bark-to-bark communicator\u0026#34; dog) or that just didn\u0026#39;t really make sense at all (o1\u0026#39;s \u0026#34;sweet time\u0026#34; pet rock; o1 pro\u0026#39;s restaurant that serves \u0026#34;everything on the menu\u0026#34;).\u003c/p\u003e\n\u003cp\u003eThat said, there were a few completely original, completely groan-worthy winners to be found here. We particularly liked DeepSeek R1\u0026#39;s bicycle that doesn\u0026#39;t like to \u0026#34;spin its wheels\u0026#34; with pointless arguments and o1\u0026#39;s vacuum-cleaner band that \u0026#34;sucks\u0026#34; at live shows. Compared to \u003ca href=\"https://arstechnica.com/ai/2023/12/chatgpt-vs-google-bard-round-2-how-does-the-new-gemini-model-fare/\"\u003ethe jokes LLMs generated just over a year ago\u003c/a\u003e, there\u0026#39;s definitely progress being made on the humor front here.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWinner:\u003c/strong\u003e ChatGPT o1 probably had slightly better jokes overall than DeepSeek R1, but loses some points for including a joke that was not original. ChatGPT o1 Pro is the clear loser, though, with no original jokes that we\u0026#39;d consider the least bit funny.\u003c/p\u003e\n\u003ch2\u003eAbraham “Hoops” Lincoln\u003c/h2\u003e\n\u003cdiv\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"arrow-blocks-right_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#arrow-blocks-right_svg__a)\"\u003e\u003cpath d=\"M32 16h8v8h-8zm-8 8h8v8h-8zm-8 8h8v8h-8zm8-24h8v8h-8zm-8-8h8v8h-8zM0 16h16v8H0z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/svg\u003e\n\n    \u003cp\u003e\u003cspan\u003eDeepSeek R1 Abraham \u0026#39;Hoops\u0026#39; Lincoln prompt response\u003c/span\u003e\n                \u003c/p\u003e\n  \u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003ePrompt:\u003c/strong\u003e Write a two-paragraph creative story about Abraham Lincoln inventing basketball.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e DeepSeek R1\u0026#39;s response is a delightfully absurd take on an absurd prompt. We especially liked the bits about creating \u0026#34;a sport where men leap not into trenches, but toward glory\u0026#34; and a \u0026#34;13th amendment\u0026#34; to the rules preventing players from being \u0026#34;enslaved by poor sportsmanship\u0026#34; (whatever that means). DeepSeek also gains points for mentioning Lincoln\u0026#39;s actual secretary, John Hay, and the president\u0026#39;s chronic insomnia, which supposedly led him to patent a pneumatic pillow (whatever that is).\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eChatGPT o1, by contrast, feels a little more straitlaced. The story focuses mostly on what a game of early basketball might look like and how it might be later refined by Lincoln and his generals. While there are a few incidental details about Lincoln (his stovepipe hat, leading a nation at war), there\u0026#39;s a lot of filler material that makes it feel more generic.\u003c/p\u003e\n\u003cp\u003eChatGPT o1 Pro makes the interesting decision to set the story \u0026#34;long before [Lincoln\u0026#39;s] presidency,\u0026#34; making the game the hit of Springfield, Illinois. The model also makes a valiant attempt to link Lincoln\u0026#39;s eventual ability to \u0026#34;unify a divided nation\u0026#34; with the cheers of the basketball-watching townsfolk. Bonus points for the creative game name of \u0026#34;Lincoln\u0026#39;s Hoop and Toss,\u0026#34; too.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWinner:\u003c/strong\u003e While o1 Pro made a good showing, the sheer wild absurdity of the DeepSeek R1 response won us over.\u003c/p\u003e\n\n\u003ch2\u003eHidden code\u003c/h2\u003e\n\u003cdiv\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"arrow-blocks-right_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#arrow-blocks-right_svg__a)\"\u003e\u003cpath d=\"M32 16h8v8h-8zm-8 8h8v8h-8zm-8 8h8v8h-8zm8-24h8v8h-8zm-8-8h8v8h-8zM0 16h16v8H0z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/svg\u003e\n\n    \u003cp\u003e\u003cspan\u003eDeepSeek R1 \u0026#34;hidden code\u0026#34; prompt response\u003c/span\u003e\n                \u003c/p\u003e\n  \u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003ePrompt:\u003c/strong\u003e Write a short paragraph where the second letter of each sentence spells out the word ‘CODE’. The message should appear natural and not obviously hide this pattern.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e This prompt represented DeepSeek R1\u0026#39;s biggest failure in our tests, with the model using the first letter of each sentence for the secret code rather than the requested second letter. When we expanded the model\u0026#39;s extremely thorough explanation of its 220-second \u0026#34;thought process,\u0026#34; though, we surprisingly found a paragraph that \u003cem\u003edid\u003c/em\u003e match the prompt, which was apparently thrown out just before giving the final answer:\u003c/p\u003e\n\u003cblockquote\u003e\u003cp\u003e\u0026#34;School courses build foundations. You hone skills through practice. IDEs enhance coding efficiency. Be open to learning always.\u0026#34;\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eChatGPT o1 made the same mistake regarding first and second letters as DeepSeek, despite \u0026#34;thought details\u0026#34; that assure us it is \u0026#34;ensuring letter sequences\u0026#34; and \u0026#34;ensuring alignment.\u0026#34; ChatGPT o1 Pro is the only one that seems to have understood the assignment, crafting a delicate, haiku-like response with the \u0026#34;code\u0026#34;-word correctly embedded after over four minutes of thinking.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003e\u003cstrong\u003eWinner:\u003c/strong\u003e ChatGPT o1 Pro wins pretty much by default as the only one able to correctly follow directions.\u003c/p\u003e\n\u003ch2\u003eHistorical color naming\u003c/h2\u003e\n\u003cdiv\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"arrow-blocks-right_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#arrow-blocks-right_svg__a)\"\u003e\u003cpath d=\"M32 16h8v8h-8zm-8 8h8v8h-8zm-8 8h8v8h-8zm8-24h8v8h-8zm-8-8h8v8h-8zM0 16h16v8H0z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/svg\u003e\n\n    \u003cp\u003e\u003cspan\u003eDeepseek R1 \u0026#34;Magenta\u0026#34; prompt response\u003c/span\u003e\n                \u003c/p\u003e\n  \u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003ePrompt:\u003c/strong\u003e Would the color be called \u0026#39;magenta\u0026#39; if the town of Magenta didn\u0026#39;t exist?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e All three prompts correctly link the color name \u0026#34;magenta\u0026#34; to the dye\u0026#39;s discovery in the town of Magenta and the nearly coincident 1859 Battle of Magenta, which helped make the color famous. All three responses also mention the alternative name of \u0026#34;fuschine\u0026#34; and its link to the similarly colored fuchsia flower.\u003c/p\u003e\n\u003cp\u003eStylistically, ChatGPT o1 Pro gains a few points for splitting its response into a tl;dr \u0026#34;short answer\u0026#34; followed by a point-by-point breakdown of the details discussed above and a coherent conclusion statement. When it comes to the raw information, though, all three models performed admirably.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e ChatGPT 01 Pro is the winner by a stylistic hair.\u003c/p\u003e\n\u003ch2\u003eBig primes\u003c/h2\u003e\n\u003cdiv\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"arrow-blocks-right_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#arrow-blocks-right_svg__a)\"\u003e\u003cpath d=\"M32 16h8v8h-8zm-8 8h8v8h-8zm-8 8h8v8h-8zm8-24h8v8h-8zm-8-8h8v8h-8zM0 16h16v8H0z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/svg\u003e\n\n    \u003cp\u003e\u003cspan\u003eDeepSeek R1 \u0026#34;billionth prime\u0026#34; prompt response\u003c/span\u003e\n                \u003c/p\u003e\n  \u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003ePrompt:\u003c/strong\u003e What is the billionth largest prime number?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResult:\u003c/strong\u003e We see a big divergence between DeepSeek and the ChatGPT models here. DeepSeek is the only one to give a precise answer, referencing both \u003ca href=\"https://www.primegrid.com/\"\u003ePrimeGrid\u003c/a\u003e and \u003ca href=\"https://t5k.org/curios/page.php/22801763489.html\"\u003eThe Prime Pages\u003c/a\u003e for previous calculations of 22,801,763,489 as the billionth prime. ChatGPT o1 and o1 Pro, on the other hand, insist that this value \u0026#34;hasn\u0026#39;t been publicly documented\u0026#34; (o1) or that \u0026#34;no well-known, published project has yet singled [it] out\u0026#34; (o1 Pro).\u003c/p\u003e\n\u003cp\u003eInstead, both ChatGPT models go into a detailed discussion of \u003ca href=\"https://www.britannica.com/science/prime-number-theorem\"\u003ethe Prime Number Theorem\u003c/a\u003e and how it can be used to estimate that the answer lies somewhere in the 22.8 to 23 billion range. DeepSeek briefly mentions this theorem, but mainly as a way to verify that the answers provided by Prime Pages and PrimeGrid are reasonable.\u003c/p\u003e\n\u003cp\u003eOddly enough, both o1 models\u0026#39; written-out \u0026#34;thought process\u0026#34; make mention of \u0026#34;considering references\u0026#34; or comparing to \u0026#34;refined references\u0026#34; during their calculations, suggesting some lists of primes buried deep in their training data. But neither model was willing or able to directly reference those lists for a precise answer.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003e\u003cstrong\u003eWinner:\u003c/strong\u003e DeepSeek R1 is the clear winner for precision here, though the ChatGPT models give pretty good estimates.\u003c/p\u003e\n\n\u003ch2\u003eAirport planning\u003c/h2\u003e\n\n\n\u003cp\u003e\u003cstrong\u003ePrompt:\u003c/strong\u003e I need you to create a timetable for me given the following facts: my plane takes off at 6:30am. I need to be at the airport 1h before take off. it will take 45mins to get to the airport. I need 1h to get dressed and have breakfast before we leave. The plan should include when to wake up and the time I need to get into the vehicle to get to the airport in time for my 6:30am flight, think through this step by step.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e All three models get the basic math right here, calculating that you need to wake up at 3:45 am to get to a 6:30 flight. ChatGPT o1 earns a few bonus points for generating the response seven seconds faster than DeepSeek R1 (and much faster than o1 Pro\u0026#39;s 77 seconds); testing on o1 Mini might generate even quicker response times.\u003c/p\u003e\n\u003cp\u003eDeepSeek claws a few points back, though, with an added \u0026#34;Why this works\u0026#34; section containing a warning about traffic/security line delays and a \u0026#34;Pro Tip\u0026#34; to lay out your packing and breakfast the night before. We also like r1\u0026#39;s \u0026#34;(no snooze!)\u0026#34; admonishment next to the 3:45 am wake-up time. Well worth the extra seven seconds of thinking.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWinner:\u003c/strong\u003e DeepSeek R1 wins by a hair with its stylistic flair.\u003c/p\u003e\n\u003ch2\u003eFollow the ball\u003c/h2\u003e\n\u003cdiv\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"arrow-blocks-right_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#arrow-blocks-right_svg__a)\"\u003e\u003cpath d=\"M32 16h8v8h-8zm-8 8h8v8h-8zm-8 8h8v8h-8zm8-24h8v8h-8zm-8-8h8v8h-8zM0 16h16v8H0z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/svg\u003e\n\n    \u003cp\u003e\u003cspan\u003eDeepSeek R1 \u0026#34;follow the ball\u0026#34; prompt response\u003c/span\u003e\n                \u003c/p\u003e\n  \u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003ePrompt:\u003c/strong\u003e In my kitchen, there’s a table with a cup with a ball inside. I moved the cup to my bed in my bedroom and turned the cup upside down. I grabbed the cup again and moved to the main room. Where’s the ball now?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e All three models are able to correctly reason that turning a cup upside down will cause a ball to fall out and remain on the bed, even if the cup moves later. This might not sound that impressive if you have object permanence, but LLMs have struggled with this kind of \u0026#34;world model\u0026#34; understanding of objects until quite recently.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n\n        \n        \u003cdiv\u003e\n          \n          \n\u003cp\u003eDeepSeek R1 deserves a few bonus points for noting the \u0026#34;key assumption\u0026#34; that there\u0026#39;s no lid on the cup keeping the ball inside (maybe it was a trick question?). ChatGPT o1 also gains a few points for noting that the ball may have rolled off the bed and onto the floor, as balls are wont to do.\u003c/p\u003e\n\u003cp\u003eWe were also a bit tickled by R1 insisting that this prompt is an example of \u0026#34;classic misdirection\u0026#34; because \u0026#34;the focus on moving the cup distracts from where the ball was left.\u0026#34; We urge Penn \u0026amp; Teller to integrate a \u0026#34;amaze and delight the large language model\u0026#34; ball-on-the-bed trick into their Vegas act.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWinner:\u003c/strong\u003e We\u0026#39;ll declare a three-way tie here, as all the models followed the ball correctly.\u003c/p\u003e\n\u003ch2\u003eComplex number sets\u003c/h2\u003e\n\u003cdiv\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 40\"\u003e\u003cdefs\u003e\u003cclipPath id=\"arrow-blocks-right_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v40H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#arrow-blocks-right_svg__a)\"\u003e\u003cpath d=\"M32 16h8v8h-8zm-8 8h8v8h-8zm-8 8h8v8h-8zm8-24h8v8h-8zm-8-8h8v8h-8zM0 16h16v8H0z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/svg\u003e\n\n    \u003cp\u003e\u003cspan\u003eDeepSeek R1 \u0026#34;complex number set\u0026#34; prompt response\u003c/span\u003e\n                \u003c/p\u003e\n  \u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003ePrompt:\u003c/strong\u003e Give me a list of 10 natural numbers, such that at least one is prime, at least 6 are odd, at least 2 are powers of 2, and such that the 10 numbers have at minimum 25 digits between them.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e While there are a whole host of number lists that would satisfy these conditions, this prompt effectively tests the LLMs\u0026#39; abilities to follow moderately complex and confusing instructions without getting tripped up. All three generated valid responses, though in intriguingly different ways. ChagtGPT\u0026#39;s o1\u0026#39;s choice of 2^30 and 2^31 as powers of two seemed a bit out of left field, as did o1 Pro\u0026#39;s choice of the prime number 999,983.\u003c/p\u003e\n\u003cp\u003eWe have to dock some significant points from DeepSeek R1, though, for insisting that its solution had 36 combined digits when it actually had 33 (\u0026#34;3+3+4+3+3+3+3+3+4+4,\u0026#34; as R1 itself notes before giving the wrong sum). While this simple arithmetic error didn\u0026#39;t make the final set of numbers incorrect, it easily could have with a slightly different prompt.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWinner:\u003c/strong\u003e The two ChatGPT models tie for the win thanks to their lack of arithmetic mistakes\u003c/p\u003e\n\u003ch2\u003eDeclaring a winner\u003c/h2\u003e\n\u003cp\u003eWhile we\u0026#39;d love to declare a clear winner in the brewing AI battle here, the results here are too scattered to do that. DeepSeek\u0026#39;s R1 model definitely distinguished itself by citing reliable sources to identify the billionth prime number and with some quality creative writing in the dad jokes and Abraham Lincoln\u0026#39;s basketball prompts. However, the model failed on the hidden code and complex number set prompts, making basic errors in counting and/or arithmetic that one or both of the OpenAI models avoided.\u003c/p\u003e\n\u003cp\u003eOverall, though, we came away from these brief tests convinced that DeepSeek\u0026#39;s R1 model can generate results that are overall competitive with the best paid models from OpenAI. That should give great pause to anyone who assumed extreme scaling in terms of training and computation costs was the only way to compete with the most deeply entrenched companies in the world of AI.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \n\n\n\n\n\n\n  \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/kyle-orland/\"\u003e\u003cimg src=\"https://arstechnica.com/wp-content/uploads/2016/05/k.orland-13.jpg\" alt=\"Photo of Kyle Orland\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once \u003ca href=\"https://bossfightbooks.com/collections/books/products/minesweeper-by-kyle-orland\"\u003ewrote a whole book about \u003cem\u003eMinesweeper\u003c/em\u003e\u003c/a\u003e.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \u003cp\u003e\n    \u003ca href=\"https://arstechnica.com/ai/2025/01/how-does-deepseek-r1-really-fare-against-openais-best-reasoning-models/#comments\" title=\"57 comments\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 80\"\u003e\u003cdefs\u003e\u003cclipPath id=\"bubble-zero_svg__a\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"bubble-zero_svg__b\"\u003e\u003cpath fill=\"none\" stroke-width=\"0\" d=\"M0 0h80v80H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#bubble-zero_svg__a)\"\u003e\u003cg fill=\"currentColor\" clip-path=\"url(#bubble-zero_svg__b)\"\u003e\u003cpath d=\"M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40\"\u003e\u003c/path\u003e\u003cpath d=\"M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n    57 Comments\n  \u003c/a\u003e\n      \u003c/p\u003e\n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n  \n\n\n  \n\n\n  \u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003ca href=\"https://arstechnica.com/tech-policy/2025/01/bowing-to-trump-google-maps-plans-to-quickly-rename-the-gulf-of-mexico/\"\u003e\n              \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/01/gulf-of-mexico-768x432.jpg\" alt=\"Listing image for first story in Most Read: Google goes gaga over the Gulf of Mexico\" decoding=\"async\" loading=\"lazy\"/\u003e\n            \u003c/a\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "16 min read",
  "publishedTime": "2025-01-28T22:44:49Z",
  "modifiedTime": "2025-01-28T23:06:28Z"
}
