{
  "id": "8a41f658-1268-49c3-93af-d94470d5cc9d",
  "title": "Hospitals use a transcription tool powered by a hallucination-prone OpenAI model",
  "link": "https://www.theverge.com/2024/10/27/24281170/open-ai-whisper-hospitals-transcription-hallucinations-studies",
  "description": "",
  "author": "Wes Davis",
  "published": "2024-10-27T19:19:36-04:00",
  "source": "https://www.theverge.com/rss/index.xml",
  "categories": null,
  "byline": "Wes Davis",
  "length": 2116,
  "excerpt": "Hospitals routinely use a tool powered by OpenAI’s Whisper transcription model, which researchers find can hallucinate entire passages during periods of silence.",
  "siteName": "The Verge",
  "favicon": "https://www.theverge.com/icons/android_chrome_512x512.png",
  "text": "A few months ago, my doctor showed off an AI transcription tool he used to record and summarize his patient meetings. In my case, the summary was fine, but researchers cited by ABC News have found that’s not always the case with OpenAI’s Whisper, which powers a tool many hospitals use — sometimes it just makes things up entirely.Whisper is used by a company called Nabla for a medical transcription tool that it estimates has transcribed 7 million medical conversations, according to ABC News. More than 30,000 clinicians and 40 health systems use it, the outlet writes. Nabla is reportedly aware that Whisper can hallucinate, and is “addressing the problem.”A group of researchers from Cornell University, the University of Washington, and others found in a study that Whisper hallucinated in about 1 percent of transcriptions, making up entire sentences with sometimes violent sentiments or nonsensical phrases during silences in recordings. The researchers, who gathered audio samples from TalkBank’s AphasiaBank as part of the study, note silence is particularly common when someone with a language disorder called aphasia is speaking. One of the researchers, Allison Koenecke of Cornel University, posted examples like the one below in a thread about the study.The researchers found that hallucinations also included invented medical conditions or phrases you might expect from a YouTube video, such as “Thank you for watching!” (OpenAI reportedly used to transcribe over a million hours of YouTube videos to train GPT-4.) The study was presented in June at the Association for Computing Machinery FAccT conference in Brazil. It’s not clear if it has been peer-reviewed.OpenAI spokesperson Taya Christianson emailed a statement to The Verge:We take this issue seriously and are continually working to improve, including reducing hallucinations. For Whisper use on our API platform, our usage policies prohibit use in certain high-stakes decision-making contexts, and our model card for open-source use includes recommendations against use in high-risk domains. We thank researchers for sharing their findings.",
  "image": "https://cdn.vox-cdn.com/thumbor/Jjaz1rr-cOLqS4qS9J4umCjkyqE=/0x0:2040x1360/1200x628/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/24390468/STK149_AI_Chatbot_K_Radtke.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eA few months ago, my doctor showed off an AI transcription tool he used to record and summarize his patient meetings. In my case, the summary was fine, but researchers cited by \u003cem\u003eABC News\u003c/em\u003e have found that’s not always the case with OpenAI’s Whisper, which powers a tool many hospitals use — sometimes it just makes things up entirely.\u003c/p\u003e\u003cp\u003eWhisper is used by a company \u003ca href=\"https://www.nabla.com/\"\u003ecalled Nabla\u003c/a\u003e for a medical transcription tool that it estimates has transcribed 7 million medical conversations, according to \u003cem\u003eABC News\u003c/em\u003e. More than 30,000 clinicians and 40 health systems use it, the outlet writes. Nabla is reportedly aware that Whisper can hallucinate, and is “addressing the problem.”\u003c/p\u003e\u003cp\u003eA group of researchers from Cornell University, the University of Washington, and others \u003ca href=\"https://facctconference.org/static/papers24/facct24-111.pdf\"\u003efound in a study\u003c/a\u003e that Whisper hallucinated in about 1 percent of transcriptions, making up entire sentences with sometimes violent sentiments or nonsensical phrases during silences in recordings. The researchers, who gathered audio samples from TalkBank’s AphasiaBank as part of the study, note silence is particularly common when someone with a language disorder called aphasia is speaking. \u003c/p\u003e\u003cp\u003eOne of the researchers, Allison Koenecke of Cornel University, posted examples like the one below in a \u003ca href=\"https://x.com/allisonkoe/status/1797662873675800843\"\u003ethread about the study\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe researchers found that hallucinations also included invented medical conditions or phrases you might expect from a YouTube video, such as “Thank you for watching!” (OpenAI reportedly used to transcribe \u003ca href=\"https://www.theverge.com/2024/4/6/24122915/openai-youtube-transcripts-gpt-4-training-data-google\"\u003eover a million hours of YouTube\u003c/a\u003e videos to train GPT-4.) \u003c/p\u003e\u003cp\u003eThe study was \u003ca href=\"https://facctconference.org/2024/acceptedpapers\"\u003epresented in June\u003c/a\u003e at the Association for Computing Machinery FAccT conference in Brazil. It’s not clear if it has been peer-reviewed.\u003c/p\u003e\u003cp\u003eOpenAI spokesperson Taya Christianson emailed a statement to \u003cem\u003eThe Verge:\u003c/em\u003e\u003c/p\u003e\u003cdiv\u003e\u003cblockquote\u003e\u003cp\u003eWe take this issue seriously and are continually working to improve, including reducing hallucinations. For Whisper use on our API platform, our usage policies prohibit use in certain high-stakes decision-making contexts, and our model card for open-source use includes recommendations against use in high-risk domains. We thank researchers for sharing their findings.\u003c/p\u003e\u003c/blockquote\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2024-10-27T23:19:36.03Z",
  "modifiedTime": "2024-10-27T23:19:36.03Z"
}
