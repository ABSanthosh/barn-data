{
  "id": "18268483-0d9e-437c-a1b6-eef24e4b57d3",
  "title": "You can't color calibrate deep space photos",
  "link": "https://maurycyz.com/misc/cc/",
  "description": "Comments",
  "author": "",
  "published": "Wed, 23 Jul 2025 00:16:18 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "",
  "length": 5272,
  "excerpt": "Jul 21, 2025 (Photography)",
  "siteName": "",
  "favicon": "",
  "text": "Jul 21, 2025 (Photography) Color is the most disputed part of astrophotography. It seems like no two images have the same colors, and the internet is full of disputes over what colors are correct or natural. Human vision and perception are very complex. This description is hugely simplified as to fit in a blog post. If you want details, consult a textbook. Roughly speaking, our eyes have four types of photoreceptors. The most abundant, rods, provide a black-and-white view of the world: very detailed but with no color. The others are cone cells, each type responds to a particular part of the visible spectrum: Left: All that you will ever see.Right: Infinite abyss of the IR. The peaks of these wavelength ranges roughly correspond to the colors red, green, and blue. Because there’s a lot of overlap between the red and green cones, our brain subtracts some green from red, yielding this spectral response: Yes, this results in red having negative sensitivity @500 nm. The perceived color depends on the ratio between the signals from each type of cone, which corresponds to the ratio of red, green and blue light entering your eye. The pixels in a camera produce a signal based on the brightness of incoming light, producing a monochrome image. To make a color camera, we overlay a grid of alternating RGB filters over the pixel grid: Typical Bayer filter matrix The camera’s computer converts the brightness ratio between neighboring pixels into color. The resulting color accuracy depends on how close the camera’s filters are to the cones in the human eye. Here’s what the spectral response of a typical camera sensor look like: ohno The most striking difference is what happens from 800-1000 nm: To our eyes, these wavelengths are mostly invisible. However, the camera sees them quite well, and because the organic dyes used as filters don’t work in the infrared, they trigger all the color channels equally. After white balancing (dimming the green and blue channels) these wavelengths become a pastel pink color: A Mira variable, spectral type M. This red giant star has a relatively cold surface, and emits most of its light in the infrared causing it to show up as pink. Our eyes don’t see the IR at all, so the star appears a dim red. There’s no way to fix this problem in post. Sure, you could just change the pinks to dim reds, but that would also affect areas that are supposed to be pink. The camera simply doesn’t capture enough information to tell the difference between real pink and fake pink. In this case, the real solution is easy: Just add a filter that prevents the infrared light from reaching the sensor — we won’t always be so lucky. The trouble with plasma: Ionized hydrogen emits multiple wavelengths of light, but mostly red H-alpha at 565 nm , and a bit of blue H-beta at 486 nm. Our eyes aren’t very good at seeing the deep red H-alpha, so the dimmer blue H-beta is able to compete, resulting a pink color. However, many cameras are very sensitive to H-alpha, so hydrogen shows up as red: This nebula formed from interstellar gas, so it's dominated by hydrogen. Many other cameras, particularly those with aggressive UV-IR cut filters, underespond to H-a, resulting in dim and blueish nebula. Often people rip out those filters (astro-modification), but this usually results in the camera overresponding instead. Hydrogen wasn’t a fluke: Ionized Oxygen has a bright emission line at 500.7 nm, creating all the greens and blues you see in nebulae. The problem is that this line is right on the edge between green and blue. To my eyes, this wavelength looks like a greenish turquoise (sRGB #00FFBA), but my camera’s sensor sees it as cyan (sRGB #50E4FF): This nebula was formed by a dying red-giant star, so it's got lots of oxygen There’s no way to calibrate this out, because making blues greener would result in green stars, which would also be wrong… and because the light is monochromatic, adding filters won’t change the color, only the intensity. Just to be sure, let’s try applying my sensor’s color calibration matrix to the image: Color \"calibrated\" version Did that just… saturate everything? It made the oxygen bluer, hydrogen redder and stars pinker. A color matrix compensates for overlap between the sensor’s color filters, which cause colors to look washed out. This can be fixed by bumping up the saturation, but if the colors are wrong, this makes them worse. Space is space Once you leave the familiar world of broadband light and pigments, of light bulbs and color charts, the premise of color calibration falls apart. There’s simply no way to covert the colors seen by a camera to the colors that would be seen by the eye. Even something as simple as white balance is problematic. In space, nothing is lit by a uniform light source or with a uniform brightness, and most objects emit light of their own. Most photographers use “daylight” white balance, but the objects we photograph are well outside of the sun’s domain. In my images, I usually leave the colors as seen by my camera’s filters and set the white balance based on the average spiral galaxy. That way, at least the white point is somewhat objective: Comparing different white balance references. The sun is a G2(V) star.",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"content\"\u003e\n\n\n\n\u003cp\u003e\u003ctime datetime=\"2025-07-21\"\u003eJul 21, 2025\u003c/time\u003e\n\n\n\n(\u003ca href=\"http://maurycyz.com/tags/photography\"\u003ePhotography\u003c/a\u003e) \n\n\n\u003c/p\u003e\u003cp\u003eColor is the most disputed part of astrophotography.\nIt seems like no two images have the same colors, and the internet is full of disputes over what colors are correct or natural.\u003c/p\u003e\n\n\u003cp\u003e\nHuman vision and perception are very complex.\nThis description is hugely simplified as to fit in a blog post.\nIf you want details, consult a textbook.\n\u003c/p\u003e\n\u003cp\u003eRoughly speaking, our eyes have four types of photoreceptors.\nThe most abundant, rods, provide a black-and-white view of the world: very detailed but with no color.\nThe others are cone cells, each type responds to a particular part of the visible spectrum:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://maurycyz.com/misc/cc/cone.png\" alt=\"Spectral response curve of cone cells\"/\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003ccenter\u003eLeft: All that you will ever see.\u003cbr/\u003eRight: Infinite abyss of the IR.\u003c/center\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe peaks of these wavelength ranges roughly correspond to the colors red, green, and blue.\nBecause there’s a lot of overlap between the red and green cones, our brain subtracts some green from red, yielding this spectral response:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://maurycyz.com/misc/cc/cmf.png\" alt=\"Color matching function, featuring negatives.\"/\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003ccenter\u003eYes, this results in red having negative sensitivity @500 nm. \u003c/center\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe perceived color depends on the ratio between the signals from each type of cone, which corresponds to the ratio of red, green and blue light entering your eye.\u003c/p\u003e\n\n\u003cp\u003eThe pixels in a camera produce a signal based on the brightness of incoming light, producing a monochrome image.\nTo make a color camera, we overlay a grid of alternating RGB filters over the pixel grid:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://maurycyz.com/misc/cc/bayer.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003ccenter\u003eTypical Bayer filter matrix\u003c/center\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe camera’s computer converts the brightness ratio between neighboring pixels into color.\nThe resulting color accuracy depends on how close the camera’s filters are to the cones in the human eye.\u003c/p\u003e\n\u003cp\u003eHere’s what the spectral response of a typical camera sensor look like:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://maurycyz.com/misc/cc/imx533.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003ccenter\u003e ohno \u003c/center\u003e\u003c/blockquote\u003e\n\n\u003cp\u003eThe most striking difference is what happens from 800-1000 nm:\nTo our eyes, these wavelengths are mostly invisible.\nHowever, the camera sees them quite well, and because the organic dyes used as filters don’t work in the infrared, they trigger all the color channels equally.\nAfter white balancing (dimming the green and blue channels) these wavelengths become a pastel pink color:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://maurycyz.com/misc/cc/purple_star.jpg\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003ccenter\u003eA Mira variable, spectral type M.\u003c/center\u003e\u003c/blockquote\u003e\n\u003cp\u003eThis red giant star has a relatively cold surface, and emits most of its light in the infrared causing it to show up as pink.\nOur eyes don’t see the IR at all, so the star appears a dim red.\u003c/p\u003e\n\u003cp\u003eThere’s no way to fix this problem in post.\nSure, you could just change the pinks to dim reds, but that would also affect areas that are supposed to be pink.\nThe camera simply doesn’t capture enough information to tell the difference between real pink and fake pink.\u003c/p\u003e\n\u003cp\u003eIn this case, the real solution is easy: Just add a filter that prevents the infrared light from reaching the sensor\n— we won’t always be so lucky.\u003c/p\u003e\n\u003ch2 id=\"the-trouble-with-plasma\"\u003eThe trouble with plasma:\u003c/h2\u003e\n\u003cp\u003eIonized hydrogen emits multiple wavelengths of light, but mostly red H-alpha at 565 nm , and a bit of blue H-beta at 486 nm.\nOur eyes aren’t very good at seeing the deep red H-alpha, so the dimmer blue H-beta is able to compete, resulting a pink color.\u003c/p\u003e\n\u003cp\u003eHowever, many cameras are very sensitive to H-alpha, so hydrogen shows up as red:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://maurycyz.com/astro/m16/\"\u003e\u003cimg src=\"https://maurycyz.com/misc/cc/pillars.jpg\" alt=\"Pillars of creation, OSC\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003ccenter\u003eThis nebula formed from interstellar gas, so it\u0026#39;s dominated by hydrogen.\u003c/center\u003e\u003c/blockquote\u003e\n\u003cp\u003eMany other cameras, particularly those with aggressive UV-IR cut filters, underespond to H-a, resulting in dim and blueish nebula.\nOften people rip out those filters (astro-modification), but this usually results in the camera overresponding instead.\u003c/p\u003e\n\u003ch2 id=\"hydrogen-wasnt-a-fluke\"\u003eHydrogen wasn’t a fluke:\u003c/h2\u003e\n\u003cp\u003eIonized Oxygen has a bright emission line at 500.7 nm, creating all the greens and blues you see in nebulae.\u003c/p\u003e\n\u003cp\u003eThe problem is that this line is right on the edge between green and blue.\nTo my eyes, this wavelength looks like a greenish turquoise \u003cspan\u003e(sRGB #00FFBA)\u003c/span\u003e,\nbut my camera’s sensor sees it as cyan \u003cspan\u003e(sRGB #50E4FF)\u003c/span\u003e:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://maurycyz.com/astro/m27/\"\u003e\u003cimg src=\"https://maurycyz.com/misc/cc/m27.jpg\" alt=\"Dumbbell Nebula\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003ccenter\u003eThis nebula was formed by a dying red-giant star, so it\u0026#39;s got lots of oxygen\u003c/center\u003e\u003c/blockquote\u003e\n\u003cp\u003eThere’s no way to calibrate this out, because making blues greener would result in green stars, which would also be wrong…\nand because the light is monochromatic, adding filters won’t change the color, only the intensity.\u003c/p\u003e\n\u003cp\u003eJust to be sure, let’s try applying my sensor’s color calibration matrix to the image:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://maurycyz.com/astro/m27/\"\u003e\u003cimg src=\"https://maurycyz.com/misc/cc/m27_2.jpg\" alt=\"\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003ccenter\u003eColor \u0026#34;calibrated\u0026#34; version\u003c/center\u003e\u003c/blockquote\u003e\n\u003cp\u003eDid that just… saturate everything?\nIt made the oxygen bluer, hydrogen redder and stars pinker.\u003c/p\u003e\n\u003cp\u003eA color matrix compensates for overlap between the sensor’s color filters, which cause colors to look washed out.\nThis can be fixed by bumping up the saturation, but if the colors are wrong, this makes them worse.\u003c/p\u003e\n\u003ch2 id=\"space-is-space\"\u003eSpace is space\u003c/h2\u003e\n\u003cp\u003eOnce you leave the familiar world of broadband light and pigments, of light bulbs and color charts, the premise of color calibration falls apart.\nThere’s simply no way to covert the colors seen by a camera to the colors that would be seen by the eye.\u003c/p\u003e\n\u003cp\u003eEven something as simple as white balance is problematic.\nIn space, nothing is lit by a uniform light source or with a uniform brightness, and most objects emit light of their own.\nMost photographers use “daylight” white balance, but the objects we photograph are well outside of the sun’s domain.\u003c/p\u003e\n\u003cp\u003eIn my images, I usually leave the colors as seen by my camera’s filters and set the white balance based on the average spiral galaxy.\nThat way, at least the white point is somewhat objective:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://maurycyz.com/astro/m51/\"\u003e\u003cimg src=\"https://maurycyz.com/misc/cc/balance.jpg\" alt=\"\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003ccenter\u003eComparing different white balance references. The sun is a G2(V) star.\u003ccenter\u003e\u003c/center\u003e\u003c/center\u003e\u003c/blockquote\u003e\n\n\n        \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
