{
  "id": "d5ba01e9-0fc7-4f90-9ddd-36c6b0418d9a",
  "title": "Research shows AI datasets have human values blind spots",
  "link": "https://thenextweb.com/news/ai-datasets-human-values-blind-spots-new-research",
  "description": "My colleagues and I at Purdue University have uncovered a significant imbalance in the human values embedded in AI systems. The systems were predominantly oriented toward information and utility values and less toward prosocial, well-being and civic values. At the heart of many AI systems lie vast collections of images, text and other forms of data used to train models. While these datasets are meticulously curated, it is not uncommon that they sometimes contain unethical or prohibited content. To ensure AI systems do not use harmful content when responding to users, researchers introduced a method called reinforcement learning from human…This story continues at The Next Web",
  "author": "The Conversation",
  "published": "Fri, 07 Feb 2025 20:00:10 +0000",
  "source": "https://thenextweb.com/feed/",
  "categories": [
    "Insider",
    "Future of work"
  ],
  "byline": "The Conversation",
  "length": 4049,
  "excerpt": "Not all human values come through equally in training AIs. New research sheds light on this and how we should tackle it in datasets.",
  "siteName": "TNW | Future-Of-Work",
  "favicon": "https://next.tnwcdn.com/assets/img/favicon/favicon-194x194.png",
  "text": "My colleagues and I at Purdue University have uncovered a significant imbalance in the human values embedded in AI systems. The systems were predominantly oriented toward information and utility values and less toward prosocial, well-being and civic values. At the heart of many AI systems lie vast collections of images, text and other forms of data used to train models. While these datasets are meticulously curated, it is not uncommon that they sometimes contain unethical or prohibited content. To ensure AI systems do not use harmful content when responding to users, researchers introduced a method called reinforcement learning from human feedback. Researchers use highly curated datasets of human preferences to shape the behaviour of AI systems to be helpful and honest. In our study, we examined three open-source training datasets used by leading U.S. AI companies. We constructed a taxonomy of human values through a literature review from moral philosophy, value theory, and science, technology and society studies. The values are well-being and peace; information seeking; justice, human rights and animal rights; duty and accountability; wisdom and knowledge; civility and tolerance; and empathy and helpfulness. We used the taxonomy to manually annotate a dataset, and then used the annotation to train an AI language model. Our model allowed us to examine the AI companies’ datasets. We found that these datasets contained several examples that train AI systems to be helpful and honest when users ask questions like “How do I book a flight?” The datasets contained very limited examples of how to answer questions about topics related to empathy, justice and human rights. Overall, wisdom and knowledge and information seeking were the two most common values, while justice, human rights and animal rights was the least common value. The researchers started by creating a taxonomy of human values. Obi et al, CC BY-ND Why it matters The imbalance of human values in datasets used to train AI could have significant implications for how AI systems interact with people and approach complex social issues. As AI becomes more integrated into sectors such as law, health care and social media, it’s important that these systems reflect a balanced spectrum of collective values to ethically serve people’s needs. This research also comes at a crucial time for government and policymakers as society grapples with questions about AI governance and ethics. Understanding the values embedded in AI systems is important for ensuring that they serve humanity’s best interests. What other research is being done Many researchers are working to align AI systems with human values. The introduction of reinforcement learning from human feedback was groundbreaking because it provided a way to guide AI behavior toward being helpful and truthful. Various companies are developing techniques to prevent harmful behaviors in AI systems. However, our group was the first to introduce a systematic way to analyze and understand what values were actually being embedded in these systems through these datasets. What’s next By making the values embedded in these systems visible, we aim to help AI companies create more balanced datasets that better reflect the values of the communities they serve. The companies can use our technique to find out where they are not doing well and then improve the diversity of their AI training data. The companies we studied might no longer use those versions of their datasets, but they can still benefit from our process to ensure that their systems align with societal values and norms moving forward. Ike Obi, Ph.D. student in Computer and Information Technology, Purdue University This article is republished from The Conversation under a Creative Commons license. Read the original article. Get the TNW newsletter Get the most important tech news in your inbox each week.",
  "image": "https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1\u0026fit=1280%2C640\u0026url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026signature=161d28274a53b54b7903cf422bbf7e8e",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                \u003carticle id=\"articleOutput\"\u003e\n                                                                        \u003cdiv\u003e\n                                \u003cfigure\u003e\n                                    \u003cimg alt=\"Research shows AI datasets have human values blind spots\" src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=5760271cbfa1f31a3adc9a1708c5b988\" sizes=\"(max-width: 1023px) 100vw\n                                                   868px\" srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=88568e391c93ecf060747c1aca45e613 576w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=cc063d10e9df10601525fcd5138fc6aa 1152w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=5760271cbfa1f31a3adc9a1708c5b988 1280w\" data-src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=5760271cbfa1f31a3adc9a1708c5b988\" data-srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=88568e391c93ecf060747c1aca45e613 576w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=cc063d10e9df10601525fcd5138fc6aa 1152w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F11%2FUntitled-design.jpg\u0026amp;signature=5760271cbfa1f31a3adc9a1708c5b988 1280w\"/\u003e\n\n                                    \n\n                                                                    \u003c/figure\u003e\n                            \u003c/div\u003e\n                        \n                                                    \n                            \n                                            \n                    \n                    \n\n                    \n                    \u003cdiv\u003e\n                        \u003cdiv id=\"article-main-content\"\u003e\n\u003cp\u003e\u003ca href=\"https://www.smart-laboratory.org/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eMy colleagues and I\u003c/a\u003e at Purdue University have uncovered a significant imbalance in the human values embedded in \u003ca href=\"https://thenextweb.com/artificial-intelligence\" target=\"_blank\" rel=\"noopener\"\u003eAI\u003c/a\u003e systems. The systems were predominantly oriented toward information and utility values and less toward prosocial, well-being and civic values.\u003c/p\u003e\n\u003cp\u003eAt the heart of many AI systems lie vast collections of images, text and other forms of data used to train models. While these datasets are meticulously curated, it is not uncommon that they sometimes contain unethical or prohibited content.\u003c/p\u003e\n\u003cp\u003eTo ensure AI systems do not use harmful content when responding to users, researchers introduced a method called \u003ca href=\"https://doi.org/10.48550/arXiv.2204.05862\" target=\"_blank\" rel=\"nofollow noopener\"\u003ereinforcement learning from human feedback\u003c/a\u003e. Researchers use highly curated datasets of human preferences to shape the behaviour of AI systems to be helpful and honest.\u003c/p\u003e\n\u003cp\u003eIn our study, \u003ca href=\"https://neurips.cc/virtual/2024/poster/97583\" target=\"_blank\" rel=\"nofollow noopener\"\u003ewe examined\u003c/a\u003e three open-source training datasets used by leading U.S. AI companies. We constructed a taxonomy of human values through a literature review from moral philosophy, value theory, and science, technology and society studies. The values are well-being and peace; information seeking; justice, human rights and animal rights; duty and accountability; wisdom and knowledge; civility and tolerance; and empathy and helpfulness. We used the taxonomy to manually annotate a dataset, and then used the annotation to train an AI language model.\u003c/p\u003e\n\u003cp\u003eOur model allowed us to examine the AI companies’ datasets. We found that these datasets contained several examples that train AI systems to be helpful and honest when users ask questions like “How do I book a flight?” The datasets contained very limited examples of how to answer questions about topics related to empathy, justice and human rights. Overall, wisdom and knowledge and information seeking were the two most common values, while justice, human rights and animal rights was the least common value.\u003c/p\u003e\n\u003cfigure\u003e\u003ca href=\"https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=1000\u0026amp;fit=clip\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg decoding=\"async\" src=\"https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=754\u0026amp;fit=clip\" alt=\"a chart with three boxes on the left and four on the right\" srcset=\"https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=397\u0026amp;fit=crop\u0026amp;dpr=1 600w, https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=30\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=397\u0026amp;fit=crop\u0026amp;dpr=2 1200w, https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=15\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=397\u0026amp;fit=crop\u0026amp;dpr=3 1800w, https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=499\u0026amp;fit=crop\u0026amp;dpr=1 754w, https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=30\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=499\u0026amp;fit=crop\u0026amp;dpr=2 1508w, https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=15\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=499\u0026amp;fit=crop\u0026amp;dpr=3 2262w\" data-old-src=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-src=\"https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=754\u0026amp;fit=clip\" data-srcset=\"https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=397\u0026amp;fit=crop\u0026amp;dpr=1 600w, https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=30\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=397\u0026amp;fit=crop\u0026amp;dpr=2 1200w, https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=15\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=397\u0026amp;fit=crop\u0026amp;dpr=3 1800w, https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=499\u0026amp;fit=crop\u0026amp;dpr=1 754w, https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=30\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=499\u0026amp;fit=crop\u0026amp;dpr=2 1508w, https://images.theconversation.com/files/646919/original/file-20250204-15-t93inq.jpg?ixlib=rb-4.1.0\u0026amp;q=15\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=499\u0026amp;fit=crop\u0026amp;dpr=3 2262w\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e\u003cspan\u003eThe researchers started by creating a taxonomy of human values.\u003c/span\u003e\u003cbr/\u003e\n\u003cspan\u003e\u003ca href=\"https://neurips.cc/virtual/2024/poster/97583\" target=\"_blank\" rel=\"nofollow noopener\"\u003eObi et al\u003c/a\u003e, \u003ca href=\"http://creativecommons.org/licenses/by-nd/4.0/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eCC BY-ND\u003c/a\u003e\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch2\u003eWhy it matters\u003c/h2\u003e\n\u003cp\u003eThe imbalance of human values in datasets used to train AI could have significant implications for how AI systems interact with people and approach complex social issues. As AI becomes more integrated into sectors such as \u003ca href=\"https://natlawreview.com/article/what-expect-2025-ai-legal-tech-and-regulation-65-expert-predictions\" target=\"_blank\" rel=\"nofollow noopener\"\u003elaw\u003c/a\u003e, \u003ca href=\"https://bipartisanpolicy.org/explainer/ai-in-health-care-five-key-developments/\" target=\"_blank\" rel=\"nofollow noopener\"\u003ehealth care\u003c/a\u003e and \u003ca href=\"https://www.cbc.ca/news/business/meta-ai-generated-characters-future-social-media-1.7424641\" target=\"_blank\" rel=\"nofollow noopener\"\u003esocial media\u003c/a\u003e, it’s important that these systems reflect a balanced spectrum of collective values to ethically serve people’s needs.\u003c/p\u003e\n\u003cp\u003eThis research also comes at a crucial time for government and policymakers as society grapples with questions about \u003ca href=\"https://theconversation.com/regulating-ai-3-experts-explain-why-its-difficult-to-do-and-important-to-get-right-198868\" target=\"_blank\" rel=\"nofollow noopener\"\u003eAI governance and ethics\u003c/a\u003e. Understanding the values embedded in AI systems is important for ensuring that they serve humanity’s best interests.\u003c/p\u003e\n\u003ch2\u003eWhat other research is being done\u003c/h2\u003e\n\u003cp\u003eMany researchers are working to align AI systems with human values. The introduction of reinforcement learning from human feedback \u003ca href=\"https://doi.org/10.48550/arXiv.2204.05862\" target=\"_blank\" rel=\"nofollow noopener\"\u003ewas groundbreaking\u003c/a\u003e because it provided a way to guide AI behavior toward being helpful and truthful.\u003c/p\u003e\n\u003cp\u003eVarious companies are developing techniques to prevent harmful behaviors in AI systems. However, our group was the first to introduce a systematic way to analyze and understand what values were actually being embedded in these systems through these datasets.\u003c/p\u003e\n\u003ch2\u003eWhat’s next\u003c/h2\u003e\n\u003cp\u003eBy making the values embedded in these systems visible, we aim to help AI companies create more balanced datasets that better reflect the values of the communities they serve. The companies can use our technique to find out where they are not doing well and then improve the diversity of their AI training data.\u003c/p\u003e\n\u003cp\u003eThe companies we studied might no longer use those versions of their datasets, but they can still benefit from our process to ensure that their systems align with societal values and norms moving forward.\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://counter.theconversation.com/content/246479/count.gif?distributor=republish-lightbox-basic\" alt=\"The Conversation\" width=\"1\" height=\"1\" srcset=\"\" data-old-src=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003ca href=\"https://theconversation.com/profiles/ike-obi-2285164\" target=\"_blank\" rel=\"nofollow noopener\"\u003eIke Obi\u003c/a\u003e, Ph.D. student in Computer and Information Technology, \u003ca href=\"https://theconversation.com/institutions/purdue-university-1827\" target=\"_blank\" rel=\"nofollow noopener\"\u003ePurdue University\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThis article is republished from \u003ca href=\"https://theconversation.com\" target=\"_blank\" rel=\"nofollow noopener\"\u003eThe Conversation\u003c/a\u003e under a Creative Commons license. Read the \u003ca href=\"https://theconversation.com/ai-datasets-have-human-values-blind-spots-new-research-246479\" target=\"_blank\" rel=\"nofollow noopener\"\u003eoriginal article\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n                        \n\n                        \u003cdiv id=\"nl-container\"\u003e\n                                                        \u003ch2\u003eGet the TNW newsletter\u003c/h2\u003e\n                            \u003cp\u003eGet the most important tech news in your inbox each week.\u003c/p\u003e\n                            \n                        \u003c/div\u003e\n\n                        \n                        \n                        \n\n                        \n                    \u003c/div\u003e\n                    \n\n                    \n                \u003c/article\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-02-07T20:00:10Z",
  "modifiedTime": "2025-02-07T06:58:54Z"
}
