{
  "id": "1394fe3f-cb38-4575-a8e1-492a3a5bb50a",
  "title": "Can OpenAI’s Strawberry program deceive humans?",
  "link": "https://thenextweb.com/news/can-openais-strawberry-program-deceive-humans",
  "description": "OpenAI, the company that made ChatGPT, has launched a new artificial intelligence (AI) system called Strawberry. It is designed not just to provide quick responses to questions, like ChatGPT, but to think or “reason”. This raises several major concerns. If Strawberry really is capable of some form of reasoning, could this AI system cheat and deceive humans? OpenAI can program the AI in ways that mitigate its ability to manipulate humans. But the company’s own evaluations rate it as a “medium risk” for its ability to assist experts in the “operational planning of reproducing a known biological threat” – in…This story continues at The Next Web",
  "author": "The Conversation",
  "published": "Thu, 31 Oct 2024 09:00:24 +0000",
  "source": "https://thenextweb.com/feed/",
  "categories": [
    "Insider",
    "Deep tech"
  ],
  "byline": "The Conversation",
  "length": 5614,
  "excerpt": "If OpenAI's Strawberry really is capable of some form of reasoning, could this AI system cheat and deceive humans?",
  "siteName": "TNW | Deep-Tech",
  "favicon": "https://next.tnwcdn.com/assets/img/favicon/favicon-194x194.png",
  "text": "OpenAI, the company that made ChatGPT, has launched a new artificial intelligence (AI) system called Strawberry. It is designed not just to provide quick responses to questions, like ChatGPT, but to think or “reason”. This raises several major concerns. If Strawberry really is capable of some form of reasoning, could this AI system cheat and deceive humans? OpenAI can program the AI in ways that mitigate its ability to manipulate humans. But the company’s own evaluations rate it as a “medium risk” for its ability to assist experts in the “operational planning of reproducing a known biological threat” – in other words, a biological weapon. It was also rated as a medium risk for its ability to persuade humans to change their thinking. It remains to be seen how such a system might be used by those with bad intentions, such as con artists or hackers. Nevertheless, OpenAI’s evaluation states that medium-risk systems can be released for wider use – a position I believe is misguided. Strawberry is not one AI “model”, or program, but several – known collectively as o1. These models are intended to answer complex questions and solve intricate maths problems. They are also capable of writing computer code – to help you make your own website or app, for example. An apparent ability to reason might come as a surprise to some, since this is generally considered a precursor to judgment and decision making – something that has often seemed a distant goal for AI. So, on the surface at least, it would seem to move artificial intelligence a step closer to human-like intelligence. When things look too good to be true, there’s often a catch. Well, this set of new AI models is designed to maximise their goals. What does this mean in practice? To achieve its desired objective, the path or the strategy chosen by AI may not always necessarily be fair, or align with human values. True intentions For example, if you were to play chess against Strawberry, in theory, could its reasoning allow it to hack the scoring system rather than figure out the best strategies for winning the game? The AI might also be able to lie to humans about its true intentions and capabilities, which would pose a serious safety concern if it were to be deployed widely. For example, if the AI knew it was infected with malware, could it “choose” to conceal this fact in the knowledge that a human operator might opt to disable the whole system if they knew? Strawberry goes a step beyond the capabilities of AI chatbots. Robert Way / Shutterstock These would be classic examples of unethical AI behaviour, where cheating or deceiving is acceptable if it leads to a desired goal. It would also be quicker for the AI, as it wouldn’t have to waste any time figuring out the next best move. It may not necessarily be morally correct, however. This leads to a rather interesting yet worrying discussion. What level of reasoning is Strawberry capable of and what could its unintended consequences be? A powerful AI system that’s capable of cheating humans could pose serious ethical, legal and financial risks to us. Such risks become grave in critical situations, such as designing weapons of mass destruction. OpenAI rates its own Strawberry models as “medium risk” for their potential to assist scientists in developing chemical, biological, radiological and nuclear weapons. OpenAI says: “Our evaluations found that o1-preview and o1-mini can help experts with the operational planning of reproducing a known biological threat.” But it goes on to say that experts already have significant expertise in these areas, so the risk would be limited in practice. It adds: “The models do not enable non-experts to create biological threats, because creating such a threat requires hands-on laboratory skills that the models cannot replace.” Powers of persuasion OpenAI’s evaluation of Strawberry also investigated the risk that it could persuade humans to change their beliefs. The new o1 models were found to be more persuasive and more manipulative than ChatGPT. OpenAI also tested a mitigation system that was able to reduce the manipulative capabilities of the AI system. Overall, Strawberry was labelled a medium risk for “persuasion” in Open AI’s tests. Strawberry was rated low risk for its ability to operate autonomously and on cybersecurity. Open AI’s policy states that “medium risk” models can be released for wide use. In my view, this underestimates the threat. The deployment of such models could be catastrophic, especially if bad actors manipulate the technology for their own pursuits. This calls for strong checks and balances that will only be possible through AI regulation and legal frameworks, such as penalising incorrect risk assessments and the misuse of AI. The UK government stressed the need for “safety, security and robustness” in their 2023 AI white paper, but that’s not nearly enough. There is an urgent need to prioritise human safety and devise rigid scrutiny protocols for AI models such as Strawberry. Shweta Singh, Assistant Professor, Information Systems and Management, Warwick Business School, University of Warwick This article is republished from The Conversation under a Creative Commons license. Read the original article. Get the TNW newsletter Get the most important tech news in your inbox each week. Also tagged with",
  "image": "https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1\u0026fit=1280%2C640\u0026url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-1-3.jpg\u0026signature=dd74a3234d0370054aab3a50c2ad5578",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                \u003carticle id=\"articleOutput\"\u003e\n                                                                        \u003cdiv\u003e\n                                \u003cfigure\u003e\n                                    \u003cimg alt=\"Can OpenAI’s Strawberry program deceive humans?\" src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-1-3.jpg\u0026amp;signature=e728f0568021232fe5c70df570ad91b6\" sizes=\"(max-width: 1023px) 100vw\n                                                   868px\" srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-1-3.jpg\u0026amp;signature=886cb81d813c85a82dd492739819966b 576w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-1-3.jpg\u0026amp;signature=a927b9b7dd066c99bfd6d7d7aa711527 1152w,\n                                                    https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-1-3.jpg\u0026amp;signature=e728f0568021232fe5c70df570ad91b6 1280w\" data-src=\"https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-1-3.jpg\u0026amp;signature=e728f0568021232fe5c70df570ad91b6\" data-srcset=\"https://img-cdn.tnwcdn.com/image?fit=576%2C324\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-1-3.jpg\u0026amp;signature=886cb81d813c85a82dd492739819966b 576w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1152%2C648\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-1-3.jpg\u0026amp;signature=a927b9b7dd066c99bfd6d7d7aa711527 1152w,\n                                                     https://img-cdn.tnwcdn.com/image?fit=1280%2C720\u0026amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2024%2F10%2FUntitled-design-1-3.jpg\u0026amp;signature=e728f0568021232fe5c70df570ad91b6 1280w\"/\u003e\n\n                                    \n\n                                                                    \u003c/figure\u003e\n                            \u003c/div\u003e\n                        \n                                                    \n                            \n                                            \n                    \n                    \n\n                    \n                    \u003cdiv\u003e\n                        \u003cdiv id=\"article-main-content\"\u003e\n\u003cp\u003eOpenAI, the company that made ChatGPT, has launched a new \u003ca href=\"https://thenextweb.com/artificial-intelligence\" target=\"_blank\" rel=\"noopener\"\u003eartificial intelligence\u003c/a\u003e (AI) system called \u003ca href=\"https://openai.com/index/learning-to-reason-with-llms/\" target=\"_blank\" rel=\"nofollow noopener\"\u003eStrawberry\u003c/a\u003e. It is designed not just to provide quick responses to questions, like ChatGPT, but to think or “reason”.\u003c/p\u003e\n\u003cp\u003eThis raises several major concerns. If Strawberry really is capable of some form of reasoning, could this AI system cheat and deceive humans?\u003c/p\u003e\n\u003cp\u003eOpenAI can program the AI in ways that mitigate its ability to manipulate humans. But \u003ca href=\"https://cdn.openai.com/o1-system-card.pdf\" target=\"_blank\" rel=\"nofollow noopener\"\u003ethe company’s own evaluations\u003c/a\u003e rate it as a “medium risk” for its ability to assist experts in the “operational planning of reproducing a known biological threat” – in other words, a biological weapon. It was also rated as a medium risk for its ability to persuade humans to change their thinking.\u003c/p\u003e\n\u003cp\u003eIt remains to be seen how such a system might be used by those with bad intentions, such as con artists or hackers. Nevertheless, OpenAI’s evaluation states that medium-risk systems can be released for wider use – a position I believe is misguided.\u003c/p\u003e\n\u003cp\u003eStrawberry is not one AI “model”, or program, but several – known collectively as o1. These models \u003ca href=\"https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt\" target=\"_blank\" rel=\"nofollow noopener\"\u003eare intended to\u003c/a\u003e answer complex questions and solve intricate maths problems. They are also capable of writing computer code – to help you make your own website or app, for example.\u003c/p\u003e\n\u003cp\u003eAn apparent ability to reason might come as a surprise to some, since this is generally considered a precursor to judgment and decision making – something that has often seemed a distant goal for AI. So, on the surface at least, it would seem to move artificial intelligence a step closer to human-like intelligence.\u003c/p\u003e\n\u003cp\u003eWhen things look too good to be true, there’s often a catch. Well, this set of new AI models is designed to maximise their goals. What does this mean in practice? To achieve its desired objective, the path or the strategy chosen by AI may \u003ca href=\"https://www.vox.com/future-perfect/371827/openai-chatgpt-artificial-intelligence-ai-risk-strawberry\" target=\"_blank\" rel=\"nofollow noopener\"\u003enot always necessarily be fair\u003c/a\u003e, or align with human values.\u003c/p\u003e\n\u003ch2\u003eTrue intentions\u003c/h2\u003e\n\u003cp\u003eFor example, if you were to play chess against Strawberry, in theory, could its reasoning allow it to \u003ca href=\"https://www.thestack.technology/openais-unripe-strawberry-model-hacked-its-testing-infrastructure/\" target=\"_blank\" rel=\"nofollow noopener\"\u003ehack the scoring system\u003c/a\u003e rather than figure out the best strategies for winning the game?\u003c/p\u003e\n\u003cp\u003eThe AI might also be able to lie to humans about its true intentions and capabilities, which would pose a serious safety concern if it were to be deployed widely. For example, if the AI knew it was infected with malware, could it “choose” to \u003ca href=\"https://www.vox.com/future-perfect/371827/openai-chatgpt-artificial-intelligence-ai-risk-strawberry\" target=\"_blank\" rel=\"nofollow noopener\"\u003econceal this fact\u003c/a\u003e in the knowledge that a human operator might opt to disable the whole system if they knew?\u003c/p\u003e\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://images.theconversation.com/files/621408/original/file-20240924-18-2b8gp0.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=754\u0026amp;fit=clip\" sizes=\"(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px\" srcset=\"https://images.theconversation.com/files/621408/original/file-20240924-18-2b8gp0.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=400\u0026amp;fit=crop\u0026amp;dpr=1 600w, https://images.theconversation.com/files/621408/original/file-20240924-18-2b8gp0.jpg?ixlib=rb-4.1.0\u0026amp;q=30\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=400\u0026amp;fit=crop\u0026amp;dpr=2 1200w, https://images.theconversation.com/files/621408/original/file-20240924-18-2b8gp0.jpg?ixlib=rb-4.1.0\u0026amp;q=15\u0026amp;auto=format\u0026amp;w=600\u0026amp;h=400\u0026amp;fit=crop\u0026amp;dpr=3 1800w, https://images.theconversation.com/files/621408/original/file-20240924-18-2b8gp0.jpg?ixlib=rb-4.1.0\u0026amp;q=45\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=503\u0026amp;fit=crop\u0026amp;dpr=1 754w, https://images.theconversation.com/files/621408/original/file-20240924-18-2b8gp0.jpg?ixlib=rb-4.1.0\u0026amp;q=30\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=503\u0026amp;fit=crop\u0026amp;dpr=2 1508w, https://images.theconversation.com/files/621408/original/file-20240924-18-2b8gp0.jpg?ixlib=rb-4.1.0\u0026amp;q=15\u0026amp;auto=format\u0026amp;w=754\u0026amp;h=503\u0026amp;fit=crop\u0026amp;dpr=3 2262w\" alt=\"AI chatbot icons\"/\u003e\u003cfigcaption\u003e\u003cspan\u003eStrawberry goes a step beyond the capabilities of AI chatbots.\u003c/span\u003e\u003cbr/\u003e\n\u003cspan\u003e\u003ca href=\"https://www.shutterstock.com/image-photo/shanghaichinafeb-2024-google-gemini-openai-chatgpt-2426619081\" target=\"_blank\" rel=\"nofollow noopener\"\u003eRobert Way / Shutterstock\u003c/a\u003e\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eThese would be classic examples of unethical AI behaviour, where cheating or deceiving is acceptable if it leads to a desired goal. It would also be quicker for the AI, as it wouldn’t have to waste any time figuring out the next best move. It may not necessarily be morally correct, however.\u003c/p\u003e\n\u003cp\u003eThis leads to a rather interesting yet worrying discussion. What level of reasoning is Strawberry capable of and what could its unintended consequences be? A powerful AI system that’s capable of cheating humans could pose serious ethical, legal and financial risks to us.\u003c/p\u003e\n\u003cp\u003eSuch risks become grave in critical situations, such as designing weapons of mass destruction. OpenAI rates its own Strawberry models as “medium risk” for their potential to assist scientists in developing \u003ca href=\"https://www.nato.int/cps/en/natohq/official_texts_197768.htm\" target=\"_blank\" rel=\"nofollow noopener\"\u003echemical, biological, radiological and nuclear weapons\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOpenAI \u003ca href=\"https://cdn.openai.com/o1-system-card.pdf\" target=\"_blank\" rel=\"nofollow noopener\"\u003esays\u003c/a\u003e: “Our evaluations found that o1-preview and o1-mini can help experts with the operational planning of reproducing a known biological threat.” But it goes on to say that experts already have significant expertise in these areas, so the risk would be limited in practice. It adds: “The models do not enable non-experts to create biological threats, because creating such a threat requires hands-on laboratory skills that the models cannot replace.”\u003c/p\u003e\n\u003ch2\u003ePowers of persuasion\u003c/h2\u003e\n\u003cp\u003eOpenAI’s evaluation of Strawberry also investigated the risk that it could persuade humans to change their beliefs. The new o1 models were found to be more persuasive and more manipulative than ChatGPT.\u003c/p\u003e\n\u003cp\u003eOpenAI also tested a mitigation system that was able to reduce the manipulative capabilities of the AI system. Overall, Strawberry was labelled a \u003ca href=\"https://cdn.openai.com/o1-system-card.pdf\" target=\"_blank\" rel=\"nofollow noopener\"\u003emedium risk for “persuasion”\u003c/a\u003e in Open AI’s tests.\u003c/p\u003e\n\u003cp\u003eStrawberry was rated low risk for its ability to operate autonomously and on cybersecurity.\u003c/p\u003e\n\u003cp\u003eOpen AI’s policy states that “medium risk” models can be released for wide use. In my view, this underestimates the threat. The deployment of such models could be catastrophic, especially if bad actors manipulate the technology for their own pursuits.\u003c/p\u003e\n\u003cp\u003eThis calls for strong checks and balances that will only be possible through AI regulation and legal frameworks, such as penalising incorrect risk assessments and the misuse of AI.\u003c/p\u003e\n\u003cp\u003eThe UK government stressed the need for “safety, security and robustness” in their 2023 AI white paper, but that’s not nearly enough. There is an urgent need to prioritise human safety and devise rigid scrutiny protocols for AI models such as Strawberry.\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://counter.theconversation.com/content/239748/count.gif?distributor=republish-lightbox-basic\" alt=\"The Conversation\" width=\"1\" height=\"1\" srcset=\"\" data-old-src=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003ca href=\"https://theconversation.com/profiles/shweta-singh-1289019\" target=\"_blank\" rel=\"nofollow noopener\"\u003eShweta Singh\u003c/a\u003e, Assistant Professor, Information Systems and Management, \u003ca href=\"https://theconversation.com/institutions/warwick-business-school-university-of-warwick-2650\" target=\"_blank\" rel=\"nofollow noopener\"\u003eWarwick Business School, University of Warwick\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThis article is republished from \u003ca href=\"https://theconversation.com\" target=\"_blank\" rel=\"nofollow noopener\"\u003eThe Conversation\u003c/a\u003e under a Creative Commons license. Read the \u003ca href=\"https://theconversation.com/openais-strawberry-program-is-reportedly-capable-of-reasoning-it-might-be-able-to-deceive-humans-239748\" target=\"_blank\" rel=\"nofollow noopener\"\u003eoriginal article\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n                        \n\n                        \u003cdiv id=\"nl-container\"\u003e\n                                                        \u003ch2\u003eGet the TNW newsletter\u003c/h2\u003e\n                            \u003cp\u003eGet the most important tech news in your inbox each week.\u003c/p\u003e\n                            \n                        \u003c/div\u003e\n\n                        \n                                                    \u003ch2\u003eAlso tagged with\u003c/h2\u003e\n\n                            \u003cbr/\u003e\n\n                            \n                        \n                        \n\n                        \n                    \u003c/div\u003e\n                    \n\n                    \n                \u003c/article\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-10-31T09:00:24Z",
  "modifiedTime": "2024-10-22T14:12:52Z"
}
