{
  "id": "d42a41f2-9b72-4a79-ad80-c2a9117f3e4a",
  "title": "Is China pulling ahead in AI video synthesis? We put Minimax to the test.",
  "link": "https://arstechnica.com/ai/2024/10/is-china-pulling-ahead-in-ai-video-synthesis-we-put-minimax-to-the-test/",
  "description": "With China's AI video generators pushing memes into weird territory, it was time to test one out.",
  "author": "Benj Edwards",
  "published": "Wed, 09 Oct 2024 21:33:34 +0000",
  "source": "http://feeds.arstechnica.com/arstechnica/index",
  "categories": [
    "AI",
    "Biz \u0026 IT",
    "Ai video generator",
    "china",
    "Chinese AI",
    "deepfake",
    "deepfakes",
    "image synthesis",
    "KLING",
    "machine learning",
    "Minimax",
    "Minimax video-01",
    "princess mononoke",
    "video synthesis",
    "will smith"
  ],
  "byline": "Benj Edwards",
  "length": 7577,
  "excerpt": "With China‚Äôs AI video generators pushing memes into weird territory, it was time to test one out.",
  "siteName": "Ars Technica",
  "favicon": "https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-300x300.png",
  "text": "Skip to content With China's AI video generators pushing memes into weird territory, it was time to test one out. A still shot from an AI-generated Minimax video-01 video with the prompt: \"A highly-intelligent person reading 'Ars Technica' on their computer when the screen explodes\" Credit: Minimax If 2022 was the year AI image generators went mainstream, 2024 has arguably been the year that AI video synthesis models exploded in capability. These models, while not yet perfect, can generate new videos from text descriptions called prompts, still images, or existing videos. After OpenAI made waves with Sora in February, two major AI models emerged from China: Kuaishou Technology's Kling and Minimax's video-01. Both Chinese models have already powered numerous viral AI-generated video projects, accelerating meme culture in weird new ways, including a recent shot-for-shot translation of the Princess Mononoke trailer using Kling that inspired death threats and a series of videos created with Minimax's platform. The videos show a synthesized version of TV chef Gordon Ramsay doing ridiculous things. After 22 million views and thousands of death threats, I felt like I needed to take this post down for my own mental health. This trailer was an EXPERIMENT to show my 300 friends on X how far we've coming in 16 months.I'm putting it back up to keep the conversation going. üßµ pic.twitter.com/tFpRPm9BMv‚Äî PJ Ace (@PJaccetturo) October 8, 2024 Kling first emerged in June, and it can generate two minutes of 1080p HD video at 30 frames per second with a level of detail and coherency that some think surpasses Sora. It's currently only available to people with a Chinese telephone number, and we have not yet used it ourselves. Around September 1, Minimax debuted the aforementioned video-01 as part of its Hailuo AI platform. That site lets anyone generate videos based on a prompt, and initial results seemed similar to Kling, so we decided to run some of our Runway Gen-3 prompts through it to see what happens. Putting Minimax to the test We generated each of the six-second-long 720p videos seen below using Minimax's free Hailuo AI platform. Each video generation took up to five to 10 minutes to complete, likely due to being in a queue with other free video users. (At one point, the whole thing froze up on us for a few days, so we didn't get a chance to generate a flaming cheeseburger.) In the spirit of not cherry-picking any results, everything you see was the first generation we received for the prompt listed above it. \"A highly intelligent person reading 'Ars Technica' on their computer when the screen explodes\" \"A cat in a car drinking a can of beer, beer commercial\" \"Will Smith eating spaghetti\" \"Robotic humanoid animals with vaudeville costumes roam the streets collecting protection money in tokens\" \"A basketball player in a haunted passenger train car with a basketball court, and he is playing against a team of ghosts\" \"A herd of one million cats running on a hillside, aerial view\" \"Video game footage of a dynamic 1990s third-person 3D platform game starring an anthropomorphic shark boy\" \"A muscular barbarian breaking a CRT television set with a weapon, cinematic, 8K, studio lighting\" Limitations of video synthesis models Overall, the Minimax video-01 results seen above feel fairly similar to Gen-3's outputs, with some differences, like the lack of a celebrity filter on Will Smith (who sadly did not actually eat the spaghetti in our tests), and the more realistic cat hands and licking motion. Some results were far worse, like the 1 million cats and the Ars Technica reader. As we explained in our hands-on test for Runway's Gen-3 Alpha, text-to-video models typically excel at combining concepts present in their training data (existing video samples used to create the model), allowing for creative mashups of existing themes and styles. However, these AI models often struggle with generalization, meaning they have difficulty applying learned information to entirely novel scenarios not represented in their training data. This limitation can lead to unexpected or unintended results when users request scenarios that deviate too far from the model's training examples. While we saw a very comical result for the cat drinking beer in the Gen-3 test, Minimax rendered a more realistic-looking result, and that could come down to better parsing of the prompt, different training data, more compute in training the model, or a different model architecture. Ultimately, there's still a lot of trial and error in generating a coherent result. It's worth noting that while China's models seem to match US video synthesis models from earlier this year, American tech companies aren't standing still. Google showed off Veo in May with some very impressive-looking demos. And last week, we reported on Meta's Movie Gen model, which appears (without using Meta's model ourselves) to potentially be a step ahead of Minimax and Kling. But China's servers are doubtlessly cranking away at training new AI video models as we speak, so this deepfake arms race probably won't slow down any time soon. Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a widely-cited tech historian. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC. 1. Two never-before-seen tools, from same group, infect air-gapped devices 2. Bankrupt Fisker says it can‚Äôt migrate its EVs to a new owner‚Äôs server 3. Sunderfolk is a couch co-op tactical RPG you play with a phone. No, really. 4. Drug makers can‚Äôt make knockoff weight-loss drugs anymore‚Äîand they‚Äôre mad 5. Man learns he‚Äôs being dumped via ‚Äúdystopian‚Äù AI summary of texts",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2024/10/still_shot.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"app\"\u003e\n    \u003cp\u003e\u003ca href=\"#main\"\u003e\n  Skip to content\n\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cmain id=\"main\"\u003e\n            \u003carticle data-id=\"2055357\"\u003e\n  \n  \u003cheader\u003e\n  \u003cdiv\u003e\n    \u003cdiv\u003e\n      \n\n      \n\n      \u003cp\u003e\n        With China\u0026#39;s AI video generators pushing memes into weird territory, it was time to test one out.\n      \u003c/p\u003e\n\n      \n    \u003c/div\u003e\n\n          \u003cdiv\u003e\n        \u003cp\u003e\u003cimg width=\"1000\" height=\"720\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/still_shot-1000x720.jpg\" alt=\"A still shot from an AI-generated Minimax video-01 video with the prompt: \u0026#34;A highly-intelligent person reading \u0026#39;Ars Technica\u0026#39; on their computer when the screen explodes\u0026#34;\" loading=\"eager\" decoding=\"async\" fetchpriority=\"high\"/\u003e\n        \u003c/p\u003e\n        \u003cdiv\u003e\n    \n    \u003cp\u003e\n      A still shot from an AI-generated Minimax video-01 video with the prompt: \u0026#34;A highly-intelligent person reading \u0026#39;Ars Technica\u0026#39; on their computer when the screen explodes\u0026#34;\n\n              \u003cspan\u003e\n          Credit:\n\n          \n          Minimax\n\n                  \u003c/span\u003e\n          \u003c/p\u003e\n  \u003c/div\u003e\n      \u003c/div\u003e\n      \u003c/div\u003e\n\u003c/header\u003e\n\n  \n\n  \n      \n    \n    \u003cdiv\u003e\n                      \n                      \n          \u003cp\u003eIf 2022 was the year AI image generators \u003ca href=\"https://arstechnica.com/information-technology/2022/12/please-slow-down-the-7-biggest-ai-stories-of-2022/\"\u003ewent mainstream\u003c/a\u003e, 2024 has arguably been the year that AI \u003ca href=\"https://arstechnica.com/tag/video-synthesis/\"\u003evideo synthesis models\u003c/a\u003e exploded in capability. These models, while not yet perfect, can generate new videos from text descriptions called prompts, still images, or existing videos. After OpenAI made waves with \u003ca href=\"https://arstechnica.com/information-technology/2024/02/openai-collapses-media-reality-with-sora-a-photorealistic-ai-video-generator/\"\u003eSora\u003c/a\u003e in February, two major AI models emerged from China: Kuaishou Technology\u0026#39;s \u003ca href=\"https://kling.kuaishou.com/\"\u003eKling\u003c/a\u003e and Minimax\u0026#39;s video-01.\u003c/p\u003e\n\u003cp\u003eBoth Chinese models have already powered numerous viral AI-generated video projects, accelerating meme culture in \u003ca href=\"https://www.reddit.com/r/aivideo/\"\u003eweird new ways\u003c/a\u003e, including a recent shot-for-shot translation of the \u003ca href=\"https://x.com/PJaccetturo/status/1843737222031519910\"\u003e\u003cem\u003ePrincess Mononoke\u003c/em\u003e trailer\u003c/a\u003e using Kling that inspired death threats and a \u003ca href=\"https://www.reddit.com/r/GordonRamsay/comments/1fxazzs/gordon_ramsay_ai/\"\u003eseries of videos\u003c/a\u003e created with Minimax\u0026#39;s platform. The videos show a synthesized version of TV chef Gordon Ramsay \u003ca href=\"https://www.youtube.com/watch?v=zXh-tLmf7_M\"\u003edoing ridiculous things\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv\u003e\u003cblockquote data-lang=\"en\"\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eAfter 22 million views and thousands of death threats, I felt like I needed to take this post down for my own mental health. \u003cbr/\u003eThis trailer was an EXPERIMENT to show my 300 friends on X how far we\u0026#39;ve coming in 16 months.\u003cbr/\u003eI\u0026#39;m putting it back up to keep the conversation going. üßµ \u003ca href=\"https://t.co/tFpRPm9BMv\"\u003epic.twitter.com/tFpRPm9BMv\u003c/a\u003e\u003c/p\u003e‚Äî PJ Ace (@PJaccetturo) \u003ca href=\"https://twitter.com/PJaccetturo/status/1843737222031519910?ref_src=twsrc%5Etfw\"\u003eOctober 8, 2024\u003c/a\u003e\u003c/blockquote\u003e\u003c/div\u003e\n\u003cp\u003eKling first emerged in June, and it can generate two minutes of 1080p HD video at 30 frames per second with a level of detail and coherency that some think surpasses Sora. It\u0026#39;s currently only available to people with a Chinese telephone number, and we have not yet used it ourselves.\u003c/p\u003e\n\u003cp\u003eAround September 1, Minimax \u003ca href=\"https://www.tomsguide.com/ai/ai-image-video/forget-sora-minimax-is-a-new-realistic-ai-video-generator-and-it-is-seriously-impressive\"\u003edebuted\u003c/a\u003e the aforementioned video-01 as part of its \u003ca href=\"https://hailuoai.com/\"\u003eHailuo AI platform\u003c/a\u003e. That site lets anyone generate videos based on a prompt, and initial results seemed similar to Kling, so we decided to run some of our \u003ca href=\"https://arstechnica.com/information-technology/2024/07/we-made-a-cat-drink-a-beer-with-runways-ai-video-generator-and-it-sprouted-hands/\"\u003eRunway Gen-3 prompts\u003c/a\u003e through it to see what happens.\u003c/p\u003e\n\u003ch2\u003ePutting Minimax to the test\u003c/h2\u003e\n\u003cp\u003eWe generated each of the six-second-long 720p videos seen below using Minimax\u0026#39;s free \u003ca href=\"https://hailuoai.com/\"\u003eHailuo AI platform\u003c/a\u003e. Each video generation took up to five to 10 minutes to complete, likely due to being in a queue with other free video users. (At one point, the whole thing froze up on us for a few days, so we didn\u0026#39;t get a chance to generate a flaming cheeseburger.)\u003c/p\u003e\n\n          \n                      \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n          \n          \n\u003cp\u003eIn the spirit of not cherry-picking any results, everything you see was the first generation we received for the prompt listed above it.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026#34;A highly intelligent person reading \u0026#39;Ars Technica\u0026#39; on their computer when the screen explodes\u0026#34;\u003c/strong\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \n\n  \u003cfigcaption id=\"caption-\"\u003e\n    \u003cspan\u003e\u003c/span\u003e\n      \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u0026#34;A cat in a car drinking a can of beer, beer commercial\u0026#34;\u003c/strong\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \n\n  \u003cfigcaption id=\"caption-\"\u003e\n    \u003cspan\u003e\u003c/span\u003e\n      \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u0026#34;Will Smith \u003ca href=\"https://arstechnica.com/information-technology/2024/02/will-smith-parodies-viral-ai-generated-video-by-actually-eating-spaghetti/\"\u003eeating spaghetti\u003c/a\u003e\u0026#34;\u003c/strong\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \n\n  \u003cfigcaption id=\"caption-\"\u003e\n    \u003cspan\u003e\u003c/span\u003e\n      \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u0026#34;Robotic humanoid animals with vaudeville costumes roam the streets collecting protection money in tokens\u0026#34;\u003c/strong\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \n\n  \u003cfigcaption id=\"caption-\"\u003e\n    \u003cspan\u003e\u003c/span\u003e\n      \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u0026#34;A basketball player in a haunted passenger train car with a basketball court, and he is playing against a team of ghosts\u0026#34;\u003c/strong\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \n\n  \u003cfigcaption id=\"caption-\"\u003e\n    \u003cspan\u003e\u003c/span\u003e\n      \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u0026#34;A herd of one million cats running on a hillside, aerial view\u0026#34;\u003c/strong\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \n\n  \u003cfigcaption id=\"caption-\"\u003e\n    \u003cspan\u003e\u003c/span\u003e\n      \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u0026#34;Video game footage of a dynamic 1990s third-person 3D platform game starring an anthropomorphic shark boy\u0026#34;\u003c/strong\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \n\n  \u003cfigcaption id=\"caption-\"\u003e\n    \u003cspan\u003e\u003c/span\u003e\n      \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u0026#34;A muscular barbarian breaking a CRT television set with a weapon, cinematic, 8K, studio lighting\u0026#34;\u003c/strong\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \n\n  \u003cfigcaption id=\"caption-\"\u003e\n    \u003cspan\u003e\u003c/span\u003e\n      \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch2\u003eLimitations of video synthesis models\u003c/h2\u003e\n\u003cp\u003eOverall, the Minimax video-01 results seen above feel fairly similar to Gen-3\u0026#39;s outputs, with some differences, like the lack of a celebrity filter on Will Smith (who sadly did not actually eat the spaghetti in our tests), and the more realistic cat hands and licking motion. Some results were far worse, like the 1 million cats and the Ars Technica reader.\u003c/p\u003e\n\n          \n                  \u003c/div\u003e\n                    \n        \n          \n    \n    \u003cdiv\u003e\n        \n        \n        \n        \u003cdiv\u003e\n          \n          \n\u003cp\u003eAs we explained in our hands-on test for \u003ca href=\"https://arstechnica.com/information-technology/2024/07/we-made-a-cat-drink-a-beer-with-runways-ai-video-generator-and-it-sprouted-hands/\"\u003eRunway\u0026#39;s Gen-3 Alpha\u003c/a\u003e, text-to-video models typically excel at combining concepts present in their training data (existing video samples used to create the model), allowing for creative mashups of existing themes and styles. However, these AI models often struggle with generalization, meaning they have difficulty applying learned information to entirely novel scenarios not represented in their training data.\u003c/p\u003e\n\u003cp\u003eThis limitation can lead to unexpected or unintended results when users request scenarios that deviate too far from the model\u0026#39;s training examples. While we saw a very comical result for the cat drinking beer in the Gen-3 test, Minimax rendered a more realistic-looking result, and that could come down to better parsing of the prompt, different training data, more compute in training the model, or a different model architecture. Ultimately, there\u0026#39;s still a lot of trial and error in generating a coherent result.\u003c/p\u003e\n\u003cp\u003eIt\u0026#39;s worth noting that while China\u0026#39;s models seem to match US video synthesis models from earlier this year, American tech companies aren\u0026#39;t standing still. Google showed off \u003ca href=\"https://arstechnica.com/information-technology/2024/05/google-unveils-veo-a-high-definition-ai-video-generator-that-may-rival-sora/\"\u003eVeo\u003c/a\u003e in May with some very impressive-looking demos. And last week, we reported on Meta\u0026#39;s \u003ca href=\"https://arstechnica.com/ai/2024/10/metas-new-movie-gen-ai-system-can-deepfake-video-from-a-single-photo/\"\u003eMovie Gen\u003c/a\u003e model, which appears (without using Meta\u0026#39;s model ourselves) to potentially be a step ahead of Minimax and Kling. But China\u0026#39;s servers are doubtlessly cranking away at training new AI video models as we speak, so this deepfake arms race probably won\u0026#39;t slow down any time soon.\u003c/p\u003e\n\n\n          \n                  \u003c/div\u003e\n\n                  \n          \u003cdiv\u003e\n  \u003cdiv\u003e\n          \u003cp\u003e\u003ca href=\"https://arstechnica.com/author/benjedwards/\"\u003e\u003cimg src=\"https://arstechnica.com/wp-content/uploads/2022/08/benj_ega.png\" alt=\"Photo of Benj Edwards\"/\u003e\u003c/a\u003e\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cdiv\u003e\n    \n\n    \u003cp\u003e\n      Benj Edwards is Ars Technica\u0026#39;s Senior AI Reporter and founder of the site\u0026#39;s dedicated AI beat in 2022. He\u0026#39;s also a widely-cited tech historian. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\n    \u003c/p\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n  \n\n\n  \n\n\n  \n              \u003c/div\u003e\n  \u003c/article\u003e\n\n\n\u003cdiv\u003e\n    \u003cheader\u003e\n      \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 40 26\"\u003e\u003cdefs\u003e\u003cclipPath id=\"most-read_svg__a\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003cclipPath id=\"most-read_svg__b\"\u003e\u003cpath fill=\"none\" d=\"M0 0h40v26H0z\"\u003e\u003c/path\u003e\u003c/clipPath\u003e\u003c/defs\u003e\u003cg clip-path=\"url(#most-read_svg__a)\"\u003e\u003cg fill=\"none\" clip-path=\"url(#most-read_svg__b)\"\u003e\u003cpath fill=\"currentColor\" d=\"M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1\"\u003e\u003c/path\u003e\u003cpath fill=\"#ff4e00\" d=\"M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\n      \n    \u003c/header\u003e\n    \u003col\u003e\n              \u003cli\u003e\n                      \u003cimg src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/10/unplugged-ethernet-cable-768x432.jpg\" alt=\"Listing image for first story in Most Read: Two never-before-seen tools, from same group, infect air-gapped devices\" decoding=\"async\" loading=\"lazy\"/\u003e\n                    \u003cdiv\u003e\n                          \n              \u003cp\u003e\u003cspan\u003e\n                \u003cspan\u003e1.\u003c/span\u003e\n                \u003cspan\u003eTwo never-before-seen tools, from same group, infect air-gapped devices\u003c/span\u003e\n              \u003c/span\u003e\n                      \u003c/p\u003e\u003c/div\u003e\n          \u003ca href=\"https://arstechnica.com/security/2024/10/two-never-before-seen-tools-from-same-group-infect-air-gapped-devices/?itm_source=parsely-api\" aria-label=\"Read Two never-before-seen tools, from same group, infect air-gapped devices\"\u003e\n          \u003c/a\u003e\n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \u003cdiv\u003e\n                          \n              \u003cp\u003e\u003cspan\u003e\n                \u003cspan\u003e2.\u003c/span\u003e\n                \u003cspan\u003eBankrupt Fisker says it can‚Äôt migrate its EVs to a new owner‚Äôs server\u003c/span\u003e\n              \u003c/span\u003e\n                      \u003c/p\u003e\u003c/div\u003e\n          \u003ca href=\"https://arstechnica.com/cars/2024/10/connected-car-failure-puts-kibosh-on-sale-of-3300-fisker-oceans/?itm_source=parsely-api\" aria-label=\"Read Bankrupt Fisker says it can‚Äôt migrate its EVs to a new owner‚Äôs server\"\u003e\n          \u003c/a\u003e\n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \u003cdiv\u003e\n                          \n              \u003cp\u003e\u003cspan\u003e\n                \u003cspan\u003e3.\u003c/span\u003e\n                \u003cspan\u003eSunderfolk is a couch co-op tactical RPG you play with a phone. No, really.\u003c/span\u003e\n              \u003c/span\u003e\n                      \u003c/p\u003e\u003c/div\u003e\n          \u003ca href=\"https://arstechnica.com/gaming/2024/10/sunderfolk-is-a-couch-co-op-tactical-rpg-you-play-with-a-phone-no-really/?itm_source=parsely-api\" aria-label=\"Read Sunderfolk is a couch co-op tactical RPG you play with a phone. No, really.\"\u003e\n          \u003c/a\u003e\n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \u003cdiv\u003e\n                          \n              \u003cp\u003e\u003cspan\u003e\n                \u003cspan\u003e4.\u003c/span\u003e\n                \u003cspan\u003eDrug makers can‚Äôt make knockoff weight-loss drugs anymore‚Äîand they‚Äôre mad\u003c/span\u003e\n              \u003c/span\u003e\n                      \u003c/p\u003e\u003c/div\u003e\n          \u003ca href=\"https://arstechnica.com/health/2024/10/drug-makers-sue-fda-so-they-can-keep-making-knockoff-weight-loss-drugs/?itm_source=parsely-api\" aria-label=\"Read Drug makers can‚Äôt make knockoff weight-loss drugs anymore‚Äîand they‚Äôre mad\"\u003e\n          \u003c/a\u003e\n        \u003c/li\u003e\n                    \u003cli\u003e\n                    \u003cdiv\u003e\n                          \n              \u003cp\u003e\u003cspan\u003e\n                \u003cspan\u003e5.\u003c/span\u003e\n                \u003cspan\u003eMan learns he‚Äôs being dumped via ‚Äúdystopian‚Äù AI summary of texts\u003c/span\u003e\n              \u003c/span\u003e\n                      \u003c/p\u003e\u003c/div\u003e\n          \u003ca href=\"https://arstechnica.com/ai/2024/10/man-learns-hes-being-dumped-via-dystopian-ai-summary-of-texts/?itm_source=parsely-api\" aria-label=\"Read Man learns he‚Äôs being dumped via ‚Äúdystopian‚Äù AI summary of texts\"\u003e\n          \u003c/a\u003e\n        \u003c/li\u003e\n                  \u003c/ol\u003e\n\u003c/div\u003e\n\n\n  \n\n  \u003c/main\u003e\n\n\n\n\n\n\n\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-10-09T21:33:34Z",
  "modifiedTime": "2024-10-10T12:44:31Z"
}
