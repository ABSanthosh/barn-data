{
  "id": "d9d7cd73-4ef3-4a4d-9c24-a48af5c63606",
  "title": "Make Ubuntu packages 90% faster by rebuilding them",
  "link": "https://gist.github.com/jwbee/7e8b27e298de8bbbf8abfa4c232db097",
  "description": "Comments",
  "author": "",
  "published": "Tue, 18 Mar 2025 23:55:17 +0000",
  "source": "https://news.ycombinator.com/rss",
  "categories": null,
  "byline": "jwbee",
  "length": 8604,
  "excerpt": "Make Ubuntu packages 90% faster by rebuilding them - jq.md",
  "siteName": "Gist",
  "favicon": "https://gist.github.com/fluidicon.png",
  "text": "TL;DR You can take the same source code package that Ubuntu uses to build jq, compile it again, and realize 90% better performance. Setting I use jq for processing GeoJSON files and other open data offered in JSON format. Today I am working with a 500MB GeoJSON file that contains the Alameda County Assessor's parcel map. I want to run a query that prints the city for every parcel worth more than a threshold amount. The program is .features[] | select(.properties.TotalNetValue \u003c 193000) | .properties.SitusCity This takes about 5 seconds with the file cached, on a Ryzen 9 9950X system. That seems a bit shabby and I am sure we can do better. Step 1: Just rebuild the package What happens if you grab the jq source code from Launchpad, then configure and rebuild it with no flags at all? Even that is about 2-4% faster than the Ubuntu binary package. We are using hyperfine to get repeatable results. The jq program is being constrained on logical CPU 2, to keep it away from system interrupts that run on CPU 0 and to ensure no CPU migrations. % hyperfine --warmup 1 --runs 3 -L binary ~/jq-jq-1.7.1/jq,/usr/bin/jq \"taskset -c 2 {binary} -rf /tmp/select.jq /tmp/parcels.geojson\" Benchmark 1: taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 4.517 s ± 0.017 s [User: 3.907 s, System: 0.610 s] Range (min … max): 4.497 s … 4.531 s 3 runs Benchmark 2: taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 4.641 s ± 0.038 s [User: 4.013 s, System: 0.628 s] Range (min … max): 4.601 s … 4.675 s 3 runs Summary taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson ran 1.03 ± 0.01 times faster than taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Step 2: Rebuild with clang and better flags Next, let's rebuild the program with my favorite compiler, a higher optimization level, LTO, and some flags that I typically want to help with debugging and profiling. Some of them are irrelevant to this case, but I use the same flags for most builds. The flags that seem to make a performance difference are: -O3 vs -O2 -flto -DNDEBUG The last of those saves a lot of cost in assertions that showed up strongly in the profiles. % CC=clang-18 LDFLAGS=\"-flto -g -Wl,--emit-relocs -Wl,-z,now -Wl,--gc-sections -fuse-ld=lld\" CFLAGS=\"-flto -DNDEBUG -fno-omit-frame-pointer -gmlt -march=native -O3 -mno-omit-leaf-frame-pointer -ffunction-sections -fdata-sections\" ./configure Benchmark 1: taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 3.853 s ± 0.033 s [User: 3.245 s, System: 0.608 s] Range (min … max): 3.822 s … 3.887 s 3 runs Benchmark 2: taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 4.631 s ± 0.047 s [User: 4.012 s, System: 0.619 s] Range (min … max): 4.602 s … 4.686 s 3 runs Summary taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson ran 1.20 ± 0.02 times faster than taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Now we are 20% faster than upstream with almost no effort. Step 3: Add TCMalloc Jq is a complex C program, and C programs of any complexity tend to rely on malloc and free, because the language offers no other cognizable way to deal with memory. Allocation is the top line in the profile by far. What if we use a better allocator, instead of the one that comes in GNU libc? Ubuntu offers a package of TCMalloc, which is actually rather obsolete and not the current TCMalloc effort, but it's an allocator package in their repo, so let's give it a whirl. Having added -L/usr/lib/x86_64-linux-gnu -ltcmalloc_minimal to the LDFLAGS and rebuilt ... Benchmark 1: taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 3.253 s ± 0.009 s [User: 2.625 s, System: 0.628 s] Range (min … max): 3.245 s … 3.262 s 3 runs Benchmark 2: taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 4.611 s ± 0.026 s [User: 4.015 s, System: 0.596 s] Range (min … max): 4.591 s … 4.640 s 3 runs Summary taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson ran 1.42 ± 0.01 times faster than taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson This is not bad. We are now \u003e 40% faster than the package upstream tried to foist on us. Step 4: What about just preloading TCMalloc dynamically? If the allocator is the issue, it stands to reason that we can get some of that benefit just by hiding the libc allocator using a dynamic preload with the stock Ubuntu binary. Benchmark 1: LD_PRELOAD= taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 4.601 s ± 0.027 s [User: 3.966 s, System: 0.634 s] Range (min … max): 4.577 s … 4.630 s 3 runs Benchmark 2: LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 4.082 s ± 0.010 s [User: 3.476 s, System: 0.606 s] Range (min … max): 4.071 s … 4.091 s 3 runs Summary LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson ran 1.13 ± 0.01 times faster than LD_PRELOAD= taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson This by itself is good for 13%. Not bad. Step 5: Dynamically loading other allocators Ubuntu also ships packages of jemalloc and mimalloc. We can try them all. It turns out that mimalloc beats all others. Note: mimalloc result obtained after setting MIMALLOC_LARGE_OS_PAGES=1 in the environment. Benchmark 1: LD_PRELOAD= taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 4.636 s ± 0.055 s [User: 4.015 s, System: 0.621 s] Range (min … max): 4.579 s … 4.767 s 10 runs Benchmark 2: LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 4.138 s ± 0.055 s [User: 3.511 s, System: 0.627 s] Range (min … max): 4.080 s … 4.255 s 10 runs Benchmark 3: LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 4.067 s ± 0.030 s [User: 3.345 s, System: 0.721 s] Range (min … max): 4.031 s … 4.123 s 10 runs Benchmark 4: LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libmimalloc.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 3.209 s ± 0.041 s [User: 2.934 s, System: 0.274 s] Range (min … max): 3.160 s … 3.274 s 10 runs Summary LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libmimalloc.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson ran 1.27 ± 0.02 times faster than LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson 1.29 ± 0.02 times faster than LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson 1.44 ± 0.03 times faster than LD_PRELOAD= taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Just preloading mimalloc gives a spectacular speedup of 44%! Wow! Step 6: Rebuild with mimalloc Its cool that mimalloc is fast in this case, but dynamic preloads aren't amazing for performance. Let's rebuild the program with mimalloc. Benchmark 1: taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 2.428 s ± 0.019 s [User: 2.161 s, System: 0.267 s] Range (min … max): 2.404 s … 2.464 s 10 runs Benchmark 2: taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Time (mean ± σ): 4.606 s ± 0.039 s [User: 3.979 s, System: 0.627 s] Range (min … max): 4.522 s … 4.640 s 10 runs Summary taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson ran 1.90 ± 0.02 times faster than taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson Jq rebuilt from source with a a better allocator is 1.9x, nearly twice as fast as the Ubuntu binary package for this workload. In another application, processing 2.2GB of JSON in 13000 files (using rush to parallelize) this build of jq does the job in 0.755s vs 1.424s for the Ubuntu package. That is a speedup of nearly 2x again. These are very satisfactory results.",
  "image": "https://github.githubassets.com/assets/gist-og-image-54fd7dc0713e.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"file-jq-md\" tabindex=\"0\" role=\"region\" aria-label=\"jq.md content, created on 11:54PM yesterday.\"\u003e\n    \u003carticle itemprop=\"text\"\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 dir=\"auto\"\u003eTL;DR\u003c/h2\u003e\u003ca id=\"user-content-tldr\" aria-label=\"Permalink: TL;DR\" href=\"#tldr\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eYou can take the same source code package that Ubuntu uses to build \u003ca href=\"https://github.com/jqlang/jq\"\u003ejq\u003c/a\u003e, compile it again, and realize 90% better performance.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 dir=\"auto\"\u003eSetting\u003c/h2\u003e\u003ca id=\"user-content-setting\" aria-label=\"Permalink: Setting\" href=\"#setting\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eI use \u003ccode\u003ejq\u003c/code\u003e for processing GeoJSON files and other open data offered in JSON format. Today I am working with a 500MB GeoJSON file that contains the Alameda County Assessor\u0026#39;s parcel map. I want to run a query that prints the city for every parcel worth more than a threshold amount. The program is\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ccode\u003e.features[] | select(.properties.TotalNetValue \u0026lt; 193000) | .properties.SitusCity\u003c/code\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThis takes about 5 seconds with the file cached, on a Ryzen 9 9950X system. That seems a bit shabby and I am sure we can do better.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 dir=\"auto\"\u003eStep 1:  Just rebuild the package\u003c/h2\u003e\u003ca id=\"user-content-step-1--just-rebuild-the-package\" aria-label=\"Permalink: Step 1:  Just rebuild the package\" href=\"#step-1--just-rebuild-the-package\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWhat happens if you grab the \u003ca href=\"https://launchpad.net/ubuntu/+archive/primary/+sourcefiles/jq/1.7.1-3build1/jq_1.7.1.orig.tar.gz\" rel=\"nofollow\"\u003ejq source code from Launchpad\u003c/a\u003e, then configure and rebuild it with no flags at all? Even that is about 2-4% faster than the Ubuntu binary package.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eWe are using \u003ca href=\"https://github.com/sharkdp/hyperfine\"\u003ehyperfine\u003c/a\u003e to get repeatable results. The \u003ccode\u003ejq\u003c/code\u003e program is being constrained on logical CPU 2, to keep it away from system interrupts that run on CPU 0 and to ensure no CPU migrations.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% hyperfine --warmup 1 --runs 3 -L binary ~/jq-jq-1.7.1/jq,/usr/bin/jq \u0026#34;taskset -c 2 {binary} -rf /tmp/select.jq /tmp/parcels.geojson\u0026#34;\nBenchmark 1: taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      4.517 s ±  0.017 s    [User: 3.907 s, System: 0.610 s]\n  Range (min … max):    4.497 s …  4.531 s    3 runs\n\nBenchmark 2: taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      4.641 s ±  0.038 s    [User: 4.013 s, System: 0.628 s]\n  Range (min … max):    4.601 s …  4.675 s    3 runs\n\nSummary\n  taskset -c 2 /home/jwb/jq-jq-1.7.1/jq  -rf /tmp/select.jq /tmp/parcels.geojson ran\n    1.03 ± 0.01 times faster than taskset -c 2 /usr/bin/jq  -rf /tmp/select.jq /tmp/parcels.geojson\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp dir=\"auto\"\u003e\u003ch3 dir=\"auto\"\u003eStep 2: Rebuild with clang and better flags\u003c/h3\u003e\u003ca id=\"user-content-step-2-rebuild-with-clang-and-better-flags\" aria-label=\"Permalink: Step 2: Rebuild with clang and better flags\" href=\"#step-2-rebuild-with-clang-and-better-flags\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eNext, let\u0026#39;s rebuild the program with my favorite compiler, a higher optimization level, LTO, and some flags that I typically want to help with debugging and profiling. Some of them are irrelevant to this case, but I use the same flags for most builds. The flags that seem to make a performance difference are:\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e-O3 vs -O2\u003c/li\u003e\n\u003cli\u003e-flto\u003c/li\u003e\n\u003cli\u003e-DNDEBUG\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003eThe last of those saves a lot of cost in assertions that showed up strongly in the profiles.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ccode\u003e% CC=clang-18 LDFLAGS=\u0026#34;-flto -g -Wl,--emit-relocs -Wl,-z,now -Wl,--gc-sections -fuse-ld=lld\u0026#34; CFLAGS=\u0026#34;-flto -DNDEBUG -fno-omit-frame-pointer -gmlt -march=native -O3 -mno-omit-leaf-frame-pointer -ffunction-sections -fdata-sections\u0026#34; ./configure\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBenchmark 1: taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      3.853 s ±  0.033 s    [User: 3.245 s, System: 0.608 s]\n  Range (min … max):    3.822 s …  3.887 s    3 runs\n\nBenchmark 2: taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      4.631 s ±  0.047 s    [User: 4.012 s, System: 0.619 s]\n  Range (min … max):    4.602 s …  4.686 s    3 runs\n\nSummary\n  taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson ran\n    1.20 ± 0.02 times faster than taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp dir=\"auto\"\u003eNow we are 20% faster than upstream with almost no effort.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 dir=\"auto\"\u003eStep 3: Add TCMalloc\u003c/h2\u003e\u003ca id=\"user-content-step-3-add-tcmalloc\" aria-label=\"Permalink: Step 3: Add TCMalloc\" href=\"#step-3-add-tcmalloc\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eJq is a complex C program, and C programs of any complexity tend to rely on malloc and free, because the language offers no other cognizable way to deal with memory. Allocation is the top line in the profile by far. What if we use a better allocator, instead of the one that comes in GNU libc? Ubuntu offers a package of TCMalloc, which is actually rather obsolete and not the current TCMalloc effort, but it\u0026#39;s an allocator package in their repo, so let\u0026#39;s give it a whirl.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eHaving added \u003ccode\u003e-L/usr/lib/x86_64-linux-gnu -ltcmalloc_minimal\u003c/code\u003e to the LDFLAGS and rebuilt ...\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBenchmark 1: taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      3.253 s ±  0.009 s    [User: 2.625 s, System: 0.628 s]\n  Range (min … max):    3.245 s …  3.262 s    3 runs\n\nBenchmark 2: taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      4.611 s ±  0.026 s    [User: 4.015 s, System: 0.596 s]\n  Range (min … max):    4.591 s …  4.640 s    3 runs\n\nSummary\n  taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson ran\n    1.42 ± 0.01 times faster than taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp dir=\"auto\"\u003eThis is not bad. We are now \u0026gt; 40% faster than the package upstream tried to foist on us.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 dir=\"auto\"\u003eStep 4: What about just preloading TCMalloc dynamically?\u003c/h2\u003e\u003ca id=\"user-content-step-4-what-about-just-preloading-tcmalloc-dynamically\" aria-label=\"Permalink: Step 4: What about just preloading TCMalloc dynamically?\" href=\"#step-4-what-about-just-preloading-tcmalloc-dynamically\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eIf the allocator is the issue, it stands to reason that we can get some of that benefit just by hiding the libc allocator using a dynamic preload with the stock Ubuntu binary.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBenchmark 1: LD_PRELOAD= taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      4.601 s ±  0.027 s    [User: 3.966 s, System: 0.634 s]\n  Range (min … max):    4.577 s …  4.630 s    3 runs\n\nBenchmark 2: LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      4.082 s ±  0.010 s    [User: 3.476 s, System: 0.606 s]\n  Range (min … max):    4.071 s …  4.091 s    3 runs\n\nSummary\n  LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so taskset -c 2 /usr/bin/jq  -rf /tmp/select.jq /tmp/parcels.geojson ran\n    1.13 ± 0.01 times faster than LD_PRELOAD= taskset -c 2 /usr/bin/jq  -rf /tmp/select.jq /tmp/parcels.geojson\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp dir=\"auto\"\u003eThis by itself is good for 13%. Not bad.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 dir=\"auto\"\u003eStep 5: Dynamically loading other allocators\u003c/h2\u003e\u003ca id=\"user-content-step-5-dynamically-loading-other-allocators\" aria-label=\"Permalink: Step 5: Dynamically loading other allocators\" href=\"#step-5-dynamically-loading-other-allocators\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eUbuntu also ships packages of \u003ca href=\"https://github.com/jemalloc/jemalloc\"\u003ejemalloc\u003c/a\u003e and \u003ca href=\"https://github.com/microsoft/mimalloc\"\u003emimalloc\u003c/a\u003e. We can try them all. It turns out that mimalloc beats all others.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eNote: mimalloc result obtained after setting \u003ccode\u003eMIMALLOC_LARGE_OS_PAGES=1\u003c/code\u003e in the environment.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBenchmark 1: LD_PRELOAD= taskset -c 2 /usr/bin/jq  -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      4.636 s ±  0.055 s    [User: 4.015 s, System: 0.621 s]\n  Range (min … max):    4.579 s …  4.767 s    10 runs\n\nBenchmark 2: LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      4.138 s ±  0.055 s    [User: 3.511 s, System: 0.627 s]\n  Range (min … max):    4.080 s …  4.255 s    10 runs\n\nBenchmark 3: LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      4.067 s ±  0.030 s    [User: 3.345 s, System: 0.721 s]\n  Range (min … max):    4.031 s …  4.123 s    10 runs\n\nBenchmark 4: LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libmimalloc.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      3.209 s ±  0.041 s    [User: 2.934 s, System: 0.274 s]\n  Range (min … max):    3.160 s …  3.274 s    10 runs\n\nSummary\n  LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libmimalloc.so taskset -c 2 /usr/bin/jq  -rf /tmp/select.jq /tmp/parcels.geojson ran\n    1.27 ± 0.02 times faster than LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n    1.29 ± 0.02 times faster than LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n    1.44 ± 0.03 times faster than LD_PRELOAD= taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp dir=\"auto\"\u003eJust preloading mimalloc gives a spectacular speedup of 44%! Wow!\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ch2 dir=\"auto\"\u003eStep 6: Rebuild with mimalloc\u003c/h2\u003e\u003ca id=\"user-content-step-6-rebuild-with-mimalloc\" aria-label=\"Permalink: Step 6: Rebuild with mimalloc\" href=\"#step-6-rebuild-with-mimalloc\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eIts cool that mimalloc is fast in this case, but dynamic preloads aren\u0026#39;t amazing for performance. Let\u0026#39;s rebuild the program with mimalloc.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBenchmark 1: taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      2.428 s ±  0.019 s    [User: 2.161 s, System: 0.267 s]\n  Range (min … max):    2.404 s …  2.464 s    10 runs\n\nBenchmark 2: taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n  Time (mean ± σ):      4.606 s ±  0.039 s    [User: 3.979 s, System: 0.627 s]\n  Range (min … max):    4.522 s …  4.640 s    10 runs\n\nSummary\n  taskset -c 2 /home/jwb/jq-jq-1.7.1/jq -rf /tmp/select.jq /tmp/parcels.geojson ran\n    1.90 ± 0.02 times faster than taskset -c 2 /usr/bin/jq -rf /tmp/select.jq /tmp/parcels.geojson\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp dir=\"auto\"\u003eJq rebuilt from source with a a better allocator is 1.9x, nearly twice as fast as the Ubuntu binary package for this workload. In another application, processing 2.2GB of JSON in 13000 files (using \u003ca href=\"https://github.com/shenwei356/rush\"\u003erush\u003c/a\u003e to parallelize) this build of jq does the job in 0.755s vs 1.424s for the Ubuntu package. That is a speedup of nearly 2x again. These are very satisfactory results.\u003c/p\u003e\n\u003c/article\u003e\n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": null,
  "modifiedTime": null
}
