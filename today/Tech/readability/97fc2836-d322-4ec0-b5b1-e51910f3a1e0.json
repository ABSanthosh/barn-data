{
  "id": "97fc2836-d322-4ec0-b5b1-e51910f3a1e0",
  "title": "Character.AI and Google sued after chatbot-obsessed teen’s death",
  "link": "https://www.theverge.com/2024/10/23/24277962/character-ai-google-wrongful-death-lawsuit",
  "description": "",
  "author": "Emma Roth",
  "published": "2024-10-23T19:11:01-04:00",
  "source": "https://www.theverge.com/rss/index.xml",
  "categories": null,
  "byline": "Emma Roth",
  "length": 3369,
  "excerpt": "Character.AI, its founders, and Google are facing a lawsuit after a 14-year-old died by suicide after growing attached to a custom chatbot.",
  "siteName": "The Verge",
  "favicon": "https://www.theverge.com/icons/android_chrome_512x512.png",
  "text": "A lawsuit has been filed against Character.AI, its founders Noam Shazeer and Daniel De Freitas, and Google in the wake of a teenager’s death, alleging wrongful death, negligence, deceptive trade practices, and product liability. Filed by the teen’s mother, Megan Garcia, it claims the platform for custom AI chatbots was “unreasonably dangerous” and lacked safety guardrails while being marketed to children.As outlined in the lawsuit, 14-year-old Sewell Setzer III began using Character.AI last year, interacting with chatbots modeled after characters from The Game of Thrones, including Daenerys Targaryen. Setzer, who chatted with the bots continuously in the months before his death, died by suicide on February 28th, 2024, “seconds” after his last interaction with the bot.Accusations include the site “anthropomorphizing” AI characters and that the platform’s chatbots offer “psychotherapy without a license.” Character.AI houses mental health-focused chatbots like “Therapist” and “Are You Feeling Lonely,” which Setzer interacted with. Garcia’s lawyers quote Shazeer saying in an interview that he and De Freitas left Google to start his own company because “there’s just too much brand risk in large companies to ever launch anything fun” and that he wanted to “maximally accelerate” the tech. It says they left after the company decided against launching the Meena LLM they’d built. Google acquired the Character.AI leadership team in August.Character.AI’s website and mobile app has hundreds of custom AI chatbots, many modeled after popular characters from TV shows, movies, and video games. A few months ago, The Verge wrote about the millions of young people, including teens, who make up the bulk of its user base, interacting with bots that might pretend to be Harry Styles or a therapist. Another recent report from Wired highlighted issues with Character.AI’s custom chatbots impersonating real people without their consent, including one posing as a teen who was murdered in 2006.Because of the way chatbots like Character.ai generate output that depends on what the user inputs, they fall into an uncanny valley of thorny questions about user-generated content and liability that, so far, lacks clear answers.Character.AI has now announced several changes to the platform, with communications head Chelsea Harrison saying in an email to The Verge, “We are heartbroken by the tragic loss of one of our users and want to express our deepest condolences to the family.”Some of the changes include:Changes to our models for minors (under the age of 18) that are designed to reduce the likelihood of encountering sensitive or suggestive content.Improved detection, response, and intervention related to user inputs that violate our Terms or Community Guidelines. A revised disclaimer on every chat to remind users that the AI is not a real person.Notification when a user has spent an hour-long session on the platform with additional user flexibility in progress.“As a company, we take the safety of our users very seriously, and our Trust and Safety team has implemented numerous new safety measures over the past six months, including a pop-up directing users to the National Suicide Prevention Lifeline that is triggered by terms of self-harm or suicidal ideation,” Harrison said. Google didn’t immediately respond to The Verge’s request for comment.",
  "image": "https://cdn.vox-cdn.com/thumbor/acjM1CnlZfkPDQem3dZ3VtxxL24=/0x0:2040x1360/1200x628/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/25362061/STK_414_AI_CHATBOT_R2_CVirginia_D.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eA lawsuit has been \u003ca href=\"https://www.documentcloud.org/documents/25248089-megan-garcia-vs-character-ai?responsive=1\u0026amp;title=1\"\u003efiled\u003c/a\u003e against Character.AI, its founders Noam Shazeer and Daniel De Freitas, and Google in the wake of a teenager’s death, alleging wrongful death, negligence, deceptive trade practices, and product liability. Filed by the teen’s mother, Megan Garcia, it claims the platform for custom AI chatbots was “unreasonably dangerous” and lacked safety guardrails while being marketed to children.\u003c/p\u003e\u003cp\u003eAs outlined in the lawsuit, 14-year-old Sewell Setzer III began using Character.AI last year, interacting with chatbots modeled after characters from \u003cem\u003eThe Game of Thrones\u003c/em\u003e, including Daenerys Targaryen. Setzer, who chatted with the bots continuously in the months before his death, died by suicide on February 28th, 2024, “seconds” after his last interaction with the bot.\u003c/p\u003e\u003cp\u003eAccusations include the site “anthropomorphizing” AI characters and that the platform’s chatbots offer “psychotherapy without a license.” Character.AI houses mental health-focused chatbots like “Therapist” and “Are You Feeling Lonely,” which Setzer interacted with. \u003c/p\u003e\u003cp\u003eGarcia’s lawyers quote Shazeer \u003ca href=\"https://www.youtube.com/watch?v=tO7Ze6ewOG8\"\u003esaying in an interview\u003c/a\u003e that he and De Freitas left Google to start his own company because “there’s just too much brand risk in large companies to ever launch anything fun” and that he wanted to “maximally accelerate” the tech. It says they left after the company decided against launching the Meena LLM they’d built. Google \u003ca href=\"https://www.theverge.com/2024/8/2/24212348/google-hires-character-ai-noam-shazeer\"\u003eacquired the Character.AI leadership team\u003c/a\u003e in August.\u003c/p\u003e\u003cp\u003eCharacter.AI’s website and mobile app has hundreds of custom AI chatbots, many modeled after popular characters from TV shows, movies, and video games. A few months ago, \u003ca href=\"https://www.theverge.com/2024/5/4/24144763/ai-chatbot-friends-character-teens\"\u003e\u003cem\u003eThe Verge\u003c/em\u003e wrote about the millions of young people\u003c/a\u003e, including teens, who make up the bulk of its user base, interacting with bots that might pretend to be Harry Styles or a therapist. Another recent report \u003ca href=\"https://www.wired.com/story/characterai-has-a-non-consensual-bot-problem/\"\u003efrom \u003cem\u003eWired\u003c/em\u003e highlighted issues\u003c/a\u003e with Character.AI’s custom chatbots impersonating real people without their consent, including one posing as a teen who was murdered in 2006.\u003c/p\u003e\u003cp\u003eBecause of the way chatbots like Character.ai generate output that depends on what the user inputs, they fall into an uncanny valley of \u003ca href=\"https://www.theverge.com/22268421/cda-section-230-25th-anniversary-reform-stakes-big-tech-internet\"\u003ethorny questions\u003c/a\u003e about user-generated content and liability that, so far, lacks clear answers.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://blog.character.ai/community-safety-updates/\"\u003eCharacter.AI has now announced several\u003c/a\u003e changes to the platform, with communications head Chelsea Harrison saying in an email to \u003cem\u003eThe Verge\u003c/em\u003e, “We are heartbroken by the tragic loss of one of our users and want to express our deepest condolences to the family.”\u003c/p\u003e\u003cp\u003eSome of the changes include:\u003c/p\u003e\u003cdiv\u003e\u003cul\u003e\u003cli\u003eChanges to our models for minors (under the age of 18) that are designed to reduce the likelihood of encountering sensitive or suggestive content.\u003c/li\u003e\u003cli\u003eImproved detection, response, and intervention related to user inputs that violate our Terms or Community Guidelines. \u003c/li\u003e\u003cli\u003eA revised disclaimer on every chat to remind users that the AI is not a real person.\u003c/li\u003e\u003cli\u003eNotification when a user has spent an hour-long session on the platform with additional user flexibility in progress.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cp\u003e“As a company, we take the safety of our users very seriously, and our Trust and Safety team has implemented numerous new safety measures over the past six months, including a pop-up directing users to the National Suicide Prevention Lifeline that is triggered by terms of self-harm or suicidal ideation,” Harrison said. Google didn’t immediately respond to \u003cem\u003eThe Verge\u003c/em\u003e’s request for comment.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-10-23T23:11:00.347Z",
  "modifiedTime": "2024-10-23T23:11:00.347Z"
}
