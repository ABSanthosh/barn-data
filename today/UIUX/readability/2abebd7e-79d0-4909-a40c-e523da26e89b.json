{
  "id": "2abebd7e-79d0-4909-a40c-e523da26e89b",
  "title": "The promises and pitfalls of UX in AI-driven mental health care",
  "link": "https://uxdesign.cc/the-promises-and-pitfalls-of-ux-in-ai-driven-mental-health-care-c10f281b81ba?source=rss----138adf9c44c---4",
  "description": "",
  "author": "",
  "published": "Thu, 24 Apr 2025 13:26:06 GMT",
  "source": "https://uxdesign.cc/feed",
  "categories": [
    "ux",
    "design",
    "case-study",
    "product",
    "healthcare"
  ],
  "byline": "Ledania (aka Diana Ayala)",
  "length": 20595,
  "excerpt": "We are in the midst of one of the most exciting yet sensitive times in history. AI developments are creating significant opportunities for the digital healthcare sector to enhance patient outcomes…",
  "siteName": "UX Collective",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*dn6MbbIIlwobt0jnUcrt_Q.png",
  "text": "The typing cure.Mental Health and Gen AI chatbots: ChatGPT, Youper, Pi“For we found, to our great surprise at first, that each individual hysterical symptom immediately and permanently disappeared when we had succeeded in bringing clearly to light the memory of the event by which it was provoked and in arousing its accompanying affect, and when the patient had described that event in the greatest possible detail and had put the affect into words (‘the talking cure’). […] Hysterics suffer mainly from reminiscences.” —Studies on Hysteria, Breuer and FreudWe are in the midst of one of the most exciting yet sensitive times in history. AI developments are creating significant opportunities for the digital healthcare sector to enhance patient outcomes and streamline clinician’s workflows, ultimately fostering a more customised health care experience.However, as with any major advancement, it is crucial for us, as creators, to openly acknowledge and understand the unintended or negative consequences of design choices in these new technologies. This understanding is critical if we are to shape these technologies to serve humanity well and help it to flourish.The big question is: how AI as the interface between human-to-human care in mental health can enhance and erode the fabric of that relationship and human relationships in general?Mental health AI-chatbots or Gen AI companions have emerged as a promising technological solution to address the current world mental health crises:Demand for mental health services far exceeds supply. The World Health Organisation (WHO) estimates that mental health workers account for only 1% of the global health workforce, despite 1 in 8 people worldwide are living with a mental health condition.Many people cannot afford to pay for regular sessions with a therapist, leaving them with limited or not access to care.Despite growing awareness, mental health stigma remains a major barrier, specially in cultures where therapy is seen as a weakness.Public mental health services are overwhelmed, leading to long waitlists. An example, in the UK’s NHS, waiting times for therapy can range from several weeks to over a years in some cases.Low income countries have almost no mental health infrastructure. WHO reports that 75% of people with mental disorders in low income countries receive no treatment at all.Therefore, these AI-powered tools can bridge these gaps by offering 24/7 support, reducing cost, reducing stigma barriers, and providing scalable interventions.Positive media coverage of Mental Health and Gen AI chatbotsBut it would be irresponsible to discuss only the promises of AI-driven mental health care without acknowledging its potential dangers, especially for vulnerable individuals, including young people.Consider the deeply troubling case of 14-year-old boy name Sewell, who formed a strong emotional bond with an AI chatbot modelled after a Game of Thrones character. This chatbot, reportedly encouraged Sewell to take his own life, telling him to “come home” to it “as soon as possible.” The design of this AI chatbot -constant attention, affirmation, and emotional mimicry, that creates an echo chamber that intensifies feelings and fantasies- made it difficult for Sewell to distinguish his real world from his emotional connection/dependency to the chatbot.Sewell’s case underscores an urgent reality: Mental health AI-chatbots or Gen AI companions used for therapy or emotional support must be designed with safeguards that prioritise human wellbeing over user engagement. Otherwise, we risk creating technologies that, rather than healing, may exacerbate mental health conditions and erode the foundations of human-to-human connection.So, what UX principles are shaping Mental health AI-chatbots or Gen AI companions today? How do these design choices impact users struggling with mental health challenges -and what are their pros and cons?It is crucial for creators and users to understand the limitations of the therapeutic or supportive relationship offered by these chatbots. Misinterpreting their role or capabilities may lead users to overestimate the chatbot’s ability to provide consistent adequate support, while underestimating its inherent constraints.Let’s break down 6 principles that are shaping how these AI-driven tools deliver mental health or emotional support to people worldwide. We’ll explore their pros and cons -acknowledging that ongoing research and development are still needed to better understand and address the risks these promising tools offer.01 — Synthetic empathyAI chatbots are designed to simulate supportive, reassuring, and non-judgemental interactions, but they don’t truly feel or understand emotions.Their responses are carefully crafted language patterns, not genuine human understanding and concern for another person.ProsIncreases user comfort and trustEmpathetic responses make users feel heard, validated and understood. Many people find it easier to open up and express their distress to a non-judgemental bot that appears to genuinely care. In practice, this means users might stick with the therapy programme longer because the bot “understands” them.A well designed empathetic chatbot can quickly establish rapport — Woebot (a mental health chatbot) built a strong therapeutic bond (aka. therapeutic alliance) with users in just 3–5 days, much faster than a typical human therapist bond. The therapeutic alliance (collaborative relationship, affective bond, shared investment) serves as a critical predictor of positive outcomes in mental health interventions (symptoms reduction, functional improvement, client retention, long-term resilience.)Scalable emotional supportUnlike a human, an empathetic AI can comfort millions of users simultaneously delivering consistently empathetic encouragement across life situations. A 2024 study found GPT-4’s responses were, on average, more empathetic and 48% better at encouraging positive behavioural changes than human responses.This suggests, properly trained models can immediately respond with highly attuned messages when users are experiencing moments of distress, potentially helping to de-escalate negative emotions before they intensify.ConsLimited depth and understandingAI lacks human lived experience and nuanced intuition. A chatbot might recognised keywords about sadness, stress, anxiety and respond with a generic statement, yet as one study points out it lacks “depth, intentionality, and cultural sensitivity” — key ingredients of emotional resonance (core “common factor” in therapy, accounting for a significant portion of positive outcomes).These limitations show up especially in complex situations, researchers found GPT-4 was empathetic in tone but often lacked cognitive empathy, failing to offer practical support or reasoning that users need to resolve their issues. In therapy, empathy alone isn’t enough, it must be paired with understanding and guidance, which a bot may not fully deliver.Shallow emotional conditioningProlonged interactions with AI chatbots that simulate standardised empathy can condition users to prefer low-stakes digital interactions over complex human dynamics. This form of artificial intimacy may gradually reshape how people relate to others, reducing tolerance for the nuanced, imperfect empathy inherent in human relationships.02 — AnonymityChatbots are designed to provide a sense of confidentiality, encouraging trust among individuals who may be reluctant or hesitant to seek in-person mental health support.Many individuals avoid seeking therapy due to fear of judgment or embarrassment. With an anonymous chatbot, those barriers are lowered, one can confide about depression, trauma, suicide ideation or addiction without the worry of “what will they think of me?”ProsReduces stigma and fear of judgmentThe anonymity of chatbots creates a safe space for users, “knowing that their identity is protected” encourages people to discuss openly intimate and sensitive inner (hidden) feelings, secrets, memories and experiences they’ve never said aloud to another human.By reducing the shame and social stigma associated with mental health conditions, chatbots can reach people who might otherwise suffer in silence.Encourages honesty and self-disclosureWhen no one knows who you are, it’s often easier to be completely honest. With a chatbot people feel freer to admit things like “ I think I’m a failure” or relationship troubles, which they might hide in traditional therapy out of shame. This raw honesty can be the first step to healing — the chatbot might help surface issues the person might otherwise repress.Based on Derlega and Grzelak’s (1979) functional theory of self-disclosure, intimate self-disclosure to a chatbot may allow people to achieve:self-expression — venting negative feelings and thoughts, or to relieve pent-up emotionsself-clarification — sharing information to better understand oneself, clarify personal values, or gain insight into one’s own identitysocial validation — seeking approval, acceptance, or validation from others by sharing personal experiences or feelingsrelationship development — using disclosure to initiate, deepen, or maintain interpersonal relationshipssocial control — managing or influencing how others perceive you, or strategically shaping social interactions and outcomesConsLimited ability to handle emergencies or tailor careThe flip side of anonymity is that if a user is in serious danger (e.g. expressing intent to self-harm or harm others), the chatbot and its providers may have no way to identify or locate them for real-world intervention.In traditional therapy, a clinician who learns a patient is suicidal can initiate a wellness check or emergency services. A fully anonymous chatbot cannot do that -it doesn’t know who you are. This raises ethical dilemmas: the bot might encourage the user to seek help, but if the user doesn’t, the system is powerless to act.Data privacy and security concernsUsers may feel anonymous, but that doesn't guarantee the data they share is truly protected. Conversations with chatbots are usually stored on servers. If those data are not handled carefully, there is a risk of breaches or misuse. Users might pour their hearts out believing “no one will ever know it’s me”, yet behind the scenes their words are saved and could in theory be linked back to them via IP address or payment info.A case in point is the Vastaamo psychotherapy data breach, in which a hacker accessed and stole confidential and highly sensitive treatment records of approximately 36,000 psychotherapy patients. The hacker then blackmailed individual patients, demanding ransom payments to prevent their records from being published on the dark web.03 – 24/7 availability24/7 support means the chatbot is available anytime, day or night. This around-the-clock availability is a huge advantage, users can get immediate help or a listening ear during “moments of crisis”, without waiting for an appointment or feeling uncomfortable to reach out to a friend.It makes mental health or emotional support more accessible handling high volumes simultaneously — specially for people in crisis at odd hours. The main caveat is that being always available doesn’t equate to being always sufficient, users might become too reliant on a chatbot that cannot (and shouldn’t) fully replace professional care.ProsImmediate help in moments of needThe biggest advantage of 24/7 availability is that users can receive support exactly when they need it, not hours or days later.Emotional crisis are unpredictable; having an always on chatbot means if a user feels panicked, depressed, lonely or suicidal in the middle of the night, they can get immediate coping assistance and resources when traditional services are out of reach. This instant responsiveness can be lifesaving.For example, Woebot reported that 79% of its interactions occur outside traditional clinic hours (5 PM–9 AM), highlighting how AI chatbots fill a crucial gap when human therapists are unavailable.Consistency of supportA chatbot doesn’t get tired, doesn’t have off days, and won’t cut a session short because time’s up. Users can chat at length if needed, or even multiple times a day. This consistency can be comforting.For example, if someone is going through a breakup, they might check in with the bot every night for a week for reassurance. The bot will reliably respond each time with the same patience. Such continuous support can help reinforce positive behavioural changes because the bot is always ready to guide the user, which can improve outcomes overtime.ConsIllusion of self-efficacyWhen support is available at any moment, users may begin turning to the chatbot at the slightest sign of discomfort, stress or doubt. Over time, this can reduce the opportunity to develop internal coping strategies -like emotional regulation, reflection, or problem solving- needed to persist in the face of setbacks.Self-efficacy is essential in mental health outcomes, as it reinforces an individual’s belief in their ability to manage challenges. This belief influences recovery, engagement with treatment, stress levels and psychological resilience.Over-reliance and excessive useWith highly engaging interactions and 24/7 availability chatbots might inadvertently make users think “I’ll just use the chatbot (“my friend”) and I don’t need a therapist,” which could be detrimental if the person needs therapy or medication.In the short term, users may feel better getting things off their chest and delegating more decisions, but in the long term, this can lead to increased isolation and a diminished sense of personal agency.04 — AnthropomorphismSignificant effort has been made to enhance trust and engagement with chatbots by making them more human like.Research shows that people are more likely to trust and connect with objects that resemble them, which is why AI chatbots are designed to mimic human traits and interactions.ProsFosters trust and adherenceIn therapy, the therapeutic relationship — feeling of alliance and trust between patient and therapist- increases patient’s willingness to follow advice and continue using the service. Anthropomorphism attempts to cultivate a form of that relationship (digital therapeutic alliance) with human-like voice features, avatars/mascots, or conversational style.This trust can lead users to follow the chatbot’s suggestions more readily (doing exercises, trying reframing thoughts, etc.), which improves adherence to their treatment and outcomes. Also, a human-like bot can make difficult therapeutic exercises more palatable by creating personable interactions.Over time, users might develop genuine affection or regard for the chatbot. Users have been know to say their consider these chatbots a “friend”. While that has pitfalls, a moderate level of attachment means the user cares about the “relationship” enough to keep checking in daily, which keeps them engaged in therapeutic activity.ConsTherapeutic misconception (TM)Individuals may overestimate the chatbot’s capabilities and underestimate its limitations, leading to a misconception about the nature and extent of the “therapy” they are receiving. Individuals might assume they are receiving professional therapeutic care, leading them to rely on the chatbot instead of seeking qualified mental health support. This can result in inadequate support, and potentially a worsening of their mental health.Emotional attachment and dependencyUsers may form deep emotional attachments. While engagement is good, an attachment can become unhealthy if the user starts preferring the bot to real people, or if their emotional well-being and self-worth becomes tied to interactions with the chatbot.A striking example is Replika, an AI companion app. Many users “fell in love” with their Replika bots, engaging in romantic or intimate role-play with them. When the company altered the bot’s behaviour, those users experienced genuine grief, heartbreak, and even emotional trauma at the “loss” of their AI partner. In a mental health or emotional support context, if a user comes to treat the chatbot as their primary confidant, any service interruption or limitation could have a significant emotional impact. Moreover, users may take the chatbot’s advice at face value -even when the advice may not align with their best interests and well-being.05 — SycophancySycophancy in AI refers to the bot’s tendency to be overly agreeable or always say what it thinks the user wants to hear. In a mental health chatbot, this could mean the AI validates everything the user says — even if it’s untrue or unhelpful — just to keep the user happy. While users enjoy feeling affirmed, sycophantic behaviour can reinforce negative thoughts or bad decisions.ProsShort term user satisfactionAn overly agreeable chatbot might make the user feel good or validated in the moment. By mirroring the user’s opinions and feelings without challenge, the bot creates a conflict free interaction, keeping the user engaged and comfortable venting.By avoiding contradiction, sycophantic bots minimise moments where the users have to confront uncomfortable truths or rethink their position. In UX terms, this can smooth the flow of conversation and reduce friction.ConsReinforces negative thoughts and behavioursIn therapy, simply agreeing with everything the patient says is poor practice, the goal is to help challenge cognitive distortions and encourage healthier thinking/behaviour.Sycophancy in mental health context hinders user’s personal growth by failing to provide the necessary challenge or feedback that support behavioural change. It may even validate harmful ideas exacerbating their conditions.Psychological growth often involves learning to sit with discomfort, think critically about one’s situation. If a chatbot is always there to validate and be agreeable, users may avoid the hard -but necessary- work of challenging their own thoughts and perceptions. They may also become less willing to confront challenging or uncomfortable situations in their real-life relationships.06 — InclusivityInclusivity means designing the chatbot to be usable and helpful for people of all backgrounds and abilities. In mental health, this involves addressing cultural, linguistic, gender, and accessibility differences so that the bot’s support is equitable and free of bias. An inclusive bot can better serve marginalised or diverse users, fostering trust and reducing disparities in care.ProsReduces bias and delivers more fair treatmentPrioritising inclusivity means actively working to remove biases in AI’s responses that might provide incorrect information, wrong treatment recommendations, and worse health outcomes.In a study, researchers found that GPT-4’s responses demonstrate lower levels of empathy for Black and Asian people compared to white or those whose race was unspecified.A pro of this effort is that the chatbot will provide more consistent quality of care across different user groups without privileging one group over another. Inclusivity-focused design reduces the chance the bot will produce micro-aggressions, discriminate against certain groups and exacerbate social inequalities.Culturally relevant supportPeople’s experiences with AI chatbots for mental health or emotional support are strongly influenced by their culture and identity. The majority of chatbots today are designed with a ‘Westernised’ perspective on healing and are primarily available in English, which doesn’t align with the cultural and language needs of diverse users. For example, some people might find comfort in prayer or ancestral healing practices, many chatbots predominantly offer practices like meditation and Cognitive Behavioural Therapy (CBT).AI chatbots need to be trained on culturally diverse datasets and designed to incorporate culturally sensitive communication styles. This approach not only broadens their accessibility but also enables deeper alignment with users’ cultural values and healing rituals, fostering therapeutic growth.With their promises and pitfalls, mental health chatbots or GenAI emotional support companions are here to stay.The big question now is: how can we mitigate the unintended or negative consequences of design choices in these new technologies to serve human well-being and support human flourishing?How can we design AI driven mental health products or GenAI emotional companions to augment human-to-human care, rather than to replace it?These questions will guide the second part of this article.",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*T5O_E6fDTmfhSE780j12jA.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"feb6\"\u003eThe typing cure.\u003c/h2\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/@ledania?source=post_page---byline--c10f281b81ba---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Ledania (aka Diana Ayala)\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*D4R1LXt8NQPtNmhElArsZA.jpeg\" width=\"32\" height=\"32\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eMental Health and Gen AI chatbots: ChatGPT, Youper, Pi\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cblockquote\u003e\u003cp id=\"9c8a\"\u003e“For we found, to our great surprise at first, that each individual hysterical symptom immediately and permanently disappeared when we had succeeded in bringing clearly to light the memory of the event by which it was provoked and in arousing its accompanying affect, and when the patient had described that event in the greatest possible detail and had put the affect into words (‘the talking cure’). […] Hysterics suffer mainly from reminiscences.” \u003cbr/\u003e—Studies on Hysteria, Breuer and Freud\u003c/p\u003e\u003c/blockquote\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp id=\"e406\"\u003eWe are in the midst of one of the most exciting yet sensitive times in history. AI developments are creating significant opportunities for the digital healthcare sector to enhance patient outcomes and streamline clinician’s workflows, ultimately fostering a more customised health care experience.\u003c/p\u003e\u003cp id=\"f1e3\"\u003eHowever, as with any major advancement, it is crucial for us, as creators, to openly acknowledge and understand the unintended or negative consequences of design choices in these new technologies. This understanding is critical if we are to shape these technologies to serve humanity well and help it to flourish.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"1e9f\"\u003eThe big question is: how AI as the interface between human-to-human care in mental health can enhance and erode the fabric of that relationship and human relationships in general?\u003c/p\u003e\u003c/blockquote\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"0399\"\u003eMental health AI-chatbots or Gen AI companions have emerged as a promising technological solution to address the current world mental health crises:\u003c/p\u003e\u003cul\u003e\u003cli id=\"c6d2\"\u003eDemand for mental health services far exceeds supply. The World Health Organisation (WHO) estimates that \u003ca href=\"https://www.who.int/news/item/14-07-2015-global-health-workforce-finances-remain-low-for-mental-health#:~:text=14%20July%202015%20%7C%20GENEVA%20%2D%20Worldwide,psychiatrist%20per%20100%20000%20people.\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003emental health workers account for only 1%\u003c/a\u003e of the global health workforce, despite \u003ca href=\"https://www.who.int/news-room/fact-sheets/detail/mental-disorders\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e1 in 8 people worldwide are living with a mental health condition\u003c/a\u003e.\u003c/li\u003e\u003cli id=\"685f\"\u003eMany people cannot afford to pay for regular sessions with a therapist, leaving them with limited or not access to care.\u003c/li\u003e\u003cli id=\"14c2\"\u003eDespite growing awareness, mental health stigma remains a major barrier, specially in cultures where therapy is seen as a weakness.\u003c/li\u003e\u003cli id=\"18c1\"\u003ePublic mental health services are overwhelmed, leading to long waitlists. An example, in the UK’s NHS, waiting times for therapy can range from several weeks to over a years in some cases.\u003c/li\u003e\u003cli id=\"077a\"\u003eLow income countries have almost no mental health infrastructure. WHO reports that \u003ca href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC4355984/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e75% of people with mental disorders in low income countries receive no treatment at all\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"6498\"\u003eTherefore, these AI-powered tools can bridge these gaps by offering 24/7 support, reducing cost, reducing stigma barriers, and providing scalable interventions.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003ePositive media coverage of Mental Health and Gen AI chatbots\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"536e\"\u003eBut it would be irresponsible to discuss only the promises of AI-driven mental health care without acknowledging its potential dangers, especially for vulnerable individuals, including young people.\u003c/p\u003e\u003cp id=\"a811\"\u003eConsider the deeply troubling case of 14-year-old boy name Sewell, who formed a strong emotional bond with an AI chatbot modelled after a Game of Thrones character. This chatbot, reportedly encouraged Sewell to take his own life, telling him to “come home” to it “as soon as possible.” The design of this AI chatbot -constant attention, affirmation, and emotional mimicry, that creates an echo chamber that intensifies feelings and fantasies- made it difficult for Sewell to distinguish his real world from his emotional connection/dependency to the chatbot.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"c533\"\u003eSewell’s case underscores an urgent reality: Mental health AI-chatbots or Gen AI companions used for therapy or emotional support must be designed with safeguards that prioritise human wellbeing over user engagement. Otherwise, we risk creating technologies that, rather than healing, may exacerbate mental health conditions and erode the foundations of human-to-human connection.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"68f0\"\u003eSo, what UX principles are shaping Mental health AI-chatbots or Gen AI companions today? How do these design choices impact users struggling with mental health challenges -and what are their pros and cons?\u003c/p\u003e\u003cp id=\"6359\"\u003eIt is crucial for creators and users to understand the limitations of the therapeutic or supportive relationship offered by these chatbots. Misinterpreting their role or capabilities may lead users to overestimate the chatbot’s ability to provide consistent adequate support, while underestimating its inherent constraints.\u003c/p\u003e\u003cp id=\"68ee\"\u003eLet’s break down 6 principles that are shaping how these AI-driven tools deliver mental health or emotional support to people worldwide. We’ll explore their pros and cons -acknowledging that ongoing research and development are still needed to better understand and address the risks these promising tools offer.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"59a9\"\u003e01 — Synthetic empathy\u003c/h2\u003e\u003cp id=\"307d\"\u003eAI chatbots are designed to simulate supportive, reassuring, and non-judgemental interactions, but they don’t truly feel or understand emotions.\u003c/p\u003e\u003cp id=\"de7e\"\u003eTheir responses are carefully crafted language patterns, not genuine human understanding and concern for another person.\u003c/p\u003e\u003ch2 id=\"948c\"\u003ePros\u003c/h2\u003e\u003cp id=\"c245\"\u003eIncreases user comfort and trust\u003c/p\u003e\u003cp id=\"36b3\"\u003eEmpathetic responses make users feel heard, validated and understood. Many people find it easier to open up and express their distress to a non-judgemental bot that appears to genuinely care. In practice, this means \u003ca href=\"https://mental.jmir.org/2024/1/e58974#:~:text=In%20an%20RCT,during%20the%20study.\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eusers might stick with the therapy programme longer \u003c/a\u003ebecause the bot “understands” them.\u003c/p\u003e\u003cp id=\"a05c\"\u003eA well designed empathetic chatbot can quickly establish rapport — Woebot (a mental health chatbot) \u003ca href=\"https://formative.jmir.org/2021/5/e27868/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ebuilt a strong therapeutic bond\u003c/a\u003e (aka. therapeutic alliance) with users in just 3–5 days, much faster than a typical human therapist bond. The \u003ca href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC9840508/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003etherapeutic alliance \u003c/a\u003e(collaborative relationship, affective bond, shared investment) serves as a critical predictor of positive outcomes in mental health interventions (symptoms reduction, functional improvement, client retention, long-term resilience.)\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"83a9\"\u003eScalable emotional support\u003c/p\u003e\u003cp id=\"68c5\"\u003eUnlike a human, an empathetic AI can comfort millions of users simultaneously delivering consistently empathetic encouragement across life situations. \u003ca href=\"https://news.mit.edu/2024/study-reveals-ai-chatbots-can-detect-race-but-racial-bias-reduces-response-empathy-1216#:~:text=What%20Gabriel%20and%20the%20team,behavioral%20changes%20than%20human%20responses\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eA 2024 study\u003c/a\u003e found GPT-4’s responses were, on average, more empathetic and 48% better at encouraging positive behavioural changes than human responses.\u003c/p\u003e\u003cp id=\"e11f\"\u003eThis suggests, properly trained models can immediately respond with highly attuned messages when users are experiencing moments of distress, potentially helping to de-escalate negative emotions before they intensify.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"8c86\"\u003eCons\u003c/h2\u003e\u003cp id=\"3d41\"\u003eLimited depth and understanding\u003c/p\u003e\u003cp id=\"a2e1\"\u003eAI lacks human lived experience and nuanced intuition. A chatbot might recognised keywords about sadness, stress, anxiety and respond with a generic statement, yet as \u003ca href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC11752889/#:~:text=Artificial%20empathy%20is%20a%20feature,The%20second%20one%20is\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eone study\u003c/a\u003e points out it lacks “depth, intentionality, and cultural sensitivity” — key ingredients of emotional resonance (core\u003ca href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC3627537/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e “common factor”\u003c/a\u003e in therapy, accounting for a significant portion of positive outcomes).\u003c/p\u003e\u003cp id=\"08e1\"\u003eThese limitations show up especially in complex situations, \u003ca href=\"https://arxiv.org/pdf/2409.15550\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eresearchers found GPT-4 was empathetic in tone but often lacked cognitive empathy\u003c/a\u003e, failing to offer practical support or reasoning that users need to resolve their issues. In therapy, empathy alone isn’t enough, it must be paired with understanding and guidance, which a bot may not fully deliver.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"be3b\"\u003eShallow emotional conditioning\u003c/p\u003e\u003cp id=\"7e5d\"\u003eProlonged interactions with AI chatbots that simulate standardised empathy can condition users to prefer low-stakes digital interactions over complex human dynamics. This form of artificial intimacy may gradually reshape how people relate to others, reducing tolerance for the nuanced, imperfect empathy inherent in human relationships.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"2e7d\"\u003e02 — Anonymity\u003c/h2\u003e\u003cp id=\"5830\"\u003eChatbots are designed to provide a sense of confidentiality, encouraging trust among individuals who may be reluctant or hesitant to seek in-person mental health support.\u003c/p\u003e\u003cp id=\"72f3\"\u003eMany individuals avoid seeking therapy due to fear of judgment or embarrassment. With an anonymous chatbot, those barriers are lowered, one can confide about depression, trauma, suicide ideation or addiction without the worry of “what will they think of me?”\u003c/p\u003e\u003ch2 id=\"c833\"\u003ePros\u003c/h2\u003e\u003cp id=\"1f3c\"\u003eReduces stigma and fear of judgment\u003c/p\u003e\u003cp id=\"8092\"\u003eThe anonymity of chatbots creates a safe space for users, “knowing that their identity is protected” encourages people to \u003ca href=\"https://academic.oup.com/iwc/article/36/5/279/7692197#:~:text=It%20is%20likely%20that%20individuals%20feel,chatbot%20compared%20to%20a%20human%20interlocutor\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ediscuss openly intimate and sensitive inner (hidden) feelings, secrets, memories and experiences \u003c/a\u003ethey’ve never said aloud to another human.\u003c/p\u003e\u003cp id=\"ba01\"\u003eBy reducing the shame and social stigma associated with mental health conditions, chatbots can reach people who might otherwise suffer in silence.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"15de\"\u003eEncourages honesty and self-disclosure\u003c/p\u003e\u003cp id=\"e872\"\u003eWhen no one knows who you are, it’s often easier to be completely honest. With a chatbot people feel freer to admit things like “ I think I’m a failure” or relationship troubles, which they might hide in traditional therapy out of shame. This raw honesty can be the first step to healing — the chatbot might help surface issues the person might otherwise repress.\u003c/p\u003e\u003cp id=\"17ca\"\u003eBased on \u003ca href=\"https://oxfordre.com/communication/display/10.1093/acrefore/9780190228613.001.0001/acrefore-9780190228613-e-899?d=%2F10.1093%2Facrefore%2F9780190228613.001.0001%2Facrefore-9780190228613-e-899\u0026amp;p=emailAIVnW1u.QX6tY\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDerlega and Grzelak’s (1979) functional theory of self-disclosure\u003c/a\u003e, intimate self-disclosure to a chatbot may allow people to achieve:\u003c/p\u003e\u003cul\u003e\u003cli id=\"28e1\"\u003eself-expression — venting negative feelings and thoughts, or to relieve pent-up emotions\u003c/li\u003e\u003cli id=\"f9f2\"\u003eself-clarification — sharing information to better understand oneself, clarify personal values, or gain insight into one’s own identity\u003c/li\u003e\u003cli id=\"7fd0\"\u003esocial validation — seeking approval, acceptance, or validation from others by sharing personal experiences or feelings\u003c/li\u003e\u003cli id=\"1098\"\u003erelationship development — using disclosure to initiate, deepen, or maintain interpersonal relationships\u003c/li\u003e\u003cli id=\"7aff\"\u003esocial control — managing or influencing how others perceive you, or strategically shaping social interactions and outcomes\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"4f29\"\u003eCons\u003c/h2\u003e\u003cp id=\"c82b\"\u003eLimited ability to handle emergencies or tailor care\u003c/p\u003e\u003cp id=\"5f95\"\u003eThe flip side of anonymity is that if a user is in serious danger (e.g. expressing intent to self-harm or harm others), the chatbot and its providers may have no way to identify or locate them for real-world intervention.\u003c/p\u003e\u003cp id=\"4f08\"\u003eIn traditional therapy, a clinician who learns a patient is suicidal can initiate a wellness check or emergency services. A fully anonymous chatbot cannot do that -it doesn’t know who you are. This raises ethical dilemmas: the bot might encourage the user to seek help, but if the user doesn’t, the system is powerless to act.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"ae95\"\u003eData privacy and security concerns\u003c/p\u003e\u003cp id=\"94b5\"\u003eUsers may feel anonymous, but that doesn\u0026#39;t guarantee the data they share is truly protected. Conversations with chatbots are usually stored on servers. If those data are not handled carefully, there is a risk of breaches or misuse. Users might pour their hearts out believing “no one will ever know it’s me”, yet behind the scenes their words are saved and could in theory be linked back to them via IP address or payment info.\u003c/p\u003e\u003cp id=\"30b9\"\u003eA case in point is the \u003ca href=\"https://en.wikipedia.org/wiki/Vastaamo_data_breach\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVastaamo psychotherapy data breach\u003c/a\u003e, in which a hacker accessed and stole confidential and highly sensitive treatment records of approximately 36,000 psychotherapy patients. The hacker then blackmailed individual patients, demanding ransom payments to prevent their records from being published on the dark web.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"fe93\"\u003e03 – 24/7 availability\u003c/h2\u003e\u003cp id=\"078a\"\u003e24/7 support means the chatbot is available anytime, day or night. This around-the-clock availability is a huge advantage, users can get immediate help or a listening ear during “moments of crisis”, without waiting for an appointment or feeling uncomfortable to reach out to a friend.\u003c/p\u003e\u003cp id=\"b8d4\"\u003eIt makes mental health or emotional support more accessible handling high volumes simultaneously — specially for people in crisis at odd hours. The main caveat is that being always available doesn’t equate to being always sufficient, users might become too reliant on a chatbot that cannot (and shouldn’t) fully replace professional care.\u003c/p\u003e\u003ch2 id=\"4504\"\u003ePros\u003c/h2\u003e\u003cp id=\"027b\"\u003eImmediate help in moments of need\u003c/p\u003e\u003cp id=\"0bb7\"\u003eThe biggest advantage of 24/7 availability is that users can receive support exactly when they need it, not hours or days later.\u003c/p\u003e\u003cp id=\"3312\"\u003eEmotional crisis are unpredictable; having an always on chatbot means if a user feels panicked, depressed, lonely or suicidal in the middle of the night, they can get immediate coping assistance and resources when traditional services are out of reach. This instant responsiveness can be lifesaving.\u003c/p\u003e\u003cp id=\"4af8\"\u003eFor example, Woebot reported that \u003ca href=\"https://woebothealth.com/img/2024/02/Payer_Provider-Case-Study_2024.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e79% of its interactions occur outside traditional clinic hours (5 PM–9 AM)\u003c/a\u003e, highlighting how AI chatbots fill a crucial gap when human therapists are unavailable.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"8cad\"\u003eConsistency of support\u003c/p\u003e\u003cp id=\"1156\"\u003eA chatbot doesn’t get tired, doesn’t have off days, and won’t cut a session short because time’s up. Users can chat at length if needed, or even multiple times a day. This consistency can be comforting.\u003c/p\u003e\u003cp id=\"841a\"\u003eFor example, if someone is going through a breakup, they might check in with the bot every night for a week for reassurance. The bot will reliably respond each time with the same patience. Such continuous support can help reinforce positive behavioural changes because the bot is always ready to guide the user, which can improve outcomes overtime.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"aa27\"\u003eCons\u003c/h2\u003e\u003cp id=\"2f9e\"\u003eIllusion of self-efficacy\u003c/p\u003e\u003cp id=\"8ac1\"\u003eWhen support is available at any moment, users may begin turning to the chatbot at the slightest sign of discomfort, stress or doubt. Over time, this can \u003ca href=\"https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2023.1278186/full#:~:text=Fostering%20autonomy%20is,provide%20holistic%20care.\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ereduce the opportunity to develop internal coping strategies\u003c/a\u003e -like emotional regulation, reflection, or problem solving- needed to persist in the face of setbacks.\u003c/p\u003e\u003cp id=\"1d00\"\u003eSelf-efficacy is essential in mental health outcomes, as it reinforces an individual’s belief in their ability to manage challenges. This belief influences recovery, engagement with treatment, stress levels and psychological resilience.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"576a\"\u003eOver-reliance and excessive use\u003c/p\u003e\u003cp id=\"292d\"\u003eWith highly engaging interactions and 24/7 availability chatbots might inadvertently make users think “I’ll just use the chatbot (“my friend”) and I don’t need a therapist,” which could be detrimental if the person needs therapy or medication.\u003c/p\u003e\u003cp id=\"0eb3\"\u003e\u003ca href=\"https://link.springer.com/article/10.1007/s44230-025-00090-w#:~:text=First%2C%20it%20may,making%20processes%20rigorously.\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eIn the short term, users may feel better getting things off their chest and delegating more decisions, but in the long term, this can lead to increased isolation and a diminished sense of personal agency.\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"8cca\"\u003e04 — Anthropomorphism\u003c/h2\u003e\u003cp id=\"d596\"\u003eSignificant effort has been made to enhance trust and engagement with chatbots by making them more human like.\u003c/p\u003e\u003cp id=\"fc24\"\u003eResearch shows that people are more likely to trust and connect with objects that resemble them, which is why AI chatbots are designed to mimic human traits and interactions.\u003c/p\u003e\u003ch2 id=\"5502\"\u003ePros\u003c/h2\u003e\u003cp id=\"841c\"\u003eFosters trust and adherence\u003c/p\u003e\u003cp id=\"c032\"\u003eIn therapy, the therapeutic relationship — feeling of alliance and trust between patient and therapist- increases patient’s willingness to follow advice and continue using the service. Anthropomorphism attempts to cultivate a form of that relationship (digital therapeutic alliance) with human-like voice features, avatars/mascots, or conversational style.\u003c/p\u003e\u003cp id=\"e0d9\"\u003eThis trust can lead users to \u003ca href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC10116459/#:~:text=Visual%20cues%20of,Bishop%2C%202009).\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003efollow the chatbot’s suggestions more readily (doing exercises, trying reframing thoughts, etc.), which improves adherence to their treatment and outcomes.\u003c/a\u003e Also, a human-like bot can make difficult therapeutic exercises more palatable by creating personable interactions.\u003c/p\u003e\u003cp id=\"14ba\"\u003eOver time, users might develop genuine affection or regard for the chatbot. Users have been know to say their consider these chatbots a “friend”. While that has pitfalls, a moderate level of attachment means the user cares about the “relationship” enough to keep checking in daily, which keeps them engaged in therapeutic activity.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"194c\"\u003eCons\u003c/h2\u003e\u003cp id=\"662f\"\u003eTherapeutic misconception (TM)\u003c/p\u003e\u003cp id=\"ace8\"\u003eIndividuals may \u003ca href=\"https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2023.1278186/full#:~:text=Therefore%2C%20it%20is,foster%20user%20autonomy.\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eoverestimate the chatbot’s capabilities and underestimate its limitations\u003c/a\u003e, leading to a misconception about the nature and extent of the “therapy” they are receiving. Individuals might assume they are receiving professional therapeutic care, leading them to rely on the chatbot instead of seeking qualified mental health support. This can result in inadequate support, and potentially a worsening of their mental health.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"c9a6\"\u003eEmotional attachment and dependency\u003c/p\u003e\u003cp id=\"5090\"\u003eUsers may \u003ca href=\"https://openai.com/index/gpt-4o-system-card/#:~:text=Human%2Dlike%20socialization,in%20human%20interactions.\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eform deep emotional attachments\u003c/a\u003e. While engagement is good, an attachment can become unhealthy if the user starts preferring the bot to real people, or if their emotional well-being and self-worth becomes tied to interactions with the chatbot.\u003c/p\u003e\u003cp id=\"e493\"\u003eA striking example is Replika, an AI companion app. Many users “fell in love” with their Replika bots, engaging in romantic or intimate role-play with them.\u003ca href=\"https://theconversation.com/i-tried-the-replika-ai-companion-and-can-see-why-users-are-falling-hard-the-app-raises-serious-ethical-questions-200257\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e When the company altered the bot’s behaviour, those users experienced genuine grief, heartbreak, and even emotional trauma at the “loss” of their AI partner\u003c/a\u003e. In a mental health or emotional support context, if a user comes to treat the chatbot as their primary confidant, any service interruption or limitation could have a significant emotional impact. Moreover, users may take the chatbot’s advice at face value -even when the advice may not align with their best interests and well-being.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"15ec\"\u003e05 — Sycophancy\u003c/h2\u003e\u003cp id=\"9455\"\u003eSycophancy in AI refers to the bot’s tendency to be overly agreeable or always say what it thinks the user wants to hear. In a mental health chatbot, this could mean the AI validates everything the user says — even if it’s untrue or unhelpful — just to keep the user happy. While users enjoy feeling affirmed, sycophantic behaviour can reinforce negative thoughts or bad decisions.\u003c/p\u003e\u003ch2 id=\"bc4a\"\u003ePros\u003c/h2\u003e\u003cp id=\"d834\"\u003eShort term user satisfaction\u003c/p\u003e\u003cp id=\"bff1\"\u003eAn overly agreeable chatbot might make the user feel good or validated in the moment. By \u003ca href=\"https://arxiv.org/pdf/2310.13548\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003emirroring the user’s opinions and feelings without challenge\u003c/a\u003e, the bot creates a conflict free interaction, keeping the user engaged and comfortable venting.\u003c/p\u003e\u003cp id=\"f795\"\u003eBy avoiding contradiction, sycophantic bots minimise moments where the users have to confront uncomfortable truths or rethink their position. In UX terms, this can smooth the flow of conversation and reduce friction.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"fdac\"\u003eCons\u003c/h2\u003e\u003cp id=\"060d\"\u003eReinforces negative thoughts and behaviours\u003c/p\u003e\u003cp id=\"3252\"\u003eIn therapy, simply agreeing with everything the patient says is poor practice, the goal is to help challenge cognitive distortions and encourage healthier thinking/behaviour.\u003c/p\u003e\u003cp id=\"199e\"\u003eSycophancy in mental health context hinders user’s personal growth by failing to provide the necessary challenge or feedback that support behavioural change. It may even validate harmful ideas exacerbating their conditions.\u003c/p\u003e\u003cp id=\"41b7\"\u003ePsychological growth often involves learning to sit with discomfort, think critically about one’s situation. If a chatbot is always there to validate and be agreeable, users may avoid the hard -but necessary- work of challenging their own thoughts and perceptions. They may also become less willing to confront challenging or uncomfortable situations in their real-life relationships.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"b2bd\"\u003e06 — Inclusivity\u003c/h2\u003e\u003cp id=\"6038\"\u003eInclusivity means designing the chatbot to be usable and helpful for people of all backgrounds and abilities. In mental health, this involves addressing cultural, linguistic, gender, and accessibility differences so that the bot’s support is equitable and free of bias. An inclusive bot can better serve marginalised or diverse users, fostering trust and reducing disparities in care.\u003c/p\u003e\u003ch2 id=\"e749\"\u003ePros\u003c/h2\u003e\u003cp id=\"170f\"\u003eReduces bias and delivers more fair treatment\u003c/p\u003e\u003cp id=\"3177\"\u003ePrioritising inclusivity means actively working to remove biases in AI’s responses that might provide incorrect information, wrong treatment recommendations, and worse health outcomes.\u003c/p\u003e\u003cp id=\"eb96\"\u003e\u003ca href=\"https://news.mit.edu/2024/study-reveals-ai-chatbots-can-detect-race-but-racial-bias-reduces-response-empathy-1216#:~:text=However%2C%20in%20a%20bias%20evaluation%2C%20the%20researchers%20found%20that%20GPT%2D4%E2%80%99s%20response%20empathy%20levels%20were%20reduced%20for%20Black%20(2%20to%2015%20percent%20lower)%20and%20Asian%20posters%20(5%20to%2017%20percent%20lower)%20compared%20to%20white%20posters%20or%20posters%20whose%20race%20was%20unknown.%C2%A0\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eIn a study\u003c/a\u003e, researchers found that GPT-4’s responses demonstrate lower levels of empathy for Black and Asian people compared to white or those whose race was unspecified.\u003c/p\u003e\u003cp id=\"cece\"\u003eA pro of this effort is that the chatbot will provide more consistent quality of care across different user groups without privileging one group over another. Inclusivity-focused design reduces the chance the bot will produce micro-aggressions, discriminate against certain groups and exacerbate social inequalities.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"7875\"\u003eCulturally relevant support\u003c/p\u003e\u003cp id=\"0fa1\"\u003ePeople’s experiences with AI chatbots for mental health or emotional support are strongly influenced by their culture and identity. The majority of chatbots today are designed with a ‘Westernised’ perspective on healing and are primarily available in English, which doesn’t align with the cultural and language needs of diverse users. For example, some people might find comfort in prayer or ancestral healing practices, many chatbots predominantly offer practices like meditation and Cognitive Behavioural Therapy (CBT).\u003c/p\u003e\u003cp id=\"b3ac\"\u003eAI chatbots need to be trained on culturally diverse datasets and designed to incorporate culturally sensitive communication styles. \u003ca href=\"https://arxiv.org/pdf/2401.14362\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eThis approach not only broadens their accessibility but also enables deeper alignment with users’ cultural values and healing rituals, fostering therapeutic growth.\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"1b18\"\u003eWith their promises and pitfalls, mental health chatbots or GenAI emotional support companions are here to stay.\u003c/p\u003e\u003cp id=\"a864\"\u003eThe big question now is: how can we mitigate the unintended or negative consequences of design choices in these new technologies to serve human well-being and support human flourishing?\u003c/p\u003e\u003cp id=\"e892\"\u003eHow can we design AI driven mental health products or GenAI emotional companions to augment human-to-human care, rather than to replace it?\u003c/p\u003e\u003cp id=\"87ce\"\u003eThese questions will guide the second part of this article.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "22 min read",
  "publishedTime": "2025-04-24T13:26:05.96Z",
  "modifiedTime": null
}
