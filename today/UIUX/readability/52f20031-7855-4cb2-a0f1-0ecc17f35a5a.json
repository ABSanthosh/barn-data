{
  "id": "52f20031-7855-4cb2-a0f1-0ecc17f35a5a",
  "title": "On-Device AI: Building Smarter, Faster, And Private Applications",
  "link": "https://smashingmagazine.com/2025/01/on-device-ai-building-smarter-faster-private-applications/",
  "description": "Shouldn’t there be a way to keep your apps or project data private and improve performance by reducing server latency? This is what on-device AI is designed to solve. It handles AI processing locally, right on your device, without connecting to the internet and sending data to the cloud. In this article, Joas Pambou explains what on-device AI is, why it’s important, the tools to build this type of technology, and how it can change the way we use technology every day.",
  "author": "Joas Pambou",
  "published": "Thu, 16 Jan 2025 13:00:00 GMT",
  "source": "https://www.smashingmagazine.com/feed",
  "categories": null,
  "byline": "About The Author",
  "length": 15200,
  "excerpt": "Shouldn’t there be a way to keep your apps or project data private and improve performance by reducing server latency? This is what on-device AI is designed to solve. It handles AI processing locally, right on your device, without connecting to the internet and sending data to the cloud. In this article, Joas Pambou explains what on-device AI is, why it’s important, the tools to build this type of technology, and how it can change the way we use technology every day.",
  "siteName": "Smashing Magazine",
  "favicon": "https://smashingmagazine.com/images/favicon/apple-touch-icon.png",
  "text": "11 min readAI, Tools, LLM, Techniques, AppsShouldn’t there be a way to keep your apps or project data private and improve performance by reducing server latency? This is what on-device AI is designed to solve. It handles AI processing locally, right on your device, without connecting to the internet and sending data to the cloud. In this article, Joas Pambou explains what on-device AI is, why it’s important, the tools to build this type of technology, and how it can change the way we use technology every day.It’s not too far-fetched to say AI is a pretty handy tool that we all rely on for everyday tasks. It handles tasks like recognizing faces, understanding or cloning speech, analyzing large data, and creating personalized app experiences, such as music playlists based on your listening habits or workout plans matched to your progress.But here’s the catch:Where AI tool actually lives and does its work matters a lot.Take self-driving cars, for example. These types of cars need AI to process data from cameras, sensors, and other inputs to make split-second decisions, such as detecting obstacles or adjusting speed for sharp turns. Now, if all that processing depends on the cloud, network latency connection issues could lead to delayed responses or system failures. That’s why the AI should operate directly within the car. This ensures the car responds instantly without needing direct access to the internet.This is what we call On-Device AI (ODAI). Simply put, ODAI means AI does its job right where you are — on your phone, your car, or your wearable device, and so on — without a real need to connect to the cloud or internet in some cases. More precisely, this kind of setup is categorized as Embedded AI (EMAI), where the intelligence is embedded into the device itself.Okay, I mentioned ODAI and then EMAI as a subset that falls under the umbrella of ODAI. However, EMAI is slightly different from other terms you might come across, such as Edge AI, Web AI, and Cloud AI. So, what’s the difference? Here’s a quick breakdown:Edge AIIt refers to running AI models directly on devices instead of relying on remote servers or the cloud. A simple example of this is a security camera that can analyze footage right where it is. It processes everything locally and is close to where the data is collected.Embedded AIIn this case, AI algorithms are built inside the device or hardware itself, so it functions as if the device has its own mini AI brain. I mentioned self-driving cars earlier — another example is AI-powered drones, which can monitor areas or map terrains. One of the main differences between the two is that EMAI uses dedicated chips integrated with AI models and algorithms to perform intelligent tasks locally.Cloud AIThis is when the AI lives and relies on the cloud or remote servers. When you use a language translation app, the app sends the text you want to be translated to a cloud-based server, where the AI processes it and the translation back. The entire operation happens in the cloud, so it requires an internet connection to work.Web AIThese are tools or apps that run in your browser or are part of websites or online platforms. You might see product suggestions that match your preferences based on what you’ve looked at or purchased before. However, these tools often rely on AI models hosted in the cloud to analyze data and generate recommendations.The main difference? It’s about where the AI does the work: on your device, nearby, or somewhere far off in the cloud or web.What Makes On-Device AI UsefulOn-device AI is, first and foremost, about privacy — keeping your data secure and under your control. It processes everything directly on your device, avoiding the need to send personal data to external servers (cloud). So, what exactly makes this technology worth using?Real-Time ProcessingOn-device AI processes data instantly because it doesn’t need to send anything to the cloud. For example, think of a smart doorbell — it recognizes a visitor’s face right away and notifies you. If it had to wait for cloud servers to analyze the image, there’d be a delay, which wouldn’t be practical for quick notifications.Enhanced Privacy and SecurityPicture this: You are opening an app using voice commands or calling a friend and receiving a summary of the conversation afterward. Your phone processes the audio data locally, and the AI system handles everything directly on your device without the help of external servers. This way, your data stays private, secure, and under your control.Offline FunctionalityA big win of ODAI is that it doesn’t need the internet to work, which means it can function even in areas with poor or no connectivity. You can take modern GPS navigation systems in a car as an example; they give you turn-by-turn directions with no signal, making sure you still get where you need to go.Reduced LatencyODAI AI skips out the round trip of sending data to the cloud and waiting for a response. This means that when you make a change, like adjusting a setting, the device processes the input immediately, making your experience smoother and more responsive.The Technical Pieces Of The On-Device AI PuzzleAt its core, ODAI uses special hardware and efficient model designs to carry out tasks directly on devices like smartphones, smartwatches, and Internet of Things (IoT) gadgets. Thanks to the advances in hardware technology, AI can now work locally, especially for tasks requiring AI-specific computer processing, such as the following:Neural Processing Units (NPUs)These chips are specifically designed for AI and optimized for neural nets, deep learning, and machine learning applications. They can handle large-scale AI training efficiently while consuming minimal power.Graphics Processing Units (GPUs)Known for processing multiple tasks simultaneously, GPUs excel in speeding up AI operations, particularly with massive datasets.Here’s a look at some innovative AI chips in the industry:ProductOrganizationKey FeaturesSpiking Neural Network ChipIndian Institute of TechnologyUltra-low power consumptionHierarchical Learning ProcessorCeromorphicAlternative transistor structureIntelligent Processing Units (IPUs)GraphcoreMultiple products targeting end devices and cloudKatana Edge AISynapticsCombines vision, motion, and sound detectionET-SoC-1 ChipEsperanto TechnologyBuilt on RISC-V for AI and non-AI workloadsNeuRRAMCEA–LetiBiologically inspired neuromorphic processor based on resistive RAM (RRAM)These chips or AI accelerators show different ways to make devices more efficient, use less power, and run advanced AI tasks.Techniques For Optimizing AI ModelsCreating AI models that fit resource-constrained devices often requires combining clever hardware utilization with techniques to make models smaller and more efficient. I’d like to cover a few choice examples of how teams are optimizing AI for increased performance using less energy.Meta’s MobileLLMMeta’s approach to ODAI introduced a model built specifically for smartphones. Instead of scaling traditional models, they designed MobileLLM from scratch to balance efficiency and performance. One key innovation was increasing the number of smaller layers rather than having fewer large ones. This design choice improved the model’s accuracy and speed while keeping it lightweight. You can try out the model either on Hugging Face or using vLLM, a library for LLM inference and serving.QuantizationThis simplifies a model’s internal calculations by using lower-precision numbers, such as 8-bit integers, instead of 32-bit floating-point numbers. Quantization significantly reduces memory requirements and computation costs, often with minimal impact on model accuracy.PruningNeural networks contain many weights (connections between neurons), but not all are crucial. Pruning identifies and removes less important weights, resulting in a smaller, faster model without significant accuracy loss.Matrix DecompositionLarge matrices are a core component of AI models. Matrix decomposition splits these into smaller matrices, reducing computational complexity while approximating the original model’s behavior.Knowledge DistillationThis technique involves training a smaller model (the “student”) to mimic the outputs of a larger, pre-trained model (the “teacher”). The smaller model learns to replicate the teacher’s behavior, achieving similar accuracy while being more efficient. For instance, DistilBERT successfully reduced BERT’s size by 40% while retaining 97% of its performance.Technologies Used For On-Device AIWell, all the model compression techniques and specialized chips are cool because they’re what make ODAI possible. But what’s even more interesting for us as developers is actually putting these tools to work. This section covers some of the key technologies and frameworks that make ODAI accessible.MediaPipe SolutionsMediaPipe Solutions is a developer toolkit for adding AI-powered features to apps and devices. It offers cross-platform, customizable tools that are optimized for running AI locally, from real-time video analysis to natural language processing.At the heart of MediaPipe Solutions is MediaPipe Tasks, a core library that lets developers deploy ML solutions with minimal code. It’s designed for platforms like Android, Python, and Web/JavaScript, so you can easily integrate AI into a wide range of applications.MediaPipe also provides various specialized tasks for different AI needs:LLM Inference APIThis API runs lightweight large language models (LLMs) entirely on-device for tasks like text generation and summarization. It supports several open models like Gemma and external options like Phi-2.Object DetectionThe tool helps you Identify and locate objects in images or videos, which is ideal for real-time applications like detecting animals, people, or objects right on the device.Image SegmentationMediaPipe can also segment images, such as isolating a person from the background in a video feed, allowing it to separate objects in both single images (like photos) and continuous video streams (like live video or recorded footage).LiteRTLiteRT or Lite Runtime (previously called TensorFlow Lite) is a lightweight and high-performance runtime designed for ODAI. It supports running pre-trained models or converting TensorFlow, PyTorch, and JAX models to a LiteRT-compatible format using AI Edge tools.Model ExplorerModel Explorer is a visualization tool that helps you analyze machine learning models and graphs. It simplifies the process of preparing these models for on-device AI deployment, letting you understand the structure of your models and fine-tune them for better performance.Model-Explorer: Visualize ML models and graphs to prepare and optimize them for On-Device ai. (Image source: Google) (Large preview)You can use Model Explorer locally or in Colab for testing and experimenting.ExecuTorchIf you’re familiar with PyTorch, ExecuTorch makes it easy to deploy models to mobile, wearables, and edge devices. It’s part of the PyTorch Edge ecosystem, which supports building AI experiences for edge devices like embedded systems and microcontrollers.Large Language Models For On-Device AIGemini is a powerful AI model that doesn’t just excel in processing text or images. It can also handle multiple types of data seamlessly. The best part? It’s designed to work right on your devices.For on-device use, there’s Gemini Nano, a lightweight version of the model. It’s built to perform efficiently while keeping everything private.What can Gemini Nano do?Call Notes on Pixel devicesThis feature creates private summaries and transcripts of conversations. It works entirely on-device, ensuring privacy for everyone involved.Pixel Recorder appWith the help of Gemini Nano and AICore, the app provides an on-device summarization feature, making it easy to extract key points from recordings.TalkBackEnhances the accessibility feature on Android phones by providing clear descriptions of images, thanks to Nano’s multimodal capabilities.Note: It’s similar to an application we built using LLaVA in a previous article.Gemini Nano is far from the only language model designed specifically for ODAI. I’ve collected a few others that are worth mentioning:ModelDeveloperResearch PaperOctopus v2NexaAIOn-device language model for super agentOpenELMApple ML ResearchA significant large language model integrated within iOS to enhance application functionalitiesFerret-v2AppleFerret-v2 significantly improves upon its predecessor, introducing enhanced visual processing capabilities and an advanced training regimenMiniCPMTsinghua UniversityA GPT-4V Level Multimodal LLM on Your PhonePhi-3MicrosoftPhi-3 Technical Report: A Highly Capable Language Model Locally on Your PhoneThe Trade-Offs of Using On-Device AIBuilding AI into devices can be exciting and practical, but it’s not without its challenges. While you may get a lightweight, private solution for your app, there are a few compromises along the way. Here’s a look at some of them:Limited ResourcesPhones, wearables, and similar devices don’t have the same computing power as larger machines. This means AI models must fit within limited storage and memory while running efficiently. Additionally, running AI can drain the battery, so the models need to be optimized to balance power usage and performance.Data and UpdatesAI in devices like drones, self-driving cars, and other similar devices process data quickly, using sensors or lidar to make decisions. However, these models or the system itself don’t usually get real-time updates or additional training unless they are connected to the cloud. Without these updates and regular model training, the system may struggle with new situations.BiasesBiases in training data are a common challenge in AI, and ODAI models are no exception. These biases can lead to unfair decisions or errors, like misidentifying people. For ODAI, keeping these models fair and reliable means not only addressing these biases during training but also ensuring the solutions work efficiently within the device’s constraints.These aren’t the only challenges of on-device AI. It’s still a new and growing technology, and the small number of professionals in the field makes it harder to implement.ConclusionChoosing between on-device and cloud-based AI comes down to what your application needs most. Here’s a quick comparison to make things clear:AspectOn-Device AICloud-Based AIPrivacyData stays on the device, ensuring privacy.Data is sent to the cloud, raising potential privacy concerns.LatencyProcesses instantly with no delay.Relies on internet speed, which can introduce delays.ConnectivityWorks offline, making it reliable in any setting.Requires a stable internet connection.Processing PowerLimited by device hardware.Leverages the power of cloud servers for complex tasks.CostNo ongoing server expenses.Can incur continuous cloud infrastructure costs.For apps that need fast processing and strong privacy, ODAI is the way to go. On the other hand, cloud-based AI is better when you need more computing power and frequent updates. The choice depends on your project’s needs and what matters most to you. (gg, yk)",
  "image": "https://files.smashing.media/articles/on-device-ai-building-smarter-faster-private-applications/on-device-ai-building-smarter-faster-private-applications.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"article__content\"\u003e\u003cul\u003e\u003cli\u003e11 min read\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://smashingmagazine.com/category/ai\"\u003eAI\u003c/a\u003e,\n\u003ca href=\"https://smashingmagazine.com/category/tools\"\u003eTools\u003c/a\u003e,\n\u003ca href=\"https://smashingmagazine.com/category/llm\"\u003eLLM\u003c/a\u003e,\n\u003ca href=\"https://smashingmagazine.com/category/techniques\"\u003eTechniques\u003c/a\u003e,\n\u003ca href=\"https://smashingmagazine.com/category/apps\"\u003eApps\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003csection aria-label=\"Quick summary\"\u003eShouldn’t there be a way to keep your apps or project data private and improve performance by reducing server latency? This is what on-device AI is designed to solve. It handles AI processing locally, right on your device, without connecting to the internet and sending data to the cloud. In this article, Joas Pambou explains what on-device AI is, why it’s important, the tools to build this type of technology, and how it can change the way we use technology every day.\u003c/section\u003e\u003c/p\u003e\u003cp\u003eIt’s not too far-fetched to say AI is a pretty handy tool that we all rely on for everyday tasks. It handles tasks like recognizing faces, understanding or cloning speech, analyzing large data, and creating personalized app experiences, such as music playlists based on your listening habits or workout plans matched to your progress.\u003c/p\u003e\u003cp\u003eBut here’s the catch:\u003c/p\u003e\u003cblockquote\u003eWhere AI tool actually lives and does its work matters a lot.\u003c/blockquote\u003e\u003cp\u003eTake self-driving cars, for example. These types of cars need AI to process data from cameras, sensors, and other inputs to make split-second decisions, such as detecting obstacles or adjusting speed for sharp turns. Now, if all that processing depends on the cloud, network latency connection issues could lead to delayed responses or system failures. That’s why the AI should operate directly within the car. This ensures the car responds instantly without needing direct access to the internet.\u003c/p\u003e\u003cp\u003eThis is what we call \u003cstrong\u003eOn-Device AI (ODAI)\u003c/strong\u003e. Simply put, ODAI means AI does its job right where you are — on your phone, your car, or your wearable device, and so on — without a real need to connect to the cloud or internet in some cases. More precisely, this kind of setup is categorized as \u003cstrong\u003eEmbedded AI (EMAI)\u003c/strong\u003e, where the intelligence is embedded into the device itself.\u003c/p\u003e\u003cp\u003eOkay, I mentioned ODAI and then EMAI as a subset that falls under the umbrella of ODAI. However, EMAI is slightly different from other terms you might come across, such as Edge AI, Web AI, and Cloud AI. So, what’s the difference? Here’s a quick breakdown:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEdge AI\u003c/strong\u003e\u003cbr/\u003eIt refers to running AI models directly on devices instead of relying on remote servers or the cloud. A simple example of this is a security camera that can analyze footage right where it is. It processes everything locally and is close to where the data is collected.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEmbedded AI\u003c/strong\u003e\u003cbr/\u003eIn this case, AI algorithms are built inside the device or hardware itself, so it functions as if the device has its own mini AI brain. I mentioned self-driving cars earlier — another example is AI-powered drones, which can monitor areas or map terrains. One of the main differences between the two is that EMAI uses dedicated chips integrated with AI models and algorithms to perform intelligent tasks locally.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCloud AI\u003c/strong\u003e\u003cbr/\u003eThis is when the AI lives and relies on the cloud or remote servers. When you use a language translation app, the app sends the text you want to be translated to a cloud-based server, where the AI processes it and the translation back. The entire operation happens in the cloud, so it requires an internet connection to work.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWeb AI\u003c/strong\u003e\u003cbr/\u003eThese are tools or apps that run in your browser or are part of websites or online platforms. You might see product suggestions that match your preferences based on what you’ve looked at or purchased before. However, these tools often rely on AI models hosted in the cloud to analyze data and generate recommendations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe main difference? It’s about \u003cem\u003ewhere\u003c/em\u003e the AI does the work: on your device, nearby, or somewhere far off in the cloud or web.\u003c/p\u003e\u003ch2 id=\"what-makes-on-device-ai-useful\"\u003eWhat Makes On-Device AI Useful\u003c/h2\u003e\u003cp\u003eOn-device AI is, first and foremost, about privacy — keeping your data secure and under your control. It processes everything directly on your device, avoiding the need to send personal data to external servers (cloud). So, what exactly makes this technology worth using?\u003c/p\u003e\u003ch3 id=\"real-time-processing\"\u003eReal-Time Processing\u003c/h3\u003e\u003cp\u003eOn-device AI processes data instantly because it doesn’t need to send anything to the cloud. For example, think of a smart doorbell — it recognizes a visitor’s face right away and notifies you. If it had to wait for cloud servers to analyze the image, there’d be a delay, which wouldn’t be practical for quick notifications.\u003c/p\u003e\u003ch3 id=\"enhanced-privacy-and-security\"\u003eEnhanced Privacy and Security\u003c/h3\u003e\u003cp\u003ePicture this: You are opening an app using voice commands or calling a friend and receiving a summary of the conversation afterward. Your phone processes the audio data locally, and the AI system handles everything directly on your device without the help of external servers. This way, your data stays private, secure, and under your control.\u003c/p\u003e\u003ch3 id=\"offline-functionality\"\u003eOffline Functionality\u003c/h3\u003e\u003cp\u003eA big win of ODAI is that it doesn’t need the internet to work, which means it can function even in areas with poor or no connectivity. You can take modern GPS navigation systems in a car as an example; they give you turn-by-turn directions with no signal, making sure you still get where you need to go.\u003c/p\u003e\u003ch3 id=\"reduced-latency\"\u003eReduced Latency\u003c/h3\u003e\u003cp\u003eODAI AI skips out the round trip of sending data to the cloud and waiting for a response. This means that when you make a change, like adjusting a setting, the device processes the input immediately, making your experience smoother and more responsive.\u003c/p\u003e\u003ch2 id=\"the-technical-pieces-of-the-on-device-ai-puzzle\"\u003eThe Technical Pieces Of The On-Device AI Puzzle\u003c/h2\u003e\u003cp\u003eAt its core, ODAI uses special hardware and efficient model designs to carry out tasks directly on devices like smartphones, smartwatches, and Internet of Things (IoT) gadgets. Thanks to the advances in hardware technology, AI can now work locally, especially for tasks requiring AI-specific computer processing, such as the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNeural Processing Units (NPUs)\u003c/strong\u003e\u003cbr/\u003eThese chips are specifically designed for AI and optimized for neural nets, deep learning, and machine learning applications. They can handle large-scale AI training efficiently while consuming minimal power.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGraphics Processing Units (GPUs)\u003c/strong\u003e\u003cbr/\u003eKnown for processing multiple tasks simultaneously, GPUs excel in speeding up AI operations, particularly with massive datasets.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHere’s a look at some innovative AI chips in the industry:\u003c/p\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eProduct\u003c/th\u003e\u003cth\u003eOrganization\u003c/th\u003e\u003cth\u003eKey Features\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://medium.com/@ansuman_72498/the-promise-of-a-new-dawn-with-neuromorphic-computing-indian-institute-of-sciences-656da6426c2c\"\u003eSpiking Neural Network Chip\u003c/a\u003e\u003c/td\u003e\u003ctd\u003eIndian Institute of Technology\u003c/td\u003e\u003ctd\u003eUltra-low power consumption\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://ceremorphic.com/ceremorphic-exits-stealth-mode-unveils-technology-plans-to-deliver-a-new-architecture-specifically-designed-for-reliable-performance-computing/\"\u003eHierarchical Learning Processor\u003c/a\u003e\u003c/td\u003e\u003ctd\u003eCeromorphic\u003c/td\u003e\u003ctd\u003eAlternative transistor structure\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://www.graphcore.ai/products/ipu\"\u003eIntelligent Processing Units (IPUs)\u003c/a\u003e\u003c/td\u003e\u003ctd\u003eGraphcore\u003c/td\u003e\u003ctd\u003eMultiple products targeting end devices and cloud\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://www.synaptics.com/company/news/synaptics-katana-evk-accelerate-development-ai-sensor-fusion\"\u003eKatana Edge AI\u003c/a\u003e\u003c/td\u003e\u003ctd\u003eSynaptics\u003c/td\u003e\u003ctd\u003eCombines vision, motion, and sound detection\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://www.esperanto.ai\"\u003eET-SoC-1 Chip\u003c/a\u003e\u003c/td\u003e\u003ctd\u003eEsperanto Technology\u003c/td\u003e\u003ctd\u003eBuilt on RISC-V for AI and non-AI workloads\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://www.leti-cea.com/cea-tech/leti/english/Pages/Leti/Projects%20supported/NEURAM3.aspx\"\u003eNeuRRAM\u003c/a\u003e\u003c/td\u003e\u003ctd\u003eCEA–Leti\u003c/td\u003e\u003ctd\u003eBiologically inspired neuromorphic processor based on resistive RAM (RRAM)\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cp\u003eThese chips or AI accelerators show different ways to make devices more efficient, use less power, and run advanced AI tasks.\u003c/p\u003e\u003ch2 id=\"techniques-for-optimizing-ai-models\"\u003eTechniques For Optimizing AI Models\u003c/h2\u003e\u003cp\u003eCreating AI models that fit resource-constrained devices often requires combining clever hardware utilization with techniques to make models smaller and more efficient. I’d like to cover a few choice examples of how teams are optimizing AI for increased performance using less energy.\u003c/p\u003e\u003ch3 id=\"meta-s-mobilellm\"\u003eMeta’s MobileLLM\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2402.14905\"\u003eMeta’s approach\u003c/a\u003e to ODAI introduced a model built specifically for smartphones. Instead of scaling traditional models, they designed MobileLLM from scratch to balance efficiency and performance. One key innovation was increasing the number of smaller layers rather than having fewer large ones. This design choice improved the model’s accuracy and speed while keeping it lightweight. You can try out the model either on Hugging Face or using vLLM, a library for LLM inference and serving.\u003c/p\u003e\u003ch3 id=\"quantization\"\u003eQuantization\u003c/h3\u003e\u003cp\u003eThis simplifies a model’s internal calculations by using lower-precision numbers, such as 8-bit integers, instead of 32-bit floating-point numbers. Quantization significantly reduces memory requirements and computation costs, often with minimal impact on model accuracy.\u003c/p\u003e\u003ch3 id=\"pruning\"\u003ePruning\u003c/h3\u003e\u003cp\u003eNeural networks contain many weights (connections between neurons), but not all are crucial. Pruning identifies and removes less important weights, resulting in a smaller, faster model without significant accuracy loss.\u003c/p\u003e\u003ch3 id=\"matrix-decomposition\"\u003eMatrix Decomposition\u003c/h3\u003e\u003cp\u003eLarge matrices are a core component of AI models. Matrix decomposition splits these into smaller matrices, reducing computational complexity while approximating the original model’s behavior.\u003c/p\u003e\u003ch3 id=\"knowledge-distillation\"\u003eKnowledge Distillation\u003c/h3\u003e\u003cp\u003eThis technique involves training a smaller model (the “student”) to mimic the outputs of a larger, pre-trained model (the “teacher”). The smaller model learns to replicate the teacher’s behavior, achieving similar accuracy while being more efficient. For instance, \u003ca href=\"https://arxiv.org/abs/1910.01108\"\u003e\u003cstrong\u003eDistilBERT\u003c/strong\u003e successfully reduced BERT’s size by 40% while retaining 97% of its performance\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"technologies-used-for-on-device-ai\"\u003eTechnologies Used For On-Device AI\u003c/h2\u003e\u003cp\u003eWell, all the model compression techniques and specialized chips are cool because they’re what make ODAI possible. But what’s even more interesting for us as developers is actually putting these tools to work. This section covers some of the key technologies and frameworks that make ODAI accessible.\u003c/p\u003e\u003ch3 id=\"mediapipe-solutions\"\u003eMediaPipe Solutions\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://chuoling.github.io/mediapipe/\"\u003eMediaPipe Solutions\u003c/a\u003e is a developer toolkit for adding AI-powered features to apps and devices. It offers cross-platform, customizable tools that are optimized for running AI locally, from real-time video analysis to natural language processing.\u003c/p\u003e\u003cp\u003eAt the heart of MediaPipe Solutions is \u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/tasks\"\u003eMediaPipe Tasks\u003c/a\u003e, a core library that lets developers deploy ML solutions with minimal code. It’s designed for platforms like Android, Python, and Web/JavaScript, so you can easily integrate AI into a wide range of applications.\u003c/p\u003e\u003cp\u003eMediaPipe also provides various specialized tasks for different AI needs:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://mediapipe-studio.webapps.google.com/demo/llm_inference\"\u003e\u003cstrong\u003eLLM Inference API\u003c/strong\u003e\u003c/a\u003e\u003cbr/\u003eThis API runs lightweight large language models (LLMs) entirely on-device for tasks like text generation and summarization. It supports several open models like Gemma and external options like Phi-2.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://mediapipe-studio.webapps.google.com/demo/object_detector\"\u003e\u003cstrong\u003eObject Detection\u003c/strong\u003e\u003c/a\u003e\u003cbr/\u003eThe tool helps you Identify and locate objects in images or videos, which is ideal for real-time applications like detecting animals, people, or objects right on the device.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://mediapipe-studio.webapps.google.com/demo/image_segmenter\"\u003e\u003cstrong\u003eImage Segmentation\u003c/strong\u003e\u003c/a\u003e\u003cbr/\u003eMediaPipe can also segment images, such as isolating a person from the background in a video feed, allowing it to separate objects in both single images (like photos) and continuous video streams (like live video or recorded footage).\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"litert\"\u003eLiteRT\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://github.com/google-ai-edge/litert\"\u003eLiteRT or Lite Runtime\u003c/a\u003e (previously called TensorFlow Lite) is a lightweight and high-performance runtime designed for ODAI. It supports running pre-trained models or converting TensorFlow, PyTorch, and JAX models to a LiteRT-compatible format using AI Edge tools.\u003c/p\u003e\u003ch3 id=\"model-explorer\"\u003eModel Explorer\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://github.com/google-ai-edge/model-explorer\"\u003eModel Explorer\u003c/a\u003e is a visualization tool that helps you analyze machine learning models and graphs. It simplifies the process of preparing these models for on-device AI deployment, letting you understand the structure of your models and fine-tune them for better performance.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/on-device-ai-building-smarter-faster-private-applications/1-model-explorer.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"554\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/on-device-ai-building-smarter-faster-private-applications/1-model-explorer.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/on-device-ai-building-smarter-faster-private-applications/1-model-explorer.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/on-device-ai-building-smarter-faster-private-applications/1-model-explorer.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/on-device-ai-building-smarter-faster-private-applications/1-model-explorer.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/on-device-ai-building-smarter-faster-private-applications/1-model-explorer.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/on-device-ai-building-smarter-faster-private-applications/1-model-explorer.png\" sizes=\"100vw\" alt=\"Screenshot of the Model Explorer tool\"/\u003e\u003c/a\u003e\u003cfigcaption\u003eModel-Explorer: Visualize ML models and graphs to prepare and optimize them for On-Device ai. (Image source: Google) (\u003ca href=\"https://files.smashing.media/articles/on-device-ai-building-smarter-faster-private-applications/1-model-explorer.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eYou can use Model Explorer locally or in \u003ca href=\"https://github.com/google-ai-edge/model-explorer/blob/main/example_colabs/quick_start.ipynb\"\u003eColab\u003c/a\u003e for testing and experimenting.\u003c/p\u003e\u003ch3 id=\"executorch\"\u003eExecuTorch\u003c/h3\u003e\u003cp\u003eIf you’re familiar with PyTorch, ExecuTorch makes it easy to deploy models to mobile, wearables, and edge devices. It’s part of the \u003ca href=\"https://pytorch.org/edge\"\u003ePyTorch Edge ecosystem\u003c/a\u003e, which supports building AI experiences for edge devices like embedded systems and microcontrollers.\u003c/p\u003e\u003ch2 id=\"large-language-models-for-on-device-ai\"\u003eLarge Language Models For On-Device AI\u003c/h2\u003e\u003cp\u003eGemini is a powerful AI model that doesn’t just excel in processing text or images. It can also handle multiple types of data seamlessly. The best part? It’s designed to work right on your devices.\u003c/p\u003e\u003cp\u003eFor on-device use, there’s \u003ca href=\"https://deepmind.google/technologies/gemini/nano/\"\u003e\u003cstrong\u003eGemini Nano\u003c/strong\u003e\u003c/a\u003e, a lightweight version of the model. It’s built to perform efficiently while keeping everything private.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat can Gemini Nano do?\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCall Notes on Pixel devices\u003c/strong\u003e\u003cbr/\u003eThis feature creates private summaries and transcripts of conversations. It works entirely on-device, ensuring privacy for everyone involved.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePixel Recorder app\u003c/strong\u003e\u003cbr/\u003eWith the help of Gemini Nano and AICore, the app provides an on-device summarization feature, making it easy to extract key points from recordings.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTalkBack\u003c/strong\u003e\u003cbr/\u003eEnhances the accessibility feature on Android phones by providing clear descriptions of images, thanks to Nano’s multimodal capabilities.\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003cem\u003eIt’s similar to an application we built using LLaVA in \u003ca href=\"https://www.smashingmagazine.com/2024/10/using-multimodal-ai-models-applications-part3/\"\u003ea previous article\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp\u003eGemini Nano is far from the only language model designed specifically for ODAI. I’ve collected a few others that are worth mentioning:\u003c/p\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eModel\u003c/th\u003e\u003cth\u003eDeveloper\u003c/th\u003e\u003cth\u003eResearch Paper\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eOctopus v2\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eNexaAI\u003c/td\u003e\u003ctd\u003e\u003ca href=\"https://arxiv.org/pdf/2404.01744\"\u003eOn-device language model for super agent\u003c/a\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eOpenELM\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eApple ML Research\u003c/td\u003e\u003ctd\u003e\u003ca href=\"https://arxiv.org/abs/2404.14619\"\u003eA significant large language model integrated within iOS to enhance application functionalities\u003c/a\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eFerret-v2\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eApple\u003c/td\u003e\u003ctd\u003e\u003ca href=\"https://arxiv.org/abs/2404.07973\"\u003eFerret-v2 significantly improves upon its predecessor, introducing enhanced visual processing capabilities and an advanced training regimen\u003c/a\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eMiniCPM\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eTsinghua University\u003c/td\u003e\u003ctd\u003e\u003ca href=\"https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5\"\u003eA GPT-4V Level Multimodal LLM on Your Phone\u003c/a\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003ePhi-3\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eMicrosoft\u003c/td\u003e\u003ctd\u003e\u003ca href=\"https://arxiv.org/pdf/2404.14219\"\u003ePhi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone\u003c/a\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003ch2 id=\"the-trade-offs-of-using-on-device-ai\"\u003eThe Trade-Offs of Using On-Device AI\u003c/h2\u003e\u003cp\u003eBuilding AI into devices can be exciting and practical, but it’s not without its challenges. While you may get a lightweight, private solution for your app, there are a few compromises along the way. Here’s a look at some of them:\u003c/p\u003e\u003ch3 id=\"limited-resources\"\u003eLimited Resources\u003c/h3\u003e\u003cp\u003ePhones, wearables, and similar devices don’t have the same computing power as larger machines. This means AI models must fit within limited storage and memory while running efficiently. Additionally, running AI can drain the battery, so the models need to be optimized to balance power usage and performance.\u003c/p\u003e\u003ch3 id=\"data-and-updates\"\u003eData and Updates\u003c/h3\u003e\u003cp\u003eAI in devices like drones, self-driving cars, and other similar devices process data quickly, using sensors or lidar to make decisions. However, these models or the system itself don’t usually get real-time updates or additional training unless they are connected to the cloud. Without these updates and regular model training, the system may struggle with new situations.\u003c/p\u003e\u003ch3 id=\"biases\"\u003eBiases\u003c/h3\u003e\u003cp\u003eBiases in training data are a common challenge in AI, and ODAI models are no exception. These biases can lead to unfair decisions or errors, like misidentifying people. For ODAI, keeping these models fair and reliable means not only addressing these biases during training but also ensuring the solutions work efficiently within the device’s constraints.\u003c/p\u003e\u003cp\u003eThese aren’t the only challenges of on-device AI. It’s still a new and growing technology, and the small number of professionals in the field makes it harder to implement.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eChoosing between on-device and cloud-based AI comes down to what your application needs most. Here’s a quick comparison to make things clear:\u003c/p\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eAspect\u003c/th\u003e\u003cth\u003eOn-Device AI\u003c/th\u003e\u003cth\u003eCloud-Based AI\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003ePrivacy\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eData stays on the device, ensuring privacy.\u003c/td\u003e\u003ctd\u003eData is sent to the cloud, raising potential privacy concerns.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eLatency\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eProcesses instantly with no delay.\u003c/td\u003e\u003ctd\u003eRelies on internet speed, which can introduce delays.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eConnectivity\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eWorks offline, making it reliable in any setting.\u003c/td\u003e\u003ctd\u003eRequires a stable internet connection.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eProcessing Power\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eLimited by device hardware.\u003c/td\u003e\u003ctd\u003eLeverages the power of cloud servers for complex tasks.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eCost\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eNo ongoing server expenses.\u003c/td\u003e\u003ctd\u003eCan incur continuous cloud infrastructure costs.\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cp\u003eFor apps that need \u003cstrong\u003efast processing\u003c/strong\u003e and \u003cstrong\u003estrong privacy\u003c/strong\u003e, ODAI is the way to go. On the other hand, cloud-based AI is better when you need \u003cstrong\u003emore computing power\u003c/strong\u003e and \u003cstrong\u003efrequent updates\u003c/strong\u003e. The choice depends on your project’s needs and what matters most to you.\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://www.smashingmagazine.com/images/logo/logo--red.png\" alt=\"Smashing Editorial\" width=\"35\" height=\"46\" loading=\"lazy\" decoding=\"async\"/\u003e\n\u003cspan\u003e(gg, yk)\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "16 min read",
  "publishedTime": "2025-01-16T13:00:00Z",
  "modifiedTime": "2025-01-16T13:00:00Z"
}
