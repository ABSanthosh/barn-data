{
  "id": "753b9c72-f12d-4e2c-bdb8-cadcb15e01bc",
  "title": "Decolonising AI: A UX approach to cultural biases and power dynamics",
  "link": "https://uxdesign.cc/decolonising-ai-a-ux-approach-to-addressing-cultural-biases-and-power-dynamics-0be99bbf639c?source=rss----138adf9c44c---4",
  "description": "",
  "author": "Micaela Dixon",
  "published": "Tue, 18 Feb 2025 19:19:03 GMT",
  "source": "https://uxdesign.cc/feed",
  "categories": [
    "design",
    "ai",
    "artificial-intelligence",
    "ux",
    "ethics"
  ],
  "byline": "Micaela Dixon",
  "length": 6083,
  "excerpt": "AI systems, trained on data reflecting historical power imbalances, often encode and reinforce inequalities, leading to discriminatory outcomes for marginalised communities. There is an urgent need…",
  "siteName": "UX Collective",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*dn6MbbIIlwobt0jnUcrt_Q.png",
  "text": "An AI system is only as good as the stories it learns, but what happens when those stories are incomplete or one-sided?‘Cunhatain — Antropofagia musical’, 2018 — Denilson Baniwa — Picture by: DuHarteAI systems, trained on data reflecting historical power imbalances, often encode and reinforce inequalities, leading to discriminatory outcomes for marginalised communities.There is an urgent need to explore ideas on how to Decolonise AI, examining how UX can play an essential role in addressing cultural biases and power dynamics. deeply ethical and political one that requires interdisciplinary collaboration and a fundamental shift in perspective.The colonial echo in algorithmic designAI systems are not neutral arbiters; they are reflections of the data they are trained on and the values of their creators. Too often, this data is sourced from Western-centric datasets, embedding cultural assumptions and biases that disadvantage non-Western populations. This phenomenon echoes Boaventura de Sousa Santos’s concept of epistemicide, where Western knowledge systems systematically suppress alternative forms of knowledge.Consider, for example, image recognition algorithms that struggle to accurately identify faces from diverse ethnic backgrounds, or natural language processing models that perform poorly on non-English languages or dialects. These failures are not simply technical glitches; they are manifestations of a biased system that prioritises certain cultural perspectives while marginalising others.Source: ‘Humans Are Biased. Generative AI Is Even Worse’ — By Leonardo Nicoletti and Dina Bass Technology + Equality — June, 2023.The subaltern and the algorithm: Who gets to speak?The issue of algorithmic bias is further complicated by the fact that marginalised communities often lack the power to shape the development and deployment of AI systems that affect their lives. Gayatri Chakravorty Spivak’s seminal essay, “Can the Subaltern Speak?” (1988), highlights the challenges faced by marginalised groups in having their voices heard within dominant power structures. This resonates strongly with the issue of algorithmic bias, where certain voices are systematically excluded from the data used to train AI systems.The consequences of this exclusion can be shocking. AI systems used in law enforcement, for example, have been shown to disproportionately target communities of colour. Similarly, AI-powered hiring tools can perpetuate existing inequalities by penalising candidates from non-Western backgrounds. Achille Mbembe’s Necropolitics (2003) examines how power operates through the control of life and death. AI systems, used in areas like law enforcement and border control, can become instruments of necropolitics, disproportionately impacting marginalised communities.Frantz Fanon’s work, particularly Black Skin, White Masks (1952), explores the psychological effects of colonialism and the ways in which it internalises feelings of inferiority and alienation. Similarly, Walter Mignolo’s, The Darker Side of Modernity: Global Futures, Decolonial Options (2011) exposes how coloniality is inherent to modernity, and remains unseen.These insights can inform our understanding of how AI systems can perpetuate colonial patterns, even when they are not explicitly biased. To truly decolonise AI, we must challenge the concentration of power in the hands of a few tech companies and governments.We must promote the development of AI systems that are democratically controlled and accountable to the communities they serve.‘AI Decolonial Manyfesto’, pushing for a re-evaluation of AI through diverse cultural viewpoints, and ‘Tierra Común’, which campaigns for interventions against data colonialism.UX as a tool for decolonisationUX, with its focus on people and empathy, can be a powerful tool for decolonising AI. By centring the experiences and perspectives of marginalised communities, UX teams can help identifying and mitigating biases in AI systems before it causes real harm.Questioning who AI truly serves and whose voices are missing from the design process, AI risks being shaped by a narrow set of experiences, reinforcing existing power structures rather than challenging them.Some of the key strategies that UX practitioners can add to their frameworks to identify and prevent harmful experiences could rely on:Diversifying data sets: Actively seek out and incorporate data \u0026 feedback from diverse cultural backgrounds to ensure that AI systems are trained on a representative sample of the population.Engaging in participatory design: Involve members of marginalised communities in the design process, giving them a voice in shaping the development of AI systems that affect their lives.Promoting transparency and explainability: Make AI systems more transparent and explainable, so that users can understand how they work and challenge potentially biased outcomes.Embracing cultural sensitivity: Design interfaces that are culturally sensitive and inclusive, taking into account the diverse needs and preferences of users from different backgrounds. Silvia Rivera Cusicanqui, a Bolivian sociologist, emphasizes the importance of ch’ixi thinking — embracing contradiction and ambiguity — in challenging colonial power structures. In AI, this could mean designing systems that acknowledge and celebrate cultural differences rather than striving for a single, universal standard.The path forwardDecolonising human-AI interactions is a complex and ongoing process that demands a collective effort. By embracing a decolonial lens, we can design AI systems that are not only accurate and reliable but also truly inclusive.This requires us to recognise the colonial legacy embedded in AI by acknowledging the historical power imbalances that have shaped its development and deployment. It also means centring the voices of marginalised communities, prioritising their experiences and perspectives as those most affected by algorithmic bias.But not only.How can we all commit to building an environment where AI empowers rather than marginalises?",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*NLY4kwdjRtb1TboCX4IpWw.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"8bff\"\u003eAn AI system is only as good as the stories it learns, but what happens when those stories are incomplete or one-sided?\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://micaeladixonn.medium.com/?source=post_page---byline--0be99bbf639c---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Micaela Dixon\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*UtjGidAKPLyp8wADclElTw.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://uxdesign.cc/?source=post_page---byline--0be99bbf639c---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"UX Collective\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*mDhF9X4VO0rCrJvWFatyxg.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cfigure\u003e\u003cfigcaption\u003e‘Cunhatain — Antropofagia musical’, 2018 — Denilson Baniwa — Picture by: DuHarte\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"5fbb\"\u003eAI systems, trained on data reflecting historical power imbalances, often encode and reinforce inequalities, leading to discriminatory outcomes for marginalised communities.\u003c/p\u003e\u003cp id=\"b24a\"\u003eThere is an urgent need to explore ideas on how to Decolonise AI, examining how UX can play an essential role in addressing \u003ca href=\"https://londoninterculturalcenter.co.uk/blogs/f/how-cultural-bias-influences-ai\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecultural biases\u003c/a\u003e and power dynamics. deeply ethical and political one that requires interdisciplinary collaboration and a fundamental shift in perspective.\u003c/p\u003e\u003ch2 id=\"fd78\"\u003eThe colonial echo in algorithmic design\u003c/h2\u003e\u003cp id=\"ba61\"\u003eAI systems are not neutral arbiters; they are reflections of the data they are trained on and the values of their creators. Too often, this data is sourced from Western-centric datasets, embedding cultural assumptions and biases that disadvantage non-Western populations. This phenomenon echoes \u003ca href=\"https://en.wikipedia.org/wiki/Boaventura_de_Sousa_Santos\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBoaventura de Sousa Santos’s\u003c/a\u003e concept of \u003cem\u003eepistemicide\u003c/em\u003e, where Western knowledge systems systematically suppress \u003cem\u003ealternative\u003c/em\u003e forms of knowledge.\u003c/p\u003e\u003cp id=\"fa58\"\u003eConsider, for example, image recognition algorithms \u003ca href=\"https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ethat struggle to accurately identify faces from diverse ethnic backgrounds\u003c/a\u003e, or natural language processing models that perform poorly on non-English languages or dialects. These failures are not simply technical glitches; they are manifestations of a \u003cstrong\u003ebiased system\u003c/strong\u003e that prioritises certain cultural perspectives while marginalising others.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eSource: \u003ca href=\"https://www.bloomberg.com/graphics/2023-generative-ai-bias/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e‘Humans Are Biased. Generative AI Is Even Worse’\u003c/a\u003e — By \u003ca href=\"https://twitter.com/Leonardonclt\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLeonardo Nicoletti\u003c/a\u003e and \u003ca href=\"https://twitter.com/dinabass\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDina Bass\u003c/a\u003e \u003ca href=\"https://www.bloomberg.com/technology\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTechnology\u003c/a\u003e + \u003ca href=\"https://www.bloomberg.com/equality\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEquality\u003c/a\u003e — June, 2023.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"d9ce\"\u003e\u003cstrong\u003eThe subaltern and the algorithm: Who gets to speak?\u003c/strong\u003e\u003c/h2\u003e\u003cp id=\"b9c6\"\u003eThe issue of algorithmic bias is further complicated by the fact that marginalised communities often lack the power to shape the development and deployment of AI systems that affect their lives. \u003ca href=\"https://en.wikipedia.org/wiki/Gayatri_Chakravorty_Spivak\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGayatri Chakravorty Spivak’s\u003c/a\u003e seminal essay, \u003cem\u003e“Can the Subaltern Speak?” \u003c/em\u003e(1988), highlights the challenges faced by marginalised groups in having their voices heard within dominant power structures. This resonates strongly with the issue of algorithmic bias, where \u003cstrong\u003ecertain voices are systematically excluded\u003c/strong\u003e from the data used to train AI systems.\u003c/p\u003e\u003cp id=\"0bb2\"\u003eThe consequences of this exclusion can be shocking. AI systems used in law enforcement, for example, \u003ca href=\"https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehave been shown to disproportionately target communities of colour\u003c/a\u003e. Similarly, AI-powered hiring tools can perpetuate existing inequalities by penalising candidates from non-Western backgrounds. \u003ca href=\"https://en.wikipedia.org/wiki/Achille_Mbembe\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAchille Mbembe’s\u003c/a\u003e \u003cem\u003eNecropolitics\u003c/em\u003e (2003) examines how power operates through the control of life and death. AI systems, used in areas like law enforcement and border control, can become instruments of necropolitics, disproportionately \u003ca href=\"https://sites.law.berkeley.edu/thenetwork/2022/01/26/how-artificial-intelligence-impacts-marginalized-communities/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003eimpacting marginalised communities\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"855d\"\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Frantz_Fanon\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFrantz Fanon’s\u003c/a\u003e work, particularly \u003cem\u003eBlack Skin, White Masks\u003c/em\u003e (1952), explores the \u003cstrong\u003epsychological effects of colonialism\u003c/strong\u003e and the ways in which it internalises feelings of inferiority and alienation. Similarly, \u003ca href=\"https://en.wikipedia.org/wiki/Walter_Mignolo\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eWalter Mignolo’s\u003c/a\u003e, \u003cem\u003eThe Darker Side of Modernity: Global Futures, Decolonial Options\u003c/em\u003e (2011) exposes how coloniality is inherent to modernity, and remains unseen.\u003c/p\u003e\u003cp id=\"9123\"\u003eThese insights can inform our understanding of how \u003cstrong\u003eAI systems can perpetuate colonial patterns\u003c/strong\u003e, even when they are not explicitly biased. To truly decolonise AI, \u003cstrong\u003ewe must challenge the concentration of power \u003c/strong\u003ein the hands of a few tech companies and governments.\u003c/p\u003e\u003cp id=\"39ef\"\u003eWe must promote the development of AI systems that are \u003cstrong\u003edemocratically controlled\u003c/strong\u003e and accountable to the communities they serve.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003ca href=\"https://manyfesto.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e‘AI Decolonial Manyfesto’\u003c/a\u003e, pushing for a re-evaluation of AI through diverse cultural viewpoints, and \u003ca href=\"https://www.tierracomun.net/en/home\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e‘Tierra Común’,\u003c/a\u003e which campaigns for interventions against data colonialism.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"f57c\"\u003e\u003cstrong\u003eUX as a tool for decolonisation\u003c/strong\u003e\u003c/h2\u003e\u003cp id=\"bc03\"\u003eUX, with its focus on \u003cstrong\u003epeople and \u003c/strong\u003e\u003ca href=\"https://www.nngroup.com/articles/sympathy-vs-empathy-ux/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003eempathy\u003c/strong\u003e\u003c/a\u003e, can be a powerful tool for decolonising AI. By centring the experiences and perspectives of marginalised communities, UX teams can help \u003cstrong\u003eidentifying\u003c/strong\u003e and \u003cstrong\u003emitigating\u003c/strong\u003e biases in AI systems before it causes real harm.\u003c/p\u003e\u003cp id=\"f607\"\u003eQuestioning who AI truly serves and \u003cstrong\u003ewhose voices are missing\u003c/strong\u003e from the design process, AI risks being shaped by a narrow set of experiences, \u003cstrong\u003ereinforcing\u003c/strong\u003e existing power structures rather than challenging them.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"e0bf\"\u003eSome of the key strategies that UX practitioners can add to their frameworks to identify and prevent harmful experiences could rely on:\u003c/p\u003e\u003cul\u003e\u003cli id=\"47c9\"\u003e\u003cstrong\u003eDiversifying data sets: \u003c/strong\u003eActively seek out and incorporate data \u0026amp; feedback from diverse cultural backgrounds to ensure that AI systems are trained on a representative sample of the population.\u003c/li\u003e\u003cli id=\"ba3d\"\u003e\u003cstrong\u003eEngaging in participatory design: \u003c/strong\u003eInvolve members of marginalised communities in the design process, giving them a voice in shaping the development of AI systems that affect their lives.\u003c/li\u003e\u003cli id=\"e640\"\u003e\u003cstrong\u003ePromoting transparency and explainability:\u003c/strong\u003e Make AI systems more transparent and explainable, so that users can understand how they work and \u003cem\u003echallenge\u003c/em\u003e potentially biased outcomes.\u003c/li\u003e\u003cli id=\"2023\"\u003e\u003cstrong\u003eEmbracing cultural sensitivity:\u003c/strong\u003e Design interfaces that are culturally sensitive and inclusive, taking into account the diverse needs and preferences of users from different backgrounds. \u003ca href=\"https://en.wikipedia.org/wiki/Silvia_Rivera_Cusicanqui\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSilvia Rivera Cusicanqui\u003c/a\u003e, a Bolivian sociologist, emphasizes the importance of \u003cem\u003ech’ixi thinking\u003c/em\u003e — embracing contradiction and ambiguity — in challenging colonial power structures. In AI, this could mean \u003cstrong\u003edesigning systems that acknowledge and celebrate cultural differences\u003c/strong\u003e rather than striving for a single, universal standard.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"b521\"\u003eThe path forward\u003c/h2\u003e\u003cp id=\"7c47\"\u003eDecolonising human-AI interactions is a complex and ongoing process that demands a \u003cstrong\u003ecollective\u003c/strong\u003e effort. By embracing a decolonial lens, we can design AI systems that are not only accurate and reliable but also \u003cstrong\u003etruly inclusive\u003c/strong\u003e.\u003c/p\u003e\u003cp id=\"66e8\"\u003eThis requires us to \u003cstrong\u003erecognise\u003c/strong\u003e the colonial legacy embedded in AI by acknowledging the \u003cstrong\u003ehistorical power imbalances\u003c/strong\u003e that have shaped its development and deployment. It also means centring the voices of marginalised communities, \u003cstrong\u003eprioritising their experiences\u003c/strong\u003e and perspectives as those most affected by algorithmic bias.\u003c/p\u003e\u003cp id=\"0aef\"\u003e\u003cstrong\u003eBut not only.\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"81f2\"\u003eHow can we all commit to building an environment where AI empowers rather than marginalises?\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-02-18T12:34:52.241Z",
  "modifiedTime": null
}
