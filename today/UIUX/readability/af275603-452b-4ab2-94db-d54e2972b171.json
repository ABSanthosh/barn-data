{
  "id": "af275603-452b-4ab2-94db-d54e2972b171",
  "title": "Using Multimodal AI Models For Your Applications (Part 3)",
  "link": "https://smashingmagazine.com/2024/10/using-multimodal-ai-models-applications-part3/",
  "description": "In this third part of the series, you are looking at two models that handle all three modalities — text, images or videos, and audio — without needing a second model for text-to-speech or speech recognition.",
  "author": "Joas Pambou",
  "published": "Fri, 11 Oct 2024 10:00:00 GMT",
  "source": "https://www.smashingmagazine.com/feed",
  "categories": null,
  "byline": "About The Author",
  "length": 19394,
  "excerpt": "In this third part of the series, you are looking at two models that handle all three modalities — text, images or videos, and audio — without needing a second model for text-to-speech or speech recognition.",
  "siteName": "Smashing Magazine",
  "favicon": "https://smashingmagazine.com/images/favicon/apple-touch-icon.png",
  "text": "14 min readAI, LLM, Techniques, ToolsYou’ve covered a lot with Joas Pambou so far in this series. In Part 1, you built a system using a vision-language model (VLM) and a text-to-speech (TTS) model to create audio descriptions of images. In Part 2, you improved the system by using LLaVA and Whisper, which provided audio descriptions of images. In this third and final installment, you are looking at two models that handle all three modalities — text, images or videos, and audio — without needing a second model for text-to-speech or speech recognition.In this third and final part of a three-part series, we’re taking a more streamlined approach to an application that supports vision-language (VLM) and text-to-speech (TTS). This time, we’ll use different models that are designed for all three modalities — images or videos, text, and audio (including speech-to-text) — in one model. These “any-to-any” models make things easier by allowing us to avoid switching between models.Specifically, we’ll focus on two powerful models: Reka and Gemini 1.5 Pro.Both models take things to the next level compared to the tools we used earlier. They eliminate the need for separate speech recognition models, providing a unified solution for multimodal tasks. With this in mind, our goal in this article is to explore how Reka and Gemini simplify building advanced applications that handle images, text, and audio all at once.Overview Of Multimodal AI ModelsThe architecture of multimodal models has evolved to enable seamless handling of various inputs, including text, images, and audio, among others. Traditional models often require separate components for each modality, but recent advancements in “any-to-any” models like Next-GPT or 4M allow developers to build systems that process multiple modalities within a unified architecture.Gato, for instance, utilizes a 1.2 billion parameter decoder-only transformer architecture with 24 layers, embedding sizes of 2048 and a hidden size of 8196 in its feed-forward layers. This structure is optimized for general tasks across various inputs, but it still relies on extensive task-specific fine-tuning.GPT-4o, on the other hand, takes a different approach with training on multiple media types within a single architecture. This means it’s a single model trained to handle a variety of inputs (e.g., text, images, code) without the need for separate systems for each. This training method allows for smoother task-switching and better generalization across tasks.Similarly, CoDi employs a multistage training scheme to handle a linear number of tasks while supporting input-output combinations across different modalities. CoDi’s architecture builds a shared multimodal space, enabling synchronized generation for intertwined modalities like video and audio, making it ideal for more dynamic multimedia tasks.Most “any-to-any” models, including the ones we’ve discussed, rely on a few key concepts to handle different tasks and inputs smoothly:Shared representation spaceThese models convert different types of inputs — text, images, audio — into a common feature space. Text is encoded into vectors, images into feature maps, and audio into spectrograms or embeddings. This shared space allows the model to process various inputs in a unified way.Attention mechanismsAttention layers help the model focus on the most relevant parts of each input, whether it’s understanding the text, generating captions from images, or interpreting audio.Cross-modal interactionIn many models, inputs from one modality (e.g., text) can guide the generation or interpretation of another modality (e.g., images), allowing for more integrated and cohesive outputs.Pre-training and fine-tuningModels are typically pre-trained on large datasets across different types of data and then fine-tuned for specific tasks, enhancing their performance in real-world applications.Reka ModelsReka is an AI research company that helps developers build powerful applications by offering models for a range of tasks. These tasks include generating text from videos and images, translating speech, and answering complex questions from long multimodal documents. Reka’s models can even write and execute code, providing flexible, real-world solutions for developers.These are the three main models Reka offers:Reka CoreA 67-billion-parameter multimodal language model designed for complex tasks. It supports inputs like images, videos, and texts while excelling in advanced reasoning and coding.Reka FlashA faster model with a 21-billion-parameter, designed for flexibility and rapid performance in multimodal settings.Reka Edge (PDF)A smaller 7-billion-parameter model was built for on-device and low-latency applications, making it efficient for local use and local or latency-sensitive applications.Reka’s models can be fine-tuned and deployed securely, whether on the cloud, on-premises, or even on-device. Let’s start by testing Reka’s capabilities directly through its playground. This allows us to experiment with its multimodal features without writing any code, providing a hands-on way to see how the models handle various tasks, such as image and video comprehension.Alright, we’ll kick things off by uploading an image of a diagram outline of the PaliGemma architecture and ask Reka for a detailed explanation.Can you provide a more detailed explanation of this image?(Large preview)Here’s what we get from Reka Core:“This image is a schematic representation of a machine-learning model named PaliGemma. The model appears to be designed for multimodal input, combining both image and text data. At the top, there are two input sources: ‘Image Input’ and ‘Text Input’. The image input is processed by a ‘SigLIP Image Encoder’, which likely stands for a specific type of image encoding technique. The output of this encoder is then subjected to a ‘Linear Projection’, which is a common step in machine learning to transform data into a lower-dimensional space. The text input is directly fed into the model. Both the projected image data and the text input are then concatenated into ‘Concatenated Tokens’, which are then processed by a component labeled ‘Gemma’. The final output of the model is ‘Text Output’, which suggests that the model is designed to generate text based on the input data.”It’s a clear and detailed breakdown of the PaliGemma architecture. The model accurately identifies image and text inputs, the SigLIP image encoder, and the linear projection step. It also grasps how the image and text inputs are combined and processed to generate text output. I should admit that the response is quite neat, offering a solid explanation of the architecture.Let’s try a video input instead. We’ll upload the video about Genny, a software, and ask a question to see how well Reka Core handles video comprehension.What is this video about?“This video is a screen recording of a group of friends discussing their plans for the weekend.”Well, it’s not a good answer compared to what LLaVA gave us in the previous article. Now let’s try a longer video, but with the Reka Flash model. The video is about a TTS model called Parler TTS. Instead of asking what the video is about, let’s prompt it like this:I’m an AI developer, and I’m not sure how this TTS model can help me in my projects. Can you explain how I can use it or what it’s useful for?(Large preview)Reka Flash generates a solid answer this time:“Of course! This TTS (Text-to-Speech) model can be incredibly useful for developers, especially those working on projects that involve creating more human-like interactions or providing audio feedback to users. Here are some ways you can leverage this TTS model in your projects:Accessibility: TTS models can greatly enhance the accessibility of your applications for users with visual impairments or reading difficulties. By providing spoken feedback or narrating content, you can make your applications more inclusiveVoice Assistants: You can integrate this TTS model into voice assistants or chatbots, allowing them to generate more natural-sounding responses. This can improve user experience and make interactions with your bots more engaging.Multimedia Applications: This TTS model can be used in multimedia applications, such as e-books, educational tools, or games, to provide spoken narration or instructions. This can make the content more engaging and easier to understand.Audio Notifications: You can use the TTS model to generate audio notifications for your applications, providing users with spoken alerts or reminders. This can be particularly useful for users who may not always have their screens visible.Language Learning: The TTS model can be used in language learning applications to provide users with correct pronunciation and intonation. It can also generate spoken feedback or narrate lessons, making the learning process more immersive.Audio Books: You can use the TTS model to generate audio books, allowing users to listen to written content instead of reading it. This can be especially helpful for users who prefer auditory learning or have difficulty reading.Explainer Videos: The TTS model can be used to generate voiceovers for explainer videos, providing users with spoken narration that explains the content on the screen. This can make the content more engaging and easier to understand.Audio Feedback: You can use the TTS model to generate audio feedback for users in your applications, such as confirming actions, providing instructions, or giving feedback on their performance. This can make the interactions more natural and intuitive.In summary, the TTS model can be a valuable tool for developers looking to create more human-like interactions, improve accessibility, and enhance the user experience in various applications.”It’s a good answer that’s informative and quite practical for developers. Let’s compare what happens when we try the Reka Edge model with a PDF input titled “Meta AI — Responsible Use Guide” (PDF).As an AI developer, how can I ensure that I am following the responsible AI guidelines outlined in this document?“To ensure that your use of AI aligns with the principles outlined in this guide, you can follow these steps and best practices:Familiarize yourself with the principles of responsible AI, such as transparency, accountability, and user empowerment, as outlined in the guide.Understand the potential impacts of AI on users and society, and be aware of the risks and challenges associated with the development and deployment of AI systems.Embrace an iterative model-development mindset, continuously striving for improvement and learning from feedback and errors.Build a responsible AI ecosystem by working with partners, experts, and communities to develop and share best practices, tools, and benchmarking resources.Follow the guidelines and best practices outlined in the guide for each stage of model development, from data collection and preparation to fine-tuning, evaluation, and deployment.Consider the specific use case and requirements for your AI system, and tailor your approach accordingly.Regularly monitor and evaluate the performance and safety of your AI system and make adjustments as needed to ensure it aligns with the principles outlined in the guide.Be transparent and accountable in your interactions with users and stakeholders, and be open to feedback and suggestions for improvement.Stay informed about the latest developments and research in the field of AI, and be prepared to adapt your approach as needed to address new challenges and opportunities.Follow the specific guidelines and best practices outlined in the addendum for Code Llama, such as defining content policies, evaluating and benchmarking models, and addressing input- and output-level risks.”Reka Edge’s answer is quite thorough and detailed, too!If you want to dive into coding, start by creating a Reka account and getting an API key. Then you can follow along with this notebook. Try out their Python SDK, and make sure to explore the different models available. It’s a great way to see how these tools work under the hood!Gemini ModelsGemini 1.5, developed by Google DeepMind, is designed to handle complex tasks efficiently, thanks to the new Mixture-of-Experts (MoE) system. MoE is like packing several specialized models inside one big model. Instead of using the whole network for every task, Gemini 1.5 activates only the parts (or what they call “experts”) that are most relevant. This means Gemini 1.5 can tackle complex tasks with less processing power than a typical monolithic model.We can explore Gemini’s features in Google AI Studio since it’s already baked right in. We’ll begin by uploading the following image illustrating VLM capabilities.(Large preview)We’ll ask Gemini to analyze the image and extract details or concepts:Explain this image. What are the specific details or concepts it represents?“The image is a flowchart illustrating different types of image understanding tasks with a Vision Language Model.Object Localization: This task aims to identify the precise location of objects within an image. The example question asks, “Is one cat behind another?”Zero-shot Segmentation: This task involves segmenting an image into different regions based on a given label, even if the model has never seen that specific label before. The example question asks to segment “striped cat.”Zero-shot Visual QA: This task requires answering questions about an image, even if the model has not been specifically trained to answer that type of question. The example question asks, “What is the breed of these cats?”One-shot Learning with Instructions: This task involves providing the model with a single example and instruction to learn a new concept and apply it to other images. The example question asks, “Striped cats are called tabby cats. What is the breed of the cats in the image?”Well, Gemini nailed it — it clearly understood the image and recognized the tasks a VLM handles. This is exactly what you’d expect from a model designed for advanced image analysis.Now, we’ll upload another image — this time of an Italian dish — and ask Gemini if it can recognize the dishes and list the ingredients needed for them. This type of “food recognition task” will help us test its ability to identify foods and ingredients.(Large preview)List the different dishes in this image, also from which country, and give me the ingredients I need to make them.“The image shows an Italian feast with the following dishes:Seafood Stew: A rich and flavorful stew with seafood like clams, mussels, shrimp, fish, and vegetables in a tomato broth.Spaghetti with Tomato Sauce: Classic spaghetti pasta tossed in a simple yet flavorful tomato sauce.Meatballs in Tomato Sauce: Tender and juicy meatballs simmered in a rich tomato sauce.Lasagna: Layered pasta with meat sauce, cheese, and tomato sauce.Side Salad: A refreshing side salad with mixed greens, tomatoes, and other vegetables.Garlic Bread: Toasted bread slices rubbed with garlic and olive oil.Marinara Sauce: A simple tomato-based sauce.This is a typical Italian meal with various dishes showcasing the flavors of Italian cuisine.As for the ingredients, you’ll need tomatoes for many of these dishes, specifically for the sauces and the side salad.”Well, the model did a good job recognizing the dishes, but it also didn’t give me all the necessary ingredients. Let’s see how it does with videos instead, using the same video we used earlier with the Reka models. This time, the task will focus on action recognition.What is the purpose of this video, and what key actions or processes are being demonstrated?“The video is a tutorial about the importance of friendship and how it can impact our lives. It demonstrates the use of a video editing software called “Genmy” to create a video and add text subtitles. The video creator is using Genmy to:Record their voice: This is seen at the top of the screen with the mic icon.Add text to speech: You see the “Text to Speech” option, which is likely being used to generate subtitles or captions for the video.Edit the timeline: They’re working with a video timeline, adding and arranging different elements.Add voice-over: The speaker is adding commentary to the video.Create a visually appealing presentation: They’re using a combination of video footage, text, and voice-over to create an engaging tutorial.”Well, Gemini gave a clear breakdown of the video, identifying it as a tutorial demonstrating the Genmy video editing software to highlight how it’s used for recording voice, adding text-to-speech, editing video timelines, and allowing voice-over. Impressive!Let’s upload one more video, the same one we used for Reka about the Parler TTS model, and ask Gemini to analyze and summarize the contents.Summarize this video and explain how the contents might be relevant or helpful for software engineers.“This video showcases different voice samples generated by a text-to-speech (TTS) system called “Parler-TTS.” The video highlights various voice characteristics like gender, pace, pitch, and even the simulated environment. For software engineers, this video demonstrates the progress made in TTS technology. These are some insights:Voice Variety: The video shows that Parler-TTS can create diverse voices, potentially useful for applications needing different personas or tones.Fine-grained Control: The video emphasizes the ability to control voice parameters like pace and pitch, showcasing the potential for customizability.Quality and Naturalness: The video focuses on the quality of the generated speech, demonstrating the advancements made in speech synthesis, which is crucial for user experience.”Nicely done! I can go with that answer. Gemini explains adjusting voice settings, like pitch and speed, and how having different voices can be useful. Gemini also emphasizes the importance of natural, high-quality speech, which is handy for developers working with TTS systems!Alright, for coding, you can grab the code from Google AI Studio by clicking the Get Code button. You can choose between formatting the code in Python, Swift, and Java, among other languages.(Large preview)ConclusionBoth Reka and Gemini are strong multimodal models for AI applications, but there are key differences between them to consider. Here’s a table that breaks those down:FeatureRekaGemini 1.5Multimodal CapabilitiesImage, video, and text processingImage, video, text, with extended token contextEfficiencyOptimized for multimodal tasksBuilt with MoE for efficiencyContext WindowStandard token windowUp to two million tokens (with Flash variant)ArchitectureFocused on multimodal task flowMoE improves specializationTraining/ServingHigh performance with efficient model switchingMore efficient training with MoE architectureDeploymentSupports on-device deploymentPrimarily cloud-based, with Vertex AI integrationUse CasesInteractive apps, edge deploymentSuited for large-scale, long-context applicationsLanguages SupportedMultiple languagesSupports many languages with long context windowsReka stands out for on-device deployment, which is super useful for apps requiring offline capabilities or low-latency processing.On the other hand, Gemini 1.5 Pro shines with its long context windows, making it a great option for handling large documents or complex queries in the cloud. (gg, yk)",
  "image": "https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/using-multimodal-ai-models-applications.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"article__content\"\u003e\u003cul\u003e\u003cli\u003e14 min read\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://smashingmagazine.com/category/ai\"\u003eAI\u003c/a\u003e,\n\u003ca href=\"https://smashingmagazine.com/category/llm\"\u003eLLM\u003c/a\u003e,\n\u003ca href=\"https://smashingmagazine.com/category/techniques\"\u003eTechniques\u003c/a\u003e,\n\u003ca href=\"https://smashingmagazine.com/category/tools\"\u003eTools\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003csection aria-label=\"Quick summary\"\u003eYou’ve covered a lot with Joas Pambou so far in this series. In \u003ca href=\"https://www.smashingmagazine.com/2024/07/integrating-image-to-text-and-text-to-speech-models-part1/\"\u003ePart 1\u003c/a\u003e, you built a system using a vision-language model (VLM) and a text-to-speech (TTS) model to create audio descriptions of images. In \u003ca href=\"https://www.smashingmagazine.com/2024/08/integrating-image-to-text-and-text-to-speech-models-part2/\"\u003ePart 2\u003c/a\u003e, you improved the system by using LLaVA and Whisper, which provided audio descriptions of images. In this third and final installment, you are looking at two models that handle all three modalities — text, images or videos, and audio — without needing a second model for text-to-speech or speech recognition.\u003c/section\u003e\u003c/p\u003e\u003cp\u003eIn this third and final part of a \u003ca href=\"https://www.smashingmagazine.com/2024/07/integrating-image-to-text-and-text-to-speech-models-part1/\"\u003ethree-part series\u003c/a\u003e, we’re taking a more streamlined approach to an application that supports vision-language (VLM) and text-to-speech (TTS). This time, we’ll use different models that are designed for all three modalities — images or videos, text, and audio (including speech-to-text) — in one model. These “any-to-any” models make things easier by allowing us to avoid switching between models.\u003c/p\u003e\u003cp\u003eSpecifically, we’ll focus on two powerful models: \u003cstrong\u003eReka\u003c/strong\u003e and \u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eBoth models take things to the next level compared to the tools we used earlier. They eliminate the need for separate speech recognition models, providing a unified solution for multimodal tasks. With this in mind, our goal in this article is to explore how Reka and Gemini simplify building advanced applications that handle images, text, and audio all at once.\u003c/p\u003e\u003ch2 id=\"overview-of-multimodal-ai-models\"\u003eOverview Of Multimodal AI Models\u003c/h2\u003e\u003cp\u003eThe architecture of multimodal models has evolved to enable seamless handling of various inputs, including text, images, and audio, among others. Traditional models often require separate components for each modality, but recent advancements in “any-to-any” models like \u003ca href=\"https://next-gpt.github.io\"\u003e\u003cstrong\u003eNext-GPT\u003c/strong\u003e\u003c/a\u003e or \u003ca href=\"https://machinelearning.apple.com/research/massively-multimodal\"\u003e\u003cstrong\u003e4M\u003c/strong\u003e\u003c/a\u003e allow developers to build systems that process multiple modalities within a unified architecture.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://deepmind.google/discover/blog/a-generalist-agent/\"\u003e\u003cstrong\u003eGato\u003c/strong\u003e\u003c/a\u003e, for instance, utilizes a 1.2 billion parameter decoder-only transformer architecture with 24 layers, embedding sizes of 2048 and a hidden size of 8196 in its feed-forward layers. This structure is optimized for general tasks across various inputs, but it still relies on extensive task-specific fine-tuning.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://openai.com/index/hello-gpt-4o/\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/a\u003e, on the other hand, takes a different approach with training on multiple media types within a single architecture. This means it’s a single model trained to handle a variety of inputs (e.g., text, images, code) without the need for separate systems for each. This training method allows for smoother task-switching and better generalization across tasks.\u003c/p\u003e\u003cp\u003eSimilarly, \u003ca href=\"https://codi-gen.github.io\"\u003e\u003cstrong\u003eCoDi\u003c/strong\u003e\u003c/a\u003e employs a multistage training scheme to handle a linear number of tasks while supporting input-output combinations across different modalities. CoDi’s architecture builds a shared multimodal space, enabling synchronized generation for intertwined modalities like video and audio, making it ideal for more dynamic multimedia tasks.\u003c/p\u003e\u003cp\u003eMost “any-to-any” models, including the ones we’ve discussed, rely on a few key concepts to handle different tasks and inputs smoothly:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eShared representation space\u003c/strong\u003e\u003cbr/\u003eThese models convert different types of inputs — text, images, audio — into a common feature space. Text is encoded into vectors, images into feature maps, and audio into spectrograms or embeddings. This shared space allows the model to process various inputs in a unified way.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAttention mechanisms\u003c/strong\u003e\u003cbr/\u003eAttention layers help the model focus on the most relevant parts of each input, whether it’s understanding the text, generating captions from images, or interpreting audio.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCross-modal interaction\u003c/strong\u003e\u003cbr/\u003eIn many models, inputs from one modality (e.g., text) can guide the generation or interpretation of another modality (e.g., images), allowing for more integrated and cohesive outputs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePre-training and fine-tuning\u003c/strong\u003e\u003cbr/\u003eModels are typically pre-trained on large datasets across different types of data and then fine-tuned for specific tasks, enhancing their performance in real-world applications.\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"reka-models\"\u003eReka Models\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://www.reka.ai\"\u003eReka\u003c/a\u003e is an AI research company that helps developers build powerful applications by offering models for a range of tasks. These tasks include generating text from videos and images, translating speech, and answering complex questions from long multimodal documents. Reka’s models can even write and execute code, providing flexible, real-world solutions for developers.\u003c/p\u003e\u003cp\u003eThese are the three main models Reka offers:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003ca href=\"https://www.reka.ai/news/reka-core-our-frontier-class-multimodal-language-model?utm\"\u003e\u003cstrong\u003eReka Core\u003c/strong\u003e\u003c/a\u003e\u003cbr/\u003eA 67-billion-parameter multimodal language model designed for complex tasks. It supports inputs like images, videos, and texts while excelling in advanced reasoning and coding.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://chat.reka.ai\"\u003e\u003cstrong\u003eReka Flash\u003c/strong\u003e\u003c/a\u003e\u003cbr/\u003eA faster model with a 21-billion-parameter, designed for flexibility and rapid performance in multimodal settings.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://publications.reka.ai/reka-core-tech-report.pdf\"\u003e\u003cstrong\u003eReka Edge\u003c/strong\u003e\u003c/a\u003e (PDF)\u003cbr/\u003eA smaller 7-billion-parameter model was built for on-device and low-latency applications, making it efficient for local use and local or latency-sensitive applications.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eReka’s models can be fine-tuned and deployed securely, whether on the cloud, on-premises, or even on-device. Let’s start by testing Reka’s capabilities directly through its \u003ca href=\"https://chat.reka.ai/chat\"\u003eplayground\u003c/a\u003e. This allows us to experiment with its multimodal features without writing any code, providing a hands-on way to see how the models handle various tasks, such as image and video comprehension.\u003c/p\u003e\u003cp\u003eAlright, we’ll kick things off by uploading an image of a diagram outline of the \u003cstrong\u003ePaliGemma architecture\u003c/strong\u003e and ask Reka for a detailed explanation.\u003c/p\u003e\u003cblockquote\u003eCan you provide a more detailed explanation of this image?\u003c/blockquote\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/1-diagram-outline-reka-explanation.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"384\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/1-diagram-outline-reka-explanation.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/1-diagram-outline-reka-explanation.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/1-diagram-outline-reka-explanation.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/1-diagram-outline-reka-explanation.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/1-diagram-outline-reka-explanation.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/1-diagram-outline-reka-explanation.png\" sizes=\"100vw\" alt=\"A detailed explanation by Reka of a diagram outline of the PaliGemma architecture\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/1-diagram-outline-reka-explanation.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eHere’s what we get from Reka Core:\u003c/p\u003e\u003cblockquote\u003e“This image is a schematic representation of a machine-learning model named PaliGemma. The model appears to be designed for multimodal input, combining both image and text data. At the top, there are two input sources: ‘Image Input’ and ‘Text Input’. The image input is processed by a ‘SigLIP Image Encoder’, which likely stands for a specific type of image encoding technique. The output of this encoder is then subjected to a ‘Linear Projection’, which is a common step in machine learning to transform data into a lower-dimensional space. The text input is directly fed into the model. Both the projected image data and the text input are then concatenated into ‘Concatenated Tokens’, which are then processed by a component labeled ‘Gemma’. The final output of the model is ‘Text Output’, which suggests that the model is designed to generate text based on the input data.”\u003c/blockquote\u003e\u003cp\u003eIt’s a clear and detailed breakdown of the PaliGemma architecture. The model accurately identifies image and text inputs, the \u003ca href=\"https://huggingface.co/docs/transformers/main/en/model_doc/siglip\"\u003eSigLIP image encoder\u003c/a\u003e, and the linear projection step. It also grasps how the image and text inputs are combined and processed to generate text output. I should admit that the response is quite neat, offering a solid explanation of the architecture.\u003c/p\u003e\u003cp\u003eLet’s try a video input instead. We’ll upload the video about \u003ca href=\"https://www.gennyapp.com\"\u003eGenny\u003c/a\u003e, a software, and ask a question to see how well Reka Core handles video comprehension.\u003c/p\u003e\u003cblockquote\u003eWhat is this video about?\u003c/blockquote\u003e\u003cblockquote\u003e“This video is a screen recording of a group of friends discussing their plans for the weekend.”\u003c/blockquote\u003e\u003cp\u003eWell, it’s not a good answer compared to \u003ca href=\"https://www.smashingmagazine.com/2024/08/integrating-image-to-text-and-text-to-speech-models-part2/#visual-instruction-tuning-and-llava\"\u003ewhat LLaVA gave us in the previous article\u003c/a\u003e. Now let’s try a longer video, but with the Reka Flash model. The video is about a TTS model called \u003ca href=\"https://github.com/huggingface/parler-tts\"\u003eParler TTS\u003c/a\u003e. Instead of asking what the video is about, let’s prompt it like this:\u003c/p\u003e\u003cblockquote\u003eI’m an AI developer, and I’m not sure how this TTS model can help me in my projects. Can you explain how I can use it or what it’s useful for?\u003c/blockquote\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/2-reka-flash-answer.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"423\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/2-reka-flash-answer.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/2-reka-flash-answer.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/2-reka-flash-answer.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/2-reka-flash-answer.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/2-reka-flash-answer.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/2-reka-flash-answer.png\" sizes=\"100vw\" alt=\"Reka Flash’s answer\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/2-reka-flash-answer.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eReka Flash generates a solid answer this time:\u003c/p\u003e\u003cblockquote\u003e“Of course! This TTS (Text-to-Speech) model can be incredibly useful for developers, especially those working on projects that involve creating more human-like interactions or providing audio feedback to users. Here are some ways you can leverage this TTS model in your projects:\u003cbr/\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eAccessibility:\u003c/strong\u003e TTS models can greatly enhance the accessibility of your applications for users with visual impairments or reading difficulties. By providing spoken feedback or narrating content, you can make your applications more inclusive\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoice Assistants:\u003c/strong\u003e You can integrate this TTS model into voice assistants or chatbots, allowing them to generate more natural-sounding responses. This can improve user experience and make interactions with your bots more engaging.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMultimedia Applications:\u003c/strong\u003e This TTS model can be used in multimedia applications, such as e-books, educational tools, or games, to provide spoken narration or instructions. This can make the content more engaging and easier to understand.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAudio Notifications:\u003c/strong\u003e You can use the TTS model to generate audio notifications for your applications, providing users with spoken alerts or reminders. This can be particularly useful for users who may not always have their screens visible.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLanguage Learning:\u003c/strong\u003e The TTS model can be used in language learning applications to provide users with correct pronunciation and intonation. It can also generate spoken feedback or narrate lessons, making the learning process more immersive.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAudio Books:\u003c/strong\u003e You can use the TTS model to generate audio books, allowing users to listen to written content instead of reading it. This can be especially helpful for users who prefer auditory learning or have difficulty reading.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eExplainer Videos:\u003c/strong\u003e The TTS model can be used to generate voiceovers for explainer videos, providing users with spoken narration that explains the content on the screen. This can make the content more engaging and easier to understand.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAudio Feedback:\u003c/strong\u003e You can use the TTS model to generate audio feedback for users in your applications, such as confirming actions, providing instructions, or giving feedback on their performance. This can make the interactions more natural and intuitive.\u003c/li\u003e\u003c/ol\u003eIn summary, the TTS model can be a valuable tool for developers looking to create more human-like interactions, improve accessibility, and enhance the user experience in various applications.”\u003c/blockquote\u003e\u003cp\u003eIt’s a good answer that’s informative and quite practical for developers. Let’s compare what happens when we try the Reka Edge model with a PDF input titled “Meta AI — Responsible Use Guide” (\u003ca href=\"https://github.com/Meta-Llama/llama/blob/main/Responsible-Use-Guide.pdf\"\u003ePDF\u003c/a\u003e).\u003c/p\u003e\u003cblockquote\u003eAs an AI developer, how can I ensure that I am following the responsible AI guidelines outlined in this document?\u003c/blockquote\u003e\u003cblockquote\u003e“To ensure that your use of AI aligns with the principles outlined in this guide, you can follow these steps and best practices:\u003cbr/\u003e\u003col\u003e\u003cli\u003eFamiliarize yourself with the principles of responsible AI, such as transparency, accountability, and user empowerment, as outlined in the guide.\u003c/li\u003e\u003cli\u003eUnderstand the potential impacts of AI on users and society, and be aware of the risks and challenges associated with the development and deployment of AI systems.\u003c/li\u003e\u003cli\u003eEmbrace an iterative model-development mindset, continuously striving for improvement and learning from feedback and errors.\u003c/li\u003e\u003cli\u003eBuild a responsible AI ecosystem by working with partners, experts, and communities to develop and share best practices, tools, and benchmarking resources.\u003c/li\u003e\u003cli\u003eFollow the guidelines and best practices outlined in the guide for each stage of model development, from data collection and preparation to fine-tuning, evaluation, and deployment.\u003c/li\u003e\u003cli\u003eConsider the specific use case and requirements for your AI system, and tailor your approach accordingly.\u003c/li\u003e\u003cli\u003eRegularly monitor and evaluate the performance and safety of your AI system and make adjustments as needed to ensure it aligns with the principles outlined in the guide.\u003c/li\u003e\u003cli\u003eBe transparent and accountable in your interactions with users and stakeholders, and be open to feedback and suggestions for improvement.\u003c/li\u003e\u003cli\u003eStay informed about the latest developments and research in the field of AI, and be prepared to adapt your approach as needed to address new challenges and opportunities.\u003c/li\u003e\u003cli\u003eFollow the specific guidelines and best practices outlined in the addendum for Code Llama, such as defining content policies, evaluating and benchmarking models, and addressing input- and output-level risks.”\u003c/li\u003e\u003c/ol\u003e\u003c/blockquote\u003e\u003cp\u003eReka Edge’s answer is quite thorough and detailed, too!\u003c/p\u003e\u003cp\u003eIf you want to dive into coding, start by \u003ca href=\"https://auth.reka.ai/u/signup?state=hKFo2SBqU1JxcUhoNGt1bThTTXY5R2JYQTRzYl9GczZrQjNHc6Fur3VuaXZlcnNhbC1sb2dpbqN0aWTZIHVXTExuRjRlcFJ6X3laTUtRTGhPb0JaRVpLN1ppOXdFo2NpZNkgYmFxNFExOG9Tc0JpV053RGFOQzRtNWhmZ1FHd1V1cE0\"\u003ecreating a Reka account\u003c/a\u003e and getting an API key. Then you can follow along with this \u003ca href=\"https://colab.research.google.com/github/reka-ai/colab/blob/main/reka_demo.ipynb#scrollTo=8bf9f004\"\u003enotebook\u003c/a\u003e. Try out their Python SDK, and make sure to explore the different \u003ca href=\"https://docs.reka.ai/available-models\"\u003emodels available.\u003c/a\u003e It’s a great way to see how these tools work under the hood!\u003c/p\u003e\u003ch2 id=\"gemini-models\"\u003eGemini Models\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/\"\u003eGemini 1.5\u003c/a\u003e, developed by \u003ca href=\"https://deepmind.google/about/\"\u003eGoogle DeepMind\u003c/a\u003e, is designed to handle complex tasks efficiently, thanks to the new Mixture-of-Experts (MoE) system. MoE is like packing several specialized models inside one big model. Instead of using the whole network for every task, Gemini 1.5 activates only the parts (or what they call “experts”) that are most relevant. This means Gemini 1.5 can tackle complex tasks with less processing power than a typical monolithic model.\u003c/p\u003e\u003cp\u003eWe can explore Gemini’s features in \u003ca href=\"https://ai.google.dev/aistudio\"\u003eGoogle AI Studio\u003c/a\u003e since it’s already baked right in. We’ll begin by uploading the following image illustrating VLM capabilities.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/3-vision-language-model.jpeg\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"450\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/3-vision-language-model.jpeg 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/3-vision-language-model.jpeg 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/3-vision-language-model.jpeg 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/3-vision-language-model.jpeg 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/3-vision-language-model.jpeg 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/3-vision-language-model.jpeg\" sizes=\"100vw\" alt=\"Illustration of the vision language model’s capabilitiues\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/3-vision-language-model.jpeg\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eWe’ll ask Gemini to analyze the image and extract details or concepts:\u003c/p\u003e\u003cblockquote\u003eExplain this image. What are the specific details or concepts it represents?\u003c/blockquote\u003e\u003cblockquote\u003e“The image is a flowchart illustrating different types of image understanding tasks with a Vision Language Model.\u003cbr/\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eObject Localization:\u003c/strong\u003e This task aims to identify the precise location of objects within an image. The example question asks, “Is one cat behind another?”\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eZero-shot Segmentation:\u003c/strong\u003e This task involves segmenting an image into different regions based on a given label, even if the model has never seen that specific label before. The example question asks to segment “striped cat.”\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eZero-shot Visual QA:\u003c/strong\u003e This task requires answering questions about an image, even if the model has not been specifically trained to answer that type of question. The example question asks, “What is the breed of these cats?”\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOne-shot Learning with Instructions:\u003c/strong\u003e This task involves providing the model with a single example and instruction to learn a new concept and apply it to other images. The example question asks, “Striped cats are called tabby cats. What is the breed of the cats in the image?”\u003c/li\u003e\u003c/ul\u003e\u003c/blockquote\u003e\u003cp\u003eWell, Gemini nailed it — it clearly understood the image and recognized the tasks a VLM handles. This is exactly what you’d expect from a model designed for advanced image analysis.\u003c/p\u003e\u003cp\u003eNow, we’ll upload another image — this time of an Italian dish — and ask Gemini if it can recognize the dishes and list the ingredients needed for them. This type of “food recognition task” will help us test its ability to identify foods and ingredients.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/4-italian-food.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"354\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/4-italian-food.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/4-italian-food.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/4-italian-food.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/4-italian-food.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/4-italian-food.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/4-italian-food.png\" sizes=\"100vw\" alt=\"Various Italian dishes\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/4-italian-food.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cblockquote\u003eList the different dishes in this image, also from which country, and give me the ingredients I need to make them.\u003c/blockquote\u003e\u003cblockquote\u003e“The image shows an Italian feast with the following dishes:\u003cbr/\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eSeafood Stew:\u003c/strong\u003e A rich and flavorful stew with seafood like clams, mussels, shrimp, fish, and vegetables in a tomato broth.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpaghetti with Tomato Sauce:\u003c/strong\u003e Classic spaghetti pasta tossed in a simple yet flavorful tomato sauce.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMeatballs in Tomato Sauce:\u003c/strong\u003e Tender and juicy meatballs simmered in a rich tomato sauce.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLasagna:\u003c/strong\u003e Layered pasta with meat sauce, cheese, and tomato sauce.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSide Salad:\u003c/strong\u003e A refreshing side salad with mixed greens, tomatoes, and other vegetables.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGarlic Bread:\u003c/strong\u003e Toasted bread slices rubbed with garlic and olive oil.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMarinara Sauce:\u003c/strong\u003e A simple tomato-based sauce.\u003c/li\u003e\u003c/ol\u003eThis is a typical Italian meal with various dishes showcasing the flavors of Italian cuisine.\u003cp\u003eAs for the ingredients, you’ll need tomatoes for many of these dishes, specifically for the sauces and the side salad.”\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003eWell, the model did a good job recognizing the dishes, but it also didn’t give me all the necessary ingredients. Let’s see how it does with videos instead, using the same video we used earlier with the Reka models. This time, the task will focus on \u003cstrong\u003eaction recognition\u003c/strong\u003e.\u003c/p\u003e\u003cblockquote\u003eWhat is the purpose of this video, and what key actions or processes are being demonstrated?\u003c/blockquote\u003e\u003cblockquote\u003e“The video is a tutorial about the importance of friendship and how it can impact our lives. It demonstrates the use of a video editing software called “Genmy” to create a video and add text subtitles. The video creator is using Genmy to:\u003cbr/\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRecord their voice:\u003c/strong\u003e This is seen at the top of the screen with the mic icon.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAdd text to speech:\u003c/strong\u003e You see the “Text to Speech” option, which is likely being used to generate subtitles or captions for the video.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEdit the timeline:\u003c/strong\u003e They’re working with a video timeline, adding and arranging different elements.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAdd voice-over:\u003c/strong\u003e The speaker is adding commentary to the video.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCreate a visually appealing presentation:\u003c/strong\u003e They’re using a combination of video footage, text, and voice-over to create an engaging tutorial.”\u003c/li\u003e\u003c/ul\u003e\u003c/blockquote\u003e\u003cp\u003eWell, Gemini gave a clear breakdown of the video, identifying it as a tutorial demonstrating the \u003ca href=\"https://www.genmy.ai\"\u003eGenmy video editing software\u003c/a\u003e to highlight how it’s used for recording voice, adding text-to-speech, editing video timelines, and allowing voice-over. Impressive!\u003c/p\u003e\u003cp\u003eLet’s upload one more video, the same one we used for Reka about the Parler TTS model, and ask Gemini to \u003cstrong\u003eanalyze and summarize\u003c/strong\u003e the contents.\u003c/p\u003e\u003cblockquote\u003eSummarize this video and explain how the contents might be relevant or helpful for software engineers.\u003c/blockquote\u003e\u003cblockquote\u003e“This video showcases different voice samples generated by a text-to-speech (TTS) system called “Parler-TTS.” The video highlights various voice characteristics like gender, pace, pitch, and even the simulated environment. For software engineers, this video demonstrates the progress made in TTS technology. These are some insights:\u003cbr/\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eVoice Variety:\u003c/strong\u003e The video shows that Parler-TTS can create diverse voices, potentially useful for applications needing different personas or tones.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFine-grained Control:\u003c/strong\u003e The video emphasizes the ability to control voice parameters like pace and pitch, showcasing the potential for customizability.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eQuality and Naturalness:\u003c/strong\u003e The video focuses on the quality of the generated speech, demonstrating the advancements made in speech synthesis, which is crucial for user experience.”\u003c/li\u003e\u003c/ul\u003e\u003c/blockquote\u003e\u003cp\u003eNicely done! I can go with that answer. Gemini explains adjusting voice settings, like pitch and speed, and how having different voices can be useful. Gemini also emphasizes the importance of natural, high-quality speech, which is handy for developers working with TTS systems!\u003c/p\u003e\u003cp\u003eAlright, for coding, you can grab the code from Google AI Studio by clicking the \u003cstrong\u003eGet Code\u003c/strong\u003e button. You can choose between formatting the code in Python, Swift, and Java, among other languages.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/5-gemini-api.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"599\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/5-gemini-api.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/5-gemini-api.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/5-gemini-api.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/5-gemini-api.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/5-gemini-api.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/5-gemini-api.png\" sizes=\"100vw\" alt=\"A screenshot of the code in Google AI Studio\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/5-gemini-api.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eBoth Reka and Gemini are strong multimodal models for AI applications, but there are key differences between them to consider. Here’s a table that breaks those down:\u003c/p\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eFeature\u003c/th\u003e\u003cth\u003eReka\u003c/th\u003e\u003cth\u003eGemini 1.5\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eMultimodal Capabilities\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eImage, video, and text processing\u003c/td\u003e\u003ctd\u003eImage, video, text, with extended token context\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eEfficiency\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eOptimized for multimodal tasks\u003c/td\u003e\u003ctd\u003eBuilt with MoE for efficiency\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eContext Window\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eStandard token window\u003c/td\u003e\u003ctd\u003eUp to two million tokens (with Flash variant)\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eArchitecture\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eFocused on multimodal task flow\u003c/td\u003e\u003ctd\u003eMoE improves specialization\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eTraining/Serving\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eHigh performance with efficient model switching\u003c/td\u003e\u003ctd\u003eMore efficient training with MoE architecture\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eDeployment\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eSupports on-device deployment\u003c/td\u003e\u003ctd\u003ePrimarily cloud-based, with Vertex AI integration\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eUse Cases\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eInteractive apps, edge deployment\u003c/td\u003e\u003ctd\u003eSuited for large-scale, long-context applications\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eLanguages Supported\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eMultiple languages\u003c/td\u003e\u003ctd\u003eSupports many languages with long context windows\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cp\u003e\u003cstrong\u003eReka\u003c/strong\u003e stands out for on-device deployment, which is super useful for apps requiring offline capabilities or low-latency processing.\u003c/p\u003e\u003cp\u003eOn the other hand, \u003cstrong\u003eGemini 1.5 Pro\u003c/strong\u003e shines with its long context windows, making it a great option for handling large documents or complex queries in the cloud.\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://www.smashingmagazine.com/images/logo/logo--red.png\" alt=\"Smashing Editorial\" width=\"35\" height=\"46\" loading=\"lazy\" decoding=\"async\"/\u003e\n\u003cspan\u003e(gg, yk)\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "21 min read",
  "publishedTime": "2024-10-11T10:00:00Z",
  "modifiedTime": "2024-10-11T10:00:00Z"
}
