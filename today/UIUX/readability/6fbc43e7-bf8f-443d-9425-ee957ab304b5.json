{
  "id": "6fbc43e7-bf8f-443d-9425-ee957ab304b5",
  "title": "How AI Models Are Trained",
  "link": "https://www.nngroup.com/articles/ai-model-training/?utm_source=rss\u0026utm_medium=feed\u0026utm_campaign=rss-syndication",
  "description": "Training modern LLMs is a costly process that shapes the model’s outputs and involves unsupervised, supervised, and reinforcement learning.",
  "author": "Tanner Kohler",
  "published": "Fri, 02 May 2025 21:00:00 +0000",
  "source": "https://www.nngroup.com/feed/rss/",
  "categories": [
    "Article"
  ],
  "byline": "Tanner Kohler",
  "length": 13131,
  "excerpt": "Training modern LLMs is a costly process that shapes the model’s outputs and involves unsupervised, supervised, and reinforcement learning.",
  "siteName": "Nielsen Norman Group",
  "favicon": "",
  "text": "Summary:  Training modern LLMs is a costly process that shapes the model’s outputs and involves unsupervised, supervised, and reinforcement learning. By this point, you've undoubtedly heard that the large language model (LLM) behind your favorite AI tool has been “trained on the whole internet.” To some extent, that’s true, but after training hundreds of UX professionals on how to use AI in their work, it’s clear that many don’t understand how the AI is trained. This is crucial for forming an accurate mental model of how these LLMs work, their limitations, and their capabilities. This article discusses four basic types of training, when these are performed within an LLM, and how they impact the role of AI in user experience. 1. The Pretraining Phase: Unsupervised Learning 2. The Finetuning Phase: Supervised Learning 3. Advanced Finetuning: Reinforcement Learning with Human Feedback (RLHF) LLMs Differ from Search Engines Environmental and Labor Costs 1. The Pretraining Phase: Unsupervised Learning When you've heard that large language models have been \"trained on the whole internet,\" people are typically talking about the pretraining phase, which involves unsupervised learning. During this phase, the model is fed enormous amounts of text and data scraped from the internet, digitized books, code repositories, and more. One example of such datasets is Common Crawl, which has terabytes of data from billions of webpages. Cleaning these massive datasets before use requires significant effort. The sheer volume of data makes it impossible for humans to label or explain it all. Instead, the model learns patterns on its own by trying to predict the next word (or \"token\") in a sequence. From exposure to word combinations across billions of examples, the model learns grammar, facts, reasoning abilities (of a sort), and even the biases present in the data. During the pretraining phase, the AI model is not learning specific tasks or 'meaning' in the human sense. It's pretty much all statistical relationships: which words are most likely to follow other words in different contexts. Unsupervised Learning Is Like a Toddler Think of unsupervised learning like a toddler immersed in language for the first two years of life. They hear countless conversations. They aren't explicitly taught every grammatical rule, but they start absorbing patterns. Eventually, they begin stringing words together in ways that mimic what they've heard, sometimes surprising you with sentences they weren't directly taught and whose meaning they don't fully grasp. Unsupervised Learning for AI-Based Design While models differ in the types of training data they use, the patterns they learn, and how they rely on these patterns while generating outputs, the principle is the same. Take Figma AI as an example: for its generative AI features, it needs a large amount of training data as a solid foundation. However, rather than starting from scratch and pretraining a brand new model by feeding it many designs created in Figma (which would have major privacy issues and be extremely costly), they used “third-party, out-of-the-box AI models.” These third-party models call APIs from more robust AI companies, like perhaps OpenAI. So, how can Figma AI generate user interfaces if it relies on models pretrained on text? Because the pretraining datasets they’re using are so vast, they’ve processed billions of lines of code and learned the patterns in how interfaces are put together. However, this doesn’t guarantee that asking it to “create a dashboard to display recent sales data” will create something useful, accessible, or reasonable. That requires finetuning. 2. The Finetuning Phase: Supervised Learning If the pretraining phase teaches the model about raw patterns, the finetuning phase (which uses supervised learning) is like giving it specific lessons and examples. Now, the toddler is older and you’re sending them off to school, where a teacher will give them many examples of correct and incorrect sentences. These carefully selected examples are meant to teach the child how to use the vast vocabulary they acquired during unsupervised learning. To train AI models, researchers create much smaller datasets containing carefully crafted examples of inputs (prompts) and desired outputs (responses). For instance, a researcher might write a specific prompt and pair it with an ideal response they want the model to emulate. They might even create several variations of the response and rate them on criteria like helpfulness, clarity, or safety. By showing the model thousands of these carefully crafted examples, they teach it to use the patterns identified during pretraining in useful, truthful ways that are aligned with human expectations. Note that the researchers are still providing all their guidance at input. They are not paying much attention yet to the AI’s outputs. Huggingface: researcher-written example of a prompt, a response, and response ratings to be used for finetuning Supervised-learning datasets are much smaller than those used for unsupervised learning but require significant human effort and cost to create. This phase is crucial for specializing the model and improving its ability to follow instructions. However, bias can creep in here too. The data used for pretraining already contains societal biases (e.g., due to certain demographics or viewpoints being overrepresented online). During finetuning, the specific examples created and rated by humans also introduce the raters’ perspectives, values, and potential biases. Different labeling-data teams might instill slightly different \"manners\" in the model. This is partly why even LLMs that were trained on vast amounts of the same data can have different tones, personalities, and approaches. They’ve been finetuned slightly differently. Supervised Learning for AI-Based Design Supervised learning isn't just for chatbots. Even Figma has finetuned some of its AI features to improve the outputs, though it mentions only features like Visual Search, Asset Search, and Add Interactions — not the text-to-UI features First Draft or AI Prototyping. Figma’s finetuning used “data from public, free community files” — that is, it relied on freely available designs rather than on high-quality ones created or vetted by Figma employees — a sensible approach that is cheap, quick, and doesn’t violate any privacy restrictions. However, to return to our analogy, the toddler’s “teacher” is using free examples from the internet rather than a curriculum designed by experts. This does not mean that Figma AI will fail to produce anything useful; it simply means that the outputs are strongly biased toward the patterns available in the free, public files. Don’t be surprised if the outputs feel generic — Figma itself says, “we need to train models that better understand design concepts and patterns.” Only when AI tools for creating UIs have been carefully finetuned by expert designers will we see as much nuanced power in AI-generated UI design as we currently see in AI-generated language. 3. Advanced Finetuning: Reinforcement Learning with Human Feedback (RLHF) This final type of training is an advanced fine-tuning technique that relies on human judgment of the model’s outputs. Now the child is doing writing exercises using the vocabulary they’ve absorbed (unsupervised learning) according to the rules and examples they’ve been taught (supervised learning). The teacher provides feedback on the child’s work. Then the child adjusts their process based on the teacher’s immediate feedback. Reinforcement learning with human feedback is much the same: humans coach the AI model based on its outputs, often guided by how they feel about the outputs when the success criteria are difficult to define. For tasks with clear, reliable success criteria, such as a robotaxi safely navigating to a predetermined destination without crashing, AI models can iteratively learn on their own based on success or failure (often simply called reinforcement learning). But what happens when success isn't easily defined by a simple rule, such as the difference between good and bad or useful and not useful? RLHF brings humans into the loop during the reinforcement-learning process. The model might generate two or more responses to a prompt. A human labeler is then asked to rank these responses — which is better? More helpful? More harmless? If you’ve ever been presented with multiple versions of a response while using an LLM, you have been involved in RLHF. Gathering this feedback from real users allows AI companies to gather feedback reflecting real user preferences at scale. ChatGPT: The user is presented with two variations of the same response and asked to choose which one they prefer as a form of reinforcement learning with human feedback. It’s important to understand that this human-preference data isn't used to directly reward or punish the main LLM. Instead, it's used to train a separate reward model (or preference model). This reward model learns to predict which kinds of responses humans tend to prefer. Then, the main LLM uses this reward model as a guide, trying to generate responses that the reward model would score highly. Essentially, the reward model learns patterns from the human feedback to create the rules of the game, then scores the main LLM on how well it's able to play by these rules. User feedback trains a reward model, which then helps shape the AI model. Again, bias is a concern. If the group providing feedback isn't representative of the diverse user base, its preferences and biases will disproportionately shape the model's behavior. We know this is still the case because, as with most technologies, the adoption of AI is not equal among all demographics. RLHF for AI-Based Design If you are involved in building any user-facing AI products, such as a chatbot on your website, or an interactive voice-response system for customer-support calls, RLHF is absolutely critical for ensuring that the model is giving users useful responses. Yes, through unsupervised learning, you can train such a system to understand your company's products and services, and through supervised learning, you can give it good examples of responses, but you are not the user. Your team can only provide so much feedback to the model regarding what it ought to say. You must also account for the reactions of the real users interacting with the product to help refine its responses over time. External vendors selling these types of services, or teams building them in-house, need to plan for some form of reward model that can continue to finetune responses based on what real users think about the outputs. LLMs Differ from Search Engines This is one of the places I see UX practitioners getting the most confused. Search Engines Retrieve Existing Information When you enter a query, a search engine searches its indexed database of web pages and documents to find and rank the most relevant existing content. Then it points you to those sources. If there is nothing in the database, it can’t give you a response. Large Language Models Generate New Information When you give an LLM a prompt, it uses the statistical patterns learned during training to predict the sequence of words most likely to form a relevant response. It's constructing the answer word by word based on probabilities, not retrieving a prewritten answer from a database. This generative nature is why LLMs can hallucinate — confidently state incorrect information. Their primary goal is to generate a plausible-sounding sequence of words relevant to the prompt, not necessarily to state the verified truth found in their training data (which might contain errors). While some newer AI tools integrate search results (through a technique called retrieval-augmented generation or RAG) to pull in real-time information before generating an answer, the core LLM response is still generated, not retrieved. Environmental and Labor Costs Training and running AI models come with significant costs: Environmental cost. Training large models via unsupervised learning requires immense computational power. It involves running thousands of specialized processors for weeks or months, consuming substantial amounts of electricity, and contributing to carbon emissions. Even running the model for users like you and me consumes energy, especially at the scale these services operate. Labor cost. While unsupervised learning does not rely on direct human labeling, reinforcement learning with human feedback and at least some types of supervised learning depend heavily on human work. Thousands of people around the world are employed to create training examples, write prompts, rate model responses, and check for harmful content. This data-labeling work is often outsourced, sometimes low-paid, and can involve exposure to sensitive or problematic content, raising ethical considerations about the conditions and compensation for this crucial human-intelligence work powering the AI.",
  "image": "https://media.nngroup.com/media/articles/opengraph_images/Social-AI-Training_Opengraph.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp\u003e\u003cspan\u003e\n                  Summary: \n                \u003c/span\u003eTraining modern LLMs is a costly process that shapes the model’s outputs and involves unsupervised, supervised, and reinforcement learning.\n              \u003c/p\u003e\u003cdiv\u003e\n              \u003cp\u003eBy this point, you\u0026#39;ve undoubtedly heard that the large language model (LLM) behind your favorite AI tool has been “trained on the whole internet.” To some extent, that’s true, but after \u003ca href=\"https://www.nngroup.com/courses/ai-for-design/\"\u003etraining hundreds of UX professionals on how to use AI in their work\u003c/a\u003e, it’s clear that many don’t understand \u003cem\u003ehow\u003c/em\u003e the AI is trained. This is crucial for forming an accurate \u003ca href=\"https://www.nngroup.com/articles/mental-models/\"\u003emental model\u003c/a\u003e of how these LLMs work, their limitations, and their capabilities.\u003c/p\u003e\n\u003cp\u003eThis article discusses four basic types of training, when these are performed within an LLM, and how they impact the role of AI in user experience.\u003c/p\u003e\n\u003cdiv\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-1-the-pretraining-phase-unsupervised-learning-1\"\u003e1. The Pretraining Phase: Unsupervised Learning\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-2-the-finetuning-phase-supervised-learning-2\"\u003e2. The Finetuning Phase: Supervised Learning\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-3-advanced-finetuning-reinforcement-learning-with-human-feedback-rlhf-3\"\u003e3. Advanced Finetuning: Reinforcement Learning with Human Feedback (RLHF)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-llms-differ-from-search-engines-4\"\u003eLLMs Differ from Search Engines\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-environmental-and-labor-costs-5\"\u003eEnvironmental and Labor Costs\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\u003ch2 id=\"toc-1-the-pretraining-phase-unsupervised-learning-1\"\u003e1. The Pretraining Phase: Unsupervised Learning\u003c/h2\u003e\n\u003cp\u003eWhen you\u0026#39;ve heard that large language models have been \u0026#34;trained on the whole internet,\u0026#34; people are typically talking about the pretraining phase, which involves unsupervised learning.\u003c/p\u003e\n\u003cp\u003eDuring this phase, the model is fed enormous amounts of text and data scraped from the internet, digitized books, code repositories, and more. One example of such datasets is \u003ca href=\"https://commoncrawl.org/\"\u003eCommon Crawl\u003c/a\u003e, which has terabytes of data from billions of webpages. Cleaning these massive datasets before use requires significant effort.\u003c/p\u003e\n\u003cp\u003eThe sheer volume of data makes it impossible for humans to label or explain it all. Instead, the model learns \u003cstrong\u003epatterns\u003c/strong\u003e on its own by trying to \u003cstrong\u003epredict the next word (or \u0026#34;\u003c/strong\u003e\u003ca href=\"https://www.nngroup.com/articles/how-ai-works/\"\u003e\u003cstrong\u003etoken\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e\u0026#34;)\u003c/strong\u003e in a sequence.\u003c/p\u003e\n\u003cp\u003eFrom exposure to word combinations across billions of examples, the model learns grammar, facts, reasoning abilities (of a sort), and even the biases present in the data.\u003c/p\u003e\n\u003cp\u003eDuring the pretraining phase, the AI model is \u003cem\u003enot\u003c/em\u003e learning specific tasks or \u0026#39;meaning\u0026#39; in the human sense. It\u0026#39;s pretty much all statistical relationships: which words are most likely to follow other words in different contexts.\u003c/p\u003e\n\u003ch3\u003eUnsupervised Learning Is Like a Toddler\u003c/h3\u003e\n\u003cp\u003eThink of unsupervised learning like a toddler immersed in language for the first two years of life. They hear countless conversations. They aren\u0026#39;t explicitly taught every grammatical rule, but they start absorbing patterns.\u003c/p\u003e\n\u003cp\u003eEventually, they begin stringing words together in ways that mimic what they\u0026#39;ve heard, sometimes surprising you with sentences they weren\u0026#39;t directly taught and whose meaning they don\u0026#39;t fully grasp.\u003c/p\u003e\n\u003ch3\u003eUnsupervised Learning for AI-Based Design\u003c/h3\u003e\n\u003cp\u003eWhile models differ in the types of training data they use, the patterns they learn, and how they rely on these patterns while generating outputs, the principle is the same.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTake Figma AI as an example:\u003c/strong\u003e for its generative AI features, it needs a large amount of training data as a solid foundation. However, rather than starting from scratch and pretraining a brand new model by feeding it many designs created in Figma (which would have major privacy issues and be extremely costly), they used “\u003ca href=\"https://www.figma.com/ai/our-approach/\"\u003ethird-party, out-of-the-box AI models.\u003c/a\u003e” These third-party models call APIs from more robust AI companies, like perhaps OpenAI.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSo, how can Figma AI generate user interfaces if it relies on models pretrained on text?\u003c/strong\u003e Because the pretraining datasets they’re using are so vast, they’ve processed billions of lines of code and learned the patterns in how interfaces are put together. However, this doesn’t guarantee that asking it to “create a dashboard to display recent sales data” will create something useful, accessible, or reasonable. \u003cstrong\u003eThat requires finetuning.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"toc-2-the-finetuning-phase-supervised-learning-2\"\u003e2. The Finetuning Phase: Supervised Learning\u003c/h2\u003e\n\u003cp\u003eIf the pretraining phase teaches the model about raw patterns, the finetuning phase (which uses supervised learning) is like giving it specific lessons and examples.\u003c/p\u003e\n\u003cp\u003eNow, the toddler is older and you’re sending them off to school, where a teacher will give them many examples of correct and incorrect sentences. These carefully selected examples are meant to teach the child how to use the vast vocabulary they acquired during unsupervised learning.\u003c/p\u003e\n\u003cp\u003eTo train AI models, researchers create much smaller datasets containing carefully crafted examples of inputs (prompts) and desired outputs (responses).\u003c/p\u003e\n\u003cp\u003eFor instance, a researcher might write a specific prompt and pair it with an ideal response they want the model to emulate. They might even create several variations of the response and rate them on criteria like helpfulness, clarity, or safety. By showing the model thousands of these carefully crafted examples, they teach it to use the patterns identified during pretraining in useful, truthful ways that are aligned with human expectations. Note that the researchers are still providing all their guidance at input. They are not paying much attention yet to the AI’s outputs.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg alt=\"A model response to a prompt about assistive devices, with scores for helpfulness, correctness, coherence, complexity, and verbosity.\" height=\"3704\" loading=\"lazy\" src=\"https://media.nngroup.com/media/editor/2025/05/01/code2.png\" width=\"7092\"/\u003e\n\u003cfigcaption\u003e\u003cem\u003eHuggingface: researcher-written example of a prompt, a response, and response ratings to be used for finetuning\u003c/em\u003e\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eSupervised-learning datasets are much smaller than those used for unsupervised learning but require significant human effort and cost to create. This phase is crucial for specializing the model and improving its ability to follow instructions.\u003c/p\u003e\n\u003cp\u003eHowever, \u003cstrong\u003ebias can creep in here\u003c/strong\u003e too. The data used for pretraining already contains societal biases (e.g., due to certain demographics or viewpoints being overrepresented online). During finetuning, the specific examples created and rated by humans also introduce \u003cem\u003ethe raters’\u003c/em\u003e perspectives, values, and potential biases.\u003c/p\u003e\n\u003cp\u003eDifferent labeling-data teams might instill slightly different \u0026#34;manners\u0026#34; in the model. This is partly why even LLMs that were trained on vast amounts of the same data can have different tones, personalities, and approaches. They’ve been finetuned slightly differently.\u003c/p\u003e\n\u003ch3\u003eSupervised Learning for AI-Based Design\u003c/h3\u003e\n\u003cp\u003eSupervised learning isn\u0026#39;t just for chatbots. Even Figma has finetuned some of its AI features to improve the outputs, though it mentions only features like \u003cem\u003eVisual Search\u003c/em\u003e, \u003cem\u003eAsset Search\u003c/em\u003e, and \u003cem\u003eAdd Interactions\u003c/em\u003e — not the text-to-UI features \u003cem\u003eFirst Draft\u003c/em\u003e or \u003cem\u003eAI Prototyping\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eFigma’s finetuning used “data from public, free community files” — that is, it relied on freely available designs rather than on high-quality ones created or vetted by Figma employees — a sensible approach that is cheap, quick, and doesn’t violate any privacy restrictions. However, to return to our analogy, \u003cstrong\u003ethe toddler’s “teacher” is using free examples from the internet rather than a curriculum designed by experts.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis does not mean that Figma AI will fail to produce anything useful; it simply means that the outputs are strongly biased toward the patterns available in the free, public files. \u003cstrong\u003eDon’t be surprised if the outputs feel generic —\u003c/strong\u003e Figma itself says, “we need to train models that better understand design concepts and patterns.”\u003c/p\u003e\n\u003cp\u003eOnly when AI tools for creating UIs have been carefully finetuned by expert designers will we see as much nuanced power in AI-generated UI design as we currently see in AI-generated language.\u003c/p\u003e\n\u003ch2 id=\"toc-3-advanced-finetuning-reinforcement-learning-with-human-feedback-rlhf-3\"\u003e3. Advanced Finetuning: Reinforcement Learning with Human Feedback (RLHF)\u003c/h2\u003e\n\u003cp\u003eThis final type of training is an advanced fine-tuning technique that relies on human judgment of the model’s outputs. Now the child is doing writing exercises using the vocabulary they’ve absorbed (unsupervised learning) according to the rules and examples they’ve been taught (supervised learning). The teacher provides feedback on the child’s work. Then the child adjusts their process based on the teacher’s immediate feedback.\u003c/p\u003e\n\u003cp\u003eReinforcement learning with human feedback is much the same: humans coach the AI model based on its outputs, often guided by how they feel about the outputs when the success criteria are difficult to define. For tasks with clear, reliable success criteria, such as a robotaxi safely navigating to a predetermined destination without crashing, AI models can iteratively learn on their own based on success or failure (often simply called \u003cstrong\u003ereinforcement learning\u003c/strong\u003e).\u003c/p\u003e\n\u003cp\u003eBut what happens when success isn\u0026#39;t easily defined by a simple rule, such as the difference between good and bad or useful and not useful?\u003c/p\u003e\n\u003cp\u003eRLHF brings humans into the loop during the reinforcement-learning process. The model might generate two or more responses to a prompt. A human labeler is then asked to rank these responses — which is better? More helpful? More harmless?\u003c/p\u003e\n\u003cp\u003eIf you’ve ever been presented with multiple versions of a response while using an LLM, you have been involved in RLHF. Gathering this feedback from real users allows AI companies to gather feedback reflecting real user preferences at scale.\u003cem\u003e \u003c/em\u003e\u003c/p\u003e\n\u003cfigure\u003e\u003cimg alt=\"Two ChatGPT responses with different layouts of information. At the top, it asks users to choose which they like better.\" height=\"2282\" loading=\"lazy\" src=\"https://media.nngroup.com/media/editor/2025/05/01/screenshot-2025-03-26-at-35053-pm-1.png\" width=\"2712\"/\u003e\n\u003cfigcaption\u003e\u003cem\u003eChatGPT: The user is presented with two variations of the same response and asked to choose which one they prefer as a form of reinforcement learning with human feedback.\u003c/em\u003e\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eIt’s important to understand that this human-preference data isn\u0026#39;t used to \u003cem\u003edirectly\u003c/em\u003e reward or punish the main LLM. Instead, it\u0026#39;s used to train a separate \u003cstrong\u003ereward model\u003c/strong\u003e (or preference model).\u003c/p\u003e\n\u003cp\u003eThis reward model learns to predict which kinds of responses humans tend to prefer. Then, the main LLM uses this reward model as a guide, trying to generate responses that the reward model would score highly. Essentially, the reward model learns patterns from the human feedback to create the rules of the game, then scores the main LLM on how well it\u0026#39;s able to play by these rules.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg alt=\"Diagram showing how a reward model learns from user preferences and gives feedback to improve an LLM\u0026#39;s output through a feedback loop.\" height=\"1324\" loading=\"lazy\" src=\"https://media.nngroup.com/media/editor/2025/05/02/ai-reward-model.jpg\" width=\"1000\"/\u003e\n\u003cfigcaption\u003e\u003cem\u003eUser feedback trains a reward model, which then helps shape the AI model.\u003c/em\u003e\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eAgain, bias is a concern. If the group providing feedback isn\u0026#39;t representative of the diverse user base, its preferences and biases will disproportionately shape the model\u0026#39;s behavior. We know this is still the case because, as with most technologies, the adoption of AI is not equal among all demographics.\u003c/p\u003e\n\u003ch3\u003eRLHF for AI-Based Design\u003c/h3\u003e\n\u003cp\u003eIf you are involved in building any user-facing AI products, such as a chatbot on your website, or an interactive voice-response system for customer-support calls, RLHF is absolutely critical for ensuring that the model is giving users useful responses. Yes, through unsupervised learning, you can train such a system to understand your company\u0026#39;s products and services, and through supervised learning, you can give it good examples of responses, but \u003ca href=\"https://www.nngroup.com/articles/false-consensus/\"\u003eyou are not the user\u003c/a\u003e. Your team can only provide so much feedback to the model regarding what it ought to say. You must also account for the reactions of the real users interacting with the product to help refine its responses over time.\u003c/p\u003e\n\u003cp\u003eExternal vendors selling these types of services, or teams building them in-house, need to plan for some form of reward model that can continue to finetune responses based on what real users think about the outputs.\u003c/p\u003e\n\u003ch2 id=\"toc-llms-differ-from-search-engines-4\"\u003eLLMs Differ from Search Engines\u003c/h2\u003e\n\u003cp\u003eThis is one of the places I see UX practitioners getting the most confused.\u003c/p\u003e\n\u003ch3\u003eSearch Engines Retrieve Existing Information\u003c/h3\u003e\n\u003cp\u003eWhen you enter a query, a search engine searches its indexed database of web pages and documents to find and rank the most relevant \u003cem\u003eexisting\u003c/em\u003e content. Then it points you to those sources. If there is nothing in the database, it can’t give you a response.\u003c/p\u003e\n\u003ch3\u003eLarge Language Models Generate New Information\u003c/h3\u003e\n\u003cp\u003eWhen you give an LLM a prompt, it uses the statistical patterns learned during training to predict the sequence of words most likely to form a relevant response. It\u0026#39;s constructing the answer word by word based on probabilities, not retrieving a prewritten answer from a database.\u003c/p\u003e\n\u003cp\u003eThis generative nature is why LLMs can \u003ca href=\"https://www.nngroup.com/articles/ai-hallucinations/\"\u003ehallucinate\u003c/a\u003e — confidently state incorrect information. Their primary goal is to generate a plausible-sounding sequence of words relevant to the prompt, not necessarily to state the verified truth found in their training data (which might contain errors). While some newer AI tools integrate search results (through a technique called \u003cstrong\u003eretrieval-augmented generation\u003c/strong\u003e or RAG) to pull in real-time information before generating an answer, the core LLM response is still generated, not retrieved.\u003c/p\u003e\n\u003ch2 id=\"toc-environmental-and-labor-costs-5\"\u003eEnvironmental and Labor Costs\u003c/h2\u003e\n\u003cp\u003eTraining and running AI models come with significant costs:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEnvironmental cost.\u003c/strong\u003e Training large models via unsupervised learning requires immense computational power. It involves running thousands of specialized processors for weeks or months, consuming substantial amounts of electricity, and contributing to carbon emissions. Even running the model for users like you and me consumes energy, especially at the scale these services operate.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLabor cost.\u003c/strong\u003e While unsupervised learning does not rely on direct human labeling, reinforcement learning with human feedback and at least some types of supervised learning depend heavily on human work. Thousands of people around the world are employed to create training examples, write prompts, rate model responses, and check for harmful content. This data-labeling work is often outsourced, sometimes low-paid, and can involve exposure to sensitive or problematic content, raising ethical considerations about the conditions and compensation for this crucial human-intelligence work powering the AI.\u003c/li\u003e\n\u003c/ul\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": "2025-05-02T21:00:00Z",
  "modifiedTime": null
}
