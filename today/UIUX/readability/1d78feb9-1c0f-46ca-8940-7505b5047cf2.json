{
  "id": "1d78feb9-1c0f-46ca-8940-7505b5047cf2",
  "title": "Integrating Image-To-Text And Text-To-Speech Models (Part 2)",
  "link": "https://smashingmagazine.com/2024/08/integrating-image-to-text-and-text-to-speech-models-part2/",
  "description": "In the second part of this series, Joas Pambou aims to build a more advanced version of the previous application that performs conversational analyses on images or videos, much like a chatbot assistant. This means you can ask and learn more about your input content.",
  "author": "Joas Pambou",
  "published": "Fri, 30 Aug 2024 09:00:00 GMT",
  "source": "https://www.smashingmagazine.com/feed",
  "categories": null,
  "byline": "About The Author",
  "length": 21296,
  "excerpt": "In the second part of this series, Joas Pambou aims to build a more advanced version of the previous application that performs conversational analyses on images or videos, much like a chatbot assistant. This means you can ask and learn more about your input content.",
  "siteName": "Smashing Magazine",
  "favicon": "https://smashingmagazine.com/images/favicon/apple-touch-icon.png",
  "text": "15 min readAI, Tools, TechniquesIn the second part of this series, Joas Pambou aims to build a more advanced version of the previous application that performs conversational analyses on images or videos, much like a chatbot assistant. This means you can ask and learn more about your input content. Joas also explores multimodal or any-to-any models that handle images, videos, text, and audio, offering a comprehensive view of cutting-edge AI applications.In Part 1 of this brief two-part series, we developed an application that turns images into audio descriptions using vision-language and text-to-speech models. We combined an image-to-text that analyses and understands images, generating description, with a text-to-speech model to create an audio description, helping people with sight challenges. We also discussed how to choose the right model to fit your needs.Now, we are taking things a step further. Instead of just providing audio descriptions, we are building that can have interactive conversations about images or videos. This is known as Conversational AI — a technology that lets users talk to systems much like chatbots, virtual assistants, or agents.While the first iteration of the app was great, the output still lacked some details. For example, if you upload an image of a dog, the description might be something like “a dog sitting on a rock in front of a pool,” and the app might produce something close but miss additional details such as the dog’s breed, the time of the day, or location.(Large preview)The aim here is simply to build a more advanced version of the previously built app so that it not only describes images but also provides more in-depth information and engages users in meaningful conversations about them.We’ll use LLaVA, a model that combines understanding images and conversational capabilities. After building our tool, we’ll explore multimodal models that can handle images, videos, text, audio, and more, all at once to give you even more options and easiness for your applications.Visual Instruction Tuning and LLaVAWe are going to look at visual instruction tuning and the multimodal capabilities of LLaVA. We’ll first explore how visual instruction tuning can enhance the large language models to understand and follow instructions that include visual information. After that, we’ll dive into LLaVA, which brings its own set of tools for image and video processing.Visual Instruction TuningVisual instruction tuning is a technique that helps large language models (LLMs) understand and follow instructions based on visual inputs. This approach connects language and vision, enabling AI systems to understand and respond to human instructions that involve both text and images. For example, Visual IT enables a model to describe an image or answer questions about a scene in a photograph. This fine-tuning method makes the model more capable of handling these complex interactions effectively.There’s a new training approach called LLaVAR that has been developed, and you can think of it as a tool for handling tasks related to PDFs, invoices, and text-heavy images. It’s pretty exciting, but we won’t dive into that since it is outside the scope of the app we’re making.Examples of Visual Instruction Tuning DatasetsTo build good models, you need good data — rubbish in, rubbish out. So, here are two datasets that you might want to use to train or evaluate your multimodal models. Of course, you can always add your own datasets to the two I’m going to mention.Vision-CAIRInstruction datasets: English;Multi-task: Datasets containing multiple tasks;Mixed dataset: Contains both human and machine-generated data.(Large preview)Vision-CAIR provides a high-quality, well-aligned image-text dataset created using conversations between two bots. This dataset was initially introduced in a paper titled “MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models,” and it provides more detailed image descriptions and can be used with predefined instruction templates for image-instruction-answer fine-tuning.LLaVA Visual Instruct 150KInstruction datasets: English;Multi-task: Datasets containing multiple tasks;Mixed dataset: Contains both human and machine-generated data.LLaVA Visual Instruct 150K is a set of GPT-generated multimodal instruction-following data. It is built for visual instruction tuning and aims to achieve GPT-4 level vision and language capabilities.(Large preview)There are more multimodal datasets out there, but these two should help you get started if you want to fine-tune your model.Let’s Take a Closer Look At LLaVALLaVA (which stands for Large Language and Vision Assistant) is a groundbreaking multimodal model developed by researchers from the University of Wisconsin, Microsoft Research, and Columbia University. The researchers aimed to create a powerful, open-source model that could compete with the best in the field, just like GPT-4, Claude 3, or Gemini, to name a few. For developers like you and me, its open nature is a huge benefit, allowing for easy fine-tuning and integration.One of LLaVA’s standout features is its ability to understand and respond to complex visual information, even with unfamiliar images and instructions. This is exactly what we need for our tool, as it goes beyond simple image descriptions to engage in meaningful conversations about the content.Architecture(Large preview)LLaVA’s strength lies in its smart use of existing models. Instead of starting from scratch, the researchers used two key models:CLIP VIT-L/14This is an advanced version of the CLIP (Contrastive Language–Image Pre-training) model developed by OpenAI. CLIP learns visual concepts from natural language descriptions. It can handle any visual classification task by simply being given the names of the visual categories, similar to the “zero-shot” capabilities of GPT-2 and GPT-3.VicunaThis is an open-source chatbot trained by fine-tuning LLaMA on 70,000 user-shared conversations collected from ShareGPT. Training Vicuna-13B costs around $300, and it performs exceptionally well, even when compared to other models like Alpaca.(Large preview)These components make LLaVA highly effective by combining state-of-the-art visual and language understanding capabilities into a single powerful model, perfectly suited for applications requiring both visual and conversational AI.TrainingLLaVA’s training process involves two important stages, which together enhance its ability to understand user instructions, interpret visual and language content, and provide accurate responses. Let’s detail what happens in these two stages:Pre-training for Feature AlignmentLLaVA ensures that its visual and language features are aligned. The goal here is to update the projection matrix, which acts as a bridge between the CLIP visual encoder and the Vicuna language model. This is done using a subset of the CC3M dataset, allowing the model to map input images and text to the same space. This step ensures that the language model can effectively understand the context from both visual and textual inputs.End-to-End Fine-TuningThe entire model undergoes fine-tuning. While the visual encoder’s weights remain fixed, the projection layer and the language model are adjusted.The second stage is tailored to specific application scenarios:Instructions-Based Fine-TuningFor general applications, the model is fine-tuned on a dataset designed for following instructions that involve both visual and textual inputs, making the model versatile for everyday tasks.Scientific reasoningFor more specialized applications, particularly in science, the model is fine-tuned on data that requires complex reasoning, helping the model excel at answering detailed scientific questions.Now that we’re keen on what LLaVA is and the role it plays in our applications, let’s turn our attention to the next component we need for our work, Whisper.Using Whisper For Text-To-SpeechIn this chapter, we’ll check out Whisper, a great model for turning text into speech. Whisper is accurate and easy to use, making it perfect for adding natural-sounding voice responses to our app. We’ve used Whisper in a different article, but here, we’re going to use a new version — large v3. This updated version of the model offers even better performance and speed.Whisper large-v3Whisper was developed by OpenAI, which is the same folks behind ChatGPT. Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. The original Whisper was trained on 680,000 hours of labeled data.Now, what’s different with Whisper large-v3 compared to other models? In my experience, it comes down to the following:Better inputsWhisper large-v3 uses 128 Mel frequency bins instead of 80. Think of Mel frequency bins as a way to break down audio into manageable chunks for the model to process. More bins mean finer detail, which helps the model better understand the audio.More trainingThis specific Whisper version was trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio that was collected from Whisper large-v2. From there, the model was trained for 2.0 epochs over this mix.Whisper models come in different sizes, from tiny to large. Here’s a table comparing the differences and similarities:SizeParametersEnglish-onlyMultilingualtiny39 M✅✅base74 M✅✅small244 M✅✅medium769 M✅✅large1550 M❌✅large-v21550 M❌✅large-v31550 M❌✅Integrating LLaVA With Our AppAlright, so we’re going with LLaVA for image inputs, and this time, we’re adding video inputs, too. This means the app can handle both images and videos, making it more versatile.We’re also keeping the speech feature so you can hear the assistant’s replies, which makes the interaction even more engaging. How cool is that?For this, we’ll use Whisper. We’ll stick with the Gradio framework for the app’s visual layout and user interface. You can, of course, always swap in other models or frameworks — the main goal is to get a working prototype.Installing and Importing the LibrariesWe will start by installing and importing all the required libraries. This includes the transformers libraries for loading the LLaVA and Whisper models, bitsandbytes for quantization, gtts, and moviepy to help in processing video files, including frame extraction.#python !pip install -q -U transformers==4.37.2 !pip install -q bitsandbytes==0.41.3 accelerate==0.25.0 !pip install -q git+https://github.com/openai/whisper.git !pip install -q gradio !pip install -q gTTS !pip install -q moviepy With these installed, we now need to import these libraries into our environment so we can use them. We’ll use colab for that:#python import torch from transformers import BitsAndBytesConfig, pipeline import whisper import gradio as gr from gtts import gTTS from PIL import Image import re import os import datetime import locale import numpy as np import nltk import moviepy.editor as mp nltk.download('punkt') from nltk import sent_tokenize # Set up locale os.environ[\"LANG\"] = \"en_US.UTF-8\" os.environ[\"LC_ALL\"] = \"en_US.UTF-8\" locale.setlocale(locale.LC_ALL, 'en_US.UTF-8') Configuring Quantization and Loading the ModelsNow, let’s set up a 4-bit quantization to make the LLaVA model more efficient in terms of performance and memory usage.#python # Configuration for quantization quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16 ) # Load the image-to-text model model_id = \"llava-hf/llava-1.5-7b-hf\" pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config}) # Load the whisper model DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" model = whisper.load_model(\"large-v3\", device=DEVICE) In this code, we’ve configured the quantization to four bits, which reduces memory usage and improves performance. Then, we load the LLaVA model with these settings. Finally, we load the whisper model, selecting the device based on GPU availability for better performance.Note: We’re using llava-v1.5-7b as the model. Please feel free to explore other versions of the model. For Whisper, we’re loading the “large” size, but you can also switch to another size like “medium” or “small” for your experiments.To get our assistant up and running, we need to implement five essential functions:Handling conversations,Converting images to text,Converting videos to text,Transcribing audio,Converting text to speech.Once these are in place, we will create another function to tie all this together seamlessly. The following sections provide the code that defines each function.Conversation HistoryWe’ll start by setting up the conversation history and a function to log it:#python # Initialize conversation history conversation_history = [] def writehistory(text): \"\"\"Write history to a log file.\"\"\" tstamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") logfile = f'{tstamp}_log.txt' with open(logfile, 'a', encoding='utf-8') as f: f.write(text + '\\n') Image to TextNext, we’ll create a function to convert images to text using LLaVA and iterative prompts.#python def img2txt(input_text, input_image): \"\"\"Convert image to text using iterative prompts.\"\"\" try: image = Image.open(input_image) if isinstance(input_text, tuple): input_text = input_text[0] # Take the first element if it's a tuple writehistory(f\"Input text: {input_text}\") prompt = \"USER: \u003cimage\u003e\\n\" + input_text + \"\\nASSISTANT:\" while True: outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200}) if outputs and outputs[0][\"generated_text\"]: match = re.search(r'ASSISTANT:\\s*(.*)', outputs[0][\"generated_text\"]) reply = match.group(1) if match else \"No response found.\" conversation_history.append((\"User\", input_text)) conversation_history.append((\"Assistant\", reply)) prompt = \"USER: \" + reply + \"\\nASSISTANT:\" return reply # Only return the first response for now else: return \"No response generated.\" except Exception as e: return str(e) Video to TextWe’ll now create a function to convert videos to text by extracting frames and analyzing them.#python def vid2txt(input_text, input_video): \"\"\"Convert video to text by extracting frames and analyzing.\"\"\" try: video = mp.VideoFileClip(input_video) frame = video.get_frame(1) # Get a frame from the video at the 1-second mark image_path = \"temp_frame.jpg\" mp.ImageClip(frame).save_frame(image_path) return img2txt(input_text, image_path) except Exception as e: return str(e) Audio TranscriptionLet’s add a function to transcribe audio to text using Whisper.#python def transcribe(audio_path): \"\"\"Transcribe audio to text using Whisper model.\"\"\" if not audio_path: return '' audio = whisper.load_audio(audio_path) audio = whisper.pad_or_trim(audio) mel = whisper.log_mel_spectrogram(audio).to(model.device) options = whisper.DecodingOptions() result = whisper.decode(model, mel, options) return result.text Text to SpeechLastly, we create a function to convert text responses into speech.#python def text_to_speech(text, file_path): \"\"\"Convert text to speech and save to file.\"\"\" language = 'en' audioobj = gTTS(text=text, lang=language, slow=False) audioobj.save(file_path) return file_path With all the necessary functions in place, we can create the main function that ties everything together:#python def chatbot_interface(audio_path, image_path, video_path, user_message): \"\"\"Process user inputs and generate chatbot response.\"\"\" global conversation_history # Handle audio input if audio_path: speech_to_text_output = transcribe(audio_path) else: speech_to_text_output = \"\" # Determine the input message input_message = user_message if user_message else speech_to_text_output # Ensure input_message is a string if isinstance(input_message, tuple): input_message = input_message[0] # Handle image or video input if image_path: chatgpt_output = img2txt(input_message, image_path) elif video_path: chatgpt_output = vid2txt(input_message, video_path) else: chatgpt_output = \"No image or video provided.\" # Add to conversation history conversation_history.append((\"User\", input_message)) conversation_history.append((\"Assistant\", chatgpt_output)) # Generate audio response processed_audio_path = text_to_speech(chatgpt_output, \"Temp3.mp3\") return conversation_history, processed_audio_path Using Gradio For The InterfaceThe final piece for us is to create the layout and user interface for the app. Again, we’re using Gradio to build that out for quick prototyping purposes.#python # Define Gradio interface iface = gr.Interface( fn=chatbot_interface, inputs=[ gr.Audio(type=\"filepath\", label=\"Record your message\"), gr.Image(type=\"filepath\", label=\"Upload an image\"), gr.Video(label=\"Upload a video\"), gr.Textbox(lines=2, placeholder=\"Type your message here...\", label=\"User message (if no audio)\") ], outputs=[ gr.Chatbot(label=\"Conversation\"), gr.Audio(label=\"Assistant's Voice Reply\") ], title=\"Interactive Visual and Voice Assistant\", description=\"Upload an image or video, record or type your question, and get detailed responses.\" ) # Launch the Gradio app iface.launch(debug=True) Here, we want to let users record or upload their audio prompts, type their questions if they prefer, upload videos, and, of course, have a conversation block.Here’s a preview of how the app will look and work:(Large preview)Looking Beyond LLaVALLaVA is a great model, but there are even greater ones that don’t require a separate ASR model to build a similar app. These are called multimodal or “any-to-any” models. They are designed to process and integrate information from multiple modalities, such as text, images, audio, and video. Instead of just combining vision and text, these models can do it all: image-to-text, video-to-text, text-to-speech, speech-to-text, text-to-video, and image-to-audio, just to name a few. It makes everything simpler and less of a hassle.Examples of Multimodal Models that Handle Images, Text, Audio, and MoreNow that we know what multimodal models are, let’s check out some cool examples. You may want to integrate these into your next personal project.CoDiSo, the first on our list is CoDi or Composable Diffusion. This model is pretty versatile, not sticking to any one type of input or output. It can take in text, images, audio, and video and turn them into different forms of media. Imagine it as a sort of AI that’s not tied down by specific tasks but can handle a mix of data types seamlessly.Source: CoDi. (Large preview)CoDi was developed by researchers from the University of North Carolina and Microsoft Azure. It uses something called Composable Diffusion to sync different types of data, like aligning audio perfectly with the video, and it can generate outputs that weren’t even in the original training data, making it super flexible and innovative.ImageBindNow, let’s talk about ImageBind, a model from Meta. This model is like a multitasking genius, capable of binding together data from six different modalities all at once: images, video, audio, text, depth, and even thermal data.Source: Meta AI. (Large preview)ImageBind doesn’t need explicit supervision to understand how these data types relate. It’s great for creating systems that use multiple types of data to enhance our understanding or create immersive experiences. For example, it could combine 3D sensor data with IMU data to design virtual worlds or enhance memory searches across different media types.GatoGato is another fascinating model. It’s built to be a generalist agent that can handle a wide range of tasks using the same network. Whether it’s playing games, chatting, captioning images, or controlling a robot arm, Gato can do it all.The key thing about Gato is its ability to switch between different types of tasks and outputs using the same model.Source: Google Deepmind. (Large preview)GPT-4oThe next on our list is GPT-4o; GPT-4o is a groundbreaking multimodal large language model (MLLM) developed by OpenAI. It can handle any mix of text, audio, image, and video inputs and give you text, audio, and image outputs. It’s super quick, responding to audio inputs in just 232ms to 320ms, almost like a real conversation.There’s a smaller version of the model called GPT-4o Mini. Small models are becoming a trend, and this one shows that even small models can perform really well. Check out this evaluation to see how the small model stacks up against other large models.Source: OpenAI. (Large preview)ConclusionWe covered a lot in this article, from setting up LLaVA for handling both images and videos to incorporating Whisper large-v3 for top-notch speech recognition. We also explored the versatility of multimodal models like CoDi or GPT-4o, showcasing their potential to handle various data types and tasks. These models can make your app more robust and capable of handling a range of inputs and outputs seamlessly.Which model are you planning to use for your next app? Let me know in the comments! (gg, yk)",
  "image": "https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/integrating-image-to-text-and-text-to-speech-models.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"article__content\"\u003e\u003cul\u003e\u003cli\u003e15 min read\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://smashingmagazine.com/category/ai\"\u003eAI\u003c/a\u003e,\n\u003ca href=\"https://smashingmagazine.com/category/tools\"\u003eTools\u003c/a\u003e,\n\u003ca href=\"https://smashingmagazine.com/category/techniques\"\u003eTechniques\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003csection aria-label=\"Quick summary\"\u003eIn the second part of this series, Joas Pambou aims to build a more advanced version of \u003ca href=\"https://www.smashingmagazine.com/2024/07/integrating-image-to-text-and-text-to-speech-models-part1/\"\u003ethe previous application\u003c/a\u003e that performs conversational analyses on images or videos, much like a chatbot assistant. This means you can ask and learn more about your input content. Joas also explores multimodal or any-to-any models that handle images, videos, text, and audio, offering a comprehensive view of cutting-edge AI applications.\u003c/section\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.smashingmagazine.com/2024/07/integrating-image-to-text-and-text-to-speech-models-part1/\"\u003eIn Part 1\u003c/a\u003e of this brief two-part series, we developed an application that turns images into audio descriptions using vision-language and text-to-speech models. We combined an image-to-text that analyses and understands images, generating description, with a text-to-speech model to create an audio description, helping people with sight challenges. We also discussed how to choose the right model to fit your needs.\u003c/p\u003e\u003cp\u003eNow, we are taking things a step further. Instead of just providing audio descriptions, we are building that can have \u003cstrong\u003einteractive conversations\u003c/strong\u003e about images or videos. This is known as \u003cstrong\u003eConversational AI\u003c/strong\u003e — a technology that lets users talk to systems much like chatbots, virtual assistants, or agents.\u003c/p\u003e\u003cp\u003eWhile the first iteration of the app was great, the output still lacked some details. For example, if you upload an image of a dog, the description might be something like “a dog sitting on a rock in front of a pool,” and the app might produce something close but miss additional details such as the dog’s breed, the time of the day, or location.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/1-app-interface.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"282\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/1-app-interface.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/1-app-interface.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/1-app-interface.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/1-app-interface.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/1-app-interface.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/1-app-interface.png\" sizes=\"100vw\" alt=\"The interface for an app with an uploaded photo of a golden retriever puppy on the left and an audio extraction on the right represented by sound waves.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/1-app-interface.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThe aim here is simply to build \u003cstrong\u003ea more advanced version of the previously built app\u003c/strong\u003e so that it not only describes images but also provides more in-depth information and engages users in meaningful conversations about them.\u003c/p\u003e\u003cp\u003eWe’ll use \u003ca href=\"https://llava-vl.github.io\"\u003eLLaVA\u003c/a\u003e, a model that combines understanding images and conversational capabilities. After building our tool, we’ll explore multimodal models that can handle images, videos, text, audio, and more, all at once to give you even more options and easiness for your applications.\u003c/p\u003e\u003ch2 id=\"visual-instruction-tuning-and-llava\"\u003eVisual Instruction Tuning and LLaVA\u003c/h2\u003e\u003cp\u003eWe are going to look at visual instruction tuning and the multimodal capabilities of LLaVA. We’ll first explore how visual instruction tuning can enhance the large language models to understand and follow instructions that include visual information. After that, we’ll dive into LLaVA, which brings its own set of tools for image and video processing.\u003c/p\u003e\u003ch3 id=\"visual-instruction-tuning\"\u003eVisual Instruction Tuning\u003c/h3\u003e\u003cp\u003eVisual instruction tuning is a technique that helps large language models (LLMs) understand and follow instructions based on visual inputs. This approach connects language and vision, enabling AI systems to understand and respond to human instructions that involve both text and images. For example, Visual IT enables a model to describe an image or answer questions about a scene in a photograph. This fine-tuning method makes the model more capable of handling these complex interactions effectively.\u003c/p\u003e\u003cp\u003eThere’s a new training approach called \u003ca href=\"https://llavar.github.io/\"\u003e\u003cstrong\u003eLLaVAR\u003c/strong\u003e\u003c/a\u003e that has been developed, and you can think of it as a tool for handling tasks related to PDFs, invoices, and text-heavy images. It’s pretty exciting, but we won’t dive into that since it is outside the scope of the app we’re making.\u003c/p\u003e\u003ch3 id=\"examples-of-visual-instruction-tuning-datasets\"\u003eExamples of Visual Instruction Tuning Datasets\u003c/h3\u003e\u003cp\u003eTo build good models, you need good data — rubbish in, rubbish out. So, here are two datasets that you might want to use to train or evaluate your multimodal models. Of course, you can always add your own datasets to the two I’m going to mention.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align?row=0\"\u003e\u003cstrong\u003eVision-CAIR\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInstruction datasets\u003c/strong\u003e: English;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMulti-task\u003c/strong\u003e: Datasets containing multiple tasks;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMixed dataset\u003c/strong\u003e: Contains both human and machine-generated data.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/2-vision-cair.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"366\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/2-vision-cair.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/2-vision-cair.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/2-vision-cair.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/2-vision-cair.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/2-vision-cair.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/2-vision-cair.png\" sizes=\"100vw\" alt=\"Vision-CAIR\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/2-vision-cair.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eVision-CAIR provides a high-quality, well-aligned image-text dataset created using conversations between two bots. This dataset was initially introduced in a paper titled “\u003ca href=\"https://arxiv.org/abs/2304.10592\"\u003eMiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models\u003c/a\u003e,” and it provides more detailed image descriptions and can be used with predefined instruction templates for image-instruction-answer fine-tuning.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp\u003e\u003ca href=\"https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K\"\u003e\u003cstrong\u003eLLaVA Visual Instruct 150K\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInstruction datasets\u003c/strong\u003e: English;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMulti-task\u003c/strong\u003e: Datasets containing multiple tasks;\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMixed dataset\u003c/strong\u003e: Contains both human and machine-generated data.\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLLaVA Visual Instruct 150K is a set of GPT-generated multimodal instruction-following data. It is built for visual instruction tuning and aims to achieve GPT-4 level vision and language capabilities.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/3-llava-details-page.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"360\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/3-llava-details-page.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/3-llava-details-page.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/3-llava-details-page.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/3-llava-details-page.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/3-llava-details-page.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/3-llava-details-page.png\" sizes=\"100vw\" alt=\"LLaVA details page on the HuggingFace website.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/3-llava-details-page.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThere are more multimodal datasets out there, but these two should help you get started if you want to fine-tune your model.\u003c/p\u003e\u003ch2 id=\"let-s-take-a-closer-look-at-llava\"\u003eLet’s Take a Closer Look At LLaVA\u003c/h2\u003e\u003cp\u003eLLaVA (which stands for Large Language and Vision Assistant) is a groundbreaking multimodal model developed by researchers from the University of Wisconsin, Microsoft Research, and Columbia University. The researchers aimed to create a powerful, open-source model that could compete with the best in the field, just like GPT-4, Claude 3, or Gemini, to name a few. For developers like you and me, its open nature is a huge benefit, allowing for easy fine-tuning and integration.\u003c/p\u003e\u003cp\u003eOne of LLaVA’s standout features is its \u003cstrong\u003eability to understand and respond to complex visual information\u003c/strong\u003e, even with unfamiliar images and instructions. This is exactly what we need for our tool, as it goes beyond simple image descriptions to engage in meaningful conversations about the content.\u003c/p\u003e\u003ch3 id=\"architecture\"\u003eArchitecture\u003c/h3\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/4-dataset-interacts-large-language-model.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"270\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/4-dataset-interacts-large-language-model.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/4-dataset-interacts-large-language-model.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/4-dataset-interacts-large-language-model.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/4-dataset-interacts-large-language-model.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/4-dataset-interacts-large-language-model.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/4-dataset-interacts-large-language-model.png\" sizes=\"100vw\" alt=\"Illustration showing how a dataset interacts with a large language model.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/4-dataset-interacts-large-language-model.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eLLaVA’s strength lies in its \u003cstrong\u003esmart use of existing models\u003c/strong\u003e. Instead of starting from scratch, the researchers used two key models:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K\"\u003e\u003cstrong\u003eCLIP VIT-L/14\u003c/strong\u003e\u003c/a\u003e\u003cbr/\u003eThis is an advanced version of the CLIP (Contrastive Language–Image Pre-training) model developed by OpenAI. CLIP learns visual concepts from natural language descriptions. It can handle any visual classification task by simply being given the names of the visual categories, similar to the “zero-shot” capabilities of GPT-2 and GPT-3.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://huggingface.co/lmsys/vicuna-13b-v1.3\"\u003e\u003cstrong\u003eVicuna\u003c/strong\u003e\u003c/a\u003e\u003cbr/\u003eThis is an open-source chatbot trained by fine-tuning \u003ca href=\"https://arxiv.org/abs/2302.13971\"\u003eLLaMA\u003c/a\u003e on 70,000 user-shared conversations collected from \u003ca href=\"https://sharegpt.com\"\u003eShareGPT\u003c/a\u003e. Training Vicuna-13B costs around $300, and it performs exceptionally well, even when compared to other models like \u003ca href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\"\u003eAlpaca\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/5-answers-alpaca-vicuna.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"519\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/5-answers-alpaca-vicuna.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/5-answers-alpaca-vicuna.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/5-answers-alpaca-vicuna.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/5-answers-alpaca-vicuna.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/5-answers-alpaca-vicuna.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/5-answers-alpaca-vicuna.png\" sizes=\"100vw\" alt=\"Comparing the generated answers created by Alpaca and Vicuna.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/5-answers-alpaca-vicuna.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eThese components make LLaVA highly effective by combining state-of-the-art visual and language understanding capabilities into a single powerful model, perfectly suited for applications requiring both visual and conversational AI.\u003c/p\u003e\u003ch3 id=\"training\"\u003eTraining\u003c/h3\u003e\u003cp\u003eLLaVA’s training process involves two important stages, which together enhance its ability to understand user instructions, interpret visual and language content, and provide accurate responses. Let’s detail what happens in these two stages:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003ePre-training for Feature Alignment\u003c/strong\u003e\u003cbr/\u003eLLaVA ensures that its visual and language features are aligned. The goal here is to update the projection matrix, which acts as a bridge between the CLIP visual encoder and the Vicuna language model. This is done using a subset of the CC3M dataset, allowing the model to map input images and text to the same space. This step ensures that the language model can effectively understand the context from both visual and textual inputs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnd-to-End Fine-Tuning\u003c/strong\u003e\u003cbr/\u003eThe entire model undergoes fine-tuning. While the visual encoder’s weights remain fixed, the projection layer and the language model are adjusted.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThe second stage is tailored to specific application scenarios:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInstructions-Based Fine-Tuning\u003c/strong\u003e\u003cbr/\u003eFor general applications, the model is fine-tuned on a dataset designed for following instructions that involve both visual and textual inputs, making the model versatile for everyday tasks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScientific reasoning\u003c/strong\u003e\u003cbr/\u003eFor more specialized applications, particularly in science, the model is fine-tuned on data that requires complex reasoning, helping the model excel at answering detailed scientific questions.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow that we’re keen on what LLaVA is and the role it plays in our applications, let’s turn our attention to the next component we need for our work, Whisper.\u003c/p\u003e\u003ch2 id=\"using-whisper-for-text-to-speech\"\u003eUsing Whisper For Text-To-Speech\u003c/h2\u003e\u003cp\u003eIn this chapter, we’ll check out Whisper, a great model for turning text into speech. Whisper is \u003cstrong\u003eaccurate\u003c/strong\u003e and \u003cstrong\u003eeasy to use\u003c/strong\u003e, making it perfect for adding natural-sounding voice responses to our app. We’ve used Whisper in a different article, but here, we’re going to use a new version — \u003cstrong\u003elarge v3\u003c/strong\u003e. This updated version of the model offers even better performance and speed.\u003c/p\u003e\u003ch3 id=\"whisper-large-v3\"\u003eWhisper large-v3\u003c/h3\u003e\u003cp\u003eWhisper was developed by OpenAI, which is the same folks behind ChatGPT. Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. The original Whisper was trained on 680,000 hours of labeled data.\u003c/p\u003e\u003cp\u003eNow, what’s different with Whisper large-v3 compared to other models? In my experience, it comes down to the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eBetter inputs\u003c/strong\u003e\u003cbr/\u003eWhisper large-v3 uses 128 Mel frequency bins instead of 80. Think of Mel frequency bins as a way to break down audio into manageable chunks for the model to process. More bins mean finer detail, which helps the model better understand the audio.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMore training\u003c/strong\u003e\u003cbr/\u003eThis specific Whisper version was trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio that was collected from Whisper large-v2. From there, the model was trained for 2.0 epochs over this mix.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhisper models come in different sizes, from tiny to large. Here’s a table comparing the differences and similarities:\u003c/p\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eSize\u003c/th\u003e\u003cth\u003eParameters\u003c/th\u003e\u003cth\u003eEnglish-only\u003c/th\u003e\u003cth\u003eMultilingual\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003etiny\u003c/td\u003e\u003ctd\u003e39 M\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003ebase\u003c/td\u003e\u003ctd\u003e74 M\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003esmall\u003c/td\u003e\u003ctd\u003e244 M\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003emedium\u003c/td\u003e\u003ctd\u003e769 M\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elarge\u003c/td\u003e\u003ctd\u003e1550 M\u003c/td\u003e\u003ctd\u003e❌\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elarge-v2\u003c/td\u003e\u003ctd\u003e1550 M\u003c/td\u003e\u003ctd\u003e❌\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elarge-v3\u003c/td\u003e\u003ctd\u003e1550 M\u003c/td\u003e\u003ctd\u003e❌\u003c/td\u003e\u003ctd\u003e✅\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003ch2 id=\"integrating-llava-with-our-app\"\u003eIntegrating LLaVA With Our App\u003c/h2\u003e\u003cp\u003eAlright, so we’re going with LLaVA for image inputs, and this time, we’re adding video inputs, too. This means the app can handle both images and videos, making it more versatile.\u003c/p\u003e\u003cp\u003eWe’re also keeping the speech feature so you can hear the assistant’s replies, which makes the interaction even more engaging. How cool is that?\u003c/p\u003e\u003cp\u003eFor this, we’ll use Whisper. We’ll stick with the Gradio framework for the app’s visual layout and user interface. You can, of course, always swap in other models or frameworks — the main goal is to get a working prototype.\u003c/p\u003e\u003ch3 id=\"installing-and-importing-the-libraries\"\u003eInstalling and Importing the Libraries\u003c/h3\u003e\u003cp\u003eWe will start by installing and importing all the required libraries. This includes the transformers libraries for loading the LLaVA and Whisper models, \u003ccode\u003ebitsandbytes\u003c/code\u003e for quantization, \u003ccode\u003egtts\u003c/code\u003e, and \u003ccode\u003emoviepy\u003c/code\u003e to help in processing video files, including frame extraction.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e#python\n!pip install -q -U transformers==4.37.2\n!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0\n!pip install -q git+https://github.com/openai/whisper.git\n!pip install -q gradio\n!pip install -q gTTS\n!pip install -q moviepy\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eWith these installed, we now need to import these libraries into our environment so we can use them. We’ll use \u003ca href=\"https://colab.research.google.com/#\"\u003e\u003ccode\u003ecolab\u003c/code\u003e\u003c/a\u003e for that:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e#python\nimport torch\nfrom transformers import BitsAndBytesConfig, pipeline\nimport whisper\nimport gradio as gr\nfrom gtts import gTTS\nfrom PIL import Image\nimport re\nimport os\nimport datetime\nimport locale\nimport numpy as np\nimport nltk\nimport moviepy.editor as mp\n\nnltk.download(\u0026#39;punkt\u0026#39;)\nfrom nltk import sent_tokenize\n\n# Set up locale\nos.environ[\u0026#34;LANG\u0026#34;] = \u0026#34;en_US.UTF-8\u0026#34;\nos.environ[\u0026#34;LC_ALL\u0026#34;] = \u0026#34;en_US.UTF-8\u0026#34;\nlocale.setlocale(locale.LC_ALL, \u0026#39;en_US.UTF-8\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"configuring-quantization-and-loading-the-models\"\u003eConfiguring Quantization and Loading the Models\u003c/h3\u003e\u003cp\u003eNow, let’s set up a 4-bit quantization to make the LLaVA model more efficient in terms of performance and memory usage.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e#python\n\n# Configuration for quantization\nquantization_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_compute_dtype=torch.float16\n)\n\n# Load the image-to-text model\nmodel_id = \u0026#34;llava-hf/llava-1.5-7b-hf\u0026#34;\npipe = pipeline(\u0026#34;image-to-text\u0026#34;,\n  model=model_id,\n  model_kwargs={\u0026#34;quantization_config\u0026#34;: quantization_config})\n\n# Load the whisper model\nDEVICE = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;\nmodel = whisper.load_model(\u0026#34;large-v3\u0026#34;, device=DEVICE)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn this code, we’ve configured the quantization to four bits, which reduces memory usage and improves performance. Then, we load the LLaVA model with these settings. Finally, we load the whisper model, selecting the device based on GPU availability for better performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003cem\u003eWe’re using \u003ca href=\"https://huggingface.co/llava-hf/llava-1.5-7b-hf\"\u003ellava-v1.5-7b\u003c/a\u003e as the model. Please feel free to explore \u003ca href=\"https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e\"\u003eother versions of the model\u003c/a\u003e. For Whisper, we’re loading the “large” size, but you can also switch to another size like “medium” or “small” for your experiments.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eTo get our assistant up and running, we need to implement five essential functions:\u003c/p\u003e\u003col\u003e\u003cli\u003eHandling conversations,\u003c/li\u003e\u003cli\u003eConverting images to text,\u003c/li\u003e\u003cli\u003eConverting videos to text,\u003c/li\u003e\u003cli\u003eTranscribing audio,\u003c/li\u003e\u003cli\u003eConverting text to speech.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eOnce these are in place, we will create another function to tie all this together seamlessly. The following sections provide the code that defines each function.\u003c/p\u003e\u003ch3 id=\"conversation-history\"\u003eConversation History\u003c/h3\u003e\u003cp\u003eWe’ll start by setting up the conversation history and a function to log it:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e#python\n\n# Initialize conversation history\nconversation_history = []\n\ndef writehistory(text):\n  \u0026#34;\u0026#34;\u0026#34;Write history to a log file.\u0026#34;\u0026#34;\u0026#34;\n  tstamp = datetime.datetime.now().strftime(\u0026#34;%Y%m%d_%H%M%S\u0026#34;)\n  logfile = f\u0026#39;{tstamp}_log.txt\u0026#39;\n  with open(logfile, \u0026#39;a\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f:\n    f.write(text + \u0026#39;\\n\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"image-to-text\"\u003eImage to Text\u003c/h3\u003e\u003cp\u003eNext, we’ll create a function to convert images to text using LLaVA and iterative prompts.\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e#python\ndef img2txt(input_text, input_image):\n  \u0026#34;\u0026#34;\u0026#34;Convert image to text using iterative prompts.\u0026#34;\u0026#34;\u0026#34;\n  try:\n    image = Image.open(input_image)\n      \n    if isinstance(input_text, tuple):\n      input_text = input_text[0]  # Take the first element if it\u0026#39;s a tuple\n\n      writehistory(f\u0026#34;Input text: {input_text}\u0026#34;)\n      prompt = \u0026#34;USER: \u0026lt;image\u0026gt;\\n\u0026#34; + input_text + \u0026#34;\\nASSISTANT:\u0026#34;\n      while True:\n        outputs = pipe(image, prompt=prompt, generate_kwargs={\u0026#34;max_new_tokens\u0026#34;: 200})\n\n          if outputs and outputs[0][\u0026#34;generated_text\u0026#34;]:\n            match = re.search(r\u0026#39;ASSISTANT:\\s*(.*)\u0026#39;, outputs[0][\u0026#34;generated_text\u0026#34;])\n            reply = match.group(1) if match else \u0026#34;No response found.\u0026#34;\n            conversation_history.append((\u0026#34;User\u0026#34;, input_text))\n            conversation_history.append((\u0026#34;Assistant\u0026#34;, reply))\n            prompt = \u0026#34;USER: \u0026#34; + reply + \u0026#34;\\nASSISTANT:\u0026#34;\n            return reply  # Only return the first response for now\n          else:\n            return \u0026#34;No response generated.\u0026#34;\n  except Exception as e:\n    return str(e)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"video-to-text\"\u003eVideo to Text\u003c/h3\u003e\u003cp\u003eWe’ll now create a function to convert videos to text by extracting frames and analyzing them.\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e#python\ndef vid2txt(input_text, input_video):\n  \u0026#34;\u0026#34;\u0026#34;Convert video to text by extracting frames and analyzing.\u0026#34;\u0026#34;\u0026#34;\n  try:\n    video = mp.VideoFileClip(input_video)\n    frame = video.get_frame(1)  # Get a frame from the video at the 1-second mark\n    image_path = \u0026#34;temp_frame.jpg\u0026#34;\n    mp.ImageClip(frame).save_frame(image_path)\n    return img2txt(input_text, image_path)\n  except Exception as e:\n    return str(e)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"audio-transcription\"\u003eAudio Transcription\u003c/h3\u003e\u003cp\u003eLet’s add a function to transcribe audio to text using Whisper.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e#python\ndef transcribe(audio_path):\n  \u0026#34;\u0026#34;\u0026#34;Transcribe audio to text using Whisper model.\u0026#34;\u0026#34;\u0026#34;\n  if not audio_path:\n    return \u0026#39;\u0026#39;\n\n  audio = whisper.load_audio(audio_path)\n  audio = whisper.pad_or_trim(audio)\n  mel = whisper.log_mel_spectrogram(audio).to(model.device)\n  options = whisper.DecodingOptions()\n  result = whisper.decode(model, mel, options)\n  return result.text\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"text-to-speech\"\u003eText to Speech\u003c/h3\u003e\u003cp\u003eLastly, we create a function to convert text responses into speech.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e#python\ndef text_to_speech(text, file_path):\n  \u0026#34;\u0026#34;\u0026#34;Convert text to speech and save to file.\u0026#34;\u0026#34;\u0026#34;\n  language = \u0026#39;en\u0026#39;\n  audioobj = gTTS(text=text, lang=language, slow=False)\n  audioobj.save(file_path)\n  return file_path\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eWith all the necessary functions in place, we can create the main function that ties everything together:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e#python\n\ndef chatbot_interface(audio_path, image_path, video_path, user_message):\n  \u0026#34;\u0026#34;\u0026#34;Process user inputs and generate chatbot response.\u0026#34;\u0026#34;\u0026#34;\n  global conversation_history\n\n  # Handle audio input\n  if audio_path:\n    speech_to_text_output = transcribe(audio_path)\n  else:\n    speech_to_text_output = \u0026#34;\u0026#34;\n\n  # Determine the input message\n  input_message = user_message if user_message else speech_to_text_output\n\n  # Ensure input_message is a string\n  if isinstance(input_message, tuple):\n    input_message = input_message[0]\n\n  # Handle image or video input\n  if image_path:\n    chatgpt_output = img2txt(input_message, image_path)\n  elif video_path:\n      chatgpt_output = vid2txt(input_message, video_path)\n  else:\n    chatgpt_output = \u0026#34;No image or video provided.\u0026#34;\n\n  # Add to conversation history\n  conversation_history.append((\u0026#34;User\u0026#34;, input_message))\n  conversation_history.append((\u0026#34;Assistant\u0026#34;, chatgpt_output))\n\n  # Generate audio response\n  processed_audio_path = text_to_speech(chatgpt_output, \u0026#34;Temp3.mp3\u0026#34;)\n\n  return conversation_history, processed_audio_path\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"using-gradio-for-the-interface\"\u003eUsing Gradio For The Interface\u003c/h3\u003e\u003cp\u003eThe final piece for us is to create the layout and user interface for the app. Again, we’re using \u003ca href=\"https://www.gradio.app\"\u003eGradio\u003c/a\u003e to build that out for quick prototyping purposes.\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e#python\n\n# Define Gradio interface\niface = gr.Interface(\n  fn=chatbot_interface,\n  inputs=[\n    gr.Audio(type=\u0026#34;filepath\u0026#34;, label=\u0026#34;Record your message\u0026#34;),\n    gr.Image(type=\u0026#34;filepath\u0026#34;, label=\u0026#34;Upload an image\u0026#34;),\n    gr.Video(label=\u0026#34;Upload a video\u0026#34;),\n    gr.Textbox(lines=2, placeholder=\u0026#34;Type your message here...\u0026#34;, label=\u0026#34;User message (if no audio)\u0026#34;)\n  ],\n  outputs=[\n    gr.Chatbot(label=\u0026#34;Conversation\u0026#34;),\n    gr.Audio(label=\u0026#34;Assistant\u0026#39;s Voice Reply\u0026#34;)\n  ],\n  title=\u0026#34;Interactive Visual and Voice Assistant\u0026#34;,\n  description=\u0026#34;Upload an image or video, record or type your question, and get detailed responses.\u0026#34;\n)\n\n# Launch the Gradio app\niface.launch(debug=True)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere, we want to let users record or upload their audio prompts, type their questions if they prefer, upload videos, and, of course, have a conversation block.\u003c/p\u003e\u003cp\u003eHere’s a preview of how the app will look and work:\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/6-app-preview.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"400\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/6-app-preview.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/6-app-preview.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/6-app-preview.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/6-app-preview.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/6-app-preview.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/6-app-preview.png\" sizes=\"100vw\" alt=\"Preview of the app\"/\u003e\u003c/a\u003e\u003cfigcaption\u003e(\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/6-app-preview.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"looking-beyond-llava\"\u003eLooking Beyond LLaVA\u003c/h2\u003e\u003cp\u003eLLaVA is a great model, but there are even greater ones that don’t require a separate ASR model to build a similar app. These are called multimodal or “any-to-any” models. They are designed to process and integrate information from multiple modalities, such as text, images, audio, and video. Instead of just combining vision and text, these models can do it all: image-to-text, video-to-text, text-to-speech, speech-to-text, text-to-video, and image-to-audio, just to name a few. It makes everything simpler and less of a hassle.\u003c/p\u003e\u003ch3 id=\"examples-of-multimodal-models-that-handle-images-text-audio-and-more\"\u003eExamples of Multimodal Models that Handle Images, Text, Audio, and More\u003c/h3\u003e\u003cp\u003eNow that we know what multimodal models are, let’s check out some cool examples. You may want to integrate these into your next personal project.\u003c/p\u003e\u003ch4 id=\"codi\"\u003eCoDi\u003c/h4\u003e\u003cp\u003eSo, the first on our list is \u003ca href=\"https://codi-gen.github.io\"\u003e\u003cstrong\u003eCoDi\u003c/strong\u003e\u003c/a\u003e or \u003cstrong\u003eComposable Diffusion\u003c/strong\u003e. This model is pretty versatile, not sticking to any one type of input or output. It can take in text, images, audio, and video and turn them into different forms of media. Imagine it as a sort of AI that’s not tied down by specific tasks but can handle a mix of data types seamlessly.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/7-codi.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"450\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/7-codi.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/7-codi.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/7-codi.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/7-codi.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/7-codi.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/7-codi.png\" sizes=\"100vw\" alt=\"CoDi\"/\u003e\u003c/a\u003e\u003cfigcaption\u003eSource: \u003ca href=\"https://codi-gen.github.io/\"\u003eCoDi\u003c/a\u003e. (\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/7-codi.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eCoDi was developed by researchers from the University of North Carolina and Microsoft Azure. It uses something called \u003ca href=\"https://arxiv.org/abs/2206.01714\"\u003eComposable Diffusion\u003c/a\u003e to sync different types of data, like aligning audio perfectly with the video, and it \u003cstrong\u003ecan generate outputs that weren’t even in the original training data\u003c/strong\u003e, making it super flexible and innovative.\u003c/p\u003e\u003ch4 id=\"imagebind\"\u003eImageBind\u003c/h4\u003e\u003cp\u003eNow, let’s talk about \u003ca href=\"https://imagebind.metademolab.com\"\u003e\u003cstrong\u003eImageBind\u003c/strong\u003e\u003c/a\u003e, a model from Meta. This model is like a multitasking genius, capable of binding together data from six different modalities all at once: images, video, audio, text, depth, and even thermal data.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/9-imagebind.gif\"\u003e\u003cimg src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/8-imagebind-800w.gif\" width=\"800\" height=\"450\" alt=\"ImageBind model\"/\u003e\u003c/a\u003e\u003cfigcaption\u003eSource: \u003ca href=\"https://imagebind.metademolab.com/\"\u003eMeta AI\u003c/a\u003e. (\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/9-imagebind.gif\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eImageBind doesn’t need explicit supervision to understand how these data types relate. It’s great for creating systems that use \u003cstrong\u003emultiple types of data\u003c/strong\u003e to enhance our understanding or create immersive experiences. For example, it could combine 3D sensor data with \u003ca href=\"https://intorobotics.com/what-is-an-imu/\"\u003eIMU data\u003c/a\u003e to design virtual worlds or enhance memory searches across different media types.\u003c/p\u003e\u003ch4 id=\"gato\"\u003eGato\u003c/h4\u003e\u003cp\u003e\u003ca href=\"https://deepmind.google/discover/blog/a-generalist-agent/\"\u003e\u003cstrong\u003eGato\u003c/strong\u003e\u003c/a\u003e is another fascinating model. It’s built to be a generalist agent that can handle a wide range of tasks using the same network. Whether it’s playing games, chatting, captioning images, or controlling a robot arm, Gato can do it all.\u003c/p\u003e\u003cp\u003eThe key thing about Gato is its ability to \u003cstrong\u003eswitch between different types of tasks and outputs using the same model\u003c/strong\u003e.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/10-gato.png\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"482\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/10-gato.png 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/10-gato.png 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/10-gato.png 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/10-gato.png 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/10-gato.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/10-gato.png\" sizes=\"100vw\" alt=\"Gato model\"/\u003e\u003c/a\u003e\u003cfigcaption\u003eSource: \u003ca href=\"https://deepmind.google/discover/blog/a-generalist-agent/\"\u003eGoogle Deepmind\u003c/a\u003e. (\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/10-gato.png\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003ch4 id=\"gpt-4o\"\u003eGPT-4o\u003c/h4\u003e\u003cp\u003eThe next on our list is \u003ca href=\"https://openai.com/index/hello-gpt-4o/\"\u003e\u003cstrong\u003eGPT-4o\u003c/strong\u003e\u003c/a\u003e; GPT-4o is a groundbreaking multimodal large language model (MLLM) developed by OpenAI. It can handle any mix of text, audio, image, and video inputs and give you text, audio, and image outputs. It’s super quick, responding to audio inputs in just 232ms to 320ms, almost like a real conversation.\u003c/p\u003e\u003cp\u003eThere’s a smaller version of the model called \u003ca href=\"https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\"\u003eGPT-4o Mini\u003c/a\u003e. Small models are becoming a trend, and this one shows that even small models can perform really well. Check out this evaluation to see how the small model stacks up against other large models.\u003c/p\u003e\u003cfigure\u003e\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/11-model-evaluation-scores-open-ai.jpeg\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" fetchpriority=\"low\" width=\"800\" height=\"476\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/11-model-evaluation-scores-open-ai.jpeg 400w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/11-model-evaluation-scores-open-ai.jpeg 800w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/11-model-evaluation-scores-open-ai.jpeg 1200w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/11-model-evaluation-scores-open-ai.jpeg 1600w,\nhttps://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/11-model-evaluation-scores-open-ai.jpeg 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/11-model-evaluation-scores-open-ai.jpeg\" sizes=\"100vw\" alt=\"Vertical bar chart comparing evaluation scores for GPT-4o mini, Gemini Flash, Claude Haiku, GPT 2.5 Turbo, and GPT-4o.\"/\u003e\u003c/a\u003e\u003cfigcaption\u003eSource: \u003ca href=\"https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\"\u003eOpenAI\u003c/a\u003e. (\u003ca href=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/11-model-evaluation-scores-open-ai.jpeg\"\u003eLarge preview\u003c/a\u003e)\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eWe covered a lot in this article, from setting up LLaVA for handling both images and videos to incorporating Whisper large-v3 for top-notch speech recognition. We also explored the versatility of multimodal models like CoDi or GPT-4o, showcasing their potential to handle various data types and tasks. These models can make your app more robust and capable of handling a range of inputs and outputs seamlessly.\u003c/p\u003e\u003cp\u003eWhich model are you planning to use for your next app? Let me know in the comments!\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://www.smashingmagazine.com/images/logo/logo--red.png\" alt=\"Smashing Editorial\" width=\"35\" height=\"46\" loading=\"lazy\" decoding=\"async\"/\u003e\n\u003cspan\u003e(gg, yk)\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "23 min read",
  "publishedTime": "2024-08-30T09:00:00Z",
  "modifiedTime": "2024-08-30T09:00:00Z"
}
