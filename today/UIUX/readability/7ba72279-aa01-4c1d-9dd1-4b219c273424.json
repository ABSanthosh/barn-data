{
  "id": "7ba72279-aa01-4c1d-9dd1-4b219c273424",
  "title": "Common-Sense AI Integration: Lessons from the Cofounder of Condens",
  "link": "https://www.nngroup.com/articles/ai-integration-condens/",
  "description": "Experience in the context of a UX-research platform shows that AI can be integrated well in focused tasks but isn’t capable of independent complex analysis.",
  "author": "Alexander Knoll",
  "published": "Fri, 21 Feb 2025 18:00:00 +0000",
  "source": "https://www.nngroup.com/feed/rss/",
  "categories": [
    "Article"
  ],
  "byline": "Alexander Knoll",
  "length": 11731,
  "excerpt": "Experience in the context of a UX-research platform shows that AI can be integrated well in focused tasks but isn’t capable of independent complex analysis.",
  "siteName": "Nielsen Norman Group",
  "favicon": "",
  "text": "Summary:  Experience in the context of a UX-research platform shows that AI can be integrated well in focused tasks but isn’t capable of independent complex analysis. Now that generative AI (genAI) has been around for a couple of years, the extreme hype is starting to simmer down. As the dust settles, we have a clearer picture of AI's potential and limitations. In this article, I share some of the lessons learned through our genAI journey at Condens, our research-analysis platform. These lessons will be useful to anyone trying to determine how to integrate AI into their product strategy or research workflow. Resisting AI Pressure Our AI-Product-Integration Strategy 3 Key Questions for AI Integration Where GenAI Succeeds and Fails Resisting AI Pressure When ChatGPT was released in 2022, I felt like a little kid with a new Game Boy, exploring prompts and imagining all the new possibilities. And like many companies, Condens began testing genAI's potential for speeding up and improving UX research. Unfortunately, we also started seeing very ambitious and exaggerated claims being made about AI’s capabilities and how it could be used — claims which conflicted with what we observed in our evaluations and testing. Examples: “High-quality insights in seconds” “Automatically analyze your projects” “Eliminate bias from your research” “User research without the users.” We found that, while generative AI could meaningfully improve research productivity, many of the claims, especially in marketing messaging, were overstated. Although many AI features looked impressive in demos, they often didn’t perform well in real research situations. This hype environment has placed considerable pressure on researchers and research vendors alike. At Condens, we had to define our evidence-backed strategy towards common-sense AI integration and stick to it. It wasn’t easy. We lost a customer for our decision to take a more measured approach. The team decided to move to a competitor that was more aggressively integrating new AI features. Sadly, that move was not the choice of the research team — it was motivated by pressure from stakeholders who had seen unrealistic marketing about efficiency gains with AI. Four months later, the disappointed research team reported having to \"drastically adjust their expectations\" when the AI features underperformed and “reality hit” them. We concluded that significant work is still needed to develop truly useful AI applications, and we have a responsibility to both drive innovation and maintain the hard-earned trust of our researchers. Our AI-Product-Integration Strategy I’d describe our approach to genAI in UX research (UXR) as cautiously optimistic. We’re eager to explore where AI can genuinely enhance UXR workflows, but we’re also mindful of its limitations. We won’t build AI features that don’t have value or reliability. To resist the hype and stay on the right track, we established some of our own AI-design guidelines: Give AI tasks that are scoped and specific. Make it easy to verify AI output and always connect it back to evidence. Make it easy to make changes to AI output. Ensure core tasks (in our case, data analysis) can also be done without AI. 3 Key Questions for AI Integration Here are 3 questions you should ask to decide whether a particular AI tool can reasonably support your task. (When you ask these questions, consider the specific API or LLM version that you have access to. Though the latest and greatest systems may be powerful, these may not be available or cost-effective.) While we developed these questions in our research context at Condens, they're valuable for any AI implementation. Does the AI Have the Necessary Context and Data? AI often requires deeper context about industries, companies, or research studies than can be provided in a prompt. Without sufficient context, results are rarely high-quality. That’s particularly true for UX work, which is extremely contextual. Automatic transcription and translation are excellent examples where AI excels because all necessary data is present in the input, requiring minimal additional context or interpretation. Does the Task Work Within the AI’s Technical Input Constraints? Research shows that LLMs have technical limitations regarding the size of the input data. For example, GPT-4o currently takes a maximum of 128k tokens, which corresponds to roughly 10 hours of interviews transcribed. When provided with too much input, these models reduce the data they consider down to what they think is relevant, likely introducing bias into the results. Additionally, studies done at the Chinese Academy of Sciences and at University of Waterloo, respectively, suggest that result quality may deteriorate as input size increases. (AI researchers are actively working to develop solutions to this problem. At the time of writing, those solutions have not been conclusively proven.) Can the Output Be Verified and Modified? AI-generated content inherently requires fact checking, which is exponentially more challenging with larger datasets. In some cases, it may be faster and better for a human to do the task instead of verifying the AI-produced work. Research findings and recommendations must be backed by evidence, so AI’s output in UXR is useless if you can’t verify the source. The results should also be easy for humans to edit when needed, because it’s unlikely that they’ll take the AI output exactly as is. Especially if the output contains an error, people will need to be able to adjust it, even if just a bit. Where GenAI Succeeds and Fails These questions have been extremely useful for our team as we evaluate different research-related tasks that are candidates for genAI. Based on its current capabilities, here are our conclusions. Summarizing Small Chunks of Content Example research task: Asking AI to summarize a long quote from a participant to make it easy and quick to understand for yourself or stakeholders Key Question Evaluation Note Necessary Context and Data? ✅ Yes The only input required is the quote. Works with Input Constraints? ✅ Yes Even if it’s a long quote, you won’t exceed the technical limits. Can Output Be Verified and Modified? ✅ Yes The summary is usually 1–2 sentences so it can be quickly reviewed and verified. Even if the summary occasionally fails to capture the core of the message, it can still be useful. Summarizing Large Amounts of Content Example research task: Asking AI to summarize an entire research project Key Question Evaluation Note Necessary Context and Data? ⚠️Partially AI's literal summarization approach becomes problematic with complex data that requires meaningful selection and interpretation. Works with Input Constraints? ⚠️Partially Project data often exceeds input limits, forcing AI to make potentially biased preselections. Can Output Be Verified and Modified? ⚠️Partially While reading a summary is quick, verifying accuracy requires deep familiarity with the source material. Simple Semantic Search Example research task: Finding explicit user complaints or pain points mentioned in current project interviews Key Question Evaluation Note Necessary Context and Data? ✅ Yes Identifying explicit mentions requires minimal context. Works with Input Constraints? ✅ Yes Most projects stay within the 10-hour transcript limit. Can Output Be Verified and Modified? ⚠️Partially Users can quickly verify if the returned quotes mention pain points, but it is harder to check whether the output includes all mentions of pain points. Complex Semantic Search Example research task: Identifying and ranking the most important pain points across all historical research data Key Question Evaluation Note Necessary Context and Data? ❌ No Ranking pain points requires significant interpretation and industry/company context that AI lacks. Works with Input Constraints? ❌ No Historical data exceeds AI limits, forcing potentially biased data selection. Can Output Be Verified and Modified? ❌ No Verification would require manual review of all source data. This task requires (1) identifying the pain points and (2) ranking them by importance. It involves interpretation and judgment and should not be left to AI. The AI output will likely miss subtle behavioral cues (e.g., facial expressions) that point to challenges encountered by users. An experienced researcher with context about the company and industry will definitely be better here. A version of this task that would likely be more realistic and feasible with AI would involve ranking pain points across all research insights generated from previous studies. That task would require substantially less data to be input into the AI and would make the output easier to verify. Quote Clustering Example research task: Grouping preselected quotes and observations into initial themes Key Question Evaluation Note Necessary Context and Data? ✅ Yes The AI can work with quote text, although it might miss subtle connections. Works with Input Constraints? ✅ Yes A single-project quote set should be fairly small. Can Output Be Verified and Modified? ⚠️Partially Effectiveness depends on how results are presented and how closely evidence links to themes AI can provide initial quote clusters based on text similarities, but it might miss connections that require reading between the lines or having a deep understanding of the topic. So, it can provide a starting point but its output will be limited in depth and uniqueness of insights.​ Automated Analysis Example research task: Generating a complete research report from raw project data Key Question Evaluation Note Necessary Context and Data? ❌ No The task requires complex reasoning and judgment that may be beyond AI's capabilities. Works with Input Constraints? ⚠️Partially For some projects, large datasets might exceed limits, leading to biased data selection. Can Output Be Verified and Modified? ❌ No Verification requires a full manual review, negating any time savings. For this task, you’re asking the AI to make judgment calls with limited context. It’s likely that biases will be introduced and relevant data will be left out of your results. Moreover, even though current models advertise being capable of “reasoning,” it is unclear whether their level of reasoning is available within your tool or would produce the same results as a human. For better results, break complex-analysis tasks into smaller, verifiable steps rather than attempting full automation. One benefit of working with subtasks is that you can have checks along the way to fix the AI’s errors (and thus “keep the human in the loop”). You thereby avoid an accumulation of AI errors and the risk of ending up in a totally wrong place. Conclusion While the limitations of AI systems are much clearer now than they were two years ago, how they will develop is still an open question. So, for the time being, it’s reasonable to limit our expectations of AI’s capabilities to scoped, well-defined tasks that can be verified easily. References Li, T., Zhang, G., Do, Q. D., Yue, X., and Chen, W. 2024. Long-context LLMs struggle with long in-context learning. arXiv preprint arXiv:2404.02060. Wang, M., Chen, L., Fu, C., Liao, S., Zhang, X., Wu, B., Yu, H., Xu, N., Zhang, L., Luo, R., Li, Y., Yang, M., Huang, F., and Li, Y. 2024. Leave no document behind: Benchmarking long-context LLMs with extended multi-doc QA. arXiv preprint arXiv:2406.17419.",
  "image": "https://media.nngroup.com/media/articles/opengraph_images/Common-Sense-AI_Opengraph.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp\u003e\u003cspan\u003e\n                  Summary: \n                \u003c/span\u003eExperience in the context of a UX-research platform shows that AI can be integrated well in focused tasks but isn’t capable of independent complex analysis.\n              \u003c/p\u003e\u003cdiv\u003e\n              \u003cp\u003eNow that \u003ca href=\"https://www.nngroup.com/articles/how-ai-works/\"\u003egenerative AI (genAI)\u003c/a\u003e has been around for a couple of years, the extreme hype is starting to simmer down. As the dust settles, we have a clearer picture of AI\u0026#39;s potential and limitations.\u003c/p\u003e\n\u003cp\u003eIn this article, I share some of the lessons learned through our genAI journey at Condens, our research-analysis platform. These lessons will be useful to anyone trying to determine how to integrate AI into their product strategy or research workflow.\u003c/p\u003e\n\u003cdiv\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-resisting-ai-pressure-1\"\u003eResisting AI Pressure\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-our-ai-product-integration-strategy-2\"\u003eOur AI-Product-Integration Strategy\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-3-key-questions-for-ai-integration-3\"\u003e3 Key Questions for AI Integration\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#toc-where-genai-succeeds-and-fails-4\"\u003eWhere GenAI Succeeds and Fails\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\u003ch2 id=\"toc-resisting-ai-pressure-1\"\u003eResisting AI Pressure\u003c/h2\u003e\n\u003cp\u003eWhen ChatGPT was released in 2022, I felt like a little kid with a new Game Boy, exploring prompts and imagining all the new possibilities. And like many companies, \u003ca href=\"https://condens.io/\"\u003eCondens\u003c/a\u003e began testing genAI\u0026#39;s potential for speeding up and improving UX research.\u003c/p\u003e\n\u003cp\u003eUnfortunately, we also started seeing very ambitious and exaggerated claims being made about AI’s capabilities and how it could be used — \u003cstrong\u003eclaims which conflicted with what we observed in our evaluations and testing.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eExamples:\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e“High-quality insights in seconds”\u003c/li\u003e\n\u003cli\u003e“Automatically analyze your projects”\u003c/li\u003e\n\u003cli\u003e“Eliminate bias from your research”\u003c/li\u003e\n\u003cli\u003e“User research without the users.”\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe found that, while generative AI could meaningfully improve research productivity, many of the claims, especially in marketing messaging, were overstated. Although many AI features looked impressive in demos, \u003ca href=\"https://www.nngroup.com/articles/ai-powered-tools-limitations/\"\u003ethey often didn’t perform well in real research situations.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis hype environment has placed considerable pressure on researchers and research vendors alike. At Condens, we had to define our \u003cstrong\u003eevidence-backed strategy towards common-sense AI integration\u003c/strong\u003e and stick to it.\u003c/p\u003e\n\u003cp\u003eIt wasn’t easy. \u003cstrong\u003eWe lost a customer \u003c/strong\u003efor our decision to take a more measured approach. The team decided to move to a competitor that was more aggressively integrating new AI features. Sadly, that move was not the choice of the research team — it was motivated by pressure from stakeholders who had seen unrealistic marketing about efficiency gains with AI. Four months later, the disappointed research team reported having to \u0026#34;drastically adjust their expectations\u0026#34; when the AI features underperformed and “reality hit” them.\u003c/p\u003e\n\u003cp\u003eWe concluded that significant work is still needed to \u003ca href=\"https://www.nngroup.com/articles/ai-user-value/\"\u003edevelop truly useful AI applications, \u003c/a\u003eand we have a responsibility to both drive innovation and maintain the hard-earned \u003ca href=\"https://www.nngroup.com/videos/building-trust-ux/\"\u003etrust\u003c/a\u003e of our researchers.\u003c/p\u003e\n\u003ch2 id=\"toc-our-ai-product-integration-strategy-2\"\u003eOur AI-Product-Integration Strategy\u003c/h2\u003e\n\u003cp\u003eI’d describe our approach to genAI in UX research (UXR) as cautiously optimistic. We’re eager to explore where AI can genuinely enhance UXR workflows, but we’re also mindful of its limitations. We won’t build AI features that don’t have value or reliability.\u003c/p\u003e\n\u003cp\u003eTo resist the hype and stay on the right track, we \u003cstrong\u003eestablished some of our own AI-design guidelines\u003c/strong\u003e:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGive AI tasks that are \u003cstrong\u003escoped\u003c/strong\u003e \u003cstrong\u003eand specific.\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eMake it easy to \u003cstrong\u003everify\u003c/strong\u003e \u003cstrong\u003eAI output\u003c/strong\u003e and always connect it back to evidence.\u003c/li\u003e\n\u003cli\u003eMake it easy to make \u003cstrong\u003echanges\u003c/strong\u003e \u003cstrong\u003eto AI output.\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eEnsure core tasks (in our case, data analysis) \u003cstrong\u003ecan also be done without AI.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"toc-3-key-questions-for-ai-integration-3\"\u003e3 Key Questions for AI Integration\u003c/h2\u003e\n\u003cp\u003eHere are 3 questions you should ask to decide whether a particular AI tool can reasonably support your task.\u003c/p\u003e\n\u003cp\u003e(When you ask these questions, consider the specific API or LLM version that you have access to. Though the latest and greatest systems may be powerful, these may not be available or cost-effective.)\u003c/p\u003e\n\u003cp\u003eWhile we developed these questions in our research context at Condens, they\u0026#39;re valuable for any AI implementation.\u003c/p\u003e\n\u003ch3\u003eDoes the AI Have the Necessary Context and Data?\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eAI often requires deeper context\u003c/strong\u003e about industries, companies, or research studies than can be provided in a prompt. Without sufficient context, results are rarely high-quality. That’s particularly true for UX work, which is extremely contextual.\u003c/p\u003e\n\u003cp\u003eAutomatic transcription and translation are excellent examples where AI excels because all necessary data is present in the input, requiring minimal additional context or interpretation.\u003c/p\u003e\n\u003ch3\u003eDoes the Task Work Within the AI’s Technical Input Constraints?\u003c/h3\u003e\n\u003cp\u003eResearch shows that\u003cstrong\u003e LLMs have technical limitations regarding the size of the input data. \u003c/strong\u003eFor example, GPT-4o currently takes a maximum of 128k tokens, which corresponds to roughly 10 hours of interviews transcribed. When provided with too much input, these models reduce the data they consider down to what they think is relevant, likely introducing bias into the results.\u003c/p\u003e\n\u003cp\u003eAdditionally, studies done at the Chinese Academy of Sciences and at University of Waterloo, respectively, suggest that result quality may deteriorate as input size increases. (AI researchers are actively working to develop solutions to this problem. At the time of writing, those solutions have not been conclusively proven.)\u003c/p\u003e\n\u003ch3\u003eCan the Output Be Verified and Modified?\u003c/h3\u003e\n\u003cp\u003eAI-generated content inherently requires fact checking, which is exponentially more challenging with larger datasets. In some cases, it may be faster and better for a human to do the task instead of verifying the AI-produced work.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eResearch findings and recommendations must be backed by evidence,\u003c/strong\u003e so AI’s output in UXR is useless if you can’t verify the source.\u003c/p\u003e\n\u003cp\u003eThe results should also be easy for humans to edit when needed, because it’s unlikely that they’ll take the AI output exactly as is. Especially if the output contains an error, people will need to be able to adjust it, even if just a bit.\u003c/p\u003e\n\u003ch2 id=\"toc-where-genai-succeeds-and-fails-4\"\u003eWhere GenAI Succeeds and Fails\u003c/h2\u003e\n\u003cp\u003eThese questions have been extremely useful for our team as we evaluate different research-related tasks that are candidates for genAI. Based on its current capabilities, here are our conclusions.\u003c/p\u003e\n\u003ch3\u003eSummarizing Small Chunks of Content\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eExample research task: \u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAsking AI to summarize a long quote from a participant to make it easy and quick to understand for yourself or stakeholders\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003eKey Question\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eEvaluation\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eNote\u003c/p\u003e\n\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eNecessary Context and Data?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e✅ Yes\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThe only input required is the quote.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eWorks with Input Constraints?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e✅ Yes\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eEven if it’s a long quote, you won’t exceed the technical limits.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eCan Output Be Verified and Modified?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e✅ Yes\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThe summary is usually 1–2 sentences so it can be quickly reviewed and verified.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eEven if the summary occasionally fails to capture the core of the message, it can still be useful.\u003c/p\u003e\n\u003ch3\u003eSummarizing Large Amounts of Content\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eExample research task:\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAsking AI to summarize an entire research project\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003eKey Question\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eEvaluation\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eNote\u003c/p\u003e\n\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eNecessary Context and Data?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e⚠️Partially\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eAI\u0026#39;s literal summarization approach becomes problematic with complex data that requires meaningful selection and interpretation.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eWorks with Input Constraints?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e⚠️Partially\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eProject data often exceeds input limits, forcing AI to make potentially biased preselections.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eCan Output Be Verified and Modified?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e⚠️Partially\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eWhile reading a summary is quick, verifying accuracy requires deep familiarity with the source material.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eSimple Semantic Search\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eExample research task:\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eFinding explicit user complaints or pain points mentioned in current project interviews\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003eKey Question\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eEvaluation\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eNote\u003c/p\u003e\n\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eNecessary Context and Data?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e✅ Yes\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eIdentifying explicit mentions requires minimal context.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eWorks with Input Constraints?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e✅ Yes\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eMost projects stay within the 10-hour transcript limit.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eCan Output Be Verified and Modified?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e⚠️Partially\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eUsers can quickly verify if the returned quotes mention pain points, but it is harder to check whether the output includes all mentions of pain points.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eComplex Semantic Search\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eExample research task:\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eIdentifying and ranking the most important pain points across all historical research data\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003eKey Question\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eEvaluation\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eNote\u003c/p\u003e\n\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eNecessary Context and Data?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e❌ No\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eRanking pain points requires significant interpretation and industry/company context that AI lacks.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eWorks with Input Constraints?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e❌ No\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eHistorical data exceeds AI limits, forcing potentially biased data selection.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eCan Output Be Verified and Modified?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e❌ No\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eVerification would require manual review of all source data.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThis task requires (1) identifying the pain points and (2) ranking them by importance. It involves interpretation and judgment and should not be left to AI. The AI output will likely miss subtle behavioral cues (e.g., facial expressions) that point to challenges encountered by users. An experienced researcher with context about the company and industry will definitely be better here.\u003c/p\u003e\n\u003cp\u003eA version of this task that would likely be more realistic and feasible with AI would involve ranking pain points across all \u003cem\u003eresearch insights\u003c/em\u003e generated from previous studies. That task would require substantially less data to be input into the AI and would make the output easier to verify.\u003c/p\u003e\n\u003ch3\u003eQuote Clustering\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eExample research task:\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eGrouping preselected quotes and observations into initial themes\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003eKey Question\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eEvaluation\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eNote\u003c/p\u003e\n\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eNecessary Context and Data?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e✅ Yes\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThe AI can work with quote text, although it might miss subtle connections.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eWorks with Input Constraints?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e✅ Yes\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eA single-project quote set should be fairly small.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eCan Output Be Verified and Modified?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e⚠️Partially\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eEffectiveness depends on how results are presented and how closely evidence links to themes\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eAI can provide initial quote clusters based on text similarities, but it might miss connections that require reading between the lines or having a deep understanding of the topic. So, it can provide a starting point but its output will be limited in depth and uniqueness of insights.​\u003c/p\u003e\n\u003ch3\u003eAutomated Analysis\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eExample research task:\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eGenerating a complete research report from raw project data\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003eKey Question\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eEvaluation\u003c/p\u003e\n\u003c/th\u003e\n\u003cth\u003e\n\u003cp\u003eNote\u003c/p\u003e\n\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eNecessary Context and Data?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e❌ No\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eThe task requires complex reasoning and judgment that may be beyond AI\u0026#39;s capabilities.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eWorks with Input Constraints?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e⚠️Partially\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eFor some projects, large datasets might exceed limits, leading to biased data selection.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003cth\u003e\n\u003cp\u003e\u003cstrong\u003eCan Output Be Verified and Modified?\u003c/strong\u003e\u003c/p\u003e\n\u003c/th\u003e\n\u003ctd\u003e\n\u003cp\u003e❌ No\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cp\u003eVerification requires a full manual review, negating any time savings.\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFor this task, you’re asking the AI to make judgment calls with limited context. It’s likely that biases will be introduced and relevant data will be left out of your results. Moreover, even though current models advertise being capable of “reasoning,” it is unclear whether their level of reasoning is available within your tool or would produce the same results as a human.\u003c/p\u003e\n\u003cp\u003eFor better results, break complex-analysis tasks into smaller, verifiable steps rather than attempting full automation. One benefit of working with subtasks is that you can have checks along the way to fix the AI’s errors (and thus “keep the human in the loop”). You thereby avoid an accumulation of AI errors and the risk of ending up in a totally wrong place.\u003c/p\u003e\n\u003ch3\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eWhile the limitations of AI systems are much clearer now than they were two years ago, how they will develop is still an open question. So, for the time being, it’s reasonable to limit our expectations of AI’s capabilities to scoped, well-defined tasks that can be verified easily.\u003c/p\u003e\n\u003ch3\u003eReferences\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLi, T., Zhang, G., Do, Q. D., Yue, X., and Chen, W. 2024. Long-context LLMs struggle with long in-context learning. \u003cem\u003earXiv preprint arXiv:2404.02060\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eWang, M., Chen, L., Fu, C., Liao, S., Zhang, X., Wu, B., Yu, H., Xu, N., Zhang, L., Luo, R., Li, Y., Yang, M., Huang, F., and Li, Y. 2024. Leave no document behind: Benchmarking long-context LLMs with extended multi-doc QA. \u003cem\u003earXiv preprint arXiv:2406.17419\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2025-02-21T18:00:00Z",
  "modifiedTime": null
}
