{
  "id": "ac1057bd-1cdc-474d-b81c-c59ccc700ecd",
  "title": "Sam Altman has an idea to get AI to ‘love humanity,’ use it to poll billions of people about their value systems",
  "link": "https://fortune.com/2024/11/21/sam-altman-one-wish-for-ai-openai/",
  "description": "The OpenAI CEO said it was his one wish for AI",
  "author": "Paolo Confino",
  "published": "Fri, 22 Nov 2024 01:03:00 +0000",
  "source": "https://fortune.com/feed",
  "categories": [
    "Business"
  ],
  "byline": "Paolo Confino",
  "length": 4857,
  "excerpt": "The OpenAI CEO said it was his one wish for AI.",
  "siteName": "Fortune",
  "favicon": "",
  "text": "He’s confident that trait could be built into AI systems—but not certain.  “I think so,” Altman said when asked the question during an interview with Harvard Business School senior associate dean Debora Spar.  The question of an AI uprising was once reserved purely for the science fiction of Isaac Asimov or the action films of James Cameron. But since the rise of AI, it has become, if not a hot-button issue, then at least a topic of debate that warrants genuine consideration. What would have once been deemed the musings of a crank, is now a genuine regulatory question.  OpenAI’s relationship with the government has been “fairly constructive,” Altman said. He added that a project as far-reaching and vast as developing AI should have been a government project.  “In a well-functioning society this would be a government project,” Altman said. “Given that it’s not happening, I think it’s better that it’s happening this way as an American project.” The federal government has yet to make significant progress on AI safety legislation. There was an effort in California to pass a law that would have held AI developers liable for catastrophic events like being used to develop weapons of mass destruction or to attack critical infrastructure. That bill passed in the legislature but was vetoed by California Governor Gavin Newsom.   Some of the preeminent figures in AI have warned that ensuring it is fully aligned with the good of mankind is a critical question. Nobel laureate Geoffrey Hinton, known as the Godfather of AI, said he couldn’t “see a path that guarantees safety.” Tesla CEO Elon Musk has regularly warned AI could lead to humanity’s extinction. Musk was instrumental to the founding of OpenAI, providing the non-profit with significant funding at its outset. Funding for which Altman remains “grateful,” despite the fact Musk is suing him.  There have been multiple organizations—like the non-profit organization the Alignment Research Center and the startup Safe Superintelligence founded by former OpenAI chief science officer—that have cropped up in recent years dedicated solely to this question.  OpenAI did not respond to a request for comment.  AI as it is currently designed is well suited to alignment, Altman said. Because of that, he argues, it would be easier than it might seem to ensure AI does not harm humanity.  “One of the things that has worked surprisingly well has been the ability to align an AI system to behave in a particular way,” he said. “So if we can articulate what that means in a bunch of different cases then, yeah, I think we can get the system to act that way.”  Altman also has a typically unique idea for how exactly OpenAI and other developers could “articulate” those principles and ideals needed to ensure AI remains on our side: use AI to poll the public at large. He suggested asking users of AI chatbots about their values and then using those answers to determine how to align an AI to protect humanity.  “I’m interested in the thought experiment [in which] an AI chats with you for a couple of hours about your value system,” he said. It “does that with me, with everybody else. And then says ‘ok I can’t make everybody happy all the time.’” Altman hopes that by communicating with and understanding billions of people “at a deep level,” the AI can identify challenges facing society more broadly. From there, AI could reach a consensus about what it would need to do to achieve the public’s general well-being. AI has an internal team dedicated to superalignment, tasked with ensuring that future digital superintelligence doesn’t go rogue and cause untold harm. In December 2023, the group released an early research paper that showed it was working on a process by which one large language model would oversee another one. This spring the leaders of that team, Sutskever and Jan Leike, left OpenAI. Their team was disbanded, according to reporting from CNBC at the time.  Leike said he left over increasing disagreements with OpenAI’s leadership about its commitment to safety as the company worked toward artificial general intelligence, a term that refers to an AI that is as smart as a human.  “Building smarter-than-human machines is an inherently dangerous endeavor,” Leike wrote on X. “OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.” When Leike left, Altman wrote on X that he was “super appreciative of [his] contributions to openai’s [sic] alignment research and safety culture.” How many degrees of separation are you from the globe's most powerful business leaders? Explore who made our brand-new list of the 100 Most Powerful People in Business. Plus, learn about the metrics we used to make it.",
  "image": "https://fortune.com/img-assets/wp-content/uploads/2024/11/GettyImages-1930535953-e1732221849946.jpg?resize=1200,600",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\n\n\u003cp\u003eHe’s confident that trait could be built into AI systems—but not certain. \u003c/p\u003e\n\n\n\n\u003cp\u003e“I think so,” Altman said when asked the question during an interview with Harvard Business School senior associate dean Debora Spar. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe question of an AI uprising was once reserved purely for the science fiction of Isaac Asimov or the action films of James Cameron. But since the rise of AI, it has become, if not a hot-button issue, then at least a \u003ca href=\"https://fortune.com/2023/05/03/openai-ex-safety-researcher-warns-ai-destroy-humanity/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2023/05/03/openai-ex-safety-researcher-warns-ai-destroy-humanity/\"\u003etopic of debate\u003c/a\u003e that warrants genuine consideration. What would have once been deemed the musings of a crank, is now a genuine regulatory question. \u003c/p\u003e\n\n\n\n\u003cp\u003eOpenAI’s relationship with the government has been “fairly constructive,” Altman said. He added that a project as far-reaching and vast as developing AI should have been a government project. \u003c/p\u003e\n\n\n\n\u003cp\u003e“In a well-functioning society this would be a government project,” Altman said. “Given that it’s not happening, I think it’s better that it’s happening this way as an American project.”\u003c/p\u003e\n\n\n\n\u003cp\u003eThe federal government has yet to make significant progress on AI safety legislation. There was an effort in California to \u003ca href=\"https://fortune.com/2024/09/20/california-ai-safety-bill-tom-siebel-gavin-newsom-tech-regulation/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2024/09/20/california-ai-safety-bill-tom-siebel-gavin-newsom-tech-regulation/\"\u003epass a law\u003c/a\u003e that would have \u003ca href=\"https://fortune.com/2024/09/19/california-gavin-newsom-ai-regulation-deepfakes-disinformation-sb1047/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2024/09/19/california-gavin-newsom-ai-regulation-deepfakes-disinformation-sb1047/\"\u003eheld AI developers liable\u003c/a\u003e for catastrophic events like being used to develop weapons of mass destruction or to attack critical infrastructure. That bill passed in the legislature but was \u003ca href=\"https://fortune.com/2024/09/29/california-governor-newsom-veto-ai-safety-bill/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2024/09/29/california-governor-newsom-veto-ai-safety-bill/\"\u003evetoed\u003c/a\u003e by California Governor Gavin Newsom.  \u003c/p\u003e\n\n\n\n\u003cp\u003eSome of the preeminent figures in AI have warned that ensuring it is fully aligned with the good of mankind is a critical question. Nobel laureate \u003ca href=\"https://fortune.com/2024/10/10/geoffrey-hinton-nobel-ai-pioneer-concerns/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2024/10/10/geoffrey-hinton-nobel-ai-pioneer-concerns/\"\u003eGeoffrey Hinton\u003c/a\u003e, known as the Godfather of AI, said he couldn’t “see a path that guarantees safety.” \u003ca href=\"https://fortune.com/company/tesla/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/tesla/\"\u003eTesla\u003c/a\u003e CEO Elon Musk has regularly warned AI could lead to humanity’s extinction. Musk was instrumental to the founding of OpenAI, providing the non-profit with significant funding at its outset. Funding for which Altman remains “\u003ca href=\"https://www.youtube.com/watch?v=xXCBz_8hM9w\" target=\"_blank\" aria-label=\"Go to https://www.youtube.com/watch?v=xXCBz_8hM9w\" rel=\"noopener\"\u003egrateful\u003c/a\u003e,” despite the fact Musk is suing him. \u003c/p\u003e\n\n\n\n\u003cp\u003eThere have been multiple organizations—like the non-profit organization the \u003ca href=\"https://fortune.com/company/aligned-research-center-arc/\" target=\"_self\" aria-label=\"Go to https://fortune.com/company/aligned-research-center-arc/\"\u003eAlignment Research Center\u003c/a\u003e and the startup \u003ca href=\"https://techcrunch.com/2024/06/19/ilya-sutskever-openais-former-chief-scientist-launches-new-ai-company/\" target=\"_blank\" aria-label=\"Go to https://techcrunch.com/2024/06/19/ilya-sutskever-openais-former-chief-scientist-launches-new-ai-company/\" rel=\"noopener\"\u003eSafe Superintelligence\u003c/a\u003e founded by former OpenAI chief science officer—that have cropped up in recent years dedicated solely to this question. \u003c/p\u003e\n\n\n\n\u003cp\u003eOpenAI did not respond to a request for comment. \u003c/p\u003e\n\n\n\n\u003cp\u003eAI as it is currently designed is well suited to alignment, Altman said. Because of that, he argues, it would be easier than it might seem to ensure AI does not harm humanity. \u003c/p\u003e\n\n\n\n\u003cp\u003e“One of the things that has worked surprisingly well has been the ability to align an AI system to behave in a particular way,” he said. “So if we can articulate what that means in a bunch of different cases then, yeah, I think we can get the system to act that way.” \u003c/p\u003e\n\n\n\n\u003cp\u003eAltman also has a typically unique idea for how exactly OpenAI and other developers could “articulate” those principles and ideals needed to ensure AI remains on our side: use AI to poll the public at large. He suggested asking users of AI chatbots about their values and then using those answers to determine how to align an AI to protect humanity. \u003c/p\u003e\n\n\n\n\u003cp\u003e“I’m interested in the thought experiment [in which] an AI chats with you for a couple of hours about your value system,” he said. It “does that with me, with everybody else. And then says ‘ok I can’t make everybody happy all the time.’”\u003c/p\u003e\n\n\n\n\u003cp\u003eAltman hopes that by communicating with and understanding billions of people “at a deep level,” the AI can identify challenges facing society more broadly. From there, AI could reach a consensus about what it would need to do to achieve the public’s general well-being.\u003c/p\u003e\n\n\n\n\u003cp\u003eAI has an internal team dedicated to \u003ca href=\"https://openai.com/index/introducing-superalignment/\" target=\"_blank\" aria-label=\"Go to https://openai.com/index/introducing-superalignment/\" rel=\"noopener\"\u003esuperalignment\u003c/a\u003e, tasked with ensuring that future digital superintelligence doesn’t go rogue and cause untold harm. In December 2023, the group released an early research paper that showed it was working on a process by which one large language model \u003ca href=\"https://www.technologyreview.com/2023/12/14/1085344/openai-super-alignment-rogue-agi-gpt-4/\" target=\"_blank\" aria-label=\"Go to https://www.technologyreview.com/2023/12/14/1085344/openai-super-alignment-rogue-agi-gpt-4/\" rel=\"noopener\"\u003ewould oversee another one\u003c/a\u003e. This spring the leaders of that team, Sutskever and Jan Leike, left OpenAI. Their team was disbanded, according to \u003ca href=\"https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html#:~:text=OpenAI%20has%20disbanded%20its%20team,from%20the%20Microsoft%2Dbacked%20startup.\" target=\"_blank\" aria-label=\"Go to https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html#:~:text=OpenAI%20has%20disbanded%20its%20team,from%20the%20Microsoft%2Dbacked%20startup.\" rel=\"noopener\"\u003ereporting\u003c/a\u003e from CNBC at the time. \u003c/p\u003e\n\n\n\n\u003cp\u003eLeike said he left over increasing disagreements with OpenAI’s leadership about its commitment to safety as the company worked toward artificial general intelligence, a term that refers to an AI that is as smart as a human. \u003c/p\u003e\n\n\n\n\u003cp\u003e“Building smarter-than-human machines is an inherently dangerous endeavor,” Leike \u003ca href=\"https://x.com/janleike/status/1791498183543251017\" target=\"_blank\" aria-label=\"Go to https://x.com/janleike/status/1791498183543251017\" rel=\"noopener\"\u003ewrote\u003c/a\u003e on \u003ca href=\"https://fortune.com/company/twitter/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/twitter/\"\u003eX\u003c/a\u003e. “OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.” \u003c/p\u003e\n\n\n\n\u003cp\u003eWhen Leike left, Altman \u003ca href=\"https://x.com/sama/status/1791543264090472660\" target=\"_blank\" aria-label=\"Go to https://x.com/sama/status/1791543264090472660\" rel=\"noopener\"\u003ewrote on X\u003c/a\u003e that he was “super appreciative of\u003ca href=\"https://x.com/janleike\" target=\"_blank\" aria-label=\"Go to https://x.com/janleike\" rel=\"noopener\"\u003e \u003c/a\u003e[his] contributions to openai’s [sic] alignment research and safety culture.” \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cb\u003eHow many degrees of separation are you from the globe\u0026#39;s most powerful business leaders? \u003c/b\u003eExplore who made our brand-new list of the \u003ca href=\"https://fortune.com/ranking/most-powerful-people/?\u0026amp;itm_source=fortune\u0026amp;itm_medium=article_tout\" target=\"_self\" aria-label=\"Go to https://fortune.com/ranking/most-powerful-people/?\u0026amp;itm_source=fortune\u0026amp;itm_medium=article_tout\"\u003e100 Most Powerful People in Business\u003c/a\u003e. Plus, learn about the metrics we used to make it.\u003c/p\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
