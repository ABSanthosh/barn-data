{
  "id": "0f7440cb-cd6f-4095-83bd-ee18ab10fa71",
  "title": "Jensen Huang Just Delivered Incredible News for Nvidia Stock Investors",
  "link": "https://finance.yahoo.com/news/jensen-huang-just-delivered-incredible-093300749.html",
  "description": "",
  "author": "",
  "published": "2024-10-10T09:33:00Z",
  "source": "https://finance.yahoo.com/news/rssindex",
  "categories": null,
  "byline": "Anthony Di Pizio",
  "length": 2579,
  "excerpt": "The CEO of Nvidia just provided an update on the company's new Blackwell artificial intelligence chips.",
  "siteName": "Yahoo Finance",
  "favicon": "https://s.yimg.com/cv/apiv2/default/finance/favicon-180x180.png",
  "text": "Nvidia (NASDAQ: NVDA) supplies the most powerful graphics processing chips (GPUs) for developing artificial intelligence (AI) models. The company has an impressive list of customers who are spending truckloads of money to fill their data centers with those chips as they battle for AI supremacy. In fact, Nvidia CEO Jensen Huang believes AI infrastructure spending will top $1 trillion over the next five years, and his company is likely to be the biggest winner because it has a dominant market share in the GPU space. In an interview with CNBC last week, Huang made a series of positive comments about Nvidia's new Blackwell GPU architecture. Here's why every investor in Nvidia stock should be excited. Blackwell will be transformative for Nvidia and AI developers alike Last year, Nvidia's H100 data center GPU set the benchmark for AI development. It's still in hot demand today, but it was superseded by the H200, which is capable of performing AI inference (ingesting live data into AI models to make predictions) at twice the speed. But Nvidia's Blackwell architecture delivers a major leap in performance. The new Blackwell-based GB200 NVL72 system is capable of performing AI inference at a staggering 30 times the speed of the equivalent H100 system. Plus, each individual GB200 GPU is expected to sell for between $30,000 and $40,000, which is in the ballpark of what many data center operators originally paid for their H100 GPUs. In other words, Blackwell is going to deliver an incredible improvement in cost efficiency. That's really important because most AI developers rent data center computing capacity by the minute from tech giants like Microsoft and Amazon. Blackwell will bring down the cost of deploying more powerful large language models (LLMs), which will make advanced AI applications affordable for a wider group of consumers and businesses. During its fiscal 2025 second quarter (ended July 28), Nvidia generated a record $26.3 billion in data center revenue, up 154% from the year-ago period. The company — along with most Wall Street analysts — expects data center revenue to continue surging higher for the foreseeable future. However, there have been questions about how long tech giants can continue spending so much money on their AI infrastructure. Microsoft, for example, reported $55.7 billion in capital expenditures (capex) during its fiscal 2024 (ended June 30), much of which went toward data centers and chips. The company plans to spend even more in fiscal 2025. How sustainable is this level of investment across the tech industry?",
  "image": "https://s.yimg.com/cv/apiv2/social/images/yahoo_default_logo.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e    \u003cp\u003e\u003cstrong\u003eNvidia\u003c/strong\u003e (NASDAQ: NVDA) supplies the most powerful graphics processing chips (GPUs) for developing artificial intelligence (AI) models. The company has an impressive list of customers who are spending truckloads of money to fill their data centers with those chips as they battle for AI supremacy.\u003c/p\u003e \u003cp\u003eIn fact, Nvidia CEO Jensen Huang believes AI infrastructure spending will top $1 trillion over the next five years, and his company is likely to be the biggest winner because it has a dominant market share in the GPU space.\u003c/p\u003e  \u003cp\u003eIn an interview with CNBC last week, Huang made a series of positive comments about Nvidia\u0026#39;s new Blackwell GPU architecture. Here\u0026#39;s why every investor in Nvidia stock should be excited.\u003c/p\u003e \u003ch2\u003eBlackwell will be transformative for Nvidia and AI developers alike\u003c/h2\u003e \u003cp\u003eLast year, Nvidia\u0026#39;s H100 data center \u003ca data-i13n=\"cpos:1;pos:1\" href=\"https://www.fool.com/terms/g/gpu/?utm_source=yahoo-host-full\u0026amp;utm_medium=feed\u0026amp;utm_campaign=article\u0026amp;referring_guid=73226c19-a911-47df-bb58-583e7273b317\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:GPU;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas\"\u003eGPU\u003c/a\u003e set the benchmark for AI development. It\u0026#39;s still in hot demand today, but it was superseded by the H200, which is capable of performing AI inference (ingesting live data into AI models to make predictions) at twice the speed.\u003c/p\u003e \u003cp\u003eBut Nvidia\u0026#39;s Blackwell architecture delivers a major leap in performance. The new Blackwell-based GB200 NVL72 system is capable of performing AI inference at a staggering 30 times the speed of the equivalent H100 system. Plus, each individual GB200 GPU is expected to sell for between $30,000 and $40,000, which is in the ballpark of what many data center operators originally paid for their H100 GPUs.\u003c/p\u003e \u003cp\u003eIn other words, Blackwell is going to deliver an \u003cem\u003eincredible\u003c/em\u003e improvement in cost efficiency. That\u0026#39;s really important because most AI developers rent data center computing capacity by the minute from tech giants like \u003cstrong\u003eMicrosoft\u003c/strong\u003e and \u003ca data-i13n=\"cpos:2;pos:1\" href=\"https://www.fool.com/investing/2024/09/10/295-warren-buffetts-3057-billion-in-2-ai-stocks/?utm_source=yahoo-host-full\u0026amp;utm_medium=feed\u0026amp;utm_campaign=article\u0026amp;referring_guid=73226c19-a911-47df-bb58-583e7273b317\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:Amazon;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas\"\u003e\u003cstrong\u003eAmazon\u003c/strong\u003e\u003c/a\u003e. Blackwell will bring down the cost of deploying more powerful large language models (LLMs), which will make advanced AI applications affordable for a wider group of consumers and businesses.\u003c/p\u003e \u003cp\u003eDuring its fiscal 2025 second quarter (ended July 28), Nvidia generated a record $26.3 billion in data center revenue, up 154% from the year-ago period. The company — along with most Wall Street analysts — expects data center revenue to continue surging higher for the foreseeable future.\u003c/p\u003e \u003cp\u003eHowever, there have been questions about how long tech giants can continue spending so much money on their AI infrastructure. Microsoft, for example, reported $55.7 billion in capital expenditures (capex) during its fiscal 2024 (ended June 30), much of which went toward data centers and chips. The company plans to spend even more in fiscal 2025. How sustainable is this level of investment across the tech industry?\u003c/p\u003e       \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2024-10-10T09:33:00Z",
  "modifiedTime": null
}
