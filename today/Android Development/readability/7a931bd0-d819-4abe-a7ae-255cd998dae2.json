{
  "id": "7a931bd0-d819-4abe-a7ae-255cd998dae2",
  "title": "Create a spotlight effect with CameraX and Jetpack Compose",
  "link": "https://medium.com/androiddevelopers/create-a-spotlight-effect-with-camerax-and-jetpack-compose-8a7fa5b76641?source=rss----95b274b437c2---4",
  "description": "",
  "author": "Jolanda Verhoef",
  "published": "Thu, 23 Jan 2025 17:03:20 GMT",
  "source": "https://medium.com/feed/androiddevelopers",
  "categories": [
    "compose",
    "mobile-app-development",
    "jetpack-compose",
    "android"
  ],
  "byline": "Jolanda Verhoef",
  "length": 9742,
  "excerpt": "Hey there! Welcome back to our series on CameraX and Jetpack Compose. In the previous posts, weâ€™ve covered the fundamentals of setting up a camera preview and added tap-to-focus functionality. Inâ€¦",
  "siteName": "Android Developers",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "Part 3 of Unlocking the Power of CameraX in Jetpack ComposeHey there! Welcome back to our series on CameraX and Jetpack Compose. In the previous posts, weâ€™ve covered the fundamentals of setting up a camera preview and added tap-to-focus functionality.ðŸ§± Part 1: Building a basic camera preview using the new camera-compose artifact. We covered permission handling and basic integration.ðŸ‘† Part 2: Using the Compose gesture system, graphics, and coroutines to implement a visual tap-to-focus.ðŸ”¦ Part 3 (this post): Exploring how to overlay Compose UI elements on top of your camera preview for a richer user experience.ðŸ“‚ Part 4: Using adaptive APIs and the Compose animation framework to smoothly animate to and from tabletop mode on foldable phones.In this post, weâ€™ll dive into something a bit more visually engaging â€” implementing a spotlight effect on top of our camera preview, using face detection as the basis for the effect. Why, you say? Iâ€™m not sure. But it sure looks cool ðŸ™‚. And, more importantly, it demonstrates how we can easily translate sensor coordinates into UI coordinates, allowing us to use them in Compose!Enable face detectionFirst, letâ€™s modify the CameraPreviewViewModel to enable face detection. Weâ€™ll use the Camera2Interop API, which allows us to interact with the underlying Camera2 API from CameraX. This gives us the opportunity to use camera features that are not exposed by CameraX directly. We need to make the following changes:Create a StateFlow that contains the face bounds as a list of Rects.Set the STATISTICS_FACE_DETECT_MODE capture request option to FULL, which enables face detection.Set a CaptureCallback to get the face information from the capture result.class CameraPreviewViewModel : ViewModel() { ... private val _sensorFaceRects = MutableStateFlow(listOf\u003cRect\u003e()) val sensorFaceRects: StateFlow\u003cList\u003cRect\u003e\u003e = _sensorFaceRects.asStateFlow() private val cameraPreviewUseCase = Preview.Builder() .apply { Camera2Interop.Extender(this) .setCaptureRequestOption( CaptureRequest.STATISTICS_FACE_DETECT_MODE, CaptureRequest.STATISTICS_FACE_DETECT_MODE_FULL ) .setSessionCaptureCallback(object : CameraCaptureSession.CaptureCallback() { override fun onCaptureCompleted( session: CameraCaptureSession, request: CaptureRequest, result: TotalCaptureResult ) { super.onCaptureCompleted(session, request, result) result.get(CaptureResult.STATISTICS_FACES) ?.map { face -\u003e face.bounds.toComposeRect() } ?.toList() ?.let { faces -\u003e _sensorFaceRects.update { faces } } } }) } .build().apply { ...}With these changes in place, our view model now emits a list of Rectobjects representing the bounding boxes of detected faces in sensor coordinates.Translate sensor coordinates to UI coordinatesThe bounding boxes of detected faces that we stored in the last section use coordinates in the sensor coordinate system. To draw the bounding boxes in our UI, we need to transform these coordinates so that they are correct in the Compose coordinate system. We need to:Transform the sensor coordinates into preview buffer coordinatesTransform the preview buffer coordinates into Compose UI coordinatesThese transformations are done using transformation matrices. Each of the transformations has its own matrix:Our SurfaceRequest holds on to a TransformationInfo instance, which contains a sensorToBufferTranform matrix.Our CameraXViewfinder has an associated CoordinateTransformer. You might remember that we already used this transformer in the previous blog post to transform tap-to-focus coordinates.We can create a helper method that can do the transformation for us:private fun List\u003cRect\u003e.transformToUiCoords( transformationInfo: SurfaceRequest.TransformationInfo?, uiToBufferCoordinateTransformer: MutableCoordinateTransformer): List\u003cRect\u003e = this.map { sensorRect -\u003e val bufferToUiTransformMatrix = Matrix().apply { setFrom(uiToBufferCoordinateTransformer.transformMatrix) invert() } val sensorToBufferTransformMatrix = Matrix().apply { transformationInfo?.let { setFrom(it.sensorToBufferTransform) } } val bufferRect = sensorToBufferTransformMatrix.map(sensorRect) val uiRect = bufferToUiTransformMatrix.map(bufferRect) uiRect}We iterate through the list of detected faces, and for each face execute the transformation.The CoordinateTransformer.transformMatrix that we get from our CameraXViewfinder transforms coordinates from UI to buffer coordinates by default. In our case, we want the matrix to work the other way around, transforming buffer coordinates into UI coordinates. Therefore, we use the invert() method to invert the matrix.We first transform the face from sensor coordinates to buffer coordinates using the sensorToBufferTransformMatrix, and then transform those buffer coordinates to UI coordinates using the bufferToUiTransformMatrix.Implement the spotlight effectNow, letâ€™s update the CameraPreviewContent composable to draw the spotlight effect. Weâ€™ll use a Canvas composable to draw a gradient mask over the preview, making the detected faces visible:@Composablefun CameraPreviewContent( viewModel: CameraPreviewViewModel, modifier: Modifier = Modifier, lifecycleOwner: LifecycleOwner = LocalLifecycleOwner.current) { val surfaceRequest by viewModel.surfaceRequest.collectAsStateWithLifecycle() val sensorFaceRects by viewModel.sensorFaceRects.collectAsStateWithLifecycle() val transformationInfo by produceState\u003cSurfaceRequest.TransformationInfo?\u003e(null, surfaceRequest) { try { surfaceRequest?.setTransformationInfoListener(Runnable::run) { transformationInfo -\u003e value = transformationInfo } awaitCancellation() } finally { surfaceRequest?.clearTransformationInfoListener() } } val shouldSpotlightFaces by remember { derivedStateOf { sensorFaceRects.isNotEmpty() \u0026\u0026 transformationInfo != null} } val spotlightColor = Color(0xDDE60991) .. surfaceRequest?.let { request -\u003e val coordinateTransformer = remember { MutableCoordinateTransformer() } CameraXViewfinder( surfaceRequest = request, coordinateTransformer = coordinateTransformer, modifier = .. ) AnimatedVisibility(shouldSpotlightFaces, enter = fadeIn(), exit = fadeOut()) { Canvas(Modifier.fillMaxSize()) { val uiFaceRects = sensorFaceRects.transformToUiCoords( transformationInfo = transformationInfo, uiToBufferCoordinateTransformer = coordinateTransformer ) // Fill the whole space with the color drawRect(spotlightColor) // Then extract each face and make it transparent uiFaceRects.forEach { faceRect -\u003e drawRect( Brush.radialGradient( 0.4f to Color.Black, 1f to Color.Transparent, center = faceRect.center, radius = faceRect.minDimension * 2f, ), blendMode = BlendMode.DstOut ) } } } }}Hereâ€™s how it works:We collect the list of faces from the view model.To make sure weâ€™re not recomposing the whole screen every time the list of detected faces changes, we use derivedStateOf to keep track of whether any faces are detected at all. This can then be used with AnimatedVisibility to animate the colored overlay in and out.The surfaceRequest contains the information we need to transform sensor coordinates to buffer coordinates in the SurfaceRequest.TransformationInfo. We use the produceState function to set up a listener in the surface request, and clear this listener when the composable leaves the composition tree.We use a Canvas to draw a translucent pink rectangle that covers the entire screen.We defer the reading of the sensorFaceRects variable until weâ€™re inside the Canvas draw block. Then we transform the coordinates into UI coordinates.We iterate over the detected faces, and for each face, we draw a radial gradient that will make the inside of the face rectangle transparent.We use BlendMode.DstOut to make sure that we are cutting out the gradient from the pink rectangle, creating the spotlight effect.Note: When you change the camera to DEFAULT_FRONT_CAMERA you will notice that the spotlight is mirrored! This is a known issue, tracked in the Google Issue Tracker.ResultWith this code, we have a fully functional spotlight effect that highlights detected faces. You can find the full code snippet here.This effect is just the beginning â€” by using the power of Compose, you can create a myriad of visually stunning camera experiences. Being able to transform sensor and buffer coordinates into Compose UI coordinates and back means we can utilize all Compose UI features and integrate them seamlessly with the underlying camera system. With animations, advanced UI graphics, simple UI state management, and full gesture control, your imagination is the limit!In the final post of the series, weâ€™ll dive into how to use adaptive APIs and the Compose animation framework to seamlessly transition between different camera UIs on foldable devices. Stay tuned!",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*Ay6Qn6hYSX4FzqZFWv2Nkw.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"7a94\"\u003ePart 3 of Unlocking the Power of CameraX in Jetpack Compose\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca rel=\"noopener follow\" href=\"https://medium.com/@lojanda?source=post_page---byline--8a7fa5b76641---------------------------------------\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Jolanda Verhoef\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*XXxKnGkW5RkuVemStEYjdw.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/androiddevelopers?source=post_page---byline--8a7fa5b76641---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Android Developers\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*4Tg6pPzer7cIarYaszIKaQ.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cp id=\"ed53\"\u003eHey there! Welcome back to our series on CameraX and Jetpack Compose. In the previous posts, weâ€™ve covered the fundamentals of setting up a camera preview and added tap-to-focus functionality.\u003c/p\u003e\u003cul\u003e\u003cli id=\"1ba8\"\u003e\u003cstrong\u003eðŸ§± \u003c/strong\u003e\u003ca rel=\"noopener\" href=\"https://medium.com/androiddevelopers/getting-started-with-camerax-in-jetpack-compose-781c722ca0c4\"\u003e\u003cstrong\u003ePart 1\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e:\u003c/strong\u003e Building a basic camera preview using the new camera-compose artifact. We covered permission handling and basic integration.\u003c/li\u003e\u003cli id=\"1dbc\"\u003e\u003cstrong\u003eðŸ‘† \u003c/strong\u003e\u003ca rel=\"noopener\" href=\"https://medium.com/androiddevelopers/tap-to-focus-mastering-camerax-transformations-in-jetpack-compose-440853280a6e\"\u003e\u003cstrong\u003ePart 2\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e:\u003c/strong\u003e Using the Compose gesture system, graphics, and coroutines to implement a visual tap-to-focus.\u003c/li\u003e\u003cli id=\"3359\"\u003e\u003cstrong\u003eðŸ”¦ Part 3 (this post):\u003c/strong\u003e Exploring how to overlay Compose UI elements on top of your camera preview for a richer user experience.\u003c/li\u003e\u003cli id=\"3d03\"\u003e\u003cstrong\u003eðŸ“‚ Part 4\u003c/strong\u003e: Using adaptive APIs and the Compose animation framework to smoothly animate to and from tabletop mode on foldable phones.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"8ef6\"\u003eIn this post, weâ€™ll dive into something a bit more visually engaging â€” implementing a spotlight effect on top of our camera preview, using face detection as the basis for the effect. Why, you say? Iâ€™m not sure. But it sure looks cool ðŸ™‚. And, more importantly, it demonstrates how we can easily translate sensor coordinates into UI coordinates, allowing us to use them in Compose!\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"794a\"\u003eEnable face detection\u003c/h2\u003e\u003cp id=\"cef7\"\u003eFirst, letâ€™s modify the CameraPreviewViewModel to enable face detection. Weâ€™ll use the \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/androidx/camera/camera2/interop/Camera2Interop\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCamera2Interop\u003c/a\u003e\u003c/code\u003e API, which allows us to interact with the underlying Camera2 API from CameraX. This gives us the opportunity to use camera features that are not exposed by CameraX directly. We need to make the following changes:\u003c/p\u003e\u003cul\u003e\u003cli id=\"6291\"\u003eCreate a StateFlow that contains the face bounds as a list of \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/compose/ui/geometry/Rect\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRect\u003c/a\u003e\u003c/code\u003es.\u003c/li\u003e\u003cli id=\"4734\"\u003eSet the \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/android/hardware/camera2/CaptureRequest#STATISTICS_FACE_DETECT_MODE\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSTATISTICS_FACE_DETECT_MODE\u003c/a\u003e\u003c/code\u003e capture request option to FULL, which enables face detection.\u003c/li\u003e\u003cli id=\"4f9c\"\u003eSet a \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCaptureSession.CaptureCallback\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCaptureCallback\u003c/a\u003e\u003c/code\u003e to get the face information from the capture result.\u003c/li\u003e\u003c/ul\u003e\u003cpre\u003e\u003cspan id=\"3ff6\"\u003eclass CameraPreviewViewModel : ViewModel() {\u003cbr/\u003e    ...\u003cbr/\u003e    private val _sensorFaceRects = MutableStateFlow(listOf\u0026lt;Rect\u0026gt;())\u003cbr/\u003e    val sensorFaceRects: StateFlow\u0026lt;List\u0026lt;Rect\u0026gt;\u0026gt; = _sensorFaceRects.asStateFlow()\u003cp\u003e    private val cameraPreviewUseCase = Preview.Builder()\u003cbr/\u003e        .apply {\u003cbr/\u003e            Camera2Interop.Extender(this)\u003cbr/\u003e                .setCaptureRequestOption(\u003cbr/\u003e                    CaptureRequest.STATISTICS_FACE_DETECT_MODE,\u003cbr/\u003e                    CaptureRequest.STATISTICS_FACE_DETECT_MODE_FULL\u003cbr/\u003e                )\u003cbr/\u003e                .setSessionCaptureCallback(object : CameraCaptureSession.CaptureCallback() {\u003cbr/\u003e                    override fun onCaptureCompleted(\u003cbr/\u003e                        session: CameraCaptureSession,\u003cbr/\u003e                        request: CaptureRequest,\u003cbr/\u003e                        result: TotalCaptureResult\u003cbr/\u003e                    ) {\u003cbr/\u003e                        super.onCaptureCompleted(session, request, result)\u003cbr/\u003e                        result.get(CaptureResult.STATISTICS_FACES)\u003cbr/\u003e                            ?.map { face -\u0026gt; face.bounds.toComposeRect() }\u003cbr/\u003e                            ?.toList()\u003cbr/\u003e                            ?.let { faces -\u0026gt; _sensorFaceRects.update { faces } }\u003cbr/\u003e                    }\u003cbr/\u003e                })\u003cbr/\u003e        }\u003cbr/\u003e        .build().apply {\u003cbr/\u003e    ...\u003cbr/\u003e}\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"26a6\"\u003eWith these changes in place, our view model now emits a list of \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/compose/ui/geometry/Rect\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRect\u003c/a\u003e\u003c/code\u003eobjects representing the bounding boxes of detected faces in sensor coordinates.\u003c/p\u003e\u003ch2 id=\"4b0f\"\u003eTranslate sensor coordinates to UI coordinates\u003c/h2\u003e\u003cp id=\"943a\"\u003eThe bounding boxes of detected faces that we stored in the last section use coordinates in the \u003cstrong\u003esensor coordinate system\u003c/strong\u003e. To draw the bounding boxes in our UI, we need to transform these coordinates so that they are correct in the Compose coordinate system. We need to:\u003c/p\u003e\u003cul\u003e\u003cli id=\"0452\"\u003eTransform the \u003cstrong\u003esensor coordinates\u003c/strong\u003e into \u003cstrong\u003epreview buffer coordinates\u003c/strong\u003e\u003c/li\u003e\u003cli id=\"b67c\"\u003eTransform the \u003cstrong\u003epreview buffer coordinates\u003c/strong\u003e into \u003cstrong\u003eCompose UI coordinates\u003c/strong\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"76f4\"\u003eThese transformations are done using transformation matrices. Each of the transformations has its own matrix:\u003c/p\u003e\u003cul\u003e\u003cli id=\"e0da\"\u003eOur \u003ccode\u003eSurfaceRequest\u003c/code\u003e holds on to a \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/androidx/camera/core/SurfaceRequest.TransformationInfo\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTransformationInfo\u003c/a\u003e\u003c/code\u003e instance, which contains a \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/androidx/camera/core/SurfaceRequest.TransformationInfo#getSensorToBufferTransform()\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esensorToBufferTranform\u003c/a\u003e\u003c/code\u003e matrix.\u003c/li\u003e\u003cli id=\"d4bd\"\u003eOur \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/camera/compose/package-summary#CameraXViewfinder(androidx.camera.core.SurfaceRequest,androidx.compose.ui.Modifier,androidx.camera.viewfinder.core.ImplementationMode,androidx.camera.viewfinder.compose.MutableCoordinateTransformer)\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCameraXViewfinder\u003c/a\u003e\u003c/code\u003e has an associated \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/camera/viewfinder/compose/CoordinateTransformer\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCoordinateTransformer\u003c/a\u003e\u003c/code\u003e. You might remember that we already used this transformer in the previous blog post to transform tap-to-focus coordinates.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"d018\"\u003eWe can create a helper method that can do the transformation for us:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"be90\"\u003eprivate fun List\u0026lt;Rect\u0026gt;.transformToUiCoords(\u003cbr/\u003e    transformationInfo: SurfaceRequest.TransformationInfo?,\u003cbr/\u003e    uiToBufferCoordinateTransformer: MutableCoordinateTransformer\u003cbr/\u003e): List\u0026lt;Rect\u0026gt; = this.map { sensorRect -\u0026gt;\u003cbr/\u003e    val bufferToUiTransformMatrix = Matrix().apply {\u003cbr/\u003e        setFrom(uiToBufferCoordinateTransformer.transformMatrix)\u003cbr/\u003e        invert()\u003cbr/\u003e    }\u003cp\u003e    val sensorToBufferTransformMatrix = Matrix().apply {\u003cbr/\u003e        transformationInfo?.let {\u003cbr/\u003e            setFrom(it.sensorToBufferTransform)\u003cbr/\u003e        }\u003cbr/\u003e    }\u003c/p\u003e\u003cp\u003e    val bufferRect = sensorToBufferTransformMatrix.map(sensorRect)\u003cbr/\u003e    val uiRect = bufferToUiTransformMatrix.map(bufferRect)\u003c/p\u003e\u003cp\u003e    uiRect\u003cbr/\u003e}\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cul\u003e\u003cli id=\"a687\"\u003eWe iterate through the list of detected faces, and for each face execute the transformation.\u003c/li\u003e\u003cli id=\"036f\"\u003eThe \u003ccode\u003eCoordinateTransformer.transformMatrix\u003c/code\u003e that we get from our \u003ccode\u003eCameraXViewfinder\u003c/code\u003e transforms coordinates from UI to buffer coordinates by default. In our case, we want the matrix to work the other way around, transforming buffer coordinates into UI coordinates. Therefore, we use the \u003ccode\u003einvert()\u003c/code\u003e method to invert the matrix.\u003c/li\u003e\u003cli id=\"8cc8\"\u003eWe first transform the face from sensor coordinates to buffer coordinates using the \u003ccode\u003esensorToBufferTransformMatrix\u003c/code\u003e, and then transform those buffer coordinates to UI coordinates using the \u003ccode\u003ebufferToUiTransformMatrix\u003c/code\u003e.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"c479\"\u003eImplement the spotlight effect\u003c/h2\u003e\u003cp id=\"ad43\"\u003eNow, letâ€™s update the \u003ccode\u003eCameraPreviewContent\u003c/code\u003e composable to draw the spotlight effect. Weâ€™ll use a \u003ccode\u003e\u003ca href=\"https://developer.android.com/reference/kotlin/androidx/compose/ui/graphics/package-summary#Canvas(android.graphics.Canvas)\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCanvas\u003c/a\u003e\u003c/code\u003e composable to draw a gradient mask over the preview, making the detected faces visible:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"80b5\"\u003e@Composable\u003cbr/\u003efun CameraPreviewContent(\u003cbr/\u003e    viewModel: CameraPreviewViewModel,\u003cbr/\u003e    modifier: Modifier = Modifier,\u003cbr/\u003e    lifecycleOwner: LifecycleOwner = LocalLifecycleOwner.current\u003cbr/\u003e) {\u003cbr/\u003e    val surfaceRequest by viewModel.surfaceRequest.collectAsStateWithLifecycle()\u003cbr/\u003e    val sensorFaceRects by viewModel.sensorFaceRects.collectAsStateWithLifecycle()\u003cbr/\u003e    val transformationInfo by\u003cbr/\u003e        produceState\u0026lt;SurfaceRequest.TransformationInfo?\u0026gt;(null, surfaceRequest) {\u003cbr/\u003e            try {\u003cbr/\u003e                surfaceRequest?.setTransformationInfoListener(Runnable::run) { transformationInfo -\u0026gt;\u003cbr/\u003e                    value = transformationInfo\u003cbr/\u003e                }\u003cbr/\u003e                awaitCancellation()\u003cbr/\u003e            } finally {\u003cbr/\u003e                surfaceRequest?.clearTransformationInfoListener()\u003cbr/\u003e            }\u003cbr/\u003e        }\u003cbr/\u003e    val shouldSpotlightFaces by remember {\u003cbr/\u003e        derivedStateOf { sensorFaceRects.isNotEmpty() \u0026amp;\u0026amp; transformationInfo != null} \u003cbr/\u003e    }\u003cbr/\u003e    val spotlightColor = Color(0xDDE60991)\u003cbr/\u003e    ..\u003cp\u003e    surfaceRequest?.let { request -\u0026gt;\u003cbr/\u003e        val coordinateTransformer = remember { MutableCoordinateTransformer() }\u003cbr/\u003e        CameraXViewfinder(\u003cbr/\u003e            surfaceRequest = request,\u003cbr/\u003e            coordinateTransformer = coordinateTransformer,\u003cbr/\u003e            modifier = ..\u003cbr/\u003e        )\u003c/p\u003e\u003cp\u003e        AnimatedVisibility(shouldSpotlightFaces, enter = fadeIn(), exit = fadeOut()) {\u003cbr/\u003e            Canvas(Modifier.fillMaxSize()) {\u003cbr/\u003e                val uiFaceRects = sensorFaceRects.transformToUiCoords(\u003cbr/\u003e                    transformationInfo = transformationInfo,\u003cbr/\u003e                    uiToBufferCoordinateTransformer = coordinateTransformer\u003cbr/\u003e                )\u003c/p\u003e\u003cp\u003e                // Fill the whole space with the color\u003cbr/\u003e                drawRect(spotlightColor)\u003cbr/\u003e                // Then extract each face and make it transparent\u003c/p\u003e\u003cp\u003e                uiFaceRects.forEach { faceRect -\u0026gt;\u003cbr/\u003e                    drawRect(\u003cbr/\u003e                        Brush.radialGradient(\u003cbr/\u003e                            0.4f to Color.Black, 1f to Color.Transparent,\u003cbr/\u003e                            center = faceRect.center,\u003cbr/\u003e                            radius = faceRect.minDimension * 2f,\u003cbr/\u003e                        ),\u003cbr/\u003e                        blendMode = BlendMode.DstOut\u003cbr/\u003e                    )\u003cbr/\u003e                }\u003cbr/\u003e            }\u003cbr/\u003e        }\u003cbr/\u003e    }\u003cbr/\u003e}\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"ace7\"\u003eHereâ€™s how it works:\u003c/p\u003e\u003cul\u003e\u003cli id=\"019b\"\u003eWe collect the list of faces from the view model.\u003c/li\u003e\u003cli id=\"8243\"\u003eTo make sure weâ€™re not recomposing the whole screen every time the list of detected faces changes, we use \u003ccode\u003ederivedStateOf\u003c/code\u003e to keep track of whether any faces are detected at all. This can then be used with \u003ccode\u003eAnimatedVisibility\u003c/code\u003e to animate the colored overlay in and out.\u003c/li\u003e\u003cli id=\"aaf0\"\u003eThe \u003ccode\u003esurfaceRequest\u003c/code\u003e contains the information we need to transform sensor coordinates to buffer coordinates in the \u003ccode\u003eSurfaceRequest.TransformationInfo\u003c/code\u003e. We use the \u003ccode\u003eproduceState\u003c/code\u003e function to set up a listener in the surface request, and clear this listener when the composable leaves the composition tree.\u003c/li\u003e\u003cli id=\"14f3\"\u003eWe use a \u003ccode\u003eCanvas\u003c/code\u003e to draw a translucent pink rectangle that covers the entire screen.\u003c/li\u003e\u003cli id=\"b931\"\u003eWe defer the reading of the \u003ccode\u003esensorFaceRects\u003c/code\u003e variable until weâ€™re inside the \u003ccode\u003eCanvas\u003c/code\u003e draw block. Then we transform the coordinates into UI coordinates.\u003c/li\u003e\u003cli id=\"3267\"\u003eWe iterate over the detected faces, and for each face, we draw a radial gradient that will make the inside of the face rectangle transparent.\u003c/li\u003e\u003cli id=\"5eb5\"\u003eWe use \u003ccode\u003eBlendMode.DstOut\u003c/code\u003e to make sure that we are cutting out the gradient from the pink rectangle, creating the spotlight effect.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"9cf1\"\u003e\u003cem\u003eNote: When you change the camera to \u003c/em\u003e\u003ccode\u003e\u003cem\u003eDEFAULT_FRONT_CAMERA\u003c/em\u003e\u003c/code\u003e\u003cem\u003e you will notice that the spotlight is mirrored! This is a known issue, tracked in the \u003c/em\u003e\u003ca href=\"https://issuetracker.google.com/390643162\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eGoogle Issue Tracker\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003ch2 id=\"a0ec\"\u003eResult\u003c/h2\u003e\u003cp id=\"3a9b\"\u003eWith this code, we have a fully functional spotlight effect that highlights detected faces. You can find the full code snippet \u003ca href=\"https://gist.github.com/JolandaVerhoef/74d4696b804736c698450bd34b5c9ff8#file-3_spotlight_effect-kt\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"50f2\"\u003eThis effect is just the beginning â€” by using the power of Compose, you can create a myriad of visually stunning camera experiences. Being able to transform sensor and buffer coordinates into Compose UI coordinates and back means we can utilize all Compose UI features and integrate them seamlessly with the underlying camera system. With animations, advanced UI graphics, simple UI state management, and full gesture control, your imagination is the limit!\u003c/p\u003e\u003cp id=\"d80c\"\u003eIn the final post of the series, weâ€™ll dive into how to use adaptive APIs and the Compose animation framework to seamlessly transition between different camera UIs on foldable devices. Stay tuned!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-01-23T17:03:20.14Z",
  "modifiedTime": null
}
