{
  "id": "14531ed0-4537-4448-82ff-9bf5c447840e",
  "title": "Better Android Testing at Airbnb, Part 6",
  "link": "https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-a11f6832773f?source=rss----53c7c27702d5--android",
  "description": "",
  "author": "Eli Hart",
  "published": "Fri, 27 Dec 2019 12:54:17 GMT",
  "source": "https://medium.com/feed/airbnb-engineering/tagged/android",
  "categories": [
    "testing",
    "mobile",
    "android"
  ],
  "byline": "Eli Hart",
  "length": 13724,
  "excerpt": "In the sixth part of our series on Android Testing at Airbnb, we look at common sources of flakiness and how they can be mitigated.",
  "siteName": "The Airbnb Tech Blog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "In the sixth part of our series on Android Testing at Airbnb, we look at common sources of flakiness and how they can be mitigated.In the previous article we detailed our test framework implementation. This was a high level look at the approach we take to running our tests. However, many small details have been added to the framework in order to make it as stable as possible. This article details the most important ones.Obstacles to Consistent MockingA constant battle with our test framework is minimizing sources of flakiness. These can manifest in many different ways, and lead to things like slight screenshot variances, differing fragment states, or spurious crashes.Flakiness issues are compounded because of our use of Flank, which dynamically assigns tests into shards. This means that test ordering is constantly changing. One test may put the app in a certain state that affects a later test, but this won’t be consistent because of the unpredictability of the test ordering.Using Android’s Test Orchestrator can help prevent some of these issues, but unfortunately we can’t currently use it because it makes our tests take seven times as long. Instead, we take the approach of manually clearing shared state between tests, and taking pains to prevent memory leaks which might lead to crashes after many tests are run.Below we present some of the flakiness issues we have run into, and how we have resolved them. In general, we’ve found it necessary to have all product features use the same patterns and architecture so that sources of flakiness can be fixed once in the underlying tools, enabling a scalable solution. This may require providing a wrapper around vanilla API’s to enforce how they are used or mocked.Async CodeThere are many ways to execute code asynchronously, and an engineer may choose to do something unpredictable in their feature. For example, common tools for asynchronous code are RxJava, Kotlin coroutines, AsyncTasks, Executors, or even manually creating and managing Threads. Any custom approaches are impossible to be controlled by the test framework. Often this is ok because the Fragment UI should be frozen to mocked State that can’t be changed, but occasionally asynchronous code can cause unexpected side effects.To minimize the chance of unexpected side effects, we provide our own functions as access points to executing asynchronous code and encourage engineers to use those. This allows the test framework to either block the code from running, or detect when it is finished.For example, MvRx uses an extension function named “execute” to subscribe to RxJava Observables. The MvRx mocking system allows the behavior of “execute” to be changed so that the observable is never subscribed to.Additionally, we use dependency injection to inject a test Coroutine Scope into our view models so that coroutines are never actually run.In both these cases we can report that an Observable or Coroutine was executed in response to a click, so that corresponding details are shown in the final interaction report.Cached State in ViewsThe Android View class often caches data in order to improve performance. For example, a draw call may do nothing if the view does not consider itself “dirty,” and measurement details can be cached so that subsequent measure passes don’t have to recalculate anything if the view has not changed. These caches can be problematic in screenshot testing when we need to force layout the Activity to show an entire RecyclerView and then draw everything to a custom Canvas. For example, ConstraintLayout has a measure cache that was causing our forced screenshot layout to not display as expected.Our solution was to iterate through the view hierarchy and call invalidate() and requestLayout() on all views. This guarantees that they layout correctly for our screenshot, and that they completely draw themselves to our custom canvas.Shared PreferencesIf one mock changes shared preferences it can affect a later mock. Android Test Orchestrator can’t solve this problem either because our framework runs multiple mocks in a single test. The solution is straightforward, have the test framework clear Shared Preferences after each mock. This can be extended to any other type of storage such as cache, local files, or databases.DatesFully mocking dates is crucial to reducing tests flakiness. Dates are inherently flaky as UI code commonly references the current time. As tests are rerun with ever changing Date values, screenshots can show text that is constantly being updated, and interaction reports can also be affected if any arguments includes Dates.The only solution here is to be able to mock the Date framework so that calls to get the current date and time return a consistent mocked value. At Airbnb we use JodaTime, wrapped around a custom internal API that hides the JodaTime implementation details. This allows us to intercept and mock any calls to now() or today().DrawablesA recurring problem for us was screenshot differences due to slight pixel variations in icons that were loaded from drawable resources. For performance reasons, Android maintains a cache of drawables loaded from resources, and the underlying bitmap can be shared in multiple locations. Due to our changing test ordering we didn’t have consistency in which tests encountered already cached bitmaps, and the cached version could vary.In one case the issue was caused by code that modified the drawable’s Bitmap — changing it to call mutate on the drawable first prevented the cached version from being changed and fixed the flakiness. While you should take care to call mutate on shared Bitmaps for this reason, we weren’t able to use this approach to fix all of our issues with drawable flakiness.We were able to solve the flakiness by forcing the cache to be cleared after each screenshot. There is not a clearly exposed API to do this, so we use an approach that takes advantage of the fact that the cache is cleared on incompatible configuration changes, which we can force like this:Note that this approach isn’t ideal, it uses restricted and deprecated API’s, and relies on understanding implementation details of what these functions do. We only test on API 28 right now and it is working great for our needs at the moment, but may need to be adapted in the future.Out Of Memory ExceptionsThere are two potential sources for these crashes:Memory leaks in the code under testInefficient management of bitmaps in the screenshotting process.To minimize memory leaks, we keep the target duration of each test shard to three minutes, to minimize how many tests are run in a single process. Additionally, we run leak canary to detect and report leaks.In the screenshotting library we may have to capture bitmaps up to 40,000 pixels long. In order to manage this, we reuse the same bitmap across all screenshots, and recycle and create a new, larger one if needed. We also run the app with large heap enabled in the manifest.Delayed RunnablesWe found that features occasionally executed Runnable callbacks with Handler#postDelay. A common use case for this was to emphasize some UI animation, such as waiting to finish an activity after a confirmation message is shown. While we are able to detect when the main thread is idle (as discussed previously), this idle detection cannot be applied to Runnables that are posted to run later, so we have no way to account for the delayed code.In this case, we have feature code call a wrapper function when they need to postDelay a callback. This wrapper executes the runnable immediately when the test framework is active. Another benefit to this wrapper approach is that we have provided a function that is more idiomatically Kotlin, such as Fragment.post(delayMs: Number = 0, callback: () -\u003e Unit).Note that in our State based architecture, ideally the UI does not execute arbitrary actions that are not tracked in the State. However, sometimes this is needed for animations, and is the simplest way to implement a basic UI behavior. This is fine as long as the UI is robust enough to recover from configuration changes that may interrupt the posted callback.Non-Mocked StateFor screen behavior to be deterministic and controllable the screen should only use data from the ViewModel’s State. At first glance this seems straightforward, but we occasionally have cases where it is violated. This can happen if the Fragment has any injected dependencies that it references, static method calls that may access a singleton, or OS level calls that may be variable, such as getting device Locale.In our app we still have some legacy systems that are accessed via static methods instead of dependency injection, and those were commonly accessed incorrectly. Besides adding the possibility of test flakiness, abusing the State pattern like this also means that the test framework is not testing variations in the missed data. Lint rules can help to prevent anti-pattern usage like this, and good project architecture with dependency injection can enforce best practices.Image LoadingUI that displays images must load those bitmaps asynchronously, which is problematic for testing. While loading images is ideal because it fully tests the image loading code and behavior, it is troublesome for several reasons:The test framework must be able to detect when all images have loaded.Image loading can have several states, such as a loading placeholder, incremental thumbnail, failure asset, and the final success state. We can’t easily test all of these separately, or easily differentiate between them.If images are loaded from network then there is the possibility for sporadic failure due to network issues, which results in flakiness.Waiting for images to load increases test time.Even if we allow images to load, when a screenshot is taken our test framework synchronously lays out the full activity including all RecyclerView items, which may not have originally been laid out. In this case we can’t wait for images in newly laid out views to be loaded since the screenshot process happens synchronously.In our app, we compromise by overriding any image load requests and instead forcibly inserting a local drawable resource, which is loaded synchronously. This has the following benefits:Shows an actual image in screenshots instead of a blank spotCovers some image load behavior, such as ImageView scaleTypeWorks synchronously so no complexity is needed to wait for image loadsWhile this solution is not perfect, it has been a good compromise for us. Additionally, we still allow screens to load different images in mocks by having a set of test urls that map to different local test assets. Mock state can choose which image urls to use to vary the type of image loaded in the screenshot, which helps to improve the quality of the screenshot.This approach is possible because we have a centralized, custom ImageView architecture which all features use. This allows the mocking to happen in a single place inside our image infrastructure, completely opaque to product engineers.Lastly, our JSON report captures the url that was set on the ImageView, so even though we don’t screenshot the resulting picture, we still test that the Fragment loaded the expected url.Webview MockingWebviews face similar problems to ImageViews. Network requests to load content are flaky and difficult to accurately wait for. Additionally, the content that is loaded is out of our control and could change the screenshot bitmap at any time.Consequently, we block all WebViews from loading content during our tests. This is accomplished by wrapping the Android WebView in our own custom view so we can mock it out in one central place.And again, like ImageViews, our JSON report captures the intended url to load, as well as other data such as user agent, headers, and request type. This helps to validate even more data than a screenshot could have.RecyclerView PrefetchingBy default, RecyclerView’s LinearLayoutManager prefetches off screen views outside its viewport while the UI thread is idle between frames. We found that this can cause flakiness, because if a view is fully laid out by the system it can behave differently than if it was laid out synchronously by our screenshotting system. Notably, animations will be completed to 100% in the first case, but will only be in the starting state in the second.Also in general, we find it is best to disable systems that don’t deterministically behave the same way. In this case the prefetching may or may not happen depending on how long we idle for before taking a screenshot.Instead, we disable this behavior; after the test activity lays out a fragment it traverses the view hierarchy for RecyclerView instances and disables prefetching on all LayoutManagers via setItemPrefetchEnabled(false).Next: CI SetupThis article presented a collection of common sources of flakiness and how we try to stomp them out at the roots in our test framework.Next, in our final article, we will cover how our tests are automatically generated and run on CI when an engineer makes a code change.Series IndexThis is a seven part article series on testing at Airbnb.Part 1 — Testing Philosophy and a Mocking SystemPart 2 — Screenshot Testing with MvRx and HappoPart 3 — Automated Interaction TestingPart 4 — A Framework for Unit Testing ViewModelsPart 5 — Architecture of our Automated Testing FrameworkPart 6 (This article)— Obstacles to Consistent MockingPart 7 — Test Generation and CI ConfigurationWe’re Hiring!Want to work with us on these and other Android projects at scale? Airbnb is hiring for several Android engineer positions across the company! See https://careers.airbnb.com for current openings.",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*-ph_0TfXeEiE1rh9keSmKQ.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca rel=\"noopener follow\" href=\"https://medium.com/@konakid?source=post_page-----a11f6832773f--------------------------------\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Eli Hart\" src=\"https://miro.medium.com/v2/resize:fill:88:88/2*qR91fuLzUz5PI59hjTTcRQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://medium.com/airbnb-engineering?source=post_page-----a11f6832773f--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"The Airbnb Tech Blog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*MlNQKg-sieBGW5prWoe9HQ.jpeg\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"c41f\"\u003e\u003cem\u003eIn the sixth part of our series on Android Testing at Airbnb, we look at common sources of flakiness and how they can be mitigated.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"e827\"\u003eIn \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-661a554a8c8b\"\u003ethe previous article\u003c/a\u003e we detailed our test framework implementation. This was a high level look at the approach we take to running our tests. However, many small details have been added to the framework in order to make it as stable as possible. This article details the most important ones.\u003c/p\u003e\u003ch2 id=\"6b9d\"\u003eObstacles to Consistent Mocking\u003c/h2\u003e\u003cp id=\"e6a5\"\u003eA constant battle with our test framework is minimizing sources of flakiness. These can manifest in many different ways, and lead to things like slight screenshot variances, differing fragment states, or spurious crashes.\u003c/p\u003e\u003cp id=\"140f\"\u003eFlakiness issues are compounded because of our use of Flank, which dynamically assigns tests into shards. This means that test ordering is constantly changing. One test may put the app in a certain state that affects a later test, but this won’t be consistent because of the unpredictability of the test ordering.\u003c/p\u003e\u003cp id=\"b78d\"\u003eUsing Android’s Test Orchestrator can help prevent some of these issues, but unfortunately we can’t currently use it because it makes our tests take seven times as long. Instead, we take the approach of manually clearing shared state between tests, and taking pains to prevent memory leaks which might lead to crashes after many tests are run.\u003c/p\u003e\u003cp id=\"330b\"\u003eBelow we present some of the flakiness issues we have run into, and how we have resolved them. In general, we’ve found it necessary to have all product features use the same patterns and architecture so that sources of flakiness can be fixed once in the underlying tools, enabling a scalable solution. This may require providing a wrapper around vanilla API’s to enforce how they are used or mocked.\u003c/p\u003e\u003ch2 id=\"a789\"\u003eAsync Code\u003c/h2\u003e\u003cp id=\"bb9c\"\u003eThere are many ways to execute code asynchronously, and an engineer may choose to do something unpredictable in their feature. For example, common tools for asynchronous code are RxJava, Kotlin coroutines, AsyncTasks, Executors, or even manually creating and managing Threads. Any custom approaches are impossible to be controlled by the test framework. Often this is ok because the Fragment UI should be frozen to mocked State that can’t be changed, but occasionally asynchronous code can cause unexpected side effects.\u003c/p\u003e\u003cp id=\"9def\"\u003eTo minimize the chance of unexpected side effects, we provide our own functions as access points to executing asynchronous code and encourage engineers to use those. This allows the test framework to either block the code from running, or detect when it is finished.\u003c/p\u003e\u003cp id=\"d16a\"\u003eFor example, MvRx uses an extension function named “\u003cstrong\u003e\u003cem\u003eexecute\u003c/em\u003e\u003c/strong\u003e” to subscribe to RxJava Observables. The MvRx mocking system allows the behavior of “\u003cstrong\u003e\u003cem\u003eexecute\u003c/em\u003e\u003c/strong\u003e” to be changed so that the observable is never subscribed to.\u003c/p\u003e\u003cp id=\"a38a\"\u003eAdditionally, we use dependency injection to inject a test Coroutine Scope into our view models so that coroutines are never actually run.\u003c/p\u003e\u003cp id=\"d5a4\"\u003eIn both these cases we can report that an Observable or Coroutine was executed in response to a click, so that corresponding details are shown in the final interaction report.\u003c/p\u003e\u003ch2 id=\"5be0\"\u003eCached State in Views\u003c/h2\u003e\u003cp id=\"240a\"\u003eThe Android View class often caches data in order to improve performance. For example, a draw call may do nothing if the view does not consider itself “dirty,” and measurement details can be cached so that subsequent measure passes don’t have to recalculate anything if the view has not changed. These caches can be problematic in screenshot testing when we need to force layout the Activity to show an entire RecyclerView and then draw everything to a custom Canvas. For example, ConstraintLayout has a measure cache that was causing our forced screenshot layout to not display as expected.\u003c/p\u003e\u003cp id=\"cd33\"\u003eOur solution was to iterate through the view hierarchy and call \u003cstrong\u003e\u003cem\u003einvalidate()\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003erequestLayout()\u003c/em\u003e\u003c/strong\u003e on all views. This guarantees that they layout correctly for our screenshot, and that they completely draw themselves to our custom canvas.\u003c/p\u003e\u003ch2 id=\"2dc8\"\u003eShared Preferences\u003c/h2\u003e\u003cp id=\"44eb\"\u003eIf one mock changes shared preferences it can affect a later mock. Android Test Orchestrator can’t solve this problem either because our framework runs multiple mocks in a single test. The solution is straightforward, have the test framework clear Shared Preferences after each mock. This can be extended to any other type of storage such as cache, local files, or databases.\u003c/p\u003e\u003ch2 id=\"1fe2\"\u003eDates\u003c/h2\u003e\u003cp id=\"161b\"\u003eFully mocking dates is crucial to reducing tests flakiness. Dates are inherently flaky as UI code commonly references the current time. As tests are rerun with ever changing Date values, screenshots can show text that is constantly being updated, and interaction reports can also be affected if any arguments includes Dates.\u003c/p\u003e\u003cp id=\"2409\"\u003eThe only solution here is to be able to mock the Date framework so that calls to get the current date and time return a consistent mocked value. At Airbnb we use JodaTime, wrapped around a custom internal API that hides the JodaTime implementation details. This allows us to intercept and mock any calls to \u003cstrong\u003e\u003cem\u003enow()\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003etoday()\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\u003ch2 id=\"42d1\"\u003eDrawables\u003c/h2\u003e\u003cp id=\"7cb0\"\u003eA recurring problem for us was screenshot differences due to slight pixel variations in icons that were loaded from drawable resources. For performance reasons, Android maintains a cache of drawables loaded from resources, and the underlying bitmap can be shared in multiple locations. Due to our changing test ordering we didn’t have consistency in which tests encountered already cached bitmaps, and the cached version could vary.\u003c/p\u003e\u003cp id=\"b337\"\u003eIn one case the issue was caused by code that modified the drawable’s Bitmap — changing it to call mutate on the drawable first prevented the cached version from being changed and fixed the flakiness. While you should take care to call mutate on shared Bitmaps for this reason, we weren’t able to use this approach to fix all of our issues with drawable flakiness.\u003c/p\u003e\u003cp id=\"429f\"\u003eWe were able to solve the flakiness by forcing the cache to be cleared after each screenshot. There is not a clearly exposed API to do this, so we use an approach that takes advantage of the fact that the cache is cleared on incompatible configuration changes, which we can force like this:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"a775\"\u003eNote that this approach isn’t ideal, it uses restricted and deprecated API’s, and relies on understanding implementation details of what these functions do. We only test on API 28 right now and it is working great for our needs at the moment, but may need to be adapted in the future.\u003c/p\u003e\u003ch2 id=\"35bb\"\u003eOut Of Memory Exceptions\u003c/h2\u003e\u003cp id=\"7c87\"\u003eThere are two potential sources for these crashes:\u003c/p\u003e\u003col\u003e\u003cli id=\"bb95\"\u003eMemory leaks in the code under test\u003c/li\u003e\u003cli id=\"6969\"\u003eInefficient management of bitmaps in the screenshotting process.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"4f9e\"\u003eTo minimize memory leaks, we keep the target duration of each test shard to three minutes, to minimize how many tests are run in a single process. Additionally, we run leak canary to detect and report leaks.\u003c/p\u003e\u003cp id=\"a3b3\"\u003eIn the screenshotting library we may have to capture bitmaps up to 40,000 pixels long. In order to manage this, we reuse the same bitmap across all screenshots, and recycle and create a new, larger one if needed. We also run the app with large heap enabled in the manifest.\u003c/p\u003e\u003ch2 id=\"a8e2\"\u003eDelayed Runnables\u003c/h2\u003e\u003cp id=\"b33e\"\u003eWe found that features occasionally executed Runnable callbacks with \u003cstrong\u003e\u003cem\u003eHandler#postDelay\u003c/em\u003e\u003c/strong\u003e. A common use case for this was to emphasize some UI animation, such as waiting to finish an activity after a confirmation message is shown. While we are able to detect when the main thread is idle (as discussed previously), this idle detection cannot be applied to Runnables that are posted to run later, so we have no way to account for the delayed code.\u003c/p\u003e\u003cp id=\"3d8f\"\u003eIn this case, we have feature code call a wrapper function when they need to postDelay a callback. This wrapper executes the runnable immediately when the test framework is active. Another benefit to this wrapper approach is that we have provided a function that is more idiomatically Kotlin, such as \u003cstrong\u003e\u003cem\u003eFragment.post(delayMs: Number = 0, callback: () -\u0026gt; Unit)\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\u003cp id=\"f2d0\"\u003eNote that in our State based architecture, ideally the UI does not execute arbitrary actions that are not tracked in the State. However, sometimes this is needed for animations, and is the simplest way to implement a basic UI behavior. This is fine as long as the UI is robust enough to recover from configuration changes that may interrupt the posted callback.\u003c/p\u003e\u003ch2 id=\"2db7\"\u003eNon-Mocked State\u003c/h2\u003e\u003cp id=\"8867\"\u003eFor screen behavior to be deterministic and controllable the screen should only use data from the ViewModel’s State. At first glance this seems straightforward, but we occasionally have cases where it is violated. This can happen if the Fragment has any injected dependencies that it references, static method calls that may access a singleton, or OS level calls that may be variable, such as getting device Locale.\u003c/p\u003e\u003cp id=\"92f8\"\u003eIn our app we still have some legacy systems that are accessed via static methods instead of dependency injection, and those were commonly accessed incorrectly. Besides adding the possibility of test flakiness, abusing the State pattern like this also means that the test framework is not testing variations in the missed data. Lint rules can help to prevent anti-pattern usage like this, and good project architecture with dependency injection can enforce best practices.\u003c/p\u003e\u003ch2 id=\"bb06\"\u003eImage Loading\u003c/h2\u003e\u003cp id=\"e704\"\u003eUI that displays images must load those bitmaps asynchronously, which is problematic for testing. While loading images is ideal because it fully tests the image loading code and behavior, it is troublesome for several reasons:\u003c/p\u003e\u003cul\u003e\u003cli id=\"de76\"\u003eThe test framework must be able to detect when all images have loaded.\u003c/li\u003e\u003cli id=\"f5d4\"\u003eImage loading can have several states, such as a loading placeholder, incremental thumbnail, failure asset, and the final success state. We can’t easily test all of these separately, or easily differentiate between them.\u003c/li\u003e\u003cli id=\"63e7\"\u003eIf images are loaded from network then there is the possibility for sporadic failure due to network issues, which results in flakiness.\u003c/li\u003e\u003cli id=\"9f53\"\u003eWaiting for images to load increases test time.\u003c/li\u003e\u003cli id=\"a0cc\"\u003eEven if we allow images to load, when a screenshot is taken our test framework synchronously lays out the full activity including all RecyclerView items, which may not have originally been laid out. In this case we can’t wait for images in newly laid out views to be loaded since the screenshot process happens synchronously.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"c7a9\"\u003eIn our app, we compromise by overriding any image load requests and instead forcibly inserting a local drawable resource, which is loaded synchronously. This has the following benefits:\u003c/p\u003e\u003cul\u003e\u003cli id=\"7239\"\u003eShows an actual image in screenshots instead of a blank spot\u003c/li\u003e\u003cli id=\"f1bf\"\u003eCovers some image load behavior, such as ImageView scaleType\u003c/li\u003e\u003cli id=\"4c67\"\u003eWorks synchronously so no complexity is needed to wait for image loads\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"ce7f\"\u003eWhile this solution is not perfect, it has been a good compromise for us. Additionally, we still allow screens to load different images in mocks by having a set of test urls that map to different local test assets. Mock state can choose which image urls to use to vary the type of image loaded in the screenshot, which helps to improve the quality of the screenshot.\u003c/p\u003e\u003cp id=\"1af0\"\u003eThis approach is possible because we have a centralized, custom ImageView architecture which all features use. This allows the mocking to happen in a single place inside our image infrastructure, completely opaque to product engineers.\u003c/p\u003e\u003cp id=\"3b27\"\u003eLastly, our JSON report captures the url that was set on the ImageView, so even though we don’t screenshot the resulting picture, we still test that the Fragment loaded the expected url.\u003c/p\u003e\u003ch2 id=\"88fd\"\u003eWebview Mocking\u003c/h2\u003e\u003cp id=\"6aa7\"\u003eWebviews face similar problems to ImageViews. Network requests to load content are flaky and difficult to accurately wait for. Additionally, the content that is loaded is out of our control and could change the screenshot bitmap at any time.\u003c/p\u003e\u003cp id=\"0e48\"\u003eConsequently, we block all WebViews from loading content during our tests. This is accomplished by wrapping the Android WebView in our own custom view so we can mock it out in one central place.\u003c/p\u003e\u003cp id=\"2c18\"\u003eAnd again, like ImageViews, our JSON report captures the intended url to load, as well as other data such as user agent, headers, and request type. This helps to validate even more data than a screenshot could have.\u003c/p\u003e\u003ch2 id=\"19c5\"\u003eRecyclerView Prefetching\u003c/h2\u003e\u003cp id=\"e597\"\u003eBy default, RecyclerView’s LinearLayoutManager prefetches off screen views outside its viewport while the UI thread is idle between frames. We found that this can cause flakiness, because if a view is fully laid out by the system it can behave differently than if it was laid out synchronously by our screenshotting system. Notably, animations will be completed to 100% in the first case, but will only be in the starting state in the second.\u003c/p\u003e\u003cp id=\"5ae2\"\u003eAlso in general, we find it is best to disable systems that don’t deterministically behave the same way. In this case the prefetching may or may not happen depending on how long we idle for before taking a screenshot.\u003c/p\u003e\u003cp id=\"2c28\"\u003eInstead, we disable this behavior; after the test activity lays out a fragment it traverses the view hierarchy for RecyclerView instances and disables prefetching on all LayoutManagers via \u003cstrong\u003e\u003cem\u003esetItemPrefetchEnabled(false)\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\u003ch2 id=\"699c\"\u003eNext: CI Setup\u003c/h2\u003e\u003cp id=\"af91\"\u003eThis article presented a collection of common sources of flakiness and how we try to stomp them out at the roots in our test framework.\u003c/p\u003e\u003cp id=\"da44\"\u003eNext, \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-eacec3a8a72f\"\u003ein our final article\u003c/a\u003e, we will cover how our tests are automatically generated and run on CI when an engineer makes a code change.\u003c/p\u003e\u003ch2 id=\"8cc4\"\u003eSeries Index\u003c/h2\u003e\u003cp id=\"18cd\"\u003eThis is a seven part article series on testing at Airbnb.\u003c/p\u003e\u003cp id=\"c733\"\u003ePart 1 — \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-3f5b90b9c40a\"\u003eTesting Philosophy and a Mocking System\u003c/a\u003e\u003c/p\u003e\u003cp id=\"1e87\"\u003ePart 2 — \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-a77ac9531cab\"\u003eScreenshot Testing with MvRx and Happo\u003c/a\u003e\u003c/p\u003e\u003cp id=\"84b5\"\u003ePart 3 — \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-1d1e91e489b4\"\u003eAutomated Interaction Testing\u003c/a\u003e\u003c/p\u003e\u003cp id=\"2c2e\"\u003ePart 4 — \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-part-4-testing-viewmodels-550d929126c8\"\u003eA Framework for Unit Testing ViewModels\u003c/a\u003e\u003c/p\u003e\u003cp id=\"82e4\"\u003ePart 5 — \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-661a554a8c8b\"\u003eArchitecture of our Automated Testing Framework\u003c/a\u003e\u003c/p\u003e\u003cp id=\"c6c8\"\u003e\u003cstrong\u003ePart 6 (This article)\u003c/strong\u003e— \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-a11f6832773f\"\u003eObstacles to Consistent Mocking\u003c/a\u003e\u003c/p\u003e\u003cp id=\"461f\"\u003ePart 7 — \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-eacec3a8a72f\"\u003eTest Generation and CI Configuration\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"a56a\"\u003eWe’re Hiring!\u003c/h2\u003e\u003cp id=\"e896\"\u003eWant to work with us on these and other Android projects at scale? Airbnb is hiring for several Android engineer positions across the company! See \u003ca href=\"https://careers.airbnb.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://careers.airbnb.com\u003c/a\u003e for current openings.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2019-12-27T12:54:17.37Z",
  "modifiedTime": null
}
