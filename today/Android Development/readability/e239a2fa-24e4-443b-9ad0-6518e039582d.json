{
  "id": "e239a2fa-24e4-443b-9ad0-6518e039582d",
  "title": "Quick introduction to Large Language Models for Android developers",
  "link": "http://android-developers.googleblog.com/2024/09/introduction-to-large-language-models-for-android-developers.html",
  "description": "",
  "author": "Android Developers",
  "published": "2024-09-30T08:56:00.001-07:00",
  "source": "http://feeds.feedburner.com/blogspot/hsDu",
  "categories": [
    "#AndroidAI",
    "#Gemini",
    "#Gemma",
    "#GenerativeAI"
  ],
  "byline": "",
  "length": 7418,
  "excerpt": "Explore key concepts and understand the differences between traditional ML and generative AI. Discover resources to integrate gen AI into your apps.",
  "siteName": "Android Developers Blog",
  "favicon": "",
  "text": "Posted by Thomas Ezan, Sr Developer Relation Engineer Android has supported traditional machine learning models for years. Frameworks and SDKs like LiteRT (formerly known as TensorFlow Lite), ML Kit and MediaPipe enabled developers to easily implement tasks like image classification and object detection. In recent years, generative AI (gen AI) and large language models (LLMs), have opened up new possibilities for language understanding and text generation. We have lowered the barriers for integrating gen AI features into your apps and this blog post will provide you with the necessary high-level knowledge to get started. Before we dive into the specificities of generative AI models, let’s take a high level look: how is machine learning (ML) different from traditional programming. Machine learning as a new programming paradigm A key difference between traditional programming and ML lies in how solutions are implemented. In traditional programming, developers write explicit algorithms that take input and produce a desired output. Machine learning takes a different approach: developers provide a large set of previously collected input data and the corresponding output, and the ML model is trained to learn how to map the input to the output. Then, the model is deployed on the Cloud or on-device to process input data. This step is called inference. This paradigm enables developers to tackle problems that were previously difficult or impossible to solve with rule-based programming. Traditional machine learning vs. generative AI on Android Traditional ML on Android includes tasks such as image classification that can be implemented using mobilenet and LiteRT, or pose estimation that can be easily added to your Android app with the ML Kit SDK. These models are often trained on specific datasets and perform extremely well on well-defined, narrow tasks. Generative AI introduces the capability to understand inputs such as text, images, audio and video and generate human-like responses. This enables applications like chatbots, language translation, text summarization, image captioning, image or code generation, creative writing assistance, and much more. Most state of the art generative AI models like the Gemini models are built on the transformer architecture. To generate images, diffusion models are often used. Understanding large language models At its core, an LLM is a neural network model trained on massive amounts of text data. It learns patterns, grammar, and semantic relationships between words and phrases, enabling it to predict and generate text that mimics human language. As mentioned earlier, most recent LLMs use the transformer architecture. It breaks down input into tokens, assigns numerical representations called “embeddings” (see Key concepts below) to these tokens, and then processes these embeddings through multiple layers of the neural network to understand the context and meaning. LLMs typically go through two main phases of training: 1. Pre-training phase: The model is exposed to vast amounts of text from different sources to learn general language patterns and knowledge. 2. Fine-tuning phase: The model is trained on specific tasks and datasets to refine its performance for particular applications. Classes of models and their capabilities. Gen AI models come in various sizes, from smaller models like Gemini Nano or Gemma 2 2B, to massive models like Gemini 1.5 Pro that run on Google Cloud. The size of a model generally correlates with the capabilities and compute power required to run it. Models are constantly evolving, with new research pushing the boundaries of their capabilities. These models are being evaluated on tasks like question answering, code generation, and creative writing, demonstrating impressive results. In addition some models are multimodal which means that they are designed to process and understand information from multiple modalities, such as images, audio, and video, alongside text. This allows them to tackle a wider range of tasks, including image captioning, visual question answering, audio transcription. Multiple Google Generative AI models such as Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini Nano with Multimodality and PaliGemma are multimodal. Key concepts Context Window Context window refers to the amount of tokens (converted from text, image, audio or video) the model considers when generating a response. For chat use cases, it includes both the current input and a history of past interactions. For reference, 100 tokens is equal to about 60-80 English words.For reference, Gemini 1.5 Pro currently supports 2M input tokens. It is enough to fit the seven Harry Potter books… and more! Embeddings Embeddings are multidimensional numerical representations of tokens that accurately encode their semantic meaning and relationships within a given vector space. Words with similar meanings are closer together, while words with opposite meanings are farther apart. The embedding process is a key component of an LLM. You can try it independently using MediaPipe Text Embedder for Android. It can be used to identify relations between words and sentences and implement a simplified semantic search directly on-device. A (very) simplified representation of the embeddings for the words “king”, “queen”, “man” and “woman” Top-K, Top-P and Temperature Parameters like Top-K, Top-P and Temperature enable you to control the creativity of the model and the randomness of its output. Top-K filters tokens for output. For example a Top-K of 3 keeps the three most probable tokens. Increasing the Top-K value will increase the randomness of the model response (learn about Top-K parameter). Then, defining the Top-P value adds another step of filtering. Tokens with the highest probabilities are selected until their sum equals the Top-P value. Lower Top-P values result in less random responses, and higher values result in more random responses (learn about Top-P parameter). Finally, the Temperature defines the randomness to select the tokens left. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results (learn about Temperature). Fine-tuning Iterating over several versions of a prompt to achieve an optimal response from the model for your use-case isn’t always enough. The next step is to fine-tune the model by re-training it with data specific to your use-case. You will then obtain a model customized to your application. More specifically, Low rank adaptation (LoRA) is a fine-tuning technique that makes LLM training much faster and more memory-efficient while maintaining the quality of the model outputs. The process to fine-tune open models via LoRA is well documented. See, for example, how you can fine-tune Gemini models through Google AI Studio without advanced ML expertise. You can also fine-tune Gemma models using the KerasNLP library. The future of generative AI on Android With ongoing research and optimization of LLMs for mobile devices, we can expect even more innovative gen AI enabled features coming to Android soon. In the meantime check out other AI on Android Spotlight Week blog posts, and go to the Android AI documentation to learn more about how to power your apps with gen AI capabilities!",
  "image": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWNLcken_pDkd2yzqcQX0QN7lGnMv-Pv1sAUMwFrkqFdA3HuVU-NshHGL87nn1eQtrFmEl_QWmUbAitr_NECTBafiTG5UVgkt26S2gdSLlaZw4bfAPFOmamZpx-uk73Lah66nnSPgD4GIZdI2baBMACEAhx2H82QwkJMLBsuIDM30yc49GgIkJLwmka7U/w1200-h630-p-k-no-nu/Large-Language-Models-Android-Social.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cmeta content=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWNLcken_pDkd2yzqcQX0QN7lGnMv-Pv1sAUMwFrkqFdA3HuVU-NshHGL87nn1eQtrFmEl_QWmUbAitr_NECTBafiTG5UVgkt26S2gdSLlaZw4bfAPFOmamZpx-uk73Lah66nnSPgD4GIZdI2baBMACEAhx2H82QwkJMLBsuIDM30yc49GgIkJLwmka7U/s1600/Large-Language-Models-Android-Social.png\" name=\"twitter:image\"/\u003e\n\u003cp\u003e\n\n\u003cem\u003ePosted by \u003ca href=\"https://x.com/lethargicpanda\" target=\"_blank\"\u003eThomas Ezan\u003c/a\u003e, Sr Developer Relation Engineer\u003c/em\u003e\n\n\u003ca href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibeg9V-GcmyYOgDd0_s1aWSvUvjjWc3ikvLjHIh1EtLVga07Bs_OAoalp9WUjPUmZBpme7CASLyJavtYzlHKTzyVyzMeB37oFepayAJJeAnfO0CmJAxLXbYdNKa6lWhyOf1UD2cehnVJ9IvKuA4UXYSp8Z9ULvYMqdVwXtuFJzkOhEEpqQzTCCAGupF28/s1600/Large-Language-Models-Android.png\"\u003e\u003cimg data-original-height=\"800\" data-original-width=\"100%\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibeg9V-GcmyYOgDd0_s1aWSvUvjjWc3ikvLjHIh1EtLVga07Bs_OAoalp9WUjPUmZBpme7CASLyJavtYzlHKTzyVyzMeB37oFepayAJJeAnfO0CmJAxLXbYdNKa6lWhyOf1UD2cehnVJ9IvKuA4UXYSp8Z9ULvYMqdVwXtuFJzkOhEEpqQzTCCAGupF28/s1600/Large-Language-Models-Android.png\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eAndroid has supported traditional machine learning models for years. Frameworks and SDKs like \u003ca href=\"https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/\" target=\"_blank\"\u003eLiteRT (formerly known as TensorFlow Lite)\u003c/a\u003e, ML Kit and MediaPipe enabled developers to easily implement tasks like image classification and object detection.\u003c/p\u003e\n\n\u003cp\u003eIn recent years, generative AI (gen AI) and large language models (LLMs), have opened up new possibilities for language understanding and text generation. We have lowered the barriers for integrating gen AI features into your apps and this blog post will provide you with the necessary high-level knowledge to get started.\u003c/p\u003e\n\n\u003cp\u003eBefore we dive into the specificities of generative AI models, let’s take a high level look: how is machine learning (ML) different from traditional programming.\u003c/p\u003e\u003cbr/\u003e\n\n\u003ch3\u003eMachine learning as a new  programming paradigm\u003c/h3\u003e\n\n\u003cp\u003eA key difference between traditional programming and ML lies in how solutions are implemented.\u003c/p\u003e\n\n\u003cp\u003eIn traditional programming, developers write explicit algorithms that take input and produce a desired output.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"A flow chart showing the process of machine learning model training. Input data is fed into the training process, resulting in a trained ML model\" height=\"209\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIj0QJIrqXi8_gDtyTmZRqG0n1eiJ4pRA7ka9-JyVz98J2pf-Bh92izs5Sc1cQeGWyZ1mfalX2zSsMtEAJVS1Be2KYLB3DdRhDmAvwQZ9ugAHazXgOp7J64sWxpfCCnTn-Gr-YbUMIKz9Y5U1Sluq7zRiMez3RTDNWMdPXCyrkmUATrhCno0f5jG68svc/w640-h209/Traditional-programming-illustration.png\" width=\"640\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eMachine learning takes a different approach: developers provide a large set of previously collected input data and the corresponding output, and the ML model is trained to learn how to map the input to the output.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"A flow chart illustrating the machine learning model training. This step is labeled above the process \u0026#39;1. Train the model with a large set of input and output data\u0026#39;. Below, arrows labeled \u0026#39;Input\u0026#39; and \u0026#39;Output\u0026#39; point to a green box labeled \u0026#39;ML Model Training\u0026#39;.  Another arrow points away from the box and is labeled \u0026#39;ML Model\u0026#39;.\" height=\"300\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKPF3gxmuXaAemKtHkHVj7DbCWQ-Z-xaXxTI5sLJ2yH3sKLwNOyFKeVknwmRpvZ5J7I3Si00lqeUGyCKugO978hUjonA_U30fWtXItCzlzrh-H4CpUPHPGESAUnkeBOy5eVNEzOder47139HTINv3RJZC-kSQIfaS4EyMEVBnrwcJAaDhX2RKbJx-yquo/w640-h300/ML-Training-model-Illustration%20(1).png\" width=\"640\"/\u003e\u003c/p\u003e\n\n\n\u003cp\u003eThen, the model is deployed on the Cloud or on-device to process input data. This step is called \u003ci\u003einference\u003c/i\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"A flow chart illustrating the inference training for training an ML model. This step is labeled above the process \u0026#39;2. Deploy the model to run inferences on input data\u0026#39;. Below, an arrow labeled \u0026#39;Input\u0026#39; points to a green box labeled \u0026#39;Run ML Inference\u0026#39;.  Another arrow points away from the box and is labeled \u0026#39;Output\u0026#39;.\" height=\"266\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjY1En9Gux62C1Ukxsl2mlRS5eT3sSPZZkkLGB3wfDZAIic0TSK2R8M-eAQzz5yaCdpL5iE1yDiXQD2oBnXsp3o81Y668f8_l-1vMKyCx-JDxqS-NJvUI2xXWnvgft37uJGey1kvcoCdWTtRuCKlULkJ4J7Hj1fDeKRpizBuoXC8PDx9BEtTRFtjFntsfA/w640-h266/ML-inference-model-Illustration.png\" width=\"640\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThis paradigm enables developers to tackle problems that were previously difficult or impossible to solve with rule-based programming.\u003c/p\u003e\u003cbr/\u003e\n\n\u003ch3\u003eTraditional machine learning vs. generative AI on Android\u003c/h3\u003e\n\n\u003cp\u003eTraditional ML on Android includes tasks such as image classification that can be implemented using \u003ca href=\"https://www.kaggle.com/models/tensorflow/mobilenet-v2/\" target=\"_blank\"\u003emobilenet\u003c/a\u003e and \u003ca href=\"https://ai.google.dev/edge/litert\" target=\"_blank\"\u003eLiteRT\u003c/a\u003e, or pose estimation that can be easily added to your Android app with the \u003ca href=\"https://developers.google.com/ml-kit/vision/pose-detection\" target=\"_blank\"\u003eML Kit SDK\u003c/a\u003e. These models are often trained on specific datasets and perform extremely well on well-defined, narrow tasks.\u003c/p\u003e\n\n\u003cp\u003eGenerative AI introduces the capability to understand inputs such as text, images, audio and video and generate human-like responses. This enables applications like chatbots, language translation, text summarization, image captioning, image or code generation, creative writing assistance, and much more.\u003c/p\u003e\n\n\u003cp\u003eMost state of the art generative AI models like the \u003ca href=\"https://ai.google.dev/gemini-api/docs/models/gemini\" target=\"_blank\"\u003eGemini models\u003c/a\u003e are built on the \u003ca href=\"https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29\" target=\"_blank\"\u003etransformer architecture\u003c/a\u003e. To generate images, \u003ca href=\"https://en.wikipedia.org/wiki/Diffusion_model\" target=\"_blank\"\u003ediffusion models\u003c/a\u003e are often used.\u003c/p\u003e\u003cbr/\u003e\n\n\u003ch3\u003eUnderstanding large language models\u003c/h3\u003e\n\n\u003cp\u003eAt its core, an LLM is a \u003ca href=\"https://en.wikipedia.org/wiki/Neural_network_%28machine_learning%29\" target=\"_blank\"\u003eneural network model\u003c/a\u003e trained on massive amounts of text data. It learns patterns, grammar, and semantic relationships between words and phrases, enabling it to predict and generate text that mimics human language.\u003c/p\u003e\n\n\u003cp\u003eAs mentioned earlier, most recent LLMs use the transformer architecture. It breaks down input into tokens, assigns numerical representations called “embeddings” (see \u003ca href=\"#Key-concepts\"\u003eKey concepts\u003c/a\u003e below) to these tokens, and then processes these embeddings through multiple layers of the neural network to understand the context and meaning.\u003c/p\u003e\n\n\u003cp\u003eLLMs typically go through two main phases of training:\u003c/p\u003e\n\u003cul\u003e\u003cul\u003e\n\u003cp\u003e1. Pre-training phase: The model is exposed to vast amounts of text from different sources to learn general language patterns and knowledge.\u003c/p\u003e\u003c/ul\u003e\u003cul\u003e\n\u003cp\u003e2. Fine-tuning phase: The model is trained on specific tasks and datasets to refine its performance for particular applications.\u003c/p\u003e\n\u003c/ul\u003e\u003c/ul\u003e\u003cbr/\u003e\n\n  \n\u003ch3\u003eClasses of models and their capabilities.\u003c/h3\u003e\n\n\u003cp\u003eGen AI models come in various sizes, from smaller models like \u003ca href=\"https://developer.android.com/ai/aicore\" target=\"_blank\"\u003eGemini Nano\u003c/a\u003e or \u003ca href=\"https://ai.google.dev/gemma/docs\" target=\"_blank\"\u003eGemma 2 2B\u003c/a\u003e, to massive models like \u003ca href=\"https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-pro\" target=\"_blank\"\u003eGemini 1.5 Pro\u003c/a\u003e that run on Google Cloud. The size of a model generally correlates with the capabilities and compute power required to run it.\u003c/p\u003e\n\n\u003cp\u003eModels are constantly evolving, with new research pushing the boundaries of their capabilities. These models are being evaluated on tasks like question answering, code generation, and creative writing, demonstrating impressive results.\u003c/p\u003e\n\n\u003cp\u003eIn addition some models are multimodal which means that they are designed to process and understand information from multiple modalities, such as images, audio, and video, alongside text. This allows them to tackle a wider range of tasks, including image captioning, visual question answering, audio transcription. Multiple Google Generative AI models such as \u003ca href=\"https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-flash\" target=\"_blank\"\u003eGemini 1.5 Flash\u003c/a\u003e, \u003ca href=\"https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-pro\" target=\"_blank\"\u003eGemini 1.5 Pro\u003c/a\u003e, \u003ca href=\"https://store.google.com/intl/en/ideas/articles/gemini-nano-google-pixel/\" target=\"_blank\"\u003eGemini Nano with Multimodality\u003c/a\u003e and \u003ca href=\"https://ai.google.dev/gemma/docs/paligemma\" target=\"_blank\"\u003ePaliGemma\u003c/a\u003e are multimodal.\u003c/p\u003e\u003cbr/\u003e\n\n\u003ch3\u003eKey concepts\u003c/h3\u003e\n\n\u003ch4\u003e\u003cspan\u003e\u003cb\u003eContext Window\u003c/b\u003e\u003c/span\u003e\u003c/h4\u003e\n\n\u003cp\u003eContext window refers to the amount of tokens (converted from text, image, audio or video) the model considers when generating a response. For chat use cases, it includes both the current input and a history of past interactions. For reference, 100 tokens is equal to about 60-80 English words.For reference, \u003ca href=\"https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-pro\" target=\"_blank\"\u003eGemini 1.5 Pro\u003c/a\u003e currently supports 2M input tokens. It is enough to fit the seven Harry Potter books… and more!\u003c/p\u003e\n\n\u003ch4\u003e\u003cspan\u003e\u003cb\u003eEmbeddings\u003c/b\u003e\u003c/span\u003e\u003c/h4\u003e \n\u003cp\u003eEmbeddings are multidimensional numerical representations of tokens that accurately encode their semantic meaning and relationships within a given vector space. Words with similar meanings are closer together, while words with opposite meanings are farther apart.\u003c/p\u003e\n\n\u003cp\u003eThe embedding process is a key component of an LLM. You can try it independently using \u003ca href=\"https://mediapipe-studio.webapps.google.com/demo/text_embedder\" target=\"_blank\"\u003eMediaPipe Text Embedder\u003c/a\u003e for Android. It can be used to identify relations between words and sentences and implement a simplified semantic search directly on-device.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"A 3-D graph plots \u0026#39;Man\u0026#39; and \u0026#39;King\u0026#39; in blue and \u0026#39;Woman\u0026#39; and \u0026#39;Queen\u0026#39; in green, with arrows pointing from \u0026#39;Man\u0026#39; to \u0026#39;Woman\u0026#39; and from \u0026#39;King\u0026#39; to \u0026#39;Queen\u0026#39;.\" height=\"308\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd4Rdw7Jw72aVvVmVQSJ-YyFl_fssJoaztEPg6CLlx3bEsi-MKXKoEeRgzK-IuZaoAD4y474VMbaMmEONOdponVn-DnTlHjtbBba0D2HEiRj9oMb9g1FmEqcmtYj6A5vyuxKOJBc96RhMQkPL2jlym9zBE5FSegFGfU_7Ef9LkHIf1K1qcursthHzLJ2g/w640-h308/embedding-Illustration.png\" width=\"640\"/\u003e\u003c/p\u003e\u003cimgcaption\u003e\u003ccenter\u003e\u003cem\u003eA (very) simplified representation of the embeddings for the words “king”, “queen”, “man” and “woman”\u003c/em\u003e\u003c/center\u003e\u003c/imgcaption\u003e\u003cbr/\u003e\n\n\u003ch4\u003e\u003cspan\u003e\u003cb\u003eTop-K, Top-P and Temperature\u003c/b\u003e\u003c/span\u003e\u003c/h4\u003e \n\n\u003cp\u003eParameters like Top-K, Top-P and Temperature enable you to control the creativity of the model and the randomness of its output.\u003c/p\u003e\n\n\u003cp\u003eTop-K filters tokens for output. For example a Top-K of 3 keeps the three most probable tokens. Increasing the Top-K value will increase the randomness of the model response (\u003ca href=\"https://ai.google.dev/gemini-api/docs/prompting-strategies#top-k\" target=\"_blank\"\u003elearn about Top-K parameter\u003c/a\u003e).\u003c/p\u003e\n\n\u003cp\u003eThen, defining the Top-P value adds another step of filtering. Tokens with the highest probabilities are selected until their sum equals the Top-P value. Lower Top-P values result in less random responses, and higher values result in more random responses (\u003ca href=\"https://ai.google.dev/gemini-api/docs/prompting-strategies#top-p\" target=\"_blank\"\u003elearn about Top-P parameter\u003c/a\u003e).\u003c/p\u003e\n\n\u003cp\u003eFinally, the Temperature defines the randomness to select the tokens left. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results (\u003ca href=\"https://ai.google.dev/gemini-api/docs/prompting-strategies#temperature\" target=\"_blank\"\u003elearn about Temperature\u003c/a\u003e).\u003c/p\u003e\n\n\u003ch4\u003e\u003cspan\u003e\u003cb\u003eFine-tuning\u003c/b\u003e\u003c/span\u003e\u003c/h4\u003e \n\n\u003cp\u003eIterating over several versions of a prompt to achieve an optimal response from the model for your use-case isn’t always enough. The next step is to fine-tune the model by re-training it with data specific to your use-case. You will then obtain a model customized to your application.\u003c/p\u003e\n\n\u003cp\u003eMore specifically, Low rank adaptation (LoRA) is a fine-tuning technique that makes LLM training much faster and more memory-efficient while maintaining the quality of the model outputs.\nThe process to fine-tune open models via LoRA is well documented. See, for example, how you can \u003ca href=\"https://ai.google.dev/gemini-api/docs/model-tuning?_gl=1*onufta*_ga*NzQ4MzE1OTM5LjE3MjI1MzQ4ODg.*_ga_P1DBVKWT6V*MTcyMjUzNDg4OC4xLjEuMTcyMjUzNTk5Ni42MC4wLjIwNDAwMDI3MDA.\" target=\"_blank\"\u003efine-tune Gemini models through Google AI Studio\u003c/a\u003e without advanced ML expertise. You can also \u003ca href=\"https://ai.google.dev/gemma/docs/lora_tuning\" target=\"_blank\"\u003efine-tune Gemma models using the KerasNLP library\u003c/a\u003e.\u003c/p\u003e\u003cbr/\u003e\n\n\u003ch3\u003eThe future of generative AI on Android\u003c/h3\u003e\n\n\u003cp\u003eWith ongoing research and optimization of LLMs for mobile devices, we can expect even more innovative gen AI enabled features coming to Android soon. In the meantime check out other \u003ca href=\"https://android-developers.googleblog.com/2024/09/welcome-to-ai-on-android-spotlight-week.html\" target=\"_blank\"\u003eAI on Android Spotlight Week blog posts\u003c/a\u003e, and go to the \u003ca href=\"http://android-developers.googleblog.com/2024/09/d.android.com/ai\" target=\"_blank\"\u003eAndroid AI documentation\u003c/a\u003e to learn more about how to power your apps with gen AI capabilities!\u003c/p\u003e\n\n\n\n\n\n\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": null,
  "modifiedTime": null
}
