{
  "id": "45166192-c7f4-4473-a11e-3fbaeca51f42",
  "title": "PyTorch machine learning models on Android",
  "link": "http://android-developers.googleblog.com/2024/10/pytorch-machine-learning-models-on-android.html",
  "description": "",
  "author": "Android Developers",
  "published": "2024-10-02T12:00:00.000-07:00",
  "source": "http://feeds.feedburner.com/blogspot/hsDu",
  "categories": [
    "#GenerativeAI",
    "AndroidAI",
    "Gemini",
    "LiteRT",
    "MediaPipe",
    "PyTorch"
  ],
  "byline": "",
  "length": 5934,
  "excerpt": "Use Google AI Edge Torch to convert PyTorch models for use on Android devices. Convert a MobileViT model for image classification and add metadata.",
  "siteName": "Android Developers Blog",
  "favicon": "",
  "text": "Posted by Paul Ruiz – Senior Developer Relations Engineer Earlier this year we launched Google AI Edge, a suite of tools with easy access to ready-to-use ML tasks, frameworks that enable you to build ML pipelines, and run popular LLMs and custom models – all on-device. For AI on Android Spotlight Week, the Google team is highlighting various ways that Android developers can use machine learning to help improve their applications. In this post, we'll dive into Google AI Edge Torch, which enables you to convert PyTorch models to run locally on Android and other platforms, using the Google AI Edge LiteRT (formerly TensorFlow Lite) and MediaPipe Tasks libraries. For insights on other powerful tools, be sure to explore the rest of the AI on Android Spotlight Week content. To get started with Google AI Edge easier, we've provided samples available on GitHub as an executable codelab. They demonstrate how to convert the MobileViT model for image classification (compatible with MediaPipe Tasks) and the DIS model for segmentation (compatible with LiteRT). DIS model output This blog guides you through how to use the MobileViT model with MediaPipe Tasks. Keep in mind that the LiteRT runtime provides similar capabilities, enabling you to build custom pipelines and features. Convert MobileViT model for image classification compatible with MediaPipe Tasks Once you've installed the necessary dependencies and utilities for your app, the first step is to retrieve the PyTorch model you wish to convert, along with any other MobileViT components you might need (such as an image processor for testing). from transformers import MobileViTImageProcessor, MobileViTForImageClassification hf_model_path = 'apple/mobilevit-small' processor = MobileViTImageProcessor.from_pretrained(hf_model_path) pt_model = MobileViTForImageClassification.from_pretrained(hf_model_path) Since the end result of this tutorial should work with MediaPipe Tasks, take an extra step to match the expected input and output shapes for image classification to what is used by the MediaPipe image classification Task. class HF2MP_ImageClassificationModelWrapper(nn.Module): def __init__(self, hf_image_classification_model, hf_processor): super().__init__() self.model = hf_image_classification_model if hf_processor.do_rescale: self.rescale_factor = hf_processor.rescale_factor else: self.rescale_factor = 1.0 def forward(self, image: torch.Tensor): # BHWC -\u003e BCHW. image = image.permute(0, 3, 1, 2) # RGB -\u003e BGR. image = image.flip(dims=(1,)) # Scale [0, 255] -\u003e [0, 1]. image = image * self.rescale_factor logits = self.model(pixel_values=image).logits # [B, 1000] float32. # Softmax is required for MediaPipe classification model. logits = torch.nn.functional.softmax(logits, dim=-1) return logits hf_model_path = 'apple/mobilevit-small' hf_mobile_vit_processor = MobileViTImageProcessor.from_pretrained(hf_model_path) hf_mobile_vit_model = MobileViTForImageClassification.from_pretrained(hf_model_path) wrapped_pt_model = HF2MP_ImageClassificationModelWrapper( hf_mobile_vit_model, hf_mobile_vit_processor).eval() Whether you plan to use the converted MobileViT model with MediaPipe Tasks or LiteRT, the next step is to convert the model to the .tflite format. First, match the input shape. In this example, the input shape is 1, 256, 256, 3 for a 256x256 pixel three-channel RGB image. Then, call AI Edge Torch's convert function to complete the conversion process. import ai_edge_torch sample_args = (torch.rand((1, 256, 256, 3)),) edge_model = ai_edge_torch.convert(wrapped_pt_model, sample_args) After converting the model, you can further refine it by incorporating metadata for the image classification labels. MediaPipe Tasks will utilize this metadata to display or return pertinent information after classification. from mediapipe.tasks.python.metadata.metadata_writers import image_classifier from mediapipe.tasks.python.metadata.metadata_writers import metadata_writer from mediapipe.tasks.python.vision.image_classifier import ImageClassifier from pathlib import Path flatbuffer_file = Path('hf_mobile_vit_mp_image_classification_raw.tflite') edge_model.export(flatbuffer_file) tflite_model_buffer = flatbuffer_file.read_bytes() //Extract the image classification labels from the HF models for later integration into the TFLite model. labels = list(hf_mobile_vit_model.config.id2label.values()) writer = image_classifier.MetadataWriter.create( tflite_model_buffer, input_norm_mean=[0.0], # Normalization is not needed for this model. input_norm_std=[1.0], labels=metadata_writer.Labels().add(labels), ) tflite_model_buffer, _ = writer.populate() With all of that completed, it's time to integrate your model into an Android app. If you're following the official Colab notebook, this involves saving the model locally. For an example of image classification with MediaPipe Tasks, explore the GitHub repository. You can find more information in the official Google AI Edge documentation. Newly converted ViT model with MediaPipe Tasks After understanding how to convert a simple image classification model, you can use the same techniques to adapt various PyTorch models for Google AI Edge LiteRT or MediaPipe Tasks tooling on Android. For further model optimization, consider methods like quantizing during conversion. Check out the GitHub example to learn more about how to convert a PyTorch image segmentation model to LiteRT and quantize it. What's Next To keep up to date on Google AI Edge developments, look for announcements on the Google for Developers YouTube channel and blog. We look forward to hearing about how you're using these features in your projects. Use #AndroidAI hashtag to share your feedback or what you've built in social media and check out other content in AI on Android Spotlight Week!",
  "image": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZjxrgmFgqtFucZ9AOr-KC-kNPi_JJoZVAOHolxpuJkhA3wzmxmxV8wi-bkxEzlggzJ8Wjk_oPl40ljl5BD3-EqIT2iQUlXRnIWGI-rlX65bPE6p9HHm1LAp4ovJ05F_OzBhPyiqpOL0BN1QLEQg-SnY5Aie3phGU76QXEjEs5_KuaaKTA6vcEUbUQdnM/w1200-h630-p-k-no-nu/AI-on-Android-Google-AI-Edge-Social.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cmeta content=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZjxrgmFgqtFucZ9AOr-KC-kNPi_JJoZVAOHolxpuJkhA3wzmxmxV8wi-bkxEzlggzJ8Wjk_oPl40ljl5BD3-EqIT2iQUlXRnIWGI-rlX65bPE6p9HHm1LAp4ovJ05F_OzBhPyiqpOL0BN1QLEQg-SnY5Aie3phGU76QXEjEs5_KuaaKTA6vcEUbUQdnM/s1600/AI-on-Android-Google-AI-Edge-Social.png\" name=\"twitter:image\"/\u003e\n\u003cp\u003e\n\n\u003cem\u003ePosted by Paul Ruiz – Senior Developer Relations Engineer\u003c/em\u003e\n\n\u003ca href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBlnDoRECljZdnZDWoxF5TfurA_Y8_5zfLBqAniIsAhSNHKMhmmZDuOqfJ3-FWSqPRBOzDZD-931BmgGWwJavtzn_BTCvMNyqirYnISrWLyr3fQEhWARziFQP2dWrMpIJnX_Z7EZSIaAnr4PK9M9EAzHWAqR7s36zY2p2pHXSLhVH1UdTWA0CJeOB82GI/s1600/AI-on-Android-Google-AI-Edge%20%285%29.png\"\u003e\u003cimg data-original-height=\"800\" data-original-width=\"100%\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBlnDoRECljZdnZDWoxF5TfurA_Y8_5zfLBqAniIsAhSNHKMhmmZDuOqfJ3-FWSqPRBOzDZD-931BmgGWwJavtzn_BTCvMNyqirYnISrWLyr3fQEhWARziFQP2dWrMpIJnX_Z7EZSIaAnr4PK9M9EAzHWAqR7s36zY2p2pHXSLhVH1UdTWA0CJeOB82GI/s1600/AI-on-Android-Google-AI-Edge%20%285%29.png\"/\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003eEarlier this year we launched \u003ca href=\"https://ai.google.dev/edge\" target=\"_blank\"\u003eGoogle AI Edge\u003c/a\u003e, a suite of tools with easy access to ready-to-use ML tasks, frameworks that enable you to build ML pipelines, and run popular LLMs and custom models – all on-device. For AI on Android Spotlight Week, the Google team is highlighting various ways that Android developers can use machine learning to help improve their applications.\u003c/p\u003e\n\n\u003cp\u003eIn this post, we\u0026#39;ll dive into Google AI Edge Torch, which enables you to convert PyTorch models to run locally on Android and other platforms, using the Google AI Edge \u003ca href=\"https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/\" target=\"_blank\"\u003eLiteRT\u003c/a\u003e (formerly TensorFlow Lite) and \u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/tasks\" target=\"_blank\"\u003eMediaPipe Tasks\u003c/a\u003e libraries. For insights on other powerful tools, be sure to explore the rest of the  \u003ca href=\"https://android-developers.googleblog.com/2024/09/welcome-to-ai-on-android-spotlight-week.html\" target=\"_blank\"\u003eAI on Android Spotlight Week\u003c/a\u003e content.\u003c/p\u003e\n\n\u003cp\u003eTo get started with Google AI Edge easier, we\u0026#39;ve provided \u003ca href=\"https://github.com/google-ai-edge/models-samples/tree/main/convert_pytorch\" target=\"_blank\"\u003esamples\u003c/a\u003e available on GitHub as an executable codelab. They demonstrate how to convert the \u003ca href=\"https://huggingface.co/apple/mobilevit-small\" target=\"_blank\"\u003eMobileViT\u003c/a\u003e model for image classification (compatible with MediaPipe Tasks) and the \u003ca href=\"https://github.com/xuebinqin/DIS\" target=\"_blank\"\u003eDIS\u003c/a\u003e model for segmentation (compatible with LiteRT).\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"a red Android figurine is shown next to a black and white silhouette of the same figure, labeled \u0026#39;Original Image\u0026#39; and \u0026#39;PT Mask\u0026#39; respectively, demonstrating image segmentation.\" height=\"267\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaJcGDf2XurjFZFuAX272fP7qqiSnNulbaQ3aG_SUALgpdWObCDeTB11yFuuZkoKqiKrI1uKJ-XikdofnlsaPGnbJcpp57cuJGGGywa5xZtuqcfazeWGJSTCfGWM3JeyG6tXpe9kPUKhBqBksvQKL7871VKX_ypCE_0b_qa_Ecn_-tMOR3WYontiJoLyM/w640-h267/image1.png\" width=\"80%\"/\u003e\u003c/p\u003e\u003cimgcaption\u003e\u003ccenter\u003e\u003cem\u003eDIS model output\u003c/em\u003e\u003c/center\u003e\u003c/imgcaption\u003e\n\n\u003cp\u003eThis blog guides you through how to use the MobileViT model with MediaPipe Tasks. Keep in mind that the LiteRT runtime provides similar capabilities, enabling you to build custom pipelines and features.\u003c/p\u003e\n\n\u003ch3\u003eConvert MobileViT model for image classification compatible with MediaPipe Tasks\u003c/h3\u003e\n\n\u003cp\u003eOnce you\u0026#39;ve installed the necessary dependencies and utilities for your app, the first step is to retrieve the PyTorch model you wish to convert, along with any other MobileViT components you might need (such as an image processor for testing).\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003efrom transformers import MobileViTImageProcessor, MobileViTForImageClassification\n\nhf_model_path = \u0026#39;apple/mobilevit-small\u0026#39;\nprocessor = MobileViTImageProcessor.from_pretrained(hf_model_path)\npt_model = MobileViTForImageClassification.from_pretrained(hf_model_path)\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eSince the end result of this tutorial should work with MediaPipe Tasks, take an extra step to match the expected input and output shapes for image classification to what is used by the MediaPipe image classification Task.\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003eclass\u003c/span\u003e \u003cspan\u003eHF2MP_ImageClassificationModelWrapper\u003c/span\u003e(nn.Module):\n\n  def \u003cspan\u003e__init__\u003c/span\u003e(self, hf_image_classification_model, hf_processor):\n    super().__init__()\n    self.model = hf_image_classification_model\n    \u003cspan\u003eif\u003c/span\u003e hf_processor.do_rescale:\n      self.rescale_factor = hf_processor.rescale_factor\n    \u003cspan\u003eelse\u003c/span\u003e:\n      self.rescale_factor = \u003cspan\u003e1.0\u003c/span\u003e\n\n  def \u003cspan\u003eforward\u003c/span\u003e(self, image: torch.Tensor):\n    # BHWC -\u0026gt; BCHW.\n    image = image.permute(\u003cspan\u003e0\u003c/span\u003e, \u003cspan\u003e3\u003c/span\u003e, \u003cspan\u003e1\u003c/span\u003e, \u003cspan\u003e2\u003c/span\u003e)\n    # RGB -\u0026gt; BGR.\n    image = image.flip(dims=(\u003cspan\u003e1\u003c/span\u003e,))\n    # Scale [\u003cspan\u003e0\u003c/span\u003e, \u003cspan\u003e255\u003c/span\u003e] -\u0026gt; [\u003cspan\u003e0\u003c/span\u003e, \u003cspan\u003e1\u003c/span\u003e].\n    image = image * self.rescale_factor\n    logits = self.model(pixel_values=image).logits  # [B, \u003cspan\u003e1000\u003c/span\u003e] float32.\n    # Softmax \u003cspan\u003eis\u003c/span\u003e required \u003cspan\u003efor\u003c/span\u003e MediaPipe classification model.\n    logits = torch.nn.functional.softmax(logits, dim=-\u003cspan\u003e1\u003c/span\u003e)\n\n    \u003cspan\u003ereturn\u003c/span\u003e logits\n\u003c/pre\u003e\u003c/div\u003e\u003cbr/\u003e\n\n\u003cdiv\u003e\u003cpre\u003ehf_model_path = \u0026#39;apple/mobilevit-small\u0026#39;\nhf_mobile_vit_processor = MobileViTImageProcessor.from_pretrained(hf_model_path)\nhf_mobile_vit_model = MobileViTForImageClassification.from_pretrained(hf_model_path)\nwrapped_pt_model = HF2MP_ImageClassificationModelWrapper(\nhf_mobile_vit_model, hf_mobile_vit_processor).eval()\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eWhether you plan to use the converted MobileViT model with MediaPipe Tasks or LiteRT, the next step is to convert the model to the \u003cspan\u003e.tflite\u003c/span\u003e format.\u003c/p\u003e\n\n\u003cp\u003eFirst, match the input shape. In this example, the input shape is \u003ci\u003e1, 256, 256, 3\u003c/i\u003e for a 256x256 pixel three-channel RGB image.\u003c/p\u003e \n\n\u003cp\u003eThen, call AI Edge Torch\u0026#39;s \u003cspan\u003econvert\u003c/span\u003e function to complete the conversion process.\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003eimport ai_edge_torch\n\nsample_args = (torch.rand((\u003cspan\u003e1\u003c/span\u003e, \u003cspan\u003e256\u003c/span\u003e, \u003cspan\u003e256\u003c/span\u003e, \u003cspan\u003e3\u003c/span\u003e)),)\nedge_model = ai_edge_torch.convert(wrapped_pt_model, sample_args)\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eAfter converting the model, you can further refine it by incorporating metadata for the image classification labels. MediaPipe Tasks will utilize this metadata to display or return pertinent information after classification.\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003efrom mediapipe.tasks.python.metadata.metadata_writers import image_classifier\nfrom mediapipe.tasks.python.metadata.metadata_writers import metadata_writer\nfrom mediapipe.tasks.python.vision.image_classifier import ImageClassifier\nfrom pathlib import Path\n\nflatbuffer_file = Path(\u0026#39;hf_mobile_vit_mp_image_classification_raw.tflite\u0026#39;)\nedge_model.export(flatbuffer_file)\ntflite_model_buffer = flatbuffer_file.read_bytes()\n\n\u003cspan\u003e//Extract the image classification labels from the HF models for later integration into the TFLite model.\u003c/span\u003e\nlabels = list(hf_mobile_vit_model.config.id2label.values())\n\nwriter = image_classifier.MetadataWriter.create(\n    tflite_model_buffer,\n    input_norm_mean=[\u003cspan\u003e0.0\u003c/span\u003e], #  Normalization \u003cspan\u003eis\u003c/span\u003e not needed \u003cspan\u003efor\u003c/span\u003e \u003cspan\u003ethis\u003c/span\u003e model.\n    input_norm_std=[\u003cspan\u003e1.0\u003c/span\u003e],\n    labels=metadata_writer.Labels().add(labels),\n)\ntflite_model_buffer, _ = writer.populate()\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eWith all of that completed, it\u0026#39;s time to integrate your model into an Android app. If you\u0026#39;re following the official Colab notebook, this involves saving the model locally. For an example of image classification with MediaPipe Tasks, explore the \u003ca href=\"https://github.com/google-ai-edge/mediapipe-samples/tree/main/examples/image_classification/android\" target=\"_blank\"\u003eGitHub\u003c/a\u003e repository. You can find more information in the official \u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/vision/image_classifier/android\" target=\"_blank\"\u003eGoogle AI Edge documentation\u003c/a\u003e.\u003c/p\u003e\n\n\n\u003cp\u003e\u003cimg alt=\"moving image of Newly converted ViT model with MediaPipe Tasks\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhrSKAbbp3X_KGq9RYH5BXLC0PUMnOl7L1JgzXwRZeKc9d8YGlt33hucBfLrf-lb63Ye_kEFcHqwyuG46-YPWH4v6oeVMVM0fnLmy1lHi24QY4rpaM_PM-ZZr7qzfdhmls-kZlEje3e3M_N1bCspYRun85pmsDLnmirDDRegOWmMK6X9SO12oOSIPFVNjg/s16000/image4.gif\"/\u003e\u003c/p\u003e\u003cimgcaption\u003e\u003ccenter\u003e\u003cem\u003eNewly converted ViT model with MediaPipe Tasks\u003c/em\u003e\u003c/center\u003e\u003c/imgcaption\u003e\n\n\n\u003cp\u003eAfter understanding how to convert a simple image classification model, you can use the same techniques to adapt various PyTorch models for Google AI Edge LiteRT or MediaPipe Tasks tooling on Android.\u003c/p\u003e\n\n\u003cp\u003eFor further model optimization, consider methods like \u003ca href=\"https://github.com/google-ai-edge/ai-edge-torch/blob/main/docs/pytorch_converter/README.md#quantization\" target=\"_blank\"\u003equantizing\u003c/a\u003e during conversion. Check out the \u003ca href=\"https://github.com/google-ai-edge/models-samples/blob/main/convert_pytorch/DIS_segmentation_and_quantization.ipynb\" target=\"_blank\"\u003eGitHub example\u003c/a\u003e to learn more about how to convert a PyTorch image segmentation model to LiteRT and quantize it.\u003c/p\u003e\n\n\u003ch3\u003eWhat\u0026#39;s Next\u003c/h3\u003e\n\n\u003cp\u003eTo keep up to date on Google AI Edge developments, look for announcements on the \u003ca href=\"https://www.youtube.com/user/googledevelopers\" target=\"_blank\"\u003eGoogle for Developers YouTube channel\u003c/a\u003e and \u003ca href=\"https://developers.googleblog.com\" target=\"_blank\"\u003eblog\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eWe look forward to hearing about how you\u0026#39;re using these features in your projects. Use #AndroidAI hashtag to share your feedback or what you\u0026#39;ve built in social media and check out other content in \u003ca href=\"https://android-developers.googleblog.com/2024/09/welcome-to-ai-on-android-spotlight-week.html\" target=\"_blank\"\u003eAI on Android Spotlight Week\u003c/a\u003e!\u003c/p\u003e\n\n\n\n\n\n\n\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": null,
  "modifiedTime": null
}
