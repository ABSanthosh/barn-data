{
  "id": "af405b85-23f4-49d9-826b-d66fb846f805",
  "title": "How to bring your AI Model to Android devices",
  "link": "http://android-developers.googleblog.com/2024/10/bring-your-ai-model-to-android-devices.html",
  "description": "",
  "author": "Android Developers",
  "published": "2024-10-02T09:00:00.000-07:00",
  "source": "http://feeds.feedburner.com/blogspot/hsDu",
  "categories": [
    "#GenerativeAI",
    "Google AI Edge",
    "on-device"
  ],
  "byline": "",
  "length": 7260,
  "excerpt": "Bring your AI models to Android devices using Google's AI Edge platform, with tools and frameworks for building, deploying, and optimizing AI models.",
  "siteName": "Android Developers Blog",
  "favicon": "",
  "text": "Posted by Kateryna Semenova – Senior Developer Relations Engineer and Mark Sherwood – Senior Product Manager During AI on Android Spotlight Week, we're diving into how you can bring your own AI model to Android-powered devices such as phones, tablets, and beyond. By leveraging the tools and technologies available from Google and other sources, you can run sophisticated AI models directly on these devices, opening up exciting possibilities for better performance, privacy, and usability. Understanding on-device AI On-device AI involves deploying and executing machine learning or generative AI models directly on hardware devices, instead of relying on cloud-based servers. This approach offers several advantages, such as reduced latency, enhanced privacy, cost saving and less dependence on internet connectivity. For generative text use cases, explore Gemini Nano that is now available in experimental access through its SDK. For many on-device AI use cases, you might want to package your own models in your app. Today we will walk through how to do so on Android. Key resources for on-device AI The Google AI Edge platform provides a comprehensive ecosystem for building and deploying AI models on edge devices. It supports various frameworks and tools, enabling developers to integrate AI capabilities seamlessly into their applications. The Google AI Edge platforms consists of: MediaPipe Tasks - Cross-platform low-code APIs to tackle common generative AI, vision, text, and audio tasks LiteRT (formerly known as TensorFlow Lite) - Lightweight runtime for deploying custom machine learning models on Android MediaPipe Framework - Pipeline framework for chaining multiple ML models along with pre and post processing logic Model Explorer - Conversion, performance, and debugging visualizer How to build custom AI features on Android 1. Define your use case: Before diving into technical details, it's crucial to clearly define what you want your AI feature to achieve. Whether you're aiming for image classification, natural language processing, or another application, having a well-defined goal will guide your development process. 2. Choose the right tools and frameworks: Depending on your use case, you might be able to use an out of the box solution or you might need to create or source your own model. Look through MediaPipe Tasks for common solutions such as gesture recognition, image segmentation or face landmark detection. If you find a solution that aligns with your needs, you can proceed directly to the testing and deployment step. If you need to create or source a custom model for your use case, you will need an on-device ML framework such as LiteRT (formerly TensorFlow Lite). LiteRT is designed specifically for mobile and edge devices and provides a lightweight runtime for deploying machine learning models. Simply follow these substeps: a. Develop and train your model: Develop your AI model using your chosen framework. Training can be performed on a powerful machine or cloud environment, but the model should be optimized for deployment on a device. Techniques like quantization and pruning can help reduce the model size and improve inference speed. Model Explorer can help understand and explore your model as you're working with it. b. Convert and optimize the model: Once your model is trained, convert it to a format suitable for on-device deployment. LiteRT, for example, requires conversion to its specific format. Optimization tools can help reduce the model’s footprint and enhance performance. AI Edge Torch allows you to convert PyTorch models to run locally on Android and other platforms, using Google AI Edge LiteRT and MediaPipe Tasks libraries. c. Accelerate your model: You can speed up model inference on Android by using GPU and NPU. LiteRT’s GPU delegate allows you to run your model on GPU today. We’re working hard on building the next generation of GPU and NPU delegates that will make your models run even faster, and enable more models to run on GPU and NPU. We’d like to invite you to participate in our early access program to try out this new GPU and NPU infrastructure. We will select participants out on a rolling basis so don’t wait to reach out. 3. Test and deploy: To ensure that your model delivers the expected performance across various devices, rigorous testing is crucial. Deploy your app to users after completing the testing phase, offering them a seamless and efficient AI experience. We're working on bringing the benefits of Google Play and Android App Bundles to delivering custom ML models for on-device AI features. Play for On-device AI takes the complexity out of launching, targeting, versioning, downloading, and updating on-device models so that you can offer your users a better user experience without compromising your app's size and at no additional cost. Complete this form to express interest in joining the Play for On-device AI early access program. Build trust in AI through privacy and transparency With the growing role of AI in everyday life, ensuring models run as intended on devices is crucial. We're emphasizing a \"zero trust\" approach, providing developers with tools to verify device integrity and user control over their data. In the zero trust approach, developers need the ability to make informed decisions about the device's trustworthiness. The Play Integrity API is recommended for developers looking to verify their app, server requests, and the device environment (and, soon, the recency of security updates on the device). You can call the API at important moments before your app’s backend decides to download and run your models. You can also consider turning on integrity checks for installing your app to reduce your app’s distribution to unknown and untrusted environments. Play Integrity API makes use of Android Platform Key Attestation to verify hardware components and generate integrity verdicts across the fleet, eliminating the need for most developers to directly integrate different attestation tools and reducing device ecosystem complexity. Developers can use one or both of these tools to assess device security and software integrity before deciding whether to trust a device to run AI models. Conclusion Bringing your own AI model to a device involves several steps, from defining your use case to deploying and testing the model. With resources like Google AI Edge, developers have access to powerful tools and insights to make this process smoother and more effective. As on-device AI continues to evolve, leveraging these resources will enable you to create cutting-edge applications that offer enhanced performance, privacy, and user experience. We are currently seeking early access partners to try out some of our latest tools and APIs at Google AI Edge. Simply fill in this form to connect and explore how we can work together to make your vision a reality. Dive into these resources and start exploring the potential of on-device AI—your next big innovation could be just a model away! Use #AndroidAI hashtag to share your feedback or what you've built on social media and catch up with the rest of the updates being shared during Spotlight Week: AI on Android.",
  "image": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNs9EPwnkUin85xb7EVpVKQXG3_fHTxSLFwUMiBpnfqZsdZQw-bOG6bv_ZHy9f3cRItTMRLnS_viRQXDRi2FPECNRdmSg01hHUFK-Q6WA64bx5YtBf6q1UDX1qALig8YIPmw2D6fY3oMltfqTjtPl7Qu7wtuMrLnDbfM5e8OmBa3q4R_uOmvY892SHQx8/w1200-h630-p-k-no-nu/AI-on-Android-MediaPipe-LiteRT-Social%20%281%29.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cmeta content=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNs9EPwnkUin85xb7EVpVKQXG3_fHTxSLFwUMiBpnfqZsdZQw-bOG6bv_ZHy9f3cRItTMRLnS_viRQXDRi2FPECNRdmSg01hHUFK-Q6WA64bx5YtBf6q1UDX1qALig8YIPmw2D6fY3oMltfqTjtPl7Qu7wtuMrLnDbfM5e8OmBa3q4R_uOmvY892SHQx8/s1600/AI-on-Android-MediaPipe-LiteRT-Social%20%281%29.png\" name=\"twitter:image\"/\u003e\n\u003cp\u003e\n\n\u003cem\u003ePosted by Kateryna Semenova – Senior Developer Relations Engineer and Mark Sherwood – Senior Product Manager\u003c/em\u003e\n\n\u003ca href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijX_AkDEoPg22WAF0s1zBdYvn4jm0ag2MhVnuKKSNA-est4K2JIAGwcikr4chc1WCVCEGSrC1FLyFpWmaFPogWBQl37-KcF_wf9vM2XndIbjsNB1BHTfXiTcA5Iu0tthbde4v2kIFS2fQvYq7npQJUA5qo5vXvS8VSdidtiCOEooq6uShs3DJ1EIcCtTc/s1600/AI-on-Android-MediaPipe-LiteRT.png\"\u003e\u003cimg data-original-height=\"800\" data-original-width=\"100%\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijX_AkDEoPg22WAF0s1zBdYvn4jm0ag2MhVnuKKSNA-est4K2JIAGwcikr4chc1WCVCEGSrC1FLyFpWmaFPogWBQl37-KcF_wf9vM2XndIbjsNB1BHTfXiTcA5Iu0tthbde4v2kIFS2fQvYq7npQJUA5qo5vXvS8VSdidtiCOEooq6uShs3DJ1EIcCtTc/s1600/AI-on-Android-MediaPipe-LiteRT.png\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eDuring \u003ca href=\"https://android-developers.googleblog.com/2024/09/welcome-to-ai-on-android-spotlight-week.html\" target=\"_blank\"\u003eAI on Android Spotlight Week\u003c/a\u003e, we\u0026#39;re diving into how you can bring your own AI model to Android-powered devices such as phones, tablets, and beyond. By leveraging the tools and technologies available from Google and other sources, you can run sophisticated AI models directly on these devices, opening up exciting possibilities for better performance, privacy, and usability.\u003c/p\u003e\n\n\u003ch2\u003e\u003cspan\u003eUnderstanding on-device AI\u003c/span\u003e\u003c/h2\u003e\n\n\u003cp\u003eOn-device AI involves deploying and executing machine learning or generative AI models directly on hardware devices, instead of relying on cloud-based servers. This approach offers several advantages, such as reduced latency, enhanced privacy, cost saving and less dependence on internet connectivity.\u003c/p\u003e\n\n\u003cp\u003eFor generative text use cases, explore \u003ca href=\"https://developer.android.com/ai/aicore\" target=\"_blank\"\u003eGemini Nano\u003c/a\u003e that is now available in experimental access through its \u003ca href=\"https://developer.android.com/ai/gemini-nano/experimental\" target=\"_blank\"\u003eSDK\u003c/a\u003e. For many on-device AI use cases, you might want to package your own models in your app. Today we will walk through how to do so on Android.\u003c/p\u003e\n\n\u003ch3\u003e\u003cspan\u003eKey resources for on-device AI\u003c/span\u003e\u003c/h3\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://ai.google.dev/edge\" target=\"_blank\"\u003e\u003cb\u003eGoogle AI Edge\u003c/b\u003e\u003c/a\u003e platform provides a comprehensive ecosystem for building and deploying AI models on edge devices. It supports various frameworks and tools, enabling developers to integrate AI capabilities seamlessly into their applications. The Google AI Edge platforms consists of: \u003c/p\u003e\n\u003cul\u003e\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/guide\" target=\"_blank\"\u003eMediaPipe Tasks\u003c/a\u003e - Cross-platform low-code APIs to tackle common generative AI, vision, text, and audio tasks\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ai.google.dev/edge/litert\" target=\"_blank\"\u003eLiteRT\u003c/a\u003e (formerly known as TensorFlow Lite) - Lightweight runtime for deploying custom machine learning models on Android\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ai.google.dev/edge/mediapipe/framework\" target=\"_blank\"\u003eMediaPipe Framework\u003c/a\u003e - Pipeline framework for chaining multiple ML models along with pre and post processing logic\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ai.google.dev/edge/model-explorer\" target=\"_blank\"\u003eModel Explorer\u003c/a\u003e - Conversion, performance, and debugging visualizer\u003c/li\u003e\n\u003c/ul\u003e\u003c/ul\u003e\n\n\n\u003cp\u003e\u003cimg alt=\"Google AI Edge Logo\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg73C3zpLkTFrj4Um7qbxpMByHN_4999i_RxpIp8cF9-vX-Kw6nFrdNiT2S8-m0KfxgNrPsWvdFS0yM7hcgpxom7BvwiRjlMv5embcnLqZdxKhXWSsyk6lx9vhX8YeoIHdIm1HKQ-Oxs9ur3N82uTAJTzvuIc2P90HWl4-FwBUoEhQv49ElFOJbEhQ1lWs/s1600/image6.png\" width=\"50%\"/\u003e\u003c/p\u003e\u003cbr/\u003e\n\n\u003ch2\u003e\u003cspan\u003eHow to build custom AI features on Android\u003c/span\u003e\u003c/h2\u003e\n\n\n\u003cul\u003e\u003cp\u003e1. \u003cb\u003eDefine your use case:\u003c/b\u003e Before diving into technical details, it\u0026#39;s crucial to clearly define what you want your AI feature to achieve. Whether you\u0026#39;re aiming for image classification, natural language processing, or another application, having a well-defined goal will guide your development process.\u003c/p\u003e\u003c/ul\u003e\n  \n\u003cul\u003e\u003cp\u003e2. \u003cb\u003eChoose the right tools and frameworks:\u003c/b\u003e Depending on your use case, you might be able to use an out of the box solution or you might need to create or source your own model. Look through \u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/tasks\" target=\"_blank\"\u003eMediaPipe Tasks\u003c/a\u003e for common solutions such as \u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer\" target=\"_blank\"\u003egesture recognition\u003c/a\u003e, \u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/vision/interactive_segmenter/android\" target=\"_blank\"\u003eimage segmentation\u003c/a\u003e or \u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/android\" target=\"_blank\"\u003eface landmark detection\u003c/a\u003e. If you find a solution that aligns with your needs, you can proceed directly to the testing and deployment step.\u003c/p\u003e\u003c/ul\u003e\n  \n\n\n\u003cp\u003e\u003cimg alt=\"Google AI Edge Logo\" height=\"400\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvItoZl-JnJlWT44f0OgnCvPH_Xk94pBvHZLZYVFXDOoo12hPxlnQ4x4Z1c9OHoYwBLPEWeieQBMVrmdVHYKMmzIMfmaOUsEmsDTj_qScKVENz8gsL53nuAjcaZL1RUyMvzOLXEp8Sq0h8SQP8yXqjsVofQJ_PDXJjdSNA3fg8eqdCJwRZoxNu_JMi2og/w236-h400/MediaPipe-Gesture-Recognizer-task.gif\" width=\"236\"/\u003e\u003c/p\u003e\u003cbr/\u003e\n\n\u003cul\u003e\n\n\u003cp\u003eIf you need to create or source a custom model for your use case, you will need an on-device ML framework such as \u003ca href=\"https://ai.google.dev/edge/litert\" target=\"_blank\"\u003eLiteRT\u003c/a\u003e (formerly TensorFlow Lite). LiteRT is designed specifically for mobile and edge devices and provides a lightweight runtime for deploying machine learning models. Simply follow these substeps:\u003c/p\u003e\n  \n\u003cul\u003e\u003cul\u003e\n\u003cp\u003ea. \u003cb\u003eDevelop and train your model:\u003c/b\u003e Develop your AI model using your chosen framework. Training can be performed on a powerful machine or cloud environment, but the model should be optimized for deployment on a device. Techniques like quantization and pruning can help reduce the model size and improve inference speed. \u003ca href=\"https://googledevai.devsite.corp.google.com/edge/model-explorer\" target=\"_blank\"\u003eModel Explorer\u003c/a\u003e can help understand and explore your model as you\u0026#39;re working with it.\u003c/p\u003e\u003c/ul\u003e\u003cul\u003e\n\n\u003cp\u003eb. \u003cb\u003eConvert and optimize the model:\u003c/b\u003e Once your model is trained, convert it to a format suitable for on-device deployment. LiteRT, for example, requires conversion to its specific format. Optimization tools can help reduce the model’s footprint and enhance performance. \u003ca href=\"https://developers.googleblog.com/en/ai-edge-torch-high-performance-inference-of-pytorch-models-on-mobile-devices/\" target=\"_blank\"\u003eAI Edge Torch\u003c/a\u003e allows you to convert PyTorch models to run locally on Android and other platforms, using Google AI Edge LiteRT and MediaPipe Tasks libraries.\u003c/p\u003e\u003c/ul\u003e\u003cul\u003e\n  \n\u003cp\u003ec. \u003cb\u003eAccelerate your model:\u003c/b\u003e You can speed up model inference on Android by using GPU and NPU. LiteRT’s \u003ca href=\"https://ai.google.dev/edge/litert/performance/gpu\" target=\"_blank\"\u003eGPU delegate\u003c/a\u003e allows you to run your model on GPU today. We’re working hard on building the next generation of GPU and NPU delegates that will make your models run even faster, and enable more models to run on GPU and NPU. We’d like to \u003ca href=\"https://forms.gle/8HRC2EsVNstPqVgm6\" target=\"_blank\"\u003einvite you to participate in our early access program\u003c/a\u003e to try out this new GPU and NPU infrastructure. We will select participants out on a rolling basis so don’t wait to reach out.\u003c/p\u003e\n\u003c/ul\u003e\u003c/ul\u003e\u003c/ul\u003e\n  \n\u003cul\u003e\u003cp\u003e 3. \u003cb\u003eTest and deploy:\u003c/b\u003e To ensure that your model delivers the expected performance across various devices, rigorous testing is crucial. Deploy your app to users after completing the testing phase, offering them a seamless and efficient AI experience. We\u0026#39;re working on bringing the benefits of Google Play and Android App Bundles to delivering custom ML models for on-device AI features. \u003cb\u003ePlay for On-device AI\u003c/b\u003e takes the complexity out of launching, targeting, versioning, downloading, and updating on-device models so that you can offer your users a better user experience without compromising your app\u0026#39;s size and at no additional cost. \u003ca href=\"https://forms.gle/udLiRgA6zFzpfxZy9\" target=\"_blank\"\u003eComplete this form\u003c/a\u003e to express interest in joining the Play for On-device AI early access program.\u003c/p\u003e\u003c/ul\u003e\n\n\u003ch2\u003e\u003cspan\u003eBuild trust in AI through privacy and transparency\u003c/span\u003e\u003c/h2\u003e\n\n\u003cp\u003eWith the growing role of AI in everyday life, ensuring models run as intended on devices is crucial. We\u0026#39;re emphasizing a \u0026#34;zero trust\u0026#34; approach, providing developers with tools to verify device integrity and user control over their data. In the zero trust approach, developers need the ability to make informed decisions about the device\u0026#39;s trustworthiness.\u003c/p\u003e\n\n\u003cp\u003eThe \u003ca href=\"http://g.co/play/integrityapi\" target=\"_blank\"\u003e\u003cb\u003ePlay Integrity API\u003c/b\u003e\u003c/a\u003e is recommended for developers looking to verify their app, server requests, and the device environment (and, soon, the recency of security updates on the device). You can call the API at important moments before your app’s backend decides to download and run your models. You can also consider turning on \u003ca href=\"https://support.google.com/googleplay/android-developer/answer/13857328#zippy=%2Cstore-listing-visibility\" target=\"_blank\"\u003eintegrity checks for installing your app\u003c/a\u003e to reduce your app’s distribution to unknown and untrusted environments.\u003c/p\u003e\n\n\u003cp\u003ePlay Integrity API makes use of \u003ca href=\"https://developer.android.com/privacy-and-security/security-key-attestation\" target=\"_blank\"\u003eAndroid Platform Key Attestation\u003c/a\u003e to verify hardware components and generate integrity verdicts across the fleet, eliminating the need for most developers to directly integrate different attestation tools and reducing device ecosystem complexity. Developers can use one or both of these tools to assess device security and software integrity before deciding whether to trust a device to run AI models.\u003c/p\u003e\n  \n\u003ch3\u003e\u003cspan\u003eConclusion\u003c/span\u003e\u003c/h3\u003e\n\n\u003cp\u003eBringing your own AI model to a device involves several steps, from defining your use case to deploying and testing the model. With resources like Google AI Edge, developers have access to powerful tools and insights to make this process smoother and more effective. As on-device AI continues to evolve, leveraging these resources will enable you to create cutting-edge applications that offer enhanced performance, privacy, and user experience. We are currently seeking early access partners to try out some of our latest tools and APIs at Google AI Edge. Simply \u003ca href=\"https://docs.google.com/forms/d/e/1FAIpQLSdXA167bUPw3Lc-g_fQ4bMfXXhXxOqskv-xXHtvV8mbqZy90A/viewform\" target=\"_blank\"\u003efill in this form to connect\u003c/a\u003e and explore how we can work together to make your vision a reality.\u003c/p\u003e\n  \n\u003cp\u003eDive into these resources and start exploring the potential of on-device AI—your next big innovation could be just a model away!\u003c/p\u003e\n\n\u003cp\u003eUse #AndroidAI hashtag to share your feedback or what you\u0026#39;ve built on social media and catch up with the \u003ca href=\"https://android-developers.googleblog.com/2024/09/welcome-to-ai-on-android-spotlight-week.html\" target=\"_blank\"\u003erest of the updates being shared during Spotlight Week: AI on Android\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": null,
  "modifiedTime": null
}
