{
  "id": "eaccbcbb-1f03-432f-bc40-dc356818052f",
  "title": "Advanced capabilities of the Gemini API for Android developers",
  "link": "http://android-developers.googleblog.com/2024/10/advanced-capabilities-of-gemini-api-for-android-developers.html",
  "description": "",
  "author": "Android Developers",
  "published": "2024-10-03T10:59:00.000-07:00",
  "source": "http://feeds.feedburner.com/blogspot/hsDu",
  "categories": [
    "#GenerativeAI",
    "AndroidAI",
    "Gemini"
  ],
  "byline": "",
  "length": 10345,
  "excerpt": "Developers can leverage advanced features of the Gemini API, like JSON support and function calling, to build generative AI features into their apps.",
  "siteName": "Android Developers Blog",
  "favicon": "",
  "text": "Posted by Thomas Ezan, Sr Developer Relation Engineer Thousands of developers across the globe are harnessing the power of the Gemini 1.5 Pro and Gemini 1.5 Flash models to infuse advanced generative AI features into their applications. Android developers are no exception, and with the upcoming launch of the stable version of VertexAI in Firebase in a few weeks (available in Beta since Google I/O), it's the perfect time to explore how your app can benefit from it. We just published a codelab to help you get started. Let's deep dive into some advanced capabilities of the Gemini API that go beyond simple text prompting and discover the exciting use cases they can unlock in your Android app. Shaping AI behavior with system instructions System instructions serve as a \"preamble\" that you incorporate before the user prompt. This enables shaping the model's behavior to align with your specific requirements and scenarios. You set the instructions when you initialize the model, and then those instructions persist through all interactions with the model, across multiple user and model turns. For example, you can use system instructions to: Define a persona or role for a chatbot (e.g, “explain like I am 5”) Specify the response to the output format (e.g., Markdown, YAML, etc.) Set the output style and tone (e.g, verbosity, formality, etc…) Define the goals or rules for the task (e.g, “return a code snippet without further explanation”) Provide additional context for the prompt (e.g., a knowledge cutoff date) To use system instructions in your Android app, pass it as parameter when you initialize the model: val generativeModel = Firebase.vertexAI.generativeModel( modelName = \"gemini-1.5-flash\", ... systemInstruction = content { text(\"You are a knowledgeable tutor. Answer the questions using the socratic tutoring method.\") } ) You can learn more about system instruction in the Vertex AI in Firebase documentation. You can also easily test your prompt with different system instructions in Vertex AI Studio, Google Cloud console tool for rapidly prototyping and testing prompts with Gemini models. Vertex AI Studio let’s you test system instructions with your prompts When you are ready to go to production it is recommended to target a specific version of the model (e.g. gemini-1.5-flash-002). But as new model versions are released and previous ones are deprecated, it is advised to use Firebase Remote Config to be able to update the version of the Gemini model without releasing a new version of your app. Beyond chatbots: leveraging generative AI for advanced use cases While chatbots are a popular application of generative AI, the capabilities of the Gemini API go beyond conversational interfaces and you can integrate multimodal GenAI-enabled features into various aspects of your Android app. Many tasks that previously required human intervention (such as analyzing text, image or video content, synthesizing data into a human readable format, engaging in a creative process to generate new content, etc… ) can be potentially automated using GenAI. Gemini JSON support Android apps don’t interface well with natural language outputs. Conversely, JSON is ubiquitous in Android development, and provides a more structured way for Android apps to consume input. However, ensuring proper key/value formatting when working with generative models can be challenging. With the general availability of Vertex AI in Firebase, implemented solutions to streamline JSON generation with proper key/value formatting: Response MIME type identifier If you have tried generating JSON with a generative AI model, it's likely you have found yourself with unwanted extra text that makes the JSON parsing more challenging. e.g: Sure, here is your JSON: ``` { \"someKey”: “someValue\", ... } ``` When using Gemini 1.5 Pro or Gemini 1.5 Flash, in the generation configuration, you can explicitly specify the model’s response mime/type as application/json and instruct the model to generate well-structured JSON output. val generativeModel = Firebase.vertexAI.generativeModel( modelName = \"gemini-1.5-flash\", … generationConfig = generationConfig { responseMimeType = \"application/json\" } ) Review the API reference for more details. Soon, the Android SDK for Vertex AI in Firebase will enable you to define the JSON schema expected in the response. Multimodal capabilities Both Gemini 1.5 Flash and Gemini 1.5 Pro are multimodal models. It means that they can process input from multiple formats, including text, images, audio, video. In addition, they both have long context windows, capable of handling up to 1 million tokens for Gemini 1.5 Flash and 2 million tokens for Gemini 1.5 Pro. These features open doors to innovative functionalities that were previously inaccessible such as automatically generate descriptive captions for images, identify topics in a conversation and generate chapters from an audio file or describe the scenes and actions in a video file. You can pass an image to the model as shown in this example: val contentResolver = applicationContext.contentResolver contentResolver.openInputStream(imageUri).use { stream -\u003e stream?.let { val bitmap = BitmapFactory.decodeStream(stream) // Provide a prompt that includes the image specified above and text val prompt = content { image(bitmap) text(\"How many people are on this picture?\") } } val response = generativeModel.generateContent(prompt) } You can also pass a video to the model: val contentResolver = applicationContext.contentResolver contentResolver.openInputStream(videoUri).use { stream -\u003e stream?.let { val bytes = stream.readBytes() // Provide a prompt that includes the video specified above and text val prompt = content { blob(\"video/mp4\", bytes) text(\"What is in the video?\") } val fullResponse = generativeModel.generateContent(prompt) } } You can learn more about multimodal prompting in the VertexAI for Firebase documentation. Note: This method enables you to pass files up to 20 MB. For larger files, use Cloud Storage for Firebase and include the file’s URL in your multimodal request. Read the documentation for more information. Function calling: Extending the model's capabilities Function calling enables you to extend the capabilities to generative models. For example you can enable the model to retrieve information in your SQL database and feed it back to the context of the prompt. You can also let the model trigger actions by calling the functions in your app source code. In essence, function calls bridge the gap between the Gemini models and your Kotlin code. Take the example of a food delivery application that is interested in implementing a conversational interface with the Gemini 1.5 Flash. Assume that this application has a getFoodOrder(cuisine: String) function that returns the list orders from the user for a specific type of cuisine: fun getFoodOrder(cuisine: String) : JSONObject { // implementation… } Note that the function, to be usable to by the model, needs to return the response in the form of a JSONObject. To make the response available to Gemini 1.5 Flash, create a definition of your function that the model will be able to understand using defineFunction: val getOrderListFunction = defineFunction( name = \"getOrderList\", description = \"Get the list of food orders from the user for a define type of cuisine.\", Schema.str(name = \"cuisineType\", description = \"the type of cuisine for the order\") ) { cuisineType -\u003e getFoodOrder(cuisineType) } Then, when you instantiate the model, share this function definition with the model using the tools parameter: val generativeModel = Firebase.vertexAI.generativeModel( modelName = \"gemini-1.5-flash\", ... tools = listOf(Tool(listOf(getExchangeRate))) ) Finally, when you get a response from the model, check in the response if the model is actually requesting to execute the function: // Send the message to the generative model var response = chat.sendMessage(prompt) // Check if the model responded with a function call response.functionCall?.let { functionCall -\u003e // Try to retrieve the stored lambda from the model's tools and // throw an exception if the returned function was not declared val matchedFunction = generativeModel.tools?.flatMap { it.functionDeclarations } ?.first { it.name == functionCall.name } ?: throw InvalidStateException(\"Function not found: ${functionCall.name}\") // Call the lambda retrieved above val apiResponse: JSONObject = matchedFunction.execute(functionCall) // Send the API response back to the generative model // so that it generates a text response that can be displayed to the user response = chat.sendMessage( content(role = \"function\") { part(FunctionResponsePart(functionCall.name, apiResponse)) } ) } // If the model responds with text, show it in the UI response.text?.let { modelResponse -\u003e println(modelResponse) } To summarize, you’ll provide the functions (or tools to the model) at initialization: And when appropriate, the model will request to execute the appropriate function and provide the results: You can read more about function calling in the VertexAI for Firebase documentation. Unlocking the potential of the Gemini API in your app The Gemini API offers a treasure trove of advanced features that empower Android developers to craft truly innovative and engaging applications. By going beyond basic text prompts and exploring the capabilities highlighted in this blog post, you can create AI-powered experiences that delight your users and set your app apart in the competitive Android landscape. Read more about how some Android apps are already starting to leverage the Gemini API. To learn more about AI on Android, check out other resources we have available during AI on Android Spotlight Week. Use #AndroidAI hashtag to share your creations or feedback on social media, and join us at the forefront of the AI revolution! The code snippets in this blog post have the following license: // Copyright 2024 Google LLC. // SPDX-License-Identifier: Apache-2.0",
  "image": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWNLcken_pDkd2yzqcQX0QN7lGnMv-Pv1sAUMwFrkqFdA3HuVU-NshHGL87nn1eQtrFmEl_QWmUbAitr_NECTBafiTG5UVgkt26S2gdSLlaZw4bfAPFOmamZpx-uk73Lah66nnSPgD4GIZdI2baBMACEAhx2H82QwkJMLBsuIDM30yc49GgIkJLwmka7U/w1200-h630-p-k-no-nu/Large-Language-Models-Android-Social.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\u003cmeta content=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWNLcken_pDkd2yzqcQX0QN7lGnMv-Pv1sAUMwFrkqFdA3HuVU-NshHGL87nn1eQtrFmEl_QWmUbAitr_NECTBafiTG5UVgkt26S2gdSLlaZw4bfAPFOmamZpx-uk73Lah66nnSPgD4GIZdI2baBMACEAhx2H82QwkJMLBsuIDM30yc49GgIkJLwmka7U/s1600/Large-Language-Models-Android-Social.png\" name=\"twitter:image\"/\u003e\n\u003cp\u003e\n\n\u003cem\u003ePosted by \u003ca href=\"https://x.com/lethargicpanda\" target=\"_blank\"\u003eThomas Ezan\u003c/a\u003e, Sr Developer Relation Engineer\u003c/em\u003e\n\n\u003ca href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibeg9V-GcmyYOgDd0_s1aWSvUvjjWc3ikvLjHIh1EtLVga07Bs_OAoalp9WUjPUmZBpme7CASLyJavtYzlHKTzyVyzMeB37oFepayAJJeAnfO0CmJAxLXbYdNKa6lWhyOf1UD2cehnVJ9IvKuA4UXYSp8Z9ULvYMqdVwXtuFJzkOhEEpqQzTCCAGupF28/s1600/Large-Language-Models-Android.png\"\u003e\u003cimg data-original-height=\"800\" data-original-width=\"100%\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibeg9V-GcmyYOgDd0_s1aWSvUvjjWc3ikvLjHIh1EtLVga07Bs_OAoalp9WUjPUmZBpme7CASLyJavtYzlHKTzyVyzMeB37oFepayAJJeAnfO0CmJAxLXbYdNKa6lWhyOf1UD2cehnVJ9IvKuA4UXYSp8Z9ULvYMqdVwXtuFJzkOhEEpqQzTCCAGupF28/s1600/Large-Language-Models-Android.png\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eThousands of developers across the globe are harnessing the power of the Gemini 1.5 Pro and Gemini 1.5 Flash models to infuse advanced generative AI features into their applications. Android developers are no exception, and with the upcoming launch of the stable version of VertexAI in Firebase in a few weeks (available in Beta since Google I/O), it\u0026#39;s the perfect time to explore how your app can benefit from it. We just published a \u003ca href=\"https://developer.android.com/codelabs/gemini-summarize#0\" target=\"_blank\"\u003ecodelab\u003c/a\u003e to help you get started.\u003c/p\u003e\n\n\u003cp\u003eLet\u0026#39;s deep dive into some advanced capabilities of the Gemini API that go beyond simple text prompting and discover the exciting use cases they can unlock in your Android app.\u003c/p\u003e\n\n\u003ch3\u003eShaping AI behavior with system instructions\u003c/h3\u003e\n\n\u003cp\u003eSystem instructions serve as a \u0026#34;preamble\u0026#34; that you incorporate before the user prompt. This enables shaping the model\u0026#39;s behavior to align with your specific requirements and scenarios. You set the instructions when you initialize the model, and then those instructions persist through all interactions with the model, across multiple user and model turns.\u003c/p\u003e\n\n\u003cp\u003eFor example, you can use system instructions to:\u003c/p\u003e\n\u003cul\u003e\u003cul\u003e\n  \u003cli\u003eDefine a persona or role for a chatbot (e.g, “explain like I am 5”)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\n\u003cli\u003eSpecify the response to the output format (e.g., Markdown, YAML, etc.)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\n\u003cli\u003eSet the output style and tone (e.g, verbosity, formality, etc…)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\n\u003cli\u003eDefine the goals or rules for the task (e.g, “return a code snippet without further explanation”)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\n\u003cli\u003eProvide additional context for the prompt (e.g., a knowledge cutoff date)\u003c/li\u003e\n\u003c/ul\u003e\u003c/ul\u003e\n\n\u003cp\u003eTo use system instructions in your Android app, pass it as parameter when you initialize the model:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003eval\u003c/span\u003e generativeModel = Firebase.vertexAI.generativeModel(\n  modelName = \u003cspan\u003e\u0026#34;gemini-1.5-flash\u0026#34;\u003c/span\u003e,\n  ...\n  systemInstruction = \n    content { text(\u003cspan\u003e\u0026#34;You are a knowledgeable tutor. Answer the questions using the socratic tutoring method.\u0026#34;\u003c/span\u003e) }\n)\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eYou can learn more about system instruction in the \u003ca href=\"https://firebase.google.com/docs/vertex-ai/system-instructions?platform=android#code-samples\" target=\"_blank\"\u003eVertex AI in Firebase documentation\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eYou can also easily test your prompt with different system instructions in \u003ca href=\"https://console.cloud.google.com/vertex-ai/studio/freeform\" target=\"_blank\"\u003eVertex AI Studio\u003c/a\u003e, Google Cloud console tool for rapidly prototyping and testing prompts with Gemini models.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"test system instructions with your prompts in Vertex AI Studio\" height=\"292\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPzigXlYS1w2zTVgfvI5oaEOE1AryBpCy5n0X66H6j7xlmahyphenhyphen2syU_CgdIZDtiw_63mPbBIfmNLU64tAv6xLRGbijRtFRiqN_qJ9adMHd_Uo0ew2hh9Fn15MZqrn7fJVKC-PSFG4472qcR3O6xELq8kpZLkQsBg0z0NuYQWatJtim0XGM2xu3JRIPq4hA/w640-h292/image2.png\" width=\"640\"/\u003e\u003c/p\u003e\u003cimgcaption\u003e\u003ccenter\u003e\u003cem\u003eVertex AI Studio let’s you test system instructions with your prompts\n\u003c/em\u003e\u003c/center\u003e\u003c/imgcaption\u003e\n\n\u003cp\u003eWhen you are ready to go to production it is recommended to target a specific version of the model (e.g. gemini-1.5-flash-002). But as new model versions are released and previous ones are deprecated, it is advised to use \u003ca href=\"https://firebase.google.com/docs/remote-config\" target=\"_blank\"\u003eFirebase Remote Config\u003c/a\u003e to be able to update the version of the Gemini model without releasing a new version of your app.\u003c/p\u003e\n\n\u003ch2\u003e\u003cspan\u003eBeyond chatbots: leveraging generative AI for advanced use cases\u003c/span\u003e\u003c/h2\u003e\n  \n\u003cp\u003eWhile chatbots are a popular application of generative AI, the capabilities of the Gemini API go beyond conversational interfaces and you can integrate multimodal GenAI-enabled features into various aspects of your Android app.\u003c/p\u003e\n\n\u003cp\u003eMany tasks that previously required human intervention (such as analyzing text, image or video content, synthesizing data into a human readable format, engaging in a creative process to generate new content, etc… ) can be potentially automated using GenAI.\u003c/p\u003e\n  \n\u003ch3\u003e\u003cspan\u003e\u003cb\u003eGemini JSON support\u003c/b\u003e\u003c/span\u003e\u003c/h3\u003e\n  \n\u003cp\u003eAndroid apps don’t interface well with natural language outputs. Conversely, JSON is ubiquitous in Android development, and provides a more structured way for Android apps to consume input. However, ensuring proper key/value formatting when working with generative models can be challenging.\u003c/p\u003e\n\n\u003cp\u003eWith the general availability of Vertex AI in Firebase, implemented solutions to streamline JSON generation with proper key/value formatting:\u003c/p\u003e\n\n\u003ch4\u003e\u003cspan\u003e\u003cb\u003eResponse MIME type identifier\u003c/b\u003e\u003c/span\u003e\u003c/h4\u003e\n\n\u003cp\u003eIf you have tried generating JSON with a generative AI model, it\u0026#39;s likely you have found yourself with unwanted extra text that makes the JSON parsing more challenging.\u003c/p\u003e\n\n\u003cp\u003ee.g:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003eSure, here \u003cspan\u003eis\u003c/span\u003e your JSON:\n```\n{\n   \u003cspan\u003e\u0026#34;someKey”: “someValue\u0026#34;\u003c/span\u003e,\n   ...\n}\n```\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eWhen using Gemini 1.5 Pro or Gemini 1.5 Flash, in the generation configuration, you can explicitly specify the model’s response mime/type as application/json and instruct the model to generate well-structured JSON output.\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003eval\u003c/span\u003e generativeModel = Firebase.vertexAI.generativeModel(\n  modelName = \u003cspan\u003e\u0026#34;gemini-1.5-flash\u0026#34;\u003c/span\u003e,\n  \u003cspan\u003e…\u003c/span\u003e\n  generationConfig = generationConfig {\n     responseMimeType = \u003cspan\u003e\u0026#34;application/json\u0026#34;\u003c/span\u003e\n  }\n)\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eReview the \u003ca href=\"https://firebase.google.com/docs/reference/kotlin/com/google/firebase/vertexai/type/GenerationConfig#responseMimeType%28%29\" target=\"_blank\"\u003eAPI reference\u003c/a\u003e for more details.\u003c/p\u003e\n\n\u003cp\u003eSoon, the Android SDK for Vertex AI in Firebase will enable you to define the JSON schema expected in the response.\u003c/p\u003e\u003cbr/\u003e\n\n\n\u003ch3\u003e\u003cspan\u003e\u003cb\u003eMultimodal capabilities\u003c/b\u003e\u003c/span\u003e\u003c/h3\u003e\n\n\u003cp\u003eBoth Gemini 1.5 Flash and Gemini 1.5 Pro are multimodal models. It means that they can process input from multiple formats, including \u003cb\u003etext, images, audio, video\u003c/b\u003e. In addition, they both have long context windows, capable of handling up to 1 million tokens for Gemini 1.5 Flash and 2 million tokens for Gemini 1.5 Pro.\u003c/p\u003e \n\n\u003cp\u003eThese features open doors to innovative functionalities that were previously inaccessible such as automatically generate descriptive captions for images, identify topics in a conversation and generate chapters from an audio file or describe the scenes and actions in a video file.\u003c/p\u003e\n\n\u003cp\u003eYou can pass an image to the model as shown in this example:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003eval\u003c/span\u003e contentResolver = applicationContext.contentResolver\ncontentResolver.openInputStream(imageUri).use { stream -\u0026gt;\n  stream?.let {\n     \u003cspan\u003eval\u003c/span\u003e bitmap = BitmapFactory.decodeStream(stream)\n\n    \u003cspan\u003e// Provide a prompt that includes the image specified above and text\u003c/span\u003e\n    \u003cspan\u003eval\u003c/span\u003e prompt = content {\n       image(bitmap)\n       text(\u003cspan\u003e\u0026#34;How many people are on this picture?\u0026#34;\u003c/span\u003e)\n    }\n  }\n  \u003cspan\u003eval\u003c/span\u003e response = generativeModel.generateContent(prompt)\n}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eYou can also pass a video to the model:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003eval\u003c/span\u003e contentResolver = applicationContext.contentResolver\ncontentResolver.openInputStream(videoUri).use { stream -\u0026gt;\n  stream?.let {\n    \u003cspan\u003eval\u003c/span\u003e bytes = stream.readBytes()\n\n    \u003cspan\u003e// Provide a prompt that includes the video specified above and text\u003c/span\u003e\n    \u003cspan\u003eval\u003c/span\u003e prompt = content {\n        blob(\u003cspan\u003e\u0026#34;video/mp4\u0026#34;\u003c/span\u003e, bytes)\n        text(\u003cspan\u003e\u0026#34;What is in the video?\u0026#34;\u003c/span\u003e)\n    }\n\n    \u003cspan\u003eval\u003c/span\u003e fullResponse = generativeModel.generateContent(prompt)\n  }\n}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eYou can learn more about multimodal prompting in the \u003ca href=\"https://firebase.google.com/docs/vertex-ai/text-gen-from-multimodal?platform=android\" target=\"_blank\"\u003eVertexAI for Firebase documentation\u003c/a\u003e.\u003c/p\u003e\n\n\u003cblockquote\u003e\u003cb\u003eNote:\u003c/b\u003e This method enables you to pass files up to 20 MB. For larger files, use Cloud Storage for Firebase and include the file’s URL in your multimodal request. Read the \u003ca href=\"https://firebase.google.com/docs/vertex-ai/solutions/cloud-storage?platform=android\" target=\"_blank\"\u003edocumentation\u003c/a\u003e for more information.\u003c/blockquote\u003e\u003cbr/\u003e\n\n\u003ch3\u003e\u003cspan\u003e\u003cb\u003eFunction calling: Extending the model\u0026#39;s capabilities\u003c/b\u003e\u003c/span\u003e\u003c/h3\u003e\n\n\u003cp\u003eFunction calling enables you to extend the capabilities to generative models. For example you can enable the model to retrieve information in your SQL database and feed it back to the context of the prompt. You can also let the model trigger actions by calling the functions in your app source code. In essence, function calls bridge the gap between the Gemini models and your Kotlin code.\u003c/p\u003e\n\n\u003cp\u003eTake the example of a food delivery application that is interested in implementing a conversational interface with the Gemini 1.5 Flash. Assume that this application has a \u003cspan\u003egetFoodOrder(cuisine: String)\u003c/span\u003e function that returns the list orders from the user for a specific type of cuisine:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003efun\u003c/span\u003e \u003cspan\u003egetFoodOrder\u003c/span\u003e(cuisine: String) : JSONObject {\n        \u003cspan\u003e// implementation…  \u003c/span\u003e\n}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eNote that the function, to be usable to by the model, needs to return the response in the form of a \u003cspan\u003eJSONObject\u003c/span\u003e.\u003c/p\u003e\n\n\u003cp\u003eTo make the response available to Gemini 1.5 Flash, create a definition of your function that the model will be able to understand using \u003cspan\u003edefineFunction\u003c/span\u003e:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003eval\u003c/span\u003e getOrderListFunction = defineFunction(\n            name = \u003cspan\u003e\u0026#34;getOrderList\u0026#34;\u003c/span\u003e,\n            description = \u003cspan\u003e\u0026#34;Get the list of food orders from the user for a define type of cuisine.\u0026#34;\u003c/span\u003e,\n            Schema.str(name = \u003cspan\u003e\u0026#34;cuisineType\u0026#34;\u003c/span\u003e, description = \u003cspan\u003e\u0026#34;the type of cuisine for the order\u0026#34;\u003c/span\u003e)\n        ) {  cuisineType -\u0026gt;\n            getFoodOrder(cuisineType)\n        }\n\u003c/pre\u003e\u003c/div\u003e\n\n\n\u003cp\u003eThen, when you instantiate the model, share this function definition with the model using the tools parameter:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003eval\u003c/span\u003e generativeModel = Firebase.vertexAI.generativeModel(\n    modelName = \u003cspan\u003e\u0026#34;gemini-1.5-flash\u0026#34;\u003c/span\u003e,\n    ...\n    tools = listOf(Tool(listOf(getExchangeRate)))\n)\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eFinally, when you get a response from the model, check in the response if the model is actually requesting to execute the function:\u003c/p\u003e\n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e// Send the message to the generative model\u003c/span\u003e\n\u003cspan\u003evar\u003c/span\u003e response = chat.sendMessage(prompt)\n\n\u003cspan\u003e// Check if the model responded with a function call\u003c/span\u003e\nresponse.functionCall?.let { functionCall -\u0026gt;\n  \u003cspan\u003e// Try to retrieve the stored lambda from the model\u0026#39;s tools and\u003c/span\u003e\n  \u003cspan\u003e// throw an exception if the returned function was not declared\u003c/span\u003e\n  \u003cspan\u003eval\u003c/span\u003e matchedFunction = generativeModel.tools?.flatMap { it.functionDeclarations }\n      ?.first { it.name == functionCall.name }\n      ?: \u003cspan\u003ethrow\u003c/span\u003e InvalidStateException(\u003cspan\u003e\u0026#34;Function not found: ${functionCall.name}\u0026#34;\u003c/span\u003e)\n  \n  \u003cspan\u003e// Call the lambda retrieved above\u003c/span\u003e\n  \u003cspan\u003eval\u003c/span\u003e apiResponse: JSONObject = matchedFunction.execute(functionCall)\n\n  \u003cspan\u003e// Send the API response back to the generative model\u003c/span\u003e\n  \u003cspan\u003e// so that it generates a text response that can be displayed to the user\u003c/span\u003e\n  response = chat.sendMessage(\n    content(role = \u003cspan\u003e\u0026#34;function\u0026#34;\u003c/span\u003e) {\n        part(FunctionResponsePart(functionCall.name, apiResponse))\n    }\n  )\n}\n\n\u003cspan\u003e// If the model responds with text, show it in the UI\u003c/span\u003e\nresponse.text?.let { modelResponse -\u0026gt;\n    println(modelResponse)\n}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eTo summarize, you’ll provide the functions (or tools to the model) at initialization:\u003c/p\u003e\n\n\n\u003cp\u003e\u003cimg alt=\"A flow diagram shows a green box labeled \u0026#39;Generative Model\u0026#39; connected to a list of model parameters and a list of tools. The parameters include \u0026#39;gemini-1.5-flash\u0026#39;, \u0026#39;api_key\u0026#39;, and \u0026#39;configuration\u0026#39;, while the tools are \u0026#39;getOrderList()\u0026#39;, \u0026#39;getDate()\u0026#39;, and \u0026#39;placeOrder()\u0026#39;.\" height=\"292\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPUAvMBEsshw0alu4q_eeHNYb4W6tM2zb0vVw-yCuuJSslY8PAwCYsu3nBc2AET-MjytPBipkX633NlTvxYc375-4_c_pee_fs9f6-LwCaxa8VL9cwSBWCxsXD3h8Dkf3IgT4BWEPX9E1pSlHod7HV9av7_zT1S9q5gAYnDj79jUJims5e2W0f4N_v9FI/s1600/image3.png\" width=\"640\"/\u003e\u003c/p\u003e\n\n\n\u003cp\u003eAnd when appropriate, the model will request to execute the appropriate function and provide the results:\u003c/p\u003e\n\n\n\u003cp\u003e\u003cimg alt=\"A flow diagram illustrating the interaction between an Android app and a \u0026#39;Generative Model\u0026#39;. The app sends \u0026#39;getDate()\u0026#39; and \u0026#39;getOrderList()\u0026#39; requests.\" height=\"292\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhA2iexLhPrzyJ7H04ZYzbzrkQJ_Nk8ZMMqBHZzYF_mFqI_0SQQ602fCW3t4LZ-PZe3T8PO_aIYhrk7imNIReYvCpZPtWEhcNvjZyK4tf_5Fr14i94XeEVnOdCyJlqzFYOrLQwVHNn3jhBEk9I77CfMUeYkEqWkZDK0iwRs45-ti8uxr5fvjlLXqav2AHU/s1600/image5.png\" width=\"640\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eYou can read more about function calling in the \u003ca href=\"https://firebase.google.com/docs/vertex-ai/function-calling?platform=android#specify-function-declaration-during-model-init\" target=\"_blank\"\u003eVertexAI for Firebase documentation\u003c/a\u003e.\u003c/p\u003e\n\n\n\u003ch2\u003e\u003cspan\u003eUnlocking the potential of the Gemini API in your app\u003c/span\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe Gemini API offers a treasure trove of advanced features that empower Android developers to craft truly innovative and engaging applications. By going beyond basic text prompts and exploring the capabilities highlighted in this blog post, you can create AI-powered experiences that delight your users and set your app apart in the competitive Android landscape.\u003c/p\u003e\n\n\u003cp\u003eRead more about how \u003ca href=\"https://android-developers.googleblog.com/2024/10/gemini-api-showcase-of-innovative-android-apps.html\" target=\"_blank\"\u003esome Android apps are already starting to leverage the Gemini API\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eTo learn more about AI on Android, check out other resources we have available during \u003ca href=\"https://android-developers.googleblog.com/2024/09/welcome-to-ai-on-android-spotlight-week.html\" target=\"_blank\"\u003eAI on Android Spotlight Week\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eUse \u003cb\u003e#AndroidAI\u003c/b\u003e hashtag to share your creations or feedback on social media, and join us at the forefront of the AI revolution!\u003c/p\u003e\n\n\u003chr/\u003e\n\u003cp\u003e\u003ci\u003eThe code snippets in this blog post have the following license:\u003c/i\u003e\u003c/p\u003e\n\u003cdiv\u003e\u003cpre\u003e// Copyright 2024 Google LLC.\n// SPDX-License-Identifier: Apache-2.0\n\u003c/pre\u003e\u003c/div\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": null,
  "modifiedTime": null
}
