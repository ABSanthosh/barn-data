{
  "id": "49cc7bd0-57e2-4444-8f8b-32bbbd17ee0b",
  "title": "Netflix App Testing At Scale",
  "link": "https://medium.com/androiddevelopers/netflix-app-testing-at-scale-eb4ef6b40124?source=rss----95b274b437c2---4",
  "description": "",
  "author": "Jose Alcérreca",
  "published": "Wed, 16 Apr 2025 15:55:49 GMT",
  "source": "https://medium.com/feed/androiddevelopers",
  "categories": [
    "androiddev",
    "case-study",
    "android",
    "testing",
    "netflix"
  ],
  "byline": "Jose Alcérreca",
  "length": 15753,
  "excerpt": "This is part of the Testing at scale series of articles where we asked industry experts to share their testing strategies. In this article, Ken Yee, Senior Engineer at Netflix, tells us about the…",
  "siteName": "Android Developers",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "Netflix App Testing At ScaleThis is part of the Testing at scale series of articles where we asked industry experts to share their testing strategies. In this article, Ken Yee, Senior Engineer at Netflix, tells us about the challenges of testing a playback app at a massive scale and how they have evolved the testing strategy since the app was created 14 years ago!IntroductionTesting at Netflix continuously evolves. In order to fully understand where it’s going and why it’s in its current state, it’s also important to understand the historical context of where it has been.The Android app was started 14 years ago. It was originally a hybrid application (native+webview), but it was converted over to a fully native app because of performance issues and the difficulty in being able to create a UI that felt/acted truly native. As with most older applications, it’s in the process of being converted to Jetpack Compose. The current codebase is roughly 1M lines of Java/Kotlin code spread across 400+ modules and, like most older apps, there is also a monolith module because the original app was one big module. The app is handled by a team of approximately 50 people.At one point, there was a dedicated mobile SDET (Software Developer Engineer in Test) team that handled writing all device tests by following the usual flow of working with developers and product managers to understand the features they were testing to create test plans for all their automation tests. At Netflix, SDETs were developers with a focus on testing; they wrote Automation tests with Espresso or UIAutomator; they also built frameworks for testing and integrated 3rd party testing frameworks. Feature Developers wrote unit tests and Robolectric tests for their own code. The dedicated SDET team was disbanded a few years ago and the automation tests are now owned by each of the feature subteams; there are still 2 supporting SDETs who help out the various teams as needed. QA (Quality Assurance) manually tests releases before they are uploaded as a final “smoke test”.In the media streaming world, one interesting challenge is the huge ecosystem of playback devices using the app. We like to support a good experience on low memory/slow devices (e.g. Android Go devices) while providing a premium experience on higher end devices. For foldables, some don’t report a hinge sensor. We support devices back to Android 7.0 (API24), but we’re setting our minimum to Android 9 soon. Some manufacturer-specific versions of Android also have quirks. As a result, physical devices are a huge part of our testingCurrent Testing ApproachAs mentioned, feature developers now handle all aspects of testing their features. Our testing layers look like this:Test Pyramid showing layers from bottom to top of: unit tests, screenshot tests, E2E automation tests, smoke testsHowever, because of our heavy usage of physical device testing and the legacy parts of the codebase, our testing pyramid looks more like an hourglass or inverted pyramid depending on which part of the code you’re in. New features do have this more typical testing pyramid shape.Our screenshot testing is also done at multiple levels: UI component, UI screen layout, and device integration screen layout. The first two are really unit tests because they don’t make any network calls. The last is a substitute for most manual QA testing.Unit Test FrameworksUnit tests are used to test business logic that is not dependent on any specific device/UI behavior. In older parts of the app, we use RxJava for asynchronous code and streams are tested. Newer parts of the app use Kotlin Flows and Composables for state flows which are much easier to reason about and test compared to RxJava.Frameworks we use for unit testing are:Strikt: for assertions because it has a fluent API like AssertJ but is written for KotlinTurbine: for the missing pieces in testing Kotlin FlowsMockito: for mocking any complex classes not relevant for the current Unit of code being testedHilt: for substituting test dependencies in our Dependency Injection graphRobolectric: for testing business logic that has to interact in some way with Android services/classes (e.g., parcelables or Services)A/B test/feature flag framework: allows overriding an automation test for a specific A/B test or enabling/disabling a specific featureDevelopers are encouraged to use plain unit tests before switching to Hilt or Robolectric because execution time goes up 10x with each step when going from plain unit tests -\u003e Hilt -\u003e Robolectric. Mockito also slows down builds when using inline mocks, so inline mocks are discouraged. Device tests are multiple orders of magnitude slower than any of these types unit tests. Speed of testing is important in large codebases.Flakiness in Unit TestsBecause unit tests are blocking in our CI pipeline, minimizing flakiness is extremely important. There are generally two causes for flakiness: leaving some state behind for the next test and testing asynchronous code.JVM (Java Virtual Machine) Unit test classes are created once and then the test methods in each class are called sequentially; instrumented tests in comparison are run from the start and the only time you can save is APK installation. Because of this, if a test method leaves some changed global state behind in dependent classes, the next test method can fail. Global state can take many forms including files on disk, databases on disk, and shared classes. Using dependency injection or recreating anything that is changed solves this issue.With asynchronous code, flakiness can always happen as multiple threads change different things. Test Dispatchers (Kotlin Coroutines) or Test Schedulers (RxJava) can be used to control time in each thread to make things deterministic when testing a specific race condition. This will make the code less realistic and possibly miss some test scenarios, but will prevent flakiness in the tests.Screenshot Test FrameworksScreenshot testing frameworks are important because they test what is seen vs. testing behavior. As a result, they are the best replacement for manual QA testing of any screens that are static (animations are still difficult to test with most screenshot testing frameworks unless the framework can control time).We use a variety of frameworks for screenshot testing:Paparazzi: for Compose UI components and screen layouts; network calls can’t be made to download images, so you have to use static image resources or an image loader that draws a pattern for the requested images (we do both)Localization screenshot testing: captures screenshots of screens in the running app in all locales for our UX teams to verify manuallyDevice screenshot testing: device testing used to test visual behavior of the running appEspresso accessibility testing: this is also a form of screenshot testing where the sizes/colors of various elements are checked for accessibility; this has also been somewhat of a pain point for us because our UX team has adopted the WCAG 44dp standard for minimum touch size instead of Android’s 48dp.Device Test FrameworksFinally, we have device tests. As mentioned, these are magnitudes slower than tests that can run on the JVM. They are a replacement for manual QA and used to smoke test the overall functionality of the app.However, since running a fully working app in a test has external dependencies (backend, network infra, lab infra), the device tests will always be flaky in some way. This cannot be emphasized enough: despite having retries, device automation tests will always be flaky over an extended period of time. Further below, we’ll cover what we do to handle some of this flakiness.We use these frameworks for device testing:Espresso: majority of device tests use Espresso which is Android’s main instrumentation testing framework for user interfacesPageObject test framework: internal screens are written as PageObjects that tests can control to ease migration from XML layouts to Compose (see below for more details)UIAutomator: a small “smoke test” set of tests uses UIAutomator to test the fully obfuscated binary that will get uploaded to the app store (a.k.a., Release Candidate tests)Performance testing framework: measures load times of various screens to check for any regressionsNetwork capture/playback framework: allows playback of recorded API calls to reduce instability of device testsBackend mocking framework: tests can ask the backend to return specific results; for example, our home page has content that is entirely driven by recommendation algorithms so a test can’t deterministically look for specific titles unless the test asks the backend to return specific videos in specific states (e.g. “leaving soon”) and specific rows filled with specific titles (e.g. a Coming Soon row with specific videos)A/B test/feature flag framework: allows overriding an automation test for a specific A/B test or enabling/disabling a specific featureAnalytics testing framework: used to verify a sequence of analytics events from a set of screen actions; analytics are the most prone to breakage when screens are changed so this is an important thing to test.PageObjects and Test StepsThe PageObject design pattern started as a web pattern, but has been applied to mobile testing. It separates test code (e.g. click on Play button) from screen-specific code (e.g. the mechanics of clicking on a button using Espresso). Because of this, it lets you abstract the test from the implementation (think interfaces vs. implementation when writing code). You can easily replace the implementation as needed when migrating from XML Layouts to Jetpack Compose layouts but the test itself (e.g. testing login) stays the same.In addition to using PageObjects to define an abstraction over screens, we have a concept of “Test Steps”. A test is composed of test steps. At the end of each step, our device lab infra will automatically create a screenshot. This gives developers a storyboard of screenshots that show the progress of the test. When a test step fails, it’s also clearly indicated (e.g., “could not click on Play button”) because a test step has a “summary” and “error description” field.Device Automation InfrastructureInside of a device lab cageNetflix was probably one of the first companies to have a dedicated device testing lab; this was before 3rd party services like Firebase Test Lab were available. Our lab infrastructure has a lot of features you’d expect to be able to do:Target specific types of devicesCapture video from running a testCapture screenshots while running a testCapture all logsInteresting device tooling features that are uniquely Netflix:Cellular tower so we can test wifi vs. cellular connections; Netflix has their own physical cellular tower in the lab that the devices are configured to connect to.Network conditioning so slow networks can be simulatedAutomatic disabling of system updates to devices so they can be locked at a specific OS levelOnly uses raw adb commands to install/run tests (all this infrastructure predates frameworks like Gradle Managed Devices or Flank)Running a suite of automated tests against an A/B testsTest hardware/software for verifying that a device doesn’t drop frames for our partners to verify their devices support Netflix playback properly; we also have a qualification program for devices to make sure they support HDR and other codecs properly.If you’re curious about more details, look at Netflix’ tech blog.Handling Test FlakinessAs mentioned above, test flakiness is one of the hardest things about inherently unstable device tests. Tooling has to be built to:Minimize flakinessIdentify causes of flakesNotify teams that own the flaky testsTooling that we’ve built to manage the flakiness:Automatically identifies the PR (Pull Request) batch that a test started to fail in and notifies PR authors that they caused a test failureTests can be marked stable/unstable/disabled instead of using @Ignore annotations; this is used to disable a subset of tests temporarily if there’s a backend issue so that false positives are not reported on PRsAutomation that figures out whether a test can be promoted to Stable by using spare device cycles to automatically evaluate test stabilityAutomated IfTTT (If This Then That) rules for retrying tests or ignoring temporary failures or repairing a deviceFailure report let us easily filter failures according to what device maker, OS, or cage the device is in, e.g. this shows how often a test fails over a period of time for these environmental factors:Test failures over time grouped by environmental factors like staging/prod backend, OS version, phone/tabletFailure report lets us triage error history to identify the most common failure reasons for a test along with screenshots:Tests can be manually set up to run multiple times across devices or OS versions or device types (phone/tablet) to reproduce flaky testsCI/Testing PipelinesWe have a typical PR (Pull Request) CI pipeline that runs unit tests (includes Paparazzi and Robolectric tests), lint, ktLint, and Detekt. Running roughly 1000 device tests is part of the PR process. In a PR, a subset of smoke tests is also run against the fully obfuscated app that can be shipped to the app store (the previous device tests run against a partially obfuscated app).Extra device automation tests are run as part of our post-merge suite. Whenever batches of PRs are merged, there is additional coverage provided by automation tests that cannot be run on PRs because we try to keep the PR device automation suite under 30 minutes.In addition, there are Daily and Weekly suites. These are run for much longer automation tests because we try to keep our post-merge suite under 120 minutes. Automation tests that go into these are typically long running stress tests (e.g., can you watch a season of a series without the app running out of memory and crashing?).In a perfect world, you have infinite resources to do all your testing. If you had infinite devices, you could run all your device tests in parallel. If you had infinite servers, you could run all your unit tests in parallel. If you had both, you could run everything on every PR. But in the real world, you have a balanced approach that runs “enough” tests on PRs, postmerge, etc. to prevent issues from getting out into the field so your customers have a better experience while also keeping your teams productive.CoverageCoverage on devices is a set of tradeoffs. On PRs, you want to maximize coverage but minimize time. On post-merge/Daily/Weekly, time is less important.When testing on devices, we have a two dimensional matrix of OS version vs. device type (phone/tablet). Layout issues are fairly common, so we always run tests on phone+tablet. We are still adding automation to foldables, but they have their own challenges like being able to test layouts before/after/during the folding process.On PRs, we normally run what we call a “narrow grid” which means a test can run on any OS version. On Postmerge/Daily/Weekly, we run what we call a “full grid” which means a test runs on every OS version. The tradeoff is that if there is an OS-specific failure, it can look like a flaky test on a PR and won’t be detected until later.Future of TestingTesting continuously evolves as you learn what works or new technologies and frameworks become available. We’re currently evaluating using emulators to speed up our PRs. We’re also evaluating using Roborazzi to reduce device-based screenshot testing; Roborazzi allows testing of interactions while Paparazzi does not. We’re building up a modular “demo app” system that allows for feature-level testing instead of app-level testing. Improving app testing never ends…",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*O4vuzNcTGYsZfJNKi_uuqA.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"a637\" data-testid=\"storyTitle\"\u003eNetflix App Testing At Scale\u003c/h2\u003e\u003cdiv\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca rel=\"noopener follow\" href=\"https://medium.com/@JoseAlcerreca?source=post_page---byline--eb4ef6b40124---------------------------------------\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Jose Alcérreca\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*77Cp-wcWyr66fqcTY8_pWw.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/androiddevelopers?source=post_page---byline--eb4ef6b40124---------------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cimg alt=\"Android Developers\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*4Tg6pPzer7cIarYaszIKaQ.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cp id=\"d9e5\"\u003e\u003cem\u003eThis is part of the \u003c/em\u003e\u003ca rel=\"noopener\" href=\"https://medium.com/androiddevelopers/introducing-testing-at-scale-blog-series-8cd300ae2795\"\u003e\u003cem\u003eTesting at scale\u003c/em\u003e\u003c/a\u003e\u003cem\u003e series of articles where we asked industry experts to share their testing strategies. In this article, \u003c/em\u003e\u003ca href=\"https://kenkyee.medium.com/\" rel=\"noopener\"\u003e\u003cstrong\u003e\u003cem\u003eKen Yee\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e\u003cem\u003e, Senior Engineer at Netflix, tells us about the challenges of testing a playback app at a massive scale and how they have evolved the testing strategy since the app was created 14 years ago!\u003c/em\u003e\u003c/p\u003e\u003ch2 id=\"b9ac\"\u003eIntroduction\u003c/h2\u003e\u003cp id=\"b86f\"\u003eTesting at Netflix continuously evolves. In order to fully understand where it’s going and why it’s in its current state, it’s also important to understand the historical context of where it has been.\u003c/p\u003e\u003cp id=\"2e83\"\u003eThe Android app was started 14 years ago. It was originally a hybrid application (native+webview), but it was converted over to a fully native app because of performance issues and the difficulty in being able to create a UI that felt/acted truly native. As with most older applications, it’s in the process of being converted to \u003ca href=\"https://developer.android.com/jetpack\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJetpack Compose\u003c/a\u003e. The current codebase is roughly 1M lines of Java/Kotlin code spread across 400+ modules and, like most older apps, there is also a monolith module because the original app was one big module. The app is handled by a team of approximately 50 people.\u003c/p\u003e\u003cp id=\"975a\"\u003eAt one point, there was a dedicated mobile SDET (\u003ca href=\"https://www.softwaretestinghelp.com/what-is-sdet/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSoftware Developer Engineer in Test\u003c/a\u003e) team that handled writing all device tests by following the usual flow of working with developers and product managers to understand the features they were testing to create test plans for all their automation tests. At Netflix, SDETs were developers with a focus on testing; they wrote Automation tests with Espresso or UIAutomator; they also built frameworks for testing and integrated 3rd party testing frameworks. Feature Developers wrote unit tests and Robolectric tests for their own code. The dedicated SDET team was disbanded a few years ago and the automation tests are now owned by each of the feature subteams; there are still 2 supporting SDETs who help out the various teams as needed. QA (Quality Assurance) manually tests releases before they are uploaded as a final “smoke test”.\u003c/p\u003e\u003cp id=\"5d7a\"\u003eIn the media streaming world, one interesting challenge is the huge ecosystem of playback devices using the app. We like to support a good experience on low memory/slow devices (e.g. Android Go devices) while providing a premium experience on higher end devices. For foldables, some don’t report a hinge sensor. We support devices back to Android 7.0 (API24), but we’re setting our minimum to Android 9 soon. Some manufacturer-specific versions of Android also have quirks. As a result, physical devices are a huge part of our testing\u003c/p\u003e\u003ch2 id=\"d966\"\u003eCurrent Testing Approach\u003c/h2\u003e\u003cp id=\"3929\"\u003eAs mentioned, feature developers now handle all aspects of testing their features. Our testing layers look like this:\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eTest Pyramid showing layers from bottom to top of: unit tests, screenshot tests, E2E automation tests, smoke tests\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"c14f\"\u003eHowever, because of our heavy usage of physical device testing and the legacy parts of the codebase, our testing pyramid looks more like an hourglass or inverted pyramid depending on which part of the code you’re in. New features do have this more typical testing pyramid shape.\u003c/p\u003e\u003cp id=\"14bb\"\u003eOur screenshot testing is also done at multiple levels: UI component, UI screen layout, and device integration screen layout. The first two are really unit tests because they don’t make any network calls. The last is a substitute for most manual QA testing.\u003c/p\u003e\u003ch2 id=\"a36d\"\u003eUnit Test Frameworks\u003c/h2\u003e\u003cp id=\"1e2a\"\u003eUnit tests are used to test business logic that is not dependent on any specific device/UI behavior. In older parts of the app, we use RxJava for asynchronous code and streams are tested. Newer parts of the app use Kotlin Flows and Composables for state flows which are much easier to reason about and test compared to RxJava.\u003c/p\u003e\u003cp id=\"ce4e\"\u003eFrameworks we use for unit testing are:\u003c/p\u003e\u003cul\u003e\u003cli id=\"7edb\"\u003e\u003ca href=\"https://strikt.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eStrikt\u003c/a\u003e: for assertions because it has a fluent API like AssertJ but is written for Kotlin\u003c/li\u003e\u003cli id=\"7340\"\u003e\u003ca href=\"https://github.com/cashapp/turbine\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTurbine\u003c/a\u003e: for the missing pieces in testing Kotlin Flows\u003c/li\u003e\u003cli id=\"f7d2\"\u003e\u003ca href=\"https://site.mockito.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMockito\u003c/a\u003e: for mocking any complex classes not relevant for the current Unit of code being tested\u003c/li\u003e\u003cli id=\"fdfb\"\u003e\u003ca href=\"https://dagger.dev/hilt/testing.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHilt\u003c/a\u003e: for substituting test dependencies in our Dependency Injection graph\u003c/li\u003e\u003cli id=\"16e0\"\u003e\u003ca href=\"https://robolectric.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRobolectric\u003c/a\u003e: for testing business logic that has to interact in some way with Android services/classes (e.g., parcelables or Services)\u003c/li\u003e\u003cli id=\"7eaa\"\u003eA/B test/feature flag framework: allows overriding an automation test for a specific A/B test or enabling/disabling a specific feature\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"7a9c\"\u003eDevelopers are encouraged to use plain unit tests before switching to Hilt or Robolectric because execution time goes up 10x with each step when going from plain unit tests -\u0026gt; Hilt -\u0026gt; Robolectric. Mockito also slows down builds when using inline mocks, so inline mocks are discouraged. Device tests are multiple orders of magnitude slower than any of these types unit tests. Speed of testing is important in large codebases.\u003c/p\u003e\u003ch2 id=\"da12\"\u003eFlakiness in Unit Tests\u003c/h2\u003e\u003cp id=\"4a50\"\u003eBecause unit tests are blocking in our CI pipeline, minimizing flakiness is extremely important. There are generally two causes for flakiness: leaving some state behind for the next test and testing asynchronous code.\u003c/p\u003e\u003cp id=\"c782\"\u003eJVM (Java Virtual Machine) Unit test classes are created once and then the test methods in each class are called sequentially; instrumented tests in comparison are run from the start and the only time you can save is APK installation. Because of this, if a test method leaves some changed global state behind in dependent classes, the next test method can fail. Global state can take many forms including files on disk, databases on disk, and shared classes. Using dependency injection or recreating anything that is changed solves this issue.\u003c/p\u003e\u003cp id=\"f137\"\u003eWith asynchronous code, flakiness can always happen as multiple threads change different things. \u003ca href=\"https://kotlinlang.org/api/kotlinx.coroutines/kotlinx-coroutines-test/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTest Dispatchers\u003c/a\u003e (Kotlin Coroutines) or \u003ca href=\"https://reactivex.io/RxJava/3.x/javadoc/io/reactivex/rxjava3/schedulers/TestScheduler.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTest Schedulers\u003c/a\u003e (RxJava) can be used to control time in each thread to make things deterministic when testing a specific race condition. This will make the code less realistic and possibly miss some test scenarios, but will prevent flakiness in the tests.\u003c/p\u003e\u003ch2 id=\"5542\"\u003eScreenshot Test Frameworks\u003c/h2\u003e\u003cp id=\"95d0\"\u003eScreenshot testing frameworks are important because they test what is seen vs. testing behavior. As a result, they are the best replacement for manual QA testing of any screens that are static (animations are still difficult to test with most screenshot testing frameworks unless the framework can control time).\u003c/p\u003e\u003cp id=\"f02b\"\u003eWe use a variety of frameworks for screenshot testing:\u003c/p\u003e\u003cul\u003e\u003cli id=\"cbb2\"\u003e\u003ca href=\"https://github.com/cashapp/paparazzi\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePaparazzi\u003c/a\u003e: for Compose UI components and screen layouts; network calls can’t be made to download images, so you have to use static image resources or an image loader that draws a pattern for the requested images (we do both)\u003c/li\u003e\u003cli id=\"cbf7\"\u003eLocalization screenshot testing: captures screenshots of screens in the running app in all locales for our UX teams to verify manually\u003c/li\u003e\u003cli id=\"5fbb\"\u003eDevice screenshot testing: device testing used to test visual behavior of the running app\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"8ebb\"\u003e\u003ca href=\"https://developer.android.com/training/testing/espresso/accessibility-checking\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEspresso accessibility testing\u003c/a\u003e: this is also a form of screenshot testing where the sizes/colors of various elements are checked for accessibility; this has also been somewhat of a pain point for us because our UX team has adopted the WCAG 44dp standard for minimum touch size instead of Android’s 48dp.\u003c/p\u003e\u003ch2 id=\"cc6d\"\u003eDevice Test Frameworks\u003c/h2\u003e\u003cp id=\"f6bc\"\u003eFinally, we have device tests. As mentioned, these are magnitudes slower than tests that can run on the JVM. They are a replacement for manual QA and used to smoke test the overall functionality of the app.\u003c/p\u003e\u003cp id=\"bf35\"\u003eHowever, since running a fully working app in a test has external dependencies (backend, network infra, lab infra), the device tests will always be flaky in some way. This cannot be emphasized enough: despite having retries, \u003cstrong\u003edevice automation tests will always be flaky over an extended period of time\u003c/strong\u003e. Further below, we’ll cover what we do to handle some of this flakiness.\u003c/p\u003e\u003cp id=\"fd68\"\u003eWe use these frameworks for device testing:\u003c/p\u003e\u003cul\u003e\u003cli id=\"8b8a\"\u003e\u003ca href=\"https://developer.android.com/training/testing/espresso\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEspresso\u003c/a\u003e: majority of device tests use Espresso which is Android’s main instrumentation testing framework for user interfaces\u003c/li\u003e\u003cli id=\"9052\"\u003ePageObject test framework: internal screens are written as \u003ca href=\"https://martinfowler.com/bliki/PageObject.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePageObjects\u003c/a\u003e that tests can control to ease migration from XML layouts to Compose (see below for more details)\u003c/li\u003e\u003cli id=\"69e9\"\u003e\u003ca href=\"https://developer.android.com/training/testing/other-components/ui-automator\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eUIAutomator\u003c/a\u003e: a small “smoke test” set of tests uses UIAutomator to test the fully obfuscated binary that will get uploaded to the app store (a.k.a., \u003ca href=\"https://developer.android.com/training/testing/fundamentals/strategies#scalable-testing\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRelease Candidate tests\u003c/a\u003e)\u003c/li\u003e\u003cli id=\"2250\"\u003ePerformance testing framework: measures load times of various screens to check for any regressions\u003c/li\u003e\u003cli id=\"6521\"\u003eNetwork capture/playback framework: allows playback of recorded API calls to reduce instability of device tests\u003c/li\u003e\u003cli id=\"030e\"\u003eBackend mocking framework: tests can ask the backend to return specific results; for example, our home page has content that is entirely driven by recommendation algorithms so a test can’t deterministically look for specific titles unless the test asks the backend to return specific videos in specific states (e.g. “leaving soon”) and specific rows filled with specific titles (e.g. a Coming Soon row with specific videos)\u003c/li\u003e\u003cli id=\"ce0c\"\u003eA/B test/feature flag framework: allows overriding an automation test for a specific A/B test or enabling/disabling a specific feature\u003c/li\u003e\u003cli id=\"9b52\"\u003eAnalytics testing framework: used to verify a sequence of analytics events from a set of screen actions; analytics are the most prone to breakage when screens are changed so this is an important thing to test.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"9661\"\u003ePageObjects and Test Steps\u003c/h2\u003e\u003cp id=\"6f42\"\u003eThe \u003ca href=\"https://martinfowler.com/bliki/PageObject.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePageObject design pattern\u003c/a\u003e started as a web pattern, but has been applied to mobile testing. It separates test code (e.g. click on Play button) from screen-specific code (e.g. the mechanics of clicking on a button using Espresso). Because of this, it lets you abstract the test from the implementation (think interfaces vs. implementation when writing code). You can easily replace the implementation as needed when migrating from XML Layouts to Jetpack Compose layouts but the test itself (e.g. testing login) stays the same.\u003c/p\u003e\u003cp id=\"5b3f\"\u003eIn addition to using PageObjects to define an abstraction over screens, we have a concept of “Test Steps”. A test is composed of test steps. At the end of each step, our device lab infra will automatically create a screenshot. This gives developers a storyboard of screenshots that show the progress of the test. When a test step fails, it’s also clearly indicated (e.g., “could not click on Play button”) because a test step has a “summary” and “error description” field.\u003c/p\u003e\u003ch2 id=\"93ab\"\u003eDevice Automation Infrastructure\u003c/h2\u003e\u003cfigure\u003e\u003cfigcaption\u003eInside of a device lab cage\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"cbec\"\u003eNetflix was probably one of the first companies to have a dedicated device testing lab; this was before 3rd party services like Firebase Test Lab were available. Our lab infrastructure has a lot of features you’d expect to be able to do:\u003c/p\u003e\u003cul\u003e\u003cli id=\"6704\"\u003eTarget specific types of devices\u003c/li\u003e\u003cli id=\"0a28\"\u003eCapture video from running a test\u003c/li\u003e\u003cli id=\"0136\"\u003eCapture screenshots while running a test\u003c/li\u003e\u003cli id=\"a532\"\u003eCapture all logs\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"1cbf\"\u003eInteresting device tooling features that are uniquely Netflix:\u003c/p\u003e\u003cul\u003e\u003cli id=\"3b17\"\u003eCellular tower so we can test wifi vs. cellular connections; Netflix has their own physical cellular tower in the lab that the devices are configured to connect to.\u003c/li\u003e\u003cli id=\"e870\"\u003eNetwork conditioning so slow networks can be simulated\u003c/li\u003e\u003cli id=\"a125\"\u003eAutomatic disabling of system updates to devices so they can be locked at a specific OS level\u003c/li\u003e\u003cli id=\"fa67\"\u003eOnly uses raw adb commands to install/run tests (all this infrastructure predates frameworks like \u003ca href=\"https://developer.android.com/studio/test/gradle-managed-devices\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGradle Managed Devices\u003c/a\u003e or \u003ca href=\"https://github.com/Flank/flank\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFlank\u003c/a\u003e)\u003c/li\u003e\u003cli id=\"7ff9\"\u003eRunning a suite of automated tests against an A/B tests\u003c/li\u003e\u003cli id=\"0c84\"\u003eTest hardware/software for verifying that a device doesn’t drop frames for our partners to verify their devices support Netflix playback properly; we also have a qualification program for devices to make sure they support HDR and other codecs properly.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"9d19\"\u003eIf you’re curious about more details, look at \u003ca href=\"https://netflixtechblog.com/automated-testing-on-devices-fc5a39f47e24\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNetflix’ tech blog\u003c/a\u003e.\u003c/p\u003e\u003ch2 id=\"ec3e\"\u003eHandling Test Flakiness\u003c/h2\u003e\u003cp id=\"e31b\"\u003eAs mentioned above, test flakiness is one of the hardest things about inherently unstable device tests. Tooling has to be built to:\u003c/p\u003e\u003cul\u003e\u003cli id=\"f6cf\"\u003eMinimize flakiness\u003c/li\u003e\u003cli id=\"3ffe\"\u003eIdentify causes of flakes\u003c/li\u003e\u003cli id=\"11e8\"\u003eNotify teams that own the flaky tests\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"f095\"\u003eTooling that we’ve built to manage the flakiness:\u003c/p\u003e\u003cul\u003e\u003cli id=\"1619\"\u003eAutomatically identifies the PR (Pull Request) batch that a test started to fail in and notifies PR authors that they caused a test failure\u003c/li\u003e\u003cli id=\"20f3\"\u003eTests can be marked stable/unstable/disabled instead of using @Ignore annotations; this is used to disable a subset of tests temporarily if there’s a backend issue so that false positives are not reported on PRs\u003c/li\u003e\u003cli id=\"34fd\"\u003eAutomation that figures out whether a test can be promoted to Stable by using spare device cycles to automatically evaluate test stability\u003c/li\u003e\u003cli id=\"256d\"\u003eAutomated IfTTT (If This Then That) rules for retrying tests or ignoring temporary failures or repairing a device\u003c/li\u003e\u003cli id=\"4317\"\u003eFailure report let us easily filter failures according to what device maker, OS, or cage the device is in, e.g. this shows how often a test fails over a period of time for these environmental factors:\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cfigcaption\u003eTest failures over time grouped by environmental factors like staging/prod backend, OS version, phone/tablet\u003c/figcaption\u003e\u003c/figure\u003e\u003cul\u003e\u003cli id=\"67ab\"\u003eFailure report lets us triage error history to identify the most common failure reasons for a test along with screenshots:\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cul\u003e\u003cli id=\"0063\"\u003eTests can be manually set up to run multiple times across devices or OS versions or device types (phone/tablet) to reproduce flaky tests\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"82e7\"\u003eCI/Testing Pipelines\u003c/h2\u003e\u003cp id=\"e982\"\u003eWe have a typical PR (Pull Request) CI pipeline that runs unit tests (includes Paparazzi and Robolectric tests), lint, ktLint, and Detekt. Running roughly 1000 device tests is part of the PR process. In a PR, a subset of smoke tests is also run against the fully obfuscated app that can be shipped to the app store (the previous device tests run against a partially obfuscated app).\u003c/p\u003e\u003cp id=\"dfde\"\u003eExtra device automation tests are run as part of our post-merge suite. Whenever batches of PRs are merged, there is additional coverage provided by automation tests that cannot be run on PRs because we try to keep the PR device automation suite under 30 minutes.\u003c/p\u003e\u003cp id=\"0ee3\"\u003eIn addition, there are Daily and Weekly suites. These are run for much longer automation tests because we try to keep our post-merge suite under 120 minutes. Automation tests that go into these are typically long running stress tests (e.g., can you watch a season of a series without the app running out of memory and crashing?).\u003c/p\u003e\u003cp id=\"e400\"\u003eIn a perfect world, you have infinite resources to do all your testing. If you had infinite devices, you could run all your device tests in parallel. If you had infinite servers, you could run all your unit tests in parallel. If you had both, you could run everything on every PR. But in the real world, you have a balanced approach that runs “enough” tests on PRs, postmerge, etc. to prevent issues from getting out into the field so your customers have a better experience while also keeping your teams productive.\u003c/p\u003e\u003ch2 id=\"094f\"\u003eCoverage\u003c/h2\u003e\u003cp id=\"4c40\"\u003eCoverage on devices is a set of tradeoffs. On PRs, you want to maximize coverage but minimize time. On post-merge/Daily/Weekly, time is less important.\u003c/p\u003e\u003cp id=\"461a\"\u003eWhen testing on devices, we have a two dimensional matrix of OS version vs. device type (phone/tablet). Layout issues are fairly common, so we always run tests on phone+tablet. We are still adding automation to foldables, but they have their own challenges like being able to test layouts before/after/during the folding process.\u003c/p\u003e\u003cp id=\"ee5b\"\u003eOn PRs, we normally run what we call a “narrow grid” which means a test can run on any OS version. On Postmerge/Daily/Weekly, we run what we call a “full grid” which means a test runs on every OS version. The tradeoff is that if there is an OS-specific failure, it can look like a flaky test on a PR and won’t be detected until later.\u003c/p\u003e\u003ch2 id=\"f0f2\"\u003eFuture of Testing\u003c/h2\u003e\u003cp id=\"80d2\"\u003eTesting continuously evolves as you learn what works or new technologies and frameworks become available. We’re currently evaluating using emulators to speed up our PRs. We’re also evaluating using Roborazzi to reduce device-based screenshot testing; Roborazzi allows testing of interactions while Paparazzi does not. We’re building up a modular “demo app” system that allows for feature-level testing instead of app-level testing. Improving app testing never ends…\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "17 min read",
  "publishedTime": "2025-04-16T15:55:48.971Z",
  "modifiedTime": null
}
