{
  "id": "bdb2372b-2ffa-4e2a-a547-4ee8bade0c02",
  "title": "Personal Data Classification",
  "link": "https://medium.com/airbnb-engineering/personal-data-classification-2d816d8ea516?source=rss----53c7c27702d5---4",
  "description": "",
  "author": "Sam Kim",
  "published": "Mon, 19 Aug 2024 16:43:29 GMT",
  "source": "https://medium.com/feed/airbnb-engineering",
  "categories": [
    "privacy",
    "data",
    "engineering",
    "security",
    "technology"
  ],
  "byline": "Sam Kim",
  "length": 15260,
  "excerpt": "Airbnb is built on trust. One key way we maintain trust with our community is by ensuring that personal data is handled with care, in a manner that meets security, privacy, and compliance…",
  "siteName": "The Airbnb Tech Blog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "An Important Foundation For Security, Privacy, and Compliance at AirbnbBy: Sam Kim, Alex Klimov, Woody Zhou, Sylvia Tomiyama, Aniket Arondekar, Ansuman AcharyaIntroductionAirbnb is built on trust. One key way we maintain trust with our community is by ensuring that personal data is handled with care, in a manner that meets security, privacy, and compliance requirements. Understanding where and what personal data exists is foundational to this.Over the past several years, we’ve built our own data classification system that adapts to the needs of our data ecosystem, to streamline our processes, and further unlock our ability to protect the data entrusted to Airbnb. This was made possible by many teams working closely to achieve this overarching, shared objective. Information Security, Privacy, Data Governance, Legal, and Engineering collaborated to tackle this problem holistically to produce a unified data identification and classification strategy across all data stores.In this blog, we will shed light on the complexities of how data classification works at Airbnb, what measurements we set to assess the quality, performance, and accuracy of the systems involved, and the important considerations when building a data classification system. We hope to share insights for others that are facing similar challenges and to provide a framework for how data classification systems can be built at scale.The Complexities of Data Classification at AirbnbData classification is the process of identifying where data exists and then organizing, detecting, and annotating that data based on a taxonomy. At Airbnb, we have established a Personal Data Taxonomy Council to define the taxonomy for personal data and to refine it over time. This taxonomy breaks down personal data into various data elements that are relevant for our ecosystem such as email address, physical address, and guest names. Once data is annotated with its applicable personal data element(s), various enforcement systems use these annotations to ensure personal data is handled according to our Security and Privacy policies. In this blog post, we will focus primarily on the data classification workflow and not each type of enforcement use case.The workflow can be classified into three pillars:Catalog: What data do we have?Detection: What data do we suspect is personal data?Reconciliation: Which classification do we choose?Personal Data Classification FlowLet’s dig deeper into how each of these form the backbone of data classification.CatalogCataloging involves building a dynamic, accurate, and scalable system to first identify where data exists and then organize the whole inventory. Cataloging is akin to mapping the data landscape or organizing a library. It involves dynamically discovering new data, enriching it with metadata from various sources, and manually inputting information. This process is crucial for enforcing data policies, accurately classifying data, and assigning it to the correct owners.Automated and Dynamic Discovery: Automation makes the cataloging process scalable and efficient. For the variety of data stores that Airbnb uses, such as production and analytical databases, object stores, and cloud storage, our catalogs connect to them and dynamically fetch the full inventory of data. Either through stream or batch processing, they dynamically update to reflect new and changed data. This ensures the catalog is a reliable and accurate source of truth.Complexity and Diversity in Data Sources: The challenge of cataloging stems from the variety and complexity of data sources, including different formats and locations. Our cataloging systems fetch metadata in several ways: through direct API calls or by crawling schemas in formats like thrift, JSON, yaml, or config files, accommodating the diverse nature of modern data storage.For search and discovery, many of our data entities are surfaced in the data management platform, Metis. This helps the data owners quickly answer questions such as which data contains personal data, who owns the data, and which controls are in place.Catalog UIDetectionFor personal data detection, we use the in-house automated detection service in our Data Protection Platform which was built to protect data in compliance with global regulations and security requirements. As our own taxonomy grows, we have expanded our capabilities and made the service easily extensible to detect all other types of personal data elements and personal Airbnb IDs.Detection engineFor each data entity stored in the catalogs, scanning jobs are scheduled through a message queue, which then samples data and runs through our list of classifiers. Recognizing the need for periodic classifier updates, the detection engine was designed for simplicity and flexibility. Since its inception, our detection engine has upgraded to include additional steps and adopted the approach of configuration-driven development. The majority of the logic of the detection engine has been rewritten as simpler configurations to increase the speed of iterating on existing classifiers, improve testing, and enable quick development of new features.The detection engine can be seen as a pipeline, which involves the scanner, validator, and thresholding.Detection EngineScanner: The scanner classifies personal data using metadata and content, employing methods like regex for emails and keyword lists for cities, and advanced machine learning models for complex data types requiring contextual understanding.Validator: Sampled data matching a scanner undergoes a customizable validation step to enhance classifier accuracy, verifying details like latitude/longitude ranges or custom ciphertexts from encryption services.Thresholding: To reduce noise, thresholding is applied before storing results, varying by data structure type (e.g., matched rows vs. findings in a document) and set based on historical data frequency and criticality.With the revamped pipeline, this has resulted in a significant decrease in false positive findings and reduced the burden on data owners to verify every result, which has historically impeded their productivity.ReconciliationNot every detection surfaced may be correct or, in other cases, more context may be required. Therefore, we employ a human-in-the-loop strategy: where data owners confirm the classifications. This step is critical in ensuring these classifications are correct before any data policies are automatically enforced to protect our data.Automated NotificationsFor compliance, we have an automated notification system that issues tickets whenever personal data is detected. These get surfaced to the appropriate data or service owners with strict SLAs.For data entities that have schemas defined in code, such as transactional tables from production services (online), Amazon S3 buckets, or tables that are exported to our data warehouse, we assist the developers by automatically creating code changes that update their table schemas with the detected personal data elements.Enforcing resolutionTo enforce resolution on these tickets, tables are automatically access controlled in the data warehouse when the tickets are not resolved within SLA. Additionally reviews are conducted to ensure our classifications are correct for data where its handling requirements apply.Tracking actions taken on these tickets for when personal data is detected has been important to assess the quality of our data classification flow and to keep an audit trail of past detections. It also highlights points of friction developers face when resolving these tickets. The investments we have made in this area have continued to improve the process and reduce the time needed to resolve tickets each year.Assessing Quality of a Data Classification SystemBecause of the complexity of the system and its sub-components, this presented a unique challenge as we strive to define what quality means for the entire system. To build with the long-term in mind, we evaluated how well our entire data classification system functions as a whole.We’ve set up measurements to assess quality of our data classification in three categories:Recall: This measures our coverage and ability to not miss where personal data may exist, crucial for protecting the stored personal data. We assess recall through:Number of data entities integrated in the data classification systemVolume of personal data that exists from all different sourcesTypes of personal data being annotated and automatically detected against our taxonomyPrecision: This evaluates the accuracy of our data classifications, vital for data owners tagging their data. High precision minimizes tagging friction. Precision is measured by:Tracking false positive rates of classifiers for each type of personal dataTracking ticket resolutions made by data owners, which also aids in understanding nuanced classification casesSpeed: This gauges the efficiency of identifying and classifying personal data, aiming to minimize compliance risks. Speed is measured by:Time it takes for the detection engine for scanning new data entitiesTime it takes for data owners to reconcile classifications and resolve ticketsThe frequency of data tagging at creation by data ownersThese measurements ensure our data classification system is effective, accurate, and efficient, safeguarding our personal data.Considerations for Building a Data Classification SystemIt is important to be aware of issues that may be present with the outlined approach in general. Below are some challenges that we’ve considered when building a data classification system:Post-Processing Classification: The outlined approach mostly relies on post-processing classification, which means that schema information is added after data has been collected and stored. In a modern data world where data and metadata are constantly changing, post-processing cannot catch up with data evolution.Inconsistent Classifications: Data generally flows from online to offline through ETL (extract, transform, and load) processes, and then reverse ETLing back to the online world. However, data classification that is performed independently in both worlds can lead to inconsistent classifications.Waste of Process Cost: Duplicate annotations can be made for the same data in the online and offline domains, which might result in increased costs for data classification processes.To address these challenges, we describe the process of “shifting left” with data classification and how we started to push developers to annotate their data at the beginning of the data lifecycle.Shifting LeftInstead of thinking about governance and data classification as an activity that happens post-hoc, we’ve started to embed the annotation process directly into data schemas as they are being created and updated. This enables us to:Shift Classification from Data to Schema: The schema annotation process takes place earlier in the data lifecycle at the point of data collection. This keeps annotations updated as data evolves and ensures data is annotated before collection and consumption, allowing for immediate policy enforcement.Shift Classification from Offline to Online: Traditionally done offline, data classification is now integrated into production services, ensuring data is structured and formatted correctly from the start. Leveraging data lineage information enables automated annotation, reducing the need for manual effort and lowering process costs.Shift from Data Steward to Data Owner: Oftentimes, stewardship, or the responsible management and oversight, of the data is conducted by people who are downstream of data creation, such as data consumers or governance professionals. This change shifts stewardship to the data producers, merging the roles of data steward and data owner. This empowers the team that owns the data to manage it more effectively and scale operations.Schema Annotation EnforcementFocusing on our most crucial online data, we have started executing on shifting left by directly integrating with our internal schema definition language that is known for its annotation capabilities. We now mandate that developers include personal data annotations at the source when creating new data models, providing guidance on accurate tagging. This requirement is enforced through checks that run in our CI/CD pipelines which:Automatically suggest data elements: Based on the schema’s metadata, we automatically detect the data elements for all fields defined in the schema with our detection service.Validate data elements: Annotations are validated against our own taxonomy and schemas are enforced and all fields are annotated, even when it is not considered personal.Warn about downstream impact: We notify data owners when annotations can impact downstream services such as offline data pipelines and direct them to the proper resources for handling.Schema Annotation EnforcementWhile shifting left has significantly helped to push classifying data earlier and increase coverage of schema annotations, this does not discount the importance of the rest of the classification process. Classifications that happen post-process are necessary for instance in cases where data storages that do not include well-defined schemas. Therefore, continued investments are still needed in detection and reconciliation to cover areas that cannot be shifted left and to verify annotations that may have already been made by owners as a second layer of protection.Conclusion/Lessons LearnedThe Airbnb data classification framework has been successful in advancing data management, security, and privacy. Reflecting on the journey, it has offered invaluable insights that have shaped our methodologies. Key takeaways include:Adopting a unified strategy for classifying online and offline personal data to streamline processesImplementing a ‘Shift Left’ approach to engage with data owners early in the development cycleAddressing classification uncertainties through clear guidelines and decision-makingEnhancing education and training initiatives for data owners and consumersAs the data landscape continues to rapidly change, these lessons will guide future data classification efforts and ensure continued trust and protection of customer data.AcknowledgementsOur data classification strategy has evolved over many years and we’ve been able to quickly adapt and iterate thanks to our decision to build an in-house solution. Security, privacy, and compliance are of utmost importance at Airbnb, and this work would not be possible without the contribution of many of our cross-functional partners and leaders.This includes, but are not limited to: Bill Meng, Aravind Selvan, Juan Tamayo, Xiao Chen, Pinyao Guo, Wendy Jin, Liam McInerney, Pat Moynahan, Gabriel Gejman, Marc Blanchou, Brendon Lynch, and many others.If this type of work interests you, check out some of our related positions at Careers at Airbnb or check out more resources in the Airbnb Tech Blog!All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*UWzBCWsCQWkrzLjV",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca rel=\"noopener follow\" href=\"https://medium.com/@samhykim?source=post_page-----2d816d8ea516--------------------------------\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Sam Kim\" src=\"https://miro.medium.com/v2/resize:fill:88:88/2*N5yziCtlgAIcagB8gcmuGQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://medium.com/airbnb-engineering?source=post_page-----2d816d8ea516--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"The Airbnb Tech Blog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*MlNQKg-sieBGW5prWoe9HQ.jpeg\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"99fa\"\u003eAn Important Foundation For Security, Privacy, and Compliance at Airbnb\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"ea0a\"\u003e\u003cstrong\u003eBy:\u003c/strong\u003e \u003ca href=\"https://www.linkedin.com/in/samhykim/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSam Kim\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/alexander-klimov-8521599/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAlex Klimov\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/woodyzhou/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eWoody Zhou\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/sylviatomiyama/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSylvia Tomiyama\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/aniket-arondekar/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAniket Arondekar\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/ansumanacharya/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAnsuman Acharya\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"1ad9\"\u003eIntroduction\u003c/h2\u003e\u003cp id=\"378b\"\u003e\u003cstrong\u003eAirbnb is built on trust\u003c/strong\u003e. One key way we maintain trust with our community is by ensuring that personal data is handled with care, in a manner that meets security, privacy, and compliance requirements. Understanding where and what personal data exists is foundational to this.\u003c/p\u003e\u003cp id=\"8f4d\"\u003eOver the past several years, we’ve built our own data classification system that adapts to the needs of our data ecosystem, to streamline our processes, and further unlock our ability to protect the data entrusted to Airbnb. This was made possible by many teams working closely to achieve this overarching, shared objective. Information Security, Privacy, Data Governance, Legal, and Engineering collaborated to tackle this problem holistically to produce a unified data identification and classification strategy across all data stores.\u003c/p\u003e\u003cp id=\"1859\"\u003eIn this blog, we will shed light on the complexities of how data classification works at Airbnb, what measurements we set to assess the quality, performance, and accuracy of the systems involved, and the important considerations when building a data classification system. We hope to share insights for others that are facing similar challenges and to provide a framework for how data classification systems can be built at scale.\u003c/p\u003e\u003ch2 id=\"ffca\"\u003eThe Complexities of Data Classification at Airbnb\u003c/h2\u003e\u003cp id=\"a6e6\"\u003eData classification is the process of identifying where data exists and then organizing, detecting, and annotating that data based on a taxonomy. At Airbnb, we have established a Personal Data Taxonomy Council to define the taxonomy for personal data and to refine it over time. This taxonomy breaks down personal data into various data elements that are relevant for our ecosystem such as email address, physical address, and guest names. Once data is annotated with its applicable personal data element(s), various enforcement systems use these annotations to ensure personal data is handled according to our Security and Privacy policies. In this blog post, we will focus primarily on the data classification workflow and not each type of enforcement use case.\u003c/p\u003e\u003cp id=\"6717\"\u003eThe workflow can be classified into three pillars:\u003c/p\u003e\u003cul\u003e\u003cli id=\"fcb3\"\u003e\u003cstrong\u003eCatalog\u003c/strong\u003e: What data do we have?\u003c/li\u003e\u003cli id=\"ed9d\"\u003e\u003cstrong\u003eDetection\u003c/strong\u003e: What data do we suspect is personal data?\u003c/li\u003e\u003cli id=\"d5fb\"\u003e\u003cstrong\u003eReconciliation\u003c/strong\u003e: Which classification do we choose?\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cfigcaption\u003ePersonal Data Classification Flow\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"6719\"\u003eLet’s dig deeper into how each of these form the backbone of data classification.\u003c/p\u003e\u003ch2 id=\"6a96\"\u003eCatalog\u003c/h2\u003e\u003cp id=\"dca6\"\u003eCataloging involves building a dynamic, accurate, and scalable system to first \u003cem\u003eidentify\u003c/em\u003e where data exists and then \u003cem\u003eorganize\u003c/em\u003e the whole inventory. Cataloging is akin to mapping the data landscape or organizing a library. It involves dynamically discovering new data, enriching it with metadata from various sources, and manually inputting information. This process is crucial for enforcing data policies, accurately classifying data, and assigning it to the correct owners.\u003c/p\u003e\u003cul\u003e\u003cli id=\"a5ba\"\u003e\u003cstrong\u003eAutomated and Dynamic Discovery:\u003c/strong\u003e Automation makes the cataloging process scalable and efficient. For the variety of data stores that Airbnb uses, such as production and analytical databases, object stores, and cloud storage, our catalogs connect to them and dynamically fetch the full inventory of data. Either through stream or batch processing, they dynamically update to reflect new and changed data. This ensures the catalog is a reliable and accurate source of truth.\u003c/li\u003e\u003cli id=\"0e14\"\u003e\u003cstrong\u003eComplexity and Diversity in Data Sources:\u003c/strong\u003e The challenge of cataloging stems from the variety and complexity of data sources, including different formats and locations. Our cataloging systems fetch metadata in several ways: through direct API calls or by crawling schemas in formats like thrift, JSON, yaml, or config files, accommodating the diverse nature of modern data storage.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"22ad\"\u003eFor search and discovery, many of our data entities are surfaced in the data management platform, \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/metis-building-airbnbs-next-generation-data-management-platform-d2c5219edf19\"\u003eMetis\u003c/a\u003e. This helps the data owners quickly answer questions such as which data contains personal data, who owns the data, and which controls are in place.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eCatalog UI\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"6069\"\u003eDetection\u003c/h2\u003e\u003cp id=\"b901\"\u003eFor personal data detection, we use the in-house automated detection service in our \u003ca rel=\"noopener\" href=\"https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08\"\u003eData Protection Platform\u003c/a\u003e which was built to protect data in compliance with global regulations and security requirements. As our own taxonomy grows, we have expanded our capabilities and made the service easily extensible to detect all other types of personal data elements and personal Airbnb IDs.\u003c/p\u003e\u003ch2 id=\"fc00\"\u003eDetection engine\u003c/h2\u003e\u003cp id=\"7d59\"\u003eFor each data entity stored in the catalogs, scanning jobs are scheduled through a message queue, which then samples data and runs through our list of classifiers. Recognizing the need for periodic classifier updates, the detection engine was designed for simplicity and flexibility. Since its inception, our detection engine has upgraded to include additional steps and adopted the approach of configuration-driven development. The majority of the logic of the detection engine has been rewritten as simpler configurations to increase the speed of iterating on existing classifiers, improve testing, and enable quick development of new features.\u003c/p\u003e\u003cp id=\"0998\"\u003eThe detection engine can be seen as a pipeline, which involves the scanner, validator, and thresholding.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eDetection Engine\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"36e1\"\u003e\u003cstrong\u003eScanner:\u003c/strong\u003e The scanner classifies personal data using metadata and content, employing methods like regex for emails and keyword lists for cities, and advanced machine learning models for complex data types requiring contextual understanding.\u003c/p\u003e\u003cp id=\"e2ca\"\u003e\u003cstrong\u003eValidator:\u003c/strong\u003e Sampled data matching a scanner undergoes a customizable validation step to enhance classifier accuracy, verifying details like latitude/longitude ranges or custom ciphertexts from encryption services.\u003c/p\u003e\u003cp id=\"a67d\"\u003e\u003cstrong\u003eThresholding:\u003c/strong\u003e To reduce noise, thresholding is applied before storing results, varying by data structure type (e.g., matched rows vs. findings in a document) and set based on historical data frequency and criticality.\u003c/p\u003e\u003cp id=\"5ee0\"\u003eWith the revamped pipeline, this has resulted in a significant decrease in false positive findings and reduced the burden on data owners to verify every result, which has historically impeded their productivity.\u003c/p\u003e\u003ch2 id=\"1dc7\"\u003eReconciliation\u003c/h2\u003e\u003cp id=\"f86e\"\u003eNot every detection surfaced may be correct or, in other cases, more context may be required. Therefore, we employ a \u003cstrong\u003ehuman-in-the-loop\u003c/strong\u003e \u003cstrong\u003estrategy\u003c/strong\u003e: where data owners confirm the classifications. This step is critical in ensuring these classifications are correct before any data policies are automatically enforced to protect our data.\u003c/p\u003e\u003ch2 id=\"534e\"\u003eAutomated Notifications\u003c/h2\u003e\u003cp id=\"d3cb\"\u003eFor compliance, we have an automated notification system that issues tickets whenever personal data is detected. These get surfaced to the appropriate data or service owners with strict SLAs.\u003c/p\u003e\u003cp id=\"ca1c\"\u003eFor data entities that have schemas defined in code, such as transactional tables from production services (online), Amazon S3 buckets, or tables that are exported to our data warehouse, we assist the developers by automatically creating code changes that update their table schemas with the detected personal data elements.\u003c/p\u003e\u003ch2 id=\"d570\"\u003eEnforcing resolution\u003c/h2\u003e\u003cp id=\"3f44\"\u003eTo enforce resolution on these tickets, tables are automatically access controlled in the data warehouse when the tickets are not resolved within SLA. Additionally reviews are conducted to ensure our classifications are correct for data where its handling requirements apply.\u003c/p\u003e\u003cp id=\"637b\"\u003eTracking actions taken on these tickets for when personal data is detected has been important to assess the quality of our data classification flow and to keep an audit trail of past detections. It also highlights points of friction developers face when resolving these tickets. The investments we have made in this area have continued to improve the process and reduce the time needed to resolve tickets each year.\u003c/p\u003e\u003ch2 id=\"ef69\"\u003eAssessing Quality of a Data Classification System\u003c/h2\u003e\u003cp id=\"1ef5\"\u003eBecause of the complexity of the system and its sub-components, this presented a unique challenge as we strive to define what quality means for the entire system. To build with the long-term in mind, we evaluated how well our entire data classification system functions as a whole.\u003c/p\u003e\u003cp id=\"d884\"\u003eWe’ve set up measurements to assess quality of our data classification in three categories:\u003c/p\u003e\u003cp id=\"5e3d\"\u003e\u003cstrong\u003eRecall\u003c/strong\u003e: This measures our coverage and ability to \u003cstrong\u003enot miss\u003c/strong\u003e where personal data may exist, crucial for protecting the stored personal data. We assess recall through:\u003c/p\u003e\u003cul\u003e\u003cli id=\"6ac4\"\u003eNumber of data entities integrated in the data classification system\u003c/li\u003e\u003cli id=\"e98d\"\u003eVolume of personal data that exists from all different sources\u003c/li\u003e\u003cli id=\"7bc1\"\u003eTypes of personal data being annotated and automatically detected against our taxonomy\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"f459\"\u003e\u003cstrong\u003ePrecision\u003c/strong\u003e: This evaluates the accuracy of our data classifications, vital for data owners tagging their data. High precision minimizes tagging friction. Precision is measured by:\u003c/p\u003e\u003cul\u003e\u003cli id=\"d153\"\u003eTracking false positive rates of classifiers for each type of personal data\u003c/li\u003e\u003cli id=\"ca36\"\u003eTracking ticket resolutions made by data owners, which also aids in understanding nuanced classification cases\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"90e6\"\u003e\u003cstrong\u003eSpeed\u003c/strong\u003e: This gauges the efficiency of identifying and classifying personal data, aiming to minimize compliance risks. Speed is measured by:\u003c/p\u003e\u003cul\u003e\u003cli id=\"18a5\"\u003eTime it takes for the detection engine for scanning new data entities\u003c/li\u003e\u003cli id=\"c4a7\"\u003eTime it takes for data owners to reconcile classifications and resolve tickets\u003c/li\u003e\u003cli id=\"0178\"\u003eThe frequency of data tagging at creation by data owners\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"dfe1\"\u003eThese measurements ensure our data classification system is effective, accurate, and efficient, safeguarding our personal data.\u003c/p\u003e\u003ch2 id=\"25f1\"\u003eConsiderations for Building a Data Classification System\u003c/h2\u003e\u003cp id=\"fc42\"\u003eIt is important to be aware of issues that may be present with the outlined approach in general. Below are some challenges that we’ve considered when building a data classification system:\u003c/p\u003e\u003cul\u003e\u003cli id=\"b672\"\u003e\u003cstrong\u003ePost-Processing Classification\u003c/strong\u003e: The outlined approach mostly relies on post-processing classification, which means that schema information is added after data has been collected and stored. In a modern data world where data and metadata are constantly changing, post-processing cannot catch up with data evolution.\u003c/li\u003e\u003cli id=\"9836\"\u003e\u003cstrong\u003eInconsistent Classifications:\u003c/strong\u003e Data generally flows from online to offline through ETL (extract, transform, and load) processes, and then reverse ETLing back to the online world. However, data classification that is performed independently in both worlds can lead to inconsistent classifications.\u003c/li\u003e\u003cli id=\"f194\"\u003e\u003cstrong\u003eWaste of Process Cost\u003c/strong\u003e: Duplicate annotations can be made for the same data in the online and offline domains, which might result in increased costs for data classification processes.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"476c\"\u003eTo address these challenges, we describe the process of “\u003cstrong\u003eshifting left\u003c/strong\u003e” with data classification and how we started to push developers to annotate their data at the beginning of the data lifecycle.\u003c/p\u003e\u003ch2 id=\"0227\"\u003eShifting Left\u003c/h2\u003e\u003cp id=\"9ce5\"\u003eInstead of thinking about governance and data classification as an activity that happens post-hoc, we’ve started to embed the annotation process directly into data schemas as they are being created and updated. This enables us to:\u003c/p\u003e\u003cul\u003e\u003cli id=\"7e21\"\u003e\u003cstrong\u003eShift Classification from Data to Schema\u003c/strong\u003e: The schema annotation process takes place earlier in the data lifecycle at the point of data collection. This keeps annotations updated as data evolves and ensures data is annotated before collection and consumption, allowing for immediate policy enforcement.\u003c/li\u003e\u003cli id=\"927a\"\u003e\u003cstrong\u003eShift Classification from Offline to Online\u003c/strong\u003e: Traditionally done offline, data classification is now integrated into production services, ensuring data is structured and formatted correctly from the start. Leveraging data lineage information enables automated annotation, reducing the need for manual effort and lowering process costs.\u003c/li\u003e\u003cli id=\"e6fd\"\u003e\u003cstrong\u003eShift from Data Steward to Data Owner\u003c/strong\u003e: Oftentimes, stewardship, or the responsible management and oversight, of the data is conducted by people who are downstream of data creation, such as data consumers or governance professionals. This change shifts stewardship to the data producers, merging the roles of data steward and data owner. This empowers the team that owns the data to manage it more effectively and scale operations.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"f14c\"\u003eSchema Annotation Enforcement\u003c/h2\u003e\u003cp id=\"9d2c\"\u003eFocusing on our most crucial online data, we have started executing on shifting left by directly integrating with our internal schema definition language that is known for its annotation capabilities. We now mandate that developers include personal data annotations at the source when creating new data models, providing guidance on accurate tagging. This requirement is enforced through checks that run in our CI/CD pipelines which:\u003c/p\u003e\u003cul\u003e\u003cli id=\"bf4e\"\u003e\u003cstrong\u003eAutomatically suggest data elements\u003c/strong\u003e: Based on the schema’s metadata, we automatically detect the data elements for all fields defined in the schema with our detection service.\u003c/li\u003e\u003cli id=\"8e05\"\u003e\u003cstrong\u003eValidate data elements\u003c/strong\u003e: Annotations are validated against our own taxonomy and schemas are enforced and all fields are annotated, even when it is not considered personal.\u003c/li\u003e\u003cli id=\"8711\"\u003e\u003cstrong\u003eWarn about downstream impact\u003c/strong\u003e: We notify data owners when annotations can impact downstream services such as offline data pipelines and direct them to the proper resources for handling.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cfigcaption\u003eSchema Annotation Enforcement\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"d1b4\"\u003eWhile shifting left has significantly helped to push classifying data earlier and increase coverage of schema annotations, \u003cstrong\u003ethis does not discount the importance of the rest of the classification process\u003c/strong\u003e. Classifications that happen post-process are necessary for instance in cases where data storages that do not include well-defined schemas. Therefore, continued investments are still needed in detection and reconciliation to cover areas that cannot be shifted left and to verify annotations that may have already been made by owners as a second layer of protection.\u003c/p\u003e\u003ch2 id=\"c4e9\"\u003eConclusion/Lessons Learned\u003c/h2\u003e\u003cp id=\"2e83\"\u003eThe Airbnb data classification framework has been successful in advancing data management, security, and privacy. Reflecting on the journey, it has offered invaluable insights that have shaped our methodologies. Key takeaways include:\u003c/p\u003e\u003cul\u003e\u003cli id=\"2d35\"\u003eAdopting a unified strategy for classifying online and offline personal data to streamline processes\u003c/li\u003e\u003cli id=\"5bdc\"\u003eImplementing a ‘Shift Left’ approach to engage with data owners early in the development cycle\u003c/li\u003e\u003cli id=\"482a\"\u003eAddressing classification uncertainties through clear guidelines and decision-making\u003c/li\u003e\u003cli id=\"5f0d\"\u003eEnhancing education and training initiatives for data owners and consumers\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"bd86\"\u003eAs the data landscape continues to rapidly change, these lessons will guide future data classification efforts and ensure continued trust and protection of customer data.\u003c/p\u003e\u003ch2 id=\"8964\"\u003eAcknowledgements\u003c/h2\u003e\u003cp id=\"08ea\"\u003eOur data classification strategy has evolved over many years and we’ve been able to quickly adapt and iterate thanks to our decision to build an in-house solution. Security, privacy, and compliance are of utmost importance at Airbnb, and this work would not be possible without the contribution of many of our cross-functional partners and leaders.\u003c/p\u003e\u003cp id=\"a941\"\u003eThis includes, but are not limited to: Bill Meng, Aravind Selvan, Juan Tamayo, Xiao Chen, Pinyao Guo, Wendy Jin, Liam McInerney, Pat Moynahan, Gabriel Gejman, Marc Blanchou, Brendon Lynch, and many others.\u003c/p\u003e\u003cp id=\"9b1d\"\u003eIf this type of work interests you, check out some of our related positions at \u003ca href=\"https://careers.airbnb.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCareers at Airbnb\u003c/a\u003e or check out more resources in the \u003ca href=\"https://medium.com/airbnb-engineering\" rel=\"noopener\"\u003eAirbnb Tech Blog\u003c/a\u003e!\u003c/p\u003e\u003cp id=\"cb75\"\u003e\u003cem\u003eAll product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "16 min read",
  "publishedTime": "2024-08-19T16:43:29.698Z",
  "modifiedTime": null
}
