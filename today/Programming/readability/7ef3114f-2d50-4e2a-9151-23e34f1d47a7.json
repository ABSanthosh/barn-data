{
  "id": "7ef3114f-2d50-4e2a-9151-23e34f1d47a7",
  "title": "Vertex AI in Firebase Aims to Simplify the Creation of Gemini-powered Mobile Apps",
  "link": "https://www.infoq.com/news/2024/10/vertex-ai-firebase-gemini/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Currently available in beta, the Vertex AI SDK for Firebase enables the creation of apps that go beyond the simple chat model and text prompting. Google has just made available a colab to help developers through the steps required to integrate it into their apps. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Thu, 17 Oct 2024 16:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Swift",
    "Large language models",
    "Flutter",
    "Android",
    "Kotlin",
    "iOS",
    "Gemini",
    "Mobile",
    "AI, ML \u0026 Data Engineering",
    "Development",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3242,
  "excerpt": "Currently available in beta, the Vertex AI SDK for Firebase enables the creation of apps that go beyond the simple chat model and text prompting. Google has just made available a colab to help develop",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241001113644/apple-touch-icon.png",
  "text": "Currently available in beta, the Vertex AI SDK for Firebase enables the creation of apps that go beyond the simple chat model and text prompting. Google has just made available a colab to help developers through the steps required to integrate it into their apps. The new colab covers several key topics, including prompt design, how to set up a Firebase project to use Vertex AI, configure your Android Studio project, and integrate it into your code using Kotlin. To build design effective prompts, explains Google engineer Thomas Ezan, developers can use Vertex AI Studio, a Cloud-based tool aimed at rapidly prototyping and testing prompts with Gemini models. A feature of Vertex AI that promises to make it easier for developers to specialize the behavior of their apps is System instructions. System instructions serve as a \"preamble\" that you incorporate before the user prompt. This enables shaping the model's behavior to align with your specific requirements and scenarios. Using system instructions developers can predefine once and for all the desired output style or tone, a persona or role (e.g, “explain like I am 5”), the goals or rules for the task (e.g, “return a code snippet without further explanation”), and any additional context that is relevant to the app user. System instructions are set at initialization time, as shown in the following snippet: val generativeModel = Firebase.vertexAI.generativeModel( modelName = \"gemini-1.5-flash\", ... systemInstruction = content { text(\"You are a knowledgeable tutor. Answer the questions using the socratic tutoring method.\") } ) The SDK also allows developers to specify a responseMimeType for the generated output. This can be useful, for example, when generating JSON output in order to exclude any undesired non-JSON content. Integrating the Gemini API into a mobile app, says Ezan, is not just limited to providing conversational interfaces, thanks to Gemini multi-modal capabilities. Indeed, Gemini can process diverse input besides text, including images, audio, and video. This means, for example, you can generate captions for images, summarize audio files, describe scenes from videos and so on. Another powerful feature that Ezan highlights is the possibility of creating functions to extend the capabilities of the model. For example, a function could fetch some data from an SQL database and add it to the prompt context. Alternatively, you could define a series of tools that the model can use to generate its output, as it is shown in the following diagram. Here, the model uses two calls into your data to retrieve the date and the order list to answer a question from a user. All of these features are nicely supported by Vertex AI in Firebase, so developers can leverage them using their preferred programming language, including Swift, Kotlin, Flutter, and JavaScript. The Vertex AI in Firebase SDK allows Android and iOS developers to access the Gemini API directly from their apps without requiring an intermediate backend service layer written in Python, Java, or Go. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2024/10/vertex-ai-firebase-gemini/en/headerimage/vertex-ai-firebase-gemini-1729177498922.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eCurrently \u003ca href=\"https://firebase.blog/posts/2024/05/introducing-vertex-ai-firebase/\"\u003eavailable in beta\u003c/a\u003e, the \u003ca href=\"https://firebase.google.com/docs/vertex-ai/get-started?platform=ios\"\u003eVertex AI SDK for Firebase\u003c/a\u003e enables the creation of \u003ca href=\"https://android-developers.googleblog.com/2024/10/advanced-capabilities-of-gemini-api-for-android-developers.html\"\u003eapps that go beyond the simple chat model and text prompting\u003c/a\u003e. Google has just made available a \u003ca href=\"https://developer.android.com/codelabs/gemini-summarize\"\u003ecolab\u003c/a\u003e to help developers through the steps required to integrate it into their apps.\u003c/p\u003e\n\n\u003cp\u003eThe new colab covers several key topics, including prompt design, how to set up a Firebase project to use Vertex AI, configure your Android Studio project, and integrate it into your code using Kotlin.\u003c/p\u003e\n\n\u003cp\u003eTo build design effective prompts, explains Google engineer Thomas Ezan, developers can use \u003ca href=\"https://console.cloud.google.com/vertex-ai/studio/freeform\"\u003eVertex AI Studio\u003c/a\u003e, a Cloud-based tool aimed at rapidly prototyping and testing prompts with Gemini models.\u003c/p\u003e\n\n\u003cp\u003eA feature of Vertex AI that promises to make it easier for developers to specialize the behavior of their apps is \u003cem\u003eSystem instructions\u003c/em\u003e.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eSystem instructions serve as a \u0026#34;preamble\u0026#34; that you incorporate before the user prompt. This enables shaping the model\u0026#39;s behavior to align with your specific requirements and scenarios.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eUsing system instructions developers can predefine once and for all the desired output style or tone, a persona or role (e.g, “explain like I am 5”), the goals or rules for the task (e.g, “return a code snippet without further explanation”), and any additional context that is relevant to the app user.\u003c/p\u003e\n\n\u003cp\u003eSystem instructions are set at initialization time, as shown in the following snippet:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003eval generativeModel = Firebase.vertexAI.generativeModel(\n  modelName = \u0026#34;gemini-1.5-flash\u0026#34;,\n  ...\n  systemInstruction = \n    content { text(\u0026#34;You are a knowledgeable tutor. Answer the questions using the socratic tutoring method.\u0026#34;) }\n)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThe SDK also allows developers to specify a \u003ccode\u003eresponseMimeType\u003c/code\u003e for the generated output. This can be useful, for example, when generating JSON output in order to exclude any undesired non-JSON content.\u003c/p\u003e\n\n\u003cp\u003eIntegrating the Gemini API into a mobile app, says Ezan, is not just limited to providing conversational interfaces, thanks to Gemini multi-modal capabilities. Indeed, Gemini can process diverse input besides text, including images, audio, and video. This means, for example, you can generate captions for images, summarize audio files, describe scenes from videos and so on.\u003c/p\u003e\n\n\u003cp\u003eAnother powerful feature that Ezan highlights is the possibility of creating functions to extend the capabilities of the model. For example, a function could fetch some data from an SQL database and add it to the prompt context. Alternatively, you could define a series of tools that the model can use to generate its output, as it is shown in the following diagram.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/10/vertex-ai-firebase-gemini/en/resources/1vertex-ai-firebase-1729177497983.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/10/vertex-ai-firebase-gemini/en/resources/1vertex-ai-firebase-1729177497983.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eHere, the model uses two calls into your data to retrieve the date and the order list to answer a question from a user.\u003c/p\u003e\n\n\u003cp\u003eAll of these features are nicely supported by Vertex AI in Firebase, so developers can leverage them using their preferred programming language, including Swift, Kotlin, Flutter, and JavaScript.\u003c/p\u003e\n\n\u003cp\u003eThe Vertex AI in Firebase SDK allows Android and iOS developers to access the Gemini API directly from their apps without requiring an intermediate backend service layer written in Python, Java, or Go.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-10-17T00:00:00Z",
  "modifiedTime": null
}
