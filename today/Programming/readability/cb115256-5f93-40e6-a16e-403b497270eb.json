{
  "id": "cb115256-5f93-40e6-a16e-403b497270eb",
  "title": "Supercharging AI Coding Assistants with Gemini Models' Long Context",
  "link": "https://developers.googleblog.com/en/supercharging-ai-coding-assistants-with-massive-context/",
  "description": "Sourcegraph's Cody AI assistant, integrated with Google's Gemini 1.5 Flash, can evaluate the advantages of using long-context windows in AI models for code generation and understanding.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Vishal Dharmadhikari, Paige Bailey, Beyang Liu",
  "length": 4226,
  "excerpt": "Sourcegraph's Cody AI assistant, integrated with Google's Gemini 1.5 Flash, can evaluate the advantages of using long-context windows in AI models for code generation and understanding.",
  "siteName": "",
  "favicon": "",
  "text": "Products More Solutions Events Learn Community Developer Program Blog One of the most exciting frontiers in the application of long-context windows is code generation and understanding. Large codebases require a deep understanding of complex relationships and dependencies, something traditional AI models struggle to grasp. By expanding the amount of code with large context windows, we can unlock a new level of accuracy and usefulness in code generation and understanding.We partnered with Sourcegraph, the creators of the Cody AI coding assistant that supports LLMs like Gemini 1.5 Pro and Flash, to explore the potential of long context windows in real-world coding scenarios. Sourcegraph's focus on integrating code search and intelligence into AI code generation, and successful deployment of Cody to enterprises with large, complex codebases such as Palo Alto Networks and Leidos, made them the ideal partner for this exploration.Sourcegraph's Approach and ResultsSourcegraph compared Cody's performance with a 1M token context window (using Google's Gemini 1.5 Flash) against its production version. This direct comparison allowed them to isolate the benefits of expanded context. They focused on technical question answering, a crucial task for developers working with large codebases. They used a dataset of challenging questions that required deep code understanding.The results were striking. Three of Sourcegraph’s key benchmarks—Essential Recall, Essential Concision, and Helpfulness—demonstrated significant improvements when using the longer context.Essential Recall: The proportion of crucial facts in the response increased substantially.Essential Concision: The proportion of essential facts normalized by response length also improved, indicating more concise and relevant answers.Helpfulness: The overall helpfulness score, normalized by response length, significantly increased, indicating a more user-friendly experience. Furthermore, the use of long-context models drastically reduced the overall hallucination rate (the generation of factually incorrect information). The hallucination rate decreased from 18.97% to 10.48%, a significant improvement in accuracy and reliability.Tradeoffs and Future DirectionWhile the benefits of long context are significant, there are tradeoffs. The time to first token increases linearly with the length of the context. To mitigate this, Sourcegraph implemented a prefetching mechanism and a layered context model architecture for model execution state caching. With Gemini 1.5 Flash and Pro long-context models, this optimized the time to first token from 30-40 seconds to around 5 seconds for 1MB contexts – a considerable improvement for real-time code generation and technical assistance.This collaboration showcases the transformative potential of long-context models in revolutionizing code understanding and generation. We’re excited to partner with companies like Sourcegraph to continue to unlock even more innovative applications and paradigms with large context windows.To dive deeper into Sourcegraph's detailed evaluation methodologies, benchmarks, and analysis, including illustrative examples, don’t miss their in-depth blog post.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemini_SuperchargingAICodingAssis.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\n        \n        \n        \n\n        \n\n\t\t\t\t\n        \n\n\n\n\n\u003cdiv top-level-nav=\"\"\u003e\n  \u003cnav aria-label=\"Side menu\"\u003e\n    \n    \u003cdiv\u003e\n        \u003cul\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/products\" data-label=\"Tab: Products\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Products\n             \u003c/span\u003e\n            \u003c/a\u003e\n            \u003cul\u003e\n              \u003cli\u003e\n                \u003cspan tabindex=\"0\" data-label=\"More Products\"\u003e\n                  \u003cspan menu=\"Products\"\u003e\n                    More\n                  \u003c/span\u003e\n                  \u003cspan menu=\"Products\"\u003e\n                    \n                  \u003c/span\u003e\n                \u003c/span\u003e\n              \u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/solutions/catalog\" data-label=\"Tab: Solutions\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Solutions\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/events\" data-label=\"Tab: Events\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Events\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/learn\" data-label=\"Tab: Learn\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Learn\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/community\" data-label=\"Tab: Community\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Community\n             \u003c/span\u003e\n            \u003c/a\u003e\n            \u003cul\u003e\n              \u003cli\u003e\n                \n              \u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/profile/u/me\" data-label=\"Tab: Developer Program\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Developer Program\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.googleblog.com/\" data-label=\"Tab: Blog\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Blog\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/div\u003e\n  \u003c/nav\u003e\n  \u003c/div\u003e\n\n\n\n        \n  \u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"svodn\"\u003eOne of the most exciting frontiers in the application of long-context windows is code generation and understanding. Large codebases require a deep understanding of complex relationships and dependencies, something traditional AI models struggle to grasp. By expanding the amount of code with large context windows, we can unlock a new level of accuracy and usefulness in code generation and understanding.\u003c/p\u003e\u003cp data-block-key=\"8lb8n\"\u003eWe partnered with Sourcegraph, the creators of the \u003ca href=\"https://sourcegraph.com/cody\"\u003eCody AI\u003c/a\u003e coding assistant that supports LLMs like \u003ca href=\"https://sourcegraph.com/blog/cody-vscode-1-22-0-release\"\u003eGemini 1.5 Pro and Flash\u003c/a\u003e, to explore the potential of long context windows in real-world coding scenarios. Sourcegraph\u0026#39;s focus on integrating code search and intelligence into AI code generation, and successful deployment of Cody to enterprises with large, complex codebases such as Palo Alto Networks and Leidos, made them the ideal partner for this exploration.\u003c/p\u003e\u003ch2 data-block-key=\"can9m\"\u003e\u003cbr/\u003eSourcegraph\u0026#39;s Approach and Results\u003c/h2\u003e\u003cp data-block-key=\"93qlf\"\u003eSourcegraph compared Cody\u0026#39;s performance with a 1M token context window (using Google\u0026#39;s Gemini 1.5 Flash) against its production version. This direct comparison allowed them to isolate the benefits of expanded context. They focused on technical question answering, a crucial task for developers working with large codebases. They used a dataset of challenging questions that required deep code understanding.\u003c/p\u003e\u003cp data-block-key=\"8ssct\"\u003eThe results were striking. Three of Sourcegraph’s key benchmarks—Essential Recall, Essential Concision, and Helpfulness—demonstrated significant improvements when using the longer context.\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"185dg\"\u003e\u003cb\u003eEssential Recall:\u003c/b\u003e The proportion of crucial facts in the response increased substantially.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"dkbr2\"\u003e\u003cb\u003eEssential Concision:\u003c/b\u003e The proportion of essential facts normalized by response length also improved, indicating more concise and relevant answers.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"5jpcm\"\u003e\u003cb\u003eHelpfulness:\u003c/b\u003e The overall helpfulness score, normalized by response length, significantly increased, indicating a more user-friendly experience.\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image4_UeUYjQP.original.png\" alt=\"Sourcegraph’s key benchmarks\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"svodn\"\u003eFurthermore, the use of long-context models drastically reduced the overall hallucination rate (the generation of factually incorrect information). The hallucination rate decreased from 18.97% to 10.48%, a significant improvement in accuracy and reliability.\u003c/p\u003e\u003ch2 data-block-key=\"4pufq\"\u003e\u003cbr/\u003eTradeoffs and Future Direction\u003c/h2\u003e\u003cp data-block-key=\"b5bu6\"\u003eWhile the benefits of long context are significant, there are tradeoffs. The time to first token increases linearly with the length of the context. To mitigate this, Sourcegraph implemented a prefetching mechanism and a layered context model architecture for model execution state caching. With Gemini 1.5 Flash and Pro long-context models, this optimized the time to first token from 30-40 seconds to around 5 seconds for 1MB contexts – a considerable improvement for real-time code generation and technical assistance.\u003c/p\u003e\u003cp data-block-key=\"4b7an\"\u003eThis collaboration showcases the transformative potential of long-context models in revolutionizing code understanding and generation. We’re excited to partner with companies like Sourcegraph to continue to unlock even more innovative applications and paradigms with large context windows.\u003c/p\u003e\u003cp data-block-key=\"3g2h7\"\u003eTo dive deeper into Sourcegraph\u0026#39;s detailed evaluation methodologies, benchmarks, and analysis, including illustrative examples, don’t miss their \u003ca href=\"https://sourcegraph.com/blog/towards-infinite-context-for-code\"\u003ein-depth blog post\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\n\n\n\t\t\t\t\n\t\t\t\t\n\n\n\n\n\n        \n\t\t\t\t\n\n        \n        \n        \n        \n\n        \n\n        \n  \n\n    \n\n\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-11-07T00:00:00Z",
  "modifiedTime": null
}
