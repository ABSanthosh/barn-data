{
  "id": "99922551-1238-4b07-980e-55c7fb50307d",
  "title": "Google Releases LMEval, an Open-Source Cross-Provider LLM Evaluation Tool",
  "link": "https://www.infoq.com/news/2025/05/google-lmeval-benchmark/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "LMEval aims to help AI researchers and developers compare the performance of different large language models. Designed to be accurate, multimodal, and easy to use, LMEval has already been used to evaluate major models in terms of safety and security. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Sat, 31 May 2025 15:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Benchmark",
    "Open Source",
    "Large language models",
    "Google",
    "Python",
    "Development",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3946,
  "excerpt": "LMEval aims to help AI researchers and developers compare the performance of different large language models. Designed to be accurate, multimodal, and easy to use, LMEval has already been used to eval",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250527074915/apple-touch-icon.png",
  "text": "LMEval aims to help AI researchers and developers compare the performance of different large language models. Designed to be accurate, multimodal, and easy to use, LMEval has already been used to evaluate major models in terms of safety and security. One of the reasons behind LMEval is the fast pace at which new models are being introduced. This makes it essential to evaluate them quickly and reliably to assess their suitability for specific applications, says Google researchers. Among its key features are compatibility with a wide range of LLM providers, incremental benchmark execution for improved efficiency, support for multimodal evaluation—including text, images, and code—and encrypted result storage for enhanced security. For cross-provider support, it is critical that evaluation benchmarks can be defined once and reused across multiple models, despite differences in their APIs. To this end, LMEval uses LiteLLM, a framework that allows developers to use the OpenAI API format to call a variety of LLM providers, including Bedrock, Hugging Face, Vertex AI, Together AI, Azure, OpenAI, Groq, and others. LiteLLM translates inputs to match each provider’s specific requirements for completion, embedding, and image generation endpoints, and produces a uniform output format. To improve execution efficiency when new models are released, LMEval runs only the evaluations that are strictly necessary, whether for new models, prompts, or questions. This is made possible by an intelligent evaluation engine that follows an incremental evaluation model. Written in Python and available on GitHub, LMEval requires you to follow a series of steps to run an evaluation. First, you define your benchmark by specifying the tasks to execute, e.g., detect eye colors in a picture, along with the prompt, the image, and the expected results. Then, you list the models to evaluate and run the benchmark: benchmark = Benchmark(name='Cat Visual Questions', description='Ask questions about cats picture') ... scorer = get_scorer(ScorerType.contain_text_insensitive) task = Task(name='Eyes color', type=TaskType.text_generation, scorer=scorer) category.add_task(task) # add questions source = QuestionSource(name='cookbook') # cat 1 question - create question then add media image question = Question(id=0, question='what is the colors of eye?', answer='blue', source=source) question.add_media('./data/media/cat_blue.jpg') task.add_question(question) ... # evaluate benchmark on two models models = [GeminiModel(), GeminiModel(model_version='gemini-1.5-pro')] prompt = SingleWordAnswerPrompt() evaluator = Evaluator(benchmark) eval_plan = evaluator.plan(models, prompt) # plan evaluation completed_benchmark = evaluator.execute() # run evaluation Optionally, you can save the evaluation results to a SQLite database and export the data to pandas for further analysis and visualization. LMEval uses encryption to store benchmark data and evaluation results to protect against crawling or indexing. LMEval also includes LMEvalboard, a visual dashboard that lets you view overall performance, analyze individual models, or compare multiple models. As mentioned, LMEval has been used to create the Phare LLM Benchmark, designed to evaluate LLM safety and security, including resistance to hallucination, factual accuracy, bias, and potential harm. LMEval is not the only cross-provider LLM evaluation framework currently available. Others include Harbor Bench and EleutherAI's LM Evaluation Harness. Harbor Bench, limited to text prompts, has the interesting feature of using an LLM to judge result quality. In contrast, EleutherAI’s LM Evaluation Harness includes over 60 benchmarks and allows users to define new ones using YAML. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/05/google-lmeval-benchmark/en/headerimage/lmeval-llm-evaluation-1748700377551.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eLMEval aims to help AI researchers and developers \u003ca href=\"https://opensource.googleblog.com/2025/05/announcing-lmeval-an-open-ource-framework-cross-model-evaluation.html\"\u003ecompare the performance of different large language models\u003c/a\u003e. Designed to be accurate, multimodal, and easy to use, LMEval has already been used to \u003ca href=\"https://phare.giskard.ai\"\u003eevaluate major models in terms of safety and security\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eOne of the reasons behind LMEval is the fast pace at which new models are being introduced. This makes it essential to evaluate them quickly and reliably to assess their suitability for specific applications, says Google researchers. Among its key features are compatibility with a wide range of LLM providers, incremental benchmark execution for improved efficiency, support for multimodal evaluation—including text, images, and code—and encrypted result storage for enhanced security.\u003c/p\u003e\n\n\u003cp\u003eFor cross-provider support, it is critical that evaluation benchmarks can be defined once and reused across multiple models, despite differences in their APIs. To this end, LMEval uses \u003ca href=\"https://github.com/BerriAI/litellm\"\u003eLiteLLM\u003c/a\u003e, a framework that allows developers to use the OpenAI API format to call a variety of LLM providers, including Bedrock, Hugging Face, Vertex AI, Together AI, Azure, OpenAI, Groq, and others. LiteLLM translates inputs to match each provider’s specific requirements for completion, embedding, and image generation endpoints, and produces a uniform output format.\u003c/p\u003e\n\n\u003cp\u003eTo improve execution efficiency when new models are released, LMEval runs only the evaluations that are strictly necessary, whether for new models, prompts, or questions. This is made possible by an intelligent evaluation engine that follows an incremental evaluation model.\u003c/p\u003e\n\n\u003cp\u003eWritten in Python and \u003ca href=\"https://github.com/google/lmeval\"\u003eavailable on GitHub\u003c/a\u003e, \u003ca href=\"https://github.com/google/lmeval/tree/main/notebooks\"\u003eLMEval requires you to follow a series of steps\u003c/a\u003e to run an evaluation. First, you define your benchmark by specifying the tasks to execute, e.g., detect eye colors in a picture, along with the prompt, the image, and the expected results. Then, you list the models to evaluate and run the benchmark:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003ebenchmark = Benchmark(name=\u0026#39;Cat Visual Questions\u0026#39;,\n                      description=\u0026#39;Ask questions about cats picture\u0026#39;)\n\n...\n\nscorer = get_scorer(ScorerType.contain_text_insensitive)\ntask = Task(name=\u0026#39;Eyes color\u0026#39;, type=TaskType.text_generation, scorer=scorer)\ncategory.add_task(task)\n\n# add questions\nsource = QuestionSource(name=\u0026#39;cookbook\u0026#39;)\n# cat 1 question - create question then add media image\nquestion = Question(id=0, question=\u0026#39;what is the colors of eye?\u0026#39;,\n                    answer=\u0026#39;blue\u0026#39;, source=source)\nquestion.add_media(\u0026#39;./data/media/cat_blue.jpg\u0026#39;)\ntask.add_question(question)\n\n...\n\n# evaluate benchmark on two models\nmodels = [GeminiModel(), GeminiModel(model_version=\u0026#39;gemini-1.5-pro\u0026#39;)]\n\nprompt = SingleWordAnswerPrompt()\nevaluator = Evaluator(benchmark)\neval_plan = evaluator.plan(models, prompt)  # plan evaluation\ncompleted_benchmark = evaluator.execute()  # run evaluation\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eOptionally, you can save the evaluation results to a SQLite database and export the data to pandas for further analysis and visualization. LMEval uses encryption to store benchmark data and evaluation results to protect against crawling or indexing.\u003c/p\u003e\n\n\u003cp\u003eLMEval also includes LMEvalboard, a visual dashboard that lets you view overall performance, analyze individual models, or compare multiple models.\u003c/p\u003e\n\n\u003cp\u003eAs mentioned, LMEval has been used to create the Phare LLM Benchmark, designed to evaluate LLM safety and security, including resistance to hallucination, factual accuracy, bias, and potential harm.\u003c/p\u003e\n\n\u003cp\u003eLMEval is not the only cross-provider LLM evaluation framework currently available. Others include \u003ca href=\"https://github.com/av/harbor/wiki/5.1.-Harbor-Bench\"\u003eHarbor Bench\u003c/a\u003e and \u003ca href=\"https://github.com/EleutherAI/lm-evaluation-harness\"\u003eEleutherAI\u0026#39;s LM Evaluation Harness\u003c/a\u003e. Harbor Bench, limited to text prompts, has the interesting feature of using an LLM to judge result quality. In contrast, EleutherAI’s LM Evaluation Harness includes over 60 benchmarks and allows users to define new ones using YAML.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-05-31T00:00:00Z",
  "modifiedTime": null
}
