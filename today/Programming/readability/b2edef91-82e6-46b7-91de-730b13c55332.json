{
  "id": "b2edef91-82e6-46b7-91de-730b13c55332",
  "title": "Meta Unveils Movie Gen, a New AI Model for Video Generation",
  "link": "https://www.infoq.com/news/2024/10/meta-ai-movie-gen/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Meta has announced Movie Gen, a new AI model designed to create high-quality 1080p videos with synchronized audio. The system enables instruction-based video editing and allows for personalized content generation using user-supplied images. By Daniel Dominguez",
  "author": "Daniel Dominguez",
  "published": "Tue, 08 Oct 2024 09:16:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Generative AI",
    "Large language models",
    "Artificial Intelligence",
    "OpenAI",
    "Facebook",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Daniel Dominguez",
  "length": 2376,
  "excerpt": "Meta has announced Movie Gen, a new AI model designed to create high-quality 1080p videos with synchronized audio. The system enables instruction-based video editing and allows for personalized conten",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241001113644/apple-touch-icon.png",
  "text": "Meta has announced Movie Gen, a new AI model designed to create high-quality 1080p videos with synchronized audio. The system enables instruction-based video editing and allows for personalized content generation using user-supplied images. The core of Movie Gen is a transformer model with 30 billion parameters. Trained on large datasets of images, videos, and audio, the model can produce 16-second videos at 16 frames per second. It also integrates latent space management and flow matching techniques, improving the generation of realistic video motion. In terms of data, Movie Gen was trained using over 100 million video-text pairs and 1 billion image-text pairs, which enhances its capacity to generalize across various media tasks. In benchmarks, Movie Gen outperforms previous state-of-the-art models and commercial systems like Runway Gen3 and OpenAI Sora in text-to-video synthesis, video personalization, and video editing tasks. While SDEdit maintained video structure but struggled with fine details, models like EVE and InsV2V performed well in generating realistic motion, though EVE relied on video captions for certain metrics. Runway Gen3 V2V had issues with preserving fine details. On the TGVE+ benchmark, Movie Gen Edit was favored 74% of the time over EVE and achieved state-of-the-art results in some metrics. It also outperformed Runway Gen3 V2V and SDEdit, particularly in maintaining video structure and detail across various tasks. Users have reported high satisfaction with the generated content, particularly the quality and realism of the videos AI Evangelist Alex Volkov posted on X: Meta steps into video generation to fill a big SORA void! While director and photographer Ravi Vora shared: This could change video forever. Challenges and future directions for Movie Gen include improving complex scene understanding, implementing safeguards against misuse, and reducing resource requirements for broader accessibility. The system's applications range from social media content creation to film production and personalized marketing campaigns. Meta plans to continue refining the model with future updates that may include expanded editing and personalization features. About the Author Daniel Dominguez",
  "image": "https://res.infoq.com/news/2024/10/meta-ai-movie-gen/en/headerimage/generatedHeaderImage-1728314582221.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eMeta has announced \u003ca href=\"https://ai.meta.com/blog/movie-gen-media-foundation-models-generative-ai-video/\"\u003eMovie Gen\u003c/a\u003e, a new AI model designed to create high-quality 1080p videos with synchronized audio. The system enables instruction-based video editing and allows for personalized content generation using user-supplied images.\u003c/p\u003e\n\n\u003cp\u003eThe core of Movie Gen is a \u003ca href=\"https://blogs.nvidia.com/blog/what-is-a-transformer-model/\"\u003etransformer model\u003c/a\u003e with 30 billion parameters. Trained on large datasets of images, videos, and audio, the model can produce 16-second videos at 16 frames per second. It also integrates latent space management and flow matching techniques, improving the generation of realistic video motion.\u003c/p\u003e\n\n\u003cp\u003eIn terms of \u003ca href=\"https://ai.meta.com/static-resource/movie-gen-research-paper\"\u003edata\u003c/a\u003e, Movie Gen was trained using over 100 million video-text pairs and 1 billion image-text pairs, which enhances its capacity to generalize across various media tasks.\u003c/p\u003e\n\n\u003cp\u003eIn \u003ca href=\"https://ai.meta.com/static-resource/movie-gen-research-paper\"\u003ebenchmarks\u003c/a\u003e, Movie Gen outperforms previous state-of-the-art models and commercial systems like \u003ca href=\"https://runwayml.com/research/introducing-gen-3-alpha\"\u003eRunway Gen3\u003c/a\u003e and \u003ca href=\"https://openai.com/index/sora/\"\u003eOpenAI Sora\u003c/a\u003e in text-to-video synthesis, video personalization, and video editing tasks. While \u003ca href=\"https://sde-image-editing.github.io/\"\u003eSDEdit\u003c/a\u003e maintained video structure but struggled with fine details, models like \u003ca href=\"https://arxiv.org/html/2403.09334v2\"\u003eEVE\u003c/a\u003e and \u003ca href=\"https://arxiv.org/html/2403.14468v1\"\u003eInsV2V\u003c/a\u003e performed well in generating realistic motion, though EVE relied on video captions for certain metrics. Runway Gen3 V2V had issues with preserving fine details. On the \u003ca href=\"https://huggingface.co/datasets/facebook/tgve_plus\"\u003eTGVE+\u003c/a\u003e benchmark, Movie Gen Edit was favored 74% of the time over EVE and achieved state-of-the-art results in some metrics. It also outperformed Runway Gen3 V2V and SDEdit, particularly in maintaining video structure and detail across various tasks.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://ai.meta.com/static-resource/movie-gen-research-paper\"\u003e\u003cimg alt=\"\" data-src=\"news/2024/10/meta-ai-movie-gen/en/resources/2graph-1728314915164.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/10/meta-ai-movie-gen/en/resources/2graph-1728314915164.png\" rel=\"share\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eUsers have reported high satisfaction with the generated content, particularly the quality and realism of the videos\u003c/p\u003e\n\n\u003cp\u003eAI Evangelist \u003ca href=\"https://x.com/altryne/status/1842203090478358651\"\u003eAlex Volkov\u003c/a\u003e posted on \u003ca href=\"https://x.com/altryne/status/1842203090478358651\"\u003eX\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eMeta steps into video generation to fill a big SORA void!\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWhile director and photographer \u003ca href=\"https://x.com/RaviVora/status/1842242247992963341\"\u003eRavi Vora\u003c/a\u003e shared:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis could change video forever.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eChallenges and future directions for Movie Gen include improving complex scene understanding, implementing safeguards against misuse, and reducing resource requirements for broader accessibility.\u003c/p\u003e\n\n\u003cp\u003eThe system\u0026#39;s applications range from social media content creation to film production and personalized marketing campaigns. Meta plans to continue refining the model with future updates that may include expanded editing and personalization features.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Daniel-Dominguez\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eDaniel Dominguez\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "3 min read",
  "publishedTime": "2024-10-08T00:00:00Z",
  "modifiedTime": null
}
