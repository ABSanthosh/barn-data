{
  "id": "2577e59f-ff3d-4de1-ae95-b476f1ae0ec6",
  "title": "OCP Summit 2024: The open future of networking hardware for AI",
  "link": "https://engineering.fb.com/2024/10/15/data-infrastructure/open-future-networking-hardware-ai-ocp-2024-meta/",
  "description": "At Open Compute Project Summit (OCP) 2024, we’re sharing details about our next-generation network fabric for our AI training clusters. We’ve expanded our network hardware portfolio and are contributing two new disaggregated network fabrics and a new NIC to OCP. We look forward to continued collaboration with OCP to open designs for racks, servers, storage [...] Read More... The post OCP Summit 2024: The open future of networking hardware for AI appeared first on Engineering at Meta.",
  "author": "",
  "published": "Tue, 15 Oct 2024 17:06:46 +0000",
  "source": "https://engineering.fb.com/feed/",
  "categories": [
    "Data Center Engineering",
    "Data Infrastructure",
    "DevInfra",
    "ML Applications",
    "Networking \u0026 Traffic",
    "Open Source",
    "Production Engineering"
  ],
  "byline": "",
  "length": 6502,
  "excerpt": "At Open Compute Project Summit (OCP) 2024, we’re sharing details about our next-generation network fabric for our AI training clusters. We’ve expanded our network hardware portfolio and are contrib…",
  "siteName": "Engineering at Meta",
  "favicon": "",
  "text": "At Open Compute Project Summit (OCP) 2024, we’re sharing details about our next-generation network fabric for our AI training clusters. We’ve expanded our network hardware portfolio and are contributing two new disaggregated network fabrics and a new NIC to OCP. We look forward to continued collaboration with OCP to open designs for racks, servers, storage boxes, and motherboards to benefit companies of all sizes across the industry. At Meta, we believe that open hardware drives innovation. In today’s world, where more and more data center infrastructure is being devoted to supporting new and emerging AI technologies, open hardware takes on an important role in assisting with disaggregation. By breaking down traditional data center technologies into their core components we can build new systems that are more flexible, scalable, and efficient.  Since helping found OCP in 2011, we’ve shared our data center and component designs, and open-sourced our network orchestration software to spark new ideas both in our own data centers and across the industry. Those ideas have made Meta’s data centers among the most sustainable and efficient in the world. Now, through OCP, we’re bringing new open advanced network technologies to our data centers, and the wider industry, for advanced AI applications. We’re announcing two new milestones for our data centers: Our next-generation network fabric for AI, and a new portfolio of network hardware that we’ve developed in close partnership with multiple vendors. Disaggregated network fabrics offer significant advantages in scalability over modular-chassis fabric switches. DSF: Scheduled fabric that is disaggregated and open  Network performance and availability play an important role in extracting the best performance out of our AI training clusters. It’s for that reason that we’ve continued to push for disaggregation in the backend network fabrics for our AI clusters. Over the past year we have developed a Disaggregated Scheduled Fabric (DSF) for our next-generation AI clusters to help us develop open, vendor-agnostic systems with interchangeable building blocks from vendors across the industry. DSF-based fabrics allow us to build large, non-blocking fabrics to support high-bandwidth AI clusters. DSF extends our disaggregating network systems to our VoQ-based switched systems that are powered by the open OCP-SAI standard and FBOSS, Meta’s own network operating system for controlling network switches. VoQ-based traffic scheduling ensures proactive congestion avoidance in the fabric rather than reactive congestion signaling and reaction. The DSF fabric supports an open and standard Ethernet-based RoCE interface to endpoints and accelerators across several xPUs and NICs, including Meta’s MTIA as well as from several vendors.  DSF platforms for next-generation AI fabrics  Arista 7700R4 series The DSF platforms, Arista 7700R4 series,  consist of dedicated leaf and spine systems that are combined to create a large, distributed switch. As a distributed system, DSF is designed to support high scale AI clusters. 7700R4C-38PE: DSF Leaf Switch DSF Distributed Leaf Switch (Broadcom Jericho3-AI based) 18 x 800GE (36 x 400GE) OSFP800 host ports 20 x 800Gbps (40 x 400Gbps) fabric ports 14.4Tbps of wirespeed performance with 16GB of buffers 7720R4-128PE: DSF Spine Switch DSF Distributed Spine Switch (Broadcom Ramon3 based) Accelerated compute optimized pipeline 128 x 800Gbps (256 x 400Gbps) fabric ports 102.4Tbps of wirespeed performance 51T switches for  next-generation 400G/800G fabrics Minipack3 (Broadcom Tomahawk5 based, designed by Meta and manufactured by Celestica) 51.2T switch. Meta will deploy two next-generation 400G fabric switches, the Minipack3 (the latest version of Minipack, Meta’s own fabric network switch) and the Cisco 8501, both of which are also backward compatible with previous 200G and 400G switches and will support upgrades to 400G and 800G. The Minipack3 utilizes Broadcom’s latest Tomahawk5 ASIC while the Cisco 8501 is based on Cisco’s Silicon One G200 ASIC. These high-performance switches transmit up to 51.2 Tbps with 64x OSFP ports, and the design is optimized without the need of retimers to achieve maximum power efficiency. They also have significantly reduced power per bit compared with predecessor models. Meta will run both the Minipack3 and Cisco 8501 on FBOSS. Cisco 8501 (Cisco Silicon One G200 based, designed and manufactured by Cisco) 51.2T switch. Optics: 2x400G FR4 optics for 400G/800G optical interconnection  Meta’s data center fabrics have evolved from 200 Gbps/400 Gbps to 400 Gbps/800 Gbps and we’ve already deployed 2x400G optics in our data centers. Evolving FBOSS and SAI for DSF We continue to embrace OCP-SAI to onboard the new network fabrics, switch hardware platforms, and optical transceivers to FBOSS. We have collaborated with vendors, and the OCP community, to evolve SAI. It now supports new features and concepts like DSF and other enhanced routing schemes. Developers and engineers from all over the world can work with this open hardware and contribute their own software that they, in turn, can use themselves and share with the wider industry. FBNIC: A multi-host foundational NIC designed by Meta We are continuing to design more ASICs, including the ASIC for FBNIC. FBNIC is a true multi-host foundational NIC and contains the first of our Meta-designed network ASICs for our server fleet and MTIA solutions. It can support up to four hosts with complete datapath isolation for each host.The FBNIC driver has been upstreamed (available from v6.11 kernel). The NIC module was designed by Marvell and has been contributed to OCP. FBNIC’s key features include: Network interfaces for up to 4×100/4×50/4×25 GE with SerDes support for up to 56G PAM4 per lane. Up to 4 independent PCIe Gen5 slices HW offloads including LSO, Checksum Line rate timestamping (for each host all the way from PHY) for PTP Header-Data split to assist Zero-Copy Compliant with OCP NIC 3.0, version 1.2.0, design specification Advancing AI means building data center infrastructure that goes beyond scale. It also has to allow for flexibility and perform efficiently and sustainably. At Meta, we envision a future of AI hardware systems that are not only scalable, but also open and collaborative. We encourage anyone who wants to help advance the future of networking hardware for AI to engage with OCP and Meta to help share the future of AI infrastructure.",
  "image": "https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta-2.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAt Open Compute Project Summit (OCP) 2024, we’re sharing details about our next-generation network fabric for our AI training clusters.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe’ve expanded our network hardware portfolio and are contributing two new disaggregated network fabrics and a new NIC to OCP.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe look forward to continued collaboration with OCP to open designs for racks, servers, storage boxes, and motherboards to benefit companies of all sizes across the industry.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eAt Meta, we believe that open hardware drives innovation. In today’s world, where more and more data center infrastructure is being devoted to supporting new and emerging AI technologies, open hardware takes on an important role in assisting with disaggregation. By breaking down traditional data center technologies into their core components we can build new systems that are more flexible, scalable, and efficient. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSince helping found OCP in 2011, we’ve shared our data center and component designs, and open-sourced our network orchestration software to spark new ideas both in our own data centers and across the industry. Those ideas have made Meta’s data centers\u003c/span\u003e \u003ca href=\"https://sustainability.atmeta.com/2024-sustainability-report/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eamong the most sustainable and efficient in the world\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. Now, through OCP, we’re bringing new open advanced network technologies to our data centers, and the wider industry, for advanced AI applications.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe’re announcing two new milestones for our data centers: Our next-generation network fabric for AI, and a new portfolio of network hardware that we’ve developed in close partnership with multiple vendors.\u003c/span\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_21877\" aria-describedby=\"caption-attachment-21877\"\u003e\u003cimg decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta-1.png?w=960\" alt=\"\" width=\"960\" height=\"540\" srcset=\"https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta-1.png 960w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta-1.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta-1.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta-1.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta-1.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta-1.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-21877\"\u003eDisaggregated network fabrics offer significant advantages in scalability over modular-chassis fabric switches.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch2\u003e\u003cspan\u003eDSF: Scheduled fabric that is disaggregated and open \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eNetwork performance and availability play an important role in extracting the best performance out of our\u003c/span\u003e \u003ca href=\"https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eAI training clusters\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. It’s for that reason that we’ve continued to push for disaggregation in the backend network fabrics for our AI clusters. Over the past year we have developed a Disaggregated Scheduled Fabric (DSF) for our next-generation AI clusters to help us develop open, vendor-agnostic systems with interchangeable building blocks from vendors across the industry. DSF-based fabrics allow us to build large, non-blocking fabrics to support high-bandwidth AI clusters.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eDSF extends our disaggregating network systems to our VoQ-based switched systems that are powered by the open\u003c/span\u003e \u003ca href=\"https://github.com/opencomputeproject/SAI\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eOCP-SAI\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e standard and\u003c/span\u003e \u003ca href=\"https://engineering.fb.com/2018/09/04/data-infrastructure/research-in-brief-building-switch-software-at-scale-and-in-the-open/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eFBOSS\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, Meta’s own network operating system for controlling network switches. VoQ-based traffic scheduling ensures proactive congestion avoidance in the fabric rather than reactive congestion signaling and reaction.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe DSF fabric supports an open and standard Ethernet-based RoCE interface to endpoints and accelerators across several xPUs and NICs, including Meta’s \u003c/span\u003e\u003ca href=\"https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/\"\u003e\u003cspan\u003eMTIA\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e as well as from several vendors. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eDSF platforms for next-generation AI fabrics \u003c/span\u003e\u003c/h2\u003e\n\u003ch3\u003e\u003cspan\u003eArista 7700R4 series\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eThe DSF platforms, Arista 7700R4 series,  consist of dedicated leaf and spine systems that are combined to create a large, distributed switch. As a distributed system, DSF is designed to support high scale AI clusters.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2024/10/7700R4C-38PE-e1729011213805.png?w=476\" alt=\"\" width=\"476\" height=\"267\" srcset=\"https://engineering.fb.com/wp-content/uploads/2024/10/7700R4C-38PE-e1729011213805.png 476w, https://engineering.fb.com/wp-content/uploads/2024/10/7700R4C-38PE-e1729011213805.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/10/7700R4C-38PE-e1729011213805.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e7700R4C-38PE: DSF Leaf Switch\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eDSF Distributed Leaf Switch (Broadcom Jericho3-AI based)\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e18 x 800GE (36 x 400GE) OSFP800 host ports\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e20 x 800Gbps (40 x 400Gbps) fabric ports\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e14.4Tbps of wirespeed performance with 16GB of buffers\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2024/10/7720R4-128PE-e1729011256820.png?w=597\" alt=\"\" width=\"597\" height=\"335\" srcset=\"https://engineering.fb.com/wp-content/uploads/2024/10/7720R4-128PE-e1729011256820.png 597w, https://engineering.fb.com/wp-content/uploads/2024/10/7720R4-128PE-e1729011256820.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/10/7720R4-128PE-e1729011256820.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/10/7720R4-128PE-e1729011256820.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e7720R4-128PE: DSF Spine Switch\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eDSF Distributed Spine Switch (Broadcom Ramon3 based)\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAccelerated compute optimized pipeline\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e128 x 800Gbps (256 x 400Gbps) fabric ports\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e102.4Tbps of wirespeed performance\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003e51T switches for  next-generation 400G/800G fabrics\u003c/span\u003e\u003c/h2\u003e\n\u003cfigure id=\"attachment_21880\" aria-describedby=\"caption-attachment-21880\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2024/10/Minipack3-e1729010564784.png?w=600\" alt=\"\" width=\"600\" height=\"401\" srcset=\"https://engineering.fb.com/wp-content/uploads/2024/10/Minipack3-e1729010564784.png 600w, https://engineering.fb.com/wp-content/uploads/2024/10/Minipack3-e1729010564784.png?resize=96,64 96w, https://engineering.fb.com/wp-content/uploads/2024/10/Minipack3-e1729010564784.png?resize=192,128 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-21880\"\u003eMinipack3 (Broadcom Tomahawk5 based, designed by Meta and manufactured by Celestica) 51.2T switch.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cspan\u003eMeta will deploy two next-generation 400G fabric switches, the Minipack3 (the latest version of \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2019/03/14/data-center-engineering/f16-minipack/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eMinipack\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, Meta’s own fabric network switch) and the Cisco 8501, both of which are also backward compatible with previous 200G and 400G switches and will support upgrades to 400G and 800G.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe Minipack3 utilizes Broadcom’s latest Tomahawk5 ASIC while the Cisco 8501 is based on Cisco’s Silicon One G200 ASIC. These high-performance switches transmit up to 51.2 Tbps with 64x OSFP ports, and the design is optimized without the need of retimers to achieve maximum power efficiency. They also have significantly reduced power per bit compared with predecessor models.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eMeta will run both the Minipack3 and Cisco 8501 on FBOSS.\u003c/span\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_21881\" aria-describedby=\"caption-attachment-21881\"\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2024/10/Cisco-8501-e1729010680692.png?w=600\" alt=\"\" width=\"600\" height=\"230\" srcset=\"https://engineering.fb.com/wp-content/uploads/2024/10/Cisco-8501-e1729010680692.png 600w, https://engineering.fb.com/wp-content/uploads/2024/10/Cisco-8501-e1729010680692.png?resize=96,37 96w, https://engineering.fb.com/wp-content/uploads/2024/10/Cisco-8501-e1729010680692.png?resize=192,74 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-21881\"\u003eCisco 8501 (Cisco Silicon One G200 based, designed and manufactured by Cisco) 51.2T switch.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch2\u003e\u003cspan\u003eOptics: 2x400G FR4 optics for 400G/800G optical interconnection \u003c/span\u003e\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2024/10/400G-FR4--e1729010852824.png?w=372\" alt=\"\" width=\"372\" height=\"209\" srcset=\"https://engineering.fb.com/wp-content/uploads/2024/10/400G-FR4--e1729010852824.png 372w, https://engineering.fb.com/wp-content/uploads/2024/10/400G-FR4--e1729010852824.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/10/400G-FR4--e1729010852824.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eMeta’s data center fabrics have evolved from 200 Gbps/400 Gbps to 400 Gbps/800 Gbps and \u003c/span\u003e\u003cspan\u003ewe’ve already deployed 2x400G optics in our data centers\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eEvolving FBOSS and SAI for DSF\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2024/10/SAI-FBOSS-logo.png?w=456\" alt=\"\" width=\"456\" height=\"168\" srcset=\"https://engineering.fb.com/wp-content/uploads/2024/10/SAI-FBOSS-logo.png 456w, https://engineering.fb.com/wp-content/uploads/2024/10/SAI-FBOSS-logo.png?resize=96,35 96w, https://engineering.fb.com/wp-content/uploads/2024/10/SAI-FBOSS-logo.png?resize=192,71 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe continue to embrace OCP-SAI to onboard the new network fabrics, switch hardware platforms, and optical transceivers to FBOSS. We have collaborated with vendors, and the OCP community, to evolve SAI. It now supports new features and concepts like DSF and other enhanced routing schemes.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eDevelopers and engineers from all over the world can work with this open hardware and contribute their own software that they, in turn, can use themselves and share with the wider industry.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eFBNIC: A multi-host foundational NIC designed by Meta\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2024/10/FBNIC-e1729010986979.png?w=600\" alt=\"\" width=\"600\" height=\"280\" srcset=\"https://engineering.fb.com/wp-content/uploads/2024/10/FBNIC-e1729010986979.png 600w, https://engineering.fb.com/wp-content/uploads/2024/10/FBNIC-e1729010986979.png?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2024/10/FBNIC-e1729010986979.png?resize=192,90 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe are continuing to design more ASICs, including the ASIC for FBNIC. FBNIC is a true multi-host foundational NIC and contains the first of our Meta-designed network ASICs for our server fleet and \u003c/span\u003e\u003ca href=\"https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/\"\u003e\u003cspan\u003eMTIA\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e solutions. It can support up to four hosts with complete datapath isolation for each host.The FBNIC driver has been upstreamed (available from v6.11 kernel). The NIC module was designed by Marvell and has been contributed to OCP.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFBNIC’s key features include:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eNetwork interfaces for up to 4×100/4×50/4×25 GE with SerDes support for up to 56G PAM4 per lane.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eUp to 4 independent PCIe Gen5 slices\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eHW offloads including LSO, Checksum\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eLine rate timestamping (for each host all the way from PHY) for PTP\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eHeader-Data split to assist Zero-Copy\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eCompliant with OCP NIC 3.0, version 1.2.0, design specification\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cspan\u003eAdvancing AI means building data center infrastructure that goes beyond scale. It also has to allow for flexibility and perform efficiently and sustainably. At Meta, we envision a future of AI hardware systems that are not only scalable, but also open and collaborative.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe encourage anyone who wants to help advance the future of networking hardware for AI to engage with OCP and Meta to help share the future of AI infrastructure. \u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-10-15T17:06:46Z",
  "modifiedTime": "2024-10-16T16:31:32Z"
}
