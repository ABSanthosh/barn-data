{
  "id": "c5d2cd7f-3389-49b5-88f8-c469b7b52629",
  "title": "How We Generated Millions of Content Annotations",
  "link": "https://engineering.atspotify.com/2024/10/how-we-generated-millions-of-content-annotations/",
  "description": "With the fields of machine learning (ML) and generative AI (GenAI) continuing to rapidly evolve and expand, it has become [...] The post How We Generated Millions of Content Annotations appeared first on Spotify Engineering.",
  "author": "alexandrawei",
  "published": "Mon, 21 Oct 2024 14:19:58 +0000",
  "source": "https://labs.spotify.com/feed/",
  "categories": [
    "Data",
    "Machine Learning",
    "machine learning"
  ],
  "byline": "alexandrawei",
  "length": 7105,
  "excerpt": "How We Generated Millions of Content Annotations - Spotify Engineering",
  "siteName": "Spotify Engineering",
  "favicon": "https://storage.googleapis.com/production-eng/1/2021/03/cropped-Engineering-Icon-RGB-512x512-light-250x250.png",
  "text": "October 21, 2024 Published by Dana Puleo (Annotation Platform Ops Manager), Meghana Seetharam (Staff Engineer), and Katarzyna Drzyzga (Staff Engineer) With the fields of machine learning (ML) and generative AI (GenAI) continuing to rapidly evolve and expand, it has become increasingly important for innovators in this field to anchor their model development on high-quality data. As one of the foundational teams at Spotify focused on understanding and enriching the core content in our catalogs, we leverage ML in many of our products. For example, we use ML to detect content relations so a new track or album will be automatically placed on the right Artist Page. We also use it to analyze podcast audio, video, and metadata to identify platform policy violations. To power such experiences, we need to build several ML models that cover entire content catalogs — hundreds of millions of tracks and podcast episodes. To implement ML at this scale, we needed a strategy to collect high-quality annotations to train and evaluate our models. We wanted to improve the data collection process to be more efficient and connected and to include the right context for engineers and domain experts to operate more effectively. Figure 1: Ad hoc data collection processes. To address this, we had to evaluate the end-to-end workflow. We took a straightforward ML classification project, identified the manual steps to generate annotations, and aimed to automate them. We developed scripts to sample predictions, served data for operator review, and integrated the results with model training and evaluation workflows. We increased the corpus of annotations by 10 times and did so with three times the improvement in annotator productivity. Taking that as a promising sign, we further experimented with this workflow for other ML tasks. Once we confirmed the benefits of our approach, we decided to invest in this solution in earnest. Our next objective was to define the strategy to build a platform that would scale to millions of annotations. Building and scaling our annotation platform We centered our strategy around three main pillars: Scaling human expertise.  Implementing annotation tooling capabilities. Establishing foundational infrastructure and integration. Figure 2: Pillars of the annotation platform. 1. Scaling human expertise. Figure 3: Annotation workflow diagram. In order to scale operations, it was imperative that we defined processes to centralize and organize our annotation resources. We established large-scale expert human workforces in several domains to address our growing use cases, with multiple levels of experts, including the following: Core annotator workforces: These workforces are domain experts, who provide first-pass review of all annotation cases. Quality analysts: Quality analysts are top-level domain experts, who act as the escalation point for all ambiguous or complex cases identified by the core annotator workforce.  Project managers: This includes individuals who connect engineering and product teams to the workforce, establish and maintain training materials, and organize feedback on data collection strategies. Beyond human expertise, we also built a configurable, LLM-based system that runs in parallel to the human experts. It has allowed us to significantly grow our corpus of high-quality annotation data with low effort and cost. 2. Implementing annotation tooling capabilities. Figure 4: Annotation tooling capabilities. Although we started with a simple classification annotation project (the annotation task being answering a question), we soon realized that we had more complex use cases — such as annotating audio/video segments, natural language processing, etc. — which led to the development of custom interfaces, so we could easily spin up new projects. In addition, we invested in tools to manage backend work, such as project management, access control, and distribution of annotations across multiple experts. This enabled us to deploy and run dozens of annotation projects in parallel, all while ensuring that experts remained productive across multiple projects. Another focus area was project metrics — such as project completion rate, data volumes, annotations per annotator, etc. These metrics helped project managers and ML teams track their projects. We also examined the annotation data itself. For some of our use cases, there were nuances in the annotation task — for example, detecting music that was overlaid in a podcast episode audio snippet. In these cases, different experts may have different answers and opinions, so we started to compute an overall “agreement” metric. Any data points without a clear resolution were automatically escalated to our quality analysts. This ensures that our models receive the highest confidence annotation for training and evaluation. 3. Establishing foundational infrastructure and integration. Figure 5: Infrastructure to integrate with the tooling. At Spotify’s scale, no one tool or application will satisfy all our needs — optionality is key. When we designed integrations with annotation tools, we were intentional about building the right abstractions. They have to be flexible and adaptable to different tools so we can leverage the right tool for the right use case. Our data models, APIs, and interfaces are generic and can be used with multiple types of annotation tooling. We built bindings for direct integration with ML workflows at various stages from inception to production. For early/new ML development, we built CLIs and UIs for ad hoc projects. For production workflows, we built integrations with internal batch orchestration and workflow infrastructure.  Figure 6: Rate of annotations over time. Conclusion  The annotation platform now allows for flexibility, agility, and speed within our annotation spaces. By democratizing high-quality annotations, we’ve been able to significantly reduce the time it takes to develop new ML models and iterate on existing systems. Putting an emphasis from the onset on both scaling our human domain expertise and machine capabilities was key. Scaling humans without scaling technical capabilities to support them would have presented various challenges, and only focusing on scaling technically would have resulted in lost opportunities.  It was a major investment to move from ad hoc projects to a full-scale platform solution to support ML and GenAI use cases. We continue to iterate on and improve the platform offering, incorporating the latest advancements in the industry. Acknowledgments A special thanks to Linden Vongsathorn and Marqia Williams for their support in launching this initiative and to the many people at Spotify today who continue to contribute to this important mission. Tags: machine learning",
  "image": "https://storage.googleapis.com/production-eng/1/2024/10/EN224-1200-x-630-2.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOctober 21, 2024\u003c/span\u003e\n                \u003cspan\u003e\n                    Published by Dana Puleo (Annotation Platform Ops Manager), Meghana Seetharam (Staff Engineer), and Katarzyna Drzyzga (Staff Engineer)                \u003c/span\u003e\n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2024/10/how-we-generated-millions-of-content-annotations/\" title=\"How We Generated Millions of Content Annotations\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/production-eng/1/2024/10/EN224-1200-x-590-3.png\" alt=\"\" decoding=\"async\" fetchpriority=\"high\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/10/EN224-1200-x-590-3.png 1800w, https://storage.googleapis.com/production-eng/1/2024/10/EN224-1200-x-590-3-250x123.png 250w, https://storage.googleapis.com/production-eng/1/2024/10/EN224-1200-x-590-3-700x344.png 700w, https://storage.googleapis.com/production-eng/1/2024/10/EN224-1200-x-590-3-768x378.png 768w, https://storage.googleapis.com/production-eng/1/2024/10/EN224-1200-x-590-3-1536x755.png 1536w, https://storage.googleapis.com/production-eng/1/2024/10/EN224-1200-x-590-3-120x59.png 120w\" sizes=\"(max-width: 1800px) 100vw, 1800px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWith the fields of machine learning (ML) and generative AI (GenAI) continuing to rapidly evolve and expand, it has become increasingly important for innovators in this field to anchor their model development on high-quality data.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs one of the foundational teams at Spotify focused on understanding and enriching the core content in our catalogs, we leverage ML in many of our products. For example, we use ML to detect content relations so a new track or album will be automatically placed on the right Artist Page. We also use it to analyze podcast audio, video, and metadata to identify platform policy violations. To power such experiences, we need to build several ML models that cover entire content catalogs — hundreds of millions of tracks and podcast episodes. To implement ML at this scale, we needed a strategy to collect high-quality annotations to train and evaluate our models. We wanted to improve the data collection process to be more efficient and connected and to include the right context for engineers and domain experts to operate more effectively.\u003c/p\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1623\" height=\"948\" src=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-1.png\" alt=\"\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-1.png 1623w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-1-250x146.png 250w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-1-700x409.png 700w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-1-768x449.png 768w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-1-1536x897.png 1536w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-1-120x70.png 120w\" sizes=\"(max-width: 1623px) 100vw, 1623px\"/\u003e\u003cfigcaption\u003eFigure 1: Ad hoc data collection processes.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eTo address this, we had to evaluate the end-to-end workflow. We took a straightforward ML classification project, identified the manual steps to generate annotations, and aimed to automate them. We developed scripts to sample predictions, served data for operator review, and integrated the results with model training and evaluation workflows. We increased the corpus of annotations by 10 times and did so with three times the improvement in annotator productivity.\u003c/p\u003e\n\n\n\n\u003cp\u003eTaking that as a promising sign, we further experimented with this workflow for other ML tasks. Once we confirmed the benefits of our approach, we decided to invest in this solution in earnest. Our next objective was to define the strategy to build a platform that would scale to millions of annotations.\u003c/p\u003e\n\n\n\n\u003ch2\u003eBuilding and scaling our annotation platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe centered our strategy around three main pillars:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eScaling human expertise. \u003c/li\u003e\n\n\n\n\u003cli\u003eImplementing annotation tooling capabilities.\u003c/li\u003e\n\n\n\n\u003cli\u003eEstablishing foundational infrastructure and integration.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"642\" height=\"944\" src=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-2.png\" alt=\"\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-2.png 642w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-2-250x368.png 250w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-2-120x176.png 120w\" sizes=\"(max-width: 642px) 100vw, 642px\"/\u003e\u003cfigcaption\u003eFigure 2: Pillars of the annotation platform.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003ch2\u003e1. Scaling human expertise.\u003c/h2\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1303\" src=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-3-1.png\" alt=\"\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-3-1.png 1999w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-3-1-250x163.png 250w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-3-1-700x456.png 700w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-3-1-768x501.png 768w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-3-1-1536x1001.png 1536w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-3-1-120x78.png 120w\" sizes=\"auto, (max-width: 1999px) 100vw, 1999px\"/\u003e\u003cfigcaption\u003eFigure 3: Annotation workflow diagram.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eIn order to scale operations, it was imperative that we defined processes to centralize and organize our annotation resources.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe established large-scale expert human workforces in several domains to address our growing use cases, with multiple levels of experts, including the following:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCore annotator workforces:\u003c/strong\u003e These workforces are domain experts, who provide first-pass review of all annotation cases.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eQuality analysts:\u003c/strong\u003e Quality analysts are top-level domain experts, who act as the escalation point for all ambiguous or complex cases identified by the core annotator workforce. \u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eProject managers:\u003c/strong\u003e This includes individuals who connect engineering and product teams to the workforce, establish and maintain training materials, and organize feedback on data collection strategies.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eBeyond human expertise, we also built a configurable, LLM-based system that runs in parallel to the human experts. It has allowed us to significantly grow our corpus of high-quality annotation data with low effort and cost.\u003c/p\u003e\n\n\n\n\u003ch2\u003e2. Implementing annotation tooling capabilities.\u003c/h2\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"949\" height=\"779\" src=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-4.png\" alt=\"\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-4.png 949w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-4-250x205.png 250w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-4-700x575.png 700w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-4-768x630.png 768w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-4-120x99.png 120w\" sizes=\"auto, (max-width: 949px) 100vw, 949px\"/\u003e\u003cfigcaption\u003eFigure 4: Annotation tooling capabilities.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eAlthough we started with a simple classification annotation project (the annotation task being answering a question), we soon realized that we had more complex use cases — such as annotating audio/video segments, natural language processing, etc. — which led to the development of custom interfaces, so we could easily spin up new projects.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn addition, we invested in tools to manage backend work, such as project management, access control, and distribution of annotations across multiple experts. This enabled us to deploy and run dozens of annotation projects in parallel, all while ensuring that experts remained productive across multiple projects.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnother focus area was project metrics — such as project completion rate, data volumes, annotations per annotator, etc. These metrics helped project managers and ML teams track their projects. We also examined the annotation data itself. For some of our use cases, there were nuances in the annotation task — for example, detecting music that was overlaid in a podcast episode audio snippet. In these cases, different experts may have different answers and opinions, so we started to compute an overall “agreement” metric. Any data points without a clear resolution were automatically escalated to our quality analysts. This ensures that our models receive the highest confidence annotation for training and evaluation.\u003c/p\u003e\n\n\n\n\u003ch2\u003e3. Establishing foundational infrastructure and integration.\u003c/h2\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1278\" height=\"926\" src=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-5-1.png\" alt=\"\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-5-1.png 1278w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-5-1-250x181.png 250w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-5-1-700x507.png 700w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-5-1-768x556.png 768w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-5-1-120x87.png 120w\" sizes=\"auto, (max-width: 1278px) 100vw, 1278px\"/\u003e\u003cfigcaption\u003eFigure 5: Infrastructure to integrate with the tooling.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eAt Spotify’s scale, no one tool or application will satisfy all our needs — optionality is key. When we designed integrations with annotation tools, we were intentional about building the right abstractions. They have to be flexible and adaptable to different tools so we can leverage the right tool for the right use case. Our data models, APIs, and interfaces are generic and can be used with multiple types of annotation tooling.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe built bindings for direct integration with ML workflows at various stages from inception to production. For early/new ML development, we built CLIs and UIs for ad hoc projects. For production workflows, we built integrations with internal batch orchestration and workflow infrastructure.\u003cem\u003e \u003c/em\u003e\u003c/p\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"823\" height=\"596\" src=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-6.png\" alt=\"\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/10/Figure-6.png 823w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-6-250x181.png 250w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-6-700x507.png 700w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-6-768x556.png 768w, https://storage.googleapis.com/production-eng/1/2024/10/Figure-6-120x87.png 120w\" sizes=\"auto, (max-width: 823px) 100vw, 823px\"/\u003e\u003cfigcaption\u003eFigure 6: Rate of annotations over time.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003ch2\u003eConclusion \u003c/h2\u003e\n\n\n\n\u003cp\u003eThe annotation platform now allows for flexibility, agility, and speed within our annotation spaces. By democratizing high-quality annotations, we’ve been able to significantly reduce the time it takes to develop new ML models and iterate on existing systems.\u003c/p\u003e\n\n\n\n\u003cp\u003ePutting an emphasis from the onset on both scaling our human domain expertise and machine capabilities was key. Scaling humans without scaling technical capabilities to support them would have presented various challenges, and only focusing on scaling technically would have resulted in lost opportunities. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt was a major investment to move from ad hoc projects to a full-scale platform solution to support ML and GenAI use cases. We continue to iterate on and improve the platform offering, incorporating the latest advancements in the industry.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAcknowledgments\u003c/h2\u003e\n\n\n\n\u003cp\u003eA special thanks to Linden Vongsathorn and Marqia Williams for their support in launching this initiative and to the many people at Spotify today who continue to contribute to this important mission.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-10-21T14:19:58Z",
  "modifiedTime": "2024-11-25T14:21:48Z"
}
