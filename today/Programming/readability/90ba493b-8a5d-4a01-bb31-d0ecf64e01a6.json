{
  "id": "90ba493b-8a5d-4a01-bb31-d0ecf64e01a6",
  "title": "Fixed-Power Designs: It’s Not IF You Peek, It’s WHAT You Peek at",
  "link": "https://engineering.atspotify.com/2024/05/fixed-power-designs-its-not-if-you-peek-its-what-you-peek-at/",
  "description": "TL;DR Sometimes we cannot estimate the required sample size needed to power an experiment before starting it. To alleviate this [...] The post Fixed-Power Designs: It’s Not IF You Peek, It’s WHAT You Peek at appeared first on Spotify Engineering.",
  "author": "Spotify Engineering",
  "published": "Wed, 15 May 2024 15:07:37 +0000",
  "source": "https://labs.spotify.com/feed/",
  "categories": [
    "Data",
    "Data Science",
    "experimentation"
  ],
  "byline": "Spotify Engineering",
  "length": 15621,
  "excerpt": "Fixed-Power Designs: It’s Not IF You Peek, It’s WHAT You Peek at - Spotify Engineering",
  "siteName": "Spotify Engineering",
  "favicon": "https://storage.googleapis.com/production-eng/1/2021/03/cropped-Engineering-Icon-RGB-512x512-light-250x250.png",
  "text": "May 15, 2024 TL;DR Sometimes we cannot estimate the required sample size needed to power an experiment before starting it. To alleviate this problem, we could run a sequential test or an A/A test. However, sequential tests are typically less sensitive and introduce bias to the treatment effect estimator. Moreover, A/A tests prolong the duration of the experiment and still don’t guarantee that the resulting sample size calculation is accurate. In this blog post, we present highlights from our recent paper (Nordin and Schultzberg, 2024), where we introduce an alternative that we call “fixed-power design.” In a fixed-power design, you start the experiment without an estimated sample size, estimate the required sample size from the currently available outcome data in the experiment, and stop when your current sample size is larger than the required sample size. We show that fixed-power designs can be analyzed using nonsequential methods without any corrections. The point estimator is consistent, and the treatment effect confidence interval has asymptotically nominal coverage. Not all forms of peeking inflate the false positive rate of fixed-sample inference.  Introduction There are many reasons why companies use online experiments. Reasons could be, for example, the following: To identify the best version of a product To quantify the impact of a product change To detect regressions before a bug reaches all users  Online experiments let you do all these things while managing the risk of making the wrong decisions.  At Spotify, some of the goals of experimentation are to learn what works, how well it works, and stop the things that don’t work early on. However, the extent to which we can achieve these goals depends on the experimental design and analysis. For example, certain designs promote early stopping but pay a price in terms of power — with reduced chances overall of finding true effects. Other designs instead focus on maximizing power, but at the expense of prolonged runtime because they don’t allow early stopping. In the next section, we dig into the most common designs for A/B tests, discuss limitations of common approaches, and introduce a new design that mitigates some of those limitations.  Sequential experimental designs: more than just sequential testing Two of the most fundamental concerns of experimental design are when to stop the experiment and when to analyze the results. Experimental designs can be roughly split into two categories: fixed-sample designs and sequential designs.  Fixed-sample designs In fixed-sample designs, the experimenter leverages power analyses, also known as sample size calculations, to set a predetermined sample size. The power analysis produces a required sample size that the experiment should meet. If it does, the comparison will have high enough precision to limit the risk of missing effects of a certain size of interest. The experiment collects data until the predecided sample size is met, at which point the statistical analysis is performed. Sequential designs In sequential designs, the sample size isn’t predetermined.1 In principle, a sequential design is a design where users enter the experiment sequentially and the experiment stops according to a rule based on the available data. The sequential design uses a stopping rule that only indirectly determines the sample size as it’s being evaluated during the experiment. The most common sequential design, often simply called “sequential testing,” stops once the test detects a significant result.  In the context of online experimentation, many recommend using sequential tests to detect regressions but recommend against using sequential tests for the shipping decision, due to power and bias concerns with sequential tests. See, for example, Fan et al. (2004) and our previous blog post on comparing sequential testing methods.   Using a hybrid design In practice, many companies, in fact, use a hybrid design. That is, the treatment effect is estimated and evaluated using statistical tests derived for fixed-sample designs. However, the design is sequential, because the experiment runs until the current sample size exceeds the estimated required sample size. The estimated required sample size, in turn, uses an estimate of the variance derived from the data collected so far in the experiment. In Nordin and Schultzberg (2024), we call this a fixed-power design — you sample new users until the power criterion, according to the currently available data — is met.  Trajectories of the estimated required sample sizes for 50 experiments with a fixed treatment effect over sequential sampling. The fixed-power design stops the first time the estimated required sample size goes under the current sample size indicated by the light blue line. The blue lines are the two replications that stopped at the smallest and largest sample sizes, respectively. Their stopping sample sizes are indicated by the dashed lines. The dotted line is the true required sample size.  The graph above shows an example of how a fixed-power design can play out. To keep the graph easy to read, the sample size is kept small. In this case, the experiment stops very close to the true required sample size. In large samples (small-powered effects), as the required sample size estimator becomes precise, the region in which the sample size crosses the estimated required sample size will often be quite narrow.  Fixed-power designs: a summary of Nordin and Schultzberg (2024) In Nordin and Schultzberg (2024), we investigate the properties of the difference-in-means average treatment effect estimator in sequential designs where the stopping rule is based on the precision of the treatment effect estimator. We express the precision in two ways, as the confidence interval (CI) width, and the current required sample size for a given hypothetical treatment effect. As shown in our paper, stopping based on the required sample size is equivalent to stopping on the confidence interval width, as it’s just a transformation of the variance of the treatment effect estimator.  These kinds of stopping rules based on precision are common in practice, with some experimentation vendors even selling them. However, to the best of our knowledge, the statistical implications of ‌precision-based stopping rules haven’t been rigorously investigated.  The fixed-power design can make your peeking problem alarm bell go off. It is, after all, a stopping rule that uses outcome data to determine whether to stop. This is what’s responsible for why we use sequential testing in the first place, and therefore, it’s not unreasonable to expect corrections to be required if you stop based on the required sample size, too. However, in Nordin and Schultzberg (2024), we show that not all stopping rules based on ‌outcome data are equally problematic for ‌statistical inference. Our research shows that functions of the sample variance are much less problematic than stopping based on, for example,‌ significance.  What aspects of the outcome data we peek at determines the effects — if any — of peeking on an inference about the estimand we are interested in. In our paper, we show that under a fixed-power design, the following are true: The difference-in-means estimator consistently estimates the average treatment effect. The fixed-sample confidence interval for the average treatment effect has asymptotically correct coverage. This means that in large samples, we can use standard inference even when we stop based on the current estimated required sample size. No further adjustments are necessary to guarantee correct inference. In the paper, we also propose conservative finite-sample versions of the fixed-power design and the fixed-width confidence interval design. Pre-experiment sample size calculation is hard Fixed-power designs let us peek at the required sample size without adjusting inference. Why is this important? Can’t we use historical data for a power analysis to determine the required sample size?  Estimating the required sample size during experiments as a complement to pre-experiment power analyses is often necessary, as historical data can fail to accurately describe the outcome distributions. At Spotify, for example, the diverse and ever-changing user base, especially in new markets and with new-user experiments, makes historical comparisons unreliable. Adding to that, historical data won’t reflect treatment effects since new variants haven’t been tested, and assuming homogeneous treatment effects across a diverse customer base is unrealistic. Users with different listening habits will likely respond differently to the same feature changes.  Leveraging observed outcomes during experiments can enhance sample size accuracy and inform the experimenter early on if their initial planning is accurate. With the guarantees of fixed-power designs, we can plan according to a required sample size calculated before the experiment, revise it during the experiment, and finally stop at the right time — all while relying on standard fixed-sample inference.  Sequential testing versus fixed-sample testing Sequential tests give valid inference under any stopping rule, so why not just rely on sequential tests and peek at the required sample size as much as we want?2  As have been discussed in many places (Larsen et al. 2024, our previous blog post), there are two main reasons: Unbiased point estimators. Sequential tests that stop on significance yield biased estimators that overestimate ‌effect size. Moreover, the idea of stopping on first significance is in stark contrast to the advice of many not to trust experiments with too low power.     Power. In most situations, experiments must run for at least a given period. This could, for example, be to obtain data on users in the experiment for a sufficiently long time to rule out novelty effects. Another reason, common at Spotify, is to avoid issues with seasonal effects on weekdays. Using sequential testing in situations where we don’t intend to stop based on the first significance is a waste of power. If stopping is prohibited during a large part of the experiment, sequential tests that aren’t taking this into account will be highly conservative.  With the fixed-power design, we get the benefits from fixed-sample designs, but with the added ability to inform the stopping based on a continuous power analysis of the experiment. Sequential designTraditional fixed-sample designFixed-power design (Nordin and Schultzberg, 2024)– Sequential tests allow early stopping in experiments with a stopping rule based on significance or any other function of the data.– Sequential tests bound false positive rates and coverage at least at the intended level under early stopping. – Sequential tests are conservative if you always want to run the experiment until you reach a certain precision. This is because they adjust for early stopping on significance (even if you don’t use it).– Sequential tests (with early stopping) give biased difference-in-means estimators. – Fixed-sample tests require the sample size to be fixed ahead of time. – To achieve a certain precision, you need to estimate the variance of the outcome(s) from historical data before the experiment is started to plan the sample size.– The difference-in-means estimator is unbiased and the standard fixed-sample CI has the right coverage.  – Fixed-power designs estimate the current required sample size from outcome data during the experiment.– Fixed-power designs stop when the current sample size is larger than the estimated required. – Under a fixed-power design, the standard difference-in-means estimator is consistent, and the fixed-sample CI has asymptotic nominal coverage.  Summary of the Differences Between Common Designs and the New Proposed Fixed-Power Design. Summary In the ever-evolving landscape of online experiments, determining the optimal time to stop an experiment remains a substantial challenge. Traditional methods, such as fixed-sample and sequential test designs, each have limitations. Fixed-sample designs predetermine the sample size but don’t allow adjustments based on incoming data, while sequential test designs can adjust but may affect the power and bias of the results. In our recent paper, Nordin and Schultzberg (2024), we introduce an innovative approach called the “fixed-power design.” This method allows an experiment to start without a predefined sample size it needs to reach. Instead, the required sample size is estimated from ongoing outcome data, and the experiment concludes when the current sample size surpasses this estimate. Crucially, this design supports standard nonsequential inference, guaranteeing consistent point estimators and maintaining nominal coverage in confidence intervals. This means that the fixed-power design allows sequential stopping without losing the power benefits from nonsequential tests. This design is particularly advantageous in environments like Spotify, where the user base is diverse and constantly changing. Traditional pre-experiment calculations based on historical data often fall short because they don’t account for the variability in treatment effects across different user segments or for new user experiences. The fixed-power design provides a practical balance between the rigidity of fixed-sample designs and the flexibility of sequential test designs, providing reliable decision-making in product development. At the same time, many challenges remain. Although the fixed-power design makes it possible to do real-time adjustments to the required sample size, it’s problematic to not know the required sample size in the planning stage. At Spotify, where we run tens of thousands of experiments, there are always limitations to how large an experiment a team can run. If it’s detected during the experiment that the required sample size is much larger than the team had anticipated, it’s not always possible to run it longer or increase the proportion of the population that the experiment targets because of other conflicting experiments. In this situation, fixed-power designs offer a way to know early in an experiment if the data is in line with the pre-experiment power analysis.  Acknowledgments: This blog post is based on a paper written in collaboration with Mattias Nordin, Department of Statistics, Uppsala University, Sweden. Get access to Spotify’s decision engine via Confidence. In Confidence, you can always access the current required sample size and current-powered effect while an experiment is live. This means that you can use a fixed-power design by simply starting the experiment using a fixed-sample design. As usual, we ensure that the statistics are all in order for any results we show you — so you can focus on building a great product.  Want to learn more about Confidence? Check out the Confidence blog for more posts on Confidence and its functionality. Confidence is currently available in private beta. If you haven’t signed up already, sign up today, and we’ll be in touch. 1 In some types of sequential tests, the maximum sample size needs to be predecided, but not the stopping sample size.2 At least so-called always-valid sequential tests. Tags: Data, experimentation",
  "image": "https://storage.googleapis.com/production-eng/1/2024/05/EN218-Fixed-Power-Designs-Its-Not-If-You-Peek.WHITE-1200-x-630-1-1.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 15, 2024\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2024/05/fixed-power-designs-its-not-if-you-peek-its-what-you-peek-at/\" title=\"Fixed-Power Designs: It’s Not IF You Peek, It’s WHAT You Peek at\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/production-eng/1/2024/05/EN218-Fixed-Power-Designs-Its-Not-If-You-Peek.WHITE-1200-x-590-1-1.png\" alt=\"\" decoding=\"async\" fetchpriority=\"high\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/05/EN218-Fixed-Power-Designs-Its-Not-If-You-Peek.WHITE-1200-x-590-1-1.png 1201w, https://storage.googleapis.com/production-eng/1/2024/05/EN218-Fixed-Power-Designs-Its-Not-If-You-Peek.WHITE-1200-x-590-1-1-250x123.png 250w, https://storage.googleapis.com/production-eng/1/2024/05/EN218-Fixed-Power-Designs-Its-Not-If-You-Peek.WHITE-1200-x-590-1-1-700x344.png 700w, https://storage.googleapis.com/production-eng/1/2024/05/EN218-Fixed-Power-Designs-Its-Not-If-You-Peek.WHITE-1200-x-590-1-1-768x377.png 768w, https://storage.googleapis.com/production-eng/1/2024/05/EN218-Fixed-Power-Designs-Its-Not-If-You-Peek.WHITE-1200-x-590-1-1-120x59.png 120w\" sizes=\"(max-width: 1201px) 100vw, 1201px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e Sometimes we cannot estimate the required sample size needed to power an experiment before starting it. To alleviate this problem, we could run a sequential test or an A/A test. However, sequential tests are typically less sensitive and introduce bias to the treatment effect estimator. Moreover, A/A tests prolong the duration of the experiment and still don’t guarantee that the resulting sample size calculation is accurate. In this blog post, we present highlights from our recent paper (\u003ca href=\"https://arxiv.org/abs/2405.03487\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eNordin and Schultzberg, 2024\u003c/a\u003e), where we introduce an alternative that we call “fixed-power design.” In a fixed-power design, you start the experiment without an estimated sample size, estimate the required sample size from the currently available outcome data in the experiment, and stop when your current sample size is larger than the required sample size. We show that fixed-power designs can be analyzed using nonsequential methods without any corrections. The point estimator is consistent, and the treatment effect confidence interval has asymptotically nominal coverage. Not all forms of peeking inflate the false positive rate of fixed-sample inference. \u003c/p\u003e\n\n\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\n\n\u003cp\u003eThere are many reasons why companies use online experiments. Reasons could be, for example, the following:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eTo identify the best version of a product\u003c/li\u003e\n\n\n\n\u003cli\u003eTo quantify the impact of a product change\u003c/li\u003e\n\n\n\n\u003cli\u003eTo detect regressions before a bug reaches all users \u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eOnline experiments let you do all these things while managing the risk of making the wrong decisions. \u003c/p\u003e\n\n\n\n\u003cp\u003eAt Spotify, some of the goals of experimentation are to learn what works, how well it works, and stop the things that don’t work early on. However, the extent to which we can achieve these goals depends on the experimental design and analysis. For example, certain designs promote early stopping but pay a price in terms of power — with reduced chances overall of finding true effects. Other designs instead focus on maximizing power, but at the expense of prolonged runtime because they don’t allow early stopping. In the next section, we dig into the most common designs for A/B tests, discuss limitations of common approaches, and introduce a new design that mitigates some of those limitations. \u003c/p\u003e\n\n\n\n\u003ch2\u003eSequential experimental designs: more than just sequential testing\u003c/h2\u003e\n\n\n\n\u003cp\u003eTwo of the most fundamental concerns of experimental design are when to stop the experiment and when to analyze the results. Experimental designs can be roughly split into two categories: \u003cstrong\u003efixed-sample designs\u003c/strong\u003e and \u003cstrong\u003esequential designs\u003c/strong\u003e. \u003c/p\u003e\n\n\n\n\u003ch3\u003eFixed-sample designs\u003c/h3\u003e\n\n\n\n\u003cp\u003eIn fixed-sample designs, the experimenter leverages power analyses, also known as sample size calculations, to set a predetermined sample size. The power analysis produces a required sample size that the experiment should meet. If it does, the comparison will have high enough precision to limit the risk of missing effects of a certain size of interest. The experiment collects data until the predecided sample size is met, at which point the statistical analysis is performed.\u003c/p\u003e\n\n\n\n\u003ch3\u003eSequential designs\u003c/h3\u003e\n\n\n\n\u003cp\u003eIn sequential designs, the sample size isn’t predetermined.\u003csup\u003e1\u003c/sup\u003e In principle, a sequential design is a design where users enter the experiment sequentially and the experiment stops according to a rule based on the available data. The sequential design uses a stopping rule that only indirectly determines the sample size as it’s being evaluated during the experiment. The most common sequential design, often simply called “sequential testing,” stops once the test detects a significant result. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn the context of online experimentation, many recommend using sequential tests to detect regressions but recommend against using sequential tests for the shipping decision, due to power and bias concerns with sequential tests. See, for example, \u003ca href=\"https://doi.org/10.1081/BIP-120037195\"\u003eFan et al. (2004)\u003c/a\u003e and our \u003ca href=\"https://engineering.atspotify.com/2023/03/choosing-sequential-testing-framework-comparisons-and-discussions/\"\u003eprevious blog post\u003c/a\u003e on comparing sequential testing methods.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eUsing a hybrid design\u003c/h3\u003e\n\n\n\n\u003cp\u003eIn practice, many companies, in fact, use a hybrid design. That is, the treatment effect is estimated and evaluated using statistical tests derived for fixed-sample designs. However, the design is sequential, because the experiment runs until the current sample size exceeds the estimated required sample size. The estimated required sample size, in turn, uses an estimate of the variance derived from the data collected so far in the experiment. In Nordin and Schultzberg (2024), we call this a \u003cstrong\u003efixed-power design\u003c/strong\u003e — you sample new users until the power criterion, according to the currently available data — is met. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1600\" height=\"1000\" src=\"https://storage.googleapis.com/production-eng/1/2024/05/image1.png\" alt=\"\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/05/image1.png 1600w, https://storage.googleapis.com/production-eng/1/2024/05/image1-250x156.png 250w, https://storage.googleapis.com/production-eng/1/2024/05/image1-700x438.png 700w, https://storage.googleapis.com/production-eng/1/2024/05/image1-768x480.png 768w, https://storage.googleapis.com/production-eng/1/2024/05/image1-1536x960.png 1536w, https://storage.googleapis.com/production-eng/1/2024/05/image1-120x75.png 120w\" sizes=\"(max-width: 1600px) 100vw, 1600px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eTrajectories of the estimated required sample sizes for 50 experiments with a fixed treat\u003c/em\u003ement effect over sequential sampling. The fixed-power design stops the first time the estimated required sample size goes under the current sample size indicated by the light blue line. The blue lines are the two replications that stopped at the smallest and largest sample sizes, respectively. Their stopping sample sizes are indicated by the dashed lines. The dotted line is the true required sample size. \u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe graph above shows an example of how a fixed-power design can play out. To keep the graph easy to read, the sample size is kept small. In this case, the experiment stops very close to the true required sample size. In large samples (small-powered effects), as the required sample size estimator becomes precise, the region in which the sample size crosses the estimated required sample size will often be quite narrow. \u003c/p\u003e\n\n\n\n\u003ch2\u003eFixed-power designs: a summary of Nordin and Schultzberg (2024)\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn \u003ca href=\"https://arxiv.org/abs/2405.03487\"\u003eNordin and Schultzberg (2024)\u003c/a\u003e, we investigate the properties of the difference-in-means average treatment effect estimator in sequential designs where the stopping rule is based on the precision of the treatment effect estimator. We express the precision in two ways, as the confidence interval (CI) width, and the current required sample size for a given hypothetical treatment effect. As shown in our paper, stopping based on the required sample size is equivalent to stopping on the confidence interval width, as it’s just a transformation of the variance of the treatment effect estimator. \u003c/p\u003e\n\n\n\n\u003cp\u003eThese kinds of stopping rules based on precision are common in practice, with some experimentation vendors even selling them. However, to the best of our knowledge, the statistical implications of ‌precision-based stopping rules haven’t been rigorously investigated. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe fixed-power design can make your peeking problem alarm bell go off. It is, after all, a stopping rule that uses outcome data to determine whether to stop. This is what’s responsible for why we use sequential testing in the first place, and therefore, it’s not unreasonable to expect corrections to be required if you stop based on the required sample size, too. However, in Nordin and Schultzberg (2024), we show that not all stopping rules based on ‌outcome data are equally problematic for ‌statistical inference.\u003cstrong\u003e \u003c/strong\u003eOur research shows that functions of the sample variance are much less problematic than stopping based on, for example,‌ significance. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhat aspects of the outcome data we peek at determines the effects — if any — of peeking on an inference about the estimand we are interested in. In our paper, we show that under a fixed-power design, the following are true:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eThe difference-in-means estimator consistently estimates the average treatment effect.\u003c/li\u003e\n\n\n\n\u003cli\u003eThe fixed-sample confidence interval for the average treatment effect has asymptotically correct coverage.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eThis means that in large samples, we can use standard inference even when we stop based on the current estimated required sample size. No further adjustments are necessary to guarantee correct inference.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the paper, we also propose conservative finite-sample versions of the fixed-power design and the fixed-width confidence interval design.\u003c/p\u003e\n\n\n\n\u003ch2\u003ePre-experiment sample size calculation is hard\u003c/h2\u003e\n\n\n\n\u003cp\u003eFixed-power designs let us peek at the required sample size without adjusting inference. Why is this important? Can’t we use historical data for a power analysis to determine the required sample size? \u003c/p\u003e\n\n\n\n\u003cp\u003eEstimating the required sample size during experiments as a complement to pre-experiment power analyses is often necessary, as historical data can fail to accurately describe the outcome distributions. At Spotify, for example, the diverse and ever-changing user base, especially in new markets and with new-user experiments, makes historical comparisons unreliable. Adding to that, historical data won’t reflect treatment effects since new variants haven’t been tested, and assuming homogeneous treatment effects across a diverse customer base is unrealistic. Users with different listening habits will likely respond differently to the same feature changes. \u003c/p\u003e\n\n\n\n\u003cp\u003eLeveraging observed outcomes during experiments can enhance sample size accuracy and inform the experimenter early on if their initial planning is accurate. With the guarantees of fixed-power designs, we can plan according to a required sample size calculated before the experiment, revise it during the experiment, and finally stop at the right time — all while relying on standard fixed-sample inference. \u003c/p\u003e\n\n\n\n\u003ch2\u003eSequential testing versus fixed-sample testing\u003c/h2\u003e\n\n\n\n\u003cp\u003eSequential tests give valid inference under any stopping rule, so why not just rely on sequential tests and peek at the required sample size as much as we want?\u003csup\u003e2\u003c/sup\u003e \u003c/p\u003e\n\n\n\n\u003cp\u003eAs have been discussed in many places (\u003ca href=\"https://doi.org/10.1080/00031305.2023.2257237\"\u003eLarsen et al. 2024\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/2023/03/choosing-sequential-testing-framework-comparisons-and-discussions/\"\u003eour previous blog post\u003c/a\u003e), there are two main reasons:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eUnbiased point estimators.\u003c/strong\u003e Sequential tests that stop on significance yield biased estimators that overestimate ‌effect size. Moreover, the idea of stopping on first significance is in stark contrast to the advice of many not to trust experiments with too low power.    \u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003ePower.\u003c/strong\u003e In most situations, experiments must run for at least a given period. This could, for example, be to obtain data on users in the experiment for a sufficiently long time to rule out novelty effects. Another reason, common at Spotify, is to avoid issues with seasonal effects on weekdays. Using sequential testing in situations where we don’t intend to stop based on the first significance is a waste of power. If stopping is prohibited during a large part of the experiment, sequential tests that aren’t taking this into account will be highly conservative. \u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eWith the fixed-power design, we get the benefits from fixed-sample designs, but with the added ability to inform the stopping based on a continuous power analysis of the experiment.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd data-align=\"left\"\u003e\u003cstrong\u003eSequential design\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"left\"\u003e\u003cstrong\u003eTraditional fixed-sample design\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"left\"\u003e\u003cstrong\u003eFixed-power design (Nordin and Schultzberg, 2024)\u003c/strong\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd data-align=\"left\"\u003e– Sequential tests allow early stopping in experiments with a stopping rule based on significance or any other function of the data.\u003cbr/\u003e– Sequential tests bound false positive rates and coverage at least at the intended level under early stopping. \u003cbr/\u003e– Sequential tests are conservative if you always want to run the experiment until you reach a certain precision. This is because they adjust for early stopping on significance (even if you don’t use it).\u003cbr/\u003e– Sequential tests (with early stopping) give biased difference-in-means estimators. \u003c/td\u003e\u003ctd data-align=\"left\"\u003e– Fixed-sample tests require the sample size to be fixed ahead of time. \u003cbr/\u003e– To achieve a certain precision, you need to estimate the variance of the outcome(s) from historical data before the experiment is started to plan the sample size.\u003cbr/\u003e– The difference-in-means estimator is unbiased and the standard fixed-sample CI has the right coverage.  \u003c/td\u003e\u003ctd data-align=\"left\"\u003e– Fixed-power designs estimate the current required sample size from outcome data during the experiment.\u003cbr/\u003e– Fixed-power designs stop when the current sample size is larger than the estimated required. \u003cbr/\u003e– Under a fixed-power design, the standard difference-in-means estimator is consistent, and the fixed-sample CI has asymptotic nominal coverage.  \u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cfigcaption\u003eSummary of the Differences Between Common Designs and the New Proposed Fixed-Power Design.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the ever-evolving landscape of online experiments, determining the optimal time to stop an experiment remains a substantial challenge. Traditional methods, such as fixed-sample and sequential test designs, each have limitations. Fixed-sample designs predetermine the sample size but don’t allow adjustments based on incoming data, while sequential test designs can adjust but may affect the power and bias of the results.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn our recent paper, \u003ca href=\"https://arxiv.org/abs/2405.03487\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eNordin and Schultzberg (2024)\u003c/a\u003e, we introduce an innovative approach called the “fixed-power design.” This method allows an experiment to start without a predefined sample size it needs to reach. Instead, the required sample size is estimated from ongoing outcome data, and the experiment concludes when the current sample size surpasses this estimate. Crucially, this design supports standard nonsequential inference, guaranteeing consistent point estimators and maintaining nominal coverage in confidence intervals. This means that the fixed-power design allows sequential stopping without losing the power benefits from nonsequential tests.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis design is particularly advantageous in environments like Spotify, where the user base is diverse and constantly changing. Traditional pre-experiment calculations based on historical data often fall short because they don’t account for the variability in treatment effects across different user segments or for new user experiences.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe fixed-power design provides a practical balance between the rigidity of fixed-sample designs and the flexibility of sequential test designs, providing reliable decision-making in product development. At the same time, many challenges remain. Although the fixed-power design makes it possible to do real-time adjustments to the required sample size, it’s problematic to not know the required sample size in the planning stage. At Spotify, where we run tens of thousands of experiments, there are always limitations to how large an experiment a team can run. If it’s detected during the experiment that the required sample size is much larger than the team had anticipated, it’s not always possible to run it longer or increase the proportion of the population that the experiment targets because of other conflicting experiments. In this situation, fixed-power designs offer a way to know early in an experiment if the data is in line with the pre-experiment power analysis. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eAcknowledgments:\u003c/strong\u003e This blog post is based on a paper written in collaboration with Mattias Nordin, Department of Statistics, Uppsala University, Sweden.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cbr/\u003e\u003cstrong\u003eGet access to Spotify’s decision engine via Confidence.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIn \u003ca href=\"https://confidence.spotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eConfidence\u003c/a\u003e, you can always access the current required sample size and current-powered effect while an experiment is live. This means that you can use a fixed-power design by simply starting the experiment using a fixed-sample design. As usual, we ensure that the statistics are all in order for any results we show you — so you can focus on building a great product. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003e\u003cem\u003eWant to learn more about Confidence? Check out the \u003ca href=\"https://confidence.spotify.com/blog\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eConfidence blog\u003c/a\u003e for more posts on Confidence and its functionality. Confidence is currently available in private beta. If you haven’t signed up already, sign up today, and we’ll be in touch.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\n\u003c/blockquote\u003e\n\u003c/blockquote\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n\u003cp\u003e\u003csup\u003e1 \u003c/sup\u003eIn some types of sequential tests, the maximum sample size needs to be predecided, but not the stopping sample size.\u003cbr/\u003e\u003csup\u003e2\u003c/sup\u003e At least so-called always-valid sequential tests.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/experimentation/\" rel=\"tag\"\u003eexperimentation\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "17 min read",
  "publishedTime": "2024-05-15T15:07:37Z",
  "modifiedTime": "2024-05-28T18:50:02Z"
}
