{
  "id": "b9525b6c-3e06-40d6-ae22-de951f80c4a9",
  "title": "Datadog Employs LLMs for Assisting with Writing Accident Postmortems",
  "link": "https://www.infoq.com/news/2025/04/datadog-postmortem-llm-genai/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Datadog combined structured metadata from its incident management app with Slack messages to create an LLM-driven functionality assisting engineers in composing incident postmortems. While working on this solution, the company dealt with the challenges of using LLMs outside of the interactive dialog systems and ensuring that high-quality content was produced. By Rafal Gancarz",
  "author": "Rafal Gancarz",
  "published": "Sun, 13 Apr 2025 16:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Post-Mortems",
    "Generative AI",
    "Operations management",
    "Large language models",
    "Incident Response",
    "AI, ML \u0026 Data Engineering",
    "Architecture \u0026 Design",
    "news"
  ],
  "byline": "Rafal Gancarz",
  "length": 3781,
  "excerpt": "Datadog combined structured metadata from its incident management app with Slack messages to create an LLM-driven functionality assisting engineers in composing incident postmortems. While working on",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250413214503/apple-touch-icon.png",
  "text": "Datadog combined structured metadata from its incident management app with Slack messages to create an LLM-driven functionality assisting engineers in composing incident postmortems. While working on this solution, the company dealt with the challenges of using LLMs outside of the interactive dialog systems and ensuring that high-quality content was produced. Datadog opted to enhance the process of creating incident postmortems by utilizing LLMs to compile various sections of the postmortem report that engineers can subsequently review and customize to create the final version. The team working on this functionality spent over 100 hours fine-tuning the structure and LLM instructions to achieve satisfactory outcomes with diverse inputs. The team explored different model alternatives such as GPT-3.5 and GPT-4 to asses cost, speed and quality of results and discovered that these can differ significantly, depending on the model version. Engineers observed, for instance, that GPT-4 produced more accurate results but was also much slower and more expensive than GPT-3.5. In the end, based on experimentation, engineers opted to use different model versions for different sections, depending on the complexity of the content, to strike a balance between cost, speed, and accuracy. Additionally, building the report was executed in parallel for different sections, resulting in the total time getting reduced from 12 minutes to below 1 minute. Parallel execution of LLM requests with different models (Source: Datadog Engineering Blog) Another interesting and important aspect of combining AI and human inputs in the context of writing postmortem reports was trust and privacy. The team focused on explicitly marking AI-generated content as such to prevent human readers, including reviewers, from blindly accepting it as final. Additionally, engineers have ensured that any sensitive information and secrets were stripped from the data fed into LLM models and replaced with placeholders. Datadog engineers explain how they addressed data security concerns: Given the sensitivity of technical incidents, protecting confidential information was paramount. As part of the ingestion API, we implemented secret scanning and filtering mechanisms that scrubbed and replaced suspected secrets with placeholders before feeding data into the LLM. Once the AI-generated results were retrieved, placeholders were filled in with the actual content, ensuring privacy and security throughout the process. As part of the AI-enhanced solution, postmortem authors got the ability to customize templates used for various sections of the report. Section templates also included LLM instructions in clear text to further promote transparency and trust in the system and allow users to adjust LLM instructions to better suit their needs. Postmortem report section (Source: Datadog Engineering Blog) Having worked on the LLM-powered functionality, the Datadog team concluded that although they believe LLMs can support operations engineers in creating postmortem reports, they can’t fully replace humans, at least at present. Still, GenAI-enhanced products can greatly improve productivity and give human engineers a head start when working on incident reports. The team has learned a great deal while working on this functionality and plans to expand on the data sources available to LLMs when generating postmortem contents, including internal wikis, RFCs, and system information. Additionally, the developers would like to test using LLMs to generate alternative postmortem versions, including custom and public postmortems. About the Author Rafal Gancarz",
  "image": "https://res.infoq.com/news/2025/04/datadog-postmortem-llm-genai/en/headerimage/generatedHeaderImage-1744463307987.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eDatadog combined structured metadata from its incident management app with Slack messages to create \u003ca href=\"https://www.datadoghq.com/blog/engineering/llms-for-postmortems/\"\u003ean LLM-driven functionality assisting engineers in composing incident postmortems\u003c/a\u003e. While working on this solution, the company dealt with the challenges of using LLMs outside of the interactive dialog systems and ensuring that high-quality content was produced.\u003c/p\u003e\n\n\u003cp\u003eDatadog opted to enhance the process of creating incident postmortems by utilizing LLMs to compile various sections of the postmortem report that engineers can subsequently review and customize to create the final version. The team working on this functionality spent over 100 hours fine-tuning the structure and LLM instructions to achieve satisfactory outcomes with diverse inputs.\u003c/p\u003e\n\n\u003cp\u003eThe team explored different model alternatives such as GPT-3.5 and \u003ca href=\"https://openai.com/index/gpt-4/\"\u003eGPT-4\u003c/a\u003e to asses cost, speed and quality of results and discovered that these can differ significantly, depending on the model version. Engineers observed, for instance, that GPT-4 produced more accurate results but was also much slower and more expensive than GPT-3.5. In the end, based on experimentation, engineers opted to use different model versions for different sections, depending on the complexity of the content, to strike a balance between cost, speed, and accuracy. Additionally, building the report was executed in parallel for different sections, resulting in the total time getting reduced from 12 minutes to below 1 minute.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/04/datadog-postmortem-llm-genai/en/resources/1datadog-postmortem-parallel-1744558917055.jpeg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/04/datadog-postmortem-llm-genai/en/resources/1datadog-postmortem-parallel-1744558917055.jpeg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eParallel execution of LLM requests with different models (Source: \u003ca href=\"https://www.datadoghq.com/blog/engineering/llms-for-postmortems/\"\u003eDatadog Engineering Blog\u003c/a\u003e)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eAnother interesting and important aspect of combining AI and human inputs in the context of writing postmortem reports was trust and privacy. The team focused on explicitly marking AI-generated content as such to prevent human readers, including reviewers, from blindly accepting it as final. Additionally, engineers have ensured that any sensitive information and secrets were stripped from the data fed into LLM models and replaced with placeholders. Datadog engineers explain how they addressed data security concerns:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eGiven the sensitivity of technical incidents, protecting confidential information was paramount. As part of the ingestion API, we implemented secret scanning and filtering mechanisms that scrubbed and replaced suspected secrets with placeholders before feeding data into the LLM. Once the AI-generated results were retrieved, placeholders were filled in with the actual content, ensuring privacy and security throughout the process.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAs part of the AI-enhanced solution, postmortem authors got the ability to customize templates used for various sections of the report. Section templates also included LLM instructions in clear text to further promote transparency and trust in the system and allow users to adjust LLM instructions to better suit their needs.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/04/datadog-postmortem-llm-genai/en/resources/1datadog-postmortems-timeline-1744558917055.jpeg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/04/datadog-postmortem-llm-genai/en/resources/1datadog-postmortems-timeline-1744558917055.jpeg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003ePostmortem report section (Source: \u003ca href=\"https://www.datadoghq.com/blog/engineering/llms-for-postmortems/\"\u003eDatadog Engineering Blog\u003c/a\u003e)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eHaving worked on the LLM-powered functionality, the Datadog team concluded that although they believe LLMs can support operations engineers in creating postmortem reports, they can’t fully replace humans, at least at present. Still, GenAI-enhanced products can greatly improve productivity and give human engineers a head start when working on incident reports. The team has learned a great deal while working on this functionality and plans to expand on the data sources available to LLMs when generating postmortem contents, including internal wikis, RFCs, and system information. Additionally, the developers would like to test using LLMs to generate alternative postmortem versions, including custom and public postmortems.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Rafal-Gancarz\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRafal Gancarz\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-04-13T00:00:00Z",
  "modifiedTime": null
}
