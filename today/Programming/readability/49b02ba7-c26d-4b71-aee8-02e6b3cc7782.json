{
  "id": "49b02ba7-c26d-4b71-aee8-02e6b3cc7782",
  "title": "Incident Report: Spotify Outage on April 16, 2025",
  "link": "https://engineering.atspotify.com/2025/5/incident-report-spotify-outage-on-april-16-2025/",
  "description": "On April 16, Spotify experienced an outage that affected users worldwide. Here is what happened and what we... The post Incident Report: Spotify Outage on April 16, 2025 appeared first on Spotify Engineering.",
  "author": "Spotify Engineering",
  "published": "Fri, 09 May 2025 13:47:56 +0000",
  "source": "https://labs.spotify.com/feed/",
  "categories": [
    "Infrastructure"
  ],
  "byline": "Spotify Engineering",
  "length": 3589,
  "excerpt": "On April 16, Spotify experienced an outage that affected users worldwide. Here is what happened and what we are going to do about it.",
  "siteName": "Spotify Engineering",
  "favicon": "",
  "text": "On April 16, Spotify experienced an outage that affected users worldwide. Here is what happened and what we are going to do about it.ContextWe use Envoy Proxy for our networking perimeter systems. The perimeter is the first piece of our software that receives users (your!) network traffic. It then distributes that traffic to other services. We use cloud regions to distribute that traffic sensibly across the globe.To enhance Envoy's capabilities, we develop and integrate our own custom filters. A specific example is our filter for rate limiting, which we discussed in detail during our recent talk at EnvoyCon 2025.What happened?On April 16 2025, between 12:20 and 15:45 UTC, we experienced an outage, affecting the majority of our users worldwide. During the incident most traffic was disrupted, except to our Asia Pacific region due to timezone differences. The graph below shows the amount of successful requests on our perimeter, the purple line is the unaffected Asia Pacific region. Graph showing the amount of successful requests on our perimeter What caused this outage?On the day of the incident we changed the order of our Envoy filters. This change was deemed low risk and as such we applied it to all regions at the same time. Changing the order triggered a bug in one of our filters which in turn caused Envoy to crash. Unlike typical isolated crashes, this crash happened simultaneously on all Envoy instances.The immediate restart of all Envoy instances, combined with client side application retry logic, created an unprecedented load spike for the perimeter. The sudden surge in traffic then exposed a misconfiguration. Envoy instances were continuously cycled by Kubernetes as the Envoy max heap size was set higher than the allowed memory limit. As soon as any new Envoy instance started up it received a very large amount of traffic, which in turn caused it to use more than the allowed Kubernetes memory limit. Kubernetes then automatically shut down the instance and the cycle repeated.Lower traffic in our Asia Pacific region at the time of the incident, due to the difference in timezone and time of day, meant the regional Envoy memory usage never reached the kubernetes limit, which is why this region was unaffected.The outage was mitigated by increasing the total perimeter server capacity which in turn allowed the Envoy servers to drop under the Kubernetes memory limits. This in turn stopped the continuous cycling of servers.Timeline12:18 UTC - Envoy filter order changed and all Envoy instances crash12:20 UTC - Alarms are triggered indicating a significant drop of incoming traffic12:28 UTC - Situation escalated, no traffic worldwide except for the Asia Pacific region14:20 UTC - Traffic fully recovered in the European regions15:10 UTC - Traffic fully recovered in the US regions15:40 UTC - All traffic patterns back to normalWhere do we go from here?We recognize the impact such  outages can have, and we’re committed to learning from it. Here are the steps we’re taking to improve our systems and prevent similar issues in the future;We have fixed the bug causing Envoy to crashWe have fixed the configuration mismatch between Envoy heap size and Kubernetes memory limitsWe will improve how we roll out configuration changes to our perimeterWe will improve our monitoring capabilities to be able to catch these issues soonerAs we have in the past, we will continue to provide transparency on similar occasions so as to hold ourselves accountable and support ongoing improvements to our services.",
  "image": "https://images.ctfassets.net/p762jor363g1/attachment_477ce786984799390b61a5f1d272c86e/e3236f86875b9fdadf7ed9f0c5de0570/attachment_477ce786984799390b61a5f1d272c86e.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eOn April 16, Spotify experienced an outage that affected users worldwide. Here is what happened and what we are going to do about it.\u003c/p\u003e\u003ch2\u003eContext\u003c/h2\u003e\u003cp\u003eWe use \u003ca href=\"https://www.envoyproxy.io/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEnvoy Proxy\u003c/a\u003e for our networking perimeter systems. The perimeter is the first piece of our software that receives users (your!) network traffic. It then distributes that traffic to other services. We use cloud regions to distribute that traffic sensibly across the globe.\u003c/p\u003e\u003cp\u003eTo enhance Envoy\u0026#39;s capabilities, we develop and integrate our own custom filters. A specific example is our filter for \u003ca href=\"https://en.wikipedia.org/wiki/Rate_limiting\" target=\"_blank\" rel=\"noopener noreferrer\"\u003erate limiting\u003c/a\u003e, which we discussed in detail during our \u003ca href=\"https://www.youtube.com/watch?v=Bof1ZAk1Ca8\" target=\"_blank\" rel=\"noopener noreferrer\"\u003erecent talk at EnvoyCon 2025\u003c/a\u003e.\u003c/p\u003e\u003ch2\u003eWhat happened?\u003c/h2\u003e\u003cp\u003eOn April 16 2025, between 12:20 and 15:45 UTC, we experienced an outage, affecting the majority of our users worldwide. During the incident most traffic was disrupted, except to our Asia Pacific region due to timezone differences. The graph below shows the amount of successful requests on our perimeter, the purple line is the unaffected Asia Pacific region.\u003c/p\u003e\n      \u003cfigure\u003e\n        \n      \u003cimg src=\"https://images.ctfassets.net/p762jor363g1/attachment_5ef29b7880669d51adf755312fe1f5e3/148b740ce3d8a045886e1071efcab36c/attachment_5ef29b7880669d51adf755312fe1f5e3.jpg\" alt=\"attachment_5ef29b7880669d51adf755312fe1f5e3\"/\u003e\n      \n        \u003cfigcaption\u003e\u003cp\u003e\u003ci\u003eGraph showing the amount of successful requests on our perimeter\u003c/i\u003e\u003c/p\u003e\u003c/figcaption\u003e\n      \u003c/figure\u003e\n      \u003ch2\u003eWhat caused this outage?\u003c/h2\u003e\u003cp\u003eOn the day of the incident we changed the order of our \u003ca href=\"https://github.com/envoyproxy/envoy/blob/main/source/docs/async_http_filters.md\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEnvoy filters\u003c/a\u003e. This change was deemed low risk and as such we applied it to all regions at the same time. Changing the order triggered a bug in one of our filters which in turn caused Envoy to crash. Unlike typical isolated crashes, this crash happened simultaneously on all Envoy instances.\u003c/p\u003e\u003cp\u003eThe immediate restart of all Envoy instances, combined with client side application retry logic, created an unprecedented load spike for the perimeter. The sudden surge in traffic then exposed a misconfiguration. Envoy instances were continuously cycled by Kubernetes as the \u003ca href=\"https://www.envoyproxy.io/docs/envoy/latest/configuration/operations/overload_manager/overload_manager\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEnvoy max heap size\u003c/a\u003e was set higher than the allowed \u003ca href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits\" target=\"_blank\" rel=\"noopener noreferrer\"\u003ememory limit\u003c/a\u003e. As soon as any new Envoy instance started up it received a very large amount of traffic, which in turn caused it to use more than the allowed Kubernetes memory limit. Kubernetes then automatically shut down the instance and the cycle repeated.\u003c/p\u003e\u003cp\u003eLower traffic in our Asia Pacific region at the time of the incident, due to the difference in timezone and time of day, meant the regional Envoy memory usage never reached the kubernetes limit, which is why this region was unaffected.\u003c/p\u003e\u003cp\u003eThe outage was mitigated by increasing the total perimeter server capacity which in turn allowed the Envoy servers to drop under the Kubernetes memory limits. This in turn stopped the continuous cycling of servers.\u003c/p\u003e\u003ch2\u003eTimeline\u003c/h2\u003e\u003cp\u003e12:18 UTC - Envoy filter order changed and all Envoy instances crash\u003c/p\u003e\u003cp\u003e12:20 UTC - Alarms are triggered indicating a significant drop of incoming traffic\u003c/p\u003e\u003cp\u003e12:28 UTC - Situation escalated, no traffic worldwide except for the Asia Pacific region\u003c/p\u003e\u003cp\u003e14:20 UTC - Traffic fully recovered in the European regions\u003c/p\u003e\u003cp\u003e15:10 UTC - Traffic fully recovered in the US regions\u003c/p\u003e\u003cp\u003e15:40 UTC - All traffic patterns back to normal\u003c/p\u003e\u003ch2\u003eWhere do we go from here?\u003c/h2\u003e\u003cp\u003eWe recognize the impact such  outages can have, and we’re committed to learning from it. Here are the steps we’re taking to improve our systems and prevent similar issues in the future;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eWe have fixed the bug causing Envoy to crash\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWe have fixed the configuration mismatch between Envoy heap size and Kubernetes memory limits\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWe will improve how we roll out configuration changes to our perimeter\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWe will improve our monitoring capabilities to be able to catch these issues sooner\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAs we have in the past, we will continue to provide transparency on similar occasions so as to hold ourselves accountable and support ongoing improvements to our services.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": null,
  "modifiedTime": null
}
