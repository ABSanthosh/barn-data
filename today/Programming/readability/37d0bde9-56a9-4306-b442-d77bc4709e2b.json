{
  "id": "37d0bde9-56a9-4306-b442-d77bc4709e2b",
  "title": "Gemma for Streaming ML with Dataflow",
  "link": "https://developers.googleblog.com/en/gemma-for-streaming-ml-with-dataflow/",
  "description": "Use the Gemma language model to gauge customer sentiment, summarize conversations, and assist with crafting responses in near real-time with minimal latency.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Reza Rokni, Ravin Kumar",
  "length": 19898,
  "excerpt": "Use the Gemma language model to gauge customer sentiment, summarize conversations, and assist with crafting responses in near real-time with minimal latency.",
  "siteName": "",
  "favicon": "",
  "text": "Ravin Kumar Google Data Scientist Language Applications Gemma 2 is the latest version in Google's family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Large language models (LLMs) like Gemma are remarkably versatile, opening up many potential integrations for business processes. This blog explores how you can use Gemma to gauge the sentiment of a conversation, summarize that conversation's content, and assist with creating a reply for a difficult conversation that can then be approved by a person. One of the key requirements is that customers who have expressed a negative sentiment have their needs addressed in near real-time, which means that we will need to make use of a streaming data pipeline that leverages LLM's with minimal latency. You can also see the complete code for this pipeline and run the example in Google Colab. See Use Gemma to gauge sentiment and summarize conversations.GemmaGemma 2 offers unmatched performance at size. Gemma models have been shown to achieve exceptional benchmark results , even outperforming some larger models. The small size of the models enables architectures where the model is deployed or embedded directly onto the streaming data processing pipeline, allowing for the benefits, such as:Data locality with local worker calls rather than RPC of data to a separate systemA single system to autoscale, allowing the use of metrics such as back pressure at source to be used as direct signals to the autoscalerA single system to observe and monitor in productionDataflow provides a scalable, unified batch and streaming processing platform. With Dataflow, you can use the Apache Beam Python SDK to develop streaming data, event processing pipelines. Dataflow provides the following benefits:Dataflow is fully managed, autoscaling up and down based on demandApache Beam provides a set of low-code turnkey transforms that can save you time, effort, and cost on writing generic boilerplate code. After all the best code is the one you don't have to writeDataflow ML directly supports GPUs, installing the necessary drivers and providing access to a range of GPU devicesThe following example shows how to embed the Gemma model within the streaming data pipeline for running inference using Dataflow.ScenarioThis scenario revolves around a bustling food chain grappling with analyzing and storing a high volume of customer support requests through various chat channels. These interactions include both chats generated by automated chatbots and more nuanced conversations that require the attention of live support staff. In response to this challenge, we've set ambitious goals:First, we want to efficiently manage and store chat data by summarizing positive interactions for easy reference and future analysis.Second, we want to implement real-time issue detection and resolution, using sentiment analysis to swiftly identify dissatisfied customers and generate tailored responses to address their concerns.The solution uses a pipeline that processes completed chat messages in near real time. Gemma is used in the first instance to carry out analysis work monitoring the sentiment of these chats. All chats are then summarized, with positive or neutral sentiment chats sent directly to a data platform, BigQuery, by using the out-of-the-box I/Os with Dataflow. For chats that report a negative sentiment, we use Gemma to ask the model to craft a contextually appropriate response for the dissatisfied customer. This response is then sent to a human for review, allowing support staff to refine the message before it reaches a potentially dissatisfied customer.With this use case, we explore some interesting aspects of using an LLM within a pipeline. For example, there are challenges with having to process the responses in code, given the non-deterministic responses that can be accepted. For example, we ask our LLM to respond in JSON, which it is not guaranteed to do. This request requires us to parse and validate the response, which is a similar process to how you would normally process data from sources that may not have correctly structured data.With this solution, customers can experience faster response times and receive personalized attention when issues arise. The automation of positive chat summarization frees up time for support staff, allowing them to focus on more complex interactions. Additionally, the in-depth analysis of chat data can drive data-driven decision-making while the system's scalability lets it effortlessly adapt to increasing chat volumes without compromising response quality.The Data processing pipelineThe pipeline flow can be seen below: The high-level pipeline can be described with a few lines:Read the review data from Pub/Sub, our event messaging source. This data contains the chat ID and the chat history as a JSON payload. This payload is processed in the pipeline.2. The pipeline passes the text from this message to Gemma with a prompt. The pipeline requests that two tasks be completed.Attach a sentiment score to the message, using the following three values: 1 for a positive chat, 0 for a neutral chat, and -1 for a negative chat.Summarize the chat with a single sentence.3. Next, the pipeline branches, depending on the sentiment score:If the score is 1 or 0, the chat with summarization is sent onwards to our data analytics system for storage and future analysis uses.If the score is -1, we ask Gemma to provide a response. This response, combined with the chat information, is then sent to an event messaging system that acts as the glue between the pipeline and other applications. This step allows a person to review the content.The pipeline codeSetupAccess and download GemmaIn our example, we use Gemma through the KerasNLP, and we use Kaggle's 'Instruction tuned' gemma2_keras_gemma2_instruct_2b_en variant. You must download the model and store it in a location that the pipeline can access.Use the Dataflow serviceAlthough it's possible to use CPUs for testing and development, given the inference times, for a production system we need to use GPUs on the Dataflow ML service. The use of GPUs with Dataflow is facilitated by a custom container. Details for this setup are available at Dataflow GPU support. We recommend that you follow the local development guide for development, which allows for rapid testing of the pipeline. You can also reference the guide for using Gemma on Dataflow, which includes links to an example Docker file.Gemma custom model handlerThe RunInference transform in Apache Beam is at the heart of this solution, making use of a model handler for configuration and abstracting the user from the boilerplate code needed for productionization. Most model types can be supported with configuration only using Beam's built in model handlers, but for Gemma, this blog makes use of a custom model handler, which gives us full control of our interactions with the model while still using all the machinery that RunInference provides for processing. The pipeline custom_model_gemma.py has an example GemmModelHandler that you can use. Please note the use of the max_length value used in the model.generate() call from that GemmModelHandler. This value controls the maximum length of Gemma's response to queries and will need to be changed to match the needs of the use case, for this blog we used the value 512.Tip: For this blog, we found that using the jax keras backend performed significantly better. To enable this, the DockerFile must contain the instruction ENV KERAS_BACKEND=\"jax\". This must be set in your container before the worker starts up Beam (which imports Keras)Build the pipelineThe first step in the pipeline is standard for event processing systems: we need to read the JSON messages that our upstream systems have created, which package chat messages into a simple structure that includes the chat ID. chats = ( pipeline | \"Read Topic\" \u003e\u003e beam.io.ReadFromPubSub(subscription=args.messages_subscription) | \"Decode\" \u003e\u003e beam.Map(lambda x: x.decode(\"utf-8\") ) The following example shows one of these JSON messages, as well as a very important discussion about pineapple and pizza, with ID 221 being our customer. { \"id\": 1, \"user_id\": 221, \"chat_message\": \"\\\\nid 221: Hay I am really annoyed that your menu includes a pizza with pineapple on it! \\\\nid 331: Sorry to hear that , but pineapple is nice on pizza\\\\nid 221: What a terrible thing to say! Its never ok, so unhappy right now! \\\\n\" } We now have a PCollection of python chat objects. In the next step, we extract the needed values from these chat messages and incorporate them into a prompt to pass to our instruction tuned LLM. To do this step, we create a prompt template that provides instructions for the model. prompt_template = \"\"\" \u003cprompt\u003e Provide the results of doing these two tasks on the chat history provided below for the user {} task 1 : assess if the tone is happy = 1 , neutral = 0 or angry = -1 task 2 : summarize the text with a maximum of 512 characters Output the results as a json with fields [sentiment, summary] @@@{}@@@ \u003canswer\u003e \"\"\" The following is a example of a prompt being sent to the model: \u003cprompt\u003e Provide the results of doing these two tasks on the chat history provided below for the user 221 task 1 : assess if the tone is happy = 1 , neutral = 0 or angry = -1 task 2 : summarize the text with a maximum of 512 characters Output the results as a json with fields [sentiment, summary] @@@\"\\\\nid 221: Hay I am really annoyed that your menu includes a pizza with pineapple on it! \\\\nid 331: Sorry to hear that , but pineapple is nice on pizza\\\\nid 221: What a terrible thing to say! Its never ok, so unhappy right now! \\\\n\"@@@ \u003canswer\u003e Some notes about the prompt:This prompt is intended as an illustrative example. For your own prompts, run complete analysis with indicative data for your application.For prototyping you can use aistudio.google.com to test Gemma and Gemini behavior quickly. There also is a one click API key if you’d like to test programmatically.2. With smaller, less powerful models, you might get better responses by simplifying the instructions to a single task and making multiple calls against the model.3. We limited chat message summaries to a maximum of 512 characters. Match this value with the value that is provided in the max_length config to the Gemma generate call.4. The three ampersands, '@@@' are used as a trick to allow us to extract the original chats from the message after processing. Other ways we can do this task include:Use the whole chat message as a key in the key-value pair.Join the results back to the original data. This approach requires a shuffle.5. As we need to process the response in code, we ask the LLM to create a JSON representation of its answer with two fields: sentiment and summary.To create the prompt, we need to parse the information from our source JSON message and then insert it into the template. We encapsulate this process in a Beam DoFN and use it in our pipeline. In our yield statement, we construct a key-value structure, with the chat ID being the key. This structure allows us to match the chat to the inference when we call the model. # Create the prompt using the information from the chat class CreatePrompt(beam.DoFn): def process(self, element, *args, **kwargs): user_chat = json.loads(element) chat_id = user_chat['id'] user_id = user_chat['user_id'] messages = user_chat['chat_message'] yield (chat_id, prompt_template.format(user_id, messages)) prompts = chats | \"Create Prompt\" \u003e\u003e beam.ParDo(CreatePrompt()) We are now ready to call our model. Thanks to the RunInference machinery, this step is straightforward. We wrap the GemmaModelHandler within a KeyedModelhandler, which tells RunInference to accept the incoming data as a key-value pair tuple. During development and testing, the model is stored in the gemma2 directory. When running the model on the Dataflow ML service, the model is stored in Google Cloud Storage, with the URI format gs://\u003cyour_bucket\u003e/gemma-directory. keyed_model_handler = KeyedModelHandler(GemmaModelHandler('gemma2')) results = prompts | \"RunInference-Gemma\" \u003e\u003e RunInference(keyed_model_handler) The results collection now contains results from the LLM call. Here things get a little interesting: although the LLM call is code, unlike calling just another function, the results are not deterministic! This includes that final bit of our prompt request \"Output the results as a JSON with fields [sentiment, summary]\". In general, the response matches that shape, but it’s not guaranteed. We need to be a little defensive here and validate our input. If it fails the validation, we output the results to an error collection. In this sample, we leave those values there. For a production pipeline, you might want to let the LLM try a second time and run the error collection results in RunInference again and then flatten the response with the results collection. Because Beam pipelines are Directed Acyclic Graphs, we can’t create a loop here.We now take the results collection and process the LLM output. To process the results of RunInference, we create a new DoFn SentimentAnalysis and function extract_model_reply This step returns an object of type PredictionResult: def extract_model_reply(model_inference): match = re.search(r\"(\\{[\\s\\S]*?\\})\", model_inference) json_str = match.group(1) result = json.loads(json_str) if all(key in result for key in ['sentiment', 'summary']): return result raise Exception('Malformed model reply') class SentimentAnalysis(beam.DoFn): def process(self, element): key = element[0] match = re.search(r\"@@@([\\s\\S]*?)@@@\", element[1].example) chats = match.group(1) try: # The result will contain the prompt, replace the prompt with \"\" result = extract_model_reply(element[1].inference.replace(element[1].example, \"\")) processed_result = (key, chats, result['sentiment'], result['summary']) if (result['sentiment'] \u003c0): output = beam.TaggedOutput('negative', processed_result) else: output = beam.TaggedOutput('main', processed_result) except Exception as err: print(\"ERROR!\" + str(err)) output = beam.TaggedOutput('error', element) yield output It's worth spending a few minutes on the need for extract_model_reply(). Because the model is self-hosted, we cannot guarantee that the text will be a JSON output. To ensure that we get a JSON output, we need to run a couple of checks. One benefit of using the Gemini API is that it includes a feature that ensures the output is always JSON, known as constrained decoding.Let’s now use these functions in our pipeline: filtered_results = (results | \"Process Results\" \u003e\u003e beam.ParDo(SentimentAnalysis()).with_outputs('main','negative','error')) Using with_outputs creates multiple accessible collections in filtered_results. The main collection has sentiments and summaries for positive and neutral reviews, while error contains any unparsable responses from the LLM. You can send these collections to other sources, such as BigQuery, with a write transform. This example doesn’t demonstrate this step, however, the negative collection is something that we want to do more within this pipeline.Negative sentiment processingMaking sure customers are happy is critical for retention. While we have used a light-hearted example with our pineapple on pizza debate, the direct interactions with a customer should always strive for empathy and positive responses from all parts of an organization. At this stage, we pass on this chat to one of the trained support representatives, but we can still see if the LLM is able to assist that support person in reducing the time to resolution.For this step, we make a call to the model and ask it to formulate a response. We again use the Gemma 2B model for this call in the code. generated_responses = (results.negative | \"Generate Response\" \u003e\u003e beam.Map(lambda x: ((x[0], x[3]), \"\u003cprompt\u003eGenerate an apology response for the user in this chat text: \" + x[1] + \"\u003canswer\u003e\")) | \"Gemma-Response\" \u003e\u003e RunInference(keyed_model_handler) In general, you wrap the prompt creation code in a DoFn, but it is also possible to use a simple lambda in the pipeline code itself. Here we generate a prompt that contains the original chat message, which was extracted in the SentimentAnalysis function.For local running and testing we can make use of some simple print statements to see the outputs on the various PCollections: generated_responses | \"Print Response\" \u003e\u003e beam.Map(print) filtered_results.main | \"Print Main\" \u003e\u003e beam.Map(print) filtered_results.error | \"Print Errors\" \u003e\u003e beam.Map(print) Of course for the real usage, these outputs will be sent to various sinks such as Pub/Sub and BigQuery.Run the pipelineLets see how the model does with the previous JSON message:Step 1: Sentiment analysis and summarization\"sentiment\": -1,\"summary\": \"User 221 is very unhappy about the presence of pineapple on pizza.\"The responses that the 2B model generated aren’t bad. The sentiment is correct, and because the results of the summary are more subjective, the correctness of the response depends on the downstream uses of this information.Step 2: Generated response\"I understand that you're upset about the pineapple pizza. It's a very personal preference, and I apologize that it might have caused you some frustration. We strive to offer a diverse menu to cater to a wide range of tastes, and we're always open to feedback. Would you like to share your thoughts on the pizza with pineapple?\"Are these responses acceptable? At this stage, we intend to send the whole data package to a support representative for analysis and if they are happy with it they can send it as is, or they can make some edits and adjustments.Next StepsMaybe at this stage we want to make use of a model with more parameters, such as Gemma2 9B or 27B. We could also use a model that is large enough that it requires an API call to an external service call, such as Gemini, instead of being loaded onto a worker. After all, we reduced the work needed to send to these larger models by using the smaller model as a filter. Making these choices is not just a technical decision, but also a business decision. The costs and benefits need to be measured. We can again make use of Dataflow to more easily set up A/B testing.You also may choose to finetune a model custom to your use case. This is one way of changing the “voice” of the model to suit your needs.A/B TestingIn our generate step, we passed all incoming negative chats to our 2B model. If we wanted to send a portion of the collection to another model, we can use the Partition function in Beam with the filtered_responses.negative collection. By directing some customer messages to different models and having support staff rate the generated responses before sending them, we can collect valuable feedback on response quality and improvement margins.SummaryWith those few lines of code, we built a system capable of processing customer sentiment data at high velocity and variability. By using the Gemma 2 open model, with its 'unmatched performance for its size', we were able to incorporate this powerful LLM within a stream processing use case that helps create a better experience for customers.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Untitled_1600_x_873_px.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \u003csection\u003e\n      \n        \n          \n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=Ravin+Kumar\"\u003eRavin Kumar\u003c/a\u003e\n            \n              \u003cspan\u003eGoogle Data Scientist\u003c/span\u003e\n            \n            \n              \u003cspan\u003eLanguage Applications\u003c/span\u003e\n            \n          \u003c/p\u003e\n        \n\n      \n      \u003c/section\u003e\n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"fq0h6\"\u003e\u003ca href=\"https://blog.google/technology/developers/google-gemma-2/\"\u003eGemma 2\u003c/a\u003e is the latest version in Google\u0026#39;s family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Large language models (LLMs) like Gemma are remarkably versatile, opening up many potential integrations for business processes. This blog explores how you can use Gemma to gauge the \u003cb\u003e\u003ci\u003esentiment\u003c/i\u003e\u003c/b\u003e of a conversation, \u003cb\u003e\u003ci\u003esummarize\u003c/i\u003e\u003c/b\u003e that conversation\u0026#39;s content, and assist with creating a reply for a difficult conversation that can then be approved by a person. One of the key requirements is that customers who have expressed a negative sentiment have their needs addressed in near real-time, which means that we will need to make use of a streaming data pipeline that leverages LLM\u0026#39;s with minimal latency. \u003c/p\u003e\u003cp data-block-key=\"92rav\"\u003eYou can also see the complete code for this pipeline and run the example in Google Colab. See \u003ca href=\"https://colab.sandbox.google.com/github/apache/beam/blob/master/examples/notebooks/beam-ml/gemma_2_sentiment_and_summarization.ipynb\"\u003eUse Gemma to gauge sentiment and summarize conversations\u003c/a\u003e.\u003c/p\u003e\u003ch2 data-block-key=\"bi37j\"\u003e\u003cb\u003e\u003cbr/\u003eGemma\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"652gn\"\u003eGemma 2 offers \u003ca href=\"https://ai.google.dev/gemma\"\u003eunmatched performance at size.\u003c/a\u003e Gemma models have been shown to achieve exceptional benchmark results , even outperforming some larger models. The small size of the models enables architectures where the model is deployed or embedded directly onto the streaming data processing pipeline, allowing for the benefits, such as:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"5kh72\"\u003eData locality with local worker calls rather than RPC of data to a separate system\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"a31ff\"\u003eA single system to autoscale, allowing the use of metrics such as back pressure at source to be used as direct signals to the autoscaler\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ctu2o\"\u003eA single system to observe and monitor in production\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"6domg\"\u003e\u003ca href=\"https://cloud.google.com/dataflow\"\u003eDataflow\u003c/a\u003e provides a scalable, unified batch and streaming processing platform. With Dataflow, you can use the Apache Beam Python SDK to develop streaming data, event processing pipelines. Dataflow provides the following benefits:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"512h5\"\u003eDataflow is fully managed, autoscaling up and down based on demand\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"eo1rj\"\u003eApache Beam provides a set of low-code turnkey transforms that can save you time, effort, and cost on writing generic boilerplate code. After all the best code is the one you don\u0026#39;t have to write\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"e32qc\"\u003eDataflow ML directly supports GPUs, installing the necessary drivers and providing access to a range of \u003ca href=\"https://cloud.google.com/dataflow/docs/gpu/gpu-support#availability\"\u003eGPU devices\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"d3e1l\"\u003eThe following example shows how to embed the Gemma model within the streaming data pipeline for running inference using Dataflow.\u003c/p\u003e\u003ch2 data-block-key=\"eim89\"\u003e\u003cb\u003e\u003cbr/\u003eScenario\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"drhd\"\u003eThis scenario revolves around a bustling food chain grappling with analyzing and storing a high volume of customer support requests through various chat channels. These interactions include both chats generated by automated chatbots and more nuanced conversations that require the attention of live support staff. In response to this challenge, we\u0026#39;ve set ambitious goals:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"7evru\"\u003eFirst, we want to efficiently manage and store chat data by summarizing positive interactions for easy reference and future analysis.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"5jju2\"\u003eSecond, we want to implement real-time issue detection and resolution, using sentiment analysis to swiftly identify dissatisfied customers and generate tailored responses to address their concerns.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"8nmrh\"\u003eThe solution uses a pipeline that processes completed chat messages in near real time. Gemma is used in the first instance to carry out analysis work monitoring the \u003cb\u003e\u003ci\u003esentiment\u003c/i\u003e\u003c/b\u003e of these chats. All chats are then \u003cb\u003e\u003ci\u003esummarized\u003c/i\u003e\u003c/b\u003e, with positive or neutral sentiment chats sent directly to a data platform, \u003ca href=\"https://cloud.google.com/bigquery\"\u003eBigQuery\u003c/a\u003e, by using the out-of-the-box I/Os with Dataflow. For chats that report a negative sentiment, we use Gemma to ask the model to craft a contextually appropriate response for the dissatisfied customer. This response is then sent to a human for review, allowing support staff to refine the message before it reaches a potentially dissatisfied customer.\u003c/p\u003e\u003cp data-block-key=\"503ib\"\u003eWith this use case, we explore some interesting aspects of using an LLM within a pipeline. For example, there are challenges with having to process the responses in code, given the non-deterministic responses that can be accepted. For example, we ask our LLM to respond in JSON, which it is not guaranteed to do. This request requires us to parse and validate the response, which is a similar process to how you would normally process data from sources that may not have correctly structured data.\u003c/p\u003e\u003cp data-block-key=\"18jbp\"\u003eWith this solution, customers can experience faster response times and receive personalized attention when issues arise. The automation of positive chat summarization frees up time for support staff, allowing them to focus on more complex interactions. Additionally, the in-depth analysis of chat data can drive data-driven decision-making while the system\u0026#39;s scalability lets it effortlessly adapt to increasing chat volumes without compromising response quality.\u003c/p\u003e\u003ch2 data-block-key=\"9grnp\"\u003e\u003cb\u003e\u003cbr/\u003eThe Data processing pipeline\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"a558m\"\u003eThe pipeline flow can be seen below:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma_V2.0_Blog.original.png\" alt=\"Data processing pipeline architecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"fq0h6\"\u003eThe high-level pipeline can be described with a few lines:\u003c/p\u003e\u003col\u003e\u003cli data-block-key=\"2kfn0\"\u003eRead the review data from \u003ca href=\"https://cloud.google.com/pubsub\"\u003ePub/Sub\u003c/a\u003e, our event messaging source. This data contains the chat ID and the chat history as a JSON payload. This payload is processed in the pipeline.\u003c/li\u003e\u003c/ol\u003e\u003cp data-block-key=\"c0fr1\"\u003e2. The pipeline passes the text from this message to Gemma with a prompt. The pipeline requests that two tasks be completed.\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"59in7\"\u003eAttach a sentiment score to the message, using the following three values: 1 for a positive chat, 0 for a neutral chat, and -1 for a negative chat.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"2iqdu\"\u003eSummarize the chat with a single sentence.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"1802i\"\u003e3. Next, the pipeline branches, depending on the sentiment score:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"376hc\"\u003eIf the score is 1 or 0, the chat with summarization is sent onwards to our data analytics system for storage and future analysis uses.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"amf05\"\u003eIf the score is -1, we ask Gemma to provide a response. This response, combined with the chat information, is then sent to an event messaging system that acts as the glue between the pipeline and other applications. This step allows a person to review the content.\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"6o224\"\u003e\u003cbr/\u003e\u003cb\u003eThe pipeline code\u003c/b\u003e\u003c/h2\u003e\u003ch3 data-block-key=\"2lli2\"\u003e\u003cb\u003eSetup\u003c/b\u003e\u003c/h3\u003e\u003ch4 data-block-key=\"513pv\"\u003eAccess and download Gemma\u003c/h4\u003e\u003cp data-block-key=\"f4k25\"\u003eIn our example, we use Gemma through the \u003ca href=\"https://keras.io/keras_nlp/\"\u003eKerasNLP\u003c/a\u003e, and we use \u003ca href=\"https://www.kaggle.com/models/google/gemma\"\u003eKaggle\u0026#39;s\u003c/a\u003e \u0026#39;Instruction tuned\u0026#39; \u003ca href=\"https://www.kaggle.com/models/google/gemma-2/keras\"\u003egemma2_keras_gemma2_instruct_2b_en variant.\u003c/a\u003e You must download the model and store it in a location that the pipeline can access.\u003c/p\u003e\u003ch4 data-block-key=\"34tdc\"\u003e\u003cbr/\u003eUse the Dataflow service\u003c/h4\u003e\u003cp data-block-key=\"399nk\"\u003eAlthough it\u0026#39;s possible to use CPUs for testing and development, given the inference times, for a production system we need to use GPUs on the Dataflow ML service. The use of GPUs with Dataflow is facilitated by a custom container. Details for this setup are available at \u003ca href=\"https://cloud.google.com/dataflow/docs/gpu/gpu-support\"\u003eDataflow GPU support\u003c/a\u003e. We recommend that you follow the \u003ca href=\"https://cloud.google.com/dataflow/docs/gpu/develop-with-gpus\"\u003elocal development\u003c/a\u003e guide for development, which allows for rapid testing of the pipeline. You can also reference the \u003ca href=\"https://cloud.google.com/dataflow/docs/machine-learning/gemma\"\u003eguide for using Gemma on Dataflow\u003c/a\u003e, which includes links to an example Docker file.\u003c/p\u003e\u003ch4 data-block-key=\"669nq\"\u003e\u003cbr/\u003eGemma custom model handler\u003c/h4\u003e\u003cp data-block-key=\"55l8f\"\u003eThe RunInference transform in Apache Beam is at the heart of this solution, making use of a model handler for configuration and abstracting the user from the boilerplate code needed for productionization. Most model types can be supported with configuration only using Beam\u0026#39;s built in model handlers, but for Gemma, this blog makes use of a custom model handler, which gives us full control of our interactions with the model while still using all the machinery that RunInference provides for processing. The pipeline \u003ca href=\"https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/dataflow/gemma/custom_model_gemma.py\"\u003ecustom_model_gemma.py\u003c/a\u003e has an example \u003ccode\u003eGemmModelHandler\u003c/code\u003e that you can use. Please note the use of the \u003ci\u003emax_length\u003c/i\u003e value used in the \u003ci\u003emodel.generate()\u003c/i\u003e call from that \u003ccode\u003eGemmModelHandler\u003c/code\u003e. This value controls the maximum length of Gemma\u0026#39;s response to queries and will need to be changed to match the needs of the use case, for this blog we used the value 512.\u003c/p\u003e\u003cp data-block-key=\"6rbfl\"\u003eTip: For this blog, we found that using the jax keras backend performed significantly better. To enable this, the DockerFile must contain the instruction \u003ccode\u003e\u003ci\u003eENV KERAS_BACKEND=\u0026#34;jax\u0026#34;\u003c/i\u003e\u003c/code\u003e. This must be set in your container before the worker starts up Beam (which imports Keras)\u003c/p\u003e\u003ch3 data-block-key=\"flq8d\"\u003e\u003cbr/\u003e\u003cb\u003eBuild the pipeline\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"6vqka\"\u003eThe first step in the pipeline is standard for event processing systems: we need to read the JSON messages that our upstream systems have created, which package chat messages into a simple structure that includes the chat ID.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003echats\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e(\u003c/span\u003e \u003cspan\u003epipeline\u003c/span\u003e \u003cspan\u003e|\u003c/span\u003e \u003cspan\u003e\u0026#34;Read Topic\u0026#34;\u003c/span\u003e \u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e\n                        \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eio\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eReadFromPubSub\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003esubscription\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eargs\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emessages_subscription\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003e|\u003c/span\u003e \u003cspan\u003e\u0026#34;Decode\u0026#34;\u003c/span\u003e \u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eMap\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elambda\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edecode\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;utf-8\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n   \u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cp data-block-key=\"tx0pf\"\u003eThe following example shows one of these JSON messages, as well as a very important discussion about pineapple and pizza, with ID 221 being our customer.\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\n\u003cspan\u003e\u0026#34;id\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\n\u003cspan\u003e\u0026#34;user_id\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e221\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\n\u003cspan\u003e\u0026#34;chat_message\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;\\\\nid 221: Hay I am really annoyed that your menu includes a pizza with pineapple on it! \\\\nid 331: Sorry to hear that , but pineapple is nice on pizza\\\\nid 221: What a terrible thing to say! Its never ok, so unhappy right now! \\\\n\u0026#34;\u003c/span\u003e\n\u003cspan\u003e}\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cp data-block-key=\"tx0pf\"\u003eWe now have a PCollection of python chat objects. In the next step, we extract the needed values from these chat messages and incorporate them into a prompt to pass to our instruction tuned LLM. To do this step, we create a prompt template that provides instructions for the model.\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003eprompt_template = \u0026#34;\u0026#34;\u0026#34;\n\u0026lt;prompt\u0026gt;\nProvide the results of doing these two tasks on the chat history provided below for the user {}\ntask 1 : assess if the tone is happy = 1 , neutral = 0 or angry = -1\ntask 2 : summarize the text with a maximum of 512 characters\nOutput the results as a json with fields [sentiment, summary]\n\n@@@{}@@@\n\u0026lt;answer\u0026gt;\n\u0026#34;\u0026#34;\u0026#34;\n\u003c/pre\u003e\u003c/div\u003e  \u003cp data-block-key=\"tx0pf\"\u003eThe following is a example of a prompt being sent to the model:\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eprompt\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\n\u003cspan\u003eProvide\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ethe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eresults\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eof\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003edoing\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ethese\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003etwo\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003etasks\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eon\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ethe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003echat\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ehistory\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eprovided\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ebelow\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efor\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ethe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003euser\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e221\u003c/span\u003e\n\u003cspan\u003etask\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eassess\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eif\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ethe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003etone\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eis\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ehappy\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eneutral\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eor\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eangry\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e-1\u003c/span\u003e\n\u003cspan\u003etask\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003esummarize\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ethe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003etext\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ewith\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ea\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003emaximum\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eof\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e512\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003echaracters\u003c/span\u003e\n\u003cspan\u003eOutput\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ethe\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eresults\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eas\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ea\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ejson\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ewith\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003efields\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003esentiment\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003esummary\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\n\n\u003cspan\u003e@@\u003c/span\u003e\u003cspan\u003e@\u0026#34;\u003c/span\u003e\u003cspan\u003e\\\\\u003c/span\u003e\u003cspan\u003enid 221: Hay I am really annoyed that your menu includes a pizza with pineapple on it! \u003c/span\u003e\u003cspan\u003e\\\\\u003c/span\u003e\u003cspan\u003enid 331: Sorry to hear that , but pineapple is nice on pizza\u003c/span\u003e\u003cspan\u003e\\\\\u003c/span\u003e\u003cspan\u003enid 221: What a terrible thing to say! Its never ok, so unhappy right now! \u003c/span\u003e\u003cspan\u003e\\\\\u003c/span\u003e\u003cspan\u003en\u0026#34;\u003c/span\u003e\u003cspan\u003e@@@\u003c/span\u003e\n\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eanswer\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"tx0pf\"\u003eSome notes about the prompt:\u003c/p\u003e\u003col\u003e\u003cli data-block-key=\"9hvki\"\u003eThis prompt is intended as an illustrative example. For your own prompts, run complete analysis with indicative data for your application.\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli data-block-key=\"9ahd5\"\u003eFor prototyping you can use \u003ca href=\"http://aistudio.google.com/\"\u003eaistudio.google.com\u003c/a\u003e to test Gemma and Gemini behavior quickly. There also is a one click API key if you’d like to test programmatically.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"e1evs\"\u003e2. With smaller, less powerful models, you might get better responses by simplifying the instructions to a single task and making multiple calls against the model.\u003c/p\u003e\u003cp data-block-key=\"ast5b\"\u003e3. We limited chat message summaries to a maximum of 512 characters. Match this value with the value that is provided in the max_length config to the Gemma generate call.\u003c/p\u003e\u003cp data-block-key=\"47i2\"\u003e4. The three ampersands, \u0026#39;@@@\u0026#39; are used as a trick to allow us to extract the original chats from the message after processing. Other ways we can do this task include:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"4b6t4\"\u003eUse the whole chat message as a key in the key-value pair.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"drkql\"\u003eJoin the results back to the original data. This approach requires a shuffle.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"58o6f\"\u003e5. As we need to process the response in code, we ask the LLM to create a JSON representation of its answer with two fields: sentiment and summary.\u003c/p\u003e\u003cp data-block-key=\"5p7tv\"\u003eTo create the prompt, we need to parse the information from our source JSON message and then insert it into the template. We encapsulate this process in a Beam DoFN and use it in our pipeline. In our yield statement, we construct a key-value structure, with the chat ID being the key. This structure allows us to match the chat to the inference when we call the model.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e# Create the prompt using the information from the chat\u003c/span\u003e\n\u003cspan\u003eclass\u003c/span\u003e \u003cspan\u003eCreatePrompt\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eDoFn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e\n  \u003cspan\u003edef\u003c/span\u003e \u003cspan\u003eprocess\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eelement\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e*\u003c/span\u003e\u003cspan\u003eargs\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e**\u003c/span\u003e\u003cspan\u003ekwargs\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e\n    \u003cspan\u003euser_chat\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ejson\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eloads\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eelement\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003echat_id\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003euser_chat\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;id\u0026#39;\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\n    \u003cspan\u003euser_id\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003euser_chat\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;user_id\u0026#39;\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\n    \u003cspan\u003emessages\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003euser_chat\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;chat_message\u0026#39;\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\n    \u003cspan\u003eyield\u003c/span\u003e \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003echat_id\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eprompt_template\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eformat\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003euser_id\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003emessages\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e\n\n\u003cspan\u003eprompts\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003echats\u003c/span\u003e \u003cspan\u003e|\u003c/span\u003e  \u003cspan\u003e\u0026#34;Create Prompt\u0026#34;\u003c/span\u003e \u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eParDo\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eCreatePrompt\u003c/span\u003e\u003cspan\u003e())\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cp data-block-key=\"tx0pf\"\u003eWe are now ready to call our model. Thanks to the RunInference machinery, this step is straightforward. We wrap the \u003ccode\u003eGemmaModelHandler\u003c/code\u003e within a \u003ccode\u003eKeyedModelhandler\u003c/code\u003e, which tells RunInference to accept the incoming data as a key-value pair tuple. During development and testing, the model is stored in the \u003ccode\u003egemma2\u003c/code\u003e directory. When running the model on the Dataflow ML service, the model is stored in Google Cloud Storage, with the URI format \u003ccode\u003egs://\u0026lt;your_bucket\u0026gt;/gemma-directory\u003c/code\u003e.\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003ekeyed_model_handler\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003eKeyedModelHandler\u003cspan\u003e(\u003c/span\u003eGemmaModelHandler\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#39;gemma2\u0026#39;\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e\n\u003cspan\u003eresults\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e  \u003c/span\u003eprompts\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e|\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;RunInference-Gemma\u0026#34;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u0026gt;\u0026gt;\u003cspan\u003e \u003c/span\u003eRunInference\u003cspan\u003e(\u003c/span\u003ekeyed_model_handler\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"tx0pf\"\u003eThe results collection now contains results from the LLM call. Here things get a little interesting: although the LLM call is code, unlike calling just another function, the results are not deterministic! This includes that final bit of our prompt request \u003ci\u003e\u0026#34;\u003c/i\u003e\u003cb\u003e\u003ci\u003eOutput the results as a JSON with fields [sentiment, summary]\u003c/i\u003e\u003c/b\u003e\u003ci\u003e\u0026#34;.\u003c/i\u003e In general, the response matches that shape, but it’s not guaranteed. We need to be a little defensive here and validate our input. If it fails the validation, we output the results to an error collection. In this sample, we leave those values there. For a production pipeline, you might want to let the LLM try a second time and run the error collection results in RunInference again and then flatten the response with the results collection. Because Beam pipelines are Directed Acyclic Graphs, we can’t create a loop here.\u003c/p\u003e\u003cp data-block-key=\"2dlmb\"\u003eWe now take the results collection and process the LLM output. To process the results of RunInference, we create a new DoFn \u003ccode\u003eSentimentAnalysis\u003c/code\u003e and function \u003ccode\u003eextract_model_reply\u003c/code\u003e This step returns an object of type \u003ca href=\"https://beam.apache.org/releases/pydoc/2.56.0/_modules/apache_beam/ml/inference/base.html#PredictionResult\"\u003ePredictionResult\u003c/a\u003e:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003edef\u003c/span\u003e \u003cspan\u003eextract_model_reply\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel_inference\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e\n    \u003cspan\u003ematch\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ere\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003esearch\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003er\u003c/span\u003e\u003cspan\u003e\u0026#34;(\\{[\\s\\S]*?\\})\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003emodel_inference\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003ejson_str\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ematch\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egroup\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003eresult\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ejson\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eloads\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ejson_str\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003eif\u003c/span\u003e \u003cspan\u003eall\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ekey\u003c/span\u003e \u003cspan\u003ein\u003c/span\u003e \u003cspan\u003eresult\u003c/span\u003e \u003cspan\u003efor\u003c/span\u003e \u003cspan\u003ekey\u003c/span\u003e \u003cspan\u003ein\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;sentiment\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#39;summary\u0026#39;\u003c/span\u003e\u003cspan\u003e]):\u003c/span\u003e\n        \u003cspan\u003ereturn\u003c/span\u003e \u003cspan\u003eresult\u003c/span\u003e\n    \u003cspan\u003eraise\u003c/span\u003e \u003cspan\u003eException\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#39;Malformed model reply\u0026#39;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eclass\u003c/span\u003e \u003cspan\u003eSentimentAnalysis\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eDoFn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e\n    \u003cspan\u003edef\u003c/span\u003e \u003cspan\u003eprocess\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eelement\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e\n        \u003cspan\u003ekey\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eelement\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e                          \n        \u003cspan\u003ematch\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ere\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003esearch\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003er\u003c/span\u003e\u003cspan\u003e\u0026#34;@@@([\\s\\S]*?)@@@\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eelement\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eexample\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003echats\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ematch\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egroup\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \n        \u003cspan\u003etry\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n            \u003cspan\u003e# The result will contain the prompt, replace the prompt with \u0026#34;\u0026#34;\u003c/span\u003e\n            \u003cspan\u003eresult\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eextract_model_reply\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eelement\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003einference\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ereplace\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eelement\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eexample\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#34;\u0026#34;\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e\n            \u003cspan\u003eprocessed_result\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ekey\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003echats\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eresult\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;sentiment\u0026#39;\u003c/span\u003e\u003cspan\u003e],\u003c/span\u003e \u003cspan\u003eresult\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;summary\u0026#39;\u003c/span\u003e\u003cspan\u003e])\u003c/span\u003e           \n            \n            \u003cspan\u003eif\u003c/span\u003e \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eresult\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#39;sentiment\u0026#39;\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e \u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e\n              \u003cspan\u003eoutput\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eTaggedOutput\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#39;negative\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eprocessed_result\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003eelse\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n              \u003cspan\u003eoutput\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eTaggedOutput\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#39;main\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eprocessed_result\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\n        \u003cspan\u003eexcept\u003c/span\u003e \u003cspan\u003eException\u003c/span\u003e \u003cspan\u003eas\u003c/span\u003e \u003cspan\u003eerr\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n            \u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;ERROR!\u0026#34;\u003c/span\u003e \u003cspan\u003e+\u003c/span\u003e \u003cspan\u003estr\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eerr\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e\n            \u003cspan\u003eoutput\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eTaggedOutput\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#39;error\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eelement\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n        \n        \u003cspan\u003eyield\u003c/span\u003e \u003cspan\u003eoutput\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"tx0pf\"\u003eIt\u0026#39;s worth spending a few minutes on the need for \u003ccode\u003eextract_model_reply()\u003c/code\u003e. Because the model is self-hosted, we cannot guarantee that the text will be a JSON output. To ensure that we get a JSON output, we need to run a couple of checks. One benefit of using the Gemini API is that it includes a feature that ensures the output is always JSON, known as \u003ca href=\"https://ai.google.dev/gemini-api/docs/api-overview\"\u003econstrained decoding\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"3al37\"\u003eLet’s now use these functions in our pipeline:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003efiltered_results\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eresults\u003c/span\u003e \u003cspan\u003e|\u003c/span\u003e \u003cspan\u003e\u0026#34;Process Results\u0026#34;\u003c/span\u003e \u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eParDo\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eSentimentAnalysis\u003c/span\u003e\u003cspan\u003e())\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ewith_outputs\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#39;main\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e\u0026#39;negative\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e\u0026#39;error\u0026#39;\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"tx0pf\"\u003eUsing \u003ccode\u003ewith_outputs\u003c/code\u003e creates multiple accessible collections in \u003ccode\u003efiltered_results\u003c/code\u003e. The main collection has sentiments and summaries for positive and neutral reviews, while error contains any unparsable responses from the LLM. You can send these collections to other sources, such as BigQuery, with a write transform. This example doesn’t demonstrate this step, however, the negative collection is something that we want to do more within this pipeline.\u003c/p\u003e\u003ch3 data-block-key=\"bu4oj\"\u003e\u003cbr/\u003e\u003cb\u003eNegative sentiment processing\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"itoo\"\u003eMaking sure customers are happy is critical for retention. While we have used a light-hearted example with our pineapple on pizza debate, the direct interactions with a customer should always strive for empathy and positive responses from all parts of an organization. At this stage, we pass on this chat to one of the trained support representatives, but we can still see if the LLM is able to assist that support person in reducing the time to resolution.\u003c/p\u003e\u003cp data-block-key=\"2s9kk\"\u003eFor this step, we make a call to the model and ask it to formulate a response. We again use the Gemma 2B model for this call in the code.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003egenerated_responses\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eresults\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003enegative\u003c/span\u003e \n       \u003cspan\u003e|\u003c/span\u003e \u003cspan\u003e\u0026#34;Generate Response\u0026#34;\u003c/span\u003e \u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eMap\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elambda\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e((\u003c/span\u003e\u003cspan\u003ex\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e],\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e3\u003c/span\u003e\u003cspan\u003e]),\u003c/span\u003e \u003cspan\u003e\u0026#34;\u0026lt;prompt\u0026gt;Generate an apology response for the user in this chat text: \u0026#34;\u003c/span\u003e \u003cspan\u003e+\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e \u003cspan\u003e+\u003c/span\u003e \u003cspan\u003e\u0026#34;\u0026lt;answer\u0026gt;\u0026#34;\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e\n       \u003cspan\u003e|\u003c/span\u003e \u003cspan\u003e\u0026#34;Gemma-Response\u0026#34;\u003c/span\u003e \u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan\u003eRunInference\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ekeyed_model_handler\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"tx0pf\"\u003eIn general, you wrap the prompt creation code in a DoFn, but it is also possible to use a simple lambda in the pipeline code itself. Here we generate a prompt that contains the original chat message, which was extracted in the \u003ccode\u003eSentimentAnalysis\u003c/code\u003e function.\u003c/p\u003e\u003cp data-block-key=\"8p1ic\"\u003eFor local running and testing we can make use of some simple print statements to see the outputs on the various PCollections:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003egenerated_responses\u003c/span\u003e \u003cspan\u003e|\u003c/span\u003e \u003cspan\u003e\u0026#34;Print Response\u0026#34;\u003c/span\u003e \u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eMap\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003efiltered_results\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emain\u003c/span\u003e \u003cspan\u003e|\u003c/span\u003e \u003cspan\u003e\u0026#34;Print Main\u0026#34;\u003c/span\u003e \u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eMap\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003efiltered_results\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eerror\u003c/span\u003e \u003cspan\u003e|\u003c/span\u003e \u003cspan\u003e\u0026#34;Print Errors\u0026#34;\u003c/span\u003e \u003cspan\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan\u003ebeam\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eMap\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"tx0pf\"\u003eOf course for the real usage, these outputs will be sent to various sinks such as Pub/Sub and BigQuery.\u003c/p\u003e\u003ch3 data-block-key=\"a19k4\"\u003e\u003cbr/\u003e\u003cb\u003eRun the pipeline\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"1d9uv\"\u003eLets see how the model does with the previous JSON message:\u003c/p\u003e\u003cp data-block-key=\"c0u60\"\u003e\u003cb\u003eStep 1: Sentiment analysis and summarization\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"asus6\"\u003e\u003ci\u003e\u0026#34;sentiment\u0026#34;: -1,\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"hfk6\"\u003e\u003ci\u003e\u0026#34;summary\u0026#34;: \u0026#34;User 221 is very unhappy about the presence of pineapple on pizza.\u0026#34;\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"ev8h\"\u003eThe responses that the 2B model generated aren’t bad. The sentiment is correct, and because the results of the summary are more subjective, the correctness of the response depends on the downstream uses of this information.\u003c/p\u003e\u003cp data-block-key=\"54qms\"\u003e\u003cb\u003eStep 2: Generated response\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"a28bb\"\u003e\u0026#34;\u003ci\u003eI understand that you\u0026#39;re upset about the pineapple pizza. It\u0026#39;s a very personal preference, and I apologize that it might have caused you some frustration. We strive to offer a diverse menu to cater to a wide range of tastes, and we\u0026#39;re always open to feedback. Would you like to share your thoughts on the pizza with pineapple?\u003c/i\u003e\u0026#34;\u003c/p\u003e\u003cp data-block-key=\"dr9sk\"\u003eAre these responses acceptable? At this stage, we intend to send the whole data package to a support representative for analysis and if they are happy with it they can send it as is, or they can make some edits and adjustments.\u003c/p\u003e\u003ch2 data-block-key=\"c3lbe\"\u003e\u003cb\u003e\u003cbr/\u003eNext Steps\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"4sgqk\"\u003eMaybe at this stage we want to make use of a model with more parameters, such as Gemma2 9B or 27B. We could also use a model that is large enough that it requires an API call to an external service call, such as Gemini, instead of being loaded onto a worker. After all, we reduced the work needed to send to these larger models by using the smaller model as a filter. Making these choices is not just a technical decision, but also a business decision. The costs and benefits need to be measured. We can again make use of Dataflow to more easily set up A/B testing.\u003c/p\u003e\u003cp data-block-key=\"bbkms\"\u003eYou also may choose to \u003ca href=\"https://ai.google.dev/gemma/docs/distributed_tuning\"\u003efinetune\u003c/a\u003e a model custom to your use case. This is one way of changing the “voice” of the model to suit your needs.\u003c/p\u003e\u003ch2 data-block-key=\"ciknk\"\u003e\u003cb\u003e\u003cbr/\u003eA/B Testing\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"fr6no\"\u003eIn our generate step, we passed all incoming negative chats to our 2B model. If we wanted to send a portion of the collection to another model, we can use the \u003ca href=\"https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.core.html#apache_beam.transforms.core.Partition\"\u003ePartition\u003c/a\u003e function in Beam with the \u003ccode\u003efiltered_responses.negative\u003c/code\u003e collection. By directing some customer messages to different models and having support staff rate the generated responses before sending them, we can collect valuable feedback on response quality and improvement margins.\u003c/p\u003e\u003ch2 data-block-key=\"40ko\"\u003e\u003cbr/\u003eSummary\u003c/h2\u003e\u003cp data-block-key=\"5o182\"\u003eWith those few lines of code, we built a system capable of processing customer sentiment data at high velocity and variability. By using the Gemma 2 open model, with its \u0026#39;unmatched performance for its size\u0026#39;, we were able to incorporate this powerful LLM within a stream processing use case that helps create a better experience for customers.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "21 min read",
  "publishedTime": "2024-08-16T00:00:00Z",
  "modifiedTime": null
}
