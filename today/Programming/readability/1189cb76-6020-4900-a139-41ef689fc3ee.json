{
  "id": "1189cb76-6020-4900-a139-41ef689fc3ee",
  "title": "Google Vertex AI Provides RAG Engine for Large Language Model Grounding",
  "link": "https://www.infoq.com/news/2025/01/google-vertes-ai-rag-engine/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Vertex AI RAG Engine is a managed orchestration service aimed to make it easier to connect large language models (LLMs) to external data sources to be more up-to-date, generate more relevant responses, and hallucinate less. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Mon, 20 Jan 2025 16:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Python",
    "Retrieval-Augmented Generation",
    "Large language models",
    "Development",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 4037,
  "excerpt": "Vertex AI RAG Engine is a managed orchestration service aimed to make it easier to connect large language models (LLMs) to external data sources to be more up-to-date, generate more relevant responses",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250116231159/apple-touch-icon.png",
  "text": "Vertex AI RAG Engine is a managed orchestration service aimed to make it easier to connect large language models (LLMs) to external data sources to be more up-to-date, generate more relevant responses, and hallucinate less. According to Google, its new RAG Engine is the \"sweet spot\" for developers using Vertex AI to implement a RAG-based LLM, providing a balance between the ease of use of Vertex AI Search and the power of a custom RAG pipeline built using lower-level Vertex AI APIs such as Text Embedding API, Ranking API, etc. The overall workflow supported by Vertex AI RAG Engine includes distinct steps for data ingestion from a number of different sources; data transformation, such as splitting data into chunks previous to indexing; embedding, which provides a numerical representation of text to capture its semantics and context; data indexing to build a corpus optimized for search; retrieval of relevant information from the knowledge based on a user's prompt; and, last, a generation step where the original user query is augmented with the retrieved information. Using Vertex AI RAG Engine you can easily integrate all those steps into your solution. The easiest way to start with Vertex AI RAG Engine is through its Python bindings which are part of the google-cloud-aiplatform package. After setting up a Google Cloud project and initializing the Vertex AI engine, you can easily create a corpus from your own local files or documents in Google Cloud Storage or Google Drive by using the upload_file or import_file methods. # Currently supports Google first-party embedding models EMBEDDING_MODEL = \"publishers/google/models/text-embedding-004\" # @param {type:\"string\", isTemplate: true} embedding_model_config = rag.EmbeddingModelConfig(publisher_model=EMBEDDING_MODEL) rag_corpus = rag.create_corpus( display_name=\"my-rag-corpus\", embedding_model_config=embedding_model_config ) rag_file = rag.upload_file( corpus_name=rag_corpus.name, path=\"test.txt\", display_name=\"test.txt\", description=\"my test file\", ) Once you have a corpus, you create a retrieval tool which is then connected to the LLM to expose a new endpoint you can use to query the augmented model: # Create a tool for the RAG Corpus rag_retrieval_tool = Tool.from_retrieval( retrieval=rag.Retrieval( source=rag.VertexRagStore( rag_corpora=[rag_corpus.name], similarity_top_k=10, vector_distance_threshold=0.5, ), ) ) # Load tool into Gemini model rag_gemini_model = GenerativeModel( \"gemini-1.5-flash-001\", # your self-deployed endpoint tools=[rag_retrieval_tool], ) response = rag_gemini_model.generate_content(\"What is RAG?\") According to Google, Vertex AI RAG Engine is particularly convenient for use cases like personalized investment advice and risk assessment, accelerated drug discovery and personalized treatment plans, and enhanced due diligence and contract review. Retrieval Augmented Generation (RAG) is a technique often use to \"ground\" a large language model, that is, making it fitter to a particular use case or enterprise environment. RAG consists of retrieving information relevant to a particular task from a source that was not accessible to the model during training and feed it to the model along with a prompt. Alternatively, a model can be \"grounded\" through fine-tuning, a process whereas the external data is used to retrain the model so it is available for each query even when not specified at the prompt level. Grounding a model enables it to better understand the context of a query and to have additional task-specific information available so it can generate a better response. More specifically in the context of enterprise data, grounding aims to circumvent a limitation of general LLMs by providing access to private data behind firewalls in a safe way. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/01/google-vertes-ai-rag-engine/en/headerimage/google-vertex-ai-rag-engine-1737385771520.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/vertex-ai-rag-engine-a-developers-tool/\"\u003eVertex AI RAG Engine\u003c/a\u003e is a managed orchestration service aimed to make it easier to connect large language models (LLMs) to external data sources to be more up-to-date, generate more relevant responses, and hallucinate less.\u003c/p\u003e\n\n\u003cp\u003eAccording to Google, its new RAG Engine is the \u0026#34;sweet spot\u0026#34; for developers using Vertex AI to implement a RAG-based LLM, providing a balance between the ease of use of Vertex AI Search and the power of a custom RAG pipeline built using lower-level Vertex AI APIs such as Text Embedding API, Ranking API, etc.\u003c/p\u003e\n\n\u003cp\u003eThe overall workflow supported by Vertex AI RAG Engine includes distinct steps for \u003cem\u003edata ingestion\u003c/em\u003e from a number of different sources; \u003cem\u003edata transformation\u003c/em\u003e, such as splitting data into chunks previous to indexing; \u003cem\u003eembedding\u003c/em\u003e, which provides a numerical representation of text to capture its semantics and context; \u003cem\u003edata indexing\u003c/em\u003e to build a \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/manage-your-rag-corpus#corpus-management\"\u003ecorpus\u003c/a\u003e optimized for search; \u003cem\u003eretrieval\u003c/em\u003e of relevant information from the knowledge based on a user\u0026#39;s prompt; and, last, a \u003cem\u003egeneration\u003c/em\u003e step where the original user query is augmented with the retrieved information.\u003c/p\u003e\n\n\u003cp\u003eUsing Vertex AI RAG Engine you can easily integrate all those steps into your solution. The easiest way to start with Vertex AI RAG Engine is through its \u003ca href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\"\u003ePython bindings\u003c/a\u003e which are part of the \u003ccode\u003egoogle-cloud-aiplatform\u003c/code\u003e package. After setting up a Google Cloud project and initializing the Vertex AI engine, you can easily create a corpus from your own local files or documents in Google Cloud Storage or Google Drive by using the \u003ccode\u003eupload_file\u003c/code\u003e or \u003ccode\u003eimport_file\u003c/code\u003e methods.\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e# Currently supports Google first-party embedding models\nEMBEDDING_MODEL = \u0026#34;publishers/google/models/text-embedding-004\u0026#34;  # @param {type:\u0026#34;string\u0026#34;, isTemplate: true}\nembedding_model_config = rag.EmbeddingModelConfig(publisher_model=EMBEDDING_MODEL)\n\nrag_corpus = rag.create_corpus(\n    display_name=\u0026#34;my-rag-corpus\u0026#34;, embedding_model_config=embedding_model_config\n)\n\nrag_file = rag.upload_file(\n    corpus_name=rag_corpus.name,\n    path=\u0026#34;test.txt\u0026#34;,\n    display_name=\u0026#34;test.txt\u0026#34;,\n    description=\u0026#34;my test file\u0026#34;,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eOnce you have a corpus, you create a \u003cem\u003eretrieval tool\u003c/em\u003e which is then connected to the LLM to expose a new endpoint you can use to query the augmented model:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e# Create a tool for the RAG Corpus\nrag_retrieval_tool = Tool.from_retrieval(\n    retrieval=rag.Retrieval(\n        source=rag.VertexRagStore(\n            rag_corpora=[rag_corpus.name],\n            similarity_top_k=10,\n            vector_distance_threshold=0.5,\n        ),\n    )\n)\n\n# Load tool into Gemini model\nrag_gemini_model = GenerativeModel(\n    \u0026#34;gemini-1.5-flash-001\u0026#34;,  # your self-deployed endpoint\n    tools=[rag_retrieval_tool],\n)\n\nresponse = rag_gemini_model.generate_content(\u0026#34;What is RAG?\u0026#34;)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eAccording to Google, Vertex AI RAG Engine is particularly convenient for use cases like personalized investment advice and risk assessment, accelerated drug discovery and personalized treatment plans, and enhanced due diligence and contract review.\u003c/p\u003e\n\n\u003cp\u003eRetrieval Augmented Generation (RAG) is a technique often use to \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview\"\u003e\u0026#34;ground\u0026#34;\u003c/a\u003e a large language model, that is, making it fitter to a particular use case or enterprise environment. RAG consists of retrieving information relevant to a particular task from a source that was not accessible to the model during training and feed it to the model along with a prompt. Alternatively, a model can be \u0026#34;grounded\u0026#34; through fine-tuning, a process whereas the external data is used to retrain the model so it is available for each query even when not specified at the prompt level.\u003c/p\u003e\n\n\u003cp\u003eGrounding a model enables it to better understand the context of a query and to have additional task-specific information available so it can generate a better response. More specifically in the context of enterprise data, grounding aims to circumvent a limitation of general LLMs by providing access to private data behind firewalls in a safe way.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-01-20T00:00:00Z",
  "modifiedTime": null
}
