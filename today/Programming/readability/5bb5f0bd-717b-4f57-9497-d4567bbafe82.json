{
  "id": "5bb5f0bd-717b-4f57-9497-d4567bbafe82",
  "title": "Gemma explained: PaliGemma architecture",
  "link": "https://developers.googleblog.com/en/gemma-explained-paligemma-architecture/",
  "description": "PaliGemma, a lightweight open vision-language model (VLM), is able to take both image and text inputs and produce a text response, adding an additional vision model to the BaseGemma model.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Ju-yeong Ji, Ravin Kumar",
  "length": 9129,
  "excerpt": "PaliGemma, a lightweight open vision-language model (VLM), is able to take both image and text inputs and produce a text response, adding an additional vision model to the BaseGemma model.",
  "siteName": "",
  "favicon": "",
  "text": "Ravin Kumar Google Data Scientist Language Applications In the previous post of Gemma explained, you reviewed RecurrentGemma architecture. In this blog post, you will explore PaliGemma architecture. Let‚Äôs dive into it!PaliGemma 3BPaliGemma is a lightweight open vision-language model (VLM) inspired by PaLI-3, and based on open components like the SigLIP vision model and the Gemma language model. Pali stands for Pathway Language and Image Model. As the name implies this model is able to take both image and text inputs and produce a text response, as you can see in this fine tuning guide.PaliGemma ArchitecturePaliGemma adds an additional vision model to the BaseGemma model, which consists of an image encoder. This encoder along with the text tokens is passed to a specialized Gemma 2B model. Both the Vision Model and Gemma model are trained in various stages both independently, and together, to produce the final joint architecture. For full details see Section 3.2 of the Pali-3 paper PaliGemmaForConditionalGeneration( (vision_tower): SiglipVisionModel( (vision_model): SiglipVisionTransformer( (embeddings): SiglipVisionEmbeddings( (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid) (position_embedding): Embedding(256, 1152) ) (encoder): SiglipEncoder( (layers): ModuleList( (0-26): 27 x SiglipEncoderLayer( (self_attn): SiglipAttention( (k_proj): Linear(in_features=1152, out_features=1152, bias=True) (v_proj): Linear(in_features=1152, out_features=1152, bias=True) (q_proj): Linear(in_features=1152, out_features=1152, bias=True) (out_proj): Linear(in_features=1152, out_features=1152, bias=True) ) (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True) (mlp): SiglipMLP( (activation_fn): PytorchGELUTanh() (fc1): Linear(in_features=1152, out_features=4304, bias=True) (fc2): Linear(in_features=4304, out_features=1152, bias=True) ) (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True) ) ) ) (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True) ) ) (multi_modal_projector): PaliGemmaMultiModalProjector( (linear): Linear(in_features=1152, out_features=2048, bias=True) ) (language_model): GemmaForCausalLM( (model): GemmaModel( (embed_tokens): Embedding(257216, 2048, padding_idx=0) (layers): ModuleList( (0-17): 18 x GemmaDecoderLayer( (self_attn): GemmaSdpaAttention( (q_proj): Linear(in_features=2048, out_features=2048, bias=False) (k_proj): Linear(in_features=2048, out_features=256, bias=False) (v_proj): Linear(in_features=2048, out_features=256, bias=False) (o_proj): Linear(in_features=2048, out_features=2048, bias=False) (rotary_emb): GemmaRotaryEmbedding() ) (mlp): GemmaMLP( (gate_proj): Linear(in_features=2048, out_features=16384, bias=False) (up_proj): Linear(in_features=2048, out_features=16384, bias=False) (down_proj): Linear(in_features=16384, out_features=2048, bias=False) (act_fn): PytorchGELUTanh() ) (input_layernorm): GemmaRMSNorm() (post_attention_layernorm): GemmaRMSNorm() ) ) (norm): GemmaRMSNorm() ) (lm_head): Linear(in_features=2048, out_features=257216, bias=False) ) ) vision_tower (SiglipVisionModel)This component is responsible for processing the input image.It uses SiglipVisionTransformer which is a type of transformer architecture designed for vision tasks.embeddings (SiglipVisionEmbeddings)PaliGemma takes as input one or more images, which are turned into ‚Äúsoft tokens‚Äù by the SigLIP encoder.It breaks the image into smaller patches, similar to how a text model processes words in a sentence. The model then learns to capture relationships between these patches, effectively understanding the image‚Äôs visual content.patch_embeddingIt uses a convolutional layer (Conv2d) with the following parameters.3: The input has 3 channels (for RGB images)1152: The output has 1152 channels, which is the embedding dimension of each patchkernel_size=(14, 14): Each patch is a 14x14 pixel squarestride=(14, 14): The patches are taken with no overlap (the convolutional filter moves 14 pixels at a time)padding=‚Äôvalid‚Äô: No padding is applied, so the output size will be smaller than the input size.position_embeddingPosition embeddings are added to each patch embedding to encode the spatial information (i.e., where each patch was located in the original image).This is done using a learned embedding layer (Embedding) that takes as input the position of each patch (up to 256 positions) and outputs a vector of size 1152 (the same as the patch embedding dimension).encoder (SiglipEncoder)The embeddings pass through a series of SiglipEncoderLayer, each consisting of self-attention and feed-forward neural networks. This helps the model capture relationships between different parts of the image.multi_modal_projector (PaliGemmaMultiModalProjector)This component projects the output of the vision tower into a multi-modal space. This is achieved using a simple linear layer and it allows the vision and language representations to be combined effectively.language_model (GemmaForCausalLM)This component is a language model based on the Gemma 2B model.It takes as input the multi-modal representation from the projector and generates text output.For the text input, each checkpoint was trained with various sequence lengths. For example, paligemma-3b-mix-224 was trained with sequence length 256 (input text + output text tokenized by Gemma‚Äôs tokenizer).PaliGemma uses the Gemma tokenizer with 256000 tokens, but extends its vocabulary with 1024 entries that represent coordinates in normalized image-space (\u003cloc0000\u003e...\u003cloc1023\u003e), and another with 128 entries (\u003cseg000\u003e...\u003cseg127\u003e) that are codewords used by a lightweight referring-expression segmentation vector-quantized variational auto-encoder (VQ-VAE). (256000 + 1024 + 128 = 257216)Object Segmentation ExampleAdditional soft tokens encode object detection and image segmentation. Below is an example output from the paligemma-3b-mix-224. You can try it by yourself from the HuggingFace live demo. Output from the PaliGemma with the prompt ‚Äúsegment floor;cat;person;‚Äù The outputs from the model are unintuitive to decode if you are not familiar with ML and computer vision tasks.The initial four location tokens represent the coordinate of the bounding box, ranging from 0 to 1023. These coordinates are independent of the aspect ratio, as the image is assumed to be resized to 1024 x 1024.For instance, the output displays the cat's location within the coordinates (382, 637) and (696, 784). In this coordinate system, the top left corner is denoted as (0,0) and the vertical coordinate is listed before the horizontal coordinate. The mask is encoded with the following 16 segmentation tokens. A neural network model (VQ-VAE) can reconstruct masks from quantized representations (codebook indices) by decoding those values. You can explore the actual code from here.At last, you can obtain this beautiful outcome from the output of the PaliGemma. SummaryIn this article, you learned about PaliGemma.The Gemma family presents a unique opportunity to understand modern large language model systems by offering a collection of open weights models with similar core architectures but designed for different use cases. These models, released by Google for researchers, developers, and end users, span various functionalities and complexities.We hope this overview provides a concise understanding of the Gemma model family, highlighting its versatility and suitability for a wide array of tasks.The Google Developer Community Discord server is an excellent platform to showcase your projects, establish connections with fellow developers, and engage in interactive discussions. Consider joining the server to explore these exciting opportunities.Thanks for reading!ReferencesPapersPaLI-3 Vision Language Models: Smaller, Faster, StrongerSigmoid Loss for Language Image Pre-TrainingAll in Tokens: Unifying Output Space of Visual Tasks via Soft TokenPaliGemma: A versatile 3B VLM for transferCode ExamplesInference with KerasFine-tuning with JAXHuggingFace live demoüìã The complete Gemma architecture seriesGemma explained: An overview of Gemma model family architecturesGemma explained: What‚Äôs new in Gemma 2Gemma explained: RecurrentGemma architectureGemma explained: PaliGemma architecture",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma_metadata_image_1600_x_873_p.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \u003csection\u003e\n      \n        \n          \n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=Ravin+Kumar\"\u003eRavin Kumar\u003c/a\u003e\n            \n              \u003cspan\u003eGoogle Data Scientist\u003c/span\u003e\n            \n            \n              \u003cspan\u003eLanguage Applications\u003c/span\u003e\n            \n          \u003c/p\u003e\n        \n\n      \n      \u003c/section\u003e\n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"l9qqv\"\u003eIn the \u003ca href=\"https://developers.googleblog.com/en/gemma-explained-recurrentgemma-architecture/\"\u003eprevious post\u003c/a\u003e of Gemma explained, you reviewed RecurrentGemma architecture. In this blog post, you will explore PaliGemma architecture. Let‚Äôs dive into it!\u003c/p\u003e\u003ch2 data-block-key=\"c9n87\"\u003e\u003cb\u003e\u003cbr/\u003ePaliGemma 3B\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"e3j1t\"\u003ePaliGemma is a lightweight open vision-language model (VLM) inspired by \u003ca href=\"https://arxiv.org/abs/2310.09199\"\u003ePaLI-3\u003c/a\u003e, and based on open components like the \u003ca href=\"https://arxiv.org/abs/2303.15343\"\u003eSigLIP vision model\u003c/a\u003e and the \u003ca href=\"https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf\"\u003eGemma language model\u003c/a\u003e. Pali stands for \u003ca href=\"https://arxiv.org/pdf/2209.06794\"\u003e\u003cb\u003ePa\u003c/b\u003ethway \u003cb\u003eL\u003c/b\u003eanguage and \u003cb\u003eI\u003c/b\u003emage Model.\u003c/a\u003e As the name implies this model is able to take both image and text inputs and produce a text response, as you can see in this fine tuning \u003ca href=\"https://ai.google.dev/gemma/docs/paligemma/fine-tuning-paligemma\"\u003eguide\u003c/a\u003e.\u003c/p\u003e\u003ch2 data-block-key=\"367j1\"\u003e\u003cbr/\u003ePaliGemma Architecture\u003c/h2\u003e\u003cp data-block-key=\"22to5\"\u003ePaliGemma adds an additional vision model to the BaseGemma model, which consists of an image encoder. This encoder along with the text tokens is passed to a specialized Gemma 2B model. Both the Vision Model and Gemma model are trained in various stages both independently, and together, to produce the final joint architecture. For full details see Section 3.2 of the \u003ca href=\"https://arxiv.org/abs/2310.09199\"\u003ePali-3 paper\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image4_W4FHDmx.original.png\" alt=\"Joint architecture of the Vision Model and Gemma 2B model\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003ePaliGemmaForConditionalGeneration\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003evision_tower\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipVisionModel\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003evision_model\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipVisionTransformer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eembeddings\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipVisionEmbeddings\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epatch_embedding\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eConv2d\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e3\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ekernel_size\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e14\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e14\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003estride\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e14\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e14\u003c/span\u003e\u003cspan\u003e),\u003c/span\u003e \u003cspan\u003epadding\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003evalid\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eposition_embedding\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eEmbedding\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eencoder\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipEncoder\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayers\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eModuleList\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e26\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003e27\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e \u003cspan\u003eSiglipEncoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself_attn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipAttention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eout_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayer_norm1\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLayerNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eelementwise_affine\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eSiglipMLP\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eactivation_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003efc1\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4304\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n              \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003efc2\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4304\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayer_norm2\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLayerNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eelementwise_affine\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epost_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLayerNorm\u003c/span\u003e\u003cspan\u003e((\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003eeps\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1e-06\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eelementwise_affine\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emulti_modal_projector\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePaliGemmaMultiModalProjector\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elinear\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1152\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elanguage_model\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaForCausalLM\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaModel\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eembed_tokens\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eEmbedding\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e257216\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003epadding_idx\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayers\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eModuleList\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e17\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003e18\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e \u003cspan\u003eGemmaDecoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself_attn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaSdpaAttention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaRotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n          \u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaMLP\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003egate_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e16384\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eup_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e16384\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edown_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e16384\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n            \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n          \u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003einput_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epost_attention_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003enorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elm_head\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e257216\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"l9qqv\"\u003e\u003cb\u003evision_tower (SiglipVisionModel)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"6rdfd\"\u003eThis component is responsible for processing the input image.\u003c/p\u003e\u003cp data-block-key=\"1odqh\"\u003eIt uses SiglipVisionTransformer which is a type of transformer architecture designed for vision tasks.\u003c/p\u003e\u003ch3 data-block-key=\"594st\"\u003e\u003cbr/\u003e\u003cb\u003eembeddings (SiglipVisionEmbeddings)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"4bbl7\"\u003ePaliGemma takes as input one or more images, which are turned into ‚Äúsoft tokens‚Äù by the SigLIP encoder.\u003c/p\u003e\u003cp data-block-key=\"e0gom\"\u003eIt breaks the image into smaller patches, similar to how a text model processes words in a sentence. The model then learns to capture relationships between these patches, effectively understanding the image‚Äôs visual content.\u003c/p\u003e\u003ch3 data-block-key=\"95ai5\"\u003e\u003cb\u003e\u003cbr/\u003epatch_embedding\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"4rij3\"\u003eIt uses a convolutional layer (Conv2d) with the following parameters.\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"522qk\"\u003e3: The input has 3 channels (for RGB images)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"662g2\"\u003e1152: The output has 1152 channels, which is the embedding dimension of each patch\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"af01l\"\u003ekernel_size=(14, 14): Each patch is a 14x14 pixel square\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ft8ts\"\u003estride=(14, 14): The patches are taken with no overlap (the convolutional filter moves 14 pixels at a time)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"es0rt\"\u003epadding=‚Äôvalid‚Äô: No padding is applied, so the output size will be smaller than the input size.\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"3n8cg\"\u003e\u003cb\u003e\u003cbr/\u003eposition_embedding\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"30n56\"\u003ePosition embeddings are added to each patch embedding to encode the spatial information (i.e., where each patch was located in the original image).\u003c/p\u003e\u003cp data-block-key=\"fgfii\"\u003eThis is done using a learned embedding layer (Embedding) that takes as input the position of each patch (up to 256 positions) and outputs a vector of size 1152 (the same as the patch embedding dimension).\u003c/p\u003e\u003ch3 data-block-key=\"4buh\"\u003e\u003cbr/\u003e\u003cb\u003eencoder (SiglipEncoder)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"elgib\"\u003eThe embeddings pass through a series of SiglipEncoderLayer, each consisting of self-attention and feed-forward neural networks. This helps the model capture relationships between different parts of the image.\u003c/p\u003e\u003ch3 data-block-key=\"3m7iv\"\u003e\u003cbr/\u003e\u003cb\u003emulti_modal_projector (PaliGemmaMultiModalProjector)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"80bi4\"\u003eThis component projects the output of the vision tower into a multi-modal space. This is achieved using a simple linear layer and it allows the vision and language representations to be combined effectively.\u003c/p\u003e\u003ch3 data-block-key=\"8d5vf\"\u003e\u003cbr/\u003e\u003cb\u003elanguage_model (GemmaForCausalLM)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"eslas\"\u003eThis component is a language model based on the Gemma 2B model.\u003c/p\u003e\u003cp data-block-key=\"7q3f1\"\u003eIt takes as input the multi-modal representation from the projector and generates text output.\u003c/p\u003e\u003cp data-block-key=\"8cf28\"\u003eFor the text input, each checkpoint was trained with various sequence lengths. For example, paligemma-3b-mix-224 was trained with sequence length 256 (input text + output text tokenized by Gemma‚Äôs tokenizer).\u003c/p\u003e\u003cp data-block-key=\"q5a1\"\u003ePaliGemma uses the Gemma tokenizer with 256000 tokens, but extends its vocabulary with 1024 entries that represent coordinates in normalized image-space (\u0026lt;loc0000\u0026gt;...\u0026lt;loc1023\u0026gt;), and another with 128 entries (\u0026lt;seg000\u0026gt;...\u0026lt;seg127\u0026gt;) that are codewords used by a lightweight referring-expression segmentation \u003ca href=\"https://arxiv.org/abs/2301.02229\"\u003evector-quantized variational auto-encoder (VQ-VAE)\u003c/a\u003e. (256000 + 1024 + 128 = 257216)\u003c/p\u003e\u003ch2 data-block-key=\"20mft\"\u003e\u003cbr/\u003eObject Segmentation Example\u003c/h2\u003e\u003cp data-block-key=\"3uli6\"\u003eAdditional soft tokens encode object detection and image segmentation. Below is an example output from the paligemma-3b-mix-224. You can try it by yourself from the \u003ca href=\"https://huggingface.co/spaces/big-vision/paligemma\"\u003eHuggingFace live demo\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image6_5Hff2p9.original.png\" alt=\"Image of a child and cat on a snowy roof top\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"l9qqv\"\u003eOutput from the PaliGemma with the prompt ‚Äú\u003ccode\u003esegment floor;cat;person;\u003c/code\u003e‚Äù\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2024-08-27_at_7.07.55PM.original.png\" alt=\"image of output from the PaliGemma with the prompt ‚Äúsegment floor;cat;person;‚Äù\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"l9qqv\"\u003eThe outputs from the model are unintuitive to decode if you are not familiar with ML and computer vision tasks.\u003c/p\u003e\u003cp data-block-key=\"5ijv9\"\u003eThe initial four location tokens represent the coordinate of the bounding box, ranging from 0 to 1023. These coordinates are independent of the aspect ratio, as the image is assumed to be resized to 1024 x 1024.\u003c/p\u003e\u003cp data-block-key=\"cp236\"\u003eFor instance, the output displays the cat\u0026#39;s location within the coordinates (382, 637) and (696, 784). In this coordinate system, the top left corner is denoted as (0,0) and the vertical coordinate is listed before the horizontal coordinate.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_cB2oyye.original.png\" alt=\"image showing the output displaying the cat\u0026#39;s location within coordinates (382, 637) and (696, 784)\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"l9qqv\"\u003eThe mask is encoded with the following 16 segmentation tokens. A neural network model (VQ-VAE) can reconstruct masks from quantized representations (codebook indices) by decoding those values. You can explore the actual code from \u003ca href=\"https://huggingface.co/spaces/big-vision/paligemma/blob/main/paligemma_parse.py\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"5ntsk\"\u003eAt last, you can obtain this beautiful outcome from the output of the PaliGemma.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_p4JSdrB.original.png\" alt=\"image showing object segmentation result, where the floor is shaded blue, the child is shaded red, and the cat is shaded yellow\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"l9qqv\"\u003e\u003cb\u003eSummary\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"bba24\"\u003eIn this article, you learned about PaliGemma.\u003c/p\u003e\u003cp data-block-key=\"69v4u\"\u003eThe Gemma family presents a unique opportunity to understand modern large language model systems by offering a collection of open weights models with similar core architectures but designed for different use cases. These models, released by Google for researchers, developers, and end users, span various functionalities and complexities.\u003c/p\u003e\u003cp data-block-key=\"9kc3m\"\u003eWe hope this overview provides a concise understanding of the Gemma model family, highlighting its versatility and suitability for a wide array of tasks.\u003c/p\u003e\u003cp data-block-key=\"c4a2a\"\u003eThe \u003ca href=\"https://discord.gg/google-dev-community\"\u003eGoogle Developer Community\u003c/a\u003e Discord server is an excellent platform to showcase your projects, establish connections with fellow developers, and engage in interactive discussions. Consider joining the server to explore these exciting opportunities.\u003c/p\u003e\u003cp data-block-key=\"3retk\"\u003eThanks for reading!\u003c/p\u003e\u003chr/\u003e\u003ch2 data-block-key=\"83biq\"\u003eReferences\u003c/h2\u003e\u003ch3 data-block-key=\"14pdc\"\u003e\u003cbr/\u003e\u003cb\u003ePapers\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"bjcdr\"\u003e\u003ca href=\"https://arxiv.org/abs/2310.09199\"\u003ePaLI-3 Vision Language Models: Smaller, Faster, Stronger\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"d9isu\"\u003e\u003ca href=\"https://arxiv.org/abs/2303.15343\"\u003eSigmoid Loss for Language Image Pre-Training\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"dsnbp\"\u003e\u003ca href=\"https://arxiv.org/abs/2301.02229\"\u003eAll in Tokens: Unifying Output Space of Visual Tasks via Soft Token\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"5khe2\"\u003e\u003ca href=\"https://arxiv.org/abs/2407.07726\"\u003ePaliGemma: A versatile 3B VLM for transfer\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"fr7ns\"\u003e\u003cbr/\u003e\u003cb\u003eCode Examples\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"14b5n\"\u003e\u003ca href=\"https://ai.google.dev/gemma/docs/paligemma/inference-with-keras\"\u003eInference with Keras\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"ffbr4\"\u003e\u003ca href=\"https://ai.google.dev/gemma/docs/paligemma/fine-tuning-paligemma\"\u003eFine-tuning with JAX\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"cfml6\"\u003e\u003ca href=\"https://huggingface.co/spaces/big-vision/paligemma\"\u003eHuggingFace live demo\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"2jgig\"\u003e\u003cbr/\u003eüìã The complete Gemma architecture series\u003c/h2\u003e\u003cul\u003e\u003cli data-block-key=\"944jt\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures\"\u003eGemma explained: An overview of Gemma model family architectures\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"8tio1\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-new-in-gemma-2\"\u003eGemma explained: What‚Äôs new in Gemma 2\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"2eid3\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-recurrentgemma-architecture/\"\u003eGemma explained: RecurrentGemma architecture\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"5s2u9\"\u003eGemma explained: PaliGemma architecture\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2024-09-05T00:00:00Z",
  "modifiedTime": null
}
