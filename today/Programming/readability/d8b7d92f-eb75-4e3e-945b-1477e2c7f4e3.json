{
  "id": "d8b7d92f-eb75-4e3e-945b-1477e2c7f4e3",
  "title": "So many tokens, so little time: Introducing a faster, more flexible byte-pair tokenizer",
  "link": "https://github.blog/ai-and-ml/llms/so-many-tokens-so-little-time-introducing-a-faster-more-flexible-byte-pair-tokenizer/",
  "description": "We released a new open source byte-pair tokenizer that is faster and more flexible than popular alternatives. The post So many tokens, so little time: Introducing a faster, more flexible byte-pair tokenizer appeared first on The GitHub Blog.",
  "author": "Hendrik van Antwerpen",
  "published": "Thu, 12 Dec 2024 13:51:13 +0000",
  "source": "https://github.blog/feed/",
  "categories": [
    "AI \u0026 ML",
    "LLMs",
    "open source"
  ],
  "byline": "Hendrik van Antwerpen, Alexander Neubeck",
  "length": 14030,
  "excerpt": "We released a new open source byte-pair tokenizer that is faster and more flexible than popular alternatives.",
  "siteName": "The GitHub Blog",
  "favicon": "https://github.blog/wp-content/uploads/2019/01/cropped-github-favicon-512.png?fit=192%2C192",
  "text": "Large language models (LLMs), such as those used by GitHub Copilot, do not operate directly on bytes but on tokens, which can complicate scaling. In this post, we explain how we solved that challenge at GitHub to support the growing number of Copilot users and features since the first launching Copilot two years ago. Tokenization is the process of turning bytes into tokens. The byte-pair encoding (BPE) algorithm is such a tokenizer, used (for example) by the OpenAI models we use at GitHub. The fastest of these algorithms not only have at least an O(n log(n)) complexity, they are also not incremental, and therefore badly suited for our use cases, which go beyond encoding an upfront known input. This limitation resulted in a number of scaling challenges that led us to create a novel algorithm to address them. Our algorithm not only scales linearly, but also easily out-performs popular libraries for all inputs. Read on to find out more about why and how we created the open source bpe algorithm that substantially improves state of the art BPE implementations in order to address our broader set of use cases. The importance of fast, flexible tokenization Retrieval augmented generation (RAG) is essential for GitHub Copilot’s capabilities. RAG is used to improve model output by augmenting the user’s prompt with relevant snippets of text or code. A typical RAG approach works as follows: Index repository content into an embeddings database to allow semantic search. Given a user’s prompt, search the embeddings database for relevant snippets and include those snippets in the prompt for the LLM. Tokenization is important for both of these steps. Most code files exceed the number of tokens that can be encoded into a single embedding, so we need to split the files into chunks that are within the token limit. When building a prompt there are also limits on the number of tokens that can be used. The amount of tokens can also impact response time and cost. Therefore, it is common to have some kind of budgeting strategy, which requires being able to track the number of tokens that components of the prompt contribute. In both of these cases, we are dynamically constructing a text and constantly need the updated token count during that process to decide how to proceed. However, most tokenizers provide only a single operation: encode a complete input text into tokens. When scaling to millions of repositories and billions of embeddings, the efficiency of token calculation really starts to matter. Additionally, we need to consider the worst-case performance of tokenization for the stability of our production systems. A system that processes untrusted user input in the form of billions of source code files cannot allow data that, intentionally or not, causes pathological run times and threatens availability. (See this discussion of potential denial-of-service issues in the context of OpenAI’s tiktoken tokenizer.) Some of the features that can help address these needs are: Keep track of the token count for a chunk while it is being built up. Count tokens for slices of an original text or abort counting when the text exceeds a given limit. Split text within a certain amount of tokens at a proper UTF-8 character boundary. Implementing these operations using current tokenization algorithms would result in at least quadratic runtime, when we would like the runtime to be linear. bpe: A fast tokenizer We were able to make substantial improvements to the state of the art BPE implementations in order to address the broader set of use cases that we have. Not only were we able to support more features, but we do so with much better performance and scalability than the existing libraries provide. Our implementation is open source with an MIT license can be found at https://github.com/github/rust-gems. The Rust crates are also published to crates.io as bpe and bpe-openai. The former contains the BPE implementation itself. The latter exposes convenience tokenizers (including pre-tokenization) for recent OpenAI token models. Read on for benchmark results and an introduction to the algorithm itself. Performance comparison We compare performance with two benchmarks. Both compare tokenization on randomly generated inputs of different sizes. The first uses any inputs, most of which will be split into smaller pieces during pre-tokenization. Pre-tokenization splits the input text into pieces (for example, using regular expressions). BPE is applied to those pieces instead of the complete input text. Since these pieces are typically small, this can significantly impact overall performance and hide the performance characteristics of the underlying BPE implementation. This benchmark allows comparing the performance that can be expected in practice. The second uses pathological inputs that won’t be split by pre-tokenization. This benchmark allows comparing the performance of the underlying BPE implementations and reflects worst-case performance. We used OpenAI’s o200k_base token model and compared our implementation with tiktoken-rs, a wrapper for OpenAI’s tiktoken library, and Huggingface’s tokenizers. All benchmarks were run single-threaded on an Apple M1 MacBook Pro. Here are the results, showing single-threaded throughput in MiB/s: The first figure shows the results for the benchmark that includes pre-tokenization. We see that our tokenizer outperforms tiktoken by almost 4x and Huggingface by about 10x. (These numbers are in line with tiktoken’s reported performance results. Note that our single-threaded performance is only matched when using eight threads.) The second figure shows the worst case complexity difference between our linear, Huggingface’s heap-based, and tiktoken’s quadratic implementation. The rest of this post details how we achieved this result. We explain the basic principle of byte-pair encoding, the insight that allows the faster algorithm, and a high-level description of the algorithm itself. Byte-pair encoding BPE is a technique to encode text as a sequence of tokens from a token dictionary. The token dictionary is just an ordered list of tokens. Each token is either a single byte, or the concatenation of a pair of previously defined tokens. A string is encoded by replacing bytes with single-byte tokens and token pairs by concatenated tokens, in dictionary order. Let’s see how the string abacbb is tokenized using the following dictionary: a b c ac bb ab acbb Initially, the string is tokenized into the single-byte tokens. Next, all occurrences (left to right) of the token pair a c are replaced by the token ac. This procedure is repeated until no more replacements are possible. For our input string abacbb, tokenization proceeds as follows: 1. a b a c b b 2. a b ac b b 3. a b ac bb 4. ab ac bb 5. ab acbb Note that initially we have several pairs of single-byte tokens that appear in the dictionary, such as a b and a c. Even though ab appears earlier in the string, ac is chosen because the token appears first in the token dictionary. It is this behavior that makes BPE non-incremental with respect to string operations such as slicing or appending. For example, the substring abacb is tokenized as ab ac b, but if another b is added, the resulting string abacbb is tokenized as ab acbb. Two tokens from the prefix abacb are gone, and the encoding for the longer string even ends up being shorter. The two main strategies for implementing BPE are: A naive approach that repeatedly iterates over the tokens to merge the next eligible token pair, resulting in quadratic complexity. A heap-based approach that keeps eligible token pairs sorted, resulting in O(n log(n)) complexity. However, all tokenizers require the full text up front. Tokenizing a substring or an extended string means starting the encoding from scratch. For that reason, the more interesting use cases from above quickly become very expensive (at least O(n2 log(n))). So, how can we do better? Composing valid encodings The difficulty of the byte-pair encoding algorithm (as described above) is that token pair replacements can happen anywhere in the string and can influence the final tokens at the beginning of the string. However, it turns out that there is a property, which we call compatibility, that allows us to build up tokenizations left-to-right: Given a valid encoding, we can append an additional token to produce a new valid encoding if the pair of the last token and the appended token are a valid encoding. A valid encoding means that the original BPE algorithm produces that same encoding. We’ll show what this means with an example, and refer to the crate’s README for a detailed proof. The sequence ab ac is a valid encoding for our example token dictionary. Is ab ac b a valid encoding? Check if the pair ac b is compatible: 1. a c b 2. ac b We got the same tokens back, which means ab ac b is the encoding ab ac b. Is ab ac bb a valid encoding? Again, check if the pair ac bb is compatible: 1. a c b b 2. ac b b 3. ac bb 4. acbb In this case, the tokens are incompatible, and ab ac bb is not valid. The next section explains how we can go from building valid encodings to finding the encoding for a given input string. Linear encoding Using the compatibility rule, we can implement linear encoding with a dynamic programming algorithm. The algorithm works by checking for each of the possible last tokens whether it is compatible with the tokenization of the remaining prefix. As we saw in the previous section, we only need the last token of the prefix’s encoding to decide this. Let’s apply this idea to our example abacbb and write down the full encodings for every prefix: a ——-\u003e a ab ——\u003e ab aba —–\u003e ab a abac —-\u003e ab ac abacb —\u003e ab ac b abacbb –\u003e ab acbb We only store the last token for every prefix. This gives us a ab a ac b for the first five prefixes. We can find the last token for a prefix with a simple lookup in the list. For example, the last token for ab is ab, and the last token for abac is ac. For the last token of abacbb we have three token candidates: b, bb, and acbb. For each of these we must check whether it is compatible with the last token of the remaining prefix: b b, ac bb, or ab acbb. Retokenizing these combinations gives bb, acbb, and ab acbb, which means acbb is the only valid choice here. The algorithm works forward by computing the last token for every position in the input, using the last tokens for previous positions in the way we just described. The resulting algorithm looks roughly like this: let last_tokens = vec![]; for pos in 0..text.len() { for candidate in all_potential_tokens_for_suffix(text[0..pos + 1]) { if token_len(candidate) == pos + 1 { last_tokens.push(candidate); break; } else if is_compatible( last_tokens[pos + 1 - token_len(candidate)], candidate, ) { last_tokens.push(candidate); break; } } } How do we implement this efficiently? Use an Aho-Corasick string matching automaton to get all suffix tokens of the string until position i. Start with the longest token, since those are more likely to survive than shorter tokens. Retokenize the token pair efficiently to implement the compatibility check. We could precompute and store all valid pairings, but with large token dictionaries this will be a lot of data. It turns out that we can retokenize pairs on the fly in effectively constant time. The string matching automaton is linear for the input length. Both the number of overlapping tokens and retokenization are bounded by constants determined by the token dictionary. Together, this gives us a linear runtime. Putting it together The Rust crate contains several different encoders based on this approach: Appending and prepending encoders that incrementally encode a text when content is added to it. The algorithm works using the approach outlined above, storing the last token for every text position. Whenever the text is extended, the last tokens for the new positions are computed using the previously computed last tokens. At any point the current token count is available in constant time. Furthermore, constant time snapshots and rollbacks are supported, which make it easy to implement dynamic chunk construction approaches. A fast full-text encoder based on backtracking. Instead of storing the last token for every text position, it only stores the tokens for the full input. The algorithm works left to right, picking a candidate token for the remaining input, and checking if it is compatible with the last token. The algorithm backtracks on the last token if none of the candidates are compatible. By trying the longest candidates first, very little backtracking happens in practice. An interval encoder which allows O(1) token counting on subranges of the original text (after preprocessing the text in O(n) time). The algorithm works by encoding the substring until the last token lines up with the token at that position for the original text. Usually, only a small prefix needs to be encoded before this alignment happens. We have explained our algorithm at a high level so far. The crate’s README contains more technical details and is a great starting point for studying the code itself. Written by Senior Software Engineering, GitHub Principal Software Engineer, GitHub Explore more from GitHub Docs Everything you need to master GitHub, all in one place. Go to Docs GitHub Build what’s next on GitHub, the place for anyone from anywhere to build anything. Start building Customer stories Meet the companies and engineering teams that build with GitHub. Learn more Work at GitHub! Check out our current job openings. Apply now",
  "image": "https://github.blog/wp-content/uploads/2024/01/Productivity-DarkMode-3.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\n\t\n\u003cp\u003eLarge language models (LLMs), such as those used by GitHub Copilot, do not operate directly on bytes but on \u003cem\u003etokens\u003c/em\u003e, which can complicate scaling. In this post, we explain how we solved that challenge at GitHub to support the growing number of Copilot users and features since the \u003ca href=\"https://github.blog/news-insights/product-news/github-copilot-is-generally-available-to-all-developers/\"\u003efirst launching Copilot\u003c/a\u003e two years ago.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eTokenization\u003c/em\u003e is the process of turning bytes into tokens. The byte-pair encoding (BPE) algorithm is such a tokenizer, used (for example) by the OpenAI models we use at GitHub. The fastest of these algorithms not only have at least an \u003cem\u003eO(n log(n))\u003c/em\u003e complexity, they are also not incremental, and therefore badly suited for our use cases, which go beyond encoding an upfront known input. This limitation resulted in a number of scaling challenges that led us to create a novel algorithm to address them. Our algorithm not only scales linearly, but also easily out-performs popular libraries for all inputs.\u003c/p\u003e\n\u003cp\u003eRead on to find out more about why and how we created the open source \u003ca href=\"https://github.com/github/rust-gems/tree/main/crates/bpe\"\u003ebpe\u003c/a\u003e algorithm that substantially improves state of the art BPE implementations in order to address our broader set of use cases.\u003c/p\u003e\n\u003ch2 id=\"the-importance-of-fast-flexible-tokenization\" id=\"the-importance-of-fast-flexible-tokenization\"\u003eThe importance of fast, flexible tokenization\u003ca href=\"#the-importance-of-fast-flexible-tokenization\" aria-label=\"The importance of fast, flexible tokenization\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.blog/ai-and-ml/generative-ai/what-is-retrieval-augmented-generation-and-what-does-it-do-for-generative-ai/\"\u003eRetrieval augmented generation\u003c/a\u003e (RAG) is essential for GitHub Copilot’s capabilities. RAG is used to improve model output by augmenting the user’s prompt with relevant snippets of text or code. A typical RAG approach works as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIndex repository content into an embeddings database to allow semantic search.\u003c/li\u003e\n\u003cli\u003eGiven a user’s prompt, search the embeddings database for relevant snippets and include those snippets in the prompt for the LLM.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization is important for both of these steps. Most code files exceed the number of tokens that can be encoded into a single embedding, so we need to split the files into chunks that are within the token limit. When building a prompt there are also limits on the number of tokens that can be used. The amount of tokens can also impact response time and cost. Therefore, it is common to have some kind of budgeting strategy, which requires being able to track the number of tokens that components of the prompt contribute. In both of these cases, we are dynamically constructing a text and constantly need the updated token count during that process to decide how to proceed. However, most tokenizers provide only a single operation: encode a complete input text into tokens.\u003c/p\u003e\n\u003cp\u003eWhen scaling to millions of repositories and billions of embeddings, the efficiency of token calculation really starts to matter. Additionally, we need to consider the worst-case performance of tokenization for the stability of our production systems. A system that processes untrusted user input in the form of billions of source code files cannot allow data that, intentionally or not, causes pathological run times and threatens availability. (See \u003ca href=\"https://github.com/openai/tiktoken/issues/195\"\u003ethis discussion\u003c/a\u003e of potential denial-of-service issues in the context of OpenAI’s tiktoken tokenizer.)\u003c/p\u003e\n\u003cp\u003eSome of the features that can help address these needs are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKeep track of the token count for a chunk while it is being built up.\u003c/li\u003e\n\u003cli\u003eCount tokens for slices of an original text or abort counting when the text exceeds a given limit.\u003c/li\u003e\n\u003cli\u003eSplit text within a certain amount of tokens at a proper UTF-8 character boundary.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eImplementing these operations using current tokenization algorithms would result in at least quadratic runtime, when we would like the runtime to be linear.\u003c/p\u003e\n\u003ch2 id=\"bpe-a-fast-tokenizer\" id=\"bpe-a-fast-tokenizer\"\u003ebpe: A fast tokenizer\u003ca href=\"#bpe-a-fast-tokenizer\" aria-label=\"bpe: A fast tokenizer\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe were able to make substantial improvements to the state of the art BPE implementations in order to address the broader set of use cases that we have. Not only were we able to support more features, but we do so with much better performance and scalability than the existing libraries provide.\u003c/p\u003e\n\u003cp\u003eOur implementation is open source with an MIT license can be found at \u003ca href=\"https://github.com/github/rust-gems\"\u003ehttps://github.com/github/rust-gems\u003c/a\u003e. The Rust crates are also published to crates.io as \u003ca href=\"https://crates.io/crates/bpe\"\u003ebpe\u003c/a\u003e and \u003ca href=\"https://crates.io/crates/bpe-openai\"\u003ebpe-openai\u003c/a\u003e. The former contains the BPE implementation itself. The latter exposes convenience tokenizers (including pre-tokenization) for recent OpenAI token models.\u003c/p\u003e\n\u003cp\u003eRead on for benchmark results and an introduction to the algorithm itself.\u003c/p\u003e\n\u003ch2 id=\"performance-comparison\" id=\"performance-comparison\"\u003ePerformance comparison\u003ca href=\"#performance-comparison\" aria-label=\"Performance comparison\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe compare performance with two benchmarks. Both compare tokenization on randomly generated inputs of different sizes.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe first uses any inputs, most of which will be split into smaller pieces during pre-tokenization. Pre-tokenization splits the input text into pieces (for example, using regular expressions). BPE is applied to those pieces instead of the complete input text. Since these pieces are typically small, this can significantly impact overall performance and hide the performance characteristics of the underlying BPE implementation. This benchmark allows comparing the performance that can be expected in practice.\u003c/li\u003e\n\u003cli\u003eThe second uses pathological inputs that won’t be split by pre-tokenization. This benchmark allows comparing the performance of the underlying BPE implementations and reflects worst-case performance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe used OpenAI’s o200k_base token model and compared our implementation with \u003ca href=\"https://crates.io/crates/tiktoken-rs/0.6.0/\"\u003etiktoken-rs\u003c/a\u003e, a wrapper for OpenAI’s tiktoken library, and Huggingface’s \u003ca href=\"https://crates.io/crates/tokenizers/0.20.1/\"\u003etokenizers\u003c/a\u003e. All benchmarks were run single-threaded on an Apple M1 MacBook Pro.\u003c/p\u003e\n\u003cp\u003eHere are the results, showing single-threaded throughput in MiB/s:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" src=\"https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=300\u0026amp;resize=300%2C300\" alt=\"Line graph displaying results for the benchmark that includes pre-tokenization. Our tokenizer outperforms tiktoken by almost 4x and Huggingface by about 10x.\" width=\"300\" height=\"300\" loading=\"lazy\" srcset=\"https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=1600 1600w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=150 150w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=300 300w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=768 768w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=1024 1024w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=1536 1536w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=1200 1200w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=600 600w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=400 400w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=200 200w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=1000 1000w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=90 90w, https://github.blog/wp-content/uploads/2024/12/throughput-with-pre-tokenization.png?w=116 116w\" sizes=\"auto, (max-width: 300px) 100vw, 300px\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" src=\"https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=300\u0026amp;resize=300%2C300\" alt=\"Line graph displaying the worst case complexity difference between our linear, Huggingface’s heap-based, and tiktoken’s quadratic implementation.\" width=\"300\" height=\"300\" loading=\"lazy\" srcset=\"https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=1600 1600w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=150 150w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=300 300w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=768 768w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=1024 1024w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=1536 1536w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=1200 1200w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=600 600w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=400 400w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=200 200w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=1000 1000w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=90 90w, https://github.blog/wp-content/uploads/2024/12/throughput-without-pre-tokenization.png?w=116 116w\" sizes=\"auto, (max-width: 300px) 100vw, 300px\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe first figure shows the results for the benchmark that includes pre-tokenization. We see that our tokenizer outperforms tiktoken by almost 4x and Huggingface by about 10x. (These numbers are in line with tiktoken’s reported \u003ca href=\"https://github.com/openai/tiktoken/blob/main/README.md#performance\"\u003eperformance results\u003c/a\u003e. Note that our single-threaded performance is only matched when using eight threads.) The second figure shows the worst case complexity difference between our linear, Huggingface’s heap-based, and tiktoken’s quadratic implementation.\u003c/p\u003e\n\u003cp\u003eThe rest of this post details how we achieved this result. We explain the basic principle of byte-pair encoding, the insight that allows the faster algorithm, and a high-level description of the algorithm itself.\u003c/p\u003e\n\u003ch2 id=\"byte-pair-encoding\" id=\"byte-pair-encoding\"\u003eByte-pair encoding\u003ca href=\"#byte-pair-encoding\" aria-label=\"Byte-pair encoding\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.blog/ai-and-ml/llms/so-many-tokens-so-little-time-introducing-a-faster-more-flexible-byte-pair-tokenizer/clanthology.org/P16-1162/\"\u003eBPE\u003c/a\u003e is a technique to encode text as a sequence of tokens from a token dictionary. The token dictionary is just an ordered list of tokens. Each token is either a single byte, or the concatenation of a pair of previously defined tokens. A string is encoded by replacing bytes with single-byte tokens and token pairs by concatenated tokens, in dictionary order.\u003c/p\u003e\n\u003cp\u003eLet’s see how the string \u003ccode\u003eabacbb\u003c/code\u003e is tokenized using the following dictionary:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ea b c ac bb ab acbb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eInitially, the string is tokenized into the single-byte tokens. Next, all occurrences (left to right) of the token pair \u003ccode\u003ea c\u003c/code\u003e are replaced by the token \u003ccode\u003eac\u003c/code\u003e. This procedure is repeated until no more replacements are possible. For our input string \u003ccode\u003eabacbb\u003c/code\u003e, tokenization proceeds as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e1. a b a c b b\n2. a b ac  b b\n3. a b ac  bb\n4. ab  ac  bb\n5. ab  acbb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that initially we have several pairs of single-byte tokens that appear in the dictionary, such as \u003ccode\u003ea b\u003c/code\u003e and \u003ccode\u003ea c\u003c/code\u003e. Even though \u003ccode\u003eab\u003c/code\u003e appears earlier in the string, \u003ccode\u003eac\u003c/code\u003e is chosen because the token appears first in the token dictionary. It is this behavior that makes BPE non-incremental with respect to string operations such as slicing or appending. For example, the substring \u003ccode\u003eabacb\u003c/code\u003e is tokenized as \u003ccode\u003eab ac b\u003c/code\u003e, but if another \u003ccode\u003eb\u003c/code\u003e is added, the resulting string \u003ccode\u003eabacbb\u003c/code\u003e is tokenized as \u003ccode\u003eab acbb\u003c/code\u003e. Two tokens from the prefix \u003ccode\u003eabacb\u003c/code\u003e are gone, and the encoding for the longer string even ends up being shorter.\u003c/p\u003e\n\u003cp\u003eThe two main strategies for implementing BPE are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA naive approach that repeatedly iterates over the tokens to merge the next eligible token pair, resulting in quadratic complexity.\u003c/li\u003e\n\u003cli\u003eA heap-based approach that keeps eligible token pairs sorted, resulting in \u003cem\u003eO(n log(n))\u003c/em\u003e complexity.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHowever, all tokenizers require the full text up front. Tokenizing a substring or an extended string means starting the encoding from scratch. For that reason, the more interesting use cases from above quickly become very expensive (at least \u003cem\u003eO(n\u003csup\u003e2\u003c/sup\u003e log(n))\u003c/em\u003e). So, how can we do better?\u003c/p\u003e\n\u003ch2 id=\"composing-valid-encodings\" id=\"composing-valid-encodings\"\u003eComposing valid encodings\u003ca href=\"#composing-valid-encodings\" aria-label=\"Composing valid encodings\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe difficulty of the byte-pair encoding algorithm (as described above) is that token pair replacements can happen anywhere in the string and can influence the final tokens at the beginning of the string. However, it turns out that there is a property, which we call \u003cem\u003ecompatibility\u003c/em\u003e, that allows us to build up tokenizations left-to-right:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eGiven a valid encoding, we can append an additional token to produce a new valid encoding if the pair of the last token and the appended token are a valid encoding.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eA valid encoding means that the original BPE algorithm produces that same encoding. We’ll show what this means with an example, and refer to the crate’s \u003ca href=\"https://github.com/github/rust-gems/blob/main/crates/bpe/README.md\"\u003eREADME\u003c/a\u003e for a detailed proof.\u003c/p\u003e\n\u003cp\u003eThe sequence \u003ccode\u003eab ac\u003c/code\u003e is a valid encoding for our example token dictionary.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIs \u003ccode\u003eab ac b\u003c/code\u003e a valid encoding? Check if the pair \u003ccode\u003eac b\u003c/code\u003e is compatible:\n\u003cpre\u003e\u003ccode\u003e1. a c b\n2. ac  b\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe got the same tokens back, which means \u003ccode\u003eab ac b\u003c/code\u003e is the encoding \u003ccode\u003eab ac b\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIs \u003ccode\u003eab ac bb\u003c/code\u003e a valid encoding? Again, check if the pair \u003ccode\u003eac bb\u003c/code\u003e is compatible:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e1. a c b b\n2. ac  b b\n3. ac  bb \n4. acbb \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn this case, the tokens are incompatible, and \u003ccode\u003eab ac bb\u003c/code\u003e is not valid.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe next section explains how we can go from \u003cem\u003ebuilding\u003c/em\u003e valid encodings to \u003cem\u003efinding\u003c/em\u003e the encoding for a given input string.\u003c/p\u003e\n\u003ch2 id=\"linear-encoding\" id=\"linear-encoding\"\u003eLinear encoding\u003ca href=\"#linear-encoding\" aria-label=\"Linear encoding\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eUsing the compatibility rule, we can implement linear encoding with a dynamic programming algorithm.\u003c/p\u003e\n\u003cp\u003eThe algorithm works by checking for each of the possible last tokens whether it is compatible with the tokenization of the remaining prefix. As we saw in the previous section, we only need the last token of the prefix’s encoding to decide this.\u003c/p\u003e\n\u003cp\u003eLet’s apply this idea to our example \u003ccode\u003eabacbb\u003c/code\u003e and write down the full encodings for every prefix:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ea\u003c/code\u003e ——-\u0026gt; \u003ccode\u003ea\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eab\u003c/code\u003e ——\u0026gt; \u003ccode\u003eab\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eaba\u003c/code\u003e —–\u0026gt; \u003ccode\u003eab a\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eabac\u003c/code\u003e —-\u0026gt; \u003ccode\u003eab ac\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eabacb\u003c/code\u003e —\u0026gt; \u003ccode\u003eab ac b\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eabacbb\u003c/code\u003e –\u0026gt; \u003ccode\u003eab acbb\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe only store the last token for every prefix. This gives us \u003ccode\u003ea ab a ac b\u003c/code\u003e for the first five prefixes. We can find the last token for a prefix with a simple lookup in the list. For example, the last token for \u003ccode\u003eab\u003c/code\u003e is \u003ccode\u003eab\u003c/code\u003e, and the last token for \u003ccode\u003eabac\u003c/code\u003e is \u003ccode\u003eac\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor the last token of \u003ccode\u003eabacbb\u003c/code\u003e we have three token candidates: \u003ccode\u003eb\u003c/code\u003e, \u003ccode\u003ebb\u003c/code\u003e, and \u003ccode\u003eacbb\u003c/code\u003e. For each of these we must check whether it is compatible with the last token of the remaining prefix: \u003ccode\u003eb b\u003c/code\u003e, \u003ccode\u003eac bb\u003c/code\u003e, or \u003ccode\u003eab acbb\u003c/code\u003e. Retokenizing these combinations gives \u003ccode\u003ebb\u003c/code\u003e, \u003ccode\u003eacbb\u003c/code\u003e, and \u003ccode\u003eab acbb\u003c/code\u003e, which means \u003ccode\u003eacbb\u003c/code\u003e is the \u003cem\u003eonly\u003c/em\u003e valid choice here. The algorithm works forward by computing the last token for every position in the input, using the last tokens for previous positions in the way we just described.\u003c/p\u003e\n\u003cp\u003eThe resulting algorithm looks roughly like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elet last_tokens = vec![];\nfor pos in 0..text.len() {\n  for candidate in all_potential_tokens_for_suffix(text[0..pos + 1]) {\n    if token_len(candidate) == pos + 1 {\n      last_tokens.push(candidate);\n      break;\n    } else if is_compatible(\n      last_tokens[pos + 1 - token_len(candidate)],\n      candidate,\n    ) {\n      last_tokens.push(candidate);\n      break;\n    }\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHow do we implement this efficiently?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUse an Aho-Corasick string matching automaton to get all suffix tokens of the string until position \u003cem\u003ei\u003c/em\u003e. Start with the longest token, since those are more likely to survive than shorter tokens.\u003c/li\u003e\n\u003cli\u003eRetokenize the token pair efficiently to implement the compatibility check. We could precompute and store all valid pairings, but with large token dictionaries this will be a lot of data. It turns out that we can retokenize pairs on the fly in effectively constant time.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe string matching automaton is linear for the input length. Both the number of overlapping tokens and retokenization are bounded by constants determined by the token dictionary. Together, this gives us a linear runtime.\u003c/p\u003e\n\u003ch2 id=\"putting-it-together\" id=\"putting-it-together\"\u003ePutting it together\u003ca href=\"#putting-it-together\" aria-label=\"Putting it together\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe Rust crate contains several different encoders based on this approach:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAppending and prepending encoders that incrementally encode a text when content is added to it.\u003c/strong\u003e The algorithm works using the approach outlined above, storing the last token for every text position. Whenever the text is extended, the last tokens for the new positions are computed using the previously computed last tokens. At any point the current token count is available in constant time. Furthermore, constant time snapshots and rollbacks are supported, which make it easy to implement dynamic chunk construction approaches.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eA fast full-text encoder based on backtracking.\u003c/strong\u003e Instead of storing the last token for every text position, it only stores the tokens for the full input. The algorithm works left to right, picking a candidate token for the remaining input, and checking if it is compatible with the last token. The algorithm backtracks on the last token if none of the candidates are compatible. By trying the longest candidates first, very little backtracking happens in practice.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAn interval encoder which allows \u003cem\u003eO(1)\u003c/em\u003e token counting on subranges of the original text (after preprocessing the text in \u003cem\u003eO(n)\u003c/em\u003e time).\u003c/strong\u003e The algorithm works by encoding the substring until the last token lines up with the token at that position for the original text. Usually, only a small prefix needs to be encoded before this alignment happens.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe have explained our algorithm at a high level so far. The crate’s \u003ca href=\"https://github.com/github/rust-gems/blob/main/crates/bpe/README.md\"\u003eREADME\u003c/a\u003e contains more technical details and is a great starting point for studying the code itself.\u003c/p\u003e\n\n\t\n\n\t\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tWritten by\t\u003c/h2\u003e\n\t\n\t\t\t\u003carticle\u003e\n\t\u003cdiv\u003e\n\t\t\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cpicture\u003e\n\t\t\t\t\t\u003csource srcset=\"https://avatars.githubusercontent.com/u/999073?v=4\u0026amp;s=200\" width=\"120\" height=\"120\" media=\"(min-width: 768px)\"/\u003e\n\t\t\t\t\t\u003cimg src=\"https://avatars.githubusercontent.com/u/999073?v=4\u0026amp;s=200\" alt=\"Hendrik van Antwerpen\" width=\"80\" height=\"80\" loading=\"lazy\" decoding=\"async\"/\u003e\n\t\t\t\t\u003c/picture\u003e\n\t\t\t\u003c/div\u003e\n\t\t\t\t\n\t\t\t\t\t\u003cp\u003eSenior Software Engineering, GitHub\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003c/article\u003e\n\t\t\t\u003carticle\u003e\n\t\u003cdiv\u003e\n\t\t\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cpicture\u003e\n\t\t\t\t\t\u003csource srcset=\"https://avatars.githubusercontent.com/u/7701635?v=4\u0026amp;s=200\" width=\"120\" height=\"120\" media=\"(min-width: 768px)\"/\u003e\n\t\t\t\t\t\u003cimg src=\"https://avatars.githubusercontent.com/u/7701635?v=4\u0026amp;s=200\" alt=\"Alexander Neubeck\" width=\"80\" height=\"80\" loading=\"lazy\" decoding=\"async\"/\u003e\n\t\t\t\t\u003c/picture\u003e\n\t\t\t\u003c/div\u003e\n\t\t\t\t\n\t\t\t\t\t\u003cp\u003ePrincipal Software Engineer, GitHub\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003c/article\u003e\n\t\u003c/div\u003e\n\u003c/section\u003e\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tExplore more from GitHub\t\u003c/h2\u003e\n\t\u003cdiv\u003e\n\t\t\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon-Circle.svg\" width=\"44\" height=\"44\" alt=\"Docs\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tDocs\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eEverything you need to master GitHub, all in one place.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Go to Docs; ref_location:bottom recirculation;\" href=\"https://docs.github.com/\" target=\"_blank\" aria-label=\"Go to Docs\"\u003e\n\t\t\t\t\tGo to Docs\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_95220f.svg\" width=\"44\" height=\"44\" alt=\"GitHub\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tGitHub\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eBuild what’s next on GitHub, the place for anyone from anywhere to build anything.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Start building; ref_location:bottom recirculation;\" href=\"https://github.blog/developer-skills/github/\" target=\"_blank\" aria-label=\"Start building\"\u003e\n\t\t\t\t\tStart building\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" fill=\"none\"\u003e\u003cpath fill=\"currentColor\" d=\"M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z\"\u003e\u003c/path\u003e\u003cpath stroke=\"currentColor\" d=\"M1.75 8H11\" stroke-width=\"1.5\" stroke-linecap=\"round\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_da43dc.svg\" width=\"44\" height=\"44\" alt=\"Customer stories\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tCustomer stories\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eMeet the companies and engineering teams that build with GitHub.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Learn more; ref_location:bottom recirculation;\" href=\"https://github.com/customer-stories\" target=\"_blank\" aria-label=\"Learn more\"\u003e\n\t\t\t\t\tLearn more\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2022/05/careers.svg\" width=\"44\" height=\"44\" alt=\"Work at GitHub!\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tWork at GitHub!\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eCheck out our current job openings.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Apply now; ref_location:bottom recirculation;\" href=\"https://www.github.careers/careers-home\" target=\"_blank\" aria-label=\"Apply now\"\u003e\n\t\t\t\t\tApply now\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\t\u003c/div\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2024-12-12T13:51:13Z",
  "modifiedTime": null
}
