{
  "id": "f553d6aa-dcfb-4d2c-ae39-156fcd9dddd3",
  "title": "Updated production-ready Gemini models, reduced 1.5 Pro pricing, increased rate limits, and more",
  "link": "https://developers.googleblog.com/en/updated-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/",
  "description": "Learn about the latest updates to Google's Gemini models, including reduced pricing for Gemini 1.5 Pro, increased rate limits, faster performance, enhanced quality, and more.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Logan Kilpatrick, Shrestha Basu Mallick",
  "length": 5384,
  "excerpt": "Learn about the latest updates to Google's Gemini models, including reduced pricing for Gemini 1.5 Pro, increased rate limits, faster performance, enhanced quality, and more.",
  "siteName": "",
  "favicon": "",
  "text": "Logan Kilpatrick Senior Product Manager Gemini API and Google AI Studio Today, we’re releasing two updated production-ready Gemini models: Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002 along with:\u003e50% reduced price on 1.5 Pro (both input and output for prompts \u003c128K)2x higher rate limits on 1.5 Flash and ~3x higher on 1.5 Pro2x faster output and 3x lower latencyUpdated default filter settingsThese new models build on our latest experimental model releases and include meaningful improvements to the Gemini 1.5 models released at Google I/O in May. Developers can access our latest models for free via Google AI Studio and the Gemini API. For larger organizations and Google Cloud customers, the models are also available on Vertex AI.Improved overall quality, with larger gains in math, long context, and visionThe Gemini 1.5 series are models that are designed for general performance across a wide range of text, code, and multimodal tasks. For example, Gemini models can be used to synthesize information from 1000 page PDFs, answer questions about repos containing more than 10 thousand lines of code, take in hour long videos and create useful content from them, and more.With the latest updates, 1.5 Pro and Flash are now better, faster, and more cost-efficient to build with in production. We see a ~7% increase in MMLU-Pro, a more challenging version of the popular MMLU benchmark. On MATH and HiddenMath (an internal holdout set of competition math problems) benchmarks, both models have made a considerable ~20% improvement. For vision and code use cases, both models also perform better (ranging from ~2-7%) across evals measuring visual understanding and Python code generation. We also improved the overall helpfulness of model responses, while continuing to uphold our content safety policies and standards. This means less punting/fewer refusals and more helpful responses across many topics.Both models now have a more concise style in response to developer feedback which is intended to make these models easier to use and reduce costs. For use cases like summarization, question answering, and extraction, the default output length of the updated models is ~5-20% shorter than previous models. For chat-based products where users might prefer longer responses by default, you can read our prompting strategies guide to learn more about how to make the models more verbose and conversational.For more details on migrating to the latest versions of Gemini 1.5 Pro and 1.5 Flash, check out the Gemini API models page.Gemini 1.5 ProWe continue to be blown away with the creative and useful applications of Gemini 1.5 Pro’s 2 million token long context window and multimodal capabilities. From video understanding to processing 1000 page PDFs, there are so many new use cases still to be built. Today we are announcing a 64% price reduction on input tokens, a 52% price reduction on output tokens, and a 64% price reduction on incremental cached tokens for our strongest 1.5 series model, Gemini 1.5 Pro, effective October 1st, 2024, on prompts less than 128K tokens. Coupled with context caching, this continues to drive the cost of building with Gemini down. Increased rate limitsTo make it even easier for developers to build with Gemini, we are increasing the paid tier rate limits for 1.5 Flash to 2,000 RPM and increasing 1.5 Pro to 1,000 RPM, up from 1,000 and 360, respectively. In the coming weeks, we expect to continue to increase the Gemini API rate limits so developers can build more with Gemini.2x faster output and 3x less latencyAlong with core improvements to our latest models, over the last few weeks we have driven down the latency with 1.5 Flash and significantly increased the output tokens per second, enabling new use cases with our most powerful models. Updated filter settingsSince the first launch of Gemini in December of 2023, building a safe and reliable model has been a key focus. With the latest versions of Gemini (-002 models), we’ve made improvements to the model's ability to follow user instructions while balancing safety. We will continue to offer a suite of safety filters that developers may apply to Google’s models. For the models released today, the filters will not be applied by default so that developers can determine the configuration best suited for their use case.Gemini 1.5 Flash-8B Experimental updatesWe are releasing a further improved version of the Gemini 1.5 model we announced in August called “Gemini-1.5-Flash-8B-Exp-0924.” This improved version includes significant performance increases across both text and multimodal use cases. It is available now via Google AI Studio and the Gemini API.The overwhelmingly positive feedback developers have shared about 1.5 Flash-8B has been incredible to see, and we will continue to shape our experimental to production release pipeline based on developer feedback.We're excited about these updates and can't wait to see what you'll build with the new Gemini models! And for Gemini Advanced users, you will soon be able to access a chat optimized version of Gemini 1.5 Pro-002.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemini-15-Flash-Social_1.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \u003csection\u003e\n      \n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=Logan+Kilpatrick\"\u003eLogan Kilpatrick\u003c/a\u003e\n            \n              \u003cspan\u003eSenior Product Manager\u003c/span\u003e\n            \n            \n              \u003cspan\u003eGemini API and Google AI Studio\u003c/span\u003e\n            \n          \u003c/p\u003e\n        \n          \n        \n\n      \n      \u003c/section\u003e\n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"hf05d\"\u003eToday, we’re releasing two updated production-ready Gemini models: \u003cb\u003e\u003ci\u003eGemini-1.5-Pro-002\u003c/i\u003e\u003c/b\u003e and \u003cb\u003e\u003ci\u003eGemini-1.5-Flash-002\u003c/i\u003e\u003c/b\u003e along with:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"36s1\"\u003e\u0026gt;50% reduced price on 1.5 Pro (both input and output for prompts \u0026lt;128K)\u003c/li\u003e\u003cli data-block-key=\"1uea\"\u003e2x higher rate limits on 1.5 Flash and ~3x higher on 1.5 Pro\u003c/li\u003e\u003cli data-block-key=\"o65m\"\u003e2x faster output and 3x lower latency\u003c/li\u003e\u003cli data-block-key=\"12oqm\"\u003eUpdated default filter settings\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"694mg\"\u003eThese new models build on our latest experimental model releases and include meaningful improvements to the Gemini 1.5 models released at Google I/O in May. Developers can access our latest models for free via \u003ca href=\"https://aistudio.google.com/app/prompts/new_chat?model=gemini-1.5-pro-002\"\u003eGoogle AI Studio\u003c/a\u003e and the \u003ca href=\"https://ai.google.dev/gemini-api/docs/models/gemini\"\u003eGemini API\u003c/a\u003e. For larger organizations and Google Cloud customers, the models are also available on \u003ca href=\"https://cloud.google.com/vertex-ai\"\u003eVertex AI\u003c/a\u003e.\u003c/p\u003e\u003ch3 data-block-key=\"7ehg\"\u003e\u003cb\u003e\u003cbr/\u003eImproved overall quality, with larger gains in math, long context, and vision\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"6afk8\"\u003eThe Gemini 1.5 series are models that are designed for general performance across a wide range of text, code, and multimodal tasks. For example, Gemini models can be used to synthesize information from 1000 page PDFs, answer questions about repos containing more than 10 thousand lines of code, take in hour long videos and create useful content from them, and more.\u003c/p\u003e\u003cp data-block-key=\"5f8nb\"\u003eWith the latest updates, 1.5 Pro and Flash are now better, faster, and more cost-efficient to build with in production. We see a ~7% increase in MMLU-Pro, a more challenging version of the popular MMLU benchmark. On MATH and HiddenMath (an internal holdout set of competition math problems) benchmarks, both models have made a considerable ~20% improvement. For vision and code use cases, both models also perform better (ranging from ~2-7%) across evals measuring visual understanding and Python code generation.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_jBYRI1Z.original.png\" alt=\"A table showcasing benchmark data, demonstrating improved performance for the latest Gemini models, Gemini 1.5 Pro and Gemini 1.5 Flash. The table highlights advancements in various capabilities including reasoning, code, and math\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"hf05d\"\u003eWe also improved the overall helpfulness of model responses, while continuing to uphold our content safety policies and standards. This means less punting/fewer refusals and more helpful responses across many topics.\u003c/p\u003e\u003cp data-block-key=\"2peu8\"\u003eBoth models now have a more concise style in response to developer feedback which is intended to make these models easier to use and reduce costs. For use cases like summarization, question answering, and extraction, the default output length of the updated models is ~5-20% shorter than previous models. For chat-based products where users might prefer longer responses by default, you can read our \u003ca href=\"https://ai.google.dev/gemini-api/docs/prompting-strategies#define-the-format-of-the-response\"\u003eprompting strategies guide\u003c/a\u003e to learn more about how to make the models more verbose and conversational.\u003c/p\u003e\u003cp data-block-key=\"be8d5\"\u003eFor more details on migrating to the latest versions of Gemini 1.5 Pro and 1.5 Flash, check out the \u003ca href=\"https://ai.google.dev/gemini-api/docs/models/gemini\"\u003eGemini API models page\u003c/a\u003e.\u003c/p\u003e\u003ch3 data-block-key=\"5c9ku\"\u003e\u003cb\u003e\u003cbr/\u003eGemini 1.5 Pro\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"1hich\"\u003eWe continue to be blown away with the creative and useful applications of Gemini 1.5 Pro’s 2 million token \u003ca href=\"https://ai.google.dev/gemini-api/docs/long-context\"\u003elong context window\u003c/a\u003e and multimodal capabilities. From video understanding to \u003ca href=\"https://ai.google.dev/gemini-api/docs/document-processing\"\u003eprocessing 1000 page PDFs\u003c/a\u003e, there are so many new use cases still to be built. Today we are announcing a 64% price reduction on input tokens, a 52% price reduction on output tokens, and a 64% price reduction on incremental cached tokens for our strongest 1.5 series model, Gemini 1.5 Pro, \u003ca href=\"https://ai.google.dev/pricing\"\u003eeffective October 1st, 2024\u003c/a\u003e, on prompts less than 128K tokens. Coupled with \u003ca href=\"https://ai.google.dev/gemini-api/docs/caching\"\u003econtext caching\u003c/a\u003e, this continues to drive the cost of building with Gemini down.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemini_Pro_Price_Chart_GRHV7Tk.original.png\" alt=\"A pricing table for the Gemini 1.5 Flash model, outlining the cost per one million tokens for input and output\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"hf05d\"\u003e\u003cb\u003eIncreased rate limits\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"5lvio\"\u003eTo make it even easier for developers to build with Gemini, we are increasing the paid tier rate limits for 1.5 Flash to 2,000 RPM and increasing 1.5 Pro to 1,000 RPM, up from 1,000 and 360, respectively. In the coming weeks, we expect to continue to increase the \u003ca href=\"https://ai.google.dev/pricing\"\u003eGemini API rate limits\u003c/a\u003e so developers can build more with Gemini.\u003c/p\u003e\u003ch3 data-block-key=\"20apm\"\u003e\u003cb\u003e\u003cbr/\u003e2x faster output and 3x less latency\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"5l45o\"\u003eAlong with core improvements to our latest models, over the last few weeks we have driven down the latency with 1.5 Flash and significantly increased the output tokens per second, enabling new use cases with our most powerful models.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_HthRi7g.original.png\" alt=\"Side-by-side graphs charting the latency of Google\u0026#39;s Gemini model over time, showing improvements.\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"hf05d\"\u003e\u003cb\u003eUpdated filter settings\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"68n0j\"\u003eSince the first launch of Gemini in December of 2023, \u003ca href=\"https://ai.google.dev/gemini-api/docs/safety-guidance\"\u003ebuilding a safe\u003c/a\u003e and reliable model has been a key focus. With the latest versions of Gemini (-002 models), we’ve made improvements to the model\u0026#39;s ability to follow user instructions while balancing safety. We will continue to offer a suite of \u003ca href=\"https://ai.google.dev/gemini-api/docs/safety-settings\"\u003esafety filters\u003c/a\u003e that developers may apply to Google’s models. For the models released today, the filters will not be applied by default so that developers can determine the configuration best suited for their use case.\u003c/p\u003e\u003ch3 data-block-key=\"cncp4\"\u003e\u003cb\u003e\u003cbr/\u003eGemini 1.5 Flash-8B Experimental updates\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"epr5n\"\u003eWe are releasing a further improved version of the Gemini 1.5 model we announced in August called “Gemini-1.5-Flash-8B-Exp-0924.” This improved version includes significant performance increases across both text and multimodal use cases. It is available now via Google AI Studio and the Gemini API.\u003c/p\u003e\u003cp data-block-key=\"4md6d\"\u003eThe overwhelmingly positive feedback developers have shared about 1.5 Flash-8B has been incredible to see, and we will continue to shape our experimental to production release pipeline based on developer feedback.\u003c/p\u003e\u003cp data-block-key=\"93g11\"\u003e\u003cbr/\u003eWe\u0026#39;re excited about these updates and can\u0026#39;t wait to see what you\u0026#39;ll build with the new Gemini models! And for \u003ca href=\"https://gemini.google/advanced/\"\u003eGemini Advanced\u003c/a\u003e users, you will soon be able to access a chat optimized version of Gemini 1.5 Pro-002.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-09-24T00:00:00Z",
  "modifiedTime": null
}
