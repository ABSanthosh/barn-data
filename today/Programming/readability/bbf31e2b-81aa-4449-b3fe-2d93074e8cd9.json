{
  "id": "bbf31e2b-81aa-4449-b3fe-2d93074e8cd9",
  "title": "Gemini 2.0 Family Expands with Cost-Efficient Flash-Lite and Pro-Experimental Models",
  "link": "https://www.infoq.com/news/2025/02/gemini-2-flash-lite-pro-models/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Announced last December, the Gemini 2.0 family of models now has a new member, Gemini 2.0 Flash-Lite, which Google says is cost-optimized for large scale text output use cases and is now available in preview. Along with Flash-Lite, Google also announced Gemini 2.0 Pro. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Thu, 13 Feb 2025 11:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "ChatGPT",
    "GPT-4",
    "Gemini",
    "Large language models",
    "Google",
    "AI, ML \u0026 Data Engineering",
    "Development",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3764,
  "excerpt": "Announced last December, the Gemini 2.0 family of models now has a new member, Gemini 2.0 Flash-Lite, which Google says is cost-optimized for large scale text output use cases and is now available in",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250213201535/apple-touch-icon.png",
  "text": "Announced last December, the Gemini 2.0 family of models now has a new member, Gemini 2.0 Flash-Lite, which Google says is cost-optimized for large scale text output use cases and is now available in preview. Along with Flash-Lite, Google also announced Gemini 2.0 Pro. Gemini 2.0 Flash-Lite is a new model with the same speed and cost as 1.5 Flash while providing better quality, says Google, with the same 1 million context window as 2.0 Flash. Compared with 2.0 Flash, 2.0 Flash-Lite does not support image or audio output. Additionally, it does not support \"search as a tool\" or \"code execution as a tool\", two \"grounding\" techniques aimed at improving the model's answers by using Google Search or code execution as a tool to help check their correctness. As a final limitation, 2.0 Flash-Lite cannot be used via the Multimodal Live API, which aims to enable natural, human-like voice conversation through low-latency bidirectional voice and audio interactions. On the performance side, 2.0 Flash-Lite significantly beats 1.5 Flash on the SimpleQA benchmark, which tests factual world knowledge, and BirdSQL, which evaluates natural language conversion to SQL. However, it performs slightly worse than 1.5 Flash on a few benchmarks, including MRCR, which evaluates long-context understanding, and LiveCodeBench, which tests Python coding. Interestingly, moreover, 2.0 Flash-Lite is comparable to or better than 1.5 Pro on several benchmarks, including Bird-SQL, FACTS Grounding, MATH, and MMMU. Along with 2.0 Flash and 2.0 Flash-Lite, Google has also released 2.0 Pro as an experimental model. According to Google, 2.0 Pro is their best model yet for coding performance and complex prompts. Indeed, 2.0 Pro turns out to be the best Gemini model to date on most benchmarks, particularly on the SimpleQA benchmark, where it improves the second-best, 2.0 Flash, by 50%. Exceptions are the Facts benchmark, where 2.0 Flahs excels, and Long-context, where 1.5 Pro performs slightly better. Being still experimental, 2.0 Pro's results could change before its general availability. As a final note about Gemini 2.0 models, Google recently released Gemini 2.0 Flash Thinking in experimental mode. This model follows the recent AI-reasoning models trend that aims to create models able to break down a prompt into a sequence of smaller tasks and devise a strategy to solve them individually by taking into account their relationships, while also being able to explain their \"thought\" process. Google's announcement raised some negative comments on Reddit for the \"pretty mediocre\" improvement the new models provide in comparison with Gemini 1.5. While this is uncontroversially true when comparing the models based on benchmarks, the actual performance they provide in real use cases is significantly better according to others. Additionally, the 1- and 2-million token context windows provided by Gemini 2.0 Flash and Pro led some Hacker News users to posit that they could make RAG-techniques superfluous in many use cases. On the positive side, Gemini 2.0 models seem to deliver on their promise to handle such large context, but cost concerns and decreasing performance with context length might still suggest using RAG instead. Slightly tongue in cheek, according to ChatGPT at the time of this writing, Gemini 2.0 model and GPT-4 are comparable in performance, with Gemini 2.0 leading in text-based understanding, code generation, and multimodal integration and GPT-4 maintaining an edge in commonsense reasoning tasks. Developers can use all Gemini 2.0 models in Google AI Studio and Vertex AI. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/02/gemini-2-flash-lite-pro-models/en/headerimage/gemini-2-flash-lite-pro-models-1739442130920.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://www.infoq.com/news/2024/12/google-gemini-2/\"\u003eAnnounced last December\u003c/a\u003e, the Gemini 2.0 family of models now has a new member, Gemini 2.0 Flash-Lite, which Google says is cost-optimized for large scale text output use cases and is now available in preview. Along with Flash-Lite, Google also announced Gemini 2.0 Pro.\u003c/p\u003e\n\n\u003cp\u003eGemini 2.0 Flash-Lite is a new model with the same speed and cost as 1.5 Flash while providing better quality, says Google, with the same 1 million context window as 2.0 Flash. Compared with 2.0 Flash, 2.0 Flash-Lite does not support image or audio output. Additionally, it does not support \u0026#34;\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/grounding-with-search\"\u003esearch as a tool\u003c/a\u003e\u0026#34; or \u0026#34;\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution\"\u003ecode execution as a tool\u003c/a\u003e\u0026#34;, two \u0026#34;grounding\u0026#34; techniques aimed at improving the model\u0026#39;s answers by using Google Search or code execution as a tool to help check their correctness. As a final limitation, 2.0 Flash-Lite cannot be used via the \u003ca href=\"https://ai.google.dev/gemini-api/docs/multimodal-live\"\u003eMultimodal Live API\u003c/a\u003e, which aims to enable natural, human-like voice conversation through low-latency bidirectional voice and audio interactions.\u003c/p\u003e\n\n\u003cp\u003eOn the performance side, 2.0 Flash-Lite significantly beats 1.5 Flash on the SimpleQA benchmark, which tests factual world knowledge, and BirdSQL, which evaluates natural language conversion to SQL. However, it performs slightly worse than 1.5 Flash on a few benchmarks, including MRCR, which evaluates long-context understanding, and LiveCodeBench, which tests Python coding. Interestingly, moreover, 2.0 Flash-Lite is comparable to or better than 1.5 Pro on several benchmarks, including Bird-SQL, FACTS Grounding, MATH, and MMMU.\u003c/p\u003e\n\n\u003cp\u003eAlong with 2.0 Flash and 2.0 Flash-Lite, Google has also released \u003ca href=\"https://blog.google/feed/gemini-exp-1206/\"\u003e2.0 Pro as an experimental model\u003c/a\u003e. According to Google, 2.0 Pro is their best model yet for coding performance and complex prompts. Indeed, 2.0 Pro turns out to be the best Gemini model to date on most benchmarks, particularly on the SimpleQA benchmark, where it improves the second-best, 2.0 Flash, by 50%. Exceptions are the Facts benchmark, where 2.0 Flahs excels, and Long-context, where 1.5 Pro performs slightly better. Being still experimental, 2.0 Pro\u0026#39;s results could change before its general availability.\u003c/p\u003e\n\n\u003cp\u003eAs a final note about Gemini 2.0 models, Google recently released \u003ca href=\"https://www.infoq.com/news/2025/01/google-deepmind-gemini/\"\u003eGemini 2.0 Flash Thinking\u003c/a\u003e in experimental mode. This model follows the recent \u003ca href=\"https://www.infoq.com/news/2025/02/deepseek-r1-release/\"\u003eAI-reasoning models trend\u003c/a\u003e that aims to create models able to break down a prompt into a sequence of smaller tasks and devise a strategy to solve them individually by taking into account their relationships, while also being able to explain their \u0026#34;thought\u0026#34; process.\u003c/p\u003e\n\n\u003cp\u003eGoogle\u0026#39;s announcement \u003ca href=\"https://www.reddit.com/r/singularity/comments/1iidnkq/google_officially_releases_gemini_2_pro_and_two/\"\u003eraised some negative comments on Reddit\u003c/a\u003e for the \u0026#34;pretty mediocre\u0026#34; improvement the new models provide in comparison with Gemini 1.5. While this is uncontroversially true when comparing the models based on benchmarks, the \u003ca href=\"https://www.reddit.com/r/OpenAI/comments/1hd2r2b/gemini_20_is_what_4o_was_supposed_to_be/\"\u003eactual performance they provide in real use cases is significantly better\u003c/a\u003e according to others.\u003c/p\u003e\n\n\u003cp\u003eAdditionally, the 1- and 2-million token context windows provided by Gemini 2.0 Flash and Pro led some Hacker News users to posit that they could make \u003ca href=\"https://news.ycombinator.com/item?id=42952286\"\u003eRAG-techniques superfluous in many use cases\u003c/a\u003e. On the positive side, Gemini 2.0 models seem to \u003ca href=\"https://news.ycombinator.com/item?id=42952569\"\u003edeliver on their promise to handle such large context\u003c/a\u003e, but \u003ca href=\"https://news.ycombinator.com/item?id=42952569\"\u003ecost concerns and decreasing performance with context length\u003c/a\u003e might still suggest using RAG instead.\u003c/p\u003e\n\n\u003cp\u003eSlightly tongue in cheek, according to ChatGPT at the time of this writing, Gemini 2.0 model and GPT-4 are comparable in performance, with Gemini 2.0 leading in text-based understanding, code generation, and multimodal integration and GPT-4 maintaining an edge in commonsense reasoning tasks.\u003c/p\u003e\n\n\u003cp\u003eDevelopers can use all Gemini 2.0 models in Google AI Studio and Vertex AI.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-02-13T00:00:00Z",
  "modifiedTime": null
}
