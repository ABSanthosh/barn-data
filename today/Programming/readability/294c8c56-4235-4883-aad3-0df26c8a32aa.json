{
  "id": "294c8c56-4235-4883-aad3-0df26c8a32aa",
  "title": "Airbnb Brandometer: Powering Brand Perception Measurement on Social Media Data with AI",
  "link": "https://medium.com/airbnb-engineering/airbnb-brandometer-powering-brand-perception-measurement-on-social-media-data-with-ai-c83019408051?source=rss----53c7c27702d5---4",
  "description": "",
  "author": "Tiantian Zhang",
  "published": "Fri, 26 Apr 2024 16:01:16 GMT",
  "source": "https://medium.com/feed/airbnb-engineering",
  "categories": [
    "machine-learning",
    "deep-learning",
    "ai",
    "engineering",
    "data-science"
  ],
  "byline": "Tiantian Zhang",
  "length": 11648,
  "excerpt": "At Airbnb, we have developed Brandometer, a state-of-the-art natural language understanding (NLU) technique for understanding brand perception based on social media data. Brand perception refers to…",
  "siteName": "The Airbnb Tech Blog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "How we quantify brand perceptions from social media platforms through deep learningBy Tiantian Zhang, Shuai Shao (Shawn)IntroductionAt Airbnb, we have developed Brandometer, a state-of-the-art natural language understanding (NLU) technique for understanding brand perception based on social media data.Brand perception refers to the general feelings and experiences of customers with a company. Quantitatively, measuring brand perception is an extremely challenging task. Traditionally, we rely on customer surveys to find out what customers think about a company. The downsides of such a qualitative study is the bias in sampling and the limitation in data scale. Social media data, on the other hand, is the largest consumer database where users share their experiences and is the ideal complementary consumer data to capture brand perceptions.Compared to traditional approaches to extract concurrency and count-based top relevant topics, Brandometer learns word embeddings and utilizes embedding distances to measure relatedness of brand perceptions (e.g., ‘belonging’, ‘connected’, ‘reliable’). Word embedding represents words in the form of real-valued vectors, and it performs well in reserving semantic meanings and relatedness of words. Word embeddings obtained from deep neural networks are arguably the most popular and evolutionary approaches in NLU. We explored a variety of word embedding models, from quintessential algorithms Word2Vec and FastText, to the latest language model DeBERTa, and compared them in terms of generating reliable brand perception scores.For concepts represented as words, we use similarity between its embedding and that of “Airbnb” to measure how important the concept is with respect to the Airbnb brand, which is named as Perception Score. Brand Perception is defined as Cosine Similarity between Airbnb and the specific keyword:whereEq. 1In this blog post, we will introduce how we process and understand social media data, capture brand perceptions via deep learning and how to ‘convert’ the cosine similarities to calibrated Brandometer metrics. We will also share the insights derived from Brandometer metrics.Brandometer MethodologyProblem Setup and DataIn order to measure brand perception on social media, we assessedall Airbnb related mentions from 19 platforms (e.g., X — formerly known as Twitter, Facebook, Reddit, etc) and generated word embeddings with state-of-the-art models.In order to use Social media data to generate meaningful word embeddings for the purpose of measuring brand perception, we conquered two challenges:Quality: Social media posts are mostly user-generated with varying content such as status sharing and reviews, and can be very noisy.Quantity: Social media post sparsity is another challenge. Considering that it typically requires some time for social media users to generate data in response to certain activities and events, a monthly rolling window maintains a good balance of promptness and detectability. Our monthly dataset is relatively small (around 20 million words) as compared to a typical dataset used to train good quality word embeddings (e.g., about 100 billion words for Google News Word2Vec model). Warm-start from pre-trained models didn’t help since the in-domain data barely moved the learned embeddings.We developed multiple data cleaning processes to improve data quality. At the same time, we innovated the modeling techniques to mitigate the impact on word embedding quality due to data quantity and quality.In addition to data, we explored and compared multiple word embedding training techniques with the goal to generate reliable brand perception scores.Word2VecWord2Vec is by far the simplest and most widely used word embedding model since 2013. We started with building CBOW-based Word2Vec models using Gensim. Word2Vec produced decent in-domain word embeddings, and more importantly, the concept of analogies. In our domain-specific word embeddings, we are able to capture analogies in the Airbnb domain, such as “host” — “provide” + “guest” ~= “need”, “city” — “mall” + “nature” ~= “park”.FastTextFastText takes into account the internal structure of words, and is more robust to out-of-vocabulary words and smaller datasets. Moreover, as inspired by Sense2Vec, we associate words with sentiments (i.e., POSITIVE, NEGATIVE, NEUTRAL), which forms brand perception concepts on the sentiment levels.DeBERTaRecent progress in transformer-based language models (e.g., BERT) has significantly improved the performance of NLU tasks with the advantage of generating contextualized word embeddings. We developed DeBERTa based word embeddings, which works better with smaller dataset and pays more attention to surrounding context via disentangled attention mechanisms. We trained everything from scratch (including tokenizer) using Transformers, and the concatenated last attention layer embeddings resulted in the best word embeddings for our case.Brand Perception Score Stabilization and CalibrationThe variability of word embeddings has been widely studied (Borah, 2021). The causes range from the underlying stochastic nature of deep learning models (e.g., random initialization of word embeddings, embedding training which leads to local optimum for global optimization criteria) to the quantity and quality changes of data corpus across time.With Brandometer, we need to reduce the variability in embedding distances to generate stable time series tracking. Stable embedding distances helped preserve the inherent patterns and structures present in the time series data, and hence it contributes to better predictability of the tracking process. Additionally, it made the tracking process more robust to noisy fluctuations. We studied the influential factors and took the following steps to reduce:Score averaging over repetitive training with bootstrap samplingRank-based perception scoreScore averaging over repetitive training with upsamplingFor each month’s data, we trained N models with the same hyper-parameters, and took the average of N perception scores as the final score for each concept. Meanwhile, we did upsampling to make sure that each model iterated on an equal number of data points across months.We defined variability as:Eq.2whereCosSim(w) refers to the cosine similarity based perception score defined in Eq. 1, A refers to the algorithm, M refers to the time window (i.e. month), V refers to the vocabulary and |V| is the vocabulary size, and n refers to the number of repetitively trained models.As N approaches 30, the score variability values converge and settle within a narrow interval. Hence, we picked N = 30 for all.Figure 1. Score variability changes of the FastText models across months in a year with increasing N.Rank-based perception scoreBased on Maria Antoniak’s work, we used the overlap between nearest neighbors to measure the stability of word embeddings, since the relative distances matter more than the absolute distance values in downstream tasks. Therefore, we also developed rank-based scores, which shows greater stability as compared to similarity-based scores.For each word, we first ranked them in descending order of cosine similarity via Eq. 1. The rank-based similarity score is then computed as 1/rank(w) where w∈V. More relevant concepts will have higher rank-based perception scores.The score variability is defined the same as Variability(A, M, V) in Eq. 2 except thatwhere RankSim(w) refers to the rank based perception score. With rank-based scores, when N approaches to 30, the score variability values converge to a much narrower interval especially for DeBERTa.Figure 2. Rank-based score variability changes of the FastText models across months in 2020 with increasing N.Selection of Score Output by Designed MetricsOne challenge of this project was that we didn’t have a simple and ultimate way to conclude which score output was better since there is no objective ‘truth’ of brand perception. Instead, we defined a new metric to learn some characteristics of the score.Average Variance Across Different Period (AVADP)We first picked the group of top relevant brand perceptions for Airbnb: ‘host,’ ‘vacation,’ ‘rental,’ ‘love,’ ‘stay,’ ‘home,’ ‘booking,’ ‘travel,’ ‘guest’.Higher value indicates more fluctuations across different periods — likely a bad thing, because the selected brand perception is assumed to be relatively stable and hence should not vary too much month by month.i ∈ (1,n), n as number of selected top perceptions, T as number of periodsWe checked these statistics on the calibrated results as shown above. We can see that the ranked-based score is the winner as compared to similarity-based scores:Lower AVADP: More fluctuations than the non-ranked across a different period — likely a good thing, because the selected brand perception is assumed to be relatively stable and hence should not vary too much month by month.Use Cases of BrandometerThough we set out to solve the problem of brand measurement, we believe use cases can go above and beyond:Use Cases Deep DiveIndustry Analysis: Top Brand Perception among Key Players [Monthly Top Perception]With top perceptions such as “Stay” and “Home,” Airbnb provides a brand image of “belonging”, echoing our mission statement and unique supply inventory, while other companies have “Rental,” “Room,” “Booking,” a description of functionality, not human sensation.Top Emerging Perception reveals major events discussed online [Monthly Top Perception]The Top 10 Perceptions are generally stable month to month. The top standing perceptions includeHome, Host, Stay, Travel, Guest, Rental, etc.Meanwhile, we use Brandometer to monitor emerging perceptions that jump to the top list, which may reflect major events associated with the brand or user preference changes.Major Campaign Monitor (Time Series Tracking)Businesses create campaigns to promote products and expand the brand image. We were able to capture a perception change on one specific Brand Theme after a related campaign.Figure 3. Example of brand perception change due to a campaign. Scores shown here are based on Calibrated Rank-based Score.These use cases are just the beginning. Essentially, this is an innovative way of gathering massive online input as we learn the needs and perception of the community. We will constantly reflect on how we leverage these insights to continually improve the Airbnb experience for our community.Next StepsAirbnb’s innovative Brandometer has already demonstrated success in capturing brand perception from social media data. There are several directions for future improvement:Better content segmentation for clearer and more concise insights.Develop more metrics reflecting social media brand perception.Enhance data foundation, not just Airbnb, but other companies in the same market segment to get more comprehensive insights.If this kind of work sounds appealing to you, check out our open roles — we’re hiring!AcknowledgmentsThanks to Mia Zhao, Bo Zeng, Cassie Cao for contributing the best ideas on improving and landing Airbnb Brandometer. Thanks to Jon Young, Narin Leininger, Allison Frelinger for the support of social media data consolidation. Thanks to Linsha Chen, Sam Barrows, Hannah Jeton, and Irina Azu who provide feedback and suggestions. Thanks to Lianghao Li, Kelvin Xiong, Nathan Triplett, Joy Zhang, Andy Yasutake for reviewing and polishing the blog post content and all the great suggestions. Thank Joy Zhang, Tina Su, Andy Yasutake for leadership support!Special thanks to Joy Zhang, who initiated the idea, for all the inspiring conversations, continuous guidance and support!",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*VWesH2nPt7K2bRIu",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca rel=\"noopener follow\" href=\"https://medium.com/@watera427_75688?source=post_page-----c83019408051--------------------------------\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Tiantian Zhang\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*YiHDr4-mtrA20R1PUJCVMg.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://medium.com/airbnb-engineering?source=post_page-----c83019408051--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"The Airbnb Tech Blog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*MlNQKg-sieBGW5prWoe9HQ.jpeg\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"f188\"\u003e\u003cstrong\u003eHow we quantify brand perceptions from social media platforms through deep learning\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"306e\"\u003eBy \u003cspan\u003e\u003cspan aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/u/09e8a5894d7f?source=post_page-----c83019408051--------------------------------\" rel=\"noopener\" target=\"_blank\"\u003eTiantian Zhang\u003c/a\u003e\u003c/span\u003e\u003c/span\u003e, \u003cspan\u003e\u003cspan aria-hidden=\"false\"\u003e\u003ca href=\"https://medium.com/u/865e093fb5f3?source=post_page-----c83019408051--------------------------------\" rel=\"noopener\" target=\"_blank\"\u003eShuai Shao (Shawn)\u003c/a\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\u003ch2 id=\"25f5\"\u003eIntroduction\u003c/h2\u003e\u003cp id=\"5461\"\u003eAt Airbnb, we have developed Brandometer, a state-of-the-art natural language understanding (NLU) technique for understanding brand perception based on social media data.\u003c/p\u003e\u003cp id=\"bab0\"\u003eBrand perception refers to the general feelings and experiences of customers with a company. Quantitatively, measuring brand perception is an extremely challenging task. Traditionally, we rely on customer surveys to find out what customers think about a company. The downsides of such a qualitative study is the bias in sampling and the limitation in data scale. Social media data, on the other hand, is the largest consumer database where users share their experiences and is the ideal complementary consumer data to capture brand perceptions.\u003c/p\u003e\u003cp id=\"bb82\"\u003eCompared to traditional approaches to extract concurrency and count-based top relevant topics, Brandometer learns \u003ca href=\"https://en.wikipedia.org/wiki/Word_embedding\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eword embeddings\u003c/a\u003e and utilizes embedding distances to measure relatedness of brand perceptions (e.g., ‘belonging’, ‘connected’, ‘reliable’). Word embedding represents words in the form of real-valued vectors, and it performs well in reserving semantic meanings and relatedness of words. Word embeddings obtained from deep neural networks are arguably the most popular and evolutionary approaches in NLU. We explored a variety of word embedding models, from quintessential algorithms \u003ca href=\"https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eWord2Vec\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/1607.04606\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFastText\u003c/a\u003e, to the latest language model \u003ca href=\"https://arxiv.org/abs/2006.03654\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDeBERTa\u003c/a\u003e, and compared them in terms of generating reliable brand perception scores.\u003c/p\u003e\u003cp id=\"02aa\"\u003eFor concepts represented as words, we use similarity between its embedding and that of “Airbnb” to measure how important the concept is with respect to the Airbnb brand, which is named as Perception Score. Brand Perception is defined as \u003ca href=\"https://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCosine Similarity\u003c/a\u003e between Airbnb and the specific keyword:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"d97d\"\u003ewhere\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eEq. 1\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"bb91\"\u003eIn this blog post, we will introduce how we process and understand social media data, capture brand perceptions via deep learning and how to ‘convert’ the cosine similarities to calibrated Brandometer metrics. We will also share the insights derived from Brandometer metrics.\u003c/p\u003e\u003ch2 id=\"e764\"\u003eBrandometer Methodology\u003c/h2\u003e\u003ch2 id=\"9c36\"\u003eProblem Setup and Data\u003c/h2\u003e\u003cp id=\"0ad2\"\u003eIn order to measure brand perception on social media, we assessedall Airbnb related mentions from 19 platforms (e.g., X — formerly known as Twitter, Facebook, Reddit, etc) and generated word embeddings with state-of-the-art models.\u003c/p\u003e\u003cp id=\"4eba\"\u003eIn order to use Social media data to generate meaningful word embeddings for the purpose of measuring brand perception, we conquered two challenges:\u003c/p\u003e\u003cul\u003e\u003cli id=\"a561\"\u003e\u003cstrong\u003eQuality\u003c/strong\u003e: Social media posts are mostly user-generated with varying content such as status sharing and reviews, and can be very noisy.\u003c/li\u003e\u003cli id=\"9bc8\"\u003e\u003cstrong\u003eQuantity\u003c/strong\u003e: Social media post sparsity is another challenge. Considering that it typically requires some time for social media users to generate data in response to certain activities and events, a monthly rolling window maintains a good balance of promptness and detectability. Our monthly dataset is relatively small (around 20 million words) as compared to a typical dataset used to train good quality word embeddings (e.g., about 100 billion words for Google News Word2Vec model). Warm-start from pre-trained models didn’t help since the in-domain data barely moved the learned embeddings.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"781a\"\u003eWe developed multiple data cleaning processes to improve data quality. At the same time, we innovated the modeling techniques to mitigate the impact on word embedding quality due to data quantity and quality.\u003c/p\u003e\u003cp id=\"2b80\"\u003eIn addition to data, we explored and compared multiple word embedding training techniques with the goal to generate reliable brand perception scores.\u003c/p\u003e\u003ch2 id=\"ea21\"\u003eWord2Vec\u003c/h2\u003e\u003cp id=\"fd7e\"\u003e\u003ca href=\"https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eWord2Vec\u003c/a\u003e is by far the simplest and most widely used word embedding model since 2013. We started with building CBOW-based Word2Vec models using \u003ca href=\"https://radimrehurek.com/gensim/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGensim\u003c/a\u003e. Word2Vec produced decent in-domain word embeddings, and more importantly, the concept of analogies. In our domain-specific word embeddings, we are able to capture analogies in the Airbnb domain, such as \u003cem\u003e“host” — “provide” + “guest” ~= “need”\u003c/em\u003e, \u003cem\u003e“city” — “mall” + “nature” ~= “park”\u003c/em\u003e.\u003c/p\u003e\u003ch2 id=\"548c\"\u003eFastText\u003c/h2\u003e\u003cp id=\"0b22\"\u003eFastText takes into account the internal structure of words, and is more robust to out-of-vocabulary words and smaller datasets. Moreover, as inspired by \u003ca href=\"https://analyticsindiamag.com/guide-to-sense2vec-contextually-keyed-word-vectors-for-nlp/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSense2Vec\u003c/a\u003e, we associate words with sentiments (i.e., POSITIVE, NEGATIVE, NEUTRAL), which forms brand perception concepts on the sentiment levels.\u003c/p\u003e\u003ch2 id=\"7ea5\"\u003eDeBERTa\u003c/h2\u003e\u003cp id=\"4432\"\u003eRecent progress in transformer-based language models (e.g., \u003ca href=\"https://arxiv.org/abs/1810.04805\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBERT\u003c/a\u003e) has significantly improved the performance of NLU tasks with the advantage of generating contextualized word embeddings. We developed \u003ca href=\"https://arxiv.org/abs/2006.03654\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDeBERTa\u003c/a\u003e based word embeddings, which works better with smaller dataset and pays more attention to surrounding context via disentangled attention mechanisms. We trained everything from scratch (including tokenizer) using \u003ca href=\"https://huggingface.co/docs/transformers/index\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTransformers\u003c/a\u003e, and the concatenated last attention layer embeddings resulted in the best word embeddings for our case.\u003c/p\u003e\u003ch2 id=\"9321\"\u003eBrand Perception Score Stabilization and Calibration\u003c/h2\u003e\u003cp id=\"1397\"\u003eThe variability of word embeddings has been widely studied (\u003ca href=\"https://arxiv.org/pdf/2104.08433.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBorah, 2021\u003c/a\u003e). The causes range from the underlying stochastic nature of deep learning models (e.g., random initialization of word embeddings, embedding training which leads to local optimum for global optimization criteria) to the quantity and quality changes of data corpus across time.\u003c/p\u003e\u003cp id=\"9487\"\u003eWith Brandometer, we need to reduce the variability in embedding distances to generate stable time series tracking. Stable embedding distances helped preserve the inherent patterns and structures present in the time series data, and hence it contributes to better predictability of the tracking process. Additionally, it made the tracking process more robust to noisy fluctuations. We studied the influential factors and took the following steps to reduce:\u003c/p\u003e\u003col\u003e\u003cli id=\"732f\"\u003eScore averaging over repetitive training with bootstrap sampling\u003c/li\u003e\u003cli id=\"b971\"\u003eRank-based perception score\u003c/li\u003e\u003c/ol\u003e\u003cblockquote\u003e\u003cp id=\"5451\"\u003eS\u003cstrong\u003ecore averaging over repetitive training with upsampling\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"b833\"\u003eFor each month’s data, we trained \u003cem\u003eN\u003c/em\u003e models with the same hyper-parameters, and took the average of \u003cem\u003eN\u003c/em\u003e perception scores as the final score for each concept. Meanwhile, we did upsampling to make sure that each model iterated on an equal number of data points across months.\u003c/p\u003e\u003cp id=\"8b5c\"\u003eWe defined variability as:\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eEq.2\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"1748\"\u003ewhere\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"4615\"\u003e\u003cem\u003eCosSim(w)\u003c/em\u003e refers to the cosine similarity based perception score defined in Eq. 1, \u003cem\u003eA\u003c/em\u003e refers to the algorithm, \u003cem\u003eM\u003c/em\u003e refers to the time window (i.e. month), \u003cem\u003eV\u003c/em\u003e refers to the vocabulary and \u003cem\u003e|V|\u003c/em\u003e is the vocabulary size, and \u003cem\u003en\u003c/em\u003e refers to the number of repetitively trained models.\u003c/p\u003e\u003cp id=\"d42f\"\u003eAs \u003cem\u003eN\u003c/em\u003e approaches 30, the score variability values converge and settle within a narrow interval. Hence, we picked \u003cem\u003eN = 30\u003c/em\u003e for all.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 1. Score variability changes of the FastText models across months in a year with increasing \u003cem\u003eN\u003c/em\u003e.\u003c/figcaption\u003e\u003c/figure\u003e\u003cblockquote\u003e\u003cp id=\"0068\"\u003e\u003cstrong\u003eRank-based perception score\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"2b44\"\u003eBased on \u003ca href=\"https://aclanthology.org/Q18-1008/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMaria Antoniak’s\u003c/a\u003e work, we used the overlap between nearest neighbors to measure the stability of word embeddings, since the relative distances matter more than the absolute distance values in downstream tasks. Therefore, we also developed rank-based scores, which shows greater stability as compared to similarity-based scores.\u003c/p\u003e\u003cp id=\"f68b\"\u003eFor each word, we first ranked them in descending order of cosine similarity via Eq. 1. The rank-based similarity score is then computed as \u003cem\u003e1/rank(w)\u003c/em\u003e where \u003cem\u003ew∈V\u003c/em\u003e. More relevant concepts will have higher rank-based perception scores.\u003c/p\u003e\u003cp id=\"0ca5\"\u003eThe score variability is defined the same as \u003cem\u003eVariability(A, M, V)\u003c/em\u003e in Eq. 2 except that\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"fd8e\"\u003ewhere \u003cem\u003eRankSim(w)\u003c/em\u003e refers to the rank based perception score. With rank-based scores, when \u003cem\u003eN\u003c/em\u003e approaches to \u003cem\u003e30\u003c/em\u003e, the score variability values converge to a much narrower interval especially for DeBERTa.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 2. Rank-based score variability changes of the FastText models across months in 2020 with increasing \u003cem\u003eN\u003c/em\u003e.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"1b7c\"\u003eSelection of Score Output by Designed Metrics\u003c/h2\u003e\u003cp id=\"78ed\"\u003eOne challenge of this project was that we didn’t have a simple and ultimate way to conclude which score output was better since there is no objective ‘truth’ of brand perception. Instead, we defined a new metric to learn some characteristics of the score.\u003c/p\u003e\u003cp id=\"e147\"\u003e\u003cstrong\u003eAverage Variance Across Different Period (AVADP)\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli id=\"5d10\"\u003eWe first picked the group of top relevant brand perceptions for Airbnb: ‘host,’ ‘vacation,’ ‘rental,’ ‘love,’ ‘stay,’ ‘home,’ ‘booking,’ ‘travel,’ ‘guest’.\u003c/li\u003e\u003cli id=\"1dbc\"\u003eHigher value indicates more fluctuations across different periods — likely a bad thing, because the selected brand perception is assumed to be relatively stable and hence should not vary too much month by month.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cfigure\u003e\u003cfigcaption\u003ei\u003cem\u003e ∈ (1,n)\u003c/em\u003e, \u003cem\u003en\u003c/em\u003e as number of selected top perceptions, T as number of periods\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"8235\"\u003eWe checked these statistics on the calibrated results as shown above. We can see that the ranked-based score is the winner as compared to similarity-based scores:\u003c/p\u003e\u003cul\u003e\u003cli id=\"9f72\"\u003e\u003cstrong\u003eLower AVADP\u003c/strong\u003e: More fluctuations than the non-ranked across a different period — likely a good thing, because the selected brand perception is assumed to be relatively stable and hence should not vary too much month by month.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"4080\"\u003eUse Cases of Brandometer\u003c/h2\u003e\u003cp id=\"b9c6\"\u003eThough we set out to solve the problem of brand measurement, we believe use cases can go above and beyond:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"9baa\"\u003eUse Cases Deep Dive\u003c/h2\u003e\u003cp id=\"1cb9\"\u003e\u003cstrong\u003eIndustry Analysis: Top Brand Perception among Key Players [Monthly Top Perception]\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"007b\"\u003eWith top perceptions such as “Stay” and “Home,” Airbnb provides a brand image of “\u003cstrong\u003ebelonging\u003c/strong\u003e”, echoing our mission statement and unique supply inventory, while other companies have “Rental,” “Room,” “Booking,” a description of functionality, not human sensation.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"56ad\"\u003e\u003cstrong\u003eTop Emerging Perception reveals major events discussed online [Monthly Top Perception]\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"e6bf\"\u003eThe \u003cstrong\u003eTop 10\u003c/strong\u003e Perceptions are generally stable month to month. The top standing perceptions include\u003c/p\u003e\u003cul\u003e\u003cli id=\"abaf\"\u003eHome, Host, Stay, Travel, Guest, Rental, etc.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"cd9a\"\u003eMeanwhile, we use Brandometer to monitor emerging perceptions that jump to the top list, which may reflect major events associated with the brand or user preference changes.\u003c/p\u003e\u003cp id=\"63f1\"\u003e\u003cstrong\u003eMajor Campaign Monitor (Time Series Tracking)\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"598a\"\u003eBusinesses create campaigns to promote products and expand the brand image. We were able to capture a perception change on one specific Brand Theme after a related campaign.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eFigure 3. Example of brand perception change due to a campaign. Scores shown here are based on Calibrated Rank-based Score.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"a078\"\u003eThese use cases are just the beginning. Essentially, this is an innovative way of gathering massive online input as we learn the needs and perception of the community. We will constantly reflect on how we leverage these insights to continually improve the Airbnb experience for our community.\u003c/p\u003e\u003ch2 id=\"7285\"\u003eNext Steps\u003c/h2\u003e\u003cp id=\"08ac\"\u003eAirbnb’s innovative Brandometer has already demonstrated success in capturing brand perception from social media data. There are several directions for future improvement:\u003c/p\u003e\u003cul\u003e\u003cli id=\"efc9\"\u003eBetter content segmentation for clearer and more concise insights.\u003c/li\u003e\u003cli id=\"2bef\"\u003eDevelop more metrics reflecting social media brand perception.\u003c/li\u003e\u003cli id=\"4ef6\"\u003eEnhance data foundation, not just Airbnb, but other companies in the same market segment to get more comprehensive insights.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"d4f2\"\u003eIf this kind of work sounds appealing to you, check out our \u003ca href=\"https://careers.airbnb.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eopen roles\u003c/a\u003e — we’re hiring!\u003c/p\u003e\u003ch2 id=\"5497\"\u003eAcknowledgments\u003c/h2\u003e\u003cp id=\"747d\"\u003eThanks to Mia Zhao, Bo Zeng, Cassie Cao for contributing the best ideas on improving and landing Airbnb Brandometer. Thanks to Jon Young, Narin Leininger, Allison Frelinger for the support of social media data consolidation. Thanks to Linsha Chen, Sam Barrows, Hannah Jeton, and Irina Azu who provide feedback and suggestions. Thanks to Lianghao Li, Kelvin Xiong, Nathan Triplett, Joy Zhang, Andy Yasutake for reviewing and polishing the blog post content and all the great suggestions. Thank Joy Zhang, Tina Su, Andy Yasutake for leadership support!\u003c/p\u003e\u003cp id=\"b029\"\u003eSpecial thanks to Joy Zhang, who initiated the idea, for all the inspiring conversations, continuous guidance and support!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2024-04-26T16:01:16.267Z",
  "modifiedTime": null
}
