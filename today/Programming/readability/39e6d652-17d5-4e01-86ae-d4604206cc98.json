{
  "id": "39e6d652-17d5-4e01-86ae-d4604206cc98",
  "title": "Investigation of a Cross-regional Network Performance Issue",
  "link": "https://netflixtechblog.com/investigation-of-a-cross-regional-network-performance-issue-422d6218fdf1?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Mon, 05 Aug 2024 22:18:00 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": [
    "tcp",
    "debugging",
    "kernel",
    "network",
    "linux"
  ],
  "byline": "Netflix Technology Blog",
  "length": 14646,
  "excerpt": "Netflix operates a highly efficient cloud computing infrastructure that supports a wide array of applications essential for our SVOD (Subscription Video on Demand), live streaming and gaming…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "Hechao Li, Roger CruzCloud Networking TopologyNetflix operates a highly efficient cloud computing infrastructure that supports a wide array of applications essential for our SVOD (Subscription Video on Demand), live streaming and gaming services. Utilizing Amazon AWS, our infrastructure is hosted across multiple geographic regions worldwide. This global distribution allows our applications to deliver content more effectively by serving traffic closer to our customers. Like any distributed system, our applications occasionally require data synchronization between regions to maintain seamless service delivery.The following diagram shows a simplified cloud network topology for cross-region traffic.The Problem At First GlanceOur Cloud Network Engineering on-call team received a request to address a network issue affecting an application with cross-region traffic. Initially, it appeared that the application was experiencing timeouts, likely due to suboptimal network performance. As we all know, the longer the network path, the more devices the packets traverse, increasing the likelihood of issues. For this incident, the client application is located in an internal subnet in the US region while the server application is located in an external subnet in a European region. Therefore, it is natural to blame the network since packets need to travel long distances through the internet.As network engineers, our initial reaction when the network is blamed is typically, “No, it can’t be the network,” and our task is to prove it. Given that there were no recent changes to the network infrastructure and no reported AWS issues impacting other applications, the on-call engineer suspected a noisy neighbor issue and sought assistance from the Host Network Engineering team.Blame the NeighborsIn this context, a noisy neighbor issue occurs when a container shares a host with other network-intensive containers. These noisy neighbors consume excessive network resources, causing other containers on the same host to suffer from degraded network performance. Despite each container having bandwidth limitations, oversubscription can still lead to such issues.Upon investigating other containers on the same host — most of which were part of the same application — we quickly eliminated the possibility of noisy neighbors. The network throughput for both the problematic container and all others was significantly below the set bandwidth limits. We attempted to resolve the issue by removing these bandwidth limits, allowing the application to utilize as much bandwidth as necessary. However, the problem persisted.Blame the NetworkWe observed some TCP packets in the network marked with the RST flag, a flag indicating that a connection should be immediately terminated. Although the frequency of these packets was not alarmingly high, the presence of any RST packets still raised suspicion on the network. To determine whether this was indeed a network-induced issue, we conducted a tcpdump on the client. In the packet capture file, we spotted one TCP stream that was closed after exactly 30 seconds.SYN at 18:47:06After the 3-way handshake (SYN,SYN-ACK,ACK), the traffic started flowing normally. Nothing strange until FIN at 18:47:36 (30 seconds later)The packet capture results clearly indicated that it was the client application that initiated the connection termination by sending a FIN packet. Following this, the server continued to send data; however, since the client had already decided to close the connection, it responded with RST packets to all subsequent data from the server.To ensure that the client wasn’t closing the connection due to packet loss, we also conducted a packet capture on the server side to verify that all packets sent by the server were received. This task was complicated by the fact that the packets passed through a NAT gateway (NGW), which meant that on the server side, the client’s IP and port appeared as those of the NGW, differing from those seen on the client side. Consequently, to accurately match TCP streams, we needed to identify the TCP stream on the client side, locate the raw TCP sequence number, and then use this number as a filter on the server side to find the corresponding TCP stream.With packet capture results from both the client and server sides, we confirmed that all packets sent by the server were correctly received before the client sent a FIN.Now, from the network point of view, the story is clear. The client initiated the connection requesting data from the server. The server kept sending data to the client with no problem. However, at a certain point, despite the server still having data to send, the client chose to terminate the reception of data. This led us to suspect that the issue might be related to the client application itself.Blame the ApplicationIn order to fully understand the problem, we now need to understand how the application works. As shown in the diagram below, the application runs in the us-east-1 region. It reads data from cross-region servers and writes the data to consumers within the same region. The client runs as containers, whereas the servers are EC2 instances.Notably, the cross-region read was problematic while the write path was smooth. Most importantly, there is a 30-second application-level timeout for reading the data. The application (client) errors out if it fails to read an initial batch of data from the servers within 30 seconds. When we increased this timeout to 60 seconds, everything worked as expected. This explains why the client initiated a FIN — because it lost patience waiting for the server to transfer data.Could it be that the server was updated to send data more slowly? Could it be that the client application was updated to receive data more slowly? Could it be that the data volume became too large to be completely sent out within 30 seconds? Sadly, we received negative answers for all 3 questions from the application owner. The server had been operating without changes for over a year, there were no significant updates in the latest rollout of the client, and the data volume had remained consistent.Blame the KernelIf both the network and the application weren’t changed recently, then what changed? In fact, we discovered that the issue coincided with a recent Linux kernel upgrade from version 6.5.13 to 6.6.10. To test this hypothesis, we rolled back the kernel upgrade and it did restore normal operation to the application.Honestly speaking, at that time I didn’t believe it was a kernel bug because I assumed the TCP implementation in the kernel should be solid and stable (Spoiler alert: How wrong was I!). But we were also out of ideas from other angles.There were about 14k commits between the good and bad kernel versions. Engineers on the team methodically and diligently bisected between the two versions. When the bisecting was narrowed to a couple of commits, a change with “tcp” in its commit message caught our attention. The final bisecting confirmed that this commit was our culprit.Interestingly, while reviewing the email history related to this commit, we found that another user had reported a Python test failure following the same kernel upgrade. Although their solution was not directly applicable to our situation, it suggested that a simpler test might also reproduce our problem. Using strace, we observed that the application configured the following socket options when communicating with the server:[pid 1699] setsockopt(917, SOL_IPV6, IPV6_V6ONLY, [0], 4) = 0[pid 1699] setsockopt(917, SOL_SOCKET, SO_KEEPALIVE, [1], 4) = 0[pid 1699] setsockopt(917, SOL_SOCKET, SO_SNDBUF, [131072], 4) = 0[pid 1699] setsockopt(917, SOL_SOCKET, SO_RCVBUF, [65536], 4) = 0[pid 1699] setsockopt(917, SOL_TCP, TCP_NODELAY, [1], 4) = 0We then developed a minimal client-server C application that transfers a file from the server to the client, with the client configuring the same set of socket options. During testing, we used a 10M file, which represents the volume of data typically transferred within 30 seconds before the client issues a FIN. On the old kernel, this cross-region transfer completed in 22 seconds, whereas on the new kernel, it took 39 seconds to finish.The Root CauseWith the help of the minimal reproduction setup, we were ultimately able to pinpoint the root cause of the problem. In order to understand the root cause, it’s essential to have a grasp of the TCP receive window.TCP Receive WindowSimply put, the TCP receive window is how the receiver tells the sender “This is how many bytes you can send me without me ACKing any of them”. Assuming the sender is the server and the receiver is the client, then we have:The Window SizeNow that we know the TCP receive window size could affect the throughput, the question is, how is the window size calculated? As an application writer, you can’t decide the window size, however, you can decide how much memory you want to use for buffering received data. This is configured using SO_RCVBUF socket option we saw in the strace result above. However, note that the value of this option means how much application data can be queued in the receive buffer. In man 7 socket, there isSO_RCVBUFSets or gets the maximum socket receive buffer in bytes. The kernel doubles this value (to allow space for bookkeeping overhead) when it is set using setsockopt(2), and this doubled value is returned by getsockopt(2). The default value is set by the /proc/sys/net/core/rmem_default file, and the maximum allowed value is set by the /proc/sys/net/core/rmem_max file. The minimum (doubled) value for this option is 256.This means, when the user gives a value X, then the kernel stores 2X in the variable sk-\u003esk_rcvbuf. In other words, the kernel assumes that the bookkeeping overhead is as much as the actual data (i.e. 50% of the sk_rcvbuf).sysctl_tcp_adv_win_scaleHowever, the assumption above may not be true because the actual overhead really depends on a lot of factors such as Maximum Transmission Unit (MTU). Therefore, the kernel provided this sysctl_tcp_adv_win_scale which you can use to tell the kernel what the actual overhead is. (I believe 99% of people also don’t know how to set this parameter correctly and I’m definitely one of them. You’re the kernel, if you don’t know the overhead, how can you expect me to know?).According to the sysctl doc,tcp_adv_win_scale — INTEGERObsolete since linux-6.6 Count buffering overhead as bytes/2^tcp_adv_win_scale (if tcp_adv_win_scale \u003e 0) or bytes-bytes/2^(-tcp_adv_win_scale), if it is \u003c= 0.Possible values are [-31, 31], inclusive.Default: 1For 99% of people, we’re just using the default value 1, which in turn means the overhead is calculated by rcvbuf/2^tcp_adv_win_scale = 1/2 * rcvbuf. This matches the assumption when setting the SO_RCVBUF value.Let’s recap. Assume you set SO_RCVBUF to 65536, which is the value set by the application as shown in the setsockopt syscall. Then we have:SO_RCVBUF = 65536rcvbuf = 2 * 65536 = 131072overhead = rcvbuf / 2 = 131072 / 2 = 65536receive window size = rcvbuf — overhead = 131072–65536 = 65536(Note, this calculation is simplified. The real calculation is more complex.)In short, the receive window size before the kernel upgrade was 65536. With this window size, the application was able to transfer 10M data within 30 seconds.The ChangeThis commit obsoleted sysctl_tcp_adv_win_scale and introduced a scaling_ratio that can more accurately calculate the overhead or window size, which is the right thing to do. With the change, the window size is now rcvbuf * scaling_ratio.So how is scaling_ratio calculated? It is calculated using skb-\u003elen/skb-\u003etruesize where skb-\u003elen is the length of the tcp data length in an skb and truesize is the total size of the skb. This is surely a more accurate ratio based on real data rather than a hardcoded 50%. Now, here is the next question: during the TCP handshake before any data is transferred, how do we decide the initial scaling_ratio? The answer is, a magic and conservative ratio was chosen with the value being roughly 0.25.Now we have:SO_RCVBUF = 65536rcvbuf = 2 * 65536 = 131072receive window size = rcvbuf * 0.25 = 131072 * 0.25 = 32768In short, the receive window size halved after the kernel upgrade. Hence the throughput was cut in half, causing the data transfer time to double.Naturally, you may ask, I understand that the initial window size is small, but why doesn’t the window grow when we have a more accurate ratio of the payload later (i.e. skb-\u003elen/skb-\u003etruesize)? With some debugging, we eventually found out that the scaling_ratio does get updated to a more accurate skb-\u003elen/skb-\u003etruesize, which in our case is around 0.66. However, another variable, window_clamp, is not updated accordingly. window_clamp is the maximum receive window allowed to be advertised, which is also initialized to 0.25 * rcvbuf using the initial scaling_ratio. As a result, the receive window size is capped at this value and can’t grow bigger.The FixIn theory, the fix is to update window_clamp along with scaling_ratio. However, in order to have a simple fix that doesn’t introduce other unexpected behaviors, our final fix was to increase the initial scaling_ratio from 25% to 50%. This will make the receive window size backward compatible with the original default sysctl_tcp_adv_win_scale.Meanwhile, notice that the problem is not only caused by the changed kernel behavior but also by the fact that the application sets SO_RCVBUF and has a 30-second application-level timeout. In fact, the application is Kafka Connect and both settings are the default configurations (receive.buffer.bytes=64k and request.timeout.ms=30s). We also created a kafka ticket to change receive.buffer.bytes to -1 to allow Linux to auto tune the receive window.ConclusionThis was a very interesting debugging exercise that covered many layers of Netflix’s stack and infrastructure. While it technically wasn’t the “network” to blame, this time it turned out the culprit was the software components that make up the network (i.e. the TCP implementation in the kernel).If tackling such technical challenges excites you, consider joining our Cloud Infrastructure Engineering teams. Explore opportunities by visiting Netflix Jobs and searching for Cloud Engineering positions.AcknowledgmentsSpecial thanks to our stunning colleagues Alok Tiagi, Artem Tkachuk, Ethan Adams, Jorge Rodriguez, Nick Mahilani, Tycho Andersen and Vinay Rayini for investigating and mitigating this issue. We would also like to thank Linux kernel network expert Eric Dumazet for reviewing and applying the patch.",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1022/0*RpHklRseVBeBJG6u",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page-----422d6218fdf1--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://netflixtechblog.com/?source=post_page-----422d6218fdf1--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix TechBlog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"680a\"\u003e\u003ca href=\"https://www.linkedin.com/in/hechaoli/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHechao Li\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/rogercruz/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRoger Cruz\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"a44a\"\u003eCloud Networking Topology\u003c/h2\u003e\u003cp id=\"8b0b\"\u003eNetflix operates a highly efficient cloud computing infrastructure that supports a wide array of applications essential for our SVOD (Subscription Video on Demand), live streaming and gaming services. Utilizing Amazon AWS, our infrastructure is hosted across multiple geographic regions worldwide. This global distribution allows our applications to deliver content more effectively by serving traffic closer to our customers. Like any distributed system, our applications occasionally require data synchronization between regions to maintain seamless service delivery.\u003c/p\u003e\u003cp id=\"1fe6\"\u003eThe following diagram shows a simplified cloud network topology for cross-region traffic.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"f7b1\"\u003eThe Problem At First Glance\u003c/h2\u003e\u003cp id=\"f312\"\u003eOur Cloud Network Engineering on-call team received a request to address a network issue affecting an application with cross-region traffic. Initially, it appeared that the application was experiencing timeouts, likely due to suboptimal network performance. As we all know, the longer the network path, the more devices the packets traverse, increasing the likelihood of issues. For this incident, \u003cstrong\u003ethe client application is located in an internal subnet in the US region while the server application is located in an external subnet in a European region\u003c/strong\u003e. Therefore, it is natural to blame the network since packets need to travel long distances through the internet.\u003c/p\u003e\u003cp id=\"d4f7\"\u003eAs network engineers, our initial reaction when the network is blamed is typically, “No, it can’t be the network,” and our task is to prove it. Given that there were no recent changes to the network infrastructure and no reported AWS issues impacting other applications, the on-call engineer suspected a noisy neighbor issue and sought assistance from the Host Network Engineering team.\u003c/p\u003e\u003ch2 id=\"aa0a\"\u003eBlame the Neighbors\u003c/h2\u003e\u003cp id=\"de5a\"\u003eIn this context, a noisy neighbor issue occurs when a container shares a host with other network-intensive containers. \u003cstrong\u003eThese noisy neighbors consume excessive network resources, causing other containers on the same host to suffer from degraded network performance. \u003c/strong\u003eDespite each container having bandwidth limitations, oversubscription can still lead to such issues.\u003c/p\u003e\u003cp id=\"f159\"\u003eUpon investigating other containers on the same host — most of which were part of the same application — we quickly eliminated the possibility of noisy neighbors. \u003cstrong\u003eThe network throughput for both the problematic container and all others was significantly below the set bandwidth limits.\u003c/strong\u003e We attempted to resolve the issue by removing these bandwidth limits, allowing the application to utilize as much bandwidth as necessary. However, the problem persisted.\u003c/p\u003e\u003ch2 id=\"8f96\"\u003eBlame the Network\u003c/h2\u003e\u003cp id=\"cdc9\"\u003eWe observed some \u003cstrong\u003eTCP packets in the network marked with the RST flag\u003c/strong\u003e, a flag indicating that a connection should be immediately terminated. Although the frequency of these packets was not alarmingly high, the presence of any RST packets still raised suspicion on the network. To determine whether this was indeed a network-induced issue, we conducted a tcpdump on the client. In the packet capture file, we spotted one TCP stream that was closed after exactly 30 seconds.\u003c/p\u003e\u003cp id=\"e4ad\"\u003eSYN at 18:47:06\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"6af6\"\u003eAfter the 3-way handshake (SYN,SYN-ACK,ACK), the traffic started flowing normally. Nothing strange until FIN at 18:47:36 (30 seconds later)\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"f0da\"\u003eThe packet capture results clearly indicated that \u003cstrong\u003eit was the client application that initiated the connection termination by sending a FIN packet\u003c/strong\u003e. Following this, the server continued to send data; however, since the client had already decided to close the connection, it responded with RST packets to all subsequent data from the server.\u003c/p\u003e\u003cp id=\"c1f7\"\u003eTo ensure that the client wasn’t closing the connection due to packet loss, we also conducted a packet capture on the server side to verify that all packets sent by the server were received. This task was complicated by the fact that the packets passed through a NAT gateway (NGW), which meant that on the server side, the client’s IP and port appeared as those of the NGW, differing from those seen on the client side. Consequently, to accurately match TCP streams, \u003cstrong\u003ewe needed to identify the TCP stream on the client side, locate the raw TCP sequence number, and then use this number as a filter on the server side to find the corresponding TCP stream.\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"92bd\"\u003eWith packet capture results from both the client and server sides, we confirmed that \u003cstrong\u003eall packets sent by the server were correctly received before the client sent a FIN\u003c/strong\u003e.\u003c/p\u003e\u003cp id=\"1f6d\"\u003eNow, from the network point of view, the story is clear. The client initiated the connection requesting data from the server. The server kept sending data to the client with no problem. However, at a certain point, \u003cstrong\u003edespite the server still having data to send, the client chose to terminate the reception of data\u003c/strong\u003e. This led us to suspect that the issue might be related to the client application itself.\u003c/p\u003e\u003ch2 id=\"6807\"\u003eBlame the Application\u003c/h2\u003e\u003cp id=\"e481\"\u003eIn order to fully understand the problem, we now need to understand how the application works. As shown in the diagram below, the application runs in the us-east-1 region. \u003cstrong\u003eIt reads data from cross-region servers and writes the data to consumers within the same region.\u003c/strong\u003e The client runs as containers, whereas the servers are EC2 instances.\u003c/p\u003e\u003cp id=\"e82c\"\u003e\u003cstrong\u003eNotably, the cross-region read was problematic \u003c/strong\u003ewhile the write path was smooth. Most importantly, there is a 30-second application-level timeout for reading the data. The application (client) errors out if it fails to read an initial batch of data from the servers within 30 seconds. When we increased this timeout to 60 seconds, everything worked as expected. \u003cstrong\u003eThis explains why the client initiated a FIN — because it lost patience waiting for the server to transfer data\u003c/strong\u003e.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"da9f\"\u003eCould it be that the server was updated to send data more slowly? Could it be that the client application was updated to receive data more slowly? Could it be that the data volume became too large to be completely sent out within 30 seconds? Sadly, \u003cstrong\u003ewe received negative answers for all 3 questions from the application owner.\u003c/strong\u003e The server had been operating without changes for over a year, there were no significant updates in the latest rollout of the client, and the data volume had remained consistent.\u003c/p\u003e\u003ch2 id=\"76c8\"\u003eBlame the Kernel\u003c/h2\u003e\u003cp id=\"efa1\"\u003eIf both the network and the application weren’t changed recently, then what changed? In fact, we discovered that the issue coincided with a recent \u003cstrong\u003eLinux kernel upgrade from version 6.5.13 to 6.6.10\u003c/strong\u003e. To test this hypothesis, we rolled back the kernel upgrade and it did restore normal operation to the application.\u003c/p\u003e\u003cp id=\"9ee2\"\u003eHonestly speaking, at that time I didn’t believe it was a kernel bug because I assumed the TCP implementation in the kernel should be solid and stable (Spoiler alert: How wrong was I!). But we were also out of ideas from other angles.\u003c/p\u003e\u003cp id=\"f536\"\u003eThere were about 14k commits between the good and bad kernel versions. Engineers on the team methodically and diligently bisected between the two versions. When the bisecting was narrowed to a couple of commits, \u003cstrong\u003ea change with “tcp” in its commit message caught our attention. The final bisecting confirmed that \u003c/strong\u003e\u003ca href=\"https://lore.kernel.org/netdev/20230717152917.751987-1-edumazet@google.com/T/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003ethis commit\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e was our culprit\u003c/strong\u003e.\u003c/p\u003e\u003cp id=\"cc01\"\u003eInterestingly, while reviewing the email history related to this commit, we found that \u003ca href=\"https://github.com/eventlet/eventlet/issues/821\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eanother user had reported a Python test failure following the same kernel upgrade\u003c/a\u003e. Although their solution was not directly applicable to our situation, it suggested that \u003cstrong\u003ea simpler test might also reproduce our problem\u003c/strong\u003e. Using \u003cem\u003estrace\u003c/em\u003e, we observed that the application configured the following socket options when communicating with the server:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"95ec\"\u003e[pid 1699] setsockopt(917, SOL_IPV6, IPV6_V6ONLY, [0], 4) = 0\u003cbr/\u003e[pid 1699] setsockopt(917, SOL_SOCKET, SO_KEEPALIVE, [1], 4) = 0\u003cbr/\u003e[pid 1699] setsockopt(917, SOL_SOCKET, SO_SNDBUF, [131072], 4) = 0\u003cbr/\u003e[pid 1699] setsockopt(917, SOL_SOCKET, SO_RCVBUF, [65536], 4) = 0\u003cbr/\u003e[pid 1699] setsockopt(917, SOL_TCP, TCP_NODELAY, [1], 4) = 0\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"3e18\"\u003eWe then developed a minimal client-server C application that transfers a file from the server to the client, with the client configuring the same set of socket options. During testing, we used a 10M file, which represents the volume of data typically transferred within 30 seconds before the client issues a FIN. \u003cstrong\u003eOn the old kernel, this cross-region transfer completed in 22 seconds, whereas on the new kernel, it took 39 seconds to finish.\u003c/strong\u003e\u003c/p\u003e\u003ch2 id=\"ed1b\"\u003eThe Root Cause\u003c/h2\u003e\u003cp id=\"1490\"\u003eWith the help of the minimal reproduction setup, we were ultimately able to pinpoint the root cause of the problem. In order to understand the root cause, it’s essential to have a grasp of the TCP receive window.\u003c/p\u003e\u003ch2 id=\"6a6e\"\u003eTCP Receive Window\u003c/h2\u003e\u003cp id=\"a997\"\u003eSimply put, \u003cstrong\u003ethe TCP receive window is how the receiver tells the sender “This is how many bytes you can send me without me ACKing any of them”\u003c/strong\u003e. Assuming the sender is the server and the receiver is the client, then we have:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"f567\"\u003eThe Window Size\u003c/h2\u003e\u003cp id=\"0d83\"\u003eNow that we know the TCP receive window size could affect the throughput, the question is, how is the window size calculated? As an application writer, you can’t decide the window size, however, you can decide how much memory you want to use for buffering received data. This is configured using \u003cstrong\u003e\u003cem\u003eSO_RCVBUF\u003c/em\u003e socket option\u003c/strong\u003e we saw in the \u003cem\u003estrace\u003c/em\u003e result above. However, note that the value of this option means how much \u003cstrong\u003eapplication data\u003c/strong\u003e can be queued in the receive buffer. In \u003ca href=\"https://man7.org/linux/man-pages/man7/socket.7.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eman 7 socket\u003c/a\u003e, there is\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"a73f\"\u003eSO_RCVBUF\u003c/p\u003e\u003cp id=\"ed59\"\u003eSets or gets the maximum socket receive buffer in bytes.\u003cbr/\u003e The kernel doubles this value (to allow space for\u003cbr/\u003e bookkeeping overhead) when it is set using setsockopt(2),\u003cbr/\u003e and this doubled value is returned by getsockopt(2). The\u003cbr/\u003e default value is set by the\u003cbr/\u003e /proc/sys/net/core/rmem_default file, and the maximum\u003cbr/\u003e allowed value is set by the /proc/sys/net/core/rmem_max\u003cbr/\u003e file. The minimum (doubled) value for this option is 256.\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"71af\"\u003eThis means, when the user gives a value X, then \u003ca href=\"https://elixir.bootlin.com/linux/v6.9-rc1/source/net/core/sock.c#L976\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ethe kernel stores 2X in the variable sk-\u0026gt;sk_rcvbuf\u003c/a\u003e. In other words, \u003cstrong\u003ethe kernel assumes that the bookkeeping overhead is as much as the actual data (i.e. 50% of the sk_rcvbuf)\u003c/strong\u003e.\u003c/p\u003e\u003ch2 id=\"7337\"\u003esysctl_tcp_adv_win_scale\u003c/h2\u003e\u003cp id=\"9a3d\"\u003eHowever, the assumption above may not be true because the actual overhead really depends on a lot of factors such as Maximum Transmission Unit (MTU). Therefore, \u003cstrong\u003ethe kernel provided this \u003cem\u003esysctl_tcp_adv_win_scale\u003c/em\u003e which you can use to tell the kernel what the actual overhead is\u003c/strong\u003e. (I believe 99% of people also don’t know how to set this parameter correctly and I’m definitely one of them. You’re the kernel, if you don’t know the overhead, how can you expect me to know?).\u003c/p\u003e\u003cp id=\"4e58\"\u003eAccording to \u003ca href=\"https://docs.kernel.org/networking/ip-sysctl.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ethe \u003cem\u003esysctl\u003c/em\u003e doc\u003c/a\u003e,\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"23e2\"\u003e\u003cem\u003etcp_adv_win_scale — INTEGER\u003c/em\u003e\u003c/p\u003e\u003cp id=\"a18b\"\u003e\u003cem\u003eObsolete since linux-6.6 Count buffering overhead as bytes/2^tcp_adv_win_scale (if tcp_adv_win_scale \u0026gt; 0) or bytes-bytes/2^(-tcp_adv_win_scale), if it is \u0026lt;= 0.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"0786\"\u003e\u003cem\u003ePossible values are [-31, 31], inclusive.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"2f88\"\u003e\u003cem\u003eDefault: 1\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp id=\"ebc0\"\u003eFor 99% of people, we’re just using the default value 1, which in turn means the overhead is calculated by \u003cem\u003ercvbuf/2^tcp_adv_win_scale = 1/2 * rcvbuf\u003c/em\u003e. This matches the assumption when setting the \u003cem\u003eSO_RCVBUF\u003c/em\u003e value.\u003c/p\u003e\u003cp id=\"289d\"\u003eLet’s recap. Assume you set \u003cem\u003eSO_RCVBUF\u003c/em\u003e to 65536, which is the value set by the application as shown in the \u003cem\u003esetsockopt\u003c/em\u003e syscall. Then we have:\u003c/p\u003e\u003cul\u003e\u003cli id=\"e79f\"\u003eSO_RCVBUF = 65536\u003c/li\u003e\u003cli id=\"72c0\"\u003ercvbuf = 2 * 65536 = 131072\u003c/li\u003e\u003cli id=\"e031\"\u003eoverhead = rcvbuf / 2 = 131072 / 2 = 65536\u003c/li\u003e\u003cli id=\"7e11\"\u003ereceive window size = rcvbuf — overhead = 131072–65536 = 65536\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"6c6d\"\u003e(Note, this calculation is simplified. The real calculation is more complex.)\u003c/p\u003e\u003cp id=\"4e0b\"\u003eIn short, the receive window size before the kernel upgrade was 65536. With this window size, the application was able to transfer 10M data within 30 seconds.\u003c/p\u003e\u003ch2 id=\"0ac0\"\u003eThe Change\u003c/h2\u003e\u003cp id=\"8abe\"\u003e\u003ca href=\"https://lore.kernel.org/netdev/20230717152917.751987-1-edumazet@google.com/T/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eThis commit\u003c/a\u003e obsoleted \u003cem\u003esysctl_tcp_adv_win_scale\u003c/em\u003e and introduced a \u003cem\u003escaling_ratio\u003c/em\u003e that can more accurately calculate the overhead or window size, which is the right thing to do. With the change, the window size is now \u003cem\u003ercvbuf * scaling_ratio\u003c/em\u003e.\u003c/p\u003e\u003cp id=\"9bdf\"\u003eSo how is \u003cem\u003escaling_ratio\u003c/em\u003e calculated? It is calculated using \u003cstrong\u003e\u003cem\u003eskb-\u0026gt;len/skb-\u0026gt;truesize\u003c/em\u003e\u003c/strong\u003e where \u003cem\u003eskb-\u0026gt;len\u003c/em\u003e is the length of the tcp data length in an \u003cem\u003eskb\u003c/em\u003e and \u003cem\u003etruesize\u003c/em\u003e is the total size of the \u003cem\u003eskb\u003c/em\u003e. \u003cstrong\u003eThis is surely a more accurate ratio based on real data rather than a hardcoded 50%.\u003c/strong\u003e Now, here is the next question: during the TCP handshake \u003cstrong\u003ebefore any data is transferred, how do we decide the initial \u003cem\u003escaling_ratio\u003c/em\u003e? \u003c/strong\u003eThe answer is, a magic and conservative ratio was chosen with the value being roughly 0.25.\u003c/p\u003e\u003cp id=\"f7cb\"\u003eNow we have:\u003c/p\u003e\u003cul\u003e\u003cli id=\"5f22\"\u003eSO_RCVBUF = 65536\u003c/li\u003e\u003cli id=\"3e62\"\u003ercvbuf = 2 * 65536 = 131072\u003c/li\u003e\u003cli id=\"88c1\"\u003ereceive window size = rcvbuf * 0.25 = 131072 * 0.25 = 32768\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"79ea\"\u003eIn short, \u003cstrong\u003ethe receive window size halved after the kernel upgrade. Hence the throughput was cut in half\u003c/strong\u003e,\u003cstrong\u003e causing the data transfer time to double.\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"de39\"\u003eNaturally, you may ask, I understand that the initial window size is small, but \u003cstrong\u003ewhy doesn’t the window grow when we have a more accurate ratio of the payload later\u003c/strong\u003e (i.e. \u003cem\u003eskb-\u0026gt;len/skb-\u0026gt;truesize\u003c/em\u003e)? With some debugging, we eventually found out that the \u003cem\u003escaling_ratio\u003c/em\u003e does \u003ca href=\"https://elixir.bootlin.com/linux/v6.7.9/source/net/ipv4/tcp_input.c#L248\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eget updated to a more accurate \u003cem\u003eskb-\u0026gt;len/skb-\u0026gt;truesize\u003c/em\u003e\u003c/a\u003e, which in our case is around 0.66. However, another variable, \u003cem\u003ewindow_clamp\u003c/em\u003e, is not updated accordingly. \u003cem\u003ewindow_clamp\u003c/em\u003e is the \u003ca href=\"https://elixir.bootlin.com/linux/v6.7.9/source/include/linux/tcp.h#L256\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003emaximum receive window allowed to be advertised\u003c/a\u003e, which is also initialized to \u003cem\u003e0.25 * rcvbuf \u003c/em\u003eusing the initial \u003cem\u003escaling_ratio\u003c/em\u003e. As a result, \u003cstrong\u003ethe receive window size is capped at this value and can’t grow bigger\u003c/strong\u003e.\u003c/p\u003e\u003ch2 id=\"a8f3\"\u003eThe Fix\u003c/h2\u003e\u003cp id=\"e049\"\u003eIn theory, the fix is to update \u003cem\u003ewindow_clamp\u003c/em\u003e along with \u003cem\u003escaling_ratio\u003c/em\u003e. However, in order to have a simple fix that doesn’t introduce other unexpected behaviors, \u003ca href=\"https://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next.git/commit/?id=697a6c8cec03\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eour final fix was to increase the initial \u003cem\u003escaling_ratio\u003c/em\u003e from 25% to 50%\u003c/a\u003e. This will make the receive window size backward compatible with the original default \u003cem\u003esysctl_tcp_adv_win_scale\u003c/em\u003e.\u003c/p\u003e\u003cp id=\"fb5c\"\u003eMeanwhile, notice that the problem is not only caused by the changed kernel behavior but also by the fact that the application sets \u003cem\u003eSO_RCVBUF\u003c/em\u003e and has a 30-second application-level timeout. In fact, the application is Kafka Connect and both settings are the default configurations (\u003ca href=\"https://kafka.apache.org/documentation/#connectconfigs_receive.buffer.bytes\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003ereceive.buffer.bytes=64k\u003c/em\u003e\u003c/a\u003e and \u003ca href=\"https://kafka.apache.org/documentation/#consumerconfigs_request.timeout.ms\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003erequest.timeout.ms=30s\u003c/em\u003e\u003c/a\u003e). We also\u003ca href=\"https://issues.apache.org/jira/browse/KAFKA-16496\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e created a kafka ticket to change receive.buffer.bytes to -1\u003c/a\u003e to allow Linux to auto tune the receive window.\u003c/p\u003e\u003ch2 id=\"53cb\"\u003eConclusion\u003c/h2\u003e\u003cp id=\"0152\"\u003eThis was a very interesting debugging exercise that covered many layers of Netflix’s stack and infrastructure. While it technically wasn’t the “network” to blame, this time it turned out the culprit was the software components that make up the network (i.e. the TCP implementation in the kernel).\u003c/p\u003e\u003cp id=\"d2c0\"\u003eIf tackling such technical challenges excites you, consider joining our Cloud Infrastructure Engineering teams. Explore opportunities by visiting \u003ca href=\"https://jobs.netflix.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNetflix Jobs\u003c/a\u003e and searching for Cloud Engineering positions.\u003c/p\u003e\u003ch2 id=\"eb85\"\u003eAcknowledgments\u003c/h2\u003e\u003cp id=\"c3e7\"\u003eSpecial thanks to our stunning colleagues \u003ca href=\"https://www.linkedin.com/in/alok-tiagi-99205015/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAlok Tiagi\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/artemtkachuk/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eArtem Tkachuk\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/jethanadams/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEthan Adams\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/jorge-rodriguez-12b5595/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJorge Rodriguez\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/nickmahilani/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNick Mahilani\u003c/a\u003e, \u003ca href=\"https://tycho.pizza/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTycho Andersen\u003c/a\u003e and \u003ca href=\"https://www.linkedin.com/in/vinay-rayini/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVinay Rayini\u003c/a\u003e for investigating and mitigating this issue. We would also like to thank Linux kernel network expert \u003ca href=\"https://www.linkedin.com/in/eric-dumazet-ba252942/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEric Dumazet\u003c/a\u003e for reviewing and applying the patch.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "16 min read",
  "publishedTime": "2024-04-24T16:33:03.776Z",
  "modifiedTime": null
}
