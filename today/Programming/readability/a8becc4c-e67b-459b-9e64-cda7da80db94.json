{
  "id": "a8becc4c-e67b-459b-9e64-cda7da80db94",
  "title": "Apple's Illusion of Thinking Paper Explores Limits of Large Reasoning Models",
  "link": "https://www.infoq.com/news/2025/07/apple-illusion-thinking/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Apple Machine Learning Research published a paper titled \"The Illusion of Thinking,\" which investigates the abilities of Large Reasoning Models (LRMs) on a set of puzzles. As the complexity of the puzzles increases, the researchers found that LRMs encounter a \"collapse\" threshold where the models reduce their reasoning effort, indicating a limit to the models' scalability. By Anthony Alford",
  "author": "Anthony Alford",
  "published": "Tue, 01 Jul 2025 13:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Apple",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Anthony Alford",
  "length": 4036,
  "excerpt": "Apple Machine Learning Research published a paper titled \u0026quot;The Illusion of Thinking,\u0026quot; which investigates the abilities of Large Reasoning Models (LRMs) on a set of puzzles. As the complexity of the puz",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250605075448/apple-touch-icon.png",
  "text": "Apple Machine Learning Research published a paper titled The Illusion of Thinking, which investigates the abilities of Large Reasoning Models (LRMs) on a set of puzzles. As the complexity of the puzzles increases, the researchers found that LRMs encounter a \"collapse\" threshold where the models reduce their reasoning effort, indicating a limit to the models' scalability. For their experiments, Apple researchers chose four puzzle problems, including Tower of Hanoi, and a variety of LRMs and standard LLMs, including o3-mini and DeepSeek-R1. Each puzzle's complexity could be varied; for example, the Tower of Hanoi puzzle can have a variable number of disks. They found that as complexity increased, model behavior went through three regimes: in the first, with simple problems, both reasoning and non-reasoning models performed similarly well. In the second, medium complexity regime, the reasoning models with their Chain-of-Thought (CoT) inference performed better than LLMs. But in the high complexity regime, both groups' performance \"collapsed to zero.\" According to Apple,  In this study, we probe the reasoning mechanisms of frontier LRMs through the lens of problem complexity....Our findings reveal fundamental limitations in current models: despite sophisticated self-reflection mechanisms, these models fail to develop generalizable reasoning capabilities beyond certain complexity thresholds....These insights challenge prevailing assumptions about LRM capabilities and suggest that current approaches may be encountering fundamental barriers to generalizable reasoning. LRMs such as o3 and DeepSeek-R1 are LLMs that have been fine-tuned to generate step-by-step instructions for itself before producing a response to users; in essence, the models \"think out loud\" to produce better answers. This allows the models to outperform their \"standard\" LLM counterparts on many tasks, especially coding, mathematics, and science benchmarks. As part of their experiments, the Apple team analyzed these reasoning traces generated by the models. They noted that for simpler problems, the models would often \"overthink:\" the correct solution would appear early in the trace, but the models would continue to explore incorrect ideas. In medium complexity problems, however, the models would explore incorrect solutions before finding the correct one. Apple's paper sparked a wide debate in the AI community. Gary Marcus, a cognitive scientist and critic of the current state of AI, wrote about the research, saying: What the Apple paper shows, most fundamentally, regardless of how you define [Artificial General Intelligence (AGI)], is that LLMs are no substitute for good well-specified conventional algorithms. (They also can’t play chess as well as conventional algorithms, can’t fold proteins like special-purpose neurosymbolic hybrids, can’t run databases as well as conventional databases, etc.) Open source developer and AI commentator Simon Willison pointed out: I'm not interested in whether or not LLMs are the \"road to AGI\". I continue to care only about whether they have useful applications today, once you've understood their limitations. Reasoning LLMs are a relatively new and interesting twist on the genre. They are demonstrably able to solve a whole bunch of problems that previous LLMs were unable to handle, hence why we've seen a rush of new models from OpenAI and Anthropic and Gemini and DeepSeek and Qwen and Mistral....They're already useful to me today, whether or not they can reliably solve the Tower of Hanoi.... Apple acknowledges several limitations of their research, noting in particular that their experiments mostly relied on \"black box\" API calls, leaving them unable to examine the inner state of the models. They also agree that the use of puzzles means that their conclusions may not generalize to all reasoning domains. About the Author Anthony Alford",
  "image": "https://res.infoq.com/news/2025/07/apple-illusion-thinking/en/headerimage/generatedHeaderImage-1750604500202.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://machinelearning.apple.com/\"\u003eApple Machine Learning Research\u003c/a\u003e published a paper titled \u003ca href=\"https://machinelearning.apple.com/research/illusion-of-thinking\"\u003eThe Illusion of Thinking\u003c/a\u003e, which investigates the abilities of Large Reasoning Models (LRMs) on a set of puzzles. As the complexity of the puzzles increases, the researchers found that LRMs encounter a \u0026#34;collapse\u0026#34; threshold where the models reduce their reasoning effort, indicating a limit to the models\u0026#39; scalability.\u003c/p\u003e\n\n\u003cp\u003eFor their experiments, Apple researchers chose four puzzle problems, including \u003ca href=\"https://en.wikipedia.org/wiki/Tower_of_Hanoi\"\u003eTower of Hanoi\u003c/a\u003e, and a variety of LRMs and standard LLMs, including \u003ca href=\"https://www.infoq.com/news/2025/02/openai-o3-mini/\"\u003eo3-mini\u003c/a\u003e and \u003ca href=\"https://www.infoq.com/news/2025/02/deepseek-r1-release/\"\u003eDeepSeek-R1\u003c/a\u003e. Each puzzle\u0026#39;s complexity could be varied; for example, the Tower of Hanoi puzzle can have a variable number of disks. They found that as complexity increased, model behavior went through three \u003cem\u003eregimes\u003c/em\u003e: in the first, with simple problems, both reasoning and non-reasoning models performed similarly well. In the second, medium complexity regime, the reasoning models with their Chain-of-Thought (CoT) inference performed better than LLMs. But in the high complexity regime, both groups\u0026#39; performance \u0026#34;collapsed to zero.\u0026#34; According to Apple, \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eIn this study, we probe the reasoning mechanisms of frontier LRMs through the lens of problem complexity....Our findings reveal fundamental limitations in current models: despite sophisticated self-reflection mechanisms, these models fail to develop generalizable reasoning capabilities beyond certain complexity thresholds....These insights challenge prevailing assumptions about LRM capabilities and suggest that current approaches may be encountering fundamental barriers to generalizable reasoning.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eLRMs such as o3 and DeepSeek-R1 are LLMs that have been fine-tuned to generate step-by-step instructions for itself before producing a response to users; in essence, the models \u0026#34;think out loud\u0026#34; to produce better answers. This allows the models to outperform their \u0026#34;standard\u0026#34; LLM counterparts on many tasks, especially coding, mathematics, and science benchmarks.\u003c/p\u003e\n\n\u003cp\u003eAs part of their experiments, the Apple team analyzed these reasoning \u003cem\u003etraces\u003c/em\u003e generated by the models. They noted that for simpler problems, the models would often \u0026#34;overthink:\u0026#34; the correct solution would appear early in the trace, but the models would continue to explore incorrect ideas. In medium complexity problems, however, the models would explore incorrect solutions before finding the correct one.\u003c/p\u003e\n\n\u003cp\u003eApple\u0026#39;s paper sparked a wide debate in the AI community. Gary Marcus, a cognitive scientist and critic of the current state of AI, \u003ca href=\"https://garymarcus.substack.com/p/a-knockout-blow-for-llms\"\u003ewrote about the research\u003c/a\u003e, saying:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhat the Apple paper shows, most fundamentally, regardless of how you define [Artificial General Intelligence (AGI)], is that LLMs are no substitute for good well-specified conventional algorithms. (They also can’t play chess as well as conventional algorithms, can’t fold proteins like special-purpose neurosymbolic hybrids, can’t run databases as well as conventional databases, etc.)\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eOpen source developer and AI commentator \u003ca href=\"https://simonwillison.net/2025/Jun/15/viral-apple-reasoning-paper/\"\u003eSimon Willison pointed out\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eI\u0026#39;m not interested in whether or not LLMs are the \u0026#34;road to AGI\u0026#34;. I continue to care only about whether they have useful applications today, once you\u0026#39;ve understood their limitations. Reasoning LLMs are a relatively new and interesting twist on the genre. They are demonstrably able to solve a whole bunch of problems that previous LLMs were unable to handle, hence why we\u0026#39;ve seen a rush of new models from OpenAI and Anthropic and Gemini and DeepSeek and Qwen and Mistral....They\u0026#39;re already useful to me today, whether or not they can reliably solve the Tower of Hanoi....\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eApple acknowledges several limitations of their research, noting in particular that their experiments mostly relied on \u0026#34;black box\u0026#34; API calls, leaving them unable to examine the inner state of the models. They also agree that the use of puzzles means that their conclusions may not generalize to all reasoning domains.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Anthony-Alford\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAnthony Alford\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-07-01T00:00:00Z",
  "modifiedTime": null
}
