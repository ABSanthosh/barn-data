{
  "id": "43ac2378-781b-47db-ba2d-0255167e5077",
  "title": "Presentation: Enhance LLMsâ€™ Explainability and Trustworthiness With Knowledge Graphs",
  "link": "https://www.infoq.com/presentations/llm-knowledge-graphs/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Leann Chen explains how knowledge graphs enhance LLM-based systems by providing structured data as a ground truth. She demonstrates how this approach combats critical LLM challenges such as hallucinations and the \"lost-in-the-middle\" phenomenon, especially within RAG applications. She also highlights the struggles of vector-based LLMs with tasks like sorting and filtering. By Leann Chen",
  "author": "Leann Chen",
  "published": "Tue, 22 Jul 2025 13:31:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Large language models",
    "Transcripts",
    "QCon San Francisco 2024",
    "Artificial Intelligence",
    "AI, ML \u0026 Data Engineering",
    "presentation"
  ],
  "byline": "Leann Chen",
  "length": 29878,
  "excerpt": "Leann Chen discusses how knowledge graphs provide structured data to enhance LLM accuracy, tackling common challenges like hallucinations and the \u0026quot;lost-in-the-middle\u0026quot; phenomenon in RAG systems.",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250717101838/apple-touch-icon.png",
  "text": "Transcript Chen: How many of you have heard of knowledge graphs? How many of you, whether you've heard of knowledge graphs or not, are curious about explainability or trustworthiness in LLMs? It seems like more people are more aware of knowledge graphs. It seems like the knowledge graphs folks are doing marketing pretty well, because knowledge graphs actually were here already in 2012, by Google. It's a marketing term. Actually, I don't know how folks know about knowledge graphs or what type of definition you have in mind about knowledge graphs. You will see a lot of information on the internet saying, it somehow looks something like this. I borrowed this from Google, when they first introduced this concept of knowledge graphs in 2012, as I just mentioned, as the entities and relationships that you can see here, such as, if I want to know who painted Mona Lisa, we can see there's a relationship between Da Vinci and Mona Lisa. Later, I'm going to talk about why this is related, like why do knowledge graphs have something to do with LLM-based systems? The most important thing I want to first define, like what knowledge graph means to us or means to you. The TL;DR version of knowledge graph is basically just structured data with connections. Previously, some people would ask, what does knowledge graph mean? Why isn't it data graph? Why does it have to be knowledge? As you can see here, there's a lot of connections between the different data points, and we're actually going to leverage some of the insights within the connections here between data points. If later you step out of the session, and you probably will have other conversations with people, and you will have different definitions, but here I'm going to use this definition as structured data with connection. I'm actually going to put more emphasis on the structured data part. Also, I want to ask another question, how many of you are experimenting or building LLM-based systems? It could be a prototype or a toy project. I try to make this interactive. Can anyone share what are the top challenges you encounter in building any applications with LLMs? Nader: Hallucination. Example 1 - The Lost-In-The-Middle-Phenomenon of LLMs Chen: Exactly. Actually, the second example I'm going to share is some of the challenges or experience we're experimenting or we have encountered. Today, aside from some of the takeaways of knowledge graphs, I also want to share the takeaways of how we overcome some of the challenges we build with LLMs. Last year, when people talked about, they want to use LLMs to build some application, they say, yes, but LLMs hallucinate. Especially for folks who are trying to build some applications in their company, they need information to be accurate. As Nader, you mentioned, hallucination is one of the challenges we encounter with LLMs, even though we could design a custom prompt template, we can fine-tune LLMs. I don't know if this relates to some of the folks here, but even if we try to tweak these prompt templates, it would somehow fix some of the bugs. Later on, some other unexpected challenges will still pop up. That's a very iterative loop for us and very challenging. Also, the challenge we solve with LLMs is the lost-in-the-middle phenomenon. Are people familiar with the lost-in-the-middle phenomenon of LLMs? That's great because I'm just going to pull this paper here. Basically, it's research around one year ago or two years ago, LLMs have this memory constraint. As you can see here, the right-hand side, they actually compare lots of different language models. For large language models, they usually perform better in the very beginning of the context you fed in or the very end. They somehow just lost or forget some of the information in the middle. This figure is for GPT-3.5. They also did another comparison, you can see pretty much all language models. All language models have this phenomenon where if you want to ask something, even if it's in the middle of the paragraph and you want to ask something, it just said, I don't remember, or, I don't know. If this paper is actually not that clear enough, I'll show one other example. We previously built a RAG system. Are people familiar with RAG? RAG is basically a summary. You want LLMs to only give you outputs based on your custom knowledge base, like based on the data that you give it. It's not like hallucinating something it doesn't know. What we build is we use Hayao Miyazaki, who's a very famous Japanese director's Wikipedia page, because Wikipedia has cleaner data compared to other PDF pages that you will find on the internet. This is something funny. We asked our RAG system, the data is a Wikipedia page about, tell me about Joe Hisaishi. Joe Hisaishi is another composer, the music that you hear in Hayao Miyazaki's movie. It said, I apologize, but the provided context does not contain any information about Joe Hisaishi. If I further ask, is Joe Hisaishi a colleague of Hayao Miyazaki? It says, based on the provided content, it does not explicitly say whether Joe Hisaishi is a colleague of Hayao Miyazaki. If I go to actually Hayao Miyazaki's Wikipedia page, and I search Joe Hisaishi, you can see it's clearly mentioned. I load this entire Wikipedia page into the RAG system. What's happening, like why it could not recognize. The problem with LLM is, I saw this phenomenon and somehow even, we don't know whether it's like LLM is hallucinating or the lost-in-the-middle effect is appearing. It's unexplainable, like why is it happening? Even if I ask the context of, if Joe Hisaishi is in the context, and it recognized, yes, but still it doesn't recognize that. Example 2 - Downstream Effects in LLM-Based Systems Another example I want to share here is the downstream effects in LLM-based systems. We're using another Wikipedia page, we got Elon Musk just to play around, and for clean data. Before we go into another example, I want to say what downstream effects mean here. For example, if you're building a RAG system, you have to first clean your data, have your data ready, and then you need to tokenize and choose an embedded model, choose a language model, choose your chunking strategy or orchestration. For example, you're probably using bs.py, or other custom orchestration templates. Then retrieval techniques and generation. There's a lot of nuances in all of these methods. This session, we're not going into details, but basically, this is pretty much what you will be encountering when you're building something like RAG. Let me show you an example. Previously, we did this experiment. For the first question, we used Nomic embedding and Llama 3, back when Llama 3's 70B first got published. We asked, did Elon Musk co-found SpaceX with other founders? Even if it has the right data, it said, yes. The correct answer is no. If you see the first question, you want to see, is LLM hallucinating, even if it's in RAG? RAG is supposed to provide the ground truth, the correct data, it should follow that. The first question, it's hallucinating. It's a yes instead of no. You can see the second example is funny. The question is supposed to be, who are the other founders Elon Musk co-founded SpaceX with? The answer is PayPal. Is PayPal a person? Probably. It shouldn't be PayPal. That's why there's hallucination in RAG still, because last year, around this time, we were hyping RAG. I was like, yes, RAG can reduce or solve hallucination. Why are we seeing this here? Why previously I showed you the LLM pipeline with lots of different components, is because the choice of different embedding models or language models actually matter. You probably recall this example I just provided. This is based on nomic embedding and Llama 3 70B. It hallucinates, they say yes. However, if I swap out a different embedding model, which is actually this ada-002 embedding model from OpenAI, plus Llama 3, and it actually provided the correct answer. It's interesting, but funny. It's like, we can't really say based on the different components, and we have different choices that lead to various possibilities or output that we're saying. We make a simple or definite conclusion of what is working or what is not, because as you still remember, there are different variables in this pipeline. Another thing I want to mention here, it's not what I'm going to address in this session, it's like, people are talking a lot about evaluations. That is very important. Right now, with LLM pipelines people are struggling to find, the answers are pretty qualitative, how do we design our evals? It's a broader discussion we all should have. It's pretty nuanced and challenging. I hope, after you've seen these examples, we have to have this mindset, yes, how do we evaluate? Because there are so many possible outputs based on one single tweak of the component. Later, I'm going to go back to the challenge that you just see, because I haven't provided a solution yet. Another thing I want to also mention, the LLM-based application, I assume a lot of you are using the mainstream approaches. It's vector-based. Vector-based means you turn those unstructured text data, as we just saw from the Wikipedia page, and you turn them into embeddings and vector and do those vector similarity search. There's another section about vector-based application. I just want to give you just a brief understanding of the mainstream RAG applications is vector-based. Sorting, Filtering, and Aggregation, with Unstructured Data Where vector-based LLM applications will struggle is tasks such as sorting, filtering, and aggregation. What does this mean? This is another experiment we did, which you can see that here, this huge visualization, you see the data points. Because in the vector-based approach, like the words or the phrases, they are converted into a list of numbers in high-dimensional space, which human eyes can really understand. What we do with UMAP, it's like a data science visualization technique. We do this dimensionality reduction technique to map it into 2D, so human's eye, we can understand better. The main point I'm trying to get across in this is, this is the vector-based approach I'm talking about, which article has the lowest sentiment? Basically, what I need to do with this query, because I assign different sentiment scores in those articles, is I need to compare the sentiment scores from high to low. However, if I do it with the vector-based search, which is like, I ask LLMs about this question, LLM is going to do the vector similarity search. The interesting part is, the correct answer is actually the yellow part. It's pretty far away from this question. The red point is the sentiment question, which article has the lowest sentiment? Based on the vector similarity approach, what LLMs will actually retrieve are the green dots here, which are actually not correct. Because what vector similarity does is it feels like, ok, these articles, I feel like this is closer, vector-wise, compared to your question. If you ask this question to LLMs, it will actually retrieve the green dots, as I mentioned, not correct, instead of the more far away data points here. I have this entire Jupyter Notebook attached on the side, so you guys can fact-check it later. This is something very interesting too, like, what's the latest news? Different from the previous one, it's sorting from the highest to lowest. At least it has an order type of thing. If you go into a vector-based approach, you will find latest actually can't really map into any of those high-dimensional space, because for LLMs, latest is a more comparison thing. It doesn't understand, it has a chronological timeline of that type of thing. I recommend, you guys, if you're interested, you could play around with the different embedding models and see what latest give you how LLMs interpret this. As you can see, the same question is here, what's the latest news? The top five similar, interpreted by LLMs by what it thinks is similar, it's these data points instead of the correct ones, which are farther away. For these two challenges, while there would be metadata filtering, those also require a lot of nuanced discussions. I'm just pointing out these two, the more challenging questions. Right now, people are trying to solve too. I'm not going to cover the solution of these two in particular, just to let you guys know there's the challenge around here. How Can Structured Information Help with More Accurate Information? I do want to go back to one of the previous challenges I showed. I mentioned knowledge graph is structured data with connections. How can structured information help with more accurate outputs or information? Back to example one. If you still remember, our first example is this one, tell me about Joe Hisaishi. It's in the context, but the LLMs somehow just get lost in the middle. Now I'm bringing in the structured data from knowledge graphs into our system and let LLMs know, this is the ground truth you need to follow. What you're going to see, like you see here, the full content, like who worked with Hayao Miyazaki. In this context, I'm using a graph database. Not all knowledge graphs need to be in a graph database. It's just in this context, I'm using graph database. It has this relationship being defined saying, ok, it does know Hayao Miyazaki has this work relationship with the other people. These are the people being returned. Of course, Joe Hisaishi is being returned. If I pass this information in combination to our vector-based approach, you will see this one, the final_ans_colleague here. This is a more enhanced and rich answer. It actually contained what we want here compared to the original answer, the bottom one, that is the original answer that is not correct and did not include Joe Hisaishi. In case this example is not providing that concrete understanding of what knowledge graph does, I want to further go back to this image and explain a little bit more. Knowledge graph what you can see is actually a map of fact. What LLMs can reference is, if I ask LLMs, where is Mona Lisa? What LLMs can do is like, ok, Mona Lisa, there's a relationship. Mona Lisa is in the museum booth. The museum is located in Paris. Paris is in France. LLMs will be able to see, there's different relationships, and traverse the relationships and find out that Mona Lisa is in France. This is just one of the examples. If we go back to the example that I showed you, this is actually what happens here, too, is it first filters out and finds out, identify Hayao Miyazaki. It figures out the relationships around Miyazaki and returns the structured information that you need. We can ask LLMs to further ground its answer with this structured ground truth. Live Demo Here's actually a MiniRAG, we call it a vector and graph-based RAG. I'm going to later show you what it means exactly. What I'm going to show is how this demo can make this illustration more concretely about, how does knowledge graph make LLM pipelines or outputs more explainable. This application was previously built for a marketing analysis. It's like, some of the market researchers, if they want to do some trend analysis, such as pull in real-time news about OpenAI. Actually, in this MiniRAG system, I have 10 recent OpenAI news in my knowledge base. Back then, we were thinking, if we just want to know about recent news, then I can literally just use Perplexity or OpenAI. It makes no difference. It's not something very unique. We further think about, if I'm reading the 10 articles or whatever number of articles I'm reading, and I want to know some of the companies mentioned in these articles and more holistically their information. For example, I'll show you a little bit about what's happening in the graph database that we're using here. I have 10 articles and their relationships. The entities and relationships are currently in this database, and it looks huge. What is this? It's not very straightforward. I just wanted to let you know we have 10 articles and the organization, such as OpenAI, in our database. In this application, we have two RAG modes. One is vector only. It's only based on the unstructured data based on the 10 articles we imported. Actually, let me just pull up an article here to let you guys know more specifically what I'm talking about. This is one of the nodes indicating an article. I click this link. It's the entire article about the next half a year to a year about AGI. Let's see. This article is about OpenAI. If I want, I can first do a basic graphing, it's like, I ask, summarize this article for me. This is live. This is a very basic application that you see in RAG. It's just like pulling in what you have in your knowledge base and summarize it for you as you can do in ChatGPT. Here we have this context data. You see, the race to artificial general intelligence. It's literally this text here. It's pulling in this data. However, when I'm reading this news and it mentions about OpenAI or Google, and spontaneously, I want to know comprehensive information about OpenAI such as the industry, competitors, or supplier. If I further ask, tell me about OpenAI such as industry, suppliers, or competitors. We're still in this vector-only mode. You would say, yes, it looks correct. The answer is correct. If I pulled in the context data, it's quite long, so we cannot actually inspect the context data. What I want to say here is like, the context data is just from the article which says nothing about OpenAI. The answer that you're seeing here is actually from the pre-trained data, the pre-trained pattern in LLMs. It has nothing to do with whether you have a RAG or not. Here, the answer, it has nothing to do with what we have in our RAG. It's just in their pre-trained data. We can't do any verification of whether this is correct or not because this goes back to the same hallucination problem that we saw in LLMs. If LLMs follow their pre-trained patterns, like from their training data, we can't verify if it's hallucinating or not. Right now, what I'm going to do is, I have this other vector plus KG mode. It's combining vector, similarity search, and structured data, knowledge graph. We ask the same question here. I'll copy this. I did not check the include chat history mode so it doesn't memorize. Every answer is independent. I'm going to ask the same question. Here is the context data. This is exactly the same query. This is previously based on the vector mode. It's all based on unstructured data. This vector plus KG mode, it has some more structured data such as like the partnership, the industry. It's classified as a research industry, subsidiary, founder, competitor, a very long list of suppliers. Also, it pulls in unstructured data, which means it references both, like if you have a more defined relationship, such as these other metadata, it will reference that. It will also reference the information from unstructured data. The LLMs will decide which is relevant to your query. This is what structured data can do for you, is like, LLMs outputs are undeterministic and we need deterministic data or information as our ground truth to tell LLM, this is the map of facts, follow those and tell me about how to answer my question. This is a mini demo that I wanted to share with you guys. This is not perfect. It just shows some of the choice that we can see, or even play around and try it out. I would definitely love to interact with you more to see, where are the other downsides or like not perfect enough and we can improve on that. Conclusion I don't know if some of you have these questions or doubt or even confusion about, do I need a graph database for a knowledge graph? Because knowledge graph, there's a graph word in this term. Do I need a graph database for this? Does it always have to look something like this? Actually, no, because if you really pull out the data that we have here, I'm going to show you, it looks like this. It could be a dictionary data. It's actually just text data, but it's structured. Some people would just import the structured data into a graph database. It really depends on your use case. For me, when I'm asked about this question, I would tell people, or based on some of the experts in the domain, I was like, it really depends on your use case. What problems are you trying to solve? My answer is, when we talk about knowledge graphs, it's not really about the graph. It's more about how you can use structured data to solve or enhance your application. Questions and Answers Participant 1: When you have the graph data as part of the RAG, how does the LLM model differentiate between the data from your knowledge graph versus from the vector database? Chen: This example I just showed, it has structured data and unstructured. For example, the LLM will recognize, ok, like OpenAI has competitor or supplier, that data is irrelevant to my query because it's still doing a vector similarity search. Is that what you're asking? Participant 1: No, my question is, how does your implementation treat the data from the vector database and then the knowledge graph differently so that you make a difference in the outcome? Chen: It's not really treating the data differently because at the end of the day, these are all text data. The previous example where I only show vector-based, it's only based on vector similarity search. There's only one method in there. If I incorporate structured data in it, it's like a metadata. It's like more information, LLMs can reference, and to decide, this information is helpful for your question. Participant 1: In other words, when you do your prompt, you put both the information from the vector search and your graph data as part of your prompt. Chen: Prompt? Yes, that could be one way. That could be it. Participant 1: If that's the case, so the LLM has the inputs from both the vector data and also the data from the knowledge graph. It's probably a more fundamental question about how LLM works internally. How do LLM, based on your experience, process the data from the vector database versus from the knowledge graph? Chen: I'll actually show another example. In this example, I'm using a graph database. Let me show here. This is like a graph. The articles we imported are actually chunked, right now, because of the context window limitation, we do need to chunk the different paragraphs. You can see, this is one of the chunked nodes and here are the embeddings. What it's doing here is like, it's still doing vector similarity search to find that node and see if there's information based on the similarity search and answer your question. This article's in our database. It summarized this, not because of the graph, it's because of vector similarity search. Participant 1: When you have the graph, how does that work differently? Chen: Here, it's like, the graph is serving the ground truth. It's because it has relationship. It's serving as a ground truth for LLMs to follow. Here, tell me about OpenAI, and I'm using this like only vector mode. This answer is based on LLM's pre-trained memory. It has no ground truth. Based on our definition, it's not in our knowledge base. If I switch to the vector plus KG mode, the knowledge graph mode, right now, the answer is following the structured data, like the structured relationship defined in our knowledge base. Participant 1: In other words, you just feed in the LLM model more information from the knowledge graph, that's it? Chen: Yes, because that's the ground truth. We have to provide that. Participant 2: Let's say you have a graph database with a huge graph, how do you map the natural language question made by the user to a subset of that graph? Chen: You mean like how do LLMs know? Like, it has a question and start to find information in the graph database? Participant 2: Exactly. Chen: For example, summarize this article, this is a question. Vector-based approaches convert this question into embeddings and you do vector similarity search. The chunked data here, yes, you see, this article, it has different chunks here and the embeddings, and it will find the vector similarity based on these embeddings as you see here. Participant 2: I'm guessing you get through the embeddings to map to actual nodes, but then nodes in the graph database have lots of relationships. How do you run the relationships you actually pass to the model so that you don't go beyond the context window? Chen: In this case, we're using Neo4j and they have their Cypher query. Right now, they do have the LLM to Cypher thing. Right now, if you want more complex queries, we still need human in the loop, but that question is actually just text to Cypher. It's pretty much like, you have your natural language and it's translated into Python or SQL, and to find those relationships. Participant 3: I have a question about that lost-in-the-middle phenomenon. How does the knowledge graph help with that? Because if I understand correctly, you have structured data, but you still put it in the context. In case there's the answer or the important thing in the middle of that context, although structured, it can still be lost? Chen: It relates to another part of the knowledge graph construction, like the process. Before LLMs, there are already natural language processing techniques, like extract entities and relationships in the articles as we saw in the graph. In that case, it could be complementary to the lost-in-the-middle phenomenon, because the vector-based, everything just gets converted into embeddings. For humans, it's a little bit hard to evaluate what's being converted, and where is it in our knowledge base. The NLP technique, it's more deterministic as a ground truth, say, this is our ground truth, and you do your vector search, that's fine, but please follow the ground truth in whatever is provided from our NLP approach. Participant 4: Regarding the nodes of the graph, how are the embeddings calculated? In the embedding processing, are all the arcs included, so the relationships, and so that's the result of the embeddings of the node? Chen: This is more related to the retrieval part, let's say if you're building a RAG system. What I just show based on the previous question is like the nodes, they are text, and basically, you can convert into embeddings. You can ask LLMs to first find the nodes based on the embeddings, and find the surrounding relationships. Or you can pretty much do, based on your database, such as Postgres or SQL, to find the relationship with their own language. There are multiple approaches, and it doesn't have to be either/or. Participant 5: About the relationships, I was curious about the previous slide, if you can. The one from Google, of the graph, which was the Mona Lisa one. I was checking the relationships. I was checking like James has lived in a Tour Eiffel. Now, living in a Tour Eiffel seems weird. I don't think he was living in it. I was thinking if we could get some ambiguous relationships in the structured graph, how we could solve this? Chen: This slide, I borrowed from Google, and I think they probably make a joke about that. To be fair, something I did not stress enough in this session is like knowledge graph, just as I mentioned, it depends on your use case. You can use the generic NLP method to find the relationship, but we still need human in the loop to evaluate, is this something you want, because it has to solve your needs. Knowledge graph is like, if you want to have good ground truth for LLMs, it's basically an indicator of data quality. If that is wrong, like James has lived in Tour Eiffel or something, then, sure, we eliminate that. That's not correct. Participant 5: We need also a human to verify the data. Chen: Even right now we have LLMs, everything is still about data, like training models, fine-tuning models. It's still like, how good is your data. The data quality thing, it's a challenging part, and PDS, but it's still very important. We still need human, at least, unless AGI comes. Participant 6: In the example that you have in your vector database, is there few articles in the vector database or is it just the article that was shown before was actually having the embeddings in there? The reason for that question is like, the context that's provided, it's having more information than just from the particular article itself. Does it have more articles or it's just one article in there? Chen: I did import 10 recent articles about OpenAI. The nodes, like this is one article, this is another, third, fourth, fifth, something like that. I did import 10 articles. Participant 7: My question is more around the change that can happen in a relationship. If something changes, is it always like a live look at the data, or is there a process that needs to run to make sure that the graph is up to date and the information that we are requesting is up to date as well? Chen: This relates more to the natural language processing techniques. Of course, we want the data to be updated, real-time. Some of the NLP techniques definitely can do that. Every time you want to change your data or something, it's better, like humans, you have to see, ok, is this the direction I'm changing? Because we rely on LLMs to do all the things. I don't have you guys play enough with this, but something I found funny about LLMs is, sometimes you ask a question and you ask two times or three times, and even you set the temperature to 0, and sometimes it'll give you different answers. It still has this undeterministic nature. If you want LLMs to handle your data, we probably should keep in mind like how consistent that is. See more presentations with transcripts",
  "image": "https://res.infoq.com/presentations/llm-knowledge-graphs/en/card_header_image/leann-chen-twitter-card-1752655617585.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"presentationNotes\"\u003e\n                                    \u003ch2\u003eTranscript\u003c/h2\u003e\n\n\u003cp\u003eChen: How many of you have heard of knowledge graphs? How many of you, whether you\u0026#39;ve heard of knowledge graphs or not, are curious about explainability or trustworthiness in LLMs? It seems like more people are more aware of knowledge graphs. It seems like the knowledge graphs folks are doing marketing pretty well, because knowledge graphs actually were here already in 2012, by Google. It\u0026#39;s a marketing term. Actually, I don\u0026#39;t know how folks know about knowledge graphs or what type of definition you have in mind about knowledge graphs. You will see a lot of information on the internet saying, it somehow looks something like this. I borrowed this from Google, when they first introduced this concept of knowledge graphs in 2012, as I just mentioned, as the entities and relationships that you can see here, such as, if I want to know who painted Mona Lisa, we can see there\u0026#39;s a relationship between Da Vinci and Mona Lisa.\u003c/p\u003e\n\n\u003cp\u003eLater, I\u0026#39;m going to talk about why this is related, like why do knowledge graphs have something to do with LLM-based systems? The most important thing I want to first define, like what knowledge graph means to us or means to you. The TL;DR version of knowledge graph is basically just structured data with connections. Previously, some people would ask, what does knowledge graph mean? Why isn\u0026#39;t it data graph? Why does it have to be knowledge? As you can see here, there\u0026#39;s a lot of connections between the different data points, and we\u0026#39;re actually going to leverage some of the insights within the connections here between data points.\u003c/p\u003e\n\n\u003cp\u003eIf later you step out of the session, and you probably will have other conversations with people, and you will have different definitions, but here I\u0026#39;m going to use this definition as structured data with connection. I\u0026#39;m actually going to put more emphasis on the structured data part. Also, I want to ask another question, how many of you are experimenting or building LLM-based systems? It could be a prototype or a toy project. I try to make this interactive. Can anyone share what are the top challenges you encounter in building any applications with LLMs?\u003c/p\u003e\n\n\u003cp\u003eNader: Hallucination.\u003c/p\u003e\n\n\u003ch2\u003eExample 1 - The Lost-In-The-Middle-Phenomenon of LLMs\u003c/h2\u003e\n\n\u003cp\u003eChen: Exactly. Actually, the second example I\u0026#39;m going to share is some of the challenges or experience we\u0026#39;re experimenting or we have encountered. Today, aside from some of the takeaways of knowledge graphs, I also want to share the takeaways of how we overcome some of the challenges we build with LLMs. Last year, when people talked about, they want to use LLMs to build some application, they say, yes, but LLMs hallucinate. Especially for folks who are trying to build some applications in their company, they need information to be accurate. As Nader, you mentioned, hallucination is one of the challenges we encounter with LLMs, even though we could design a custom prompt template, we can fine-tune LLMs. I don\u0026#39;t know if this relates to some of the folks here, but even if we try to tweak these prompt templates, it would somehow fix some of the bugs.\u003c/p\u003e\n\n\u003cp\u003eLater on, some other unexpected challenges will still pop up. That\u0026#39;s a very iterative loop for us and very challenging. Also, the challenge we solve with LLMs is the lost-in-the-middle phenomenon. Are people familiar with the lost-in-the-middle phenomenon of LLMs? That\u0026#39;s great because I\u0026#39;m just going to pull this paper here. Basically, it\u0026#39;s research around one year ago or two years ago, LLMs have this memory constraint.\u003c/p\u003e\n\n\u003cp\u003eAs you can see here, the right-hand side, they actually compare lots of different language models. For large language models, they usually perform better in the very beginning of the context you fed in or the very end. They somehow just lost or forget some of the information in the middle. This figure is for GPT-3.5. They also did another comparison, you can see pretty much all language models. All language models have this phenomenon where if you want to ask something, even if it\u0026#39;s in the middle of the paragraph and you want to ask something, it just said, I don\u0026#39;t remember, or, I don\u0026#39;t know.\u003c/p\u003e\n\n\u003cp\u003eIf this paper is actually not that clear enough, I\u0026#39;ll show one other example. We previously built a RAG system. Are people familiar with RAG? RAG is basically a summary. You want LLMs to only give you outputs based on your custom knowledge base, like based on the data that you give it. It\u0026#39;s not like hallucinating something it doesn\u0026#39;t know. What we build is we use Hayao Miyazaki, who\u0026#39;s a very famous Japanese director\u0026#39;s Wikipedia page, because Wikipedia has cleaner data compared to other PDF pages that you will find on the internet. This is something funny. We asked our RAG system, the data is a Wikipedia page about, tell me about Joe Hisaishi.\u003c/p\u003e\n\n\u003cp\u003eJoe Hisaishi is another composer, the music that you hear in Hayao Miyazaki\u0026#39;s movie. It said, I apologize, but the provided context does not contain any information about Joe Hisaishi. If I further ask, is Joe Hisaishi a colleague of Hayao Miyazaki? It says, based on the provided content, it does not explicitly say whether Joe Hisaishi is a colleague of Hayao Miyazaki. If I go to actually Hayao Miyazaki\u0026#39;s Wikipedia page, and I search Joe Hisaishi, you can see it\u0026#39;s clearly mentioned. I load this entire Wikipedia page into the RAG system. What\u0026#39;s happening, like why it could not recognize. The problem with LLM is, I saw this phenomenon and somehow even, we don\u0026#39;t know whether it\u0026#39;s like LLM is hallucinating or the lost-in-the-middle effect is appearing. It\u0026#39;s unexplainable, like why is it happening? Even if I ask the context of, if Joe Hisaishi is in the context, and it recognized, yes, but still it doesn\u0026#39;t recognize that.\u003c/p\u003e\n\n\u003ch2\u003eExample 2 - Downstream Effects in LLM-Based Systems\u003c/h2\u003e\n\n\u003cp\u003eAnother example I want to share here is the downstream effects in LLM-based systems. We\u0026#39;re using another Wikipedia page, we got Elon Musk just to play around, and for clean data. Before we go into another example, I want to say what downstream effects mean here. For example, if you\u0026#39;re building a RAG system, you have to first clean your data, have your data ready, and then you need to tokenize and choose an embedded model, choose a language model, choose your chunking strategy or orchestration.\u003c/p\u003e\n\n\u003cp\u003eFor example, you\u0026#39;re probably using bs.py, or other custom orchestration templates. Then retrieval techniques and generation. There\u0026#39;s a lot of nuances in all of these methods. This session, we\u0026#39;re not going into details, but basically, this is pretty much what you will be encountering when you\u0026#39;re building something like RAG. Let me show you an example. Previously, we did this experiment. For the first question, we used Nomic embedding and Llama 3, back when Llama 3\u0026#39;s 70B first got published. We asked, did Elon Musk co-found SpaceX with other founders? Even if it has the right data, it said, yes. The correct answer is no.\u003c/p\u003e\n\n\u003cp\u003eIf you see the first question, you want to see, is LLM hallucinating, even if it\u0026#39;s in RAG? RAG is supposed to provide the ground truth, the correct data, it should follow that. The first question, it\u0026#39;s hallucinating. It\u0026#39;s a yes instead of no. You can see the second example is funny. The question is supposed to be, who are the other founders Elon Musk co-founded SpaceX with? The answer is PayPal. Is PayPal a person? Probably. It shouldn\u0026#39;t be PayPal. That\u0026#39;s why there\u0026#39;s hallucination in RAG still, because last year, around this time, we were hyping RAG. I was like, yes, RAG can reduce or solve hallucination. Why are we seeing this here? Why previously I showed you the LLM pipeline with lots of different components, is because the choice of different embedding models or language models actually matter. You probably recall this example I just provided. This is based on nomic embedding and Llama 3 70B. It hallucinates, they say yes.\u003c/p\u003e\n\n\u003cp\u003eHowever, if I swap out a different embedding model, which is actually this ada-002 embedding model from OpenAI, plus Llama 3, and it actually provided the correct answer. It\u0026#39;s interesting, but funny. It\u0026#39;s like, we can\u0026#39;t really say based on the different components, and we have different choices that lead to various possibilities or output that we\u0026#39;re saying. We make a simple or definite conclusion of what is working or what is not, because as you still remember, there are different variables in this pipeline.\u003c/p\u003e\n\n\u003cp\u003eAnother thing I want to mention here, it\u0026#39;s not what I\u0026#39;m going to address in this session, it\u0026#39;s like, people are talking a lot about evaluations. That is very important. Right now, with LLM pipelines people are struggling to find, the answers are pretty qualitative, how do we design our evals? It\u0026#39;s a broader discussion we all should have. It\u0026#39;s pretty nuanced and challenging. I hope, after you\u0026#39;ve seen these examples, we have to have this mindset, yes, how do we evaluate? Because there are so many possible outputs based on one single tweak of the component.\u003c/p\u003e\n\n\u003cp\u003eLater, I\u0026#39;m going to go back to the challenge that you just see, because I haven\u0026#39;t provided a solution yet. Another thing I want to also mention, the LLM-based application, I assume a lot of you are using the mainstream approaches. It\u0026#39;s vector-based. Vector-based means you turn those unstructured text data, as we just saw from the Wikipedia page, and you turn them into embeddings and vector and do those vector similarity search. There\u0026#39;s another section about vector-based application. I just want to give you just a brief understanding of the mainstream RAG applications is vector-based.\u003c/p\u003e\n\n\u003ch2\u003eSorting, Filtering, and Aggregation, with Unstructured Data\u003c/h2\u003e\n\n\u003cp\u003eWhere vector-based LLM applications will struggle is tasks such as sorting, filtering, and aggregation. What does this mean? This is another experiment we did, which you can see that here, this huge visualization, you see the data points. Because in the vector-based approach, like the words or the phrases, they are converted into a list of numbers in high-dimensional space, which human eyes can really understand. What we do with UMAP, it\u0026#39;s like a data science visualization technique. We do this dimensionality reduction technique to map it into 2D, so human\u0026#39;s eye, we can understand better. The main point I\u0026#39;m trying to get across in this is, this is the vector-based approach I\u0026#39;m talking about, which article has the lowest sentiment? Basically, what I need to do with this query, because I assign different sentiment scores in those articles, is I need to compare the sentiment scores from high to low.\u003c/p\u003e\n\n\u003cp\u003eHowever, if I do it with the vector-based search, which is like, I ask LLMs about this question, LLM is going to do the vector similarity search. The interesting part is, the correct answer is actually the yellow part. It\u0026#39;s pretty far away from this question. The red point is the sentiment question, which article has the lowest sentiment? Based on the vector similarity approach, what LLMs will actually retrieve are the green dots here, which are actually not correct. Because what vector similarity does is it feels like, ok, these articles, I feel like this is closer, vector-wise, compared to your question. If you ask this question to LLMs, it will actually retrieve the green dots, as I mentioned, not correct, instead of the more far away data points here. I have this entire Jupyter Notebook attached on the side, so you guys can fact-check it later.\u003c/p\u003e\n\n\u003cp\u003eThis is something very interesting too, like, what\u0026#39;s the latest news? Different from the previous one, it\u0026#39;s sorting from the highest to lowest. At least it has an order type of thing. If you go into a vector-based approach, you will find latest actually can\u0026#39;t really map into any of those high-dimensional space, because for LLMs, latest is a more comparison thing. It doesn\u0026#39;t understand, it has a chronological timeline of that type of thing. I recommend, you guys, if you\u0026#39;re interested, you could play around with the different embedding models and see what latest give you how LLMs interpret this.\u003c/p\u003e\n\n\u003cp\u003eAs you can see, the same question is here, what\u0026#39;s the latest news? The top five similar, interpreted by LLMs by what it thinks is similar, it\u0026#39;s these data points instead of the correct ones, which are farther away. For these two challenges, while there would be metadata filtering, those also require a lot of nuanced discussions. I\u0026#39;m just pointing out these two, the more challenging questions. Right now, people are trying to solve too. I\u0026#39;m not going to cover the solution of these two in particular, just to let you guys know there\u0026#39;s the challenge around here.\u003c/p\u003e\n\n\u003ch2\u003eHow Can Structured Information Help with More Accurate Information?\u003c/h2\u003e\n\n\u003cp\u003eI do want to go back to one of the previous challenges I showed. I mentioned knowledge graph is structured data with connections. How can structured information help with more accurate outputs or information? Back to example one. If you still remember, our first example is this one, tell me about Joe Hisaishi. It\u0026#39;s in the context, but the LLMs somehow just get lost in the middle. Now I\u0026#39;m bringing in the structured data from knowledge graphs into our system and let LLMs know, this is the ground truth you need to follow. What you\u0026#39;re going to see, like you see here, the full content, like who worked with Hayao Miyazaki. In this context, I\u0026#39;m using a graph database. Not all knowledge graphs need to be in a graph database. It\u0026#39;s just in this context, I\u0026#39;m using graph database. It has this relationship being defined saying, ok, it does know Hayao Miyazaki has this work relationship with the other people. These are the people being returned. Of course, Joe Hisaishi is being returned.\u003c/p\u003e\n\n\u003cp\u003eIf I pass this information in combination to our vector-based approach, you will see this one, the final_ans_colleague here. This is a more enhanced and rich answer. It actually contained what we want here compared to the original answer, the bottom one, that is the original answer that is not correct and did not include Joe Hisaishi. In case this example is not providing that concrete understanding of what knowledge graph does, I want to further go back to this image and explain a little bit more. Knowledge graph what you can see is actually a map of fact. What LLMs can reference is, if I ask LLMs, where is Mona Lisa? What LLMs can do is like, ok, Mona Lisa, there\u0026#39;s a relationship. Mona Lisa is in the museum booth. The museum is located in Paris. Paris is in France. LLMs will be able to see, there\u0026#39;s different relationships, and traverse the relationships and find out that Mona Lisa is in France. This is just one of the examples.\u003c/p\u003e\n\n\u003cp\u003eIf we go back to the example that I showed you, this is actually what happens here, too, is it first filters out and finds out, identify Hayao Miyazaki. It figures out the relationships around Miyazaki and returns the structured information that you need. We can ask LLMs to further ground its answer with this structured ground truth.\u003c/p\u003e\n\n\u003ch2\u003eLive Demo\u003c/h2\u003e\n\n\u003cp\u003eHere\u0026#39;s actually a MiniRAG, we call it a vector and graph-based RAG. I\u0026#39;m going to later show you what it means exactly. What I\u0026#39;m going to show is how this demo can make this illustration more concretely about, how does knowledge graph make LLM pipelines or outputs more explainable. This application was previously built for a marketing analysis. It\u0026#39;s like, some of the market researchers, if they want to do some trend analysis, such as pull in real-time news about OpenAI. Actually, in this MiniRAG system, I have 10 recent OpenAI news in my knowledge base. Back then, we were thinking, if we just want to know about recent news, then I can literally just use Perplexity or OpenAI. It makes no difference. It\u0026#39;s not something very unique. We further think about, if I\u0026#39;m reading the 10 articles or whatever number of articles I\u0026#39;m reading, and I want to know some of the companies mentioned in these articles and more holistically their information.\u003c/p\u003e\n\n\u003cp\u003eFor example, I\u0026#39;ll show you a little bit about what\u0026#39;s happening in the graph database that we\u0026#39;re using here. I have 10 articles and their relationships. The entities and relationships are currently in this database, and it looks huge. What is this? It\u0026#39;s not very straightforward. I just wanted to let you know we have 10 articles and the organization, such as OpenAI, in our database. In this application, we have two RAG modes. One is vector only. It\u0026#39;s only based on the unstructured data based on the 10 articles we imported. Actually, let me just pull up an article here to let you guys know more specifically what I\u0026#39;m talking about. This is one of the nodes indicating an article. I click this link. It\u0026#39;s the entire article about the next half a year to a year about AGI. Let\u0026#39;s see. This article is about OpenAI.\u003c/p\u003e\n\n\u003cp\u003eIf I want, I can first do a basic graphing, it\u0026#39;s like, I ask, summarize this article for me. This is live. This is a very basic application that you see in RAG. It\u0026#39;s just like pulling in what you have in your knowledge base and summarize it for you as you can do in ChatGPT. Here we have this context data. You see, the race to artificial general intelligence. It\u0026#39;s literally this text here. It\u0026#39;s pulling in this data. However, when I\u0026#39;m reading this news and it mentions about OpenAI or Google, and spontaneously, I want to know comprehensive information about OpenAI such as the industry, competitors, or supplier. If I further ask, tell me about OpenAI such as industry, suppliers, or competitors. We\u0026#39;re still in this vector-only mode. You would say, yes, it looks correct. The answer is correct.\u003c/p\u003e\n\n\u003cp\u003eIf I pulled in the context data, it\u0026#39;s quite long, so we cannot actually inspect the context data. What I want to say here is like, the context data is just from the article which says nothing about OpenAI. The answer that you\u0026#39;re seeing here is actually from the pre-trained data, the pre-trained pattern in LLMs. It has nothing to do with whether you have a RAG or not. Here, the answer, it has nothing to do with what we have in our RAG. It\u0026#39;s just in their pre-trained data. We can\u0026#39;t do any verification of whether this is correct or not because this goes back to the same hallucination problem that we saw in LLMs. If LLMs follow their pre-trained patterns, like from their training data, we can\u0026#39;t verify if it\u0026#39;s hallucinating or not.\u003c/p\u003e\n\n\u003cp\u003eRight now, what I\u0026#39;m going to do is, I have this other vector plus KG mode. It\u0026#39;s combining vector, similarity search, and structured data, knowledge graph. We ask the same question here. I\u0026#39;ll copy this. I did not check the include chat history mode so it doesn\u0026#39;t memorize. Every answer is independent. I\u0026#39;m going to ask the same question. Here is the context data. This is exactly the same query. This is previously based on the vector mode. It\u0026#39;s all based on unstructured data. This vector plus KG mode, it has some more structured data such as like the partnership, the industry. It\u0026#39;s classified as a research industry, subsidiary, founder, competitor, a very long list of suppliers.\u003c/p\u003e\n\n\u003cp\u003eAlso, it pulls in unstructured data, which means it references both, like if you have a more defined relationship, such as these other metadata, it will reference that. It will also reference the information from unstructured data. The LLMs will decide which is relevant to your query. This is what structured data can do for you, is like, LLMs outputs are undeterministic and we need deterministic data or information as our ground truth to tell LLM, this is the map of facts, follow those and tell me about how to answer my question. This is a mini demo that I wanted to share with you guys. This is not perfect. It just shows some of the choice that we can see, or even play around and try it out. I would definitely love to interact with you more to see, where are the other downsides or like not perfect enough and we can improve on that.\u003c/p\u003e\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\u003cp\u003eI don\u0026#39;t know if some of you have these questions or doubt or even confusion about, do I need a graph database for a knowledge graph? Because knowledge graph, there\u0026#39;s a graph word in this term. Do I need a graph database for this? Does it always have to look something like this? Actually, no, because if you really pull out the data that we have here, I\u0026#39;m going to show you, it looks like this. It could be a dictionary data. It\u0026#39;s actually just text data, but it\u0026#39;s structured. Some people would just import the structured data into a graph database. It really depends on your use case.\u003c/p\u003e\n\n\u003cp\u003eFor me, when I\u0026#39;m asked about this question, I would tell people, or based on some of the experts in the domain, I was like, it really depends on your use case. What problems are you trying to solve? My answer is, when we talk about knowledge graphs, it\u0026#39;s not really about the graph. It\u0026#39;s more about how you can use structured data to solve or enhance your application.\u003c/p\u003e\n\n\u003ch2\u003eQuestions and Answers\u003c/h2\u003e\n\n\u003cp\u003eParticipant 1: When you have the graph data as part of the RAG, how does the LLM model differentiate between the data from your knowledge graph versus from the vector database?\u003c/p\u003e\n\n\u003cp\u003eChen: This example I just showed, it has structured data and unstructured. For example, the LLM will recognize, ok, like OpenAI has competitor or supplier, that data is irrelevant to my query because it\u0026#39;s still doing a vector similarity search. Is that what you\u0026#39;re asking?\u003c/p\u003e\n\n\u003cp\u003eParticipant 1: No, my question is, how does your implementation treat the data from the vector database and then the knowledge graph differently so that you make a difference in the outcome?\u003c/p\u003e\n\n\u003cp\u003eChen: It\u0026#39;s not really treating the data differently because at the end of the day, these are all text data. The previous example where I only show vector-based, it\u0026#39;s only based on vector similarity search. There\u0026#39;s only one method in there. If I incorporate structured data in it, it\u0026#39;s like a metadata. It\u0026#39;s like more information, LLMs can reference, and to decide, this information is helpful for your question.\u003c/p\u003e\n\n\u003cp\u003eParticipant 1: In other words, when you do your prompt, you put both the information from the vector search and your graph data as part of your prompt.\u003c/p\u003e\n\n\u003cp\u003eChen: Prompt? Yes, that could be one way. That could be it.\u003c/p\u003e\n\n\u003cp\u003eParticipant 1: If that\u0026#39;s the case, so the LLM has the inputs from both the vector data and also the data from the knowledge graph. It\u0026#39;s probably a more fundamental question about how LLM works internally. How do LLM, based on your experience, process the data from the vector database versus from the knowledge graph?\u003c/p\u003e\n\n\u003cp\u003eChen: I\u0026#39;ll actually show another example. In this example, I\u0026#39;m using a graph database. Let me show here. This is like a graph. The articles we imported are actually chunked, right now, because of the context window limitation, we do need to chunk the different paragraphs. You can see, this is one of the chunked nodes and here are the embeddings. What it\u0026#39;s doing here is like, it\u0026#39;s still doing vector similarity search to find that node and see if there\u0026#39;s information based on the similarity search and answer your question. This article\u0026#39;s in our database. It summarized this, not because of the graph, it\u0026#39;s because of vector similarity search.\u003c/p\u003e\n\n\u003cp\u003eParticipant 1: When you have the graph, how does that work differently?\u003c/p\u003e\n\n\u003cp\u003eChen: Here, it\u0026#39;s like, the graph is serving the ground truth. It\u0026#39;s because it has relationship. It\u0026#39;s serving as a ground truth for LLMs to follow. Here, tell me about OpenAI, and I\u0026#39;m using this like only vector mode. This answer is based on LLM\u0026#39;s pre-trained memory. It has no ground truth. Based on our definition, it\u0026#39;s not in our knowledge base. If I switch to the vector plus KG mode, the knowledge graph mode, right now, the answer is following the structured data, like the structured relationship defined in our knowledge base.\u003c/p\u003e\n\n\u003cp\u003eParticipant 1: In other words, you just feed in the LLM model more information from the knowledge graph, that\u0026#39;s it?\u003c/p\u003e\n\n\u003cp\u003eChen: Yes, because that\u0026#39;s the ground truth. We have to provide that.\u003c/p\u003e\n\n\u003cp\u003eParticipant 2: Let\u0026#39;s say you have a graph database with a huge graph, how do you map the natural language question made by the user to a subset of that graph?\u003c/p\u003e\n\n\u003cp\u003eChen: You mean like how do LLMs know? Like, it has a question and start to find information in the graph database?\u003c/p\u003e\n\n\u003cp\u003eParticipant 2: Exactly.\u003c/p\u003e\n\n\u003cp\u003eChen: For example, summarize this article, this is a question. Vector-based approaches convert this question into embeddings and you do vector similarity search. The chunked data here, yes, you see, this article, it has different chunks here and the embeddings, and it will find the vector similarity based on these embeddings as you see here.\u003c/p\u003e\n\n\u003cp\u003eParticipant 2: I\u0026#39;m guessing you get through the embeddings to map to actual nodes, but then nodes in the graph database have lots of relationships. How do you run the relationships you actually pass to the model so that you don\u0026#39;t go beyond the context window?\u003c/p\u003e\n\n\u003cp\u003eChen: In this case, we\u0026#39;re using Neo4j and they have their Cypher query. Right now, they do have the LLM to Cypher thing. Right now, if you want more complex queries, we still need human in the loop, but that question is actually just text to Cypher. It\u0026#39;s pretty much like, you have your natural language and it\u0026#39;s translated into Python or SQL, and to find those relationships.\u003c/p\u003e\n\n\u003cp\u003eParticipant 3: I have a question about that lost-in-the-middle phenomenon. How does the knowledge graph help with that? Because if I understand correctly, you have structured data, but you still put it in the context. In case there\u0026#39;s the answer or the important thing in the middle of that context, although structured, it can still be lost?\u003c/p\u003e\n\n\u003cp\u003eChen: It relates to another part of the knowledge graph construction, like the process. Before LLMs, there are already natural language processing techniques, like extract entities and relationships in the articles as we saw in the graph. In that case, it could be complementary to the lost-in-the-middle phenomenon, because the vector-based, everything just gets converted into embeddings. For humans, it\u0026#39;s a little bit hard to evaluate what\u0026#39;s being converted, and where is it in our knowledge base. The NLP technique, it\u0026#39;s more deterministic as a ground truth, say, this is our ground truth, and you do your vector search, that\u0026#39;s fine, but please follow the ground truth in whatever is provided from our NLP approach.\u003c/p\u003e\n\n\u003cp\u003eParticipant 4: Regarding the nodes of the graph, how are the embeddings calculated? In the embedding processing, are all the arcs included, so the relationships, and so that\u0026#39;s the result of the embeddings of the node?\u003c/p\u003e\n\n\u003cp\u003eChen: This is more related to the retrieval part, let\u0026#39;s say if you\u0026#39;re building a RAG system. What I just show based on the previous question is like the nodes, they are text, and basically, you can convert into embeddings. You can ask LLMs to first find the nodes based on the embeddings, and find the surrounding relationships. Or you can pretty much do, based on your database, such as Postgres or SQL, to find the relationship with their own language. There are multiple approaches, and it doesn\u0026#39;t have to be either/or.\u003c/p\u003e\n\n\u003cp\u003eParticipant 5: About the relationships, I was curious about the previous slide, if you can. The one from Google, of the graph, which was the Mona Lisa one. I was checking the relationships. I was checking like James has lived in a Tour Eiffel. Now, living in a Tour Eiffel seems weird. I don\u0026#39;t think he was living in it. I was thinking if we could get some ambiguous relationships in the structured graph, how we could solve this?\u003c/p\u003e\n\n\u003cp\u003eChen: This slide, I borrowed from Google, and I think they probably make a joke about that. To be fair, something I did not stress enough in this session is like knowledge graph, just as I mentioned, it depends on your use case. You can use the generic NLP method to find the relationship, but we still need human in the loop to evaluate, is this something you want, because it has to solve your needs. Knowledge graph is like, if you want to have good ground truth for LLMs, it\u0026#39;s basically an indicator of data quality. If that is wrong, like James has lived in Tour Eiffel or something, then, sure, we eliminate that. That\u0026#39;s not correct.\u003c/p\u003e\n\n\u003cp\u003eParticipant 5: We need also a human to verify the data.\u003c/p\u003e\n\n\u003cp\u003eChen: Even right now we have LLMs, everything is still about data, like training models, fine-tuning models. It\u0026#39;s still like, how good is your data. The data quality thing, it\u0026#39;s a challenging part, and PDS, but it\u0026#39;s still very important. We still need human, at least, unless AGI comes.\u003c/p\u003e\n\n\u003cp\u003eParticipant 6: In the example that you have in your vector database, is there few articles in the vector database or is it just the article that was shown before was actually having the embeddings in there? The reason for that question is like, the context that\u0026#39;s provided, it\u0026#39;s having more information than just from the particular article itself. Does it have more articles or it\u0026#39;s just one article in there?\u003c/p\u003e\n\n\u003cp\u003eChen: I did import 10 recent articles about OpenAI. The nodes, like this is one article, this is another, third, fourth, fifth, something like that. I did import 10 articles.\u003c/p\u003e\n\n\u003cp\u003eParticipant 7: My question is more around the change that can happen in a relationship. If something changes, is it always like a live look at the data, or is there a process that needs to run to make sure that the graph is up to date and the information that we are requesting is up to date as well?\u003c/p\u003e\n\n\u003cp\u003eChen: This relates more to the natural language processing techniques. Of course, we want the data to be updated, real-time. Some of the NLP techniques definitely can do that. Every time you want to change your data or something, it\u0026#39;s better, like humans, you have to see, ok, is this the direction I\u0026#39;m changing? Because we rely on LLMs to do all the things. I don\u0026#39;t have you guys play enough with this, but something I found funny about LLMs is, sometimes you ask a question and you ask two times or three times, and even you set the temperature to 0, and sometimes it\u0026#39;ll give you different answers. It still has this undeterministic nature. If you want LLMs to handle your data, we probably should keep in mind like how consistent that is.\u003c/p\u003e\n\n\n\n\n\u003cp\u003e\u003cbig\u003e\u003cstrong\u003eSee more \u003ca href=\"https://www.infoq.com/transcripts/presentations/\"\u003epresentations with transcripts\u003c/a\u003e\u003c/strong\u003e\u003c/big\u003e\u003c/p\u003e\n\n\n\n                                \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "32 min read",
  "publishedTime": "2025-07-22T00:00:00Z",
  "modifiedTime": null
}
