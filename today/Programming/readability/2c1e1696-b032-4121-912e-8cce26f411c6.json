{
  "id": "2c1e1696-b032-4121-912e-8cce26f411c6",
  "title": "Safer and Multimodal: Responsible AI with Gemma",
  "link": "https://developers.googleblog.com/en/safer-and-multimodal-responsible-ai-with-gemma/",
  "description": "ShieldGemma 2, built on Gemma 3, is a 4 billion parameter model that can be used as an input filter for vision language models or an output filter for image generation systems, and is designed to respond to a wide range of diverse and nuanced imagery.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Dana Kurniawan, Wenjun Zeng, Ryan Mullins",
  "length": 3017,
  "excerpt": "ShieldGemma 2, built on Gemma 3, is a 4 billion parameter model that can be used as an input filter for vision language models or an output filter for image generation systems, and is designed to respond to a wide range of diverse and nuanced imagery.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "Last year, we released ShieldGemma, a suite of safety content classifier models built on Gemma 2 and designed to detect harmful content in AI models’ text inputs and outputs. As we debut Gemma 3 today, we’re excited to build on our foundation of responsible AI by announcing ShieldGemma 2. ShieldGemma 2, built on Gemma 3, is a 4 billion (4B) parameter model that checks the safety of your synthetic and natural images against key categories to help you build robust datasets and models. With this addition to the Gemma family of models, researchers and developers can now easily minimize the risk of harmful content in their models across key areas of harm:Sexually explicit contentDangerous contentViolence We recommend using ShieldGemma 2 as an input filter to vision language models, or as an output filter of image generation systems. ShieldGemma can be used on both synthetic and natural images.What’s different in ShieldGemma 2?Moving beyond text, training and understanding image safety in multimodal models brings new challenges, which is why ShieldGemma 2 is built to respond to a wide range of diverse and nuanced styles of imagery.To train a robust image safety model, we curated training datasets of natural and synthetic images, and instruction-tuned Gemma 3 to demonstrate strong performance. We compared safety policies to the following benchmarks, and will be releasing a technical report that also incorporates third party benchmarks. Evaluation results based on optimal F1 score (%, higher is better) on our internal benchmark Here’s how ShieldGemma can help you build safer AI image applications:Flexibility: Upload any synthetic or natural images, and edit our prompt template to adapt to your needs. Fine-tune on Google Colab or your own GPU.Versatility: All tools that support Gemma 3 support ShieldGemma 2, including popular frameworks like Transformers, JAX, Keras, Ollama, and others.Collaborative: ShieldGemma is open by nature and welcomes community collaborators to keep building inclusively as we collectively push industry safety standards onwards and upwards.Deploying open models responsibly relies on a whole community effort, and we look forward to exploring how ShieldGemma 2 can be delivered in smaller sizes, across more harm areas, and aligned with multimodal ML Commons taxonomy in the near future.We’re excited to continue building for safe and responsible multimodal AI!Get started todayExplore ShieldGemma 2 on our developer site, and see our model card for more information.Try ShieldGemma 2 on Google AI Studio, Hugging Face, Ollama, and other platforms.Team AcknowledgementWenjun Zeng, Ryan Mullins, Dana Kurniawan, Yuchi Liu, Mani Malek, Yiwen Song, Dirichi Ike-Njoku, Hamid Palangi, Jindong Gu, Shravan Dheep, Karthik Narashimhan, Tamoghna Saha, Joon Baek, Rick Pereira, Cai Xu, Jingjing Zhou, Aparna Joshi, Will Hawkins",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/SheildGemma2_WagtailMDBlog_RD1_V0.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cp data-block-key=\"4jfjz\"\u003eLast year, we released \u003ca href=\"https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/\"\u003eShieldGemma\u003c/a\u003e, a suite of safety content classifier models built on Gemma 2 and designed to detect harmful content in AI models’ text inputs and outputs. As we debut Gemma 3 today, we’re excited to build on our foundation of responsible AI by announcing ShieldGemma 2.\u003c/p\u003e    \u003cdiv\u003e\n    \u003cp data-block-key=\"3zuxx\"\u003eShieldGemma 2, built on Gemma 3, is a 4 billion (4B) parameter model that checks the safety of your synthetic and natural images against key categories to help you build robust datasets and models. With this addition to the Gemma family of models, researchers and developers can now easily minimize the risk of harmful content in their models across key areas of harm:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"6b9ae\"\u003eSexually explicit content\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"br5rb\"\u003eDangerous content\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ba31k\"\u003eViolence\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_U7pe0vV.original.png\" alt=\"Use ShieldGemma as an input filter to any vision language model, or an an output filter of image generation models\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"3zuxx\"\u003eWe recommend using ShieldGemma 2 as an input filter to vision language models, or as an output filter of image generation systems. ShieldGemma can be used on both synthetic and natural images.\u003c/p\u003e\u003ch2 data-block-key=\"eqgcf\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhat’s different in ShieldGemma 2?\u003c/h2\u003e\u003cp data-block-key=\"7h13k\"\u003eMoving beyond text, training and understanding image safety in multimodal models brings new challenges, which is why ShieldGemma 2 is built to respond to a wide range of diverse and nuanced styles of imagery.\u003c/p\u003e\u003cp data-block-key=\"2j6fu\"\u003eTo train a robust image safety model, we curated training datasets of natural and synthetic images, and instruction-tuned Gemma 3 to demonstrate strong performance. We compared safety policies to the following benchmarks, and will be releasing a technical report that also incorporates third party benchmarks.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ShieldGemma-2-performance.original.png\" alt=\"ShieldGemma 2 performance\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    Evaluation results based on optimal F1 score (%, higher is better) on our internal benchmark\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"3zuxx\"\u003e\u003cb\u003eHere’s how ShieldGemma can help you build safer AI image applications:\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"2aj6c\"\u003e\u003cb\u003eFlexibility:\u003c/b\u003e Upload any synthetic or natural images, and edit our prompt template to adapt to your needs. Fine-tune on Google Colab or your own GPU.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"8q0b5\"\u003e\u003cb\u003eVersatility:\u003c/b\u003e All tools that support Gemma 3 support ShieldGemma 2, including popular frameworks like Transformers, JAX, Keras, Ollama, and others.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"b34a2\"\u003e\u003cb\u003eCollaborative:\u003c/b\u003e ShieldGemma is open by nature and welcomes community collaborators to keep building inclusively as we collectively push industry safety standards onwards and upwards.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"7n9lc\"\u003eDeploying open models responsibly relies on a whole community effort, and we look forward to exploring how ShieldGemma 2 can be delivered in smaller sizes, across more harm areas, and aligned with multimodal ML Commons taxonomy in the near future.\u003c/p\u003e\u003cp data-block-key=\"aj4cq\"\u003eWe’re excited to continue building for safe and responsible multimodal AI!\u003c/p\u003e\u003ch2 data-block-key=\"8gd1o\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eGet started today\u003c/h2\u003e\u003cul\u003e\u003cli data-block-key=\"9rmpv\"\u003eExplore ShieldGemma 2 on our developer site, and see our model card for more information.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"1ga28\"\u003eTry ShieldGemma 2 on Google AI Studio, Hugging Face, Ollama, and other platforms.\u003c/li\u003e\u003c/ul\u003e\u003chr/\u003e\u003ch3 data-block-key=\"2trmd\"\u003e\u003cb\u003eTeam Acknowledgement\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"3d6ri\"\u003e\u003csub\u003eWenjun Zeng, Ryan Mullins, Dana Kurniawan, Yuchi Liu, Mani Malek, Yiwen Song, Dirichi Ike-Njoku, Hamid Palangi, Jindong Gu, Shravan Dheep, Karthik Narashimhan, Tamoghna Saha, Joon Baek, Rick Pereira, Cai Xu, Jingjing Zhou, Aparna Joshi, Will Hawkins\u003c/sub\u003e\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-03-12T00:00:00Z",
  "modifiedTime": null
}
