{
  "id": "e4d44d39-ac8c-4db4-a8c4-88ad558fd089",
  "title": "Small Models, Big Impact: Why JetBrains is Betting on Focal LLMs",
  "link": "https://blog.jetbrains.com/ai/2025/07/small-models-big-impact-why-jetbrains-is-betting-on-focal-llms/",
  "description": "At AI Summit London 2025, Kris Kang, Head of Product for AI at JetBrains, gave a talk that questioned a common belief in AI development: that bigger means better. The industry has focused heavily on massive, general-purpose language models. These models offer impressive capabilities, but the cost of building and running them at scale is […]",
  "author": "Conrad Schwellnus",
  "published": "Wed, 16 Jul 2025 14:29:48 +0000",
  "source": "https://blog.jetbrains.com/feed",
  "categories": [
    "events",
    "jetbrains-ai",
    "news",
    "research",
    "ai",
    "summit"
  ],
  "byline": "Conrad Schwellnus",
  "length": 6385,
  "excerpt": "At AI Summit London 2025, Kris Kang, Head of Product for AI at JetBrains, gave a talk that questioned a common belief in AI development: that bigger means better.",
  "siteName": "The JetBrains Blog",
  "favicon": "https://blog.jetbrains.com/wp-content/uploads/2024/01/cropped-mstile-310x310-1-180x180.png",
  "text": "Supercharge your tools with AI-powered features inside many JetBrains products Events JetBrains AI News ResearchSmall Models, Big Impact: Why JetBrains is Betting on Focal LLMs At AI Summit London 2025, Kris Kang, Head of Product for AI at JetBrains, gave a talk that questioned a common belief in AI development: that bigger means better. The industry has focused heavily on massive, general-purpose language models. These models offer impressive capabilities, but the cost of building and running them at scale is top of mind for many enterprise decision-markers. In his talk “Small Models, Big Impact”, Kris introduced an alternative: focal models. These compact, domain-specific LLMs aim to deliver strong performance while reducing energy use and total cost of ownership. These models can be used as a workhorse complement to frontier models.” Here’s why this matters and how JetBrains is acting on it. The energy cost of chasing scale AI models now operate at a scale that was hard to imagine a few years ago. Although there are no official numbers, observers estimate that GPT-3 has 175 billion parameters, while estimates place GPT-4 at 1.8 trillion and Grok-3 at 2.7 trillion. To accommodate the growing size of models, data centers with as much as 2 million GPUs are being built. The energy required for this is significant. Processing 1 billion AI chats, with a frontier model, can use more than 124 GWh per year (at 0.34 Wh per day), or equivalent to powering 31k UK households per year. The environmental impact alone raises important concerns, and the financial cost can be just as significant. For a mid-sized company, using a frontier model daily might cost around USD 15,000, which adds up to USD 45,000 per employee per year. That figure is six times higher than the average IT expenditure per employee in 2023. This assumes: $15 per 10 requests, and an enterprise of 1,000 makes 10,000 requests per day, then that’s $15,000 per day. This is a conservative estimate. What are focal models? Focal models aim to solve a single problem with scalability in mind. They are: Small, usually with fewer than 10 billion parameters Domain-specific, not general-purpose Post-trained to maximise cost efficiency and inference speed Built using techniques like quantisation, distillation, and Mixture of Experts techniques Rather than trying to be good at everything (generating images, text-to-speech, coding, and more) they focus on high performance in a narrow field, such as code, legal, or medical tasks. This specialisation allows enterprises to apply focal models to specific use cases that have the highest ROI per cost, where ROI might also include energy efficiency. Mellum: A focal model purpose-built for code JetBrains has taken this concept and is applying it directly to software development. Mellum, our 4-billion-parameter model, is the first in a family we intend to open source. It was built from the ground up to support code completion in real-world settings. Mellum works across multiple programming languages and has been optimized to run quickly on modest hardware (e.g. it can run locally easily on a Macbook M4 Max). Developers can use it in isolated environments without relying on third-party providers and without giving up control over their data. You can already access Mellum through JetBrains IDEs via our AI Assistant or on Hugging Face if you prefer to run it independently. Rather than aiming for general use, Mellum stays focused on one job: helping developers write better code more efficiently. Why focal models matter for the future of AI Focal models offer a practical answer to AI’s rising technical, financial, and environmental costs. By narrowing the model’s purpose, teams can work with systems that cost less to operate and offer greater flexibility in how and where they’re deployed. This also opens the door to deeper enterprise integration. Companies can fine-tune these models to fit their workflows or adapt them to meet compliance requirements without needing access to large-scale computeing clusters. Focal models do not replace frontier models but offer a complement that better suits many real-world applications. The JetBrains perspective: Smarter, not just bigger JetBrains believes the next meaningful shift in AI will not come from chasing larger models but from refining how and where we apply them in order to help companies grow their businesses sustainably. We have invested in focal model development because our customers are asking for it, and we believe this approach delivers more balanced results. Mellum is our first step in this direction. It shows what is possible when you focus on purpose instead of size. Try Mellum today You can try Mellum in several ways: On Hugging Face, for experimentation or offline use Inside JetBrains IDEs, through AI Assistant Soon, as a containerised deployment in NVIDIA AI Enterprise Whether you prioritise sustainability, performance, or security, focal models like Mellum offer a practical way to adopt LLMs where they bring the most value. The future of AI is not just about using frontier models as they hit the market, it’s about sustainably embedding AI to your business that leads to positive outcomes. Subscribe to JetBrains AI Blog updates Discover more",
  "image": "https://blog.jetbrains.com/wp-content/uploads/2025/07/AI-social-BlogSocialShare-1280x720-2x-6.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n    \u003cdiv\u003e\n                        \u003ca href=\"https://blog.jetbrains.com/ai/\"\u003e\n                            \u003cimg src=\"https://blog.jetbrains.com/wp-content/uploads/2024/01/JetBrains-AI.svg\" alt=\"Ai logo\"/\u003e\n                                                                                                \n                                                                                    \u003c/a\u003e\n                                                    \u003cp\u003eSupercharge your tools with AI-powered features inside many JetBrains products\u003c/p\u003e\n                                            \u003c/div\u003e\n                            \u003csection data-clarity-region=\"article\"\u003e\n                \u003cdiv\u003e\n                    \t\t\t\t\u003cp\u003e\u003ca href=\"https://blog.jetbrains.com/ai/category/events/\"\u003eEvents\u003c/a\u003e\n\t\t\t\u003ca href=\"https://blog.jetbrains.com/ai/category/jetbrains-ai/\"\u003eJetBrains AI\u003c/a\u003e\n\t\t\t\u003ca href=\"https://blog.jetbrains.com/ai/category/news/\"\u003eNews\u003c/a\u003e\n\t\t\t\u003ca href=\"https://blog.jetbrains.com/ai/category/research/\"\u003eResearch\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"major-updates\"\u003eSmall Models, Big Impact: Why JetBrains is Betting on Focal LLMs\u003c/h2\u003e                    \n                    \n\u003cp\u003eAt AI Summit London 2025, Kris Kang, Head of Product for AI at JetBrains, gave a talk that questioned a common belief in AI development: that bigger means better.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe industry has focused heavily on massive, general-purpose language models. These models offer impressive capabilities, but the cost of building and running them at scale is top of mind for many enterprise decision-markers.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn his talk \u003cem\u003e“Small Models, Big Impact”\u003c/em\u003e, Kris introduced an alternative: focal models. These compact, domain-specific LLMs aim to deliver strong performance while reducing energy use and total cost of ownership. These models can be used as a workhorse complement to frontier models.”\u003c/p\u003e\n\n\n\n\u003cp\u003eHere’s why this matters and how JetBrains is acting on it.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eThe energy cost of chasing scale\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eAI models now operate at a scale that was hard to imagine a few years ago. Although there are no official numbers, observers estimate that GPT-3 has 175 billion parameters, while estimates place GPT-4 at 1.8 trillion and Grok-3 at 2.7 trillion. To accommodate the growing size of models, data centers with as much as 2 million GPUs are being built.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe energy required for this is significant. Processing 1 billion AI chats, with a frontier model, can use more than 124 GWh per year (at 0.34 Wh per day), or equivalent to powering 31k UK households per year. The environmental impact alone raises important concerns, and the financial cost can be just as significant.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor a mid-sized company, using a frontier model daily might cost around USD 15,000, which adds up to USD 45,000 per employee per year. That figure is \u003ca href=\"https://vistaitgroup.com/news/how-much-should-your-business-be-spending-on-it/\" target=\"_blank\" rel=\"noopener\"\u003esix times higher\u003c/a\u003e than the average IT expenditure per employee in 2023. This assumes: $15 per 10 requests, and an enterprise of 1,000 makes 10,000 requests per day, then that’s $15,000 per day. This is a conservative estimate.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eWhat are focal models?\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eFocal models aim to solve a single problem with scalability in mind. They are:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eSmall, usually with fewer than 10 billion parameters\u003c/li\u003e\n\n\n\n\u003cli\u003eDomain-specific, not general-purpose\u003c/li\u003e\n\n\n\n\u003cli\u003ePost-trained to maximise cost efficiency and inference speed\u003c/li\u003e\n\n\n\n\u003cli\u003eBuilt using techniques like quantisation, distillation, and Mixture of Experts techniques\u003cbr/\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eRather than trying to be good at everything (generating images, text-to-speech, coding, and more) they focus on high performance in a narrow field, such as code, legal, or medical tasks.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis specialisation allows enterprises to apply focal models to specific use cases that have the highest ROI per cost, where ROI might also include energy efficiency.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eMellum: A focal model purpose-built for code\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eJetBrains has taken this concept and is applying it directly to software development. Mellum, our 4-billion-parameter model, is the first in a family we intend to open source. It was built from the ground up to support code completion in real-world settings.\u003c/p\u003e\n\n\n\n\u003cp\u003eMellum works across multiple programming languages and has been optimized to run quickly on modest hardware (e.g. it can run locally easily on a Macbook M4 Max). Developers can use it in isolated environments without relying on third-party providers and without giving up control over their data. You can already access Mellum through JetBrains IDEs via our AI Assistant or on Hugging Face if you prefer to run it independently.\u003c/p\u003e\n\n\n\n\u003cp\u003eRather than aiming for general use, Mellum stays focused on one job: helping developers write better code more efficiently.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eWhy focal models matter for the future of AI\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eFocal models offer a practical answer to AI’s rising technical, financial, and environmental costs.\u003c/p\u003e\n\n\n\n\u003cp\u003eBy narrowing the model’s purpose, teams can work with systems that cost less to operate and offer greater flexibility in how and where they’re deployed.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis also opens the door to deeper enterprise integration. Companies can fine-tune these models to fit their workflows or adapt them to meet compliance requirements without needing access to large-scale computeing clusters.\u003c/p\u003e\n\n\n\n\u003cp\u003eFocal models do not replace frontier models but offer a complement that better suits many real-world applications.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eThe JetBrains perspective: Smarter, not just bigger\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eJetBrains believes the next meaningful shift in AI will not come from chasing larger models but from refining how and where we apply them in order to help companies grow their businesses sustainably. We have invested in focal model development because our customers are asking for it, and we believe this approach delivers more balanced results.\u003c/p\u003e\n\n\n\n\u003cp\u003eMellum is our first step in this direction. It shows what is possible when you focus on purpose instead of size.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eTry Mellum today\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eYou can try Mellum in several ways:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eOn Hugging Face, for experimentation or offline use\u003c/li\u003e\n\n\n\n\u003cli\u003eInside JetBrains IDEs, through AI Assistant\u003c/li\u003e\n\n\n\n\u003cli\u003eSoon, as a containerised deployment in NVIDIA AI Enterprise\u003cbr/\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eWhether you prioritise sustainability, performance, or security, focal models like Mellum offer a practical way to adopt LLMs where they bring the most value.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe future of AI is not just about using frontier models as they hit the market, it’s about sustainably embedding AI to your business that leads to positive outcomes.\u003c/p\u003e\n                    \n                                                                                                                                                                                                                            \u003cdiv\u003e\n                                \u003cdiv\u003e\n                                                                            \u003ch4\u003eSubscribe to JetBrains AI Blog updates\u003c/h4\u003e\n                                                                                                            \n                                \u003c/div\u003e\n                                \n                                \u003cp\u003e\u003cimg src=\"https://blog.jetbrains.com/wp-content/themes/jetbrains/assets/img/img-form.svg\" alt=\"image description\"/\u003e\n                                                                    \u003c/p\u003e\n                            \u003c/div\u003e\n                                                            \u003c/div\u003e\n                \u003ca href=\"#\"\u003e\u003c/a\u003e\n                \n                \n            \u003c/section\u003e\n                    \u003cdiv\u003e\n                \u003cp\u003e\n                    \u003ch2\u003eDiscover more\u003c/h2\u003e\n                \u003c/p\u003e\n                \n            \u003c/div\u003e\n                \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": null,
  "modifiedTime": null
}
