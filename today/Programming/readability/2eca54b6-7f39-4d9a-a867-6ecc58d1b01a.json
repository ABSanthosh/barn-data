{
  "id": "2eca54b6-7f39-4d9a-a867-6ecc58d1b01a",
  "title": "Using AI Agents for Notebook Debugging",
  "link": "https://blog.jetbrains.com/ai/2025/03/using-ai-agents-for-notebook-debugging/",
  "description": "Debugging computational notebooks can be quite frustrating for a variety of reasons, including issues like out-of-order cell execution, missing data files, and library conflicts. Traditional AI tools often struggle with these problems due to the interactive nature of notebooks, but at JetBrains, we are exploring new solutions. Our research team has developed a prototype of […]",
  "author": "Galina Ryazanskaya",
  "published": "Wed, 19 Mar 2025 12:53:50 +0000",
  "source": "https://blog.jetbrains.com/feed",
  "categories": [
    "research",
    "ai-agent",
    "ai-agents",
    "debug",
    "debugging"
  ],
  "byline": "Galina Ryazanskaya",
  "length": 11275,
  "excerpt": "Debugging computational notebooks can be quite frustrating for a variety of reasons, including issues like out-of-order cell execution, missing data files, and library conflicts. Traditional AI tools",
  "siteName": "The JetBrains Blog",
  "favicon": "https://blog.jetbrains.com/wp-content/uploads/2024/01/cropped-mstile-310x310-1-180x180.png",
  "text": "Supercharge your tools with AI-powered features inside many JetBrains products ResearchUsing AI Agents for Notebook Debugging Debugging computational notebooks can be quite frustrating for a variety of reasons, including issues like out-of-order cell execution, missing data files, and library conflicts. Traditional AI tools often struggle with these problems due to the interactive nature of notebooks, but at JetBrains, we are exploring new solutions. Our research team has developed a prototype of an AI agent that autonomously fixes notebook errors by modifying and executing cells until the problems are solved.  In this post, we’ll explore how the prototype works, how it improves the debugging process, and how it compares to simpler LLM-based solutions in terms of cost and user experience. Why automate notebook debugging Computational notebooks are widely used for data analysis, research, and hypothesis testing because they allow users to integrate different data types and execute cells non-linearly. This creates vast opportunities for experimentation and use in data science. However, this flexibility and interactivity come at the cost of reproducibility and having to deal with frequent bugs. Studies show that up to 75% of Jupyter notebooks cannot be re-run without issues. At the same time, debugging them is particularly difficult, as it requires the interactive, iterative inspection of errors and their context rather than simple static analysis. Using LLMs like GPT-4 is standard practice for other debugging tasks. However, for notebook debugging, it is rather challenging due to context size limitations. Notebooks require runtime information to reflect their state, which means they often need a large context window for error resolution. AI agents address this by interacting with notebooks iteratively – much like humans do, but significantly faster. The JetBrains Research team has created a prototype of just such an AI agent for Datalore. It is capable of creating, editing, and executing cells to resolve runtime exceptions. Here’s a short demo showcasing the debugging AI agent in action: If you want to see a detailed report on this project, check out this preprint. These results were also presented at EMNLP, a leading conference for natural language processing and artificial intelligence. How does the debugging AI agent work? Let’s start with a broad overview of our AI agent system. It consists of three main components – the LLM module, the environment, and the user interface – which work together as follows: Start: Whenever the execution of a cell results in an exception, the debugging AI agent can be called with a Fix with AI Agent button that appears below the error message. When clicked, it starts the agentic debugging cycle, which is shown as an interaction with an LLM in a separate window.  Cycle: At each iteration of the debugging cycle, the LLM module collects the required information and adds it to the debugging prompt passed to the LLM, in this case, GPT-4. The LLM then generates an answer, which includes some reasoning as well as the next action, such as editing and executing cells or finishing the debugging process once the error has been resolved. The action suggested by the model is then executed in the environment, and the results are added to the prompt for the next round of debugging.  Finish: This iterative loop continues until the error is resolved or until the limit on time or the number of iterations is reached. In our experiments, most of the debugging runs ended in the error being successfully resolved after only a few steps, so the limits were rarely required in practice. How does the AI agent prompt the LLM? Let’s take a closer look at the core of our AI agent system, namely, the LLM module. Our AI agent prompts the LLM using a “system prompt” that covers the general approach to the task and a “user prompt” containing specific details about the current stage of the debugging run. Below, you can see shortened versions of the two prompts. System promptGeneral purposeYou are a coding assistant that should help solve the user’s error in a computational notebook. You should use functions to help handle real-time user queries and return outputs only in the form of valid JSON.Available toolsYou have a few ways of interacting with the environment:change the code of the existing cells, run it, and give the output.add a new cell with your own code, run it, and give the output.run any cell as is and give the output.If you’re sure the error won’t show up in the cell where it was found, you can run finish.Workflow guidelinesKeep trying for at least 10 steps before you stop, but if you think you’ve solved the problem, you can run finish right away.If you can fix the error without changing any code, do that. Don’t edit the existing code or add new code unless you really need to.Use only the functions given to you. If you have many functions to choose from, pick the one that solves the problem quickest.When you think you’ve fixed the problem, run finish to say you’re done.HacksJust adding try – except is not a solution. Commenting out the code that produced the error is not a solution. You should propose only meaningful solutions.  Initial user promptHere’s a Jupyter notebook. It uses {separator} as a separator between cells. Note that cell indexes START FROM 1! {notebook} An error occurred in the cell with num {cell_num}. The error trace is the following: {error} Please resolve the error. You must use only the tools defined above to resolve the error. Return output only as valid JSON.  The system prompt opens with a general description of the problem, provides some guidelines on how it should be solved, and then closes with instructions about actions. We use reflection as the algorithm for choosing the next action, asking the LLM to reflect on the outcomes of the previous actions taken before selecting the next tool to call. The reflections are output as comments for each selected action and are available at all following steps. The guidelines also encourage the LLM to explore the environment and discourage large outputs and “hacks”, such as deleting the error cell or commenting out the problematic code.  The user prompt initially consists of the notebook code and the error cell number, again accompanied by a short task description. After each step, the actions suggested by the LLM are converted into commands and executed, and the result is collected as an input for the next user prompt, along with the conversation history. You can observe the entire debugging process in real time via the interface where the conversation and the actions are visualized. How costly is the AI agent? A significant cost with any AI system comes from running large language models, with costs primarily driven by the number of request and response tokens processed. To compare costs, we evaluated the AI agent against a simpler zero-shot debugging approach in Datalore, using 100 hours of hackathon coding records. The dataset featured an extensive notebook action history, so the errors that occurred could be reproduced and addressed by either of the AI solutions. On average, resolving an error with the AI agent was 2.5 times as expensive, costing $0.22 per error with GPT-4 versus $0.09 with the simpler method. The AI agent’s higher cost came mainly from using three times as many request tokens for its memory stack, while the number of response tokens was similar between methods. Nevertheless, the cost remained practical for industry use since input tokens are cheaper and most errors were fixed on the first step. User feedback about the AI agent We used the same zero-shot baseline to compare the user experiences with these two AI algorithms. In particular, we wanted to know how well users thought the AI agent resolved problems and how much they believed it could improve their productivity. We had two groups of programmers complete the same data-filtering task. One group used the zero-shot baseline, and the other used our agentic solution. We found that the users were quite satisfied with both solutions, rating them either good or excellent, and they slightly favored the AI agent. The users also seemed to rely more on the AI agent. However, the participants also reported that the agentic solution was less easy to use. We suspect this may be due to the lack of user control over its actions as well as the speed at which it acts.  In the feedback, the users suggested that slowing down the conversation and highlighting the introduced changes in the notebook would provide more insight into the AI agent’s thought process and solution. They also proposed making the agentic solution process more interactive so they have a way of interrupting or changing the actions suggested by the AI agent. Finally, they asked for a way to access the agent before any error has occurred. Benefits of AI debugging: Looking forward Finally, we compared the AI agent to the unaided human debugging process that comprised our dataset. In this comparison, the agentic solution offers obvious benefits: Faster debugging: The AI agent typically fixed issues in under a minute, while humans took an average of 3.5 minutes and up to 20 minutes for more complex cases. Effectiveness for common small errors: The agent quickly resolved small bugs, like incorrect pandas functions or arguments. Insights into root causes: The agent highlighted and explained changes, which helped to identify root causes of and solutions for errors. The main takeaways of the study for the agentic debugging research concern the areas of improvement: Reduced cost: One solution to the higher cost of the agentic approach could be history caching, which can reduce the number of request tokens used.  Increased user control: The study of user experience highlighted that users would prefer to have more control over the process, such as the option to call the agent before any error occurs.  Building on top of this work, we will continue making Datalore smarter with AI-based features. Subscribe to JetBrains AI Blog updates Discover more",
  "image": "https://blog.jetbrains.com/wp-content/uploads/2025/03/jbai-social_share_blog_1280x720_en-2.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n    \u003cdiv\u003e\n                        \u003ca href=\"https://blog.jetbrains.com/ai/\"\u003e\n                            \u003cimg src=\"https://blog.jetbrains.com/wp-content/uploads/2024/01/JetBrains-AI.svg\" alt=\"Ai logo\"/\u003e\n                                                                                                \n                                                                                    \u003c/a\u003e\n                                                    \u003cp\u003eSupercharge your tools with AI-powered features inside many JetBrains products\u003c/p\u003e\n                                            \u003c/div\u003e\n                            \u003csection data-clarity-region=\"article\"\u003e\n                \u003cdiv\u003e\n                    \t\t\t\t\u003cp\u003e\u003ca href=\"https://blog.jetbrains.com/ai/category/research/\"\u003eResearch\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"major-updates\"\u003eUsing AI Agents for Notebook Debugging\u003c/h2\u003e                    \n                    \n\u003cp\u003eDebugging computational notebooks can be quite frustrating for a variety of reasons, including issues like out-of-order cell execution, missing data files, and library conflicts. Traditional AI tools often struggle with these problems due to the interactive nature of notebooks, but at JetBrains, we are exploring new solutions.\u003c/p\u003e\n\n\n\n\u003cp\u003eOur research team has developed a prototype of an AI agent that autonomously fixes notebook errors by modifying and executing cells until the problems are solved. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn this post, we’ll explore how the prototype works, how it improves the debugging process, and how it compares to simpler LLM-based solutions in terms of cost and user experience.\u003c/p\u003e\n\n\n\n\u003ch3\u003eWhy automate notebook debugging\u003c/h3\u003e\n\n\n\n\u003cp\u003eComputational notebooks are widely used for data analysis, research, and hypothesis testing because they allow users to integrate different data types and execute cells non-linearly. This creates vast opportunities for experimentation and use in data science.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, this flexibility and interactivity come at the cost of reproducibility and having to deal with frequent bugs. Studies show that up to 75% of Jupyter notebooks \u003ca href=\"https://link.springer.com/article/10.1007/s10664-021-09961-9\" target=\"_blank\" rel=\"noopener\"\u003ecannot\u003c/a\u003e be re-run without issues. At the same time, debugging them is particularly difficult, as it requires the interactive, iterative inspection of errors and their context rather than simple static analysis.\u003c/p\u003e\n\n\n\n\u003cp\u003eUsing LLMs like GPT-4 is standard practice for other debugging tasks. However, for notebook debugging, it is rather challenging due to context size limitations. Notebooks require runtime information to reflect their state, which means they often need a large context window for error resolution. AI agents address this by interacting with notebooks iteratively – much like humans do, but significantly faster.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe JetBrains Research team has created a prototype of just such an AI agent for Datalore. It is capable of creating, editing, and executing cells to resolve runtime exceptions.\u003c/p\u003e\n\n\n\n\u003cp\u003eHere’s a short demo showcasing the debugging AI agent in action:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\n\u003ciframe title=\"Demo. Debug Smarter, Not Harder: AI Agents for Error Resolution in Computational Notebooks\" width=\"500\" height=\"281\" src=\"https://www.youtube.com/embed/kOGQO2Gtvs4?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003c/p\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIf you want to see a detailed report on this project, check out \u003ca href=\"https://aclanthology.org/2024.emnlp-demo.38/\" target=\"_blank\" rel=\"noopener\"\u003ethis preprint\u003c/a\u003e. These results were also presented at \u003ca href=\"https://2024.emnlp.org/program/demo/\" target=\"_blank\" rel=\"noopener\"\u003eEMNLP\u003c/a\u003e, a leading conference for natural language processing and artificial intelligence.\u003c/p\u003e\n\n\n\n\u003ch3\u003eHow does the debugging AI agent work?\u003c/h3\u003e\n\n\n\n\u003ch2\u003e\u003cimg decoding=\"async\" fetchpriority=\"high\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdMy8MtqAeNSwdUEAfJIn3909M_Cau0OgmzK8tzVJ9lWnIW1kDF3jRoH0HTTUIeYtHEfUNn8o84RMp86ONNqIG9F5hV5n2-juNclunNBMasCnXy5bwLlvdtA5tvP3G2Fdrmwdk6?key=6COu_-nzVoNQpnuO_cPEwP-y\" width=\"624\" height=\"289\"/\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eLet’s start with a broad overview of our AI agent system. It consists of three main components – the LLM module, the environment, and the user interface – which work together as follows:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eStart:\u003c/strong\u003e Whenever the execution of a cell results in an exception, the debugging AI agent can be called with a \u003cem\u003eFix with AI Agent\u003c/em\u003e button that appears below the error message. When clicked, it starts the agentic debugging cycle, which is shown as an interaction with an LLM in a separate window. \u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeaJSh6Eb0VZC75i70WOEK08H2FkPZHkQjZJ4IHoduR9CZjknYa1blnX7OW_l6MCwSvoJvjQ2xYq4X-QSNTu5jXepUHVXQA-fRIipdAxSAtCuVn6nqJYcHJ3yr79Qb2UnJTR5PPqA?key=6COu_-nzVoNQpnuO_cPEwP-y\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003col start=\"2\"\u003e\n\u003cli\u003e\u003cstrong\u003eCycle\u003c/strong\u003e: At each iteration of the debugging cycle, the LLM module collects the required information and adds it to the debugging prompt passed to the LLM, in this case, GPT-4. The LLM then generates an answer, which includes some reasoning as well as the next action, such as editing and executing cells or finishing the debugging process once the error has been resolved. The action suggested by the model is then executed in the environment, and the results are added to the prompt for the next round of debugging. \u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdAsQKC_SeTP9aDCRyZKeN0z9E5IyylCLmsvPp6UK4-wsma3UPXvLUn1Pu7o4PZUQuBa1jtBlScboc3Y2uDfR88M1aTiC9GBtoheY0tvUlqCAS_ovHxTtsdrzMpB-6yFbPa5x97?key=6COu_-nzVoNQpnuO_cPEwP-y\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003col start=\"3\"\u003e\n\u003cli\u003e\u003cstrong\u003eFinish\u003c/strong\u003e: This iterative loop continues until the error is resolved or until the limit on time or the number of iterations is reached. In our experiments, most of the debugging runs ended in the error being successfully resolved after only a few steps, so the limits were rarely required in practice.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003ch3\u003eHow does the AI agent prompt the LLM?\u003c/h3\u003e\n\n\n\n\u003cp\u003eLet’s take a closer look at the core of our AI agent system, namely, the LLM module. Our AI agent prompts the LLM using a “system prompt” that covers the general approach to the task and a “user prompt” containing specific details about the current stage of the debugging run. Below, you can see shortened versions of the two prompts.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eSystem prompt\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eGeneral purpose\u003c/td\u003e\u003ctd\u003eYou are a coding assistant that should help solve the user’s error in a computational notebook. You should use functions to help handle real-time user queries and return outputs only in the form of valid JSON.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eAvailable tools\u003c/td\u003e\u003ctd\u003eYou have a few ways of interacting with the environment:\u003cbr/\u003echange the code of the existing cells, run it, and give the output.add a new cell with your own code, run it, and give the output.run any cell as is and give the output.If you’re sure the error won’t show up in the cell where it was found, you can run finish.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eWorkflow guidelines\u003c/td\u003e\u003ctd\u003eKeep trying for at least 10 steps before you stop, but if you think you’ve solved the problem, you can run finish right away.If you can fix the error without changing any code, do that. Don’t edit the existing code or add new code unless you really need to.Use only the functions given to you. If you have many functions to choose from, pick the one that solves the problem quickest.When you think you’ve fixed the problem, run finish to say you’re done.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eHacks\u003c/td\u003e\u003ctd\u003eJust adding try – except is not a solution. Commenting out the code that produced the error is not a solution. You should propose only meaningful solutions. \u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eInitial user prompt\u003c/td\u003e\u003ctd\u003eHere’s a Jupyter notebook. It uses {separator} as a separator between cells. Note that cell indexes START FROM 1! {notebook} An error occurred in the cell with num {cell_num}. The error trace is the following: {error} Please resolve the error. You must use only the tools defined above to resolve the error. Return output only as valid JSON. \u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe system prompt opens with a general description of the problem, provides some guidelines on how it should be solved, and then closes with instructions about actions. We use reflection as the algorithm for choosing the next action, asking the LLM to reflect on the outcomes of the previous actions taken before selecting the next tool to call. The reflections are output as comments for each selected action and are available at all following steps. The guidelines also encourage the LLM to explore the environment and discourage large outputs and “hacks”, such as deleting the error cell or commenting out the problematic code. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe user prompt initially consists of the notebook code and the error cell number, again accompanied by a short task description. After each step, the actions suggested by the LLM are converted into commands and executed, and the result is collected as an input for the next user prompt, along with the conversation history.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou can observe the entire debugging process in real time via the interface where the conversation and the actions are visualized.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow costly is the AI agent?\u003c/h2\u003e\n\n\n\n\u003cp\u003eA significant cost with any AI system comes from running large language models, with costs primarily driven by the number of request and response tokens processed.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo compare costs, we evaluated the AI agent against a simpler zero-shot debugging approach in Datalore, using 100 hours of hackathon coding records. The dataset featured an extensive notebook action history, so the errors that occurred could be reproduced and addressed by either of the AI solutions. On average, resolving an error with the AI agent was 2.5 times as expensive, costing $0.22 per error with GPT-4 versus $0.09 with the simpler method.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe AI agent’s higher cost came mainly from using three times as many request tokens for its memory stack, while the number of response tokens was similar between methods. Nevertheless, the cost remained practical for industry use since input tokens are cheaper and most errors were fixed on the first step.\u003c/p\u003e\n\n\n\n\u003ch2\u003eUser feedback about the AI agent\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe used the same zero-shot baseline to compare the user experiences with these two AI algorithms. In particular, we wanted to know how well users thought the AI agent resolved problems and how much they believed it could improve their productivity. We had two groups of programmers complete the same data-filtering task. One group used the zero-shot baseline, and the other used our agentic solution.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfZXbDn8nc2dSsGutdNKxMZMTaX8yssBpNM8iPP3bJW5MxtoYYMDloUhpsAxziXYmefeain2mNUfTkfBQ7lj78nqs7wyKWrrVJbnkXoJOKDxDZMJ8PoAbezLpkJmeQ34pw3bUEf?key=6COu_-nzVoNQpnuO_cPEwP-y\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWe found that the users were quite satisfied with both solutions, rating them either good or excellent, and they slightly favored the AI agent. The users also seemed to rely more on the AI agent. However, the participants also reported that the agentic solution was less easy to use. We suspect this may be due to the lack of user control over its actions as well as the speed at which it acts. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn the feedback, the users suggested that slowing down the conversation and highlighting the introduced changes in the notebook would provide more insight into the AI agent’s thought process and solution. They also proposed making the agentic solution process more interactive so they have a way of interrupting or changing the actions suggested by the AI agent. Finally, they asked for a way to access the agent before any error has occurred.\u003c/p\u003e\n\n\n\n\u003ch2\u003eBenefits of AI debugging: Looking forward\u003c/h2\u003e\n\n\n\n\u003cp\u003eFinally, we compared the AI agent to the unaided human debugging process that comprised our dataset. In this comparison, the agentic solution offers obvious benefits:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFaster debugging\u003c/strong\u003e: The AI agent typically fixed issues in under a minute, while humans took an average of 3.5 minutes and up to 20 minutes for more complex cases.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eEffectiveness for common small errors\u003c/strong\u003e: The agent quickly resolved small bugs, like incorrect pandas functions or arguments.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eInsights into root causes\u003c/strong\u003e: The agent highlighted and explained changes, which helped to identify root causes of and solutions for errors.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eThe main takeaways of the study for the agentic debugging research concern the areas of improvement:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReduced cost\u003c/strong\u003e: One solution to the higher cost of the agentic approach could be history caching, which can reduce the number of request tokens used. \u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eIncreased user control\u003c/strong\u003e: The study of user experience highlighted that users would prefer to have more control over the process, such as the option to call the agent before any error occurs. \u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eBuilding on top of this work, we will continue making Datalore smarter with AI-based features.\u003c/p\u003e\n                    \n                                                                                                                                                                                                                            \u003cdiv\u003e\n                                \u003cdiv\u003e\n                                                                            \u003ch4\u003eSubscribe to JetBrains AI Blog updates\u003c/h4\u003e\n                                                                                                            \n                                \u003c/div\u003e\n                                \n                                \u003cp\u003e\u003cimg src=\"https://blog.jetbrains.com/wp-content/themes/jetbrains/assets/img/img-form.svg\" alt=\"image description\"/\u003e\n                                                                    \u003c/p\u003e\n                            \u003c/div\u003e\n                                                            \u003c/div\u003e\n                \u003ca href=\"#\"\u003e\u003c/a\u003e\n                \n                \n            \u003c/section\u003e\n                    \u003cdiv\u003e\n                \u003cp\u003e\n                    \u003ch2\u003eDiscover more\u003c/h2\u003e\n                \u003c/p\u003e\n                \n            \u003c/div\u003e\n                \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": null,
  "modifiedTime": null
}
