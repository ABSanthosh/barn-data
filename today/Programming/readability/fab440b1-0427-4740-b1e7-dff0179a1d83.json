{
  "id": "fab440b1-0427-4740-b1e7-dff0179a1d83",
  "title": "Databricks Contributes Spark Declarative Pipelines to Apache Spark",
  "link": "https://www.infoq.com/news/2025/07/databricks-declarative-pipelines/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "At the Databricks Data+AI Summit, held in San Francisco, USA, from June 10 to 12, Databricks announced that it is contributing the technology behind Delta Live Tables (DLT) to the Apache Spark project, where it will be called Spark Declarative Pipelines. This move will make it easier for Spark users to develop and maintain streaming pipelines, and furthers Databrick’s commitment to open source. By Patrick Farry",
  "author": "Patrick Farry",
  "published": "Thu, 03 Jul 2025 13:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Apache Spark",
    "Open Source",
    "Event Stream Processing",
    "Architecture \u0026 Design",
    "news"
  ],
  "byline": "Patrick Farry",
  "length": 3659,
  "excerpt": "At the Databricks Data+AI Summit, held in San Francisco, USA, from June 10 to 12, Databricks announced that it is contributing the technology behind Delta Live Tables (DLT) to the Apache Spark project",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250605075544/apple-touch-icon.png",
  "text": "At the Databricks Data+AI Summit, held in San Francisco, USA, from June 10 to 12, Databricks announced that it is contributing the technology behind Delta Live Tables (DLT) to the Apache Spark project, where it will be called Spark Declarative Pipelines. This move will make it easier for Spark users to develop and maintain streaming pipelines, and furthers Databrick’s commitment to open source. The new feature will allow developers to define data streaming pipelines without needing to create the usual imperative commands in Spark. While the changes simplify the task of writing and maintaining pipeline code, users will still need to understand the runtime behavior of Spark and be able to troubleshoot issues such as performance and correctness. In a blog post that describes the new feature, Databricks wrote that pipelines could be defined using SQL syntax or via a simple Python SDK that declares the stream data sources, tables and their relationship, rather than writing imperative Spark commands. The company claims this will reduce the need for orchestrators such as Apache Airflow to manage pipelines. Behind the scenes, the framework interprets the query then creates a dependency graph and optimized execution plan. Declarative Pipelines supports streaming tables from stream data sources such as Apache Kafka topics, and materialized views for storing aggregates and results. The materialized views are updated automatically as new data arrives from the streaming tables. Databricks provide an overview of the SQL syntax in their documentation. An excerpt is shown here. The example is based on the New York City TLC Trip Record Data data set. -- Bronze layer: Raw data ingestion CREATE OR REFRESH STREAMING TABLE taxi_raw_records (CONSTRAINT valid_distance EXPECT (trip_distance \u003e 0.0) ON VIOLATION DROP ROW) AS SELECT * FROM STREAM(samples.nyctaxi.trips); -- Silver layer 1: Flagged rides CREATE OR REFRESH STREAMING TABLE flagged_rides AS SELECT date_trunc(\"week\", tpep_pickup_datetime) as week, pickup_zip as zip, fare_amount, trip_distance FROM STREAM(LIVE.taxi_raw_records) WHERE ((pickup_zip = dropoff_zip AND fare_amount \u003e 50) OR (trip_distance \u003c 5 AND fare_amount \u003e 50)); The example shows how a pipeline can be built by defining streams, with the CREATE STREAMING TABLE command, and then consuming them with a FROM statement in subsequent queries.. Of note in the example is the ability to include data quality checks in the pipeline with the syntax CONSTRAIN … EXPECT … ON VIOLATION. While the Apache Spark changes are not yet released, many articles already describe the experience of engineers using Databricks DLT. In an article in Medium titled “Why I Liked Delta Live Tables in Databricks,” Mariusz Kujawski describes the features of DLT and how they can best be used: “With DLT, you can build an ingestion pipeline in just a few hours, compared to the days required to develop a custom framework. Additionally, built-in data quality enforcement provides an extra layer of reliability.” In addition to a declarative syntax for defining a pipeline, Spark Declarative Pipelines also supports change data capture (CDC), batch and stream logic, built in retry logic, and observability hooks. Declarative pipelines are in the process of being merged into the Spark project. The feature is planned for the next Spark Release, 4.10, which is expected in January 2026. Progress can be followed on the Apache Jira Spark project in ticket SPARK-51727. About the Author Patrick Farry",
  "image": "https://res.infoq.com/news/2025/07/databricks-declarative-pipelines/en/headerimage/databricks-declarative-pipelines-1751267295205.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eAt the \u003ca href=\"https://www.databricks.com/dataaisummit\"\u003eDatabricks Data+AI Summit\u003c/a\u003e, held in San Francisco, USA, from June 10 to 12, Databricks announced that it is \u003ca href=\"https://www.databricks.com/blog/bringing-declarative-pipelines-apache-spark-open-source-project\"\u003econtributing the technology behind Delta Live Tables (DLT) to the Apache Spark project\u003c/a\u003e, where it will be called Spark Declarative Pipelines. This move will make it easier for Spark users to develop and maintain streaming pipelines, and furthers Databrick’s commitment to open source.\u003c/p\u003e\n\n\u003cp\u003eThe new feature will allow developers to define data streaming pipelines without needing to create the usual imperative commands in Spark. While the changes simplify the task of writing and maintaining pipeline code, users will still need to understand the runtime behavior of Spark and be able to troubleshoot issues such as performance and correctness.\u003c/p\u003e\n\n\u003cp\u003eIn a blog post that \u003ca href=\"https://www.databricks.com/blog/bringing-declarative-pipelines-apache-spark-open-source-project\"\u003edescribes the new feature\u003c/a\u003e, Databricks wrote that pipelines could be defined using SQL syntax or via a simple Python SDK that declares the stream data sources, tables and their relationship, rather than writing imperative Spark commands. The company claims this will reduce the need for orchestrators such as Apache Airflow to manage pipelines.\u003c/p\u003e\n\n\u003cp\u003eBehind the scenes, the framework interprets the query then creates a dependency graph and optimized execution plan.\u003c/p\u003e\n\n\u003cp\u003eDeclarative Pipelines supports streaming tables from stream data sources such as Apache Kafka topics, and materialized views for storing aggregates and results. The materialized views are updated automatically as new data arrives from the streaming tables.\u003c/p\u003e\n\n\u003cp\u003eDatabricks provide an \u003ca href=\"https://www.databricks.com/discover/pages/getting-started-with-delta-live-tables\"\u003eoverview of the SQL syntax\u003c/a\u003e in their documentation. An excerpt is shown here. The example is based on the New York City TLC Trip Record Data data set.\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e-- Bronze layer: Raw data ingestion\nCREATE OR REFRESH STREAMING TABLE taxi_raw_records \n(CONSTRAINT valid_distance EXPECT (trip_distance \u0026gt; 0.0) ON VIOLATION DROP ROW)\nAS SELECT *\nFROM STREAM(samples.nyctaxi.trips);\n\n-- Silver layer 1: Flagged rides\nCREATE OR REFRESH STREAMING TABLE flagged_rides \nAS SELECT\n  date_trunc(\u0026#34;week\u0026#34;, tpep_pickup_datetime) as week,\n  pickup_zip as zip, \n  fare_amount, trip_distance\nFROM\n  STREAM(LIVE.taxi_raw_records)\nWHERE ((pickup_zip = dropoff_zip AND fare_amount \u0026gt; 50) OR\n       (trip_distance \u0026lt; 5 AND fare_amount \u0026gt; 50));\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThe example shows how a pipeline can be built by defining streams, with the CREATE STREAMING TABLE command, and then consuming them with a FROM statement in subsequent queries.. Of note in the example is the ability to include data quality checks in the pipeline with the syntax CONSTRAIN … EXPECT … ON VIOLATION.\u003c/p\u003e\n\n\u003cp\u003eWhile the Apache Spark changes are not yet released, many articles already describe the experience of engineers using Databricks DLT. In an article in Medium titled “\u003ca href=\"https://medium.com/@mariusz_kujawski/why-i-liked-delta-live-tables-in-databricks-b55b5a97c55c\"\u003eWhy I Liked Delta Live Tables in Databricks\u003c/a\u003e,” Mariusz Kujawski describes the features of DLT and how they can best be used: “With DLT, you can build an ingestion pipeline in just a few hours, compared to the days required to develop a custom framework. Additionally, built-in data quality enforcement provides an extra layer of reliability.”\u003c/p\u003e\n\n\u003cp\u003eIn addition to a declarative syntax for defining a pipeline, Spark Declarative Pipelines also supports change data capture (CDC), batch and stream logic, built in retry logic, and observability hooks.\u003c/p\u003e\n\n\u003cp\u003eDeclarative pipelines are in the process of being merged into the Spark project. The feature is planned for the next Spark Release, 4.10, which is expected in January 2026. Progress can be followed on the Apache Jira Spark project in ticket \u003ca href=\"https://issues.apache.org/jira/browse/SPARK-51727\"\u003eSPARK-51727\u003c/a\u003e.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Patrick-Farry\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003ePatrick Farry\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-07-03T00:00:00Z",
  "modifiedTime": null
}
