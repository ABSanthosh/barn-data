{
  "id": "afd46355-f097-4593-84f1-1b4c85e7962f",
  "title": "Automated Accessibility Testing at Slack",
  "link": "https://slack.engineering/automated-accessibility-testing-at-slack/",
  "description": "At Slack, customer love is our first priority and accessibility is a core tenet of customer trust. We have our own Slack Accessibility Standards that product teams follow to guarantee their features are compliant with Web Content Accessibility Guidelines (WCAG). Our dedicated accessibility team supports developers in following these guidelines throughout the development process. We… The post Automated Accessibility Testing at Slack appeared first on Engineering at Slack.",
  "author": "nstormann",
  "published": "Tue, 07 Jan 2025 20:46:20 +0000",
  "source": "https://slack.engineering/feed",
  "categories": [
    "Uncategorized",
    "a11y",
    "accessibility",
    "automation-testing",
    "testing"
  ],
  "byline": "",
  "length": 18083,
  "excerpt": "At Slack, customer love is our first priority and accessibility is a core tenet of customer trust. We have our own Slack Accessibility Standards that product teams follow to guarantee their features are compliant with Web Content Accessibility Guidelines (WCAG). Our dedicated accessibility team supports developers in following these guidelines throughout the development process. We…",
  "siteName": "Engineering at Slack",
  "favicon": "https://slack.engineering/wp-content/uploads/sites/7/2020/05/cropped-octothrope-1.png?w=192",
  "text": "At Slack, customer love is our first priority and accessibility is a core tenet of customer trust. We have our own Slack Accessibility Standards that product teams follow to guarantee their features are compliant with Web Content Accessibility Guidelines (WCAG). Our dedicated accessibility team supports developers in following these guidelines throughout the development process. We also frequently collaborate with external manual testers that specialize in accessibility.  In 2022, we started to supplement Slack’s accessibility strategy by setting up automated accessibility tests for desktop to catch a subset of accessibility violations throughout the development process. At Slack, we see automated accessibility testing as a valuable addition to our broader testing strategy. This broader strategy also includes involving people with disabilities early in the design process, conducting design and prototype review with these users, and performing manual testing across all of the assistive technologies we support. Automated tools can overlook nuanced accessibility issues that require human judgment, such as screen reader usability. Additionally, these tools can also flag issues that don’t align with the product’s specific design considerations. Despite that, we still felt there would be value in integrating an accessibility testing tool into our test frameworks as part of the overall, comprehensive testing strategy. Ideally, we were hoping to add another layer of support by integrating the accessibility validation directly into our existing frameworks so test owners could easily add checks, or better yet, not have to think about adding checks at all. Exploration and Limitations  Unexpected Complexities: Axe, Jest, and React Testing Library (RTL)  We chose to work with Axe, a popular and easily configurable accessibility testing tool, for its extensive capabilities and compatibility with our current end-to-end (E2E) test frameworks. Axe checks against a wide variety of accessibility guidelines, most of which correspond to specific success criteria from WCAG, and it does so in a way that minimizes false positives. Initially we explored the possibility of embedding Axe accessibility checks directly into our React Testing Library (RTL) framework. By wrapping RTL’s render method with a custom render function that included the Axe check, we could remove a lot of friction from the developer workflow. However, we immediately encountered an issue related to the way we’ve customized our Jest set up at Slack. Running accessibility checks through a separate Jest configuration worked, but would require developers to write tests specifically for accessibility, which we wanted to avoid. Reworking our custom Jest setup was deemed too tricky and not worth the time and resource investment, so we pivoted to focus on our Playwright framework. The Best Solution for Axe Checks: Playwright With Jest ruled out as a candidate for Axe, we turned to Playwright, the E2E test framework utilized at Slack. Playwright supports accessibility testing with Axe through the @axe-core/playwright package. Axe Core provides most of what you’ll need to filter and customize accessibility checks. It provides an exclusion method right out of the box, to prevent certain rules and selectors from being analyzed. It also comes with a set of accessibility tags to further specify the type of analysis to conduct (‘wcag2a‘, ‘wcag2aa‘, etc.).  Our initial goal was to “bake” accessibility checks directly into Playwright’s interaction methods, such as clicks and navigation, to automatically run Axe without requiring test authors to explicitly call it.  In working towards that goal, we found that the main challenge with this approach stems from Playwright’s Locator object. The Locator object is designed to simplify interaction with page elements by managing auto-waiting, loading, and ensuring the element is fully interactable before any action is performed. This automatic behavior is integral to Playwright’s ability to maintain stable tests, but it complicated our attempts to embed Axe into the framework.  Accessibility checks should run when the entire page or key components are fully rendered, but Playwright’s Locator only ensures the readiness of individual elements, not the overall page. Modifying the Locator could lead to unreliable audits because accessibility issues might go undetected if checks were run at the wrong time. Another option, using deprecated methods like waitForElement to control when accessibility checks are triggered, was also problematic. These older methods are less optimized, causing performance degradation, potential duplication of errors, and conflicts with the abstraction model that Playwright follows.  So while embedding Axe checks into Playwright’s core interaction methods seemed ideal, the complexity of Playwright’s internal mechanisms required us to explore some further solutions. Customizations and Workarounds  To circumvent the roadblocks we encountered with embedding accessibility checks into the frameworks, we decided to make some concessions while still prioritizing a simplified developer workflow. We continued to focus on Playwright because it offered more flexibility in how we could selectively hide or apply accessibility checks, allowing us to more easily manage when and where these checks were run. Additionally, Axe Core came with some great customization features, such as filtering rules and using specific accessibility tags.  Using the @axe-core/playwright package, we can describe the flow of our accessibility check: Playwright test lands on a page/view Axe analyzes the page Pre-defined exclusions are filtered out Violations and artifacts are saved to a file  First, we set up our main function, runAxeAndSaveViolations, and customized the scope using what the AxeBuilder class provides.  We wanted to check for compliance with WCAG 2.1, Levels A and AA.  constructor(page: Page) { this.page = page; this.defaultTags = ['wcag2a', 'wcag2aa', 'wcag21a', 'wcag21aa']; this.bodyText = ''; this.baseFileName = `${test.info().title}-violations`.replace(/\\//g, '-'); A11y.filenameCounter = 0; } We created a list of selectors to exclude from our violations report. These fell into two main categories: Known accessibility issues – issues that we are aware of and have already been ticketed  Rules that don’t apply – Axe rules outside of the scope of how Slack is designed for accessibility // Exclude selectors for known bugs and elements that we do not consider accessibility issues constants.ACCESSIBILITY.AXE_EXCLUDED_SELECTORS.forEach((excludedSelector) =\u003e { axe.exclude(excludedSelector); }); We also wanted to filter for duplication and severity level. We created methods to check for the uniqueness of each violation and filter out duplication. We chose to report only the violations deemed Critical according to the WCAG. Serious, Moderate, and Mild are other possible severity levels that we may add in the future. /** * Filter violations based on criticality, then ensure we * are removing any duplicate violations within a single test file * Please note: this only removes duplicates on a single test, not the entire run */ private filterAndRemoveDuplicateViolations(violations: Violation[]) { return violations .filter((violation) =\u003e ['critical'].includes(violation.impact)) .map(this.mapViolation) .filter(this.isUniqueViolation.bind(this)); } We took advantage of the Playwright fixture model. Fixtures are Playwright’s way to build up and teardown state outside of the test itself. Within our framework we’ve created a custom fixture called slack which provides access to all of our API calls, UI views, workflows and utilities related to Slack. Using this fixture, we can access all of these resources directly in our tests without having to go through the setup process every time. We moved our accessibility helper to be part of the pre-existing slack fixture. This allowed us to call it directly in the test spec, minimizing some of the overhead for our test authors. // Run accessibility checks and get the violations await slack.utils.a11y.runAxeAndSaveViolations(); We also took advantage of the ability to customize Playwright’s test.step. We added the custom label “Running accessibility checks in runAxeAndSaveViolations” to make it easier to detect where an accessibility violation has occurred: Test Steps - Before Hooks - apiResponse.json— ../support/api/api.ts:137 - browserContext.waitForEvent— ../support/workflows/login.workflow.ts:273 - Running accessibility checks in runAxeAndSaveViolations— ../support/utils/accessibility.ts:54 Placement of Accessibility Checks in End to End Tests To kick the project off, we set up a test suite that mirrored our suite for testing critical functionality at Slack. We renamed the suite to make it clear it was for accessibility tests, and we set it to run as non-blocking. This meant developers would see the test results, but a failure or violation would not prevent them from merging their code to production. This initial suite encompassed 91 tests in total. Strategically, we considered the placement of accessibility checks within these critical flow tests. In general, we aimed to add an accessibility check for each new view, page, or flow covered in the test. In most cases, this meant placing a check directly after a button click, for example, or a link that leads to navigation. In other scenarios, our accessibility check needed to be placed after signing in as a second user or after a redirect.  It was important to make sure the same view wasn’t being analyzed twice in one test, or potentially twice across multiple tests with the same UI flow. Duplication like this would result in unnecessary error messages and saved artifacts, and slow down our tests. We were also careful to place our Axe calls only after the page or view had fully loaded and all content had rendered. With this approach, we needed to be deeply familiar with the application and the context of each test case.  Violations Reporting We spent some time iterating on our accessibility violations report. Initially, we created a simple text file to save the results of a local run, storing it in an artifacts folder. A few developers gave us early feedback and requested screenshots of the pages where accessibility violations occurred. To achieve this, we integrated Playwright’s screenshot functionality and began saving these screenshots alongside our text report in the same artifact folder. To make our reports more coherent and readable, we leveraged the Playwright HTML Reporter. This tool not only aggregates test results but also allows us to attach artifacts such as screenshots and violation reports to the HTML output. By configuring the HTML reporter, we were able to display all of our accessibility artifacts, including screenshots and detailed violation reports, in a single test report.  Lastly, we wanted our violation error message to be helpful and easy to understand, so we wrote some code to pull out key pieces of information from the violation. We also customized how the violations were displayed in the reports and on the console, by parsing and condensing the error message.    Error - [A11Y]: CRITICAL Description: Ensures an element's role supports its ARIA attributes Help: Elements must only use supported ARIA attributes Target selector: #add-channel-tab Fix all of the following: ARIA attribute is not allowed: aria-selected=\"false\" HTML: \u003cbutton class=\"c-button-unstyled addTab__brBMy c-tabs__tab js-tab\" data-qa=\"unstyled-button\" Environment Setup and Running Tests  Once we had integrated our Axe checks and set up our test suite, we needed to determine how and when developers should run them. To streamline the process for developers, we introduced an environment flag, A11Y_ENABLE, to control the activation of accessibility checks within our framework. By default, we set the flag to false, preventing unnecessary runs.  This setup allowed us to offer developers the following options: On-Demand Testing: Developers can manually enable the flag when they need to run accessibility checks locally on their branch. Scheduled Runs: Developers can configure periodic runs during off-peak hours. We have a daily regression run configured in Buildkite to pipe accessibility test run results into a Slack alert channel on a daily cadence. CI Integration: Optionally, the flag can be enabled in continuous integration pipelines for thorough testing before merging significant changes.  Triage and Ownership Ownership of individual tests and test suites is often a hot topic when it comes to maintaining tests. Once we had added Axe calls to the critical flows in our Playwright E2E tests, we needed to decide who would be responsible for triaging accessibility issues discovered via our automation and who would own the test maintenance for existing tests.  At Slack, we enable developers to own test creation and maintenance for their tests. To support developers to better understand the framework changes and new accessibility automation, we created documentation and partnered with the internal Slack accessibility team to come up with a comprehensive triage process that would fit into their existing workflow for triaging accessibility issues.  The internal accessibility team at Slack had already established a process for triaging and labeling incoming accessibility issues, using the internal Slack Accessibility Standards as a guideline. To enhance the process, we created a new label for “automated accessibility” so we could track the issues discovered via our automation. To make cataloging these issues easier, we set up a Jira workflow in our alerts channel that would spin up a Jira ticket with a pre-populated template. The ticket is created via the workflow and automatically labeled with automated accessibility and placed in a Jira Epic for triaging. A11Y Automation Bug Ticket Creator - Automatically create JIRA bug tickets for A11Y automation violations Hi there, Would you like to create a new JIRA defect? Button clicked. A new JIRA bug ticket, A11YAUTO-37, was created. What to do next: 1. Please fill out all of the necessary information listed here: https://jira.tinyspeck.com/browse/A11YAUTO-37. 2. Please add this locator to the list of known issues and include the new JIRA bug ticket in the comment. Conducting Audits  We perform regular audits of our accessibility Playwright calls to check for duplication of Axe calls, and ensure proper coverage of accessibility checks across tests and test suites. We developed a script and an environment flag specifically to facilitate the auditing process. Audits can be performed either through sandbox test runs (ideal for suite-wide audits) or locally (for specific tests or subsets). When performing an audit, running the script allows us to take a screenshot of every page that performs an Axe call. The screenshots are then saved to a folder and can be easily compared to spot duplicates.  This process is more manual than we like, and we are looking into ways to eliminate this step, potentially leaning on AI assistance to perform the audit for us – or have AI add our accessibility calls to each new page/view, thereby eliminating the need to perform any kind of audit at all. What’s Next  We plan to continue partnering with the internal accessibility team at Slack to design a small blocking test suite. These tests will be dedicated to the flows of core features within Slack, with a focus on keyboard navigation.  We’d also like to explore AI-driven approaches to the post-processing of accessibility test results and look into the option of having AI assistants audit our suites to determine the placement of our accessibility checks, further reducing the manual effort for developers.  Closing Thoughts  We had to make some unexpected trade-offs in this project, balancing the practical limitations of automated testing tools with the goal of reducing the burden on developers. While we couldn’t integrate accessibility checks completely into our frontend frameworks, we made significant strides towards that goal. We simplified the process for developers to add accessibility checks, ensured test results were easy to interpret, provided clear documentation, and streamlined triage through Slack workflows. In the end, we were able to add test coverage for accessibility in the Slack product, ensuring that our customers that require accessibility features have a consistent experience. Our automated Axe checks have reduced our reliance on manual testing and now complement other essential forms of testing—like manual testing and usability studies. At the moment, developers need to manually add checks, but we’ve laid the groundwork to make the process as straightforward as possible with the possibility for AI-driven creation of accessibility tests. Roadblocks like framework complexity or setup difficulties shouldn’t discourage you from pursuing automation as part of a broader accessibility strategy. Even when it’s not feasible to hide the automated checks completely behind the scenes of the framework, there are ways to make the work impactful by focusing on the developer experience. This project has not only strengthened our overall accessibility testing approach, it’s also reinforced the culture of accessibility that has always been central to Slack. We hope it inspires others to look more closely at how automated accessibility might fit into your testing strategy, even if it requires navigating a few technical hurdles along the way. Thank you to everyone who spent significant time on the editing and revision of this blog post – Courtney Anderson-Clark, Lucy Cheng, Miriam Holland and Sergii Gorbachov. And a massive thank you to the Accessibility Team at Slack, Chanan Walia, Yura Zenevich, Chris Xu and Hye Jung Choi, for your help with everything related to this project, including editing this blog post!",
  "image": "https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg?w=640",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\u003cp\u003e\u003cspan\u003eAt Slack, customer love is our first priority and accessibility is a core tenet of customer trust. We have our own Slack Accessibility Standards that product teams follow to guarantee their features are compliant with Web Content Accessibility Guidelines (WCAG). Our dedicated accessibility team supports developers in following these guidelines throughout the development process. We also frequently collaborate with external manual testers that specialize in accessibility. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn 2022, we started to supplement Slack’s accessibility strategy by setting up automated accessibility tests for desktop to catch a subset of accessibility violations throughout the development process. At Slack, we see automated accessibility testing as a valuable addition to our broader testing strategy. This broader strategy also includes involving people with disabilities early in the design process, conducting design and prototype review with these users, and performing manual testing across all of the assistive technologies we support. Automated tools can overlook nuanced accessibility issues that require human judgment, such as screen reader usability. Additionally, these tools can also flag issues that don’t align with the product’s specific design considerations.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eDespite that, we still felt there would be value in integrating an accessibility testing tool into our test frameworks as part of the overall, comprehensive testing strategy. Ideally, we were hoping to add another layer of support by integrating the accessibility validation directly into our existing frameworks so test owners could easily add checks, or better yet, not have to think about adding checks at all.\u003c/span\u003e\u003cb\u003e\u003c/b\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003ch3\u003e\u003cb\u003eExploration and Limitations \u003c/b\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cb\u003eUnexpected Complexities: Axe, Jest, and React Testing Library (RTL) \u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe chose to work with Axe, a popular and easily configurable accessibility testing tool, for its extensive capabilities and compatibility with our current end-to-end (E2E) test frameworks. Axe checks against a wide variety of accessibility guidelines, most of which correspond to specific success criteria from WCAG, and it does so in a way that minimizes false positives.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eInitially we explored the possibility of embedding Axe accessibility checks directly into our React Testing Library (RTL) framework. By wrapping RTL’s render method with a custom render function that included the Axe check, we could remove a lot of friction from the developer workflow. However, we immediately encountered an issue related to the way we’ve customized our Jest set up at Slack. Running accessibility checks through a separate Jest configuration worked, but would require developers to write tests specifically for accessibility, which we wanted to avoid. Reworking our custom Jest setup was deemed too tricky and not worth the time and resource investment, so we pivoted to focus on our Playwright framework.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eThe Best Solution for Axe Checks: Playwright\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith Jest ruled out as a candidate for Axe, we turned to Playwright, the E2E test framework utilized at Slack. Playwright supports accessibility testing with Axe through the \u003c/span\u003e\u003ca href=\"https://www.npmjs.com/package/@axe-core/playwright\"\u003e\u003cspan\u003e@axe-core/playwright\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e package. Axe Core provides most of what you’ll need to filter and customize accessibility checks. It provides an exclusion method right out of the box, to prevent certain rules and selectors from being analyzed. It also comes with a set of accessibility tags to further specify the type of analysis to conduct (‘\u003ccode\u003ewcag2a\u003c/code\u003e‘, ‘\u003ccode\u003ewcag2aa\u003c/code\u003e‘, etc.). \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur initial goal was to “bake” accessibility checks directly into Playwright’s interaction methods, such as clicks and navigation, to automatically run Axe without requiring test authors to explicitly call it. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn working towards that goal, we found that the main challenge with this approach stems from Playwright’s Locator object. The Locator object is designed to simplify interaction with page elements by managing auto-waiting, loading, and ensuring the element is fully interactable before any action is performed. This automatic behavior is integral to Playwright’s ability to maintain stable tests, but it complicated our attempts to embed Axe into the framework. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAccessibility checks should run when the entire page or key components are fully rendered, but Playwright’s Locator only ensures the readiness of individual elements, not the overall page. Modifying the Locator could lead to unreliable audits because accessibility issues might go undetected if checks were run at the wrong time.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAnother option, using deprecated methods like \u003c/span\u003e\u003ccode\u003e\u003cspan\u003ewaitForElement \u003c/span\u003e\u003c/code\u003e\u003cspan\u003eto control when accessibility checks are triggered, was also problematic. These older methods are less optimized, causing performance degradation, potential duplication of errors, and conflicts with the abstraction model that Playwright follows. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSo while embedding Axe checks into Playwright’s core interaction methods seemed ideal, the complexity of Playwright’s internal mechanisms required us to explore some further solutions.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" width=\"6960\" height=\"4640\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg?w=640\" alt=\"\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg 6960w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg?resize=640,427 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg?resize=768,512 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg?resize=1280,853 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg?resize=1536,1024 1536w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg?resize=2048,1365 2048w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg?resize=380,253 380w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg?resize=800,533 800w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/joachim-schnurle-4j732BLw0BI-unsplash.jpg?resize=1160,773 1160w\" sizes=\"(max-width: 6960px) 100vw, 6960px\"/\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003ch3\u003e\u003cb\u003eCustomizations and Workarounds \u003c/b\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eTo circumvent the roadblocks we encountered with embedding accessibility checks into the frameworks, we decided to make some concessions while still prioritizing a simplified developer workflow. We continued to focus on Playwright because it offered more flexibility in how we could selectively hide or apply accessibility checks, allowing us to more easily manage when and where these checks were run. Additionally, Axe Core came with some great customization features, such as filtering rules and using specific accessibility tags. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eUsing the \u003c/span\u003e\u003ca href=\"https://www.npmjs.com/package/@axe-core/playwright\"\u003e\u003cspan\u003e@axe-core/playwright\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e package, we can describe the flow of our accessibility check:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan\u003ePlaywright test lands on a page/view\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eAxe analyzes the page\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003ePre-defined exclusions are filtered out\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eViolations and artifacts are saved to a file \u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eFirst, we set up our main function, \u003c/span\u003e\u003ccode\u003e\u003cspan\u003erunAxeAndSaveViolations\u003c/span\u003e\u003c/code\u003e\u003cspan\u003e, and customized the scope using what the \u003ccode\u003eAxeBuilder\u003c/code\u003e class provides. \u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003eWe wanted to check for compliance with WCAG 2.1, Levels A and AA. \u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003econstructor(page: Page) { \n     this.page = page; \n     this.defaultTags = [\u0026#39;wcag2a\u0026#39;, \u0026#39;wcag2aa\u0026#39;, \u0026#39;wcag21a\u0026#39;, \u0026#39;wcag21aa\u0026#39;]; \n     this.bodyText = \u0026#39;\u0026#39;; \n     this.baseFileName = `${test.info().title}-violations`.replace(/\\//g, \u0026#39;-\u0026#39;); \n     A11y.filenameCounter = 0; \n}\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003eWe created a list of selectors to exclude from our violations report. These fell into two main categories:\u003c/span\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan\u003eKnown accessibility issues – issues that we are aware of and have already been ticketed \u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eRules that don’t apply – Axe rules outside of the scope of how Slack is designed for accessibility\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e// Exclude selectors for known bugs and elements that we do not consider accessibility issues\nconstants.ACCESSIBILITY.AXE_EXCLUDED_SELECTORS.forEach((excludedSelector) =\u0026gt; {\n     axe.exclude(excludedSelector);\n});\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003eWe also wanted to filter for duplication and severity level. We created methods to check for the uniqueness of each violation and filter out duplication. We chose to report only the violations deemed \u003ccode\u003eCritical\u003c/code\u003e according to the WCAG. \u003ccode\u003eSerious\u003c/code\u003e, \u003ccode\u003eModerate\u003c/code\u003e, and \u003ccode\u003eMild\u003c/code\u003e are other possible severity levels that we may add in the future.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e/**\n* Filter violations based on criticality, then ensure we\n* are removing any duplicate violations within a single test file\n* Please note: this only removes duplicates on a single test, not the entire run\n*/\nprivate filterAndRemoveDuplicateViolations(violations: Violation[]) {\n   return violations\n\t.filter((violation) =\u0026gt; [\u0026#39;critical\u0026#39;].includes(violation.impact))\n\t.map(this.mapViolation)\n\t.filter(this.isUniqueViolation.bind(this));\n}\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003eWe took advantage of the Playwright fixture model. Fixtures are Playwright’s way to build up and teardown state outside of the test itself. Within our framework we’ve created a custom fixture called \u003c/span\u003e\u003ccode\u003e\u003cspan\u003eslack\u003c/span\u003e\u003c/code\u003e\u003cspan\u003e which provides access to all of our API calls, UI views, workflows and utilities related to Slack. Using this fixture, we can access all of these resources directly in our tests without having to go through the setup process every time.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eWe moved our accessibility helper to be part of the pre-existing \u003c/span\u003e\u003ccode\u003e\u003cspan\u003eslack\u003c/span\u003e\u003c/code\u003e\u003cspan\u003e fixture. This allowed us to call it directly in the test spec, minimizing some of the overhead for our test authors.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e// Run accessibility checks and get the violations\nawait slack.utils.a11y.runAxeAndSaveViolations();\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003eWe also took advantage of the ability to customize Playwright’s \u003c/span\u003e\u003ccode\u003e\u003cspan\u003etest.step\u003c/span\u003e\u003c/code\u003e\u003cspan\u003e. We added the custom label “Running accessibility checks in \u003ccode\u003erunAxeAndSaveViolations\u003c/code\u003e” to make it easier to detect where an accessibility violation has occurred:\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003eTest Steps\n- Before Hooks\n- apiResponse.json— ../support/api/api.ts:137\n- browserContext.waitForEvent— ../support/workflows/login.workflow.ts:273\n- Running accessibility checks in runAxeAndSaveViolations— ../support/utils/accessibility.ts:54\u003c/code\u003e\u003c/pre\u003e\n\n\u003chr/\u003e\n\u003ch3\u003e\u003cb\u003ePlacement of Accessibility Checks in End to End Tests\u003c/b\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eTo kick the project off, we set up a test suite that mirrored our suite for testing critical functionality at Slack. We renamed the suite to make it clear it was for accessibility tests, and we set it to run as non-blocking. This meant developers would see the test results, but a failure or violation would not prevent them from merging their code to production. This initial suite encompassed 91 tests in total.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eStrategically, we considered the placement of accessibility checks within these critical flow tests. In general, we aimed to add an accessibility check for each new view, page, or flow covered in the test. In most cases, this meant placing a check directly after a button click, for example, or a link that leads to navigation. In other scenarios, our accessibility check needed to be placed after signing in as a second user or after a redirect. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIt was important to make sure the same view wasn’t being analyzed twice in one test, or potentially twice across multiple tests with the same UI flow. Duplication like this would result in unnecessary error messages and saved artifacts, and slow down our tests. We were also careful to place our Axe calls only after the page or view had fully loaded and all content had rendered.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith this approach, we needed to be deeply familiar with the application and the context of each test case. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/12/krystian-tambur-k7rZ8wTfABA-unsplash.jpg?w=640\" alt=\"\" width=\"591\" height=\"443\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/12/krystian-tambur-k7rZ8wTfABA-unsplash.jpg 4000w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/krystian-tambur-k7rZ8wTfABA-unsplash.jpg?resize=640,480 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/krystian-tambur-k7rZ8wTfABA-unsplash.jpg?resize=768,576 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/krystian-tambur-k7rZ8wTfABA-unsplash.jpg?resize=1280,960 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/krystian-tambur-k7rZ8wTfABA-unsplash.jpg?resize=1536,1152 1536w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/krystian-tambur-k7rZ8wTfABA-unsplash.jpg?resize=2048,1536 2048w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/krystian-tambur-k7rZ8wTfABA-unsplash.jpg?resize=380,285 380w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/krystian-tambur-k7rZ8wTfABA-unsplash.jpg?resize=800,600 800w, https://slack.engineering/wp-content/uploads/sites/7/2024/12/krystian-tambur-k7rZ8wTfABA-unsplash.jpg?resize=1160,870 1160w\" sizes=\"(max-width: 591px) 100vw, 591px\"/\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003ch3\u003e\u003cb\u003eViolations Reporting\u003c/b\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe spent some time iterating on our accessibility violations report. Initially, we created a simple text file to save the results of a local run, storing it in an artifacts folder. A few developers gave us early feedback and requested screenshots of the pages where accessibility violations occurred. To achieve this, we integrated Playwright’s screenshot functionality and began saving these screenshots alongside our text report in the same artifact folder.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo make our reports more coherent and readable, we leveraged the Playwright HTML Reporter. This tool not only aggregates test results but also allows us to attach artifacts such as screenshots and violation reports to the HTML output. By configuring the HTML reporter, we were able to display all of our accessibility artifacts, including screenshots and detailed violation reports, in a single test report. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eLastly, we wanted our violation error message to be helpful and easy to understand, so we wrote some code to pull out key pieces of information from the violation. We also customized how the violations were displayed in the reports and on the console, by parsing and condensing the error message.   \u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eError - [A11Y]: CRITICAL \nDescription: Ensures an element\u0026#39;s role supports its ARIA attributes \nHelp: Elements must only use supported ARIA attributes Target selector: #add-channel-tab \nFix all of the following:\nARIA attribute is not allowed: aria-selected=\u0026#34;false\u0026#34; \nHTML: \u0026lt;button class=\u0026#34;c-button-unstyled addTab__brBMy c-tabs__tab js-tab\u0026#34; data-qa=\u0026#34;unstyled-button\u0026#34;\u003c/code\u003e\u003c/pre\u003e\n\n\u003chr/\u003e\n\u003ch3\u003e\u003cb\u003eEnvironment Setup and Running Tests \u003c/b\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eOnce we had integrated our Axe checks and set up our test suite, we needed to determine how and when developers should run them. To streamline the process for developers, we introduced an environment flag, \u003c/span\u003e\u003ccode\u003e\u003cspan\u003eA11Y_ENABLE\u003c/span\u003e\u003c/code\u003e\u003cspan\u003e, to control the activation of accessibility checks within our framework. By default, we set the flag to false, preventing unnecessary runs. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis setup allowed us to offer developers the following options:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003eOn-Demand Testing\u003c/b\u003e\u003cspan\u003e: Developers can manually enable the flag when they need to run accessibility checks locally on their branch.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eScheduled Runs\u003c/b\u003e\u003cspan\u003e: Developers can configure periodic runs during off-peak hours. We have a daily regression run configured in Buildkite to pipe accessibility test run results into a Slack alert channel on a daily cadence.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eCI Integration\u003c/b\u003e\u003cspan\u003e: Optionally, the flag can be enabled in continuous integration pipelines for thorough testing before merging significant changes. \u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr/\u003e\n\u003ch3\u003e\u003cb\u003eTriage and Ownership\u003c/b\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eOwnership of individual tests and test suites is often a hot topic when it comes to maintaining tests. Once we had added Axe calls to the critical flows in our Playwright E2E tests, we needed to decide who would be responsible for triaging accessibility issues discovered via our automation and who would own the test maintenance for existing tests. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAt Slack, we enable developers to own test creation and maintenance for their tests. To support developers to better understand the framework changes and new accessibility automation, we created documentation and partnered with the internal Slack accessibility team to come up with a comprehensive triage process that would fit into their existing workflow for triaging accessibility issues. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe internal accessibility team at Slack had already established a process for triaging and labeling incoming accessibility issues, using the internal Slack Accessibility Standards as a guideline. To enhance the process, we created a new label for “automated accessibility” so we could track the issues discovered via our automation.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo make cataloging these issues easier, we set up a Jira workflow in our alerts channel that would spin up a Jira ticket with a pre-populated template. The ticket is created via the workflow and automatically labeled with \u003ccode\u003eautomated accessibility\u003c/code\u003e and placed in a Jira Epic fo\u003c/span\u003e\u003cspan\u003er triaging.\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eA11Y Automation Bug Ticket Creator - \nAutomatically create JIRA bug tickets for A11Y automation violations\n\nHi there, Would you like to create a new JIRA defect? \nButton clicked. \nA new JIRA bug ticket, A11YAUTO-37, was created. \n\nWhat to do next: \n1. Please fill out all of the necessary information listed here: \nhttps://jira.tinyspeck.com/browse/A11YAUTO-37. \n\n2. Please add this locator to the list of known issues \nand include the new JIRA bug ticket in the comment.\u003c/code\u003e\u003c/pre\u003e\n\u003chr/\u003e\n\u003ch3\u003e\u003cb\u003eConducting Audits\u003c/b\u003e\u003cb\u003e \u003c/b\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe perform regular audits of our accessibility Playwright calls to check for duplication of Axe calls, and ensure proper coverage of accessibility checks across tests and test suites.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe developed a script and an environment flag specifically to facilitate the auditing process. Audits can be performed either through sandbox test runs (ideal for suite-wide audits) or locally (for specific tests or subsets). \u003c/span\u003e\u003cspan\u003eWhen performing an audit, running the script allows us to take a screenshot of every page that performs an Axe call. The screenshots are then saved to a folder and can be easily compared to spot duplicates. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis process is more manual than we like, and we are looking into ways to eliminate this step, potentially leaning on AI assistance to perform the audit for us – or have AI add our accessibility calls to each new page/view, thereby eliminating the need to perform any kind of audit at all.\u003c/span\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003ch3\u003e\u003cb\u003eWhat’s Next \u003c/b\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe plan to continue partnering with the internal accessibility team at Slack to design a small blocking test suite. These tests will be dedicated to the flows of core features within Slack, with a focus on keyboard navigation. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe’d also like to explore AI-driven approaches to the post-processing of accessibility test results and look into the option of having AI assistants audit our suites to determine the placement of our accessibility checks, further reducing the manual effort for developers. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eClosing Thoughts \u003c/b\u003e\u003c/p\u003e\n\u003cp\u003eWe had to make some unexpected trade-offs in this project, balancing the practical limitations of automated testing tools with the goal of reducing the burden on developers. While we couldn’t integrate accessibility checks completely into our frontend frameworks, we made significant strides towards that goal. We simplified the process for developers to add accessibility checks, ensured test results were easy to interpret, provided clear documentation, and streamlined triage through Slack workflows. In the end, we were able to add test coverage for accessibility in the Slack product, ensuring that our customers that require accessibility features have a consistent experience.\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur automated Axe checks have reduced our reliance on manual testing and now complement other essential forms of testing—like manual testing and usability studies. At the moment, developers need to manually add checks, but we’ve laid the groundwork to make the process as straightforward as possible with the possibility for AI-driven creation of accessibility tests.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eRoadblocks like framework complexity or setup difficulties shouldn’t discourage you from pursuing automation as part of a broader accessibility strategy. Even when it’s not feasible to hide the automated checks completely behind the scenes of the framework, there are ways to make the work impactful by focusing on the developer experience. This project has not only strengthened our overall accessibility testing approach, it’s also reinforced the culture of accessibility that has always been central to Slack. We hope it inspires others to look more closely at how automated accessibility might fit into your testing strategy, even if it requires navigating a few technical hurdles along the way.\u003c/span\u003e\u003c/p\u003e\n\n\u003chr/\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003eThank you to everyone who spent significant time on the editing and revision of this blog post – Courtney Anderson-Clark, Lucy Cheng, Miriam Holland and Sergii Gorbachov.\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003eAnd a massive thank you to the Accessibility Team at Slack, Chanan Walia, Yura Zenevich, Chris Xu and Hye Jung Choi, for your help with everything related to this project, including editing this blog post!\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "19 min read",
  "publishedTime": "2025-01-07T20:46:20Z",
  "modifiedTime": "2025-01-08T22:33:59Z"
}
