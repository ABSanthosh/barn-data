{
  "id": "372a17f7-f657-4f37-828b-1909f49ead14",
  "title": "IBM Granite 3.2 Brings New Vision Language Model, Chain of Thought Reasoning, Improved TimeSeries",
  "link": "https://www.infoq.com/news/2025/03/ibm-granite-3-2/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "IBM has introduced its new Granite 3.2 multi-modal and reasoning model. Granite 3.2 features experimental chain-of-thought reasoning capabilities that significantly improve its predecessor's performance, a new vision language model (VLM) outperforming larger models on several benchmarks, and smaller models for more efficient deployments. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Sun, 02 Mar 2025 15:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Time Series Data",
    "Large language models",
    "Enterprise",
    "IBM",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 4657,
  "excerpt": "IBM has introduced its new Granite 3.2 multi-modal and reasoning model. Granite 3.2 features experimental chain-of-thought reasoning capabilities that significantly improve its predecessor's performan",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250228123444/apple-touch-icon.png",
  "text": "IBM has introduced its new Granite 3.2 multi-modal and reasoning model. Granite 3.2 features experimental chain-of-thought reasoning capabilities that significantly improve its predecessor's performance, a new vision language model (VLM) outperforming larger models on several benchmarks, and smaller models for more efficient deployments. IBM says its Granite 3.2 8B Instruct and Granite 3.2 2B Instruct significantly outperform their 3.1 predecessors thanks to enhanced reasoning capabilities. Instead of providing specialized reasoning models as other companies currently do, IBM chose to include reasoning in their Instruct models as an option that can be toggled on and off depending on the particular task at hand. One technique IBM is using in Granite 3.2 to build their reasoning capabilities is inference scaling, which is inspired by the idea of letting an LLM generate multiple answers and then pick the best based on some reward model — only, applied to the reasoning process. In the context of reasoning tasks, this idea of scoring multiple answers to pick the best answer can be applied also to the “chain of thought” that often precedes answer generation. In fact, you don’t need to wait for the entire reasoning to be completed before deciding whether the reasoning was good or not. IBM's approach advances the one popularized by DeepSeek, which uses one inference model to measure its own progress by also using a search model to explore the reasoning space. So, the process reward model helps the LLM detect and avoid wrong reasoning turns, while the search algorithm makes the process more flexible. According to IBM, their inference scaling approach is able to boost performance on the MATH500 and AIME2024 math-reasoning benchmarks and let Granite 3.2 outperform much larger models like GPT-4o-0513 and Claude3.5-Sonnet-1022 for single-pass inference. Granite 3.2 also includes a VLM particularly aimed at document understanding, named Granite Vision 3.2 2B. According to IBM, this lightweight model rivals larger models on enterprise benchmarks, such as DocVQA and ChartQA, but is not intended to be used as a replacement for text-only Granite models. It was trained using a specific dataset, DocFM, that IBM built on curated enterprise data, including general document images, charts, flowcharts, and diagrams. Another component of the Granite family is Granite Guardian 3.2, a guardrail model able to detect risks in prompts and responses. Guardian 3.2 provides a similar performance to Guardian 3.1 at greater speed with lower inference costs and memory usage, IBM says. It introduces a new feature, verbalized confidence, to assess the potential risk in a more nuanced way by providing a confidence value. Guardian 3.2 comes in two variants, Guardian 3.2 5B (down from 8B in Granite 3.1) and Guardian 3.2 3B-A800M, with the added optimization of activating only 800 million parameters out of the total three billion at inference time. As a final note on Granite 3.2, it is worth mentioning that it brings new timeseries models (TTM) supporting weekly and daily forecasting in addition to the minutely to hourly resolutions already supported by its predecessor. TTM-R2 models (including the new TTM-R2.1 variants) top all models for point forecasting accuracy as measured by mean absolute scaled error (MASE). TTM-R2 also ranks in the top 5 for probabilistic forecasting, as measured by continuous ranked probability score (CRPS). In its announcement, IBM does not let go unnoticed that its TTM models are \"tiny\" in comparison to Google’s TimesFM-2.0 (500M parameters) and Amazon’s Chronos-Bolt-Base (205M parameters), which ranks second and third by MASE. While IBM's announcement appeared an impressive feat to some reddit users, others highlighted the fact that their reported performance may look like overfitting a few benchmarks while ignoring others. Still, although it would be naive to think that such small models (8B and 2B parameters) can be preferable to larger models performing much better overall or for complex tasks like coding, it is true that they may be a good fit for more specialized tasks. Others speculate about the fact that IBM's offering specifically targets enterprises, where it is important to have legal guarantees in case things go wrong or with potential IP issues with datasets used for training. All Granite models are licensed under the Apache 2.0 license and available on HuggingFace, watsonx.ai, Ollama, and LM Studio. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/03/ibm-granite-3-2/en/headerimage/ibm-granite-3-2-1740924483356.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://newsroom.ibm.com/2025-02-26-ibm-expands-granite-model-family-with-new-multi-modal-and-reasoning-ai-built-for-the-enterprise\"\u003eIBM has introduced its new Granite 3.2 multi-modal and reasoning model\u003c/a\u003e. Granite 3.2 features experimental chain-of-thought reasoning capabilities that significantly improve its predecessor\u0026#39;s performance, a new vision language model (VLM) outperforming larger models on several benchmarks, and smaller models for more efficient deployments.\u003c/p\u003e\n\n\u003cp\u003eIBM says its Granite 3.2 8B Instruct and Granite 3.2 2B Instruct significantly outperform their 3.1 predecessors thanks to enhanced reasoning capabilities. Instead of providing specialized reasoning models as other companies currently do, IBM chose to include reasoning in their Instruct models as an option that can be toggled on and off depending on the particular task at hand.\u003c/p\u003e\n\n\u003cp\u003eOne technique IBM is using in Granite 3.2 to build their reasoning capabilities is \u003ca href=\"https://research.ibm.com/blog/inference-scaling-reasoning-ai-model\"\u003e\u003cstrong\u003einference scaling\u003c/strong\u003e\u003c/a\u003e, which is inspired by the idea of letting an LLM generate multiple answers and then pick the best based on some reward model — only, applied to the reasoning process.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eIn the context of reasoning tasks, this idea of scoring multiple answers to pick the best answer can be applied also to the “chain of thought” that often precedes answer generation. In fact, you don’t need to wait for the entire reasoning to be completed before deciding whether the reasoning was good or not.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIBM\u0026#39;s approach advances the one popularized by DeepSeek, which uses one inference model to measure its own progress by also using a \u003ca href=\"https://probabilistic-inference-scaling.github.io/\"\u003esearch model\u003c/a\u003e to explore the reasoning space. So, the process reward model helps the LLM detect and avoid wrong reasoning turns, while the search algorithm makes the process more flexible.\u003c/p\u003e\n\n\u003cp\u003eAccording to IBM, their inference scaling approach is able to boost performance on the MATH500 and AIME2024 math-reasoning benchmarks and let Granite 3.2 outperform much larger models like GPT-4o-0513 and Claude3.5-Sonnet-1022 for single-pass inference.\u003c/p\u003e\n\n\u003cp\u003eGranite 3.2 also includes a VLM particularly aimed at document understanding, named Granite Vision 3.2 2B. According to IBM, this lightweight model rivals larger models on enterprise benchmarks, such as DocVQA and ChartQA, but is not intended to be used as a replacement for text-only Granite models. It was trained using a specific dataset, DocFM, that \u003ca href=\"https://arxiv.org/abs/2502.09927\"\u003eIBM built on curated enterprise data\u003c/a\u003e, including general document images, charts, flowcharts, and diagrams.\u003c/p\u003e\n\n\u003cp\u003eAnother component of the Granite family is Granite Guardian 3.2, a \u003ca href=\"https://www.ibm.com/granite/docs/models/guardian/\"\u003eguardrail model\u003c/a\u003e able to detect risks in prompts and responses. Guardian 3.2 provides a similar performance to Guardian 3.1 at greater speed with lower inference costs and memory usage, IBM says. It introduces a new feature, \u003cem\u003everbalized confidence\u003c/em\u003e, to assess the potential risk in a more nuanced way by providing a \u003cem\u003econfidence\u003c/em\u003e value.\u003c/p\u003e\n\n\u003cp\u003eGuardian 3.2 comes in two variants, Guardian 3.2 5B (down from 8B in Granite 3.1) and Guardian 3.2 3B-A800M, with the added optimization of activating only 800 million parameters out of the total three billion at inference time.\u003c/p\u003e\n\n\u003cp\u003eAs a final note on Granite 3.2, it is worth mentioning that it brings new timeseries models (TTM) supporting weekly and daily forecasting in addition to the minutely to hourly resolutions already supported by its predecessor.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eTTM-R2 models (including the new TTM-R2.1 variants) top all models for point forecasting accuracy as measured by \u003ca href=\"https://www.ibm.com/docs/en/cognos-analytics/11.1.0?topic=forecasting-statistical-details\"\u003emean absolute scaled error (MASE)\u003c/a\u003e. TTM-R2 also ranks in the top 5 for probabilistic forecasting, as measured by continuous ranked probability score (CRPS).\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIn its announcement, IBM does not let go unnoticed that its TTM models are \u0026#34;tiny\u0026#34; in comparison to Google’s TimesFM-2.0 (500M parameters) and Amazon’s Chronos-Bolt-Base (205M parameters), which ranks second and third by MASE.\u003c/p\u003e\n\n\u003cp\u003eWhile IBM\u0026#39;s announcement appeared an \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1iyudod/comment/mexihz2/\"\u003eimpressive feat to some reddit users\u003c/a\u003e, others highlighted the fact that their reported performance may look like \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1iyudod/comment/mexkjox/\"\u003eoverfitting a few benchmarks while ignoring others\u003c/a\u003e. Still, although it would be naive to think that such small models (8B and 2B parameters) can be preferable to larger models performing much better \u003cem\u003eoverall\u003c/em\u003e or for \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1iyudod/comment/mexigtq/\"\u003ecomplex tasks like coding\u003c/a\u003e, it is true that they may be a good fit for more specialized tasks.\u003c/p\u003e\n\n\u003cp\u003eOthers speculate about the fact that IBM\u0026#39;s offering specifically \u003ca href=\"https://news.ycombinator.com/item?id=40293946\"\u003etargets enterprises\u003c/a\u003e, where it is important to have \u003ca href=\"https://news.ycombinator.com/item?id=40301461\"\u003elegal guarantees in case things go wrong\u003c/a\u003e or with \u003ca href=\"https://news.ycombinator.com/item?id=40297162\"\u003epotential IP issues with datasets used for training\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eAll Granite models are licensed under the Apache 2.0 license and available on \u003ca href=\"https://huggingface.co/collections/ibm-granite/granite-32-language-models-67b3bc8c13508f6d064cff9a\"\u003eHuggingFace\u003c/a\u003e, \u003ca href=\"https://watsonx.ai/\"\u003ewatsonx.ai\u003c/a\u003e, \u003ca href=\"https://ollama.com/library/granite3.2\"\u003eOllama\u003c/a\u003e, and LM Studio.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-03-02T00:00:00Z",
  "modifiedTime": null
}
