{
  "id": "486ca112-ad31-43a9-a7dd-8dad7446c2d9",
  "title": "LiteRT: Maximum performance, simplified",
  "link": "https://developers.googleblog.com/en/litert-maximum-performance-simplified/",
  "description": "LiteRT has been improved to boost AI model performance and efficiency on mobile devices by effectively utilizing GPUs and NPUs, now requiring significantly less code, enabling simplified hardware accelerator selection, and more for optimal on-device performance.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Mogan Shieh, Terry (Woncheol) Heo, Jingjiang Li",
  "length": 10434,
  "excerpt": "LiteRT has been improved to boost AI model performance and efficiency on mobile devices by effectively utilizing GPUs and NPUs, now requiring significantly less code, enabling simplified hardware accelerator selection, and more for optimal on-device performance.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "Over the past decade, mobile phones have incorporated increasingly powerful purpose-specific accelerators including GPUs and recently, more powerful NPUs (Neural Processing Units). By accelerating your AI models on mobile GPUs and NPUs, you can speed up your models by up to 25x compared to CPU while also reducing power consumption by up to 5x. However, unlocking these outstanding performance benefits has proven challenging for most developers, as it requires wrangling HW-specific APIs in case of GPU inference or wrangling vendor-specific SDKs, formats, and runtimes for NPU inference.Listening to your feedback, the Google AI Edge team is excited to announce multiple improvements to LiteRT solving the challenges above, and accelerating AI on mobile more easily with increased performance. Our new release includes a new LiteRT API making on-device ML inference easier than ever, our latest cutting-edge GPU acceleration, new NPU support co-developed with MediaTek and Qualcomm (open for early access), and advanced inference features to maximize performance for on-device applications. Let’s dive in!MLDrift: Best GPU Acceleration YetGPUs have always been at the heart of LiteRT’s acceleration story, providing the broadest support and most consistent performance improvement. MLDrift, our latest version of GPU acceleration, pushes the bar even further with faster performance and improvements to support models of a significantly larger size through:Smarter Data Organization: MLDrift arranges data in a more efficient way by using optimized tensor layouts and storage types specifically tailored for how GPUs process data, reducing memory access time and speeding up AI calculations.Workgroup Optimization: Smart computation based on context (stage) and resource constraintsImproved Data Handling: Streamlining the way the accelerator receives and sends out tensor data to reduce overhead in data transfer and conversion optimizing for data locality.This results in significantly faster performance than CPUs, than previous versions of our TFLite GPU delegate, and even other GPU enabled frameworks particularly for CNN and Transformer models. Figure: Inference latency per model of LiteRT GPU compared to TFLite GPU, measured on Samsung 24. Find examples in our documentation and give GPU acceleration a try today.NPUs, AI specific accelerators, are becoming increasingly common in flagship phones. They allow you to run AI models much more efficiently, and in many cases much faster. In our internal testing compared to CPUs this acceleration can be up to 25x faster, and 5x more power efficient. (May 2025, based on internal testing)Typically, each vendor provides their own SDKs, including compilers, runtime, and other dependencies, to compile and execute models on their SoCs. The SDK must precisely match the specific SoC version and requires proper download and installation. LiteRT now provides a uniform way to develop and deploy models on NPUs, abstracting away all these complexities.Vendor compiler distribution: When installing the LiteRT PyPI package, we will automatically download the vendor SDKs for compiling models.Model and vendor runtime distribution: The compiled model and SoC runtime will need to be distributed with the app. As a developer you can handle this distribution yourself, or you can have Google Play distribute them for you. In our example code you can see how to use AI Packs and Feature Delivery to deliver the right model and runtime to the right device.We’re excited to partner with MediaTek and Qualcomm to allow developers to accelerate a wide variety of classic ML models, such as vision, audio, and NLP models, on MediaTek and Qualcomm NPUs. Increased model and domain support will continue over the coming year.This feature is available in private preview. For early access apply here. Simplified GPU and NPU Hardware AccelerationWe’ve made GPUs and NPUs easier than ever to use by simplifying the process in the latest version of the LiteRT APIs. With the latest changes, we have simplified the setup significantly with the ability to specify the target backend as an option. As an example, this is how a developer would specify GPU acceleration: // 1. Load model. auto model = *Model::Load(\"mymodel.tflite\"); // 2. Create a compiled model targeting GPU. auto compiled_model = *CompiledModel::Create(model, kLiteRtHwAcceleratorGpu); C++ Copied As you can see, the new CompiledModel API greatly simplifies how to specify the model and target backend(s) for acceleration.Advanced Inference for Performance OptimizationWhile using high performance backends is helpful, optimal performance of your application can be hindered by memory, or processor bottlenecks. With the new LiteRT APIs, you can address these challenges by leveraging built-in buffer interoperability to eliminate costly memory copy operations, and asynchronous execution to utilize idle processors in parallel.Seamless Buffer InteroperabilityThe new TensorBuffer API provides an efficient way to handle input/output data with LiteRT. It allows you to directly use data residing in hardware memory, such as OpenGL Buffers, as inputs or outputs for your CompiledModel, completely eliminating the need for costly CPU copies. auto tensor_buffer = *litert::TensorBuffer::CreateFromGlBuffer(tensor_type, opengl_buffer); C++ Copied This significantly reduces unnecessary CPU overhead and boosts performance.Additionally, the TensorBuffer API enables seamless copy-free conversions between different hardware memory types when supported by the system. Imagine effortlessly transforming data from an OpenGL Buffer to an OpenCL Buffer or even to an Android HardwareBuffer without any intermediate CPU transfers.This technique is key to handling the increasing data volumes and demanding performance required by increasingly complex AI models. You can find examples in our documentation on how to use TensorBuffer.Asynchronous ExecutionAsynchronous execution allows different parts of the AI model or independent tasks to run concurrently across CPU, GPU, and NPUs allowing you to opportunistically leverage available compute cycles from different processors to improve efficiency and responsiveness. For instance:the CPU might handle data preprocessingthe GPU could accelerate matrix multiplications in a neural network layer, andthe NPU might efficiently manage specific inference tasks – all happening in parallel.In applications which require real-time AI interactions, a task can be initiated on one processor and continue with other operations on another. Parallel processing minimizes latency and provides a smoother, more interactive user experience. By effectively managing and overlapping computations across multiple processors, asynchronous execution maximizes system throughput and ensures that the AI application remains fluid and reactive, even under heavy computational loads.Async execution is implemented by using OS-level mechanisms (e.g., sync fences on Android/Linux) allowing one HW accelerator to trigger upon the completion of another HW accelerator directly without involving the CPU. This reduces latency (up to 2x in our GPU async demo) and power consumption while making the pipeline more deterministic.Here is the code snippet showing async inference with OpenGL buffer input: // Create an input TensorBuffer based on tensor_type that wraps the given OpenGL // Buffer. env is an LiteRT environment to use existing EGL display and context. auto tensor_buffer_from_opengl = *litert::TensorBuffer::CreateFromGlBuffer(env, tensor_type, opengl_buffer); // Create an input event and attach it to the input buffer. Internally, it // creates and inserts a fence sync object into the current EGL command queue. auto input_event = *Event::CreateManaged(env, LiteRtEventTypeEglSyncFence); tensor_buffer_from_opengl.SetEvent(std::move(input_event)); // Create the input and output TensorBuffers… // Run async inference compiled_model1.RunAsync(input_buffers, output_buffers); C++ Copied More code examples are available in our documentation on how to leverage async execution.We encourage you to try out the latest acceleration capabilities and performance improvement techniques to bring your users the best possible experience while leveraging the latest AI models. To help you get started, check out our sample app with fully integrated examples of how to use all the features.All new LiteRT features mentioned in this blog can be found at: https://github.com/google-ai-edge/LiteRTFor more Google AI Edge news, read about our updates in on-device GenAI and our new AI Edge Portal service for broad coverage on-device benchmarking and evals.Explore this announcement and all Google I/O 2025 updates on io.google starting May 22.AcknowledgementsThank you to the members of the team, and collaborators for their contributions in making the advancements in this release possible: Advait Jain, Alan Kelly, Alexander Shaposhnikov, Andrei Kulik, Andrew Zhang, Akshat Sharma, Byungchul Kim, Chunlei Niu, Chuo-Ling Chang, Claudio Basile, Cormac Brick, David Massoud, Dillon Sharlet, Eamon Hugh, Ekaterina Ignasheva, Fengwu Yao, Frank Ban, Frank Barchard, Gerardo Carranza, Grant Jensen, Henry Wang, Ho Ko, Jae Yoo, Jiuqiang Tang, Juhyun Lee, Julius Kammerl, Khanh LeViet, Kris Tonthat, Lin Chen, Lu Wang, Luke Boyer, Marissa Ikonomidis, Mark Sherwood, Matt Kreileder, Matthias Grundmann, Misha Gutman, Pedro Gonnet, Ping Yu, Quentin Khan, Raman Sarokin, Sachin Kotwani, Steven Toribio, Suleman Shahid, Teng-Hui Zhu, Volodymyr Kysenko, Wai Hon Law, Weiyi Wang, Youchuan Hu, Yu-Hui Chen",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/O25-BHero-AI-3-Meta.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"oyqvt\"\u003eOver the past decade, mobile phones have incorporated increasingly powerful purpose-specific accelerators including GPUs and recently, more powerful NPUs (Neural Processing Units). By accelerating your AI models on mobile GPUs and NPUs, you can speed up your models by up to 25x compared to CPU while also reducing power consumption by up to 5x. However, unlocking these outstanding performance benefits has proven challenging for most developers, as it requires wrangling HW-specific APIs in case of GPU inference or wrangling vendor-specific SDKs, formats, and runtimes for NPU inference.\u003c/p\u003e\u003cp data-block-key=\"bp808\"\u003eListening to your feedback, the Google AI Edge team is excited to announce multiple improvements to LiteRT solving the challenges above, and accelerating AI on mobile more easily with increased performance. Our new release includes a new LiteRT API making on-device ML inference easier than ever, our latest cutting-edge GPU acceleration, new NPU support co-developed with MediaTek and Qualcomm (open for early access), and advanced inference features to maximize performance for on-device applications. Let’s dive in!\u003c/p\u003e\u003ch2 data-block-key=\"0uzy7\" id=\"mldrift:-best-gpu-acceleration-yet\"\u003e\u003cbr/\u003eMLDrift: Best GPU Acceleration Yet\u003c/h2\u003e\u003cp data-block-key=\"alcbr\"\u003eGPUs have always been at the heart of LiteRT’s acceleration story, providing the broadest support and most consistent performance improvement. MLDrift, our latest version of GPU acceleration, pushes the bar even further with faster performance and improvements to support models of a significantly larger size through:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"15fkp\"\u003e\u003cb\u003eSmarter Data Organization\u003c/b\u003e: MLDrift arranges data in a more efficient way by using optimized tensor layouts and storage types specifically tailored for how GPUs process data, reducing memory access time and speeding up AI calculations.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"1g96l\"\u003e\u003cb\u003eWorkgroup Optimization\u003c/b\u003e: Smart computation based on context (stage) and resource constraints\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"1p4ee\"\u003e\u003cb\u003eImproved Data Handling\u003c/b\u003e: Streamlining the way the accelerator receives and sends out tensor data to reduce overhead in data transfer and conversion optimizing for data locality.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"9goc0\"\u003e\u003cbr/\u003eThis results in significantly faster performance than CPUs, than previous versions of our TFLite GPU delegate, and even other GPU enabled frameworks particularly for CNN and Transformer models.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n            \n                \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_hP4r4BV.original.png\" alt=\"model of LiteRT GPU compared to TFLite GPU\"/\u003e\u003c/p\u003e\u003cp\u003e\n                        Figure: Inference latency per model of LiteRT GPU compared to TFLite GPU, measured on Samsung 24.\n                    \u003c/p\u003e\n                \n            \n        \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"a39v3\"\u003eFind examples in our \u003ca href=\"https://ai.google.dev/edge/litert/next/gpu\"\u003edocumentation\u003c/a\u003e and give GPU acceleration a try today.\u003c/p\u003e\u003cp data-block-key=\"80hml\"\u003eNPUs, AI specific accelerators, are becoming increasingly common in flagship phones. They allow you to run AI models much more efficiently, and in many cases much faster. In our internal testing compared to CPUs this acceleration can be up to 25x faster, and 5x more power efficient. (May 2025, based on internal testing)\u003c/p\u003e\u003cp data-block-key=\"46snb\"\u003eTypically, each vendor provides their own SDKs, including compilers, runtime, and other dependencies, to compile and execute models on their SoCs. The SDK must precisely match the specific SoC version and requires proper download and installation. LiteRT now provides a uniform way to develop and deploy models on NPUs, abstracting away all these complexities.\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"8h2r8\"\u003e\u003cb\u003eVendor compiler distribution:\u003c/b\u003e When installing the LiteRT PyPI package, we will automatically download the vendor SDKs for compiling models.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"4ggq5\"\u003e\u003cb\u003eModel and vendor runtime distribution:\u003c/b\u003e The compiled model and SoC runtime will need to be distributed with the app. As a developer you can handle this distribution yourself, or you can have Google Play distribute them for you. In our example code you can see how to use \u003ca href=\"https://developer.android.com/google/play/on-device-ai#use_litert_and_mediapipe_with_ai_packs\"\u003eAI Packs\u003c/a\u003e and \u003ca href=\"https://developer.android.com/guide/playcore/feature-delivery\"\u003eFeature Delivery\u003c/a\u003e to deliver the right model and runtime to the right device.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"822bh\"\u003e\u003cbr/\u003eWe’re excited to partner with MediaTek and Qualcomm to allow developers to accelerate a wide variety of classic ML models, such as vision, audio, and NLP models, on MediaTek and Qualcomm NPUs. Increased model and domain support will continue over the coming year.\u003c/p\u003e\u003cp data-block-key=\"9tddm\"\u003eThis feature is available in private preview. For early access apply \u003ca href=\"https://forms.gle/CoH4jpLwxiEYvDvF6\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_1ZK2LfL.original.png\" alt=\"classic ML models\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"8aivt\" id=\"simplified-gpu-and-npu-hardware-acceleration\"\u003eSimplified GPU and NPU Hardware Acceleration\u003c/h2\u003e\u003cp data-block-key=\"blon8\"\u003eWe’ve made GPUs and NPUs easier than ever to use by simplifying the process in the latest version of the LiteRT APIs. With the latest changes, we have simplified the setup significantly with the ability to specify the target backend as an option. As an example, this is how a developer would specify GPU acceleration:\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003e// 1. Load model.\n    auto model = *Model::Load(\u0026#34;mymodel.tflite\u0026#34;);\n\n// 2. Create a compiled model targeting GPU.\n    auto compiled_model = *CompiledModel::Create(model, kLiteRtHwAcceleratorGpu);\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        C++\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"rnjgw\"\u003eAs you can see, the new CompiledModel API greatly simplifies how to specify the model and target backend(s) for acceleration.\u003c/p\u003e\u003ch2 data-block-key=\"47ssd\" id=\"advanced-inference-for-performance-optimization\"\u003e\u003cbr/\u003eAdvanced Inference for Performance Optimization\u003c/h2\u003e\u003cp data-block-key=\"77ivd\"\u003eWhile using high performance backends is helpful, optimal performance of your application can be hindered by memory, or processor bottlenecks. With the new LiteRT APIs, you can address these challenges by leveraging built-in buffer interoperability to eliminate costly memory copy operations, and asynchronous execution to utilize idle processors in parallel.\u003c/p\u003e\u003ch3 data-block-key=\"9mwal\" id=\"seamless-buffer-interoperability\"\u003e\u003cb\u003e\u003cbr/\u003eSeamless Buffer Interoperability\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"6ae0r\"\u003eThe new TensorBuffer API provides an efficient way to handle input/output data with LiteRT. It allows you to directly use data residing in hardware memory, such as OpenGL Buffers, as inputs or outputs for your CompiledModel, completely eliminating the need for costly CPU copies.\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003eauto tensor_buffer = *litert::TensorBuffer::CreateFromGlBuffer(tensor_type, opengl_buffer);\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        C++\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cdiv data-block-key=\"a39v3\"\u003e\u003cp\u003eThis significantly reduces unnecessary CPU overhead and boosts performance.\u003c/p\u003e\u003cp\u003eAdditionally, the TensorBuffer API enables seamless copy-free conversions between different hardware memory types when supported by the system. Imagine effortlessly transforming data from an OpenGL Buffer to an OpenCL Buffer or even to an Android HardwareBuffer without any intermediate CPU transfers.\u003c/p\u003e\u003c/div\u003e\u003cp data-block-key=\"3piso\"\u003eThis technique is key to handling the increasing data volumes and demanding performance required by increasingly complex AI models. You can find examples in our documentation on how to use \u003ca href=\"https://ai.google.dev/edge/litert/next/gpu#zero-copy-gpu\"\u003eTensorBuffer\u003c/a\u003e.\u003c/p\u003e\u003ch3 data-block-key=\"t9bzi\" id=\"asynchronous-execution\"\u003e\u003cb\u003e\u003cbr/\u003eAsynchronous Execution\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"3r3am\"\u003eAsynchronous execution allows different parts of the AI model or independent tasks to run concurrently across CPU, GPU, and NPUs allowing you to opportunistically leverage available compute cycles from different processors to improve efficiency and responsiveness. For instance:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"9p27l\"\u003ethe CPU might handle data preprocessing\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"lhfp\"\u003ethe GPU could accelerate matrix multiplications in a neural network layer, and\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"c9bv8\"\u003ethe NPU might efficiently manage specific inference tasks – all happening in parallel.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"bbvh\"\u003e\u003cbr/\u003eIn applications which require real-time AI interactions, a task can be initiated on one processor and continue with other operations on another. Parallel processing minimizes latency and provides a smoother, more interactive user experience. By effectively managing and overlapping computations across multiple processors, asynchronous execution maximizes system throughput and ensures that the AI application remains fluid and reactive, even under heavy computational loads.\u003c/p\u003e\u003cp data-block-key=\"fv75k\"\u003eAsync execution is implemented by using OS-level mechanisms (e.g., sync fences on Android/Linux) allowing one HW accelerator to trigger upon the completion of another HW accelerator directly without involving the CPU. This reduces latency (up to 2x in our GPU async \u003ca href=\"https://github.com/google-ai-edge/LiteRT/tree/e61cd3ba0930f410702df8b109f85448c6df71ef/litert/samples/async_segmentation\"\u003edemo\u003c/a\u003e) and power consumption while making the pipeline more deterministic.\u003c/p\u003e\u003cp data-block-key=\"69en1\"\u003eHere is the code snippet showing async inference with OpenGL buffer input:\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003e// Create an input TensorBuffer based on tensor_type that wraps the given OpenGL \n// Buffer. env is an LiteRT environment to use existing EGL display and context.\n    auto tensor_buffer_from_opengl = *litert::TensorBuffer::CreateFromGlBuffer(env, \n    tensor_type, opengl_buffer);\n\n// Create an input event and attach it to the input buffer. Internally, it \n// creates and inserts a fence sync object into the current EGL command queue.                                                                                                                                                                                                                                                                   \n    auto input_event = *Event::CreateManaged(env, LiteRtEventTypeEglSyncFence);\n    tensor_buffer_from_opengl.SetEvent(std::move(input_event));\n\n// Create the input and output TensorBuffers…\n\n// Run async inference                                                                                                                                                \n    compiled_model1.RunAsync(input_buffers, output_buffers);\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        C++\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"a39v3\"\u003eMore code examples are available in our documentation on how to leverage \u003ca href=\"https://ai.google.dev/edge/litert/next/gpu#asynchronous-execution\"\u003easync execution\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"cd3jd\"\u003eWe encourage you to try out the latest acceleration capabilities and performance improvement techniques to bring your users the best possible experience while leveraging the latest AI models. To help you get started, check out our \u003ca href=\"https://github.com/google-ai-edge/LiteRT/tree/main/litert/samples/image_segmentation\"\u003esample app\u003c/a\u003e with fully integrated examples of how to use all the features.\u003c/p\u003e\u003cp data-block-key=\"7upar\"\u003eAll new LiteRT features mentioned in this blog can be found at: \u003ca href=\"https://github.com/google-ai-edge/LiteRT\"\u003ehttps://github.com/google-ai-edge/LiteRT\u003c/a\u003e\u003c/p\u003e\u003cp data-block-key=\"6o4bo\"\u003eFor more Google AI Edge news, read about our updates in \u003ca href=\"https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling\"\u003eon-device GenAI\u003c/a\u003e and our new \u003ca href=\"https://cloud.google.com/blog/products/ai-machine-learning/ai-edge-portal-brings-on-device-ml-testing-at-scale\"\u003eAI Edge Portal\u003c/a\u003e service for broad coverage on-device benchmarking and evals.\u003c/p\u003e\u003cp data-block-key=\"4eo10\"\u003eExplore this announcement and all Google I/O 2025 updates on \u003ca href=\"https://io.google/2025/?utm_source=blogpost\u0026amp;utm_medium=pr\u0026amp;utm_campaign=event\u0026amp;utm_content=\"\u003eio.google\u003c/a\u003e starting May 22.\u003c/p\u003e\u003chr/\u003e\u003ch3 data-block-key=\"9uqht\" id=\"acknowledgements\"\u003e\u003cb\u003eAcknowledgements\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"9hlor\"\u003e\u003csup\u003eThank you to the members of the team, and collaborators for their contributions in making the advancements in this release possible: Advait Jain, Alan Kelly, Alexander Shaposhnikov, Andrei Kulik, Andrew Zhang, Akshat Sharma, Byungchul Kim, Chunlei Niu, Chuo-Ling Chang, Claudio Basile, Cormac Brick, David Massoud, Dillon Sharlet, Eamon Hugh, Ekaterina Ignasheva, Fengwu Yao, Frank Ban, Frank Barchard, Gerardo Carranza, Grant Jensen, Henry Wang, Ho Ko, Jae Yoo, Jiuqiang Tang, Juhyun Lee, Julius Kammerl, Khanh LeViet, Kris Tonthat, Lin Chen, Lu Wang, Luke Boyer, Marissa Ikonomidis, Mark Sherwood, Matt Kreileder, Matthias Grundmann, Misha Gutman, Pedro Gonnet, Ping Yu, Quentin Khan, Raman Sarokin, Sachin Kotwani, Steven Toribio, Suleman Shahid, Teng-Hui Zhu, Volodymyr Kysenko, Wai Hon Law, Weiyi Wang, Youchuan Hu, Yu-Hui Chen\u003c/sup\u003e\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2025-05-20T00:00:00Z",
  "modifiedTime": null
}
